```diff
diff --git a/Android.bp b/Android.bp
index 095d0aff3d..3c384f6d95 100644
--- a/Android.bp
+++ b/Android.bp
@@ -60,3 +60,15 @@ phony_rule {
         default: true,
     }),
 }
+
+genrule {
+    name: "cpplint-art-all",
+    tools: ["cpplint"],
+    srcs: [
+        "**/*.h",
+        "**/*.cc",
+        "CPPLINT.cfg",
+    ],
+    out: ["cpplint_output.txt"],
+    cmd: "$(location cpplint) --quiet art/**/*.h art/**/*.cc > $(out)",
+}
diff --git a/Android.mk b/Android.mk
index d784cf6842..6de1caff20 100644
--- a/Android.mk
+++ b/Android.mk
@@ -252,14 +252,14 @@ LOCAL_PATH := $(art_path)
 build-art: build-art-host
 
 # For host, we extract the ICU data from the apex and install it to HOST_OUT/I18N_APEX.
-$(HOST_I18N_DATA): $(TARGET_OUT)/apex/$(I18N_APEX).apex $(HOST_OUT)/bin/deapexer
+$(HOST_I18N_DATA): $(TARGET_OUT)/apex/$(I18N_APEX).apex $(HOST_OUT)/bin/deapexer $(HOST_OUT)/bin/debugfs_static
 	$(call extract-from-apex,$(I18N_APEX))
 	rm -rf $(HOST_OUT)/$(I18N_APEX)
 	mkdir -p $(HOST_OUT)/$(I18N_APEX)/
 	cp -R $(TARGET_OUT)/apex/$(I18N_APEX)/etc/ $(HOST_OUT)/$(I18N_APEX)/
 	touch $@
 
-$(HOST_TZDATA_DATA): $(TARGET_OUT)/apex/$(TZDATA_APEX).apex $(HOST_OUT)/bin/deapexer
+$(HOST_TZDATA_DATA): $(TARGET_OUT)/apex/$(TZDATA_APEX).apex $(HOST_OUT)/bin/deapexer $(HOST_OUT)/bin/debugfs_static
 	$(call extract-from-apex,$(TZDATA_APEX))
 	rm -rf $(HOST_OUT)/$(TZDATA_APEX)
 	mkdir -p $(HOST_OUT)/$(TZDATA_APEX)/
diff --git a/TEST_MAPPING b/TEST_MAPPING
index 9bb66bf864..a9821a8792 100644
--- a/TEST_MAPPING
+++ b/TEST_MAPPING
@@ -5955,10 +5955,5 @@
     {
       "name": "libnativeloader_test"
     }
-  ],
-  "avf-presubmit": [
-    {
-      "name": "ComposHostTestCases"
-    }
   ]
 }
diff --git a/adbconnection/adbconnection.cc b/adbconnection/adbconnection.cc
index 2ef7a842fa..69f8136cde 100644
--- a/adbconnection/adbconnection.cc
+++ b/adbconnection/adbconnection.cc
@@ -184,7 +184,7 @@ void AdbConnectionDebuggerController::StartDebugger() {
   if (IsDebuggableOrProfilable()) {
     connection_->StartDebuggerThreads();
   } else {
-    LOG(ERROR) << "Not starting debugger since process cannot load the jdwp agent.";
+    LOG(VERBOSE) << "Not starting debugger since process cannot load the jdwp agent.";
   }
 }
 
@@ -299,7 +299,7 @@ static art::ObjPtr<art::mirror::Object> CreateAdbConnectionThread(art::Thread* s
           system_thread_group_field->GetOffset()));
   return art::WellKnownClasses::java_lang_Thread_init
       ->NewObject<'L', 'L', 'I', 'Z'>(
-          hs, self, system_thread_group, thr_name, /*priority=*/0, /*daemon=*/true)
+          hs, self, system_thread_group, thr_name, /*niceness=*/0, /*daemon=*/true)
       .Get();
 }
 
diff --git a/artd/Android.bp b/artd/Android.bp
index 63e4914f04..ab37e5adf5 100644
--- a/artd/Android.bp
+++ b/artd/Android.bp
@@ -108,9 +108,6 @@ art_cc_test {
     shared_libs: [
         "libartservice",
         "libarttools",
-        "libbase",
-        "libdexfile",
-        "libprofile",
     ],
 }
 
@@ -125,9 +122,6 @@ art_cc_test {
     static_libs: [
         "libartservice",
         "libarttools",
-        "libbase",
-        "libdexfile",
-        "libprofile",
     ],
     test_config_template: "art_standalone_artd_tests.xml",
 }
diff --git a/artd/artd.cc b/artd/artd.cc
index 144b783a29..7476a892f2 100644
--- a/artd/artd.cc
+++ b/artd/artd.cc
@@ -129,6 +129,7 @@ using ::android::base::Error;
 using ::android::base::Join;
 using ::android::base::make_scope_guard;
 using ::android::base::ParseInt;
+using ::android::base::ParseUint;
 using ::android::base::ReadFileToString;
 using ::android::base::Result;
 using ::android::base::Split;
@@ -1739,6 +1740,7 @@ Result<OatFileAssistantContext*> Artd::GetOatFileAssistantContext() {
                 .boot_class_path = *OR_RETURN(GetBootClassPath()),
                 .boot_class_path_locations = *OR_RETURN(GetBootClassPath()),
                 .deny_art_apex_data_files = DenyArtApexDataFiles(),
+                .sdk_version = GetSdkVersion(),
             }));
     std::string error_msg;
     if (!ofa_context_->FetchAll(&error_msg)) {
@@ -1831,6 +1833,28 @@ bool Artd::DenyArtApexDataFilesLocked() {
   return cached_deny_art_apex_data_files_.value();
 }
 
+uint32_t Artd::GetSdkVersion() {
+  std::lock_guard<std::mutex> lock(cache_mu_);
+  return GetSdkVersionLocked();
+}
+
+uint32_t Artd::GetSdkVersionLocked() {
+  if (!cached_sdk_version_.has_value()) {
+    const std::string sdk_version_str =
+        pre_reboot_build_props_ != nullptr
+            ? pre_reboot_build_props_->GetOrEmpty("ro.build.version.sdk")
+            : props_->GetOrEmpty("ro.build.version.sdk");
+
+    uint32_t sdk_version = static_cast<uint32_t>(SdkVersion::kUnset);
+    if (!sdk_version_str.empty() && !ParseUint(sdk_version_str, &sdk_version)) {
+      LOG(WARNING) << "Failed to parse the build version SDK: " << sdk_version_str;
+    }
+    cached_sdk_version_ = sdk_version;
+  }
+
+  return cached_sdk_version_.value();
+}
+
 Result<std::string> Artd::GetProfman() { return BuildArtBinPath("profman"); }
 
 Result<CmdlineBuilder> Artd::GetArtExecCmdlineBuilder() {
@@ -1908,6 +1932,14 @@ void Artd::AddCompilerConfigFlags(const std::string& instruction_set,
   args.AddRuntimeIf(DenyArtApexDataFiles(), "-Xdeny-art-apex-data-files")
       .AddRuntime("-Xtarget-sdk-version:%d", dexopt_options.targetSdkVersion)
       .AddRuntimeIf(dexopt_options.hiddenApiPolicyEnabled, "-Xhidden-api-policy:enabled");
+
+  const uint32_t sdk_version = GetSdkVersion();
+  if (sdk_version != static_cast<uint32_t>(SdkVersion::kUnset)) {
+    // TODO(b/204924812): Reuse the appropriate SDK_INT signature from
+    // AssumeValueOptions to generate the correctly formatted argument.
+    constexpr const char* kSdkIntSignature = "Landroid/os/Build$VERSION;->SDK_INT";
+    args.Add(ART_FORMAT("--assume-value={}:{}", kSdkIntSignature, sdk_version));
+  }
 }
 
 void Artd::AddPerfConfigFlags(PriorityClass priority_class,
@@ -2022,6 +2054,10 @@ ScopedAStatus Artd::preRebootInit(
   OR_RETURN_NON_FATAL(PreRebootInitClearEnvs());
   OR_RETURN_NON_FATAL(
       PreRebootInitSetEnvFromFile(init_environ_rc_path_.value_or("/init.environ.rc")));
+  if (pre_reboot_build_props_ == nullptr) {
+    pre_reboot_build_props_ = std::make_unique<BuildSystemProperties>(
+        OR_RETURN_NON_FATAL(BuildSystemProperties::Create("/system/build.prop")));
+  }
   if (!preparation_done) {
     OR_RETURN_NON_FATAL(PreRebootInitDeriveClasspath(classpath_file));
   }
@@ -2092,10 +2128,6 @@ Result<void> Artd::PreRebootInitDeriveClasspath(const std::string& path) {
     return ErrnoErrorf("Failed to create '{}'", path);
   }
 
-  if (pre_reboot_build_props_ == nullptr) {
-    pre_reboot_build_props_ = std::make_unique<BuildSystemProperties>(
-        OR_RETURN(BuildSystemProperties::Create("/system/build.prop")));
-  }
   std::string sdk_version = pre_reboot_build_props_->GetOrEmpty("ro.build.version.sdk");
   std::string codename = pre_reboot_build_props_->GetOrEmpty("ro.build.version.codename");
   std::string known_codenames =
diff --git a/artd/artd.h b/artd/artd.h
index d48a209b0d..66b44cff9d 100644
--- a/artd/artd.h
+++ b/artd/artd.h
@@ -328,6 +328,9 @@ class Artd : public aidl::com::android::server::art::BnArtd {
   bool DenyArtApexDataFiles() EXCLUDES(cache_mu_);
   bool DenyArtApexDataFilesLocked() REQUIRES(cache_mu_);
 
+  uint32_t GetSdkVersion() EXCLUDES(cache_mu_);
+  uint32_t GetSdkVersionLocked() REQUIRES(cache_mu_);
+
   android::base::Result<int> ExecAndReturnCode(const std::vector<std::string>& arg_vector,
                                                int timeout_sec,
                                                const ExecCallbacks& callbacks = ExecCallbacks(),
@@ -381,6 +384,7 @@ class Artd : public aidl::com::android::server::art::BnArtd {
   std::optional<bool> cached_use_jit_zygote_ GUARDED_BY(cache_mu_);
   std::optional<std::string> cached_user_defined_boot_image_locations_ GUARDED_BY(cache_mu_);
   std::optional<bool> cached_deny_art_apex_data_files_ GUARDED_BY(cache_mu_);
+  std::optional<uint32_t> cached_sdk_version_ GUARDED_BY(cache_mu_);
 
   std::mutex ofa_context_mu_;
   std::unique_ptr<OatFileAssistantContext> ofa_context_ GUARDED_BY(ofa_context_mu_);
diff --git a/artd/artd_test.cc b/artd/artd_test.cc
index f6eeda7e57..68161ec2f7 100644
--- a/artd/artd_test.cc
+++ b/artd/artd_test.cc
@@ -66,7 +66,7 @@
 #include "gtest/gtest.h"
 #include "oat/oat_file.h"
 #include "path_utils.h"
-#include "profile/profile_compilation_info.cc"
+#include "profile/profile_compilation_info.h"
 #include "profman/profman_result.h"
 #include "testing.h"
 #include "tools/binder_utils.h"
@@ -1025,6 +1025,7 @@ TEST_F(ArtdTest, dexoptDefaultFlagsWhenNoSystemProps) {
                                     Not(Contains(Flag("-Xmx", _))),
                                     Not(Contains("--compile-individually")),
                                     Not(Contains(Flag("--image-format=", _))),
+                                    Not(Contains(Flag("--assume-value=", _))),
                                     Not(Contains("--force-jit-zygote")),
                                     Not(Contains(Flag("--boot-image=", _))))),
                   _,
@@ -1056,6 +1057,7 @@ TEST_F(ArtdTest, dexoptFlagsFromSystemProps) {
   EXPECT_CALL(*mock_props_, GetProperty("dalvik.vm.boot-image")).WillOnce(Return("boot-image"));
   EXPECT_CALL(*mock_props_, GetProperty("dalvik.vm.dex2oat-flags"))
       .WillOnce(Return("--flag1 --flag2  --flag3"));
+  EXPECT_CALL(*mock_props_, GetProperty("ro.build.version.sdk")).WillOnce(Return("77"));
 
   EXPECT_CALL(*mock_exec_utils_,
               DoExecAndReturnCode(
@@ -1076,6 +1078,8 @@ TEST_F(ArtdTest, dexoptFlagsFromSystemProps) {
                                     Contains(Flag("--image-format=", "imgfmt")),
                                     Not(Contains("--force-jit-zygote")),
                                     Contains(Flag("--boot-image=", "boot-image")),
+                                    Contains(Flag("--assume-value=",
+                                                  "Landroid/os/Build$VERSION;->SDK_INT:77")),
                                     Contains("--flag1"),
                                     Contains("--flag2"),
                                     Contains("--flag3"))),
@@ -2965,6 +2969,8 @@ TEST_F(ArtdTest, BuildSystemProperties) {
 
 class ArtdPreRebootTest : public ArtdTest {
  protected:
+  static constexpr const char* kDefaultBuildVersionSdk = "35";
+
   void SetUp() override {
     ArtdTest::SetUp();
 
@@ -2982,7 +2988,7 @@ class ArtdPreRebootTest : public ArtdTest {
 
     ON_CALL(*mock_pre_reboot_build_props_, GetProperty).WillByDefault(Return(""));
     ON_CALL(*mock_pre_reboot_build_props_, GetProperty("ro.build.version.sdk"))
-        .WillByDefault(Return("35"));
+        .WillByDefault(Return(kDefaultBuildVersionSdk));
     ON_CALL(*mock_pre_reboot_build_props_, GetProperty("ro.build.version.codename"))
         .WillByDefault(Return("Baklava"));
     ON_CALL(*mock_pre_reboot_build_props_, GetProperty("ro.build.version.known_codenames"))
@@ -3042,7 +3048,8 @@ TEST_F(ArtdPreRebootTest, preRebootInit) {
                                     AllOf(Contains(art_root_ + "/bin/art_exec"),
                                           Contains("--drop-capabilities")),
                                     AllOf(Contains("/apex/com.android.sdkext/bin/derive_classpath"),
-                                          Contains(Flag("--override-device-sdk-version=", "35")),
+                                          Contains(Flag("--override-device-sdk-version=",
+                                                        kDefaultBuildVersionSdk)),
                                           Contains(Flag("--override-device-codename=", "Baklava")),
                                           Contains(Flag("--override-device-known-codenames=",
                                                         "VanillaIceCream,Baklava")))),
@@ -3204,13 +3211,21 @@ TEST_F(ArtdPreRebootTest, preRebootInitCancelled) {
 
 TEST_F(ArtdPreRebootTest, dexopt) {
   std::string profile_file = OR_FATAL(BuildProfileOrDmPath(profile_path_.value()));
+  std::string assume_value_sdk_int =
+      std::string("Landroid/os/Build$VERSION;->SDK_INT:") + kDefaultBuildVersionSdk;
 
   dexopt_options_.generateAppImage = true;
 
   EXPECT_CALL(
       *mock_exec_utils_,
       DoExecAndReturnCode(
-          WhenSplitBy("--", _, Contains(Flag("--profile-file-fd=", FdOf(profile_file)))), _, _))
+          WhenSplitBy(
+              "--",
+              _,
+              AllOf(Contains(Flag("--profile-file-fd=", FdOf(profile_file))),
+                    Contains(Flag("--assume-value=", assume_value_sdk_int)))),
+          _,
+          _))
       .WillOnce(DoAll(WithArg<0>(WriteToFdFlag("--oat-fd=", "oat")),
                       WithArg<0>(WriteToFdFlag("--output-vdex-fd=", "vdex")),
                       WithArg<0>(WriteToFdFlag("--app-image-fd=", "art")),
diff --git a/benchmark/Android.bp b/benchmark/Android.bp
index e3c9482493..1f055d936f 100644
--- a/benchmark/Android.bp
+++ b/benchmark/Android.bp
@@ -29,10 +29,8 @@ art_cc_library {
     host_supported: true,
     defaults: ["art_defaults"],
     srcs: [
-        "jni_loader.cc",
         "jobject-benchmark/jobject_benchmark.cc",
         "jni-perf/perf_jni.cc",
-        "micro-native/micro_native.cc",
         "scoped-primitive-array/scoped_primitive_array.cc",
     ],
     target: {
@@ -57,34 +55,6 @@ art_cc_library {
     ],
 }
 
-art_cc_library {
-    name: "libartbenchmark-micronative-host",
-    host_supported: true,
-    device_supported: false,
-    defaults: ["art_debug_defaults"],
-    srcs: [
-        "jni_loader.cc",
-        "micro-native/micro_native.cc",
-    ],
-    shared_libs: [
-    ],
-    static_libs: [
-    ],
-    header_libs: ["jni_headers"],
-    stl: "libc++_static",
-    target: {
-        // This has to be duplicated for android and host to make sure it
-        // comes after the -Wframe-larger-than warnings inserted by art.go
-        // target-specific properties
-        android: {
-            cflags: ["-Wno-frame-larger-than="],
-        },
-        host: {
-            cflags: ["-Wno-frame-larger-than="],
-        },
-    },
-}
-
 art_cc_library {
     name: "libgolemtiagent",
     host_supported: true,
diff --git a/benchmark/micro-native/micro_native.cc b/benchmark/micro-native/micro_native.cc
deleted file mode 100644
index dffbf3b11d..0000000000
--- a/benchmark/micro-native/micro_native.cc
+++ /dev/null
@@ -1,146 +0,0 @@
-/*
- * Copyright (C) 2016 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include <jni.h>
-#include <stdio.h>
-
-#ifndef NATIVE_METHOD
-#define NATIVE_METHOD(className, functionName, signature) \
-    { #functionName, signature, reinterpret_cast<void*>(className ## _ ## functionName) }
-#endif
-#define NELEM(x) (sizeof(x)/sizeof((x)[0]))
-
-#define GLUE4(a, b, c, d) a ## b ## c ## d
-#define GLUE4_(a, b, c, d) GLUE4(a, b, c, d)
-
-#define CLASS_NAME "benchmarks/MicroNative/java/NativeMethods"
-#define CLASS_INFIX benchmarks_MicroNative_java_NativeMethods
-
-#define NAME_NORMAL_JNI_METHOD(name) GLUE4_(Java_, CLASS_INFIX, _, name)
-#define NAME_CRITICAL_JNI_METHOD(name) GLUE4_(JavaCritical_, CLASS_INFIX, _, name)
-
-#define DEFINE_NORMAL_JNI_METHOD(ret, name) extern "C" JNIEXPORT ret JNICALL GLUE4_(Java_, CLASS_INFIX, _, name)
-#define DEFINE_CRITICAL_JNI_METHOD(ret, name) extern "C" JNIEXPORT ret JNICALL GLUE4_(JavaCritical_, CLASS_INFIX, _, name)
-
-static void NativeMethods_emptyJniStaticSynchronizedMethod0(JNIEnv*, jclass) { }
-static void NativeMethods_emptyJniSynchronizedMethod0(JNIEnv*, jclass) { }
-
-static JNINativeMethod gMethods_NormalOnly[] = {
-  NATIVE_METHOD(NativeMethods, emptyJniStaticSynchronizedMethod0, "()V"),
-  NATIVE_METHOD(NativeMethods, emptyJniSynchronizedMethod0, "()V"),
-};
-
-static void NativeMethods_emptyJniMethod0(JNIEnv*, jobject) { }
-static void NativeMethods_emptyJniMethod6(JNIEnv*, jobject, int, int, int, int, int, int) { }
-static void NativeMethods_emptyJniMethod6L(JNIEnv*, jobject, jobject, jarray, jarray, jobject,
-                                           jarray, jarray) { }
-static void NativeMethods_emptyJniStaticMethod6L(JNIEnv*, jclass, jobject, jarray, jarray, jobject,
-                                                 jarray, jarray) { }
-
-static void NativeMethods_emptyJniStaticMethod0(JNIEnv*, jclass) { }
-static void NativeMethods_emptyJniStaticMethod6(JNIEnv*, jclass, int, int, int, int, int, int) { }
-
-static JNINativeMethod gMethods[] = {
-  NATIVE_METHOD(NativeMethods, emptyJniMethod0, "()V"),
-  NATIVE_METHOD(NativeMethods, emptyJniMethod6, "(IIIIII)V"),
-  NATIVE_METHOD(NativeMethods, emptyJniMethod6L, "(Ljava/lang/String;[Ljava/lang/String;[[ILjava/lang/Object;[Ljava/lang/Object;[[[[Ljava/lang/Object;)V"),
-  NATIVE_METHOD(NativeMethods, emptyJniStaticMethod6L, "(Ljava/lang/String;[Ljava/lang/String;[[ILjava/lang/Object;[Ljava/lang/Object;[[[[Ljava/lang/Object;)V"),
-  NATIVE_METHOD(NativeMethods, emptyJniStaticMethod0, "()V"),
-  NATIVE_METHOD(NativeMethods, emptyJniStaticMethod6, "(IIIIII)V"),
-};
-
-static void NativeMethods_emptyJniMethod0_Fast(JNIEnv*, jobject) { }
-static void NativeMethods_emptyJniMethod6_Fast(JNIEnv*, jobject, int, int, int, int, int, int) { }
-static void NativeMethods_emptyJniMethod6L_Fast(JNIEnv*, jobject, jobject, jarray, jarray, jobject,
-                                                jarray, jarray) { }
-static void NativeMethods_emptyJniStaticMethod6L_Fast(JNIEnv*, jclass, jobject, jarray, jarray,
-                                                      jobject, jarray, jarray) { }
-
-static void NativeMethods_emptyJniStaticMethod0_Fast(JNIEnv*, jclass) { }
-static void NativeMethods_emptyJniStaticMethod6_Fast(JNIEnv*, jclass, int, int, int, int, int, int) { }
-
-static JNINativeMethod gMethods_Fast[] = {
-  NATIVE_METHOD(NativeMethods, emptyJniMethod0_Fast, "()V"),
-  NATIVE_METHOD(NativeMethods, emptyJniMethod6_Fast, "(IIIIII)V"),
-  NATIVE_METHOD(NativeMethods, emptyJniMethod6L_Fast, "(Ljava/lang/String;[Ljava/lang/String;[[ILjava/lang/Object;[Ljava/lang/Object;[[[[Ljava/lang/Object;)V"),
-  NATIVE_METHOD(NativeMethods, emptyJniStaticMethod6L_Fast, "(Ljava/lang/String;[Ljava/lang/String;[[ILjava/lang/Object;[Ljava/lang/Object;[[[[Ljava/lang/Object;)V"),
-  NATIVE_METHOD(NativeMethods, emptyJniStaticMethod0_Fast, "()V"),
-  NATIVE_METHOD(NativeMethods, emptyJniStaticMethod6_Fast, "(IIIIII)V"),
-};
-
-// Have both a Java_ and a JavaCritical_ version of the same empty method.
-// The runtime automatically selects the right one when doing a dlsym-based native lookup.
-DEFINE_NORMAL_JNI_METHOD(void,   emptyJniStaticMethod0_1Critical)(JNIEnv*, jclass) { }
-DEFINE_CRITICAL_JNI_METHOD(void, emptyJniStaticMethod0_1Critical)() { }
-DEFINE_NORMAL_JNI_METHOD(void,   emptyJniStaticMethod6_1Critical)(JNIEnv*, jclass, int, int, int, int, int, int) { }
-DEFINE_CRITICAL_JNI_METHOD(void, emptyJniStaticMethod6_1Critical)(int, int, int, int, int, int) { }
-
-static JNINativeMethod gMethods_Critical[] = {
-  // Don't use NATIVE_METHOD because the name is mangled differently.
-  { "emptyJniStaticMethod0_Critical", "()V",
-        reinterpret_cast<void*>(NAME_CRITICAL_JNI_METHOD(emptyJniStaticMethod0_1Critical)) },
-  { "emptyJniStaticMethod6_Critical", "(IIIIII)V",
-        reinterpret_cast<void*>(NAME_CRITICAL_JNI_METHOD(emptyJniStaticMethod6_1Critical)) }
-};
-
-void jniRegisterNativeMethods(JNIEnv* env,
-                              const char* className,
-                              const JNINativeMethod* methods,
-                              int numMethods) {
-    jclass c = env->FindClass(className);
-    if (c == nullptr) {
-        char* tmp;
-        const char* msg;
-        if (asprintf(&tmp,
-                     "Native registration unable to find class '%s'; aborting...",
-                     className) == -1) {
-            // Allocation failed, print default warning.
-            msg = "Native registration unable to find class; aborting...";
-        } else {
-            msg = tmp;
-        }
-        env->FatalError(msg);
-    }
-
-    if (env->RegisterNatives(c, methods, numMethods) < 0) {
-        char* tmp;
-        const char* msg;
-        if (asprintf(&tmp, "RegisterNatives failed for '%s'; aborting...", className) == -1) {
-            // Allocation failed, print default warning.
-            msg = "RegisterNatives failed; aborting...";
-        } else {
-            msg = tmp;
-        }
-        env->FatalError(msg);
-    }
-}
-
-void register_micro_native_methods(JNIEnv* env) {
-  jniRegisterNativeMethods(env, CLASS_NAME, gMethods_NormalOnly, NELEM(gMethods_NormalOnly));
-  jniRegisterNativeMethods(env, CLASS_NAME, gMethods, NELEM(gMethods));
-  jniRegisterNativeMethods(env, CLASS_NAME, gMethods_Fast, NELEM(gMethods_Fast));
-
-  if (env->FindClass("dalvik/annotation/optimization/CriticalNative") != nullptr) {
-    // Only register them explicitly if the annotation is present.
-    jniRegisterNativeMethods(env, CLASS_NAME, gMethods_Critical, NELEM(gMethods_Critical));
-  } else {
-    if (env->ExceptionCheck()) {
-      // It will throw NoClassDefFoundError
-      env->ExceptionClear();
-    }
-  }
-  // else let them be registered implicitly.
-}
diff --git a/build/Android.bp b/build/Android.bp
index 17f6891c1b..bccd7a0e56 100644
--- a/build/Android.bp
+++ b/build/Android.bp
@@ -13,6 +13,7 @@ bootstrap_go_package {
     pkgPath: "android/soong/art",
     deps: [
         "blueprint",
+        "blueprint-gobtools",
         "blueprint-pathtools",
         "blueprint-proptools",
         "soong",
@@ -22,6 +23,7 @@ bootstrap_go_package {
     ],
     srcs: [
         "art.go",
+        "art_gob_enc.go",
         "codegen.go",
         "makevars.go",
     ],
@@ -303,6 +305,48 @@ cc_defaults {
     },
 }
 
+// Add a dependency on liblog suitable for use in *_static_defaults.
+cc_defaults {
+    name: "art_liblog_static_defaults",
+    target: {
+        android: {
+            shared_libs: ["liblog"],
+        },
+        not_windows: {
+            whole_static_libs: ["liblog"],
+        },
+        windows: {
+            whole_static_libs: ["liblog"],
+        },
+    },
+}
+
+// Add a dependency on libz suitable for use in *_static_defaults.
+cc_defaults {
+    name: "art_libz_static_defaults",
+    target: {
+        android: {
+            shared_libs: ["libz"],
+        },
+        // Link libz dynamically on host to avoid odr-violation errors in ASAN
+        // builds, because the LLVM runtime may load libz-host.so dynamically.
+        not_windows: {
+            shared_libs: ["libz"],
+        },
+        windows: {
+            whole_static_libs: ["libz"],
+        },
+        // art/tools/build-linux-x86-host-tools.sh uses BUILD_HOST_static=true
+        // to build static binaries, which means we need to use static libz
+        // there. We cannot target static binaries here, so let's target musl
+        // instead, because that script uses musl and we don't use it anywhere
+        // else.
+        linux_musl: {
+            whole_static_libs: ["libz"],
+        },
+    },
+}
+
 // A version of conscrypt only for enabling the "-hostdex" version to test ART on host.
 java_library {
     // We need our own name to not clash with the conscrypt library.
diff --git a/build/Android.cpplint.mk b/build/Android.cpplint.mk
index 6eeaa49d8d..f364baf9b1 100644
--- a/build/Android.cpplint.mk
+++ b/build/Android.cpplint.mk
@@ -42,11 +42,6 @@ ART_CPPLINT_CFG := $(addprefix $(LOCAL_PATH)/, $(call all-subdir-named-files,CPP
 .PHONY: cpplint-art
 cpplint-art: cpplint-art-phony
 
-# "mm cpplint-art-all" to manually execute cpplint.py on all files (very slow).
-.PHONY: cpplint-art-all
-cpplint-art-all:
-	$(ART_CPPLINT) $(ART_CPPLINT_FLAGS) $(ART_CPPLINT_SRC)
-
 OUT_CPPLINT := $(TARGET_COMMON_OUT_ROOT)/cpplint
 
 # Build up the list of all targets for linting the ART source files.
diff --git a/build/Android.gtest.mk b/build/Android.gtest.mk
index fae37a2f76..8a89abdddd 100644
--- a/build/Android.gtest.mk
+++ b/build/Android.gtest.mk
@@ -260,9 +260,14 @@ else
 # (with the x86-64 ABI, as this allows symbolization of both x86 and x86-64). We don't do this in
 # general as it loses all the color output, and we have our own symbolization step when not running
 # under ASAN.
+# TODO(b/413709861): detect_odr_violation=1 added to work around ODR violation
+# in the runtime: It loads libopenjdk(d).so using dlopen, which depends on
+# libopenjdkjvm(d).so, which depends on libartbase(d).so and libart(d).so, which
+# are also statically linked into gtests.
 $$(gtest_output): $$(gtest_exe) $$(gtest_deps)
 	$(hide) ($$(call ART_TEST_SKIP,$$(NAME)) && set -o pipefail && \
-		ASAN_OPTIONS=detect_leaks=1 timeout --foreground -k 180s 3600s \
+		ASAN_OPTIONS=detect_leaks=1:detect_odr_violation=1 \
+			timeout --foreground -k 180s 3600s \
 			$(HOST_OUT_EXECUTABLES)/signal_dumper -s 15 \
 				$$< --gtest_output=xml:$$@ 2>&1 | tee $$<.tmp.out >&2 && \
 		{ $$(call ART_TEST_PASSED,$$(NAME)) ; rm $$<.tmp.out ; }) || \
diff --git a/build/OWNERS b/build/OWNERS
new file mode 100644
index 0000000000..5789722062
--- /dev/null
+++ b/build/OWNERS
@@ -0,0 +1,2 @@
+# Soong plugin owned by Soong team.
+per-file Android.bp,*.go,go.mod,go.work,go.work.sum = file:platform/build/soong:/OWNERS
diff --git a/build/apex/Android.bp b/build/apex/Android.bp
index f91b7863ee..fad1e15acf 100644
--- a/build/apex/Android.bp
+++ b/build/apex/Android.bp
@@ -145,6 +145,11 @@ apex_defaults {
             ],
         },
     },
+
+    licenses: [
+        "art_license",
+        "opensourcerequest",
+    ],
 }
 
 // Default values shared by Debug and Testing ART APEXes.
@@ -339,6 +344,10 @@ apex_test {
     native_shared_libs: [
         "libart",
         "libartd",
+        "libarttest",
+        "libarttestd",
+        "libtiagent",
+        "libtiagentd",
     ],
     multilib: {
         first: {
diff --git a/build/apex/art_apex_test.py b/build/apex/art_apex_test.py
index 3c39e43b9f..51a4ca08cc 100755
--- a/build/apex/art_apex_test.py
+++ b/build/apex/art_apex_test.py
@@ -603,6 +603,11 @@ class TestingChecker:
     self._checker.check_art_test_executable('art_runtime_tests')
     self._checker.check_art_test_executable('art_sigchain_tests')
 
+    # Some libraries are in odd location (libarttest(d) and libtiagent(d)).
+    # We intend to remove the whole testing apex, so just ignore those for now.
+    self._checker.ignore_path('lib*/com.android.art')
+    self._checker.ignore_path('lib*/com.android.art/lib*')
+
     # Check ART test tools.
     self._checker.check_executable('signal_dumper')
 
diff --git a/build/art.go b/build/art.go
index c4e9f9230f..bc22570aa2 100644
--- a/build/art.go
+++ b/build/art.go
@@ -18,8 +18,8 @@ import (
 	"fmt"
 	"path/filepath"
 	"strings"
-	"sync"
 
+	"github.com/google/blueprint"
 	"github.com/google/blueprint/proptools"
 
 	"android/soong/android"
@@ -27,8 +27,18 @@ import (
 	"android/soong/cc/config"
 )
 
+//go:generate go run ../../build/blueprint/gobtools/codegen/gob_gen.go
+
 var supportedArches = []string{"arm", "arm64", "riscv64", "x86", "x86_64"}
 
+// @auto-generate: gob
+type testInstallInfo struct {
+	Testcases map[string]string
+	TestMap   map[string][]string
+}
+
+var testInstallInfoProvider = blueprint.NewProvider[testInstallInfo]()
+
 func globalFlags(ctx android.LoadHookContext) ([]string, []string) {
 	var cflags []string
 	var asflags []string
@@ -70,7 +80,7 @@ func globalFlags(ctx android.LoadHookContext) ([]string, []string) {
 	// TODO: deprecate and then eventually remove ART_USE_GENERATIONAL_CC in favor of
 	// ART_USE_GENERATIONAL_GC
 	if !ctx.Config().IsEnvFalse("ART_USE_READ_BARRIER") && ctx.Config().ArtUseReadBarrier() &&
-	   !ctx.Config().IsEnvTrue("ART_USE_RESTRICTED_MODE") {
+		!ctx.Config().IsEnvTrue("ART_USE_RESTRICTED_MODE") {
 		// Used to change the read barrier type. Valid values are BAKER, TABLELOOKUP.
 		// The default is BAKER.
 		barrierType := ctx.Config().GetenvWithDefault("ART_READ_BARRIER_TYPE", "BAKER")
@@ -82,7 +92,7 @@ func globalFlags(ctx android.LoadHookContext) ([]string, []string) {
 			"-DART_READ_BARRIER_TYPE_IS_"+barrierType+"=1")
 
 		if !(ctx.Config().IsEnvFalse("ART_USE_GENERATIONAL_CC") ||
-		     ctx.Config().IsEnvFalse("ART_USE_GENERATIONAL_GC")) {
+			ctx.Config().IsEnvFalse("ART_USE_GENERATIONAL_GC")) {
 			cflags = append(cflags, "-DART_USE_GENERATIONAL_GC=1")
 		}
 		// Force CC only if ART_USE_READ_BARRIER was set to true explicitly during
@@ -94,7 +104,7 @@ func globalFlags(ctx android.LoadHookContext) ([]string, []string) {
 	} else if gcType == "CMC" {
 		tlab = true
 		if !(ctx.Config().IsEnvFalse("ART_USE_GENERATIONAL_CC") ||
-		     ctx.Config().IsEnvFalse("ART_USE_GENERATIONAL_GC")) {
+			ctx.Config().IsEnvFalse("ART_USE_GENERATIONAL_GC")) {
 			cflags = append(cflags, "-DART_USE_GENERATIONAL_GC=1")
 		}
 	}
@@ -289,66 +299,47 @@ func prefer32Bit(ctx android.LoadHookContext) {
 	ctx.PrependProperties(p)
 }
 
-var testMapKey = android.NewOnceKey("artTests")
-
-func testMap(config android.Config) map[string][]string {
-	return config.Once(testMapKey, func() interface{} {
-		return make(map[string][]string)
-	}).(map[string][]string)
-}
-
-func testInstall(ctx android.InstallHookContext) {
-	testMap := testMap(ctx.Config())
-
-	var name string
-	if ctx.Host() {
-		name = "host_"
-	} else {
-		name = "device_"
+func testInstall(data *testInstallInfo) func(ctx android.InstallHookContext) {
+	return func(ctx android.InstallHookContext) {
+		var name string
+		if ctx.Host() {
+			name = "host_"
+		} else {
+			name = "device_"
+		}
+		name += ctx.Arch().ArchType.String() + "_" + ctx.ModuleName()
+		data.TestMap[name] = append(data.TestMap[name], ctx.Path().String())
 	}
-	name += ctx.Arch().ArchType.String() + "_" + ctx.ModuleName()
-
-	artTestMutex.Lock()
-	defer artTestMutex.Unlock()
-
-	tests := testMap[name]
-	tests = append(tests, ctx.Path().String())
-	testMap[name] = tests
-}
-
-var testcasesContentKey = android.NewOnceKey("artTestcasesContent")
-
-func testcasesContent(config android.Config) map[string]string {
-	return config.Once(testcasesContentKey, func() interface{} {
-		return make(map[string]string)
-	}).(map[string]string)
 }
 
 // Binaries and libraries also need to be copied in the testcases directory for
 // running tests on host.  This method adds module to the list of needed files.
 // The 'key' is the file in testcases and 'value' is the path to copy it from.
 // The actual copy will be done in make since soong does not do installations.
-func addTestcasesFile(ctx android.InstallHookContext) {
-	if ctx.Os() != ctx.Config().BuildOS || ctx.Target().HostCross || ctx.Module().IsSkipInstall() {
-		return
-	}
-
-	testcasesContent := testcasesContent(ctx.Config())
-
-	artTestMutex.Lock()
-	defer artTestMutex.Unlock()
+func addTestcasesFile(data *testInstallInfo) func(ctx android.InstallHookContext) {
+	return func(ctx android.InstallHookContext) {
+		if ctx.Os() != ctx.Config().BuildOS || ctx.Target().HostCross || ctx.Module().IsSkipInstall() {
+			return
+		}
 
-	src := ctx.SrcPath().String()
-	path := strings.Split(ctx.Path().String(), "/")
-	// Keep last two parts of the install path (e.g. bin/dex2oat).
-	dst := strings.Join(path[len(path)-2:], "/")
-	if oldSrc, ok := testcasesContent[dst]; ok {
-		ctx.ModuleErrorf("Conflicting sources for %s: %s and %s", dst, oldSrc, src)
+		src := ctx.SrcPath().String()
+		path := strings.Split(ctx.Path().String(), "/")
+		// Keep last two parts of the install path (e.g. bin/dex2oat).
+		dst := strings.Join(path[len(path)-2:], "/")
+		if oldSrc, ok := data.Testcases[dst]; ok {
+			ctx.ModuleErrorf("Conflicting sources for %s: %s and %s", dst, oldSrc, src)
+		}
+		data.Testcases[dst] = src
 	}
-	testcasesContent[dst] = src
 }
 
-var artTestMutex sync.Mutex
+func setTestInstallInfo(data *testInstallInfo) func(ctx android.ModuleContext) {
+	return func(ctx android.ModuleContext) {
+		if len(data.Testcases) > 0 || len(data.TestMap) > 0 {
+			android.SetProvider(ctx, testInstallInfoProvider, *data)
+		}
+	}
+}
 
 func init() {
 	artModuleTypes := []string{
@@ -396,7 +387,11 @@ func artLibrary() android.Module {
 	installCodegenCustomizer(module, staticAndSharedLibrary)
 
 	android.AddLoadHook(module, addImplicitFlags)
-	android.AddInstallHook(module, addTestcasesFile)
+	data := &testInstallInfo{
+		Testcases: make(map[string]string),
+	}
+	android.AddInstallHook(module, addTestcasesFile(data))
+	android.AddPostGenerateAndroidBuildActionsHook(module, setTestInstallInfo(data))
 	return module
 }
 
@@ -415,7 +410,11 @@ func artBinary() android.Module {
 	android.AddLoadHook(module, addImplicitFlags)
 	android.AddLoadHook(module, customLinker)
 	android.AddLoadHook(module, prefer32Bit)
-	android.AddInstallHook(module, addTestcasesFile)
+	data := &testInstallInfo{
+		Testcases: make(map[string]string),
+	}
+	android.AddInstallHook(module, addTestcasesFile(data))
+	android.AddPostGenerateAndroidBuildActionsHook(module, setTestInstallInfo(data))
 	return module
 }
 
@@ -427,7 +426,11 @@ func artTest() android.Module {
 	android.AddLoadHook(module, addImplicitFlags)
 	android.AddLoadHook(module, customLinker)
 	android.AddLoadHook(module, prefer32Bit)
-	android.AddInstallHook(module, testInstall)
+	data := &testInstallInfo{
+		TestMap: make(map[string][]string),
+	}
+	android.AddInstallHook(module, testInstall(data))
+	android.AddPostGenerateAndroidBuildActionsHook(module, setTestInstallInfo(data))
 	return module
 }
 
@@ -438,6 +441,10 @@ func artTestLibrary() android.Module {
 
 	android.AddLoadHook(module, addImplicitFlags)
 	android.AddLoadHook(module, prefer32Bit)
-	android.AddInstallHook(module, testInstall)
+	data := &testInstallInfo{
+		TestMap: make(map[string][]string),
+	}
+	android.AddInstallHook(module, testInstall(data))
+	android.AddPostGenerateAndroidBuildActionsHook(module, setTestInstallInfo(data))
 	return module
 }
diff --git a/build/art_gob_enc.go b/build/art_gob_enc.go
new file mode 100644
index 0000000000..b5a2522907
--- /dev/null
+++ b/build/art_gob_enc.go
@@ -0,0 +1,112 @@
+// Code generated by go run gob_gen.go; DO NOT EDIT.
+
+package art
+
+import (
+	"bytes"
+	"github.com/google/blueprint/gobtools"
+)
+
+func init() {
+	testInstallInfoGobRegId = gobtools.RegisterType(func() gobtools.CustomDec { return new(testInstallInfo) })
+}
+
+func (r testInstallInfo) Encode(buf *bytes.Buffer) error {
+	var err error
+
+	if err = gobtools.EncodeSimple(buf, int32(len(r.Testcases))); err != nil {
+		return err
+	}
+	for k, v := range r.Testcases {
+		if err = gobtools.EncodeString(buf, k); err != nil {
+			return err
+		}
+		if err = gobtools.EncodeString(buf, v); err != nil {
+			return err
+		}
+	}
+
+	if err = gobtools.EncodeSimple(buf, int32(len(r.TestMap))); err != nil {
+		return err
+	}
+	for k, v := range r.TestMap {
+		if err = gobtools.EncodeString(buf, k); err != nil {
+			return err
+		}
+		if err = gobtools.EncodeSimple(buf, int32(len(v))); err != nil {
+			return err
+		}
+		for val1 := 0; val1 < len(v); val1++ {
+			if err = gobtools.EncodeString(buf, v[val1]); err != nil {
+				return err
+			}
+		}
+	}
+	return err
+}
+
+func (r *testInstallInfo) Decode(buf *bytes.Reader) error {
+	var err error
+
+	var val1 int32
+	err = gobtools.DecodeSimple[int32](buf, &val1)
+	if err != nil {
+		return err
+	}
+	if val1 > 0 {
+		r.Testcases = make(map[string]string, val1)
+		for val2 := 0; val2 < int(val1); val2++ {
+			var k string
+			var v string
+			err = gobtools.DecodeString(buf, &k)
+			if err != nil {
+				return err
+			}
+			err = gobtools.DecodeString(buf, &v)
+			if err != nil {
+				return err
+			}
+			r.Testcases[k] = v
+		}
+	}
+
+	var val5 int32
+	err = gobtools.DecodeSimple[int32](buf, &val5)
+	if err != nil {
+		return err
+	}
+	if val5 > 0 {
+		r.TestMap = make(map[string][]string, val5)
+		for val6 := 0; val6 < int(val5); val6++ {
+			var k string
+			var v []string
+			err = gobtools.DecodeString(buf, &k)
+			if err != nil {
+				return err
+			}
+			var val9 int32
+			err = gobtools.DecodeSimple[int32](buf, &val9)
+			if err != nil {
+				return err
+			}
+			if val9 > 0 {
+				v = make([]string, val9)
+				for val10 := 0; val10 < int(val9); val10++ {
+					err = gobtools.DecodeString(buf, &v[val10])
+					if err != nil {
+						return err
+					}
+				}
+			}
+			r.TestMap[k] = v
+		}
+	}
+
+	return err
+}
+
+var testInstallInfoGobRegId int16
+
+func (r testInstallInfo) GetTypeId() int16 {
+	return testInstallInfoGobRegId
+}
diff --git a/build/boot/boot-image-profile.txt b/build/boot/boot-image-profile.txt
index 412534c750..24a675d674 100644
--- a/build/boot/boot-image-profile.txt
+++ b/build/boot/boot-image-profile.txt
@@ -808,7 +808,6 @@ HSPLcom/android/org/bouncycastle/asn1/ASN1InputStream;->readObject()Lcom/android
 HSPLcom/android/org/bouncycastle/asn1/ASN1InputStream;->readVector(Lcom/android/org/bouncycastle/asn1/DefiniteLengthInputStream;)Lcom/android/org/bouncycastle/asn1/ASN1EncodableVector;
 HSPLcom/android/org/bouncycastle/asn1/ASN1Integer;-><init>(Ljava/math/BigInteger;)V
 HSPLcom/android/org/bouncycastle/asn1/ASN1Integer;->encode(Lcom/android/org/bouncycastle/asn1/ASN1OutputStream;Z)V
-HSPLcom/android/org/bouncycastle/asn1/ASN1Integer;->encodedLength()I
 HSPLcom/android/org/bouncycastle/asn1/ASN1Integer;->getInstance(Ljava/lang/Object;)Lcom/android/org/bouncycastle/asn1/ASN1Integer;
 HSPLcom/android/org/bouncycastle/asn1/ASN1Integer;->getValue()Ljava/math/BigInteger;
 HSPLcom/android/org/bouncycastle/asn1/ASN1Integer;->isMalformed([B)Z
@@ -817,27 +816,19 @@ HSPLcom/android/org/bouncycastle/asn1/ASN1Object;->getEncoded(Ljava/lang/String;
 HSPLcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier$OidHandle;-><init>([B)V
 HSPLcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier$OidHandle;->equals(Ljava/lang/Object;)Z
 HSPLcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier$OidHandle;->hashCode()I
-HSPLcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier;-><init>([B)V
 HSPLcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier;->asn1Equals(Lcom/android/org/bouncycastle/asn1/ASN1Primitive;)Z
 HSPLcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier;->doOutput(Ljava/io/ByteArrayOutputStream;)V
 HSPLcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier;->encode(Lcom/android/org/bouncycastle/asn1/ASN1OutputStream;Z)V
-HSPLcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier;->encodedLength()I
-HSPLcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier;->getBody()[B
 HSPLcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier;->hashCode()I
 HSPLcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier;->toString()Ljava/lang/String;
-HSPLcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier;->writeField(Ljava/io/ByteArrayOutputStream;J)V
 HSPLcom/android/org/bouncycastle/asn1/ASN1OutputStream;-><init>(Ljava/io/OutputStream;)V
 HSPLcom/android/org/bouncycastle/asn1/ASN1OutputStream;->write(I)V
 HSPLcom/android/org/bouncycastle/asn1/ASN1OutputStream;->write([BII)V
-HSPLcom/android/org/bouncycastle/asn1/ASN1OutputStream;->writeEncoded(ZI[B)V
-HSPLcom/android/org/bouncycastle/asn1/ASN1OutputStream;->writeLength(I)V
 HSPLcom/android/org/bouncycastle/asn1/ASN1OutputStream;->writeObject(Lcom/android/org/bouncycastle/asn1/ASN1Encodable;)V
 HSPLcom/android/org/bouncycastle/asn1/ASN1Primitive;->equals(Lcom/android/org/bouncycastle/asn1/ASN1Primitive;)Z
 HSPLcom/android/org/bouncycastle/asn1/ASN1Primitive;->fromByteArray([B)Lcom/android/org/bouncycastle/asn1/ASN1Primitive;
 HSPLcom/android/org/bouncycastle/asn1/ASN1Primitive;->toASN1Primitive()Lcom/android/org/bouncycastle/asn1/ASN1Primitive;
 HSPLcom/android/org/bouncycastle/asn1/ASN1Primitive;->toDERObject()Lcom/android/org/bouncycastle/asn1/ASN1Primitive;
-HSPLcom/android/org/bouncycastle/asn1/ASN1Sequence$1;-><init>(Lcom/android/org/bouncycastle/asn1/ASN1Sequence;)V
-HSPLcom/android/org/bouncycastle/asn1/ASN1Sequence$1;->nextElement()Ljava/lang/Object;
 HSPLcom/android/org/bouncycastle/asn1/ASN1Sequence;-><init>()V
 HSPLcom/android/org/bouncycastle/asn1/ASN1Sequence;-><init>(Lcom/android/org/bouncycastle/asn1/ASN1EncodableVector;)V
 HSPLcom/android/org/bouncycastle/asn1/ASN1Sequence;->getInstance(Ljava/lang/Object;)Lcom/android/org/bouncycastle/asn1/ASN1Sequence;
@@ -848,23 +839,19 @@ HSPLcom/android/org/bouncycastle/asn1/ASN1Sequence;->toDERObject()Lcom/android/o
 HSPLcom/android/org/bouncycastle/asn1/ASN1Set;-><init>()V
 HSPLcom/android/org/bouncycastle/asn1/ASN1Set;-><init>(Lcom/android/org/bouncycastle/asn1/ASN1EncodableVector;Z)V
 HSPLcom/android/org/bouncycastle/asn1/ASN1Set;->getInstance(Ljava/lang/Object;)Lcom/android/org/bouncycastle/asn1/ASN1Set;
-HSPLcom/android/org/bouncycastle/asn1/DERBitString;->getInstance(Ljava/lang/Object;)Lcom/android/org/bouncycastle/asn1/DERBitString;
-HSPLcom/android/org/bouncycastle/asn1/DERNull;->encodedLength()I
 HSPLcom/android/org/bouncycastle/asn1/DEROutputStream;->getDERSubStream()Lcom/android/org/bouncycastle/asn1/DEROutputStream;
 HSPLcom/android/org/bouncycastle/asn1/DERSequence;->encode(Lcom/android/org/bouncycastle/asn1/ASN1OutputStream;Z)V
-HSPLcom/android/org/bouncycastle/asn1/DERSequence;->encodedLength()I
-HSPLcom/android/org/bouncycastle/asn1/DERSequence;->getBodyLength()I
 HSPLcom/android/org/bouncycastle/asn1/DERSequence;->toDERObject()Lcom/android/org/bouncycastle/asn1/ASN1Primitive;
 HSPLcom/android/org/bouncycastle/asn1/DLFactory;-><clinit>()V
 HSPLcom/android/org/bouncycastle/asn1/DefiniteLengthInputStream;->read()I
 HSPLcom/android/org/bouncycastle/asn1/DefiniteLengthInputStream;->read([BII)I
 HSPLcom/android/org/bouncycastle/asn1/DefiniteLengthInputStream;->toByteArray()[B
-HSPLcom/android/org/bouncycastle/asn1/StreamUtil;->calculateBodyLength(I)I
 HSPLcom/android/org/bouncycastle/asn1/StreamUtil;->findLimit(Ljava/io/InputStream;)I
 HSPLcom/android/org/bouncycastle/asn1/x509/AlgorithmIdentifier;->getInstance(Ljava/lang/Object;)Lcom/android/org/bouncycastle/asn1/x509/AlgorithmIdentifier;
 HSPLcom/android/org/bouncycastle/asn1/x509/AlgorithmIdentifier;->toASN1Primitive()Lcom/android/org/bouncycastle/asn1/ASN1Primitive;
 HSPLcom/android/org/bouncycastle/asn1/x509/SubjectPublicKeyInfo;->getInstance(Ljava/lang/Object;)Lcom/android/org/bouncycastle/asn1/x509/SubjectPublicKeyInfo;
-HSPLcom/android/org/bouncycastle/crypto/BufferedBlockCipher;->reset()V
+HSPLcom/android/org/bouncycastle/crypto/CryptoServicesRegistrar$$ExternalSyntheticBackportWithForwarding0;->m(Ljava/util/concurrent/atomic/AtomicReference;Ljava/lang/Object;Ljava/lang/Object;)Z
+HSPLcom/android/org/bouncycastle/crypto/CryptoServicesRegistrar$ThreadLocalSecureRandomProvider;->get()Ljava/security/SecureRandom;
 HSPLcom/android/org/bouncycastle/crypto/CryptoServicesRegistrar;->getSecureRandom()Ljava/security/SecureRandom;
 HSPLcom/android/org/bouncycastle/crypto/PBEParametersGenerator;->PKCS12PasswordToBytes([C)[B
 HSPLcom/android/org/bouncycastle/crypto/digests/OpenSSLDigest$SHA1;-><init>()V
@@ -878,11 +865,9 @@ HSPLcom/android/org/bouncycastle/crypto/engines/AESEngine;-><init>()V
 HSPLcom/android/org/bouncycastle/crypto/engines/AESEngine;->getAlgorithmName()Ljava/lang/String;
 HSPLcom/android/org/bouncycastle/crypto/engines/AESEngine;->getBlockSize()I
 HSPLcom/android/org/bouncycastle/crypto/engines/AESEngine;->init(ZLcom/android/org/bouncycastle/crypto/CipherParameters;)V
-HSPLcom/android/org/bouncycastle/crypto/engines/AESEngine;->packBlock([BI)V
 HSPLcom/android/org/bouncycastle/crypto/engines/AESEngine;->reset()V
 HSPLcom/android/org/bouncycastle/crypto/engines/AESEngine;->shift(II)I
 HSPLcom/android/org/bouncycastle/crypto/engines/AESEngine;->subWord(I)I
-HSPLcom/android/org/bouncycastle/crypto/engines/AESEngine;->unpackBlock([BI)V
 HSPLcom/android/org/bouncycastle/crypto/engines/DESEngine;-><clinit>()V
 HSPLcom/android/org/bouncycastle/crypto/engines/DESEngine;-><init>()V
 HSPLcom/android/org/bouncycastle/crypto/generators/PKCS12ParametersGenerator;->generateDerivedKey(II)[B
@@ -1092,10 +1077,12 @@ HSPLdalvik/system/SocketTagger;->get()Ldalvik/system/SocketTagger;
 HSPLdalvik/system/SocketTagger;->set(Ldalvik/system/SocketTagger;)V
 HSPLdalvik/system/SocketTagger;->tag(Ljava/net/Socket;)V
 HSPLdalvik/system/SocketTagger;->untag(Ljava/net/Socket;)V
+HSPLdalvik/system/VMRuntime$SdkVersionContainer;->-$$Nest$sfgetsdkExtensionS()I
 HSPLdalvik/system/VMRuntime$SdkVersionContainer;->-$$Nest$sfgetsdkVersion()I
 HSPLdalvik/system/VMRuntime;->addPostCleanupCallback(Ljava/lang/Runnable;)V
 HSPLdalvik/system/VMRuntime;->getInstructionSet(Ljava/lang/String;)Ljava/lang/String;
 HSPLdalvik/system/VMRuntime;->getRuntime()Ldalvik/system/VMRuntime;
+HSPLdalvik/system/VMRuntime;->getSdkExtensionSLevel()I
 HSPLdalvik/system/VMRuntime;->getSdkVersion()I
 HSPLdalvik/system/VMRuntime;->getTargetSdkVersion()I
 HSPLdalvik/system/VMRuntime;->hiddenApiUsed(ILjava/lang/String;Ljava/lang/String;IZ)V
@@ -1913,6 +1900,7 @@ HSPLjava/lang/Character;->isHighSurrogate(C)Z
 HSPLjava/lang/Character;->isISOControl(I)Z
 HSPLjava/lang/Character;->isJavaIdentifierPart(C)Z
 HSPLjava/lang/Character;->isJavaIdentifierPart(I)Z
+SPLjava/lang/Character;->isJavaIdentifierStart(C)Z
 HSPLjava/lang/Character;->isJavaIdentifierStart(I)Z
 HSPLjava/lang/Character;->isLetter(C)Z
 HSPLjava/lang/Character;->isLetter(I)Z
@@ -1993,6 +1981,7 @@ HSPLjava/lang/Class;->isAnnotationPresent(Ljava/lang/Class;)Z
 HSPLjava/lang/Class;->isArray()Z
 HSPLjava/lang/Class;->isAssignableFrom(Ljava/lang/Class;)Z
 HSPLjava/lang/Class;->isEnum()Z
+SPLjava/lang/Class;->isHidden()Z
 HSPLjava/lang/Class;->isInstance(Ljava/lang/Object;)Z
 HSPLjava/lang/Class;->isInterface()Z
 HSPLjava/lang/Class;->isLocalClass()Z
@@ -2021,6 +2010,34 @@ HSPLjava/lang/ClassLoader;->loadClass(Ljava/lang/String;)Ljava/lang/Class;
 HSPLjava/lang/ClassLoader;->loadClass(Ljava/lang/String;Z)Ljava/lang/Class;
 HSPLjava/lang/ClassNotFoundException;-><init>(Ljava/lang/String;)V
 HSPLjava/lang/ClassNotFoundException;-><init>(Ljava/lang/String;Ljava/lang/Throwable;)V
+HSPLjava/lang/ClassValue$ClassValueMap;-><init>()V
+HSPLjava/lang/ClassValue$ClassValueMap;->addToCache(Ljava/lang/ClassValue;Ljava/lang/ClassValue$Entry;)V
+HSPLjava/lang/ClassValue$ClassValueMap;->checkCacheLoad()V
+HSPLjava/lang/ClassValue$ClassValueMap;->finishEntry(Ljava/lang/ClassValue;Ljava/lang/ClassValue$Entry;)Ljava/lang/ClassValue$Entry;
+HSPLjava/lang/ClassValue$ClassValueMap;->getCache()[Ljava/lang/ClassValue$Entry;
+HSPLjava/lang/ClassValue$ClassValueMap;->loadFromCache([Ljava/lang/ClassValue$Entry;I)Ljava/lang/ClassValue$Entry;
+HSPLjava/lang/ClassValue$ClassValueMap;->overwrittenEntry(Ljava/lang/ClassValue$Entry;)Ljava/lang/ClassValue$Entry;
+HSPLjava/lang/ClassValue$ClassValueMap;->placeInCache([Ljava/lang/ClassValue$Entry;ILjava/lang/ClassValue$Entry;Z)Ljava/lang/ClassValue$Entry;
+HSPLjava/lang/ClassValue$ClassValueMap;->probeBackupLocations([Ljava/lang/ClassValue$Entry;Ljava/lang/ClassValue;)Ljava/lang/ClassValue$Entry;
+HSPLjava/lang/ClassValue$ClassValueMap;->probeHomeLocation([Ljava/lang/ClassValue$Entry;Ljava/lang/ClassValue;)Ljava/lang/ClassValue$Entry;+]Ljava/lang/ClassValue;Ljava/lang/Enum$1;
+HSPLjava/lang/ClassValue$ClassValueMap;->sizeCache(I)V
+HSPLjava/lang/ClassValue$ClassValueMap;->startEntry(Ljava/lang/ClassValue;)Ljava/lang/ClassValue$Entry;
+HSPLjava/lang/ClassValue$Entry;-><init>(Ljava/lang/ClassValue$Version;Ljava/lang/Object;)V
+HSPLjava/lang/ClassValue$Entry;->assertNotPromise()V
+HSPLjava/lang/ClassValue$Entry;->isPromise()Z
+HSPLjava/lang/ClassValue$Entry;->value()Ljava/lang/Object;
+HSPLjava/lang/ClassValue$Entry;->version()Ljava/lang/ClassValue$Version;
+HSPLjava/lang/ClassValue$Version;->promise()Ljava/lang/ClassValue$Entry;
+HSPLjava/lang/ClassValue;->castEntry(Ljava/lang/ClassValue$Entry;)Ljava/lang/ClassValue$Entry;
+HSPLjava/lang/ClassValue;->get(Ljava/lang/Class;)Ljava/lang/Object;+]Ljava/lang/ClassValue$Entry;Ljava/lang/ClassValue$Entry;]Ljava/lang/ClassValue;missing_types
+HSPLjava/lang/ClassValue;->getCacheCarefully(Ljava/lang/Class;)[Ljava/lang/ClassValue$Entry;+]Ljava/lang/ClassValue$ClassValueMap;Ljava/lang/ClassValue$ClassValueMap;
+HSPLjava/lang/ClassValue;->getFromBackup([Ljava/lang/ClassValue$Entry;Ljava/lang/Class;)Ljava/lang/Object;
+HSPLjava/lang/ClassValue;->getFromHashMap(Ljava/lang/Class;)Ljava/lang/Object;
+HSPLjava/lang/ClassValue;->getMap(Ljava/lang/Class;)Ljava/lang/ClassValue$ClassValueMap;
+HSPLjava/lang/ClassValue;->initializeMap(Ljava/lang/Class;)Ljava/lang/ClassValue$ClassValueMap;
+HSPLjava/lang/ClassValue;->makeEntry(Ljava/lang/ClassValue$Version;Ljava/lang/Object;)Ljava/lang/ClassValue$Entry;
+HSPLjava/lang/ClassValue;->match(Ljava/lang/ClassValue$Entry;)Z+]Ljava/lang/ClassValue$Entry;Ljava/lang/ClassValue$Entry;
+HSPLjava/lang/ClassValue;->version()Ljava/lang/ClassValue$Version;
 HSPLjava/lang/Daemons$Daemon;->interrupt(Ljava/lang/Thread;)V
 HSPLjava/lang/Daemons$Daemon;->isRunning()Z
 HSPLjava/lang/Daemons$Daemon;->run()V
@@ -2073,8 +2090,8 @@ HSPLjava/lang/Double;->toString()Ljava/lang/String;
 HSPLjava/lang/Double;->toString(D)Ljava/lang/String;
 HSPLjava/lang/Double;->valueOf(D)Ljava/lang/Double;
 HSPLjava/lang/Double;->valueOf(Ljava/lang/String;)Ljava/lang/Double;
-HSPLjava/lang/Enum$1;->create(Ljava/lang/Class;)[Ljava/lang/Object;
-HSPLjava/lang/Enum$1;->create(Ljava/lang/Object;)Ljava/lang/Object;
+HSPLjava/lang/Enum$1;->computeValue(Ljava/lang/Class;)Ljava/lang/Object;
+HSPLjava/lang/Enum$1;->computeValue(Ljava/lang/Class;)[Ljava/lang/Object;
 HSPLjava/lang/Enum;->-$$Nest$smenumValues(Ljava/lang/Class;)[Ljava/lang/Object;
 HSPLjava/lang/Enum;-><init>(Ljava/lang/String;I)V
 HSPLjava/lang/Enum;->compareTo(Ljava/lang/Enum;)I
@@ -2179,6 +2196,9 @@ HSPLjava/lang/Integer;->valueOf(Ljava/lang/String;)Ljava/lang/Integer;
 HSPLjava/lang/Integer;->valueOf(Ljava/lang/String;I)Ljava/lang/Integer;
 HSPLjava/lang/InterruptedException;-><init>()V
 HSPLjava/lang/Iterable;->forEach(Ljava/util/function/Consumer;)V+]Ljava/lang/Iterable;megamorphic_types]Ljava/util/Iterator;megamorphic_types]Ljava/util/function/Consumer;missing_types
+HSPLjava/lang/JavaLangAccess;->getCarrierThreadLocal(Ljdk/internal/misc/CarrierThreadLocal;)Ljava/lang/Object;+]Ljava/lang/ThreadLocal;Ljdk/internal/misc/TerminatingThreadLocal$1;,Lsun/nio/fs/NativeBuffers$1;
+SPLjava/lang/JavaLangAccess;->setCarrierThreadLocal(Ljdk/internal/misc/CarrierThreadLocal;Ljava/lang/Object;)V
+HSPLjava/lang/JavaLangAccess;->start(Ljava/lang/Thread;Ljdk/internal/vm/ThreadContainer;)V+]Ljava/lang/Thread;missing_types
 HSPLjava/lang/LinkageError;-><init>(Ljava/lang/String;)V
 HSPLjava/lang/Long;-><init>(J)V
 HSPLjava/lang/Long;->bitCount(J)I
@@ -2550,11 +2570,14 @@ HSPLjava/lang/Thread;-><init>(Ljava/lang/ThreadGroup;Ljava/lang/String;)V
 HSPLjava/lang/Thread;-><init>(Ljava/lang/ThreadGroup;Ljava/lang/String;IZ)V
 HSPLjava/lang/Thread;->activeCount()I
 HSPLjava/lang/Thread;->blockedOn(Lsun/nio/ch/Interruptible;)V
+SPLjava/lang/Thread;->cachingPriorityForNiceness(I)I
 HSPLjava/lang/Thread;->checkAccess()V
+HSPLjava/lang/Thread;->currentCarrierThread()Ljava/lang/Thread;
 HSPLjava/lang/Thread;->getContextClassLoader()Ljava/lang/ClassLoader;
 HSPLjava/lang/Thread;->getDefaultUncaughtExceptionHandler()Ljava/lang/Thread$UncaughtExceptionHandler;
 HSPLjava/lang/Thread;->getId()J
 HSPLjava/lang/Thread;->getName()Ljava/lang/String;
+SPLjava/lang/Thread;->getPosixNicenessInternal()I
 HSPLjava/lang/Thread;->getPriority()I
 HSPLjava/lang/Thread;->getStackTrace()[Ljava/lang/StackTraceElement;
 HSPLjava/lang/Thread;->getState()Ljava/lang/Thread$State;
@@ -2575,13 +2598,16 @@ HSPLjava/lang/Thread;->setContextClassLoader(Ljava/lang/ClassLoader;)V
 HSPLjava/lang/Thread;->setDaemon(Z)V
 HSPLjava/lang/Thread;->setDefaultUncaughtExceptionHandler(Ljava/lang/Thread$UncaughtExceptionHandler;)V
 HSPLjava/lang/Thread;->setName(Ljava/lang/String;)V
+SPLjava/lang/Thread;->setPosixNicenessInternal(I)I
 HSPLjava/lang/Thread;->setPriority(I)V
 HSPLjava/lang/Thread;->setSystemDaemon(Z)V
+SPLjava/lang/Thread;->setThreadContainer(Ljdk/internal/vm/ThreadContainer;)V
 HSPLjava/lang/Thread;->setUncaughtExceptionHandler(Ljava/lang/Thread$UncaughtExceptionHandler;)V
 HSPLjava/lang/Thread;->setUncaughtExceptionPreHandler(Ljava/lang/Thread$UncaughtExceptionHandler;)V
 HSPLjava/lang/Thread;->sleep(J)V
 HSPLjava/lang/Thread;->sleep(JI)V
 HSPLjava/lang/Thread;->start()V
+HSPLjava/lang/Thread;->start(Ljdk/internal/vm/ThreadContainer;)V+]Ljava/lang/Thread;missing_types]Ljava/lang/ThreadGroup;Ljava/lang/ThreadGroup;]Ljdk/internal/vm/ThreadContainer;Ljdk/internal/vm/SharedThreadContainer;
 HSPLjava/lang/Thread;->toString()Ljava/lang/String;
 HSPLjava/lang/ThreadGroup;-><init>(Ljava/lang/ThreadGroup;Ljava/lang/String;)V
 HSPLjava/lang/ThreadGroup;-><init>(Ljava/lang/Void;Ljava/lang/ThreadGroup;Ljava/lang/String;)V
@@ -2626,12 +2652,16 @@ HSPLjava/lang/ThreadLocal;-><init>()V
 HSPLjava/lang/ThreadLocal;->createInheritedMap(Ljava/lang/ThreadLocal$ThreadLocalMap;)Ljava/lang/ThreadLocal$ThreadLocalMap;
 HSPLjava/lang/ThreadLocal;->createMap(Ljava/lang/Thread;Ljava/lang/Object;)V
 HSPLjava/lang/ThreadLocal;->get()Ljava/lang/Object;
+HSPLjava/lang/ThreadLocal;->get(Ljava/lang/Thread;)Ljava/lang/Object;+]Ljava/lang/ThreadLocal;megamorphic_types
+HSPLjava/lang/ThreadLocal;->getCarrierThreadLocal()Ljava/lang/Object;
 HSPLjava/lang/ThreadLocal;->getMap(Ljava/lang/Thread;)Ljava/lang/ThreadLocal$ThreadLocalMap;
 HSPLjava/lang/ThreadLocal;->initialValue()Ljava/lang/Object;
 HSPLjava/lang/ThreadLocal;->nextHashCode()I
 HSPLjava/lang/ThreadLocal;->remove()V
 HSPLjava/lang/ThreadLocal;->set(Ljava/lang/Object;)V
-HSPLjava/lang/ThreadLocal;->setInitialValue()Ljava/lang/Object;
+HSPLjava/lang/ThreadLocal;->set(Ljava/lang/Thread;Ljava/lang/Object;)V+]Ljava/lang/ThreadLocal;megamorphic_types
+HSPLjava/lang/ThreadLocal;->setCarrierThreadLocal(Ljava/lang/Object;)V
+HSPLjava/lang/ThreadLocal;->setInitialValue(Ljava/lang/Thread;)Ljava/lang/Object;+]Ljava/lang/ThreadLocal;megamorphic_types
 HSPLjava/lang/ThreadLocal;->withInitial(Ljava/util/function/Supplier;)Ljava/lang/ThreadLocal;
 HSPLjava/lang/Throwable$PrintStreamOrWriter;-><init>()V
 HSPLjava/lang/Throwable$WrappedPrintStream;-><init>(Ljava/io/PrintStream;)V
@@ -3891,6 +3921,7 @@ HSPLjava/nio/file/Files$AcceptAllFilter;->accept(Ljava/lang/Object;)Z
 HSPLjava/nio/file/Files$AcceptAllFilter;->accept(Ljava/nio/file/Path;)Z
 HSPLjava/nio/file/Files;->exists(Ljava/nio/file/Path;[Ljava/nio/file/LinkOption;)Z
 HSPLjava/nio/file/Files;->followLinks([Ljava/nio/file/LinkOption;)Z
+SPLjava/nio/file/Files;->getLastModifiedTime(Ljava/nio/file/Path;[Ljava/nio/file/LinkOption;)Ljava/nio/file/attribute/FileTime;
 HSPLjava/nio/file/Files;->isAccessible(Ljava/nio/file/Path;[Ljava/nio/file/AccessMode;)Z
 HSPLjava/nio/file/Files;->isRegularFile(Ljava/nio/file/Path;[Ljava/nio/file/LinkOption;)Z
 HSPLjava/nio/file/Files;->isWritable(Ljava/nio/file/Path;)Z
@@ -3904,6 +3935,7 @@ HSPLjava/nio/file/Files;->read(Ljava/io/InputStream;I)[B
 HSPLjava/nio/file/Files;->readAllBytes(Ljava/nio/file/Path;)[B
 HSPLjava/nio/file/Files;->readAttributes(Ljava/nio/file/Path;Ljava/lang/Class;[Ljava/nio/file/LinkOption;)Ljava/nio/file/attribute/BasicFileAttributes;
 HSPLjava/nio/file/NoSuchFileException;-><init>(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;)V
+HSPLjava/nio/file/Path;->resolve(Ljava/lang/String;)Ljava/nio/file/Path;+]Ljava/nio/file/FileSystem;Lsun/nio/fs/LinuxFileSystem;]Ljava/nio/file/Path;Lsun/nio/fs/UnixPath;
 HSPLjava/nio/file/Paths;->get(Ljava/lang/String;[Ljava/lang/String;)Ljava/nio/file/Path;
 HSPLjava/nio/file/StandardOpenOption;->values()[Ljava/nio/file/StandardOpenOption;
 HSPLjava/nio/file/attribute/FileTime;-><init>(JLjava/util/concurrent/TimeUnit;Ljava/time/Instant;)V
@@ -5841,13 +5873,11 @@ HSPLjava/util/ImmutableCollections$SetN;-><init>([Ljava/lang/Object;)V
 HSPLjava/util/ImmutableCollections$SetN;->contains(Ljava/lang/Object;)Z
 HSPLjava/util/ImmutableCollections$SetN;->iterator()Ljava/util/Iterator;
 HSPLjava/util/ImmutableCollections$SetN;->probe(Ljava/lang/Object;)I
-HSPLjava/util/ImmutableCollections;->-$$Nest$sfgetREVERSE()Z
-HSPLjava/util/ImmutableCollections;->-$$Nest$sfgetSALT32L()J
 HSPLjava/util/ImmutableCollections;-><clinit>()V
 HSPLjava/util/ImmutableCollections;->listCopy(Ljava/util/Collection;)Ljava/util/List;
 HSPLjava/util/ImmutableCollections;->listFromTrustedArray([Ljava/lang/Object;)Ljava/util/List;
 HSPLjava/util/ImmutableCollections;->listFromTrustedArrayNullsAllowed([Ljava/lang/Object;)Ljava/util/List;
-HSPLjava/util/Iterator;->forEachRemaining(Ljava/util/function/Consumer;)V+]Ljava/util/Iterator;Landroid/util/MapCollections$ArrayIterator;,Landroid/util/MapCollections$MapIterator;,Ljava/util/AbstractList$Itr;,Ljava/util/AbstractMap$2$1;]Ljava/util/function/Consumer;megamorphic_types
+HSPLjava/util/Iterator;->forEachRemaining(Ljava/util/function/Consumer;)V+]Ljava/util/Iterator;Landroid/util/MapCollections$ArrayIterator;,Landroid/util/MapCollections$MapIterator;,Ljava/util/AbstractList$Itr;,Ljava/util/AbstractMap$2$1;,Ljava/util/LinkedHashMap$LinkedEntryIterator;,Ljava/util/LinkedHashMap$LinkedKeyIterator;,Ljava/util/LinkedHashMap$LinkedValueIterator;]Ljava/util/function/Consumer;megamorphic_types
 HSPLjava/util/JumboEnumSet$EnumSetIterator;-><init>(Ljava/util/JumboEnumSet;)V
 HSPLjava/util/JumboEnumSet$EnumSetIterator;->hasNext()Z
 HSPLjava/util/JumboEnumSet$EnumSetIterator;->next()Ljava/lang/Enum;
@@ -6043,7 +6073,7 @@ HSPLjava/util/Map;->forEach(Ljava/util/function/BiConsumer;)V
 HSPLjava/util/Map;->getOrDefault(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+]Ljava/util/Map;Landroid/util/ArrayMap;
 HSPLjava/util/Map;->of(Ljava/lang/Object;Ljava/lang/Object;)Ljava/util/Map;
 HSPLjava/util/Map;->ofEntries([Ljava/util/Map$Entry;)Ljava/util/Map;
-HSPLjava/util/Map;->putIfAbsent(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+]Ljava/util/Map;Landroid/util/ArrayMap;
+HSPLjava/util/Map;->putIfAbsent(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;
 HSPLjava/util/MissingResourceException;-><init>(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;)V
 HSPLjava/util/NoSuchElementException;-><init>()V
 HSPLjava/util/NoSuchElementException;-><init>(Ljava/lang/String;)V
@@ -6057,6 +6087,7 @@ HSPLjava/util/Objects;->nonNull(Ljava/lang/Object;)Z
 HSPLjava/util/Objects;->requireNonNull(Ljava/lang/Object;)Ljava/lang/Object;
 HSPLjava/util/Objects;->requireNonNull(Ljava/lang/Object;Ljava/lang/String;)Ljava/lang/Object;
 HSPLjava/util/Objects;->requireNonNullElse(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;
+HSPLjava/util/Objects;->toIdentityString(Ljava/lang/Object;)Ljava/lang/String;
 HSPLjava/util/Objects;->toString(Ljava/lang/Object;)Ljava/lang/String;
 HSPLjava/util/Objects;->toString(Ljava/lang/Object;Ljava/lang/String;)Ljava/lang/String;
 HSPLjava/util/Observable;-><init>()V
@@ -6238,7 +6269,7 @@ HSPLjava/util/SimpleTimeZone;->getOffset(J)I
 HSPLjava/util/SimpleTimeZone;->getOffsets(J[I)I
 HSPLjava/util/SimpleTimeZone;->getRawOffset()I
 HSPLjava/util/SimpleTimeZone;->hasSameRules(Ljava/util/TimeZone;)Z
-HSPLjava/util/Spliterator$OfInt;->forEachRemaining(Ljava/util/function/Consumer;)V+]Ljava/util/Spliterator$OfInt;Ljava/lang/StringUTF16$CodePointsSpliteratorForString;,Ljava/util/Spliterators$IntArraySpliterator;,Ljava/util/stream/Streams$RangeIntSpliterator;
+HSPLjava/util/Spliterator$OfInt;->forEachRemaining(Ljava/util/function/Consumer;)V+]Ljava/util/Spliterator$OfInt;Ljava/lang/StringUTF16$CodePointsSpliteratorForString;,Ljava/util/Spliterators$EmptySpliterator$OfInt;,Ljava/util/Spliterators$IntArraySpliterator;,Ljava/util/stream/Streams$RangeIntSpliterator;
 HSPLjava/util/Spliterator;->getExactSizeIfKnown()J+]Ljava/util/Spliterator;megamorphic_types
 HSPLjava/util/Spliterators$ArraySpliterator;-><init>([Ljava/lang/Object;I)V
 HSPLjava/util/Spliterators$ArraySpliterator;-><init>([Ljava/lang/Object;III)V
@@ -6400,12 +6431,20 @@ HSPLjava/util/TreeMap$TreeMapEntry;-><init>(Ljava/lang/Object;Ljava/lang/Object;
 HSPLjava/util/TreeMap$TreeMapEntry;->getKey()Ljava/lang/Object;
 HSPLjava/util/TreeMap$TreeMapEntry;->getValue()Ljava/lang/Object;
 HSPLjava/util/TreeMap$TreeMapEntry;->setValue(Ljava/lang/Object;)Ljava/lang/Object;
+HSPLjava/util/TreeMap$TreeMapSpliterator;-><init>(Ljava/util/TreeMap;Ljava/util/TreeMap$TreeMapEntry;Ljava/util/TreeMap$TreeMapEntry;III)V
+HSPLjava/util/TreeMap$TreeMapSpliterator;->estimateSize()J
+HSPLjava/util/TreeMap$TreeMapSpliterator;->getEstimate()I
 HSPLjava/util/TreeMap$ValueIterator;-><init>(Ljava/util/TreeMap;Ljava/util/TreeMap$TreeMapEntry;)V
 HSPLjava/util/TreeMap$ValueIterator;->next()Ljava/lang/Object;
+HSPLjava/util/TreeMap$ValueSpliterator;-><init>(Ljava/util/TreeMap;Ljava/util/TreeMap$TreeMapEntry;Ljava/util/TreeMap$TreeMapEntry;III)V
+HSPLjava/util/TreeMap$ValueSpliterator;->characteristics()I
+HSPLjava/util/TreeMap$ValueSpliterator;->forEachRemaining(Ljava/util/function/Consumer;)V
 HSPLjava/util/TreeMap$Values;-><init>(Ljava/util/TreeMap;)V
 HSPLjava/util/TreeMap$Values;->iterator()Ljava/util/Iterator;
 HSPLjava/util/TreeMap$Values;->size()I
+HSPLjava/util/TreeMap$Values;->spliterator()Ljava/util/Spliterator;
 HSPLjava/util/TreeMap;->-$$Nest$fgetmodCount(Ljava/util/TreeMap;)I
+HSPLjava/util/TreeMap;->-$$Nest$fgetsize(Ljava/util/TreeMap;)I
 HSPLjava/util/TreeMap;-><init>()V
 HSPLjava/util/TreeMap;-><init>(Ljava/util/Comparator;)V
 HSPLjava/util/TreeMap;-><init>(Ljava/util/Map;)V
@@ -6649,7 +6688,9 @@ HSPLjava/util/concurrent/ConcurrentHashMap$ForwardingNode;->find(ILjava/lang/Obj
 HSPLjava/util/concurrent/ConcurrentHashMap$KeyIterator;-><init>([Ljava/util/concurrent/ConcurrentHashMap$Node;IIILjava/util/concurrent/ConcurrentHashMap;)V
 HSPLjava/util/concurrent/ConcurrentHashMap$KeyIterator;->next()Ljava/lang/Object;
 HSPLjava/util/concurrent/ConcurrentHashMap$KeySetView;-><init>(Ljava/util/concurrent/ConcurrentHashMap;Ljava/lang/Object;)V
+HSPLjava/util/concurrent/ConcurrentHashMap$KeySetView;->add(Ljava/lang/Object;)Z
 HSPLjava/util/concurrent/ConcurrentHashMap$KeySetView;->iterator()Ljava/util/Iterator;
+HSPLjava/util/concurrent/ConcurrentHashMap$KeySetView;->remove(Ljava/lang/Object;)Z+]Ljava/util/concurrent/ConcurrentHashMap;Ljava/util/concurrent/ConcurrentHashMap;
 HSPLjava/util/concurrent/ConcurrentHashMap$KeySetView;->spliterator()Ljava/util/Spliterator;
 HSPLjava/util/concurrent/ConcurrentHashMap$MapEntry;-><init>(Ljava/lang/Object;Ljava/lang/Object;Ljava/util/concurrent/ConcurrentHashMap;)V
 HSPLjava/util/concurrent/ConcurrentHashMap$MapEntry;->getKey()Ljava/lang/Object;
@@ -6924,7 +6965,7 @@ HSPLjava/util/concurrent/LinkedBlockingQueue;->size()I
 HSPLjava/util/concurrent/LinkedBlockingQueue;->take()Ljava/lang/Object;
 HSPLjava/util/concurrent/LinkedTransferQueue$DualNode;-><clinit>()V
 HSPLjava/util/concurrent/LinkedTransferQueue$DualNode;-><init>(Ljava/lang/Object;Z)V
-HSPLjava/util/concurrent/LinkedTransferQueue$DualNode;->await(Ljava/lang/Object;JLjava/lang/Object;Z)Ljava/lang/Object;+]Ljava/lang/Thread;missing_types
+HSPLjava/util/concurrent/LinkedTransferQueue$DualNode;->await(Ljava/lang/Object;JLjava/lang/Object;Z)Ljava/lang/Object;
 HSPLjava/util/concurrent/LinkedTransferQueue$DualNode;->checkForUniprocessor(Z)V
 HSPLjava/util/concurrent/LinkedTransferQueue$DualNode;->cmpExItem(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;
 HSPLjava/util/concurrent/LinkedTransferQueue;->cmpExHead(Ljava/util/concurrent/LinkedTransferQueue$DualNode;Ljava/util/concurrent/LinkedTransferQueue$DualNode;)Ljava/util/concurrent/LinkedTransferQueue$DualNode;
@@ -7147,6 +7188,7 @@ HSPLjava/util/concurrent/atomic/AtomicIntegerFieldUpdater$AtomicIntegerFieldUpda
 HSPLjava/util/concurrent/atomic/AtomicIntegerFieldUpdater$AtomicIntegerFieldUpdaterImpl;->compareAndSet(Ljava/lang/Object;II)Z
 HSPLjava/util/concurrent/atomic/AtomicIntegerFieldUpdater$AtomicIntegerFieldUpdaterImpl;->decrementAndGet(Ljava/lang/Object;)I
 HSPLjava/util/concurrent/atomic/AtomicIntegerFieldUpdater$AtomicIntegerFieldUpdaterImpl;->getAndAdd(Ljava/lang/Object;I)I
+HSPLjava/util/concurrent/atomic/AtomicIntegerFieldUpdater$AtomicIntegerFieldUpdaterImpl;->getAndDecrement(Ljava/lang/Object;)I
 HSPLjava/util/concurrent/atomic/AtomicIntegerFieldUpdater$AtomicIntegerFieldUpdaterImpl;->getAndIncrement(Ljava/lang/Object;)I
 HSPLjava/util/concurrent/atomic/AtomicIntegerFieldUpdater$AtomicIntegerFieldUpdaterImpl;->incrementAndGet(Ljava/lang/Object;)I
 HSPLjava/util/concurrent/atomic/AtomicIntegerFieldUpdater$AtomicIntegerFieldUpdaterImpl;->set(Ljava/lang/Object;I)V
@@ -7898,7 +7940,7 @@ HSPLjava/util/stream/ReferencePipeline;->sorted()Ljava/util/stream/Stream;
 HSPLjava/util/stream/ReferencePipeline;->sorted(Ljava/util/Comparator;)Ljava/util/stream/Stream;
 HSPLjava/util/stream/ReferencePipeline;->toArray()[Ljava/lang/Object;
 HSPLjava/util/stream/ReferencePipeline;->toArray(Ljava/util/function/IntFunction;)[Ljava/lang/Object;
-HSPLjava/util/stream/ReferencePipeline;->toList()Ljava/util/List;+]Ljdk/internal/access/JavaUtilCollectionAccess;Ljava/util/ImmutableCollections$Access$1;
+HSPLjava/util/stream/ReferencePipeline;->toList()Ljava/util/List;
 HSPLjava/util/stream/ReferencePipeline;->wrap(Ljava/util/stream/PipelineHelper;Ljava/util/function/Supplier;Z)Ljava/util/Spliterator;
 HSPLjava/util/stream/Sink$ChainedInt;-><init>(Ljava/util/stream/Sink;)V
 HSPLjava/util/stream/Sink$ChainedInt;->begin(J)V
@@ -8047,7 +8089,7 @@ HSPLjava/util/zip/ZipCoder$UTF8ZipCoder;->checkedHash([BII)I
 HSPLjava/util/zip/ZipCoder$UTF8ZipCoder;->hasTrailingSlash(Ljava/nio/DirectByteBuffer;I)Z
 HSPLjava/util/zip/ZipCoder$UTF8ZipCoder;->hasTrailingSlash([BI)Z
 HSPLjava/util/zip/ZipCoder$UTF8ZipCoder;->isUTF8()Z
-HSPLjava/util/zip/ZipCoder$UTF8ZipCoder;->toString(Ljava/nio/DirectByteBuffer;II)Ljava/lang/String;+]Ljava/nio/DirectByteBuffer;Ljava/nio/DirectByteBuffer;
+HSPLjava/util/zip/ZipCoder$UTF8ZipCoder;->toString(Ljava/nio/DirectByteBuffer;II)Ljava/lang/String;
 HSPLjava/util/zip/ZipCoder$UTF8ZipCoder;->toString([BII)Ljava/lang/String;
 HSPLjava/util/zip/ZipCoder;-><init>(Ljava/nio/charset/Charset;)V
 HSPLjava/util/zip/ZipCoder;->decoder()Ljava/nio/charset/CharsetDecoder;
@@ -8155,7 +8197,7 @@ HSPLjava/util/zip/ZipUtils;->LOCNAM([B)I
 HSPLjava/util/zip/ZipUtils;->LOCSIG([B)J
 HSPLjava/util/zip/ZipUtils;->SH([BI)I
 HSPLjava/util/zip/ZipUtils;->get16([BI)I
-HSPLjava/util/zip/ZipUtils;->get32(Ljava/nio/DirectByteBuffer;I)J+]Ljava/nio/DirectByteBuffer;Ljava/nio/DirectByteBuffer;
+HSPLjava/util/zip/ZipUtils;->get32(Ljava/nio/DirectByteBuffer;I)J
 HSPLjava/util/zip/ZipUtils;->get32([BI)J
 HSPLjava/util/zip/ZipUtils;->unixTimeToFileTime(J)Ljava/nio/file/attribute/FileTime;
 HSPLjavax/crypto/Cipher$CipherSpiAndProvider;-><init>(Ljavax/crypto/CipherSpi;Ljava/security/Provider;)V
@@ -8500,6 +8542,12 @@ HSPLjdk/internal/math/MathUtils;->flog2pow10(I)I
 HSPLjdk/internal/math/MathUtils;->g0(I)J
 HSPLjdk/internal/math/MathUtils;->g1(I)J
 HSPLjdk/internal/math/MathUtils;->pow10(I)J
+HSPLjdk/internal/misc/CarrierThreadLocal;->get()Ljava/lang/Object;
+SPLjdk/internal/misc/CarrierThreadLocal;->set(Ljava/lang/Object;)V
+SPLjdk/internal/misc/TerminatingThreadLocal$1;->initialValue()Ljava/lang/Object;
+HSPLjdk/internal/misc/TerminatingThreadLocal$1;->initialValue()Ljava/util/Collection;
+HSPLjdk/internal/misc/TerminatingThreadLocal;->register(Ljdk/internal/misc/TerminatingThreadLocal;)V
+HSPLjdk/internal/misc/TerminatingThreadLocal;->set(Ljava/lang/Object;)V
 HSPLjdk/internal/misc/Unsafe;->arrayBaseOffset(Ljava/lang/Class;)I
 HSPLjdk/internal/misc/Unsafe;->compareAndSetObject(Ljava/lang/Object;JLjava/lang/Object;Ljava/lang/Object;)Z
 HSPLjdk/internal/misc/Unsafe;->getAndAddInt(Ljava/lang/Object;JI)I
@@ -8562,6 +8610,18 @@ HSPLjdk/internal/util/WeakReferenceKey;->hashCode()I
 HSPLjdk/internal/util/random/RandomSupport;-><clinit>()V
 HSPLjdk/internal/util/random/RandomSupport;->mixMurmur64(J)J
 HSPLjdk/internal/util/random/RandomSupport;->secureRandomSeedRequested()Z
+SPLjdk/internal/vm/SharedThreadContainer;-><init>(Ljava/lang/String;)V
+HSPLjdk/internal/vm/SharedThreadContainer;->close()V
+SPLjdk/internal/vm/SharedThreadContainer;->create(Ljava/lang/String;)Ljdk/internal/vm/SharedThreadContainer;
+SPLjdk/internal/vm/SharedThreadContainer;->create(Ljdk/internal/vm/ThreadContainer;Ljava/lang/String;)Ljdk/internal/vm/SharedThreadContainer;
+HSPLjdk/internal/vm/SharedThreadContainer;->onStart(Ljava/lang/Thread;)V
+HSPLjdk/internal/vm/SharedThreadContainer;->start(Ljava/lang/Thread;)V
+SPLjdk/internal/vm/StackableScope;-><init>(Z)V
+HSPLjdk/internal/vm/StackableScope;->owner()Ljava/lang/Thread;
+SPLjdk/internal/vm/ThreadContainer;-><init>(Z)V
+SPLjdk/internal/vm/ThreadContainers;->expungeStaleEntries()V
+SPLjdk/internal/vm/ThreadContainers;->registerContainer(Ljdk/internal/vm/ThreadContainer;)Ljava/lang/Object;
+SPLjdk/internal/vm/ThreadContainers;->root()Ljdk/internal/vm/ThreadContainer;
 HSPLlibcore/content/type/MimeMap$Builder$Element;-><init>(Ljava/lang/String;Z)V
 HSPLlibcore/content/type/MimeMap$Builder$Element;->ofExtensionSpec(Ljava/lang/String;)Llibcore/content/type/MimeMap$Builder$Element;
 HSPLlibcore/content/type/MimeMap$Builder$Element;->ofMimeSpec(Ljava/lang/String;)Llibcore/content/type/MimeMap$Builder$Element;
@@ -8907,7 +8967,7 @@ HSPLlibcore/util/NativeAllocationRegistry;->createMalloced(Ljava/lang/ClassLoade
 HSPLlibcore/util/NativeAllocationRegistry;->createMalloced(Ljava/lang/ClassLoader;JJ)Llibcore/util/NativeAllocationRegistry;
 HSPLlibcore/util/NativeAllocationRegistry;->createNonmalloced(Ljava/lang/Class;JJ)Llibcore/util/NativeAllocationRegistry;
 HSPLlibcore/util/NativeAllocationRegistry;->createNonmalloced(Ljava/lang/ClassLoader;JJ)Llibcore/util/NativeAllocationRegistry;
-HSPLlibcore/util/NativeAllocationRegistry;->getMetrics()Ljava/util/Collection;+]Ljava/util/Iterator;Ljava/util/WeakHashMap$KeyIterator;]Ljava/util/Map;Ljava/util/WeakHashMap;]Ljava/util/Set;Ljava/util/WeakHashMap$KeySet;
+HSPLlibcore/util/NativeAllocationRegistry;->getMetrics()Ljava/util/Collection;
 HSPLlibcore/util/NativeAllocationRegistry;->isMalloced()Z
 HSPLlibcore/util/NativeAllocationRegistry;->registerNativeAllocation(J)V
 HSPLlibcore/util/NativeAllocationRegistry;->registerNativeAllocation(Ljava/lang/Object;J)Ljava/lang/Runnable;
@@ -9147,6 +9207,7 @@ HSPLsun/misc/LRUCache;->forName(Ljava/lang/Object;)Ljava/lang/Object;
 HSPLsun/misc/LRUCache;->moveToFront([Ljava/lang/Object;I)V
 HSPLsun/misc/Unsafe;->arrayBaseOffset(Ljava/lang/Class;)I
 HSPLsun/misc/Unsafe;->arrayIndexScale(Ljava/lang/Class;)I
+SPLsun/misc/Unsafe;->forbidObtainingRecordFieldOffsets()Z
 HSPLsun/misc/Unsafe;->getAndAddInt(Ljava/lang/Object;JI)I
 HSPLsun/misc/Unsafe;->getAndAddLong(Ljava/lang/Object;JJ)J
 HSPLsun/misc/Unsafe;->getAndSetInt(Ljava/lang/Object;JI)I
@@ -9377,9 +9438,6 @@ HSPLsun/nio/cs/ThreadLocalCoders$Cache;->moveToFront([Ljava/lang/Object;I)V
 HSPLsun/nio/cs/ThreadLocalCoders;->decoderFor(Ljava/lang/Object;)Ljava/nio/charset/CharsetDecoder;
 HSPLsun/nio/cs/ThreadLocalCoders;->encoderFor(Ljava/lang/Object;)Ljava/nio/charset/CharsetEncoder;
 HSPLsun/nio/fs/AbstractBasicFileAttributeView;-><init>()V
-HSPLsun/nio/fs/AbstractPath;-><init>()V
-HSPLsun/nio/fs/AbstractPath;->resolve(Ljava/lang/String;)Ljava/nio/file/Path;
-HSPLsun/nio/fs/AbstractPath;->toFile()Ljava/io/File;
 HSPLsun/nio/fs/LinuxFileSystemProvider;->getFileAttributeView(Ljava/nio/file/Path;Ljava/lang/Class;[Ljava/nio/file/LinkOption;)Ljava/nio/file/attribute/FileAttributeView;
 HSPLsun/nio/fs/LinuxFileSystemProvider;->readAttributes(Ljava/nio/file/Path;Ljava/lang/Class;[Ljava/nio/file/LinkOption;)Ljava/nio/file/attribute/BasicFileAttributes;
 HSPLsun/nio/fs/NativeBuffer$Deallocator;-><init>(J)V
@@ -9401,7 +9459,6 @@ HSPLsun/nio/fs/UnixChannelFactory;->newFileChannel(ILsun/nio/fs/UnixPath;Ljava/l
 HSPLsun/nio/fs/UnixChannelFactory;->newFileChannel(Lsun/nio/fs/UnixPath;Ljava/util/Set;I)Ljava/nio/channels/FileChannel;
 HSPLsun/nio/fs/UnixChannelFactory;->open(ILsun/nio/fs/UnixPath;Ljava/lang/String;Lsun/nio/fs/UnixChannelFactory$Flags;I)Ljava/io/FileDescriptor;
 HSPLsun/nio/fs/UnixDirectoryStream$UnixDirectoryIterator;-><clinit>()V
-HSPLsun/nio/fs/UnixDirectoryStream$UnixDirectoryIterator;-><init>(Lsun/nio/fs/UnixDirectoryStream;Ljava/nio/file/DirectoryStream;)V
 HSPLsun/nio/fs/UnixDirectoryStream$UnixDirectoryIterator;->hasNext()Z
 HSPLsun/nio/fs/UnixDirectoryStream$UnixDirectoryIterator;->isSelfOrParent([B)Z
 HSPLsun/nio/fs/UnixDirectoryStream$UnixDirectoryIterator;->next()Ljava/lang/Object;
@@ -9459,13 +9516,16 @@ HSPLsun/nio/fs/UnixFileSystem;->provider()Ljava/nio/file/spi/FileSystemProvider;
 HSPLsun/nio/fs/UnixFileSystemProvider$3;-><clinit>()V
 HSPLsun/nio/fs/UnixFileSystemProvider;->checkAccess(Ljava/nio/file/Path;[Ljava/nio/file/AccessMode;)V
 HSPLsun/nio/fs/UnixFileSystemProvider;->checkPath(Ljava/nio/file/Path;)Lsun/nio/fs/UnixPath;
+HSPLsun/nio/fs/UnixFileSystemProvider;->exists(Ljava/nio/file/Path;)Z+]Lsun/nio/fs/UnixPath;Lsun/nio/fs/UnixPath;
 HSPLsun/nio/fs/UnixFileSystemProvider;->getFileAttributeView(Ljava/nio/file/Path;Ljava/lang/Class;[Ljava/nio/file/LinkOption;)Ljava/nio/file/attribute/FileAttributeView;
 HSPLsun/nio/fs/UnixFileSystemProvider;->newByteChannel(Ljava/nio/file/Path;Ljava/util/Set;[Ljava/nio/file/attribute/FileAttribute;)Ljava/nio/channels/SeekableByteChannel;
 HSPLsun/nio/fs/UnixFileSystemProvider;->newDirectoryStream(Ljava/nio/file/Path;Ljava/nio/file/DirectoryStream$Filter;)Ljava/nio/file/DirectoryStream;
 HSPLsun/nio/fs/UnixFileSystemProvider;->newFileChannel(Ljava/nio/file/Path;Ljava/util/Set;[Ljava/nio/file/attribute/FileAttribute;)Ljava/nio/channels/FileChannel;
 HSPLsun/nio/fs/UnixFileSystemProvider;->readAttributes(Ljava/nio/file/Path;Ljava/lang/Class;[Ljava/nio/file/LinkOption;)Ljava/nio/file/attribute/BasicFileAttributes;
+HSPLsun/nio/fs/UnixFileSystemProvider;->theFileSystem()Lsun/nio/fs/UnixFileSystem;
 HSPLsun/nio/fs/UnixNativeDispatcher;->access(Lsun/nio/fs/UnixPath;I)V
 HSPLsun/nio/fs/UnixNativeDispatcher;->copyToNativeBuffer(Lsun/nio/fs/UnixPath;)Lsun/nio/fs/NativeBuffer;
+HSPLsun/nio/fs/UnixNativeDispatcher;->exists(Lsun/nio/fs/UnixPath;)Z+]Lsun/nio/fs/NativeBuffer;Lsun/nio/fs/NativeBuffer;
 HSPLsun/nio/fs/UnixNativeDispatcher;->lstat(Lsun/nio/fs/UnixPath;Lsun/nio/fs/UnixFileAttributes;)V
 HSPLsun/nio/fs/UnixNativeDispatcher;->open(Lsun/nio/fs/UnixPath;II)I
 HSPLsun/nio/fs/UnixNativeDispatcher;->openatSupported()Z
@@ -10155,8 +10215,9 @@ HSPLsun/util/locale/BaseLocale;->getVariant()Ljava/lang/String;
 HSPLsun/util/locale/BaseLocale;->hashCode()I
 HSPLsun/util/locale/Extension;->getID()Ljava/lang/String;
 HSPLsun/util/locale/Extension;->setValue(Ljava/lang/String;)V
-HSPLsun/util/locale/Extension;->toString()Ljava/lang/String;+]Lsun/util/locale/Extension;Lsun/util/locale/UnicodeLocaleExtension;
+HSPLsun/util/locale/Extension;->toString()Ljava/lang/String;
 HSPLsun/util/locale/InternalLocaleBuilder$CaseInsensitiveChar;->hashCode()I
+HSPLsun/util/locale/InternalLocaleBuilder$CaseInsensitiveString;->hashCode()I
 HSPLsun/util/locale/InternalLocaleBuilder;-><init>()V
 HSPLsun/util/locale/InternalLocaleBuilder;->checkVariants(Ljava/lang/String;Ljava/lang/String;)I
 HSPLsun/util/locale/InternalLocaleBuilder;->clear()Lsun/util/locale/InternalLocaleBuilder;
@@ -10168,7 +10229,7 @@ HSPLsun/util/locale/InternalLocaleBuilder;->setLanguage(Ljava/lang/String;)Lsun/
 HSPLsun/util/locale/InternalLocaleBuilder;->setLanguageTag(Lsun/util/locale/LanguageTag;)Lsun/util/locale/InternalLocaleBuilder;
 HSPLsun/util/locale/InternalLocaleBuilder;->setRegion(Ljava/lang/String;)Lsun/util/locale/InternalLocaleBuilder;
 HSPLsun/util/locale/InternalLocaleBuilder;->setScript(Ljava/lang/String;)Lsun/util/locale/InternalLocaleBuilder;
-HSPLsun/util/locale/InternalLocaleBuilder;->setUnicodeLocaleExtension(Ljava/lang/String;)V+]Ljava/util/Map;Ljava/util/HashMap;
+HSPLsun/util/locale/InternalLocaleBuilder;->setUnicodeLocaleExtension(Ljava/lang/String;)V
 HSPLsun/util/locale/InternalLocaleBuilder;->setVariant(Ljava/lang/String;)Lsun/util/locale/InternalLocaleBuilder;
 HSPLsun/util/locale/LanguageTag;-><init>()V
 HSPLsun/util/locale/LanguageTag;->canonicalizeLanguage(Ljava/lang/String;)Ljava/lang/String;
@@ -10196,10 +10257,10 @@ HSPLsun/util/locale/LanguageTag;->parsePrivateuse(Lsun/util/locale/StringTokenIt
 HSPLsun/util/locale/LanguageTag;->parseRegion(Lsun/util/locale/StringTokenIterator;Lsun/util/locale/ParseStatus;)Z
 HSPLsun/util/locale/LanguageTag;->parseScript(Lsun/util/locale/StringTokenIterator;Lsun/util/locale/ParseStatus;)Z
 HSPLsun/util/locale/LanguageTag;->parseVariants(Lsun/util/locale/StringTokenIterator;Lsun/util/locale/ParseStatus;)Z
-HSPLsun/util/locale/LocaleExtensions;-><init>(Ljava/util/Map;Ljava/util/Set;Ljava/util/Map;)V+]Ljava/util/Iterator;Ljava/util/HashMap$EntryIterator;]Ljava/util/Map$Entry;Ljava/util/HashMap$Node;]Ljava/util/Map;Ljava/util/HashMap;]Ljava/util/Set;Ljava/util/HashMap$EntrySet;
+HSPLsun/util/locale/LocaleExtensions;-><init>(Ljava/util/Map;Ljava/util/Set;Ljava/util/Map;)V
 HSPLsun/util/locale/LocaleExtensions;->equals(Ljava/lang/Object;)Z
 HSPLsun/util/locale/LocaleExtensions;->hashCode()I
-HSPLsun/util/locale/LocaleExtensions;->toID(Ljava/util/SortedMap;)Ljava/lang/String;+]Ljava/util/Iterator;Ljava/util/TreeMap$EntryIterator;]Ljava/util/Map$Entry;Ljava/util/TreeMap$TreeMapEntry;]Ljava/util/Set;Ljava/util/TreeMap$EntrySet;]Ljava/util/SortedMap;Ljava/util/TreeMap;
+HSPLsun/util/locale/LocaleExtensions;->toID(Ljava/util/SortedMap;)Ljava/lang/String;
 HSPLsun/util/locale/LocaleObjectCache$CacheEntry;-><init>(Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/ref/ReferenceQueue;)V
 HSPLsun/util/locale/LocaleObjectCache$CacheEntry;->getKey()Ljava/lang/Object;
 HSPLsun/util/locale/LocaleObjectCache;->cleanStaleEntries()V
@@ -10234,7 +10295,9 @@ HSPLsun/util/locale/StringTokenIterator;->isDone()Z
 HSPLsun/util/locale/StringTokenIterator;->next()Ljava/lang/String;
 HSPLsun/util/locale/StringTokenIterator;->nextDelimiter(I)I
 HSPLsun/util/locale/StringTokenIterator;->setStart(I)Lsun/util/locale/StringTokenIterator;
-HSPLsun/util/locale/UnicodeLocaleExtension;-><init>(Ljava/util/SortedSet;Ljava/util/SortedMap;)V+]Ljava/util/Iterator;Ljava/util/Collections$EmptyIterator;,Ljava/util/TreeMap$EntryIterator;]Ljava/util/Map$Entry;Ljava/util/TreeMap$TreeMapEntry;]Ljava/util/Map;Ljava/util/TreeMap;]Ljava/util/Set;Ljava/util/Collections$EmptySet;,Ljava/util/TreeMap$EntrySet;]Lsun/util/locale/UnicodeLocaleExtension;Lsun/util/locale/UnicodeLocaleExtension;
+HSPLsun/util/locale/UnicodeLocaleExtension;-><init>(Ljava/util/SortedSet;Ljava/util/SortedMap;)V
+HSPLsun/util/locale/UnicodeLocaleExtension;->getID()Ljava/lang/String;
+HSPLsun/util/locale/UnicodeLocaleExtension;->toString()Ljava/lang/String;
 HSPLsun/util/locale/provider/CalendarDataUtility;->retrieveFirstDayOfWeek(Ljava/util/Locale;I)I
 HSPLsun/util/logging/LoggingSupport$2;-><init>()V
 HSPLsun/util/logging/LoggingSupport$2;->run()Ljava/lang/Object;
@@ -10246,20 +10309,19 @@ HSPLsun/util/logging/PlatformLogger$JavaLoggerProxy;-><init>(Ljava/lang/String;L
 HSPLsun/util/logging/PlatformLogger$LoggerProxy;-><init>(Ljava/lang/String;)V
 HSPLsun/util/logging/PlatformLogger;-><init>(Ljava/lang/String;)V
 HSPLsun/util/logging/PlatformLogger;->getLogger(Ljava/lang/String;)Lsun/util/logging/PlatformLogger;
-Landroid/app/ActivityThread$AndroidOs;
 Landroid/app/AppOpsManager$$ExternalSyntheticLambda4;
 Landroid/compat/Compatibility$1;
 Landroid/compat/Compatibility$BehaviorChangeDelegate;
 Landroid/compat/Compatibility$ChangeConfig;
 Landroid/compat/Compatibility;
 Landroid/content/ContentCaptureOptions$ContentProtectionOptions$$ExternalSyntheticLambda0;
-Landroid/content/res/Resources$$ExternalSyntheticLambda1;
 Landroid/crypto/hpke/HpkeSpi;
-Landroid/graphics/ColorSpace$Rgb$$ExternalSyntheticLambda4;
 Landroid/graphics/ColorSpace$Rgb$$ExternalSyntheticLambda6;
 Landroid/graphics/ColorSpace$Rgb$$ExternalSyntheticLambda9;
-Landroid/icu/text/RuleBasedBreakIterator;
-Landroid/os/ParcelFileDescriptor$AutoCloseInputStream;
+Landroid/media/AudioDeviceAttributes;
+Landroid/net/TrafficStats$1;
+Landroid/os/StrictMode$1;
+Landroid/os/StrictMode$4;
 Landroid/os/StrictMode$AndroidBlockGuardPolicy;
 Landroid/service/notification/StatusBarNotification$$ExternalSyntheticLambda0;
 Landroid/system/ErrnoException;
@@ -10269,6 +10331,7 @@ Landroid/system/Int64Ref;
 Landroid/system/NetlinkSocketAddress;
 Landroid/system/Os;
 Landroid/system/OsConstants;
+Landroid/system/OsConstantsHolder;
 Landroid/system/PacketSocketAddress;
 Landroid/system/StructAddrinfo;
 Landroid/system/StructCapUserData;
@@ -10291,6 +10354,7 @@ Landroid/system/SystemCleaner;
 Landroid/system/UnixSocketAddress;
 Landroid/system/VmSocketAddress;
 Landroid/util/ArrayMap;
+Landroid/util/ArraySet;
 Landroid/util/IndentingPrintWriter;
 Landroid/util/MapCollections$ArrayIterator;
 Landroid/util/MapCollections$EntrySet;
@@ -10465,28 +10529,53 @@ Lcom/android/okhttp/okio/Source;
 Lcom/android/okhttp/okio/Timeout$1;
 Lcom/android/okhttp/okio/Timeout;
 Lcom/android/okhttp/okio/Util;
-Lcom/android/org/bouncycastle/asn1/ASN1ApplicationSpecific;
-Lcom/android/org/bouncycastle/asn1/ASN1ApplicationSpecificParser;
+Lcom/android/org/bouncycastle/asn1/ASN1BMPString$1;
+Lcom/android/org/bouncycastle/asn1/ASN1BMPString;
+Lcom/android/org/bouncycastle/asn1/ASN1BitString$1;
 Lcom/android/org/bouncycastle/asn1/ASN1BitString;
+Lcom/android/org/bouncycastle/asn1/ASN1BitStringParser;
+Lcom/android/org/bouncycastle/asn1/ASN1Boolean$1;
 Lcom/android/org/bouncycastle/asn1/ASN1Boolean;
 Lcom/android/org/bouncycastle/asn1/ASN1Choice;
 Lcom/android/org/bouncycastle/asn1/ASN1Encodable;
 Lcom/android/org/bouncycastle/asn1/ASN1EncodableVector;
+Lcom/android/org/bouncycastle/asn1/ASN1Enumerated$1;
 Lcom/android/org/bouncycastle/asn1/ASN1Enumerated;
 Lcom/android/org/bouncycastle/asn1/ASN1Exception;
+Lcom/android/org/bouncycastle/asn1/ASN1External$1;
 Lcom/android/org/bouncycastle/asn1/ASN1External;
+Lcom/android/org/bouncycastle/asn1/ASN1GeneralString$1;
+Lcom/android/org/bouncycastle/asn1/ASN1GeneralString;
+Lcom/android/org/bouncycastle/asn1/ASN1GeneralizedTime$1;
 Lcom/android/org/bouncycastle/asn1/ASN1GeneralizedTime;
+Lcom/android/org/bouncycastle/asn1/ASN1GraphicString$1;
+Lcom/android/org/bouncycastle/asn1/ASN1GraphicString;
+Lcom/android/org/bouncycastle/asn1/ASN1IA5String$1;
+Lcom/android/org/bouncycastle/asn1/ASN1IA5String;
 Lcom/android/org/bouncycastle/asn1/ASN1InputStream;
+Lcom/android/org/bouncycastle/asn1/ASN1Integer$1;
 Lcom/android/org/bouncycastle/asn1/ASN1Integer;
+Lcom/android/org/bouncycastle/asn1/ASN1Null$1;
 Lcom/android/org/bouncycastle/asn1/ASN1Null;
+Lcom/android/org/bouncycastle/asn1/ASN1NumericString$1;
+Lcom/android/org/bouncycastle/asn1/ASN1NumericString;
 Lcom/android/org/bouncycastle/asn1/ASN1Object;
+Lcom/android/org/bouncycastle/asn1/ASN1ObjectDescriptor$1;
+Lcom/android/org/bouncycastle/asn1/ASN1ObjectDescriptor;
+Lcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier$1;
 Lcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier$OidHandle;
 Lcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier;
+Lcom/android/org/bouncycastle/asn1/ASN1OctetString$1;
 Lcom/android/org/bouncycastle/asn1/ASN1OctetString;
 Lcom/android/org/bouncycastle/asn1/ASN1OctetStringParser;
 Lcom/android/org/bouncycastle/asn1/ASN1OutputStream;
 Lcom/android/org/bouncycastle/asn1/ASN1Primitive;
+Lcom/android/org/bouncycastle/asn1/ASN1PrintableString$1;
+Lcom/android/org/bouncycastle/asn1/ASN1PrintableString;
+Lcom/android/org/bouncycastle/asn1/ASN1RelativeOID$1;
+Lcom/android/org/bouncycastle/asn1/ASN1RelativeOID;
 Lcom/android/org/bouncycastle/asn1/ASN1Sequence$1;
+Lcom/android/org/bouncycastle/asn1/ASN1Sequence$2;
 Lcom/android/org/bouncycastle/asn1/ASN1Sequence;
 Lcom/android/org/bouncycastle/asn1/ASN1SequenceParser;
 Lcom/android/org/bouncycastle/asn1/ASN1Set$1;
@@ -10494,19 +10583,35 @@ Lcom/android/org/bouncycastle/asn1/ASN1Set;
 Lcom/android/org/bouncycastle/asn1/ASN1SetParser;
 Lcom/android/org/bouncycastle/asn1/ASN1StreamParser;
 Lcom/android/org/bouncycastle/asn1/ASN1String;
+Lcom/android/org/bouncycastle/asn1/ASN1T61String$1;
+Lcom/android/org/bouncycastle/asn1/ASN1T61String;
+Lcom/android/org/bouncycastle/asn1/ASN1Tag;
 Lcom/android/org/bouncycastle/asn1/ASN1TaggedObject;
 Lcom/android/org/bouncycastle/asn1/ASN1TaggedObjectParser;
+Lcom/android/org/bouncycastle/asn1/ASN1Type;
+Lcom/android/org/bouncycastle/asn1/ASN1UTCTime$1;
 Lcom/android/org/bouncycastle/asn1/ASN1UTCTime;
-Lcom/android/org/bouncycastle/asn1/BERApplicationSpecific;
-Lcom/android/org/bouncycastle/asn1/BERApplicationSpecificParser;
+Lcom/android/org/bouncycastle/asn1/ASN1UTF8String$1;
+Lcom/android/org/bouncycastle/asn1/ASN1UTF8String;
+Lcom/android/org/bouncycastle/asn1/ASN1UniversalString$1;
+Lcom/android/org/bouncycastle/asn1/ASN1UniversalString;
+Lcom/android/org/bouncycastle/asn1/ASN1UniversalType;
+Lcom/android/org/bouncycastle/asn1/ASN1VideotexString$1;
+Lcom/android/org/bouncycastle/asn1/ASN1VideotexString;
+Lcom/android/org/bouncycastle/asn1/ASN1VisibleString$1;
+Lcom/android/org/bouncycastle/asn1/ASN1VisibleString;
+Lcom/android/org/bouncycastle/asn1/BERBitString;
+Lcom/android/org/bouncycastle/asn1/BERBitStringParser;
+Lcom/android/org/bouncycastle/asn1/BERFactory;
 Lcom/android/org/bouncycastle/asn1/BEROctetString;
 Lcom/android/org/bouncycastle/asn1/BEROctetStringParser;
 Lcom/android/org/bouncycastle/asn1/BERSequence;
 Lcom/android/org/bouncycastle/asn1/BERSequenceParser;
 Lcom/android/org/bouncycastle/asn1/BERSet;
 Lcom/android/org/bouncycastle/asn1/BERSetParser;
+Lcom/android/org/bouncycastle/asn1/BERTaggedObject;
 Lcom/android/org/bouncycastle/asn1/BERTaggedObjectParser;
-Lcom/android/org/bouncycastle/asn1/BERTags;
+Lcom/android/org/bouncycastle/asn1/ConstructedBitStream;
 Lcom/android/org/bouncycastle/asn1/ConstructedOctetStream;
 Lcom/android/org/bouncycastle/asn1/DERBMPString;
 Lcom/android/org/bouncycastle/asn1/DERBitString;
@@ -10527,7 +10632,6 @@ Lcom/android/org/bouncycastle/asn1/DERUTF8String;
 Lcom/android/org/bouncycastle/asn1/DERUniversalString;
 Lcom/android/org/bouncycastle/asn1/DERVideotexString;
 Lcom/android/org/bouncycastle/asn1/DERVisibleString;
-Lcom/android/org/bouncycastle/asn1/DLApplicationSpecific;
 Lcom/android/org/bouncycastle/asn1/DLBitString;
 Lcom/android/org/bouncycastle/asn1/DLExternal;
 Lcom/android/org/bouncycastle/asn1/DLFactory;
@@ -10562,18 +10666,31 @@ Lcom/android/org/bouncycastle/crypto/BlockCipher;
 Lcom/android/org/bouncycastle/crypto/BufferedBlockCipher;
 Lcom/android/org/bouncycastle/crypto/CipherParameters;
 Lcom/android/org/bouncycastle/crypto/CryptoException;
+Lcom/android/org/bouncycastle/crypto/CryptoServiceConstraintsException;
+Lcom/android/org/bouncycastle/crypto/CryptoServiceProperties;
+Lcom/android/org/bouncycastle/crypto/CryptoServicePurpose;
+Lcom/android/org/bouncycastle/crypto/CryptoServicesConstraints;
 Lcom/android/org/bouncycastle/crypto/CryptoServicesPermission;
+Lcom/android/org/bouncycastle/crypto/CryptoServicesRegistrar$$ExternalSyntheticBackportWithForwarding0;
+Lcom/android/org/bouncycastle/crypto/CryptoServicesRegistrar$1;
 Lcom/android/org/bouncycastle/crypto/CryptoServicesRegistrar$Property;
+Lcom/android/org/bouncycastle/crypto/CryptoServicesRegistrar$ThreadLocalSecureRandomProvider;
 Lcom/android/org/bouncycastle/crypto/CryptoServicesRegistrar;
 Lcom/android/org/bouncycastle/crypto/DataLengthException;
+Lcom/android/org/bouncycastle/crypto/DefaultBufferedBlockCipher;
+Lcom/android/org/bouncycastle/crypto/DefaultMultiBlockCipher;
 Lcom/android/org/bouncycastle/crypto/Digest;
 Lcom/android/org/bouncycastle/crypto/ExtendedDigest;
 Lcom/android/org/bouncycastle/crypto/InvalidCipherTextException;
 Lcom/android/org/bouncycastle/crypto/Mac;
+Lcom/android/org/bouncycastle/crypto/MultiBlockCipher;
 Lcom/android/org/bouncycastle/crypto/OutputLengthException;
 Lcom/android/org/bouncycastle/crypto/PBEParametersGenerator;
 Lcom/android/org/bouncycastle/crypto/RuntimeCryptoException;
+Lcom/android/org/bouncycastle/crypto/SavableDigest;
+Lcom/android/org/bouncycastle/crypto/SecureRandomProvider;
 Lcom/android/org/bouncycastle/crypto/Wrapper;
+Lcom/android/org/bouncycastle/crypto/constraints/DefaultServiceProperties;
 Lcom/android/org/bouncycastle/crypto/digests/AndroidDigestFactory;
 Lcom/android/org/bouncycastle/crypto/digests/AndroidDigestFactoryBouncyCastle;
 Lcom/android/org/bouncycastle/crypto/digests/AndroidDigestFactoryInterface;
@@ -10595,7 +10712,9 @@ Lcom/android/org/bouncycastle/crypto/digests/SHA256Digest;
 Lcom/android/org/bouncycastle/crypto/digests/SHA384Digest;
 Lcom/android/org/bouncycastle/crypto/digests/SHA512Digest;
 Lcom/android/org/bouncycastle/crypto/engines/AESEngine;
+Lcom/android/org/bouncycastle/crypto/engines/DESBase;
 Lcom/android/org/bouncycastle/crypto/engines/DESEngine;
+Lcom/android/org/bouncycastle/crypto/engines/Utils;
 Lcom/android/org/bouncycastle/crypto/generators/OpenSSLPBEParametersGenerator;
 Lcom/android/org/bouncycastle/crypto/generators/PKCS12ParametersGenerator;
 Lcom/android/org/bouncycastle/crypto/generators/PKCS5S1ParametersGenerator;
@@ -10605,6 +10724,7 @@ Lcom/android/org/bouncycastle/crypto/macs/HMac;
 Lcom/android/org/bouncycastle/crypto/modes/AEADBlockCipher;
 Lcom/android/org/bouncycastle/crypto/modes/AEADCipher;
 Lcom/android/org/bouncycastle/crypto/modes/CBCBlockCipher;
+Lcom/android/org/bouncycastle/crypto/modes/CBCModeCipher;
 Lcom/android/org/bouncycastle/crypto/paddings/BlockCipherPadding;
 Lcom/android/org/bouncycastle/crypto/paddings/PKCS7Padding;
 Lcom/android/org/bouncycastle/crypto/paddings/PaddedBufferedBlockCipher;
@@ -10619,16 +10739,12 @@ Lcom/android/org/bouncycastle/crypto/params/DSAValidationParameters;
 Lcom/android/org/bouncycastle/crypto/params/KeyParameter;
 Lcom/android/org/bouncycastle/crypto/params/ParametersWithIV;
 Lcom/android/org/bouncycastle/crypto/params/ParametersWithRandom;
-Lcom/android/org/bouncycastle/jcajce/PBKDFKey;
 Lcom/android/org/bouncycastle/jcajce/PKCS12Key;
-Lcom/android/org/bouncycastle/jcajce/PKCS12KeyWithParameters;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/DH$Mappings;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/DH;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/DSA$Mappings;
-Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/EC$Mappings;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/RSA$Mappings;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/RSA;
-Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/X509$Mappings;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/dh/KeyFactorySpi;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/dsa/BCDSAPublicKey;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/dsa/DSAUtil;
@@ -10647,8 +10763,6 @@ Lcom/android/org/bouncycastle/jcajce/provider/config/ConfigurableProvider;
 Lcom/android/org/bouncycastle/jcajce/provider/config/ProviderConfiguration;
 Lcom/android/org/bouncycastle/jcajce/provider/config/ProviderConfigurationPermission;
 Lcom/android/org/bouncycastle/jcajce/provider/digest/DigestAlgorithmProvider;
-Lcom/android/org/bouncycastle/jcajce/provider/digest/MD5$Mappings;
-Lcom/android/org/bouncycastle/jcajce/provider/digest/MD5;
 Lcom/android/org/bouncycastle/jcajce/provider/digest/SHA1$Mappings;
 Lcom/android/org/bouncycastle/jcajce/provider/digest/SHA1;
 Lcom/android/org/bouncycastle/jcajce/provider/digest/SHA224$Mappings;
@@ -10703,6 +10817,7 @@ Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/BlockCipherProvider
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/ClassUtil$1;
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/ClassUtil;
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/GcmSpecUtil$2;
+Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/GcmSpecUtil$3;
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/GcmSpecUtil;
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/PBE$Util;
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/PBE;
@@ -10710,16 +10825,15 @@ Lcom/android/org/bouncycastle/jcajce/provider/util/AlgorithmProvider;
 Lcom/android/org/bouncycastle/jcajce/provider/util/AsymmetricAlgorithmProvider;
 Lcom/android/org/bouncycastle/jcajce/provider/util/AsymmetricKeyInfoConverter;
 Lcom/android/org/bouncycastle/jcajce/provider/util/DigestFactory;
-Lcom/android/org/bouncycastle/jcajce/spec/AEADParameterSpec;
 Lcom/android/org/bouncycastle/jcajce/spec/PBKDF2KeySpec;
 Lcom/android/org/bouncycastle/jcajce/util/BCJcaJceHelper;
 Lcom/android/org/bouncycastle/jcajce/util/DefaultJcaJceHelper;
 Lcom/android/org/bouncycastle/jcajce/util/JcaJceHelper;
 Lcom/android/org/bouncycastle/jcajce/util/ProviderJcaJceHelper;
 Lcom/android/org/bouncycastle/jce/X509Principal;
-Lcom/android/org/bouncycastle/jce/interfaces/BCKeyStore;
 Lcom/android/org/bouncycastle/jce/interfaces/PKCS12BagAttributeCarrier;
 Lcom/android/org/bouncycastle/jce/provider/BouncyCastleProvider$1;
+Lcom/android/org/bouncycastle/jce/provider/BouncyCastleProvider$JcaCryptoService;
 Lcom/android/org/bouncycastle/jce/provider/BouncyCastleProvider$PrivateProvider;
 Lcom/android/org/bouncycastle/jce/provider/BouncyCastleProvider;
 Lcom/android/org/bouncycastle/jce/provider/BouncyCastleProviderConfiguration;
@@ -10727,9 +10841,7 @@ Lcom/android/org/bouncycastle/jce/provider/CertStoreCollectionSpi;
 Lcom/android/org/bouncycastle/jce/provider/X509CRLObject;
 Lcom/android/org/bouncycastle/util/Arrays;
 Lcom/android/org/bouncycastle/util/BigIntegers;
-Lcom/android/org/bouncycastle/util/Encodable;
 Lcom/android/org/bouncycastle/util/Integers;
-Lcom/android/org/bouncycastle/util/Iterable;
 Lcom/android/org/bouncycastle/util/Memoable;
 Lcom/android/org/bouncycastle/util/Pack;
 Lcom/android/org/bouncycastle/util/Properties$1;
@@ -10786,6 +10898,10 @@ Ldalvik/system/VMRuntime$HiddenApiUsageLogger;
 Ldalvik/system/VMRuntime$SdkVersionContainer;
 Ldalvik/system/VMRuntime;
 Ldalvik/system/VMStack;
+Ldalvik/system/VirtualThreadContext;
+Ldalvik/system/VirtualThreadFrame;
+Ldalvik/system/VirtualThreadParkedStates;
+Ldalvik/system/VirtualThreadParkingError;
 Ldalvik/system/ZipPathValidator$1;
 Ldalvik/system/ZipPathValidator$Callback;
 Ldalvik/system/ZipPathValidator;
@@ -10953,7 +11069,10 @@ Ljava/lang/ClassLoader$$ExternalSyntheticLambda2;
 Ljava/lang/ClassLoader$SystemClassLoader;
 Ljava/lang/ClassLoader;
 Ljava/lang/ClassNotFoundException;
+Ljava/lang/ClassValue$ClassValueMap;
 Ljava/lang/ClassValue$Entry;
+Ljava/lang/ClassValue$Identity;
+Ljava/lang/ClassValue$Version;
 Ljava/lang/ClassValue;
 Ljava/lang/CloneNotSupportedException;
 Ljava/lang/Cloneable;
@@ -10993,6 +11112,7 @@ Ljava/lang/Integer;
 Ljava/lang/InternalError;
 Ljava/lang/InterruptedException;
 Ljava/lang/Iterable;
+Ljava/lang/JavaLangAccess;
 Ljava/lang/LinkageError;
 Ljava/lang/Long$LongCache;
 Ljava/lang/Long;
@@ -11066,11 +11186,13 @@ Ljava/lang/StringUTF16;
 Ljava/lang/System$PropertiesWithNonOverrideableDefaults;
 Ljava/lang/System;
 Ljava/lang/Thread$1;
+Ljava/lang/Thread$AllThreadsRecord;
 Ljava/lang/Thread$Caches;
 Ljava/lang/Thread$State;
 Ljava/lang/Thread$ThreadIdentifiers;
 Ljava/lang/Thread$UncaughtExceptionHandler;
 Ljava/lang/Thread$WeakClassKey;
+Ljava/lang/Thread-IA;
 Ljava/lang/Thread;
 Ljava/lang/ThreadDeath;
 Ljava/lang/ThreadGroup;
@@ -11130,6 +11252,9 @@ Ljava/lang/invoke/ByteArrayViewVarHandle;
 Ljava/lang/invoke/ByteBufferViewVarHandle;
 Ljava/lang/invoke/CallSite;
 Ljava/lang/invoke/ConstantCallSite;
+Ljava/lang/invoke/DirectMethodHandle$Holder;
+Ljava/lang/invoke/DirectMethodHandle$MethodKey;
+Ljava/lang/invoke/DirectMethodHandle;
 Ljava/lang/invoke/FieldVarHandle;
 Ljava/lang/invoke/MethodHandle;
 Ljava/lang/invoke/MethodHandleImpl$HandleInfo;
@@ -11171,7 +11296,6 @@ Ljava/lang/invoke/SerializedLambda;
 Ljava/lang/invoke/StaticFieldVarHandle;
 Ljava/lang/invoke/Transformers$AlwaysThrow;
 Ljava/lang/invoke/Transformers$ArrayConstructor;
-Ljava/lang/invoke/Transformers$ArrayLength;
 Ljava/lang/invoke/Transformers$AsTypeAdapter;
 Ljava/lang/invoke/Transformers$BindTo;
 Ljava/lang/invoke/Transformers$CatchException;
@@ -11188,8 +11312,6 @@ Ljava/lang/invoke/Transformers$InsertArguments;
 Ljava/lang/invoke/Transformers$Invoker;
 Ljava/lang/invoke/Transformers$Loop;
 Ljava/lang/invoke/Transformers$PermuteArguments;
-Ljava/lang/invoke/Transformers$ReferenceArrayElementGetter;
-Ljava/lang/invoke/Transformers$ReferenceArrayElementSetter;
 Ljava/lang/invoke/Transformers$ReferenceIdentity;
 Ljava/lang/invoke/Transformers$Spreader;
 Ljava/lang/invoke/Transformers$TableSwitch;
@@ -11408,6 +11530,7 @@ Ljava/nio/channels/ByteChannel;
 Ljava/nio/channels/CancelledKeyException;
 Ljava/nio/channels/Channel;
 Ljava/nio/channels/Channels$1;
+Ljava/nio/channels/Channels$ReadableByteChannelImpl;
 Ljava/nio/channels/Channels$WritableByteChannelImpl;
 Ljava/nio/channels/Channels;
 Ljava/nio/channels/ClosedByInterruptException;
@@ -11458,7 +11581,9 @@ Ljava/nio/charset/CoderResult$Cache;
 Ljava/nio/charset/CoderResult;
 Ljava/nio/charset/CodingErrorAction;
 Ljava/nio/charset/IllegalCharsetNameException;
+Ljava/nio/charset/MalformedInputException;
 Ljava/nio/charset/StandardCharsets;
+Ljava/nio/charset/UnmappableCharacterException;
 Ljava/nio/charset/UnsupportedCharsetException;
 Ljava/nio/charset/spi/CharsetProvider;
 Ljava/nio/file/AccessDeniedException;
@@ -11471,8 +11596,6 @@ Ljava/nio/file/DirectoryStream;
 Ljava/nio/file/FileAlreadyExistsException;
 Ljava/nio/file/FileSystem;
 Ljava/nio/file/FileSystemException;
-Ljava/nio/file/FileSystems$DefaultFileSystemHolder$1;
-Ljava/nio/file/FileSystems$DefaultFileSystemHolder;
 Ljava/nio/file/FileSystems;
 Ljava/nio/file/FileVisitResult;
 Ljava/nio/file/FileVisitor;
@@ -12025,6 +12148,7 @@ Ljava/util/ImmutableCollections$Map1;
 Ljava/util/ImmutableCollections$MapN$1;
 Ljava/util/ImmutableCollections$MapN$MapNIterator;
 Ljava/util/ImmutableCollections$MapN;
+Ljava/util/ImmutableCollections$NoPreloadHolder;
 Ljava/util/ImmutableCollections$Set12;
 Ljava/util/ImmutableCollections$SetN$SetNIterator;
 Ljava/util/ImmutableCollections$SetN;
@@ -12050,6 +12174,7 @@ Ljava/util/LinkedHashSet;
 Ljava/util/LinkedList$DescendingIterator;
 Ljava/util/LinkedList$ListItr;
 Ljava/util/LinkedList$Node;
+Ljava/util/LinkedList-IA;
 Ljava/util/LinkedList;
 Ljava/util/List;
 Ljava/util/ListIterator;
@@ -12169,7 +12294,9 @@ Ljava/util/TreeMap$NavigableSubMap$SubMapKeyIterator;
 Ljava/util/TreeMap$NavigableSubMap;
 Ljava/util/TreeMap$PrivateEntryIterator;
 Ljava/util/TreeMap$TreeMapEntry;
+Ljava/util/TreeMap$TreeMapSpliterator;
 Ljava/util/TreeMap$ValueIterator;
+Ljava/util/TreeMap$ValueSpliterator;
 Ljava/util/TreeMap$Values;
 Ljava/util/TreeMap;
 Ljava/util/TreeSet;
@@ -12275,6 +12402,7 @@ Ljava/util/concurrent/ConcurrentMap;
 Ljava/util/concurrent/ConcurrentNavigableMap;
 Ljava/util/concurrent/ConcurrentSkipListMap$Index;
 Ljava/util/concurrent/ConcurrentSkipListMap$Iter;
+Ljava/util/concurrent/ConcurrentSkipListMap$KeyIterator;
 Ljava/util/concurrent/ConcurrentSkipListMap$KeySet;
 Ljava/util/concurrent/ConcurrentSkipListMap$Node;
 Ljava/util/concurrent/ConcurrentSkipListMap$ValueIterator;
@@ -12899,6 +13027,7 @@ Ljdk/internal/math/FormattedFloatingDecimal$1;
 Ljdk/internal/math/FormattedFloatingDecimal$Form;
 Ljdk/internal/math/FormattedFloatingDecimal;
 Ljdk/internal/math/MathUtils;
+Ljdk/internal/misc/CarrierThreadLocal;
 Ljdk/internal/misc/TerminatingThreadLocal$1;
 Ljdk/internal/misc/TerminatingThreadLocal;
 Ljdk/internal/misc/Unsafe;
@@ -12925,6 +13054,12 @@ Ljdk/internal/util/StaticProperty;
 Ljdk/internal/util/StrongReferenceKey;
 Ljdk/internal/util/WeakReferenceKey;
 Ljdk/internal/util/random/RandomSupport;
+Ljdk/internal/vm/SharedThreadContainer;
+Ljdk/internal/vm/StackableScope;
+Ljdk/internal/vm/ThreadContainer;
+Ljdk/internal/vm/ThreadContainers$RootContainer$TrackingRootContainer;
+Ljdk/internal/vm/ThreadContainers$RootContainer;
+Ljdk/internal/vm/ThreadContainers;
 Llibcore/content/type/MimeMap$$ExternalSyntheticLambda0;
 Llibcore/content/type/MimeMap$Builder$Element;
 Llibcore/content/type/MimeMap$Builder;
@@ -12940,16 +13075,6 @@ Llibcore/icu/SimpleDateFormatData;
 Llibcore/icu/TimeZoneNames$1;
 Llibcore/icu/TimeZoneNames$ZoneStringsCache;
 Llibcore/icu/TimeZoneNames;
-Llibcore/internal/Java21LanguageFeatures$$ExternalSyntheticTypeSwitch10;
-Llibcore/internal/Java21LanguageFeatures$$ExternalSyntheticTypeSwitch1;
-Llibcore/internal/Java21LanguageFeatures$$ExternalSyntheticTypeSwitch2;
-Llibcore/internal/Java21LanguageFeatures$$ExternalSyntheticTypeSwitch3;
-Llibcore/internal/Java21LanguageFeatures$$ExternalSyntheticTypeSwitch4;
-Llibcore/internal/Java21LanguageFeatures$$ExternalSyntheticTypeSwitch5;
-Llibcore/internal/Java21LanguageFeatures$$ExternalSyntheticTypeSwitch6;
-Llibcore/internal/Java21LanguageFeatures$$ExternalSyntheticTypeSwitch7;
-Llibcore/internal/Java21LanguageFeatures$$ExternalSyntheticTypeSwitch8;
-Llibcore/internal/Java21LanguageFeatures$$ExternalSyntheticTypeSwitch9;
 Llibcore/internal/StringPool;
 Llibcore/io/AsynchronousCloseMonitor;
 Llibcore/io/BlockGuardOs;
@@ -12961,6 +13086,9 @@ Llibcore/io/ForwardingOs;
 Llibcore/io/IoBridge;
 Llibcore/io/IoTracker$Mode;
 Llibcore/io/IoTracker;
+Llibcore/io/IoUtils$$ExternalSyntheticAutoCloseableDispatcher0;
+Llibcore/io/IoUtils$$ExternalSyntheticAutoCloseableForwarder1;
+Llibcore/io/IoUtils$$ExternalSyntheticThrowIAE2;
 Llibcore/io/IoUtils$FileReader;
 Llibcore/io/IoUtils;
 Llibcore/io/Libcore;
@@ -13185,7 +13313,6 @@ Lsun/nio/cs/ThreadLocalCoders$Cache;
 Lsun/nio/cs/ThreadLocalCoders;
 Lsun/nio/fs/AbstractBasicFileAttributeView;
 Lsun/nio/fs/AbstractFileSystemProvider;
-Lsun/nio/fs/AbstractPath;
 Lsun/nio/fs/DefaultFileSystemProvider;
 Lsun/nio/fs/DynamicFileAttributeView;
 Lsun/nio/fs/FileOwnerAttributeViewImpl;
@@ -13193,6 +13320,7 @@ Lsun/nio/fs/LinuxFileSystem;
 Lsun/nio/fs/LinuxFileSystemProvider;
 Lsun/nio/fs/NativeBuffer$Deallocator;
 Lsun/nio/fs/NativeBuffer;
+Lsun/nio/fs/NativeBuffers$1;
 Lsun/nio/fs/NativeBuffers;
 Lsun/nio/fs/UnixChannelFactory$1;
 Lsun/nio/fs/UnixChannelFactory$Flags;
@@ -13453,12 +13581,15 @@ Lsun/util/logging/PlatformLogger;
 [Lcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier;
 [Lcom/android/org/bouncycastle/asn1/ASN1OctetString;
 [Lcom/android/org/bouncycastle/asn1/ASN1Primitive;
+[Lcom/android/org/bouncycastle/crypto/CryptoServiceProperties;
 [Lcom/android/org/bouncycastle/crypto/params/DHParameters;
 [Lcom/android/org/bouncycastle/crypto/params/DSAParameters;
 [Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/x509/PEMUtil$Boundaries;
 [Lcom/android/org/kxml2/io/KXmlParser$ValueContext;
 [Ldalvik/system/DexPathList$Element;
 [Ldalvik/system/DexPathList$NativeLibraryElement;
+[Ldalvik/system/VirtualThreadFrame;
+[Ljava/io/Closeable;
 [Ljava/io/File$PathStatus;
 [Ljava/io/File;
 [Ljava/io/FileDescriptor;
diff --git a/build/boot/preloaded-classes b/build/boot/preloaded-classes
index f857972d6d..6bbfa299a6 100644
--- a/build/boot/preloaded-classes
+++ b/build/boot/preloaded-classes
@@ -21,7 +21,6 @@
 #
 # This file has been derived for mainline phone (and tablet) usage.
 #
-android.app.ActivityThread$AndroidOs
 android.app.AppOpsManager$$ExternalSyntheticLambda4
 android.compat.Compatibility$1
 android.compat.Compatibility$BehaviorChangeDelegate
@@ -29,11 +28,12 @@ android.compat.Compatibility$ChangeConfig
 android.compat.Compatibility
 android.content.ContentCaptureOptions$ContentProtectionOptions$$ExternalSyntheticLambda0
 android.crypto.hpke.HpkeSpi
-android.graphics.ColorSpace$Rgb$$ExternalSyntheticLambda4
-android.graphics.ColorSpace$Rgb$$ExternalSyntheticLambda6
-android.graphics.ColorSpace$Rgb$$ExternalSyntheticLambda9
-android.icu.text.RuleBasedBreakIterator
+android.media.AudioDeviceAttributes
+android.net.TrafficStats$1
+android.os.StrictMode$1
+android.os.StrictMode$4
 android.os.StrictMode$AndroidBlockGuardPolicy
+android.service.notification.StatusBarNotification$$ExternalSyntheticLambda0
 android.system.ErrnoException
 android.system.GaiException
 android.system.Int32Ref
@@ -41,6 +41,7 @@ android.system.Int64Ref
 android.system.NetlinkSocketAddress
 android.system.Os
 android.system.OsConstants
+android.system.OsConstantsHolder
 android.system.PacketSocketAddress
 android.system.StructAddrinfo
 android.system.StructCapUserData
@@ -63,7 +64,7 @@ android.system.SystemCleaner
 android.system.UnixSocketAddress
 android.system.VmSocketAddress
 android.util.ArrayMap
-android.util.IndentingPrintWriter
+android.util.ArraySet
 android.util.MapCollections$ArrayIterator
 android.util.MapCollections$EntrySet
 android.util.MapCollections$KeySet
@@ -74,7 +75,6 @@ com.android.art.flags.FeatureFlags
 com.android.art.flags.FeatureFlagsImpl
 com.android.art.flags.Flags
 com.android.internal.util.FastPrintWriter
-com.android.internal.util.IndentingPrintWriter
 com.android.libcore.FeatureFlags
 com.android.libcore.FeatureFlagsImpl
 com.android.libcore.Flags
@@ -237,28 +237,53 @@ com.android.okhttp.okio.Source
 com.android.okhttp.okio.Timeout$1
 com.android.okhttp.okio.Timeout
 com.android.okhttp.okio.Util
-com.android.org.bouncycastle.asn1.ASN1ApplicationSpecific
-com.android.org.bouncycastle.asn1.ASN1ApplicationSpecificParser
+com.android.org.bouncycastle.asn1.ASN1BMPString$1
+com.android.org.bouncycastle.asn1.ASN1BMPString
+com.android.org.bouncycastle.asn1.ASN1BitString$1
 com.android.org.bouncycastle.asn1.ASN1BitString
+com.android.org.bouncycastle.asn1.ASN1BitStringParser
+com.android.org.bouncycastle.asn1.ASN1Boolean$1
 com.android.org.bouncycastle.asn1.ASN1Boolean
 com.android.org.bouncycastle.asn1.ASN1Choice
 com.android.org.bouncycastle.asn1.ASN1Encodable
 com.android.org.bouncycastle.asn1.ASN1EncodableVector
+com.android.org.bouncycastle.asn1.ASN1Enumerated$1
 com.android.org.bouncycastle.asn1.ASN1Enumerated
 com.android.org.bouncycastle.asn1.ASN1Exception
+com.android.org.bouncycastle.asn1.ASN1External$1
 com.android.org.bouncycastle.asn1.ASN1External
+com.android.org.bouncycastle.asn1.ASN1GeneralString$1
+com.android.org.bouncycastle.asn1.ASN1GeneralString
+com.android.org.bouncycastle.asn1.ASN1GeneralizedTime$1
 com.android.org.bouncycastle.asn1.ASN1GeneralizedTime
+com.android.org.bouncycastle.asn1.ASN1GraphicString$1
+com.android.org.bouncycastle.asn1.ASN1GraphicString
+com.android.org.bouncycastle.asn1.ASN1IA5String$1
+com.android.org.bouncycastle.asn1.ASN1IA5String
 com.android.org.bouncycastle.asn1.ASN1InputStream
+com.android.org.bouncycastle.asn1.ASN1Integer$1
 com.android.org.bouncycastle.asn1.ASN1Integer
+com.android.org.bouncycastle.asn1.ASN1Null$1
 com.android.org.bouncycastle.asn1.ASN1Null
+com.android.org.bouncycastle.asn1.ASN1NumericString$1
+com.android.org.bouncycastle.asn1.ASN1NumericString
 com.android.org.bouncycastle.asn1.ASN1Object
+com.android.org.bouncycastle.asn1.ASN1ObjectDescriptor$1
+com.android.org.bouncycastle.asn1.ASN1ObjectDescriptor
+com.android.org.bouncycastle.asn1.ASN1ObjectIdentifier$1
 com.android.org.bouncycastle.asn1.ASN1ObjectIdentifier$OidHandle
 com.android.org.bouncycastle.asn1.ASN1ObjectIdentifier
+com.android.org.bouncycastle.asn1.ASN1OctetString$1
 com.android.org.bouncycastle.asn1.ASN1OctetString
 com.android.org.bouncycastle.asn1.ASN1OctetStringParser
 com.android.org.bouncycastle.asn1.ASN1OutputStream
 com.android.org.bouncycastle.asn1.ASN1Primitive
+com.android.org.bouncycastle.asn1.ASN1PrintableString$1
+com.android.org.bouncycastle.asn1.ASN1PrintableString
+com.android.org.bouncycastle.asn1.ASN1RelativeOID$1
+com.android.org.bouncycastle.asn1.ASN1RelativeOID
 com.android.org.bouncycastle.asn1.ASN1Sequence$1
+com.android.org.bouncycastle.asn1.ASN1Sequence$2
 com.android.org.bouncycastle.asn1.ASN1Sequence
 com.android.org.bouncycastle.asn1.ASN1SequenceParser
 com.android.org.bouncycastle.asn1.ASN1Set$1
@@ -266,19 +291,35 @@ com.android.org.bouncycastle.asn1.ASN1Set
 com.android.org.bouncycastle.asn1.ASN1SetParser
 com.android.org.bouncycastle.asn1.ASN1StreamParser
 com.android.org.bouncycastle.asn1.ASN1String
+com.android.org.bouncycastle.asn1.ASN1T61String$1
+com.android.org.bouncycastle.asn1.ASN1T61String
+com.android.org.bouncycastle.asn1.ASN1Tag
 com.android.org.bouncycastle.asn1.ASN1TaggedObject
 com.android.org.bouncycastle.asn1.ASN1TaggedObjectParser
+com.android.org.bouncycastle.asn1.ASN1Type
+com.android.org.bouncycastle.asn1.ASN1UTCTime$1
 com.android.org.bouncycastle.asn1.ASN1UTCTime
-com.android.org.bouncycastle.asn1.BERApplicationSpecific
-com.android.org.bouncycastle.asn1.BERApplicationSpecificParser
+com.android.org.bouncycastle.asn1.ASN1UTF8String$1
+com.android.org.bouncycastle.asn1.ASN1UTF8String
+com.android.org.bouncycastle.asn1.ASN1UniversalString$1
+com.android.org.bouncycastle.asn1.ASN1UniversalString
+com.android.org.bouncycastle.asn1.ASN1UniversalType
+com.android.org.bouncycastle.asn1.ASN1VideotexString$1
+com.android.org.bouncycastle.asn1.ASN1VideotexString
+com.android.org.bouncycastle.asn1.ASN1VisibleString$1
+com.android.org.bouncycastle.asn1.ASN1VisibleString
+com.android.org.bouncycastle.asn1.BERBitString
+com.android.org.bouncycastle.asn1.BERBitStringParser
+com.android.org.bouncycastle.asn1.BERFactory
 com.android.org.bouncycastle.asn1.BEROctetString
 com.android.org.bouncycastle.asn1.BEROctetStringParser
 com.android.org.bouncycastle.asn1.BERSequence
 com.android.org.bouncycastle.asn1.BERSequenceParser
 com.android.org.bouncycastle.asn1.BERSet
 com.android.org.bouncycastle.asn1.BERSetParser
+com.android.org.bouncycastle.asn1.BERTaggedObject
 com.android.org.bouncycastle.asn1.BERTaggedObjectParser
-com.android.org.bouncycastle.asn1.BERTags
+com.android.org.bouncycastle.asn1.ConstructedBitStream
 com.android.org.bouncycastle.asn1.ConstructedOctetStream
 com.android.org.bouncycastle.asn1.DERBMPString
 com.android.org.bouncycastle.asn1.DERBitString
@@ -299,7 +340,6 @@ com.android.org.bouncycastle.asn1.DERUTF8String
 com.android.org.bouncycastle.asn1.DERUniversalString
 com.android.org.bouncycastle.asn1.DERVideotexString
 com.android.org.bouncycastle.asn1.DERVisibleString
-com.android.org.bouncycastle.asn1.DLApplicationSpecific
 com.android.org.bouncycastle.asn1.DLBitString
 com.android.org.bouncycastle.asn1.DLExternal
 com.android.org.bouncycastle.asn1.DLFactory
@@ -334,18 +374,31 @@ com.android.org.bouncycastle.crypto.BlockCipher
 com.android.org.bouncycastle.crypto.BufferedBlockCipher
 com.android.org.bouncycastle.crypto.CipherParameters
 com.android.org.bouncycastle.crypto.CryptoException
+com.android.org.bouncycastle.crypto.CryptoServiceConstraintsException
+com.android.org.bouncycastle.crypto.CryptoServiceProperties
+com.android.org.bouncycastle.crypto.CryptoServicePurpose
+com.android.org.bouncycastle.crypto.CryptoServicesConstraints
 com.android.org.bouncycastle.crypto.CryptoServicesPermission
+com.android.org.bouncycastle.crypto.CryptoServicesRegistrar$$ExternalSyntheticBackportWithForwarding0
+com.android.org.bouncycastle.crypto.CryptoServicesRegistrar$1
 com.android.org.bouncycastle.crypto.CryptoServicesRegistrar$Property
+com.android.org.bouncycastle.crypto.CryptoServicesRegistrar$ThreadLocalSecureRandomProvider
 com.android.org.bouncycastle.crypto.CryptoServicesRegistrar
 com.android.org.bouncycastle.crypto.DataLengthException
+com.android.org.bouncycastle.crypto.DefaultBufferedBlockCipher
+com.android.org.bouncycastle.crypto.DefaultMultiBlockCipher
 com.android.org.bouncycastle.crypto.Digest
 com.android.org.bouncycastle.crypto.ExtendedDigest
 com.android.org.bouncycastle.crypto.InvalidCipherTextException
 com.android.org.bouncycastle.crypto.Mac
+com.android.org.bouncycastle.crypto.MultiBlockCipher
 com.android.org.bouncycastle.crypto.OutputLengthException
 com.android.org.bouncycastle.crypto.PBEParametersGenerator
 com.android.org.bouncycastle.crypto.RuntimeCryptoException
+com.android.org.bouncycastle.crypto.SavableDigest
+com.android.org.bouncycastle.crypto.SecureRandomProvider
 com.android.org.bouncycastle.crypto.Wrapper
+com.android.org.bouncycastle.crypto.constraints.DefaultServiceProperties
 com.android.org.bouncycastle.crypto.digests.AndroidDigestFactory
 com.android.org.bouncycastle.crypto.digests.AndroidDigestFactoryBouncyCastle
 com.android.org.bouncycastle.crypto.digests.AndroidDigestFactoryInterface
@@ -367,7 +420,9 @@ com.android.org.bouncycastle.crypto.digests.SHA256Digest
 com.android.org.bouncycastle.crypto.digests.SHA384Digest
 com.android.org.bouncycastle.crypto.digests.SHA512Digest
 com.android.org.bouncycastle.crypto.engines.AESEngine
+com.android.org.bouncycastle.crypto.engines.DESBase
 com.android.org.bouncycastle.crypto.engines.DESEngine
+com.android.org.bouncycastle.crypto.engines.Utils
 com.android.org.bouncycastle.crypto.generators.OpenSSLPBEParametersGenerator
 com.android.org.bouncycastle.crypto.generators.PKCS12ParametersGenerator
 com.android.org.bouncycastle.crypto.generators.PKCS5S1ParametersGenerator
@@ -377,6 +432,7 @@ com.android.org.bouncycastle.crypto.macs.HMac
 com.android.org.bouncycastle.crypto.modes.AEADBlockCipher
 com.android.org.bouncycastle.crypto.modes.AEADCipher
 com.android.org.bouncycastle.crypto.modes.CBCBlockCipher
+com.android.org.bouncycastle.crypto.modes.CBCModeCipher
 com.android.org.bouncycastle.crypto.paddings.BlockCipherPadding
 com.android.org.bouncycastle.crypto.paddings.PKCS7Padding
 com.android.org.bouncycastle.crypto.paddings.PaddedBufferedBlockCipher
@@ -391,16 +447,12 @@ com.android.org.bouncycastle.crypto.params.DSAValidationParameters
 com.android.org.bouncycastle.crypto.params.KeyParameter
 com.android.org.bouncycastle.crypto.params.ParametersWithIV
 com.android.org.bouncycastle.crypto.params.ParametersWithRandom
-com.android.org.bouncycastle.jcajce.PBKDFKey
 com.android.org.bouncycastle.jcajce.PKCS12Key
-com.android.org.bouncycastle.jcajce.PKCS12KeyWithParameters
 com.android.org.bouncycastle.jcajce.provider.asymmetric.DH$Mappings
 com.android.org.bouncycastle.jcajce.provider.asymmetric.DH
 com.android.org.bouncycastle.jcajce.provider.asymmetric.DSA$Mappings
-com.android.org.bouncycastle.jcajce.provider.asymmetric.EC$Mappings
 com.android.org.bouncycastle.jcajce.provider.asymmetric.RSA$Mappings
 com.android.org.bouncycastle.jcajce.provider.asymmetric.RSA
-com.android.org.bouncycastle.jcajce.provider.asymmetric.X509$Mappings
 com.android.org.bouncycastle.jcajce.provider.asymmetric.dh.KeyFactorySpi
 com.android.org.bouncycastle.jcajce.provider.asymmetric.dsa.BCDSAPublicKey
 com.android.org.bouncycastle.jcajce.provider.asymmetric.dsa.DSAUtil
@@ -419,8 +471,6 @@ com.android.org.bouncycastle.jcajce.provider.config.ConfigurableProvider
 com.android.org.bouncycastle.jcajce.provider.config.ProviderConfiguration
 com.android.org.bouncycastle.jcajce.provider.config.ProviderConfigurationPermission
 com.android.org.bouncycastle.jcajce.provider.digest.DigestAlgorithmProvider
-com.android.org.bouncycastle.jcajce.provider.digest.MD5$Mappings
-com.android.org.bouncycastle.jcajce.provider.digest.MD5
 com.android.org.bouncycastle.jcajce.provider.digest.SHA1$Mappings
 com.android.org.bouncycastle.jcajce.provider.digest.SHA1
 com.android.org.bouncycastle.jcajce.provider.digest.SHA224$Mappings
@@ -475,6 +525,7 @@ com.android.org.bouncycastle.jcajce.provider.symmetric.util.BlockCipherProvider
 com.android.org.bouncycastle.jcajce.provider.symmetric.util.ClassUtil$1
 com.android.org.bouncycastle.jcajce.provider.symmetric.util.ClassUtil
 com.android.org.bouncycastle.jcajce.provider.symmetric.util.GcmSpecUtil$2
+com.android.org.bouncycastle.jcajce.provider.symmetric.util.GcmSpecUtil$3
 com.android.org.bouncycastle.jcajce.provider.symmetric.util.GcmSpecUtil
 com.android.org.bouncycastle.jcajce.provider.symmetric.util.PBE$Util
 com.android.org.bouncycastle.jcajce.provider.symmetric.util.PBE
@@ -482,16 +533,15 @@ com.android.org.bouncycastle.jcajce.provider.util.AlgorithmProvider
 com.android.org.bouncycastle.jcajce.provider.util.AsymmetricAlgorithmProvider
 com.android.org.bouncycastle.jcajce.provider.util.AsymmetricKeyInfoConverter
 com.android.org.bouncycastle.jcajce.provider.util.DigestFactory
-com.android.org.bouncycastle.jcajce.spec.AEADParameterSpec
 com.android.org.bouncycastle.jcajce.spec.PBKDF2KeySpec
 com.android.org.bouncycastle.jcajce.util.BCJcaJceHelper
 com.android.org.bouncycastle.jcajce.util.DefaultJcaJceHelper
 com.android.org.bouncycastle.jcajce.util.JcaJceHelper
 com.android.org.bouncycastle.jcajce.util.ProviderJcaJceHelper
 com.android.org.bouncycastle.jce.X509Principal
-com.android.org.bouncycastle.jce.interfaces.BCKeyStore
 com.android.org.bouncycastle.jce.interfaces.PKCS12BagAttributeCarrier
 com.android.org.bouncycastle.jce.provider.BouncyCastleProvider$1
+com.android.org.bouncycastle.jce.provider.BouncyCastleProvider$JcaCryptoService
 com.android.org.bouncycastle.jce.provider.BouncyCastleProvider$PrivateProvider
 com.android.org.bouncycastle.jce.provider.BouncyCastleProvider
 com.android.org.bouncycastle.jce.provider.BouncyCastleProviderConfiguration
@@ -499,9 +549,7 @@ com.android.org.bouncycastle.jce.provider.CertStoreCollectionSpi
 com.android.org.bouncycastle.jce.provider.X509CRLObject
 com.android.org.bouncycastle.util.Arrays
 com.android.org.bouncycastle.util.BigIntegers
-com.android.org.bouncycastle.util.Encodable
 com.android.org.bouncycastle.util.Integers
-com.android.org.bouncycastle.util.Iterable
 com.android.org.bouncycastle.util.Memoable
 com.android.org.bouncycastle.util.Pack
 com.android.org.bouncycastle.util.Properties$1
@@ -558,6 +606,10 @@ dalvik.system.VMRuntime$HiddenApiUsageLogger
 dalvik.system.VMRuntime$SdkVersionContainer
 dalvik.system.VMRuntime
 dalvik.system.VMStack
+dalvik.system.VirtualThreadContext
+dalvik.system.VirtualThreadFrame
+dalvik.system.VirtualThreadParkedStates
+dalvik.system.VirtualThreadParkingError
 dalvik.system.ZipPathValidator$1
 dalvik.system.ZipPathValidator$Callback
 dalvik.system.ZipPathValidator
@@ -725,7 +777,10 @@ java.lang.ClassLoader$$ExternalSyntheticLambda2
 java.lang.ClassLoader$SystemClassLoader
 java.lang.ClassLoader
 java.lang.ClassNotFoundException
+java.lang.ClassValue$ClassValueMap
 java.lang.ClassValue$Entry
+java.lang.ClassValue$Identity
+java.lang.ClassValue$Version
 java.lang.ClassValue
 java.lang.CloneNotSupportedException
 java.lang.Cloneable
@@ -765,6 +820,7 @@ java.lang.Integer
 java.lang.InternalError
 java.lang.InterruptedException
 java.lang.Iterable
+java.lang.JavaLangAccess
 java.lang.LinkageError
 java.lang.Long$LongCache
 java.lang.Long
@@ -838,11 +894,13 @@ java.lang.StringUTF16
 java.lang.System$PropertiesWithNonOverrideableDefaults
 java.lang.System
 java.lang.Thread$1
+java.lang.Thread$AllThreadsRecord
 java.lang.Thread$Caches
 java.lang.Thread$State
 java.lang.Thread$ThreadIdentifiers
 java.lang.Thread$UncaughtExceptionHandler
 java.lang.Thread$WeakClassKey
+java.lang.Thread-IA
 java.lang.Thread
 java.lang.ThreadDeath
 java.lang.ThreadGroup
@@ -902,6 +960,9 @@ java.lang.invoke.ByteArrayViewVarHandle
 java.lang.invoke.ByteBufferViewVarHandle
 java.lang.invoke.CallSite
 java.lang.invoke.ConstantCallSite
+java.lang.invoke.DirectMethodHandle$Holder
+java.lang.invoke.DirectMethodHandle$MethodKey
+java.lang.invoke.DirectMethodHandle
 java.lang.invoke.FieldVarHandle
 java.lang.invoke.MethodHandle
 java.lang.invoke.MethodHandleImpl$HandleInfo
@@ -943,7 +1004,6 @@ java.lang.invoke.SerializedLambda
 java.lang.invoke.StaticFieldVarHandle
 java.lang.invoke.Transformers$AlwaysThrow
 java.lang.invoke.Transformers$ArrayConstructor
-java.lang.invoke.Transformers$ArrayLength
 java.lang.invoke.Transformers$AsTypeAdapter
 java.lang.invoke.Transformers$BindTo
 java.lang.invoke.Transformers$CatchException
@@ -960,8 +1020,6 @@ java.lang.invoke.Transformers$InsertArguments
 java.lang.invoke.Transformers$Invoker
 java.lang.invoke.Transformers$Loop
 java.lang.invoke.Transformers$PermuteArguments
-java.lang.invoke.Transformers$ReferenceArrayElementGetter
-java.lang.invoke.Transformers$ReferenceArrayElementSetter
 java.lang.invoke.Transformers$ReferenceIdentity
 java.lang.invoke.Transformers$Spreader
 java.lang.invoke.Transformers$TableSwitch
@@ -1180,6 +1238,7 @@ java.nio.channels.ByteChannel
 java.nio.channels.CancelledKeyException
 java.nio.channels.Channel
 java.nio.channels.Channels$1
+java.nio.channels.Channels$ReadableByteChannelImpl
 java.nio.channels.Channels$WritableByteChannelImpl
 java.nio.channels.Channels
 java.nio.channels.ClosedByInterruptException
@@ -1230,7 +1289,9 @@ java.nio.charset.CoderResult$Cache
 java.nio.charset.CoderResult
 java.nio.charset.CodingErrorAction
 java.nio.charset.IllegalCharsetNameException
+java.nio.charset.MalformedInputException
 java.nio.charset.StandardCharsets
+java.nio.charset.UnmappableCharacterException
 java.nio.charset.UnsupportedCharsetException
 java.nio.charset.spi.CharsetProvider
 java.nio.file.AccessDeniedException
@@ -1243,8 +1304,6 @@ java.nio.file.DirectoryStream
 java.nio.file.FileAlreadyExistsException
 java.nio.file.FileSystem
 java.nio.file.FileSystemException
-java.nio.file.FileSystems$DefaultFileSystemHolder$1
-java.nio.file.FileSystems$DefaultFileSystemHolder
 java.nio.file.FileSystems
 java.nio.file.FileVisitResult
 java.nio.file.FileVisitor
@@ -1802,6 +1861,7 @@ java.util.ImmutableCollections$SetN$SetNIterator
 java.util.ImmutableCollections$SetN
 java.util.ImmutableCollections$SubList
 java.util.ImmutableCollections-IA
+java.util.ImmutableCollections
 java.util.InputMismatchException
 java.util.Iterator
 java.util.JumboEnumSet$EnumSetIterator
@@ -1821,6 +1881,7 @@ java.util.LinkedHashSet
 java.util.LinkedList$DescendingIterator
 java.util.LinkedList$ListItr
 java.util.LinkedList$Node
+java.util.LinkedList-IA
 java.util.LinkedList
 java.util.List
 java.util.ListIterator
@@ -1940,7 +2001,9 @@ java.util.TreeMap$NavigableSubMap$SubMapKeyIterator
 java.util.TreeMap$NavigableSubMap
 java.util.TreeMap$PrivateEntryIterator
 java.util.TreeMap$TreeMapEntry
+java.util.TreeMap$TreeMapSpliterator
 java.util.TreeMap$ValueIterator
+java.util.TreeMap$ValueSpliterator
 java.util.TreeMap$Values
 java.util.TreeMap
 java.util.TreeSet
@@ -2046,6 +2109,7 @@ java.util.concurrent.ConcurrentMap
 java.util.concurrent.ConcurrentNavigableMap
 java.util.concurrent.ConcurrentSkipListMap$Index
 java.util.concurrent.ConcurrentSkipListMap$Iter
+java.util.concurrent.ConcurrentSkipListMap$KeyIterator
 java.util.concurrent.ConcurrentSkipListMap$KeySet
 java.util.concurrent.ConcurrentSkipListMap$Node
 java.util.concurrent.ConcurrentSkipListMap$ValueIterator
@@ -2668,6 +2732,7 @@ jdk.internal.math.FormattedFloatingDecimal$1
 jdk.internal.math.FormattedFloatingDecimal$Form
 jdk.internal.math.FormattedFloatingDecimal
 jdk.internal.math.MathUtils
+jdk.internal.misc.CarrierThreadLocal
 jdk.internal.misc.TerminatingThreadLocal$1
 jdk.internal.misc.TerminatingThreadLocal
 jdk.internal.misc.Unsafe
@@ -2694,6 +2759,12 @@ jdk.internal.util.StaticProperty
 jdk.internal.util.StrongReferenceKey
 jdk.internal.util.WeakReferenceKey
 jdk.internal.util.random.RandomSupport
+jdk.internal.vm.SharedThreadContainer
+jdk.internal.vm.StackableScope
+jdk.internal.vm.ThreadContainer
+jdk.internal.vm.ThreadContainers$RootContainer$TrackingRootContainer
+jdk.internal.vm.ThreadContainers$RootContainer
+jdk.internal.vm.ThreadContainers
 libcore.content.type.MimeMap$$ExternalSyntheticLambda0
 libcore.content.type.MimeMap$Builder$Element
 libcore.content.type.MimeMap$Builder
@@ -2709,16 +2780,6 @@ libcore.icu.SimpleDateFormatData
 libcore.icu.TimeZoneNames$1
 libcore.icu.TimeZoneNames$ZoneStringsCache
 libcore.icu.TimeZoneNames
-libcore.internal.Java21LanguageFeatures$$ExternalSyntheticTypeSwitch10
-libcore.internal.Java21LanguageFeatures$$ExternalSyntheticTypeSwitch1
-libcore.internal.Java21LanguageFeatures$$ExternalSyntheticTypeSwitch2
-libcore.internal.Java21LanguageFeatures$$ExternalSyntheticTypeSwitch3
-libcore.internal.Java21LanguageFeatures$$ExternalSyntheticTypeSwitch4
-libcore.internal.Java21LanguageFeatures$$ExternalSyntheticTypeSwitch5
-libcore.internal.Java21LanguageFeatures$$ExternalSyntheticTypeSwitch6
-libcore.internal.Java21LanguageFeatures$$ExternalSyntheticTypeSwitch7
-libcore.internal.Java21LanguageFeatures$$ExternalSyntheticTypeSwitch8
-libcore.internal.Java21LanguageFeatures$$ExternalSyntheticTypeSwitch9
 libcore.internal.StringPool
 libcore.io.AsynchronousCloseMonitor
 libcore.io.BlockGuardOs
@@ -2730,6 +2791,9 @@ libcore.io.ForwardingOs
 libcore.io.IoBridge
 libcore.io.IoTracker$Mode
 libcore.io.IoTracker
+libcore.io.IoUtils$$ExternalSyntheticAutoCloseableDispatcher0
+libcore.io.IoUtils$$ExternalSyntheticAutoCloseableForwarder1
+libcore.io.IoUtils$$ExternalSyntheticThrowIAE2
 libcore.io.IoUtils$FileReader
 libcore.io.IoUtils
 libcore.io.Libcore
@@ -2772,6 +2836,7 @@ libcore.util.HexEncoding
 libcore.util.NativeAllocationRegistry$CleanerRunner
 libcore.util.NativeAllocationRegistry$CleanerThunk
 libcore.util.NativeAllocationRegistry$Metrics
+libcore.util.NativeAllocationRegistry-IA
 libcore.util.NativeAllocationRegistry
 libcore.util.Objects
 libcore.util.SneakyThrow
@@ -2953,7 +3018,6 @@ sun.nio.cs.ThreadLocalCoders$Cache
 sun.nio.cs.ThreadLocalCoders
 sun.nio.fs.AbstractBasicFileAttributeView
 sun.nio.fs.AbstractFileSystemProvider
-sun.nio.fs.AbstractPath
 sun.nio.fs.DefaultFileSystemProvider
 sun.nio.fs.DynamicFileAttributeView
 sun.nio.fs.FileOwnerAttributeViewImpl
@@ -2961,6 +3025,7 @@ sun.nio.fs.LinuxFileSystem
 sun.nio.fs.LinuxFileSystemProvider
 sun.nio.fs.NativeBuffer$Deallocator
 sun.nio.fs.NativeBuffer
+sun.nio.fs.NativeBuffers$1
 sun.nio.fs.NativeBuffers
 sun.nio.fs.UnixChannelFactory$1
 sun.nio.fs.UnixChannelFactory$Flags
@@ -3220,12 +3285,15 @@ sun.util.logging.PlatformLogger
 [Lcom.android.org.bouncycastle.asn1.ASN1ObjectIdentifier;
 [Lcom.android.org.bouncycastle.asn1.ASN1OctetString;
 [Lcom.android.org.bouncycastle.asn1.ASN1Primitive;
+[Lcom.android.org.bouncycastle.crypto.CryptoServiceProperties;
 [Lcom.android.org.bouncycastle.crypto.params.DHParameters;
 [Lcom.android.org.bouncycastle.crypto.params.DSAParameters;
 [Lcom.android.org.bouncycastle.jcajce.provider.asymmetric.x509.PEMUtil$Boundaries;
 [Lcom.android.org.kxml2.io.KXmlParser$ValueContext;
 [Ldalvik.system.DexPathList$Element;
 [Ldalvik.system.DexPathList$NativeLibraryElement;
+[Ldalvik.system.VirtualThreadFrame;
+[Ljava.io.Closeable;
 [Ljava.io.File$PathStatus;
 [Ljava.io.File;
 [Ljava.io.FileDescriptor;
@@ -3248,6 +3316,7 @@ sun.util.logging.PlatformLogger
 [Ljava.lang.Daemons$Daemon;
 [Ljava.lang.Double;
 [Ljava.lang.Enum;
+[Ljava.lang.Float;
 [Ljava.lang.Integer;
 [Ljava.lang.Iterable;
 [Ljava.lang.Long;
diff --git a/build/flags/Android.bp b/build/flags/Android.bp
index e0fbb1ca37..c1979725ea 100644
--- a/build/flags/Android.bp
+++ b/build/flags/Android.bp
@@ -36,6 +36,21 @@ aconfig_declarations {
     ],
 }
 
+// TODO: Remove this temporary measure of an extra .aconfig file for C++ only.
+//   When b/422986526 is implemented, or no exported flag from art-flags.aconfig is used in libcore,
+//     and all flags in art-native-flags.aconfig are fully launched, we can
+//     1) make "libcore-aconfig-flags-lib" into "force-read-only" mode,
+//     2) remove this aconfig_declarations and aconfig file,
+//     3) put all new read-write flags into art-flags.aconfig, and
+//     4) put all new exported flags into libcore.aconfig.
+aconfig_declarations {
+    name: "art-aconfig-native-flags",
+    package: "com.android.art.native.flags",
+    container: "com.android.art",
+    exportable: false,
+    srcs: ["art-native-flags.aconfig"],
+}
+
 cc_aconfig_library {
     name: "art-aconfig-flags-lib",
     aconfig_declarations: "art-aconfig-flags",
@@ -47,6 +62,17 @@ cc_aconfig_library {
     ],
 }
 
+cc_aconfig_library {
+    name: "art-aconfig-native-flags-lib",
+    aconfig_declarations: "art-aconfig-native-flags",
+    host_supported: true,
+    min_sdk_version: "31",
+    apex_available: [
+        "//apex_available:platform",
+        "//apex_available:anyapex",
+    ],
+}
+
 java_aconfig_library {
     name: "art-aconfig-flags-java-lib",
     aconfig_declarations: "art-aconfig-flags",
@@ -59,14 +85,19 @@ java_aconfig_library {
         "//apex_available:platform",
         "//apex_available:anyapex",
     ],
+    visibility: [
+        "//art:__subpackages__",
+        "//cts/tests/libcore/vmdebug:__subpackages__",
+        "//libcore:__subpackages__",
+    ],
 }
 
 java_aconfig_library {
     name: "art-aconfig-flags-java-lib-host",
     host_supported: true,
+    device_supported: false,
     aconfig_declarations: "art-aconfig-flags",
     visibility: [
         "//cts/hostsidetests/compilation:__subpackages__",
-        "//cts/tests/libcore/vmdebug:__subpackages__",
     ],
 }
diff --git a/build/flags/art-flags.aconfig b/build/flags/art-flags.aconfig
index 919c6fa97d..15b1dc0ebc 100644
--- a/build/flags/art-flags.aconfig
+++ b/build/flags/art-flags.aconfig
@@ -15,11 +15,24 @@
 package: "com.android.art.flags"
 container: "com.android.art"
 
+# Flag for using userfaultfd move ioctl in CMC
+flag {
+  name: "use_uffd_move_ioctl"
+  namespace: "art_performance"
+  description: "Try to use userfaultfd's move ioctl in CMC GC. We may not be able to use it if"
+  "1. the kernel doesn't support it (<6.1), or"
+  "2. the necessary kernel bug-fixes aren't there, or"
+  "3. app's seccomp filter doesn't allow."
+  bug: "343220989"
+  is_fixed_read_only: true
+  is_exported: false
+}
+
 # Flag for generational CMC feature
 flag {
   name: "use_generational_cmc"
   namespace: "art_performance"
-  description: "Flag to control whether CMC's generational logic should be used or not"
+  description: "Use CMC's generational logic"
   bug: "343220989"
   is_fixed_read_only: true
   is_exported: false
@@ -35,6 +48,19 @@ flag {
   is_exported: true
 }
 
+# Flag for better spill slot reuse in the register allocator.
+flag {
+  name: "reg_alloc_spill_slot_reuse"
+  namespace: "art_performance"
+  description: "Flag to control spill slot reuse in register allocator"
+  bug: "264678256"
+  is_fixed_read_only: true
+  is_exported: false
+  metadata {
+      purpose: PURPOSE_BUGFIX
+  }
+}
+
 # TODO(b/352723620): Ramp this fully, then add tests with it.
 flag {
   name: "test"
@@ -54,6 +80,15 @@ flag {
   is_fixed_read_only: true
 }
 
+flag {
+  namespace: "system_performance"
+  name: "executable_method_file_offsets_deprecation"
+  is_exported: true
+  description: "Deprecate the original getExecutableMethodFileOffsets API which takes a Method"
+  bug: "400457896"
+  is_fixed_read_only: true
+}
+
 flag {
   namespace: "system_performance"
   name: "executable_method_file_offsets_v2"
@@ -89,3 +124,21 @@ flag {
   is_fixed_read_only: true
   is_exported: false
 }
+
+flag {
+  name: "compile_sdk_int_constant"
+  namespace: "system_performance"
+  description: "Flag to enable use of SDK_INT as a compile-time constant"
+  bug: "204924812"
+  is_fixed_read_only: true
+  is_exported: false
+}
+
+flag {
+  name: "enforce_permitted_subclasses_checks"
+  namespace: "core_libraries"
+  description: "Flag to enable enforcement of the sealed classes annotations"
+  bug: "416480439"
+  is_fixed_read_only: true
+  is_exported: false
+}
diff --git a/build/flags/art-native-flags.aconfig b/build/flags/art-native-flags.aconfig
new file mode 100644
index 0000000000..63226095d3
--- /dev/null
+++ b/build/flags/art-native-flags.aconfig
@@ -0,0 +1,28 @@
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+package: "com.android.art.native.flags"
+container: "com.android.art"
+
+# No flags here can be used in java. It should only contain read-write flags in this file.
+# Read-only flags should go into the art-flags.aconfig file instead.
+
+flag {
+  name: "test_rw"
+  namespace: "art_mainline"
+  description: "Permanent test read-write flag."
+  bug: "391379705"
+  is_fixed_read_only: false
+  is_exported: false
+}
diff --git a/build/go.work b/build/go.work
index f5f427ecf1..757e031e64 100644
--- a/build/go.work
+++ b/build/go.work
@@ -1,4 +1,4 @@
-go 1.19
+go 1.23
 
 use (
 	.
diff --git a/build/makevars.go b/build/makevars.go
index 00124ec621..e789c7d546 100644
--- a/build/makevars.go
+++ b/build/makevars.go
@@ -16,7 +16,6 @@ package art
 
 import (
 	"path/filepath"
-	"sort"
 	"strings"
 
 	"android/soong/android"
@@ -47,20 +46,28 @@ func makeVarsProvider(ctx android.MakeVarsContext) {
 	ctx.Strict("LIBART_IMG_HOST_BASE_ADDRESS", ctx.Config().LibartImgHostBaseAddress())
 	ctx.Strict("LIBART_IMG_TARGET_BASE_ADDRESS", ctx.Config().LibartImgDeviceBaseAddress())
 
-	testMap := testMap(ctx.Config())
-	var testNames []string
-	for name := range testMap {
-		testNames = append(testNames, name)
-	}
-
-	sort.Strings(testNames)
+	testMap := make(map[string][]string)
+	testcasesContent := make(map[string]string)
+	ctx.VisitAllModuleProxies(func(m android.ModuleProxy) {
+		if provider, ok := android.OtherModuleProvider(ctx, m, testInstallInfoProvider); ok {
+			for k, v := range provider.Testcases {
+				if oldSrc, ok := testcasesContent[k]; ok {
+					ctx.ModuleErrorf(m, "Conflicting sources for %s: %s and %s", k, oldSrc, v)
+					return
+				}
+				testcasesContent[k] = v
+			}
+			for k, v := range provider.TestMap {
+				testMap[k] = append(testMap[k], v...)
+			}
+		}
+	})
 
-	for _, name := range testNames {
+	for _, name := range android.SortedKeys(testMap) {
 		ctx.Strict("ART_TEST_LIST_"+name, strings.Join(android.FirstUniqueStrings(testMap[name]), " "))
 	}
 
 	// Create list of copy commands to install the content of the testcases directory.
-	testcasesContent := testcasesContent(ctx.Config())
 	copy_cmds := []string{}
 	for _, key := range android.SortedKeys(testcasesContent) {
 		copy_cmds = append(copy_cmds, testcasesContent[key]+":"+key)
diff --git a/cmdline/cmdline_types.h b/cmdline/cmdline_types.h
index 524f181a8e..14a1f89060 100644
--- a/cmdline/cmdline_types.h
+++ b/cmdline/cmdline_types.h
@@ -550,7 +550,7 @@ struct XGcOption {
   // These defaults are used when the command line arguments for -Xgc:
   // are either omitted completely or partially.
   gc::CollectorType collector_type_ = gc::kCollectorTypeDefault;
-  bool verify_pre_gc_heap_ = kIsDebugBuild;
+  bool verify_pre_gc_heap_ = false;
   bool verify_pre_sweeping_heap_ = kIsDebugBuild;
   bool generational_gc = kEnableGenerationalGCByDefault;
   bool verify_post_gc_heap_ = kIsDebugBuild;
diff --git a/compiler/Android.bp b/compiler/Android.bp
index 73994f7d95..93915547ff 100644
--- a/compiler/Android.bp
+++ b/compiler/Android.bp
@@ -136,6 +136,7 @@ art_cc_defaults {
     srcs: [
         "debug/elf_debug_writer.cc",
         "dex/inline_method_analyser.cc",
+        "driver/assume_value_options.cc",
         "driver/compiler_options.cc",
         "driver/dex_compilation_unit.cc",
         "jit/jit_compiler.cc",
@@ -165,6 +166,7 @@ art_cc_defaults {
         "optimizing/induction_var_range.cc",
         "optimizing/inliner.cc",
         "optimizing/instruction_builder.cc",
+        "optimizing/instruction_list.cc",
         "optimizing/instruction_simplifier.cc",
         "optimizing/intrinsic_objects.cc",
         "optimizing/intrinsics.cc",
@@ -174,6 +176,7 @@ art_cc_defaults {
         "optimizing/load_store_elimination.cc",
         "optimizing/locations.cc",
         "optimizing/loop_analysis.cc",
+        "optimizing/loop_information.cc",
         "optimizing/loop_optimization.cc",
         "optimizing/nodes.cc",
         "optimizing/nodes_vector.cc",
@@ -402,36 +405,58 @@ cc_library_headers {
     ],
 }
 
-// Properties common to `libart-compiler-gtest` and `libartd-compiler-gtest`.
-art_cc_defaults {
-    name: "libart-compiler-gtest-common",
+cc_defaults {
+    name: "libart-compiler-gtest-defaults",
+    defaults: ["libart-gtest-defaults"],
     srcs: [
         "common_compiler_test.cc",
     ],
     header_libs: [
         "libart_headers",
+        "libnativehelper_header_only",
+    ],
+    shared_libs: [ // Since we're building static libs we're only using the headers from these.
+        "libdexfile",
     ],
 }
 
-art_cc_library_static {
+cc_library_static {
     name: "libart-compiler-gtest",
     defaults: [
-        "libart-gtest-defaults",
-        "libart-compiler-gtest-common",
+        "libart-compiler-gtest-defaults",
+    ],
+}
+
+cc_defaults {
+    name: "libart-compiler-gtest_static_defaults",
+    defaults: [
         "libart-compiler-for-test_static_defaults",
+        "libdexfile_static_defaults",
+    ],
+    whole_static_libs: [
+        "libart-compiler-gtest",
     ],
 }
 
-art_cc_library_static {
+cc_library_static {
     name: "libartd-compiler-gtest",
     defaults: [
         "art_debug_defaults",
-        "libart-gtest-defaults",
-        "libart-compiler-gtest-common",
-        // Note that `libartd-compiler-for-test` is not required here, because
-        // `libartd-compiler` doesn't use LTO.
+        "libart-compiler-gtest-defaults",
+    ],
+}
+
+cc_defaults {
+    name: "libartd-compiler-gtest_static_defaults",
+    defaults: [
+        "libdexfiled_static_defaults",
+        // Note that something like `libartd-compiler-for-test_static_defaults`
+        // is not required here, because `libartd-compiler` doesn't use LTO.
         "libartd-compiler_static_defaults",
     ],
+    whole_static_libs: [
+        "libartd-compiler-gtest",
+    ],
 }
 
 art_cc_defaults {
@@ -537,9 +562,6 @@ art_cc_defaults {
 
     target: {
         host: {
-            shared_libs: [
-                "libartd-simulator",
-            ],
             required: ["art_boot_images"],
         },
     },
@@ -552,9 +574,7 @@ art_cc_test {
     defaults: [
         "art_gtest_defaults",
         "art_compiler_tests_defaults",
-    ],
-    static_libs: [
-        "libartd-simulator-container",
+        "libartd-simulator-for-test_static_defaults",
     ],
 }
 
@@ -564,11 +584,9 @@ art_cc_test {
     defaults: [
         "art_standalone_gtest_defaults",
         "art_compiler_tests_defaults",
+        "libart-simulator-for-test_static_defaults",
     ],
     data: [":generate-boot-image"],
-    static_libs: [
-        "libart-simulator-container",
-    ],
     test_config: "art_standalone_compiler_tests.xml",
 }
 
diff --git a/compiler/cfi_test.h b/compiler/cfi_test.h
index 5010b5cbf6..d02d897511 100644
--- a/compiler/cfi_test.h
+++ b/compiler/cfi_test.h
@@ -21,6 +21,7 @@
 #include <sstream>
 #include <vector>
 
+#include "android-base/parseint.h"
 #include "arch/instruction_set.h"
 #include "base/macros.h"
 #include "base/pointer_size.h"
@@ -47,17 +48,19 @@ class CFITest : public dwarf::DwarfTest {
     HexDump(f, actual_cfi);
     fprintf(f, "\n};\n");
     // Pretty-print CFI opcodes.
-    constexpr bool is64bit = false;
+    bool is64bit = Is64BitInstructionSet(isa);
     dwarf::DebugFrameOpCodeWriter<> initial_opcodes;
+    // Output a DW_CFA_def_cfa (reg + offset) in the CIE so that llvm-dwarfdump
+    // doesn't complain about DW_CFA_cfa_offset in the FDE.
+    initial_opcodes.DefCFA(dwarf::Reg(8), 0);
     dwarf::WriteCIE(is64bit, dwarf::Reg(8), initial_opcodes, &debug_frame_data_);
-    std::vector<uintptr_t> debug_frame_patches;
     dwarf::WriteFDE(is64bit,
                     /* cie_pointer= */ 0,
                     /* code_address= */ 0,
                     actual_asm.size(),
                     actual_cfi,
                     &debug_frame_data_);
-    ReformatCfi(Objdump(false, "-W"), &lines);
+    ReformatCfi(Objdump(isa, "--debug-frame"), &lines);
     // Pretty-print assembly.
     const uint8_t* asm_base = actual_asm.data();
     const uint8_t* asm_end = asm_base + actual_asm.size();
@@ -110,28 +113,20 @@ class CFITest : public dwarf::DwarfTest {
   // Find interesting parts of objdump output and prefix the lines with address.
   static void ReformatCfi(const std::vector<std::string>& lines,
                           std::vector<std::string>* output) {
-    std::string address;
+    uint32_t address = 0;
     for (const std::string& line : lines) {
       if (line.find("DW_CFA_nop") != std::string::npos) {
         // Ignore.
       } else if (line.find("DW_CFA_advance_loc") != std::string::npos) {
-        // The last 8 characters are the address.
-        address = "0x" + line.substr(line.size() - 8);
+        // Example line: "DW_CFA_advance_loc: 4 to 0x28"
+        size_t pos = line.rfind("0x");
+        ASSERT_NE(std::string::npos, pos);
+        bool success = android::base::ParseUint(line.substr(pos), &address);
+        ASSERT_TRUE(success);
       } else if (line.find("DW_CFA_") != std::string::npos) {
-        std::string new_line(line);
-        // "bad register" warning is caused by always using host (x86) objdump.
-        const char* bad_reg = "bad register: ";
-        size_t pos;
-        if ((pos = new_line.find(bad_reg)) != std::string::npos) {
-          new_line = new_line.replace(pos, strlen(bad_reg), "");
-        }
-        // Remove register names in parentheses since they have x86 names.
-        if ((pos = new_line.find(" (")) != std::string::npos) {
-          new_line = new_line.replace(pos, FindEndOf(new_line, ")") - pos, "");
-        }
         // Use the .cfi_ prefix.
-        new_line = ".cfi_" + new_line.substr(FindEndOf(new_line, "DW_CFA_"));
-        output->push_back(ART_FORMAT("{}: {}", address, new_line));
+        std::string new_line = ".cfi_" + line.substr(FindEndOf(line, "DW_CFA_"));
+        output->push_back(ART_FORMAT("0x{:08x}: {}", address, new_line));
       }
     }
   }
diff --git a/compiler/debug/dwarf/dwarf_test.cc b/compiler/debug/dwarf/dwarf_test.cc
index 87a94b48e3..7c2d204c76 100644
--- a/compiler/debug/dwarf/dwarf_test.cc
+++ b/compiler/debug/dwarf/dwarf_test.cc
@@ -30,7 +30,8 @@ namespace dwarf {
 #ifndef ART_TARGET_ANDROID
 
 TEST_F(DwarfTest, DebugFrame) {
-  const bool is64bit = false;
+  const InstructionSet isa = InstructionSet::kX86;
+  const bool is64bit = Is64BitInstructionSet(isa);
 
   // Pick offset value which would catch Uleb vs Sleb errors.
   const int offset = 40000;
@@ -44,7 +45,6 @@ TEST_F(DwarfTest, DebugFrame) {
   DW_CHECK(".debug_frame contents:");
   DW_CHECK("FDE");
   DW_CHECK_NEXT("DWARF32");
-  DW_CHECK_NEXT("DW_CFA_nop:");  // TODO: Why is a nop here.
   int pc = 0;
   for (int i : {0, 1, 0x3F, 0x40, 0xFF, 0x100, 0xFFFF, 0x10000}) {
     pc += i;
@@ -128,11 +128,12 @@ TEST_F(DwarfTest, DebugFrame) {
            ArrayRef<const uint8_t>(*opcodes.data()),
            &debug_frame_data_);
 
-  CheckObjdumpOutput(is64bit, "-debug-frame");
+  CheckObjdumpOutput(isa, "-debug-frame");
 }
 
 TEST_F(DwarfTest, DISABLED_DebugFrame64) {
-  constexpr bool is64bit = true;
+  const InstructionSet isa = InstructionSet::kX86_64;
+  const bool is64bit = Is64BitInstructionSet(isa);
   DebugFrameOpCodeWriter<> initial_opcodes;
   WriteCIE(is64bit, Reg(16), initial_opcodes, &debug_frame_data_);
   DebugFrameOpCodeWriter<> opcodes;
@@ -145,13 +146,14 @@ TEST_F(DwarfTest, DISABLED_DebugFrame64) {
            &debug_frame_data_);
   DW_CHECK("FDE cie=00000000 pc=100000000000000..300000000000000");
 
-  CheckObjdumpOutput(is64bit, "-debug-frame");
+  CheckObjdumpOutput(isa, "-debug-frame");
 }
 
 // Test x86_64 register mapping. It is the only non-trivial architecture.
 // ARM and X86 have: dwarf_reg = art_reg + constant.
 TEST_F(DwarfTest, x86_64_RegisterMapping) {
-  constexpr bool is64bit = true;
+  const InstructionSet isa = InstructionSet::kX86_64;
+  const bool is64bit = Is64BitInstructionSet(isa);
   DebugFrameOpCodeWriter<> opcodes;
   DW_CHECK(".debug_frame contents:");
   for (int i = 0; i < 16; i++) {
@@ -159,7 +161,6 @@ TEST_F(DwarfTest, x86_64_RegisterMapping) {
   }
   DW_CHECK("FDE");
   DW_CHECK_NEXT("DWARF32");
-  DW_CHECK_NEXT("DW_CFA_nop:");  // TODO: Why is a nop here.
   DW_CHECK_NEXT("DW_CFA_offset: RAX 0");
   DW_CHECK_NEXT("DW_CFA_offset: RCX 0");
   DW_CHECK_NEXT("DW_CFA_offset: RDX 0");
@@ -186,11 +187,12 @@ TEST_F(DwarfTest, x86_64_RegisterMapping) {
            ArrayRef<const uint8_t>(*opcodes.data()),
            &debug_frame_data_);
 
-  CheckObjdumpOutput(is64bit, "-debug-frame");
+  CheckObjdumpOutput(isa, "-debug-frame");
 }
 
 TEST_F(DwarfTest, DebugLine) {
-  const bool is64bit = false;
+  const InstructionSet isa = InstructionSet::kX86;
+  const bool is64bit = Is64BitInstructionSet(isa);
   const int code_factor_bits = 1;
   DebugLineOpCodeWriter<> opcodes(is64bit, code_factor_bits);
   DW_CHECK(".debug_line contents:");
@@ -241,12 +243,13 @@ TEST_F(DwarfTest, DebugLine) {
 
   WriteDebugLineTable(include_directories, files, opcodes, &debug_line_data_);
 
-  CheckObjdumpOutput(is64bit, "-debug-line");
+  CheckObjdumpOutput(isa, "-debug-line");
 }
 
 // DWARF has special one byte codes which advance PC and line at the same time.
 TEST_F(DwarfTest, DebugLineSpecialOpcodes) {
-  const bool is64bit = false;
+  const InstructionSet isa = InstructionSet::kX86;
+  const bool is64bit = Is64BitInstructionSet(isa);
   const int code_factor_bits = 1;
   uint32_t pc = 0x01000000;
   int line = 1;
@@ -278,11 +281,12 @@ TEST_F(DwarfTest, DebugLineSpecialOpcodes) {
   std::vector<FileEntry> files = { { "file.c", 0, 1000, 2000 } };
   WriteDebugLineTable(directories, files, opcodes, &debug_line_data_);
 
-  CheckObjdumpOutput(is64bit, "-debug-line");
+  CheckObjdumpOutput(isa, "-debug-line");
 }
 
 TEST_F(DwarfTest, DebugInfo) {
-  constexpr bool is64bit = false;
+  const InstructionSet isa = InstructionSet::kX86;
+  const bool is64bit = Is64BitInstructionSet(isa);
 
   DebugAbbrevWriter<> debug_abbrev(&debug_abbrev_data_);
   DW_CHECK(".debug_abbrev contents:");
@@ -334,7 +338,7 @@ TEST_F(DwarfTest, DebugInfo) {
 
   dwarf::WriteDebugInfoCU(/* debug_abbrev_offset= */ 0, info, &debug_info_data_);
 
-  CheckObjdumpOutput(is64bit, "-debug-info -debug-abbrev");
+  CheckObjdumpOutput(isa, "-debug-info -debug-abbrev");
 }
 
 #endif  // ART_TARGET_ANDROID
diff --git a/compiler/debug/dwarf/dwarf_test.h b/compiler/debug/dwarf/dwarf_test.h
index 1a0a798d74..93879c816f 100644
--- a/compiler/debug/dwarf/dwarf_test.h
+++ b/compiler/debug/dwarf/dwarf_test.h
@@ -59,10 +59,8 @@ class DwarfTest : public CommonCompilerTest {
 
   // Pretty-print the generated DWARF data using objdump.
   template<typename ElfTypes>
-  std::vector<std::string> Objdump(const char* args) {
+  std::vector<std::string> ObjdumpImpl(InstructionSet isa, const char* args) {
     // Write simple elf file with just the DWARF sections.
-    InstructionSet isa =
-        (sizeof(typename ElfTypes::Addr) == 8) ? InstructionSet::kX86_64 : InstructionSet::kX86;
     ScratchFile file;
     FileOutputStream output_stream(file.GetFile());
     ElfBuilder<ElfTypes> builder(isa, &output_stream);
@@ -111,17 +109,17 @@ class DwarfTest : public CommonCompilerTest {
     return lines;
   }
 
-  std::vector<std::string> Objdump(bool is64bit, const char* args) {
-    if (is64bit) {
-      return Objdump<ElfTypes64>(args);
+  std::vector<std::string> Objdump(InstructionSet isa, const char* args) {
+    if (Is64BitInstructionSet(isa)) {
+      return ObjdumpImpl<ElfTypes64>(isa, args);
     } else {
-      return Objdump<ElfTypes32>(args);
+      return ObjdumpImpl<ElfTypes32>(isa, args);
     }
   }
 
   // Compare objdump output to the recorded checks.
-  void CheckObjdumpOutput(bool is64bit, const char* args) {
-    std::vector<std::string> actual_lines = Objdump(is64bit, args);
+  void CheckObjdumpOutput(InstructionSet isa, const char* args) {
+    std::vector<std::string> actual_lines = Objdump(isa, args);
     auto actual_line = actual_lines.begin();
     bool failed = false;
     for (const ExpectedLine& expected : expected_lines_) {
diff --git a/compiler/driver/assume_value_options.cc b/compiler/driver/assume_value_options.cc
new file mode 100644
index 0000000000..d8f321df1e
--- /dev/null
+++ b/compiler/driver/assume_value_options.cc
@@ -0,0 +1,36 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "assume_value_options.h"
+
+#include "base/logging.h"
+#include "com_android_art_flags.h"
+
+namespace art_flags = com::android::art::flags;
+
+namespace art HIDDEN {
+
+bool AssumeValueOptions::MaybeSetAssumedValue(const detail::AssumeValueSignature& signature,
+                                              int32_t value) {
+  if (kSdkInt.Equals(signature)) {
+    DCHECK(art_flags::compile_sdk_int_constant());
+    sdk_int_ = value;
+    return true;
+  }
+  return false;
+}
+
+}  // namespace art
diff --git a/compiler/driver/assume_value_options.h b/compiler/driver/assume_value_options.h
new file mode 100644
index 0000000000..9a9669fc5e
--- /dev/null
+++ b/compiler/driver/assume_value_options.h
@@ -0,0 +1,82 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_COMPILER_DRIVER_ASSUME_VALUE_OPTIONS_H_
+#define ART_COMPILER_DRIVER_ASSUME_VALUE_OPTIONS_H_
+
+#include <cstdint>
+#include <ostream>
+#include <string>
+#include <string_view>
+
+#include "base/macros.h"
+#include "base/sdk_version.h"
+
+namespace art HIDDEN {
+
+class AssumeValueOptions;
+
+namespace detail {
+
+class AssumeValueSignature {
+ public:
+  bool Equals(const AssumeValueSignature& other) const {
+    return class_descriptor_ == other.class_descriptor_ && member_name_ == other.member_name_;
+  }
+
+ private:
+  friend class art::AssumeValueOptions;
+
+  constexpr AssumeValueSignature(std::string_view class_descriptor, std::string_view member_name)
+      : class_descriptor_(class_descriptor), member_name_(member_name) {}
+
+  const std::string_view class_descriptor_;
+  const std::string_view member_name_;
+};
+
+}  // namespace detail
+
+// A helper class containing configured values that can be safely assumed during compile time.
+class AssumeValueOptions final {
+ public:
+  static constexpr detail::AssumeValueSignature kSdkInt{"Landroid/os/Build$VERSION;", "SDK_INT"};
+
+  static constexpr uint32_t kUnsetSdkInt = static_cast<uint32_t>(SdkVersion::kUnset);
+
+  // The assumed Build.VERSION.SDK_INT value to use for compilation.
+  // Defaults to kUnsetSdkInt unless explicitly configured.
+  uint32_t SdkInt() const { return sdk_int_; }
+
+  // Whether a valid, explicit SDK_INT has been set for compilation.
+  bool HasValidSdkInt() const { return sdk_int_ != kUnsetSdkInt; }
+
+  // Sets the assumed value for a given member, if supported.
+  EXPORT bool MaybeSetAssumedValue(const detail::AssumeValueSignature& signature, int32_t value);
+
+  bool MaybeSetAssumedValue(std::string_view class_descriptor,
+                            std::string_view member_name,
+                            int32_t value) {
+    return MaybeSetAssumedValue(detail::AssumeValueSignature(class_descriptor, member_name), value);
+  }
+
+ private:
+  // The assumed android.os.Build.VERSION.SDK_INT value to use during compilation.
+  uint32_t sdk_int_ = kUnsetSdkInt;
+};
+
+}  // namespace art
+
+#endif  // ART_COMPILER_DRIVER_ASSUME_VALUE_OPTIONS_H_
diff --git a/compiler/driver/compiler_options.h b/compiler/driver/compiler_options.h
index a3957ce232..fde7aa676f 100644
--- a/compiler/driver/compiler_options.h
+++ b/compiler/driver/compiler_options.h
@@ -22,6 +22,7 @@
 #include <string>
 #include <vector>
 
+#include "assume_value_options.h"
 #include "base/compiler_filter.h"
 #include "base/globals.h"
 #include "base/hash_set.h"
@@ -383,6 +384,9 @@ class CompilerOptions final {
   // which at runtime we will need to dirty after initialization.
   EXPORT bool ShouldCompileWithClinitCheck(ArtMethod* method) const;
 
+  const AssumeValueOptions& GetAssumeValueOptions() const { return assume_value_options_; }
+  AssumeValueOptions& GetAssumeValueOptions() { return assume_value_options_; }
+
  private:
   EXPORT bool ParseDumpInitFailures(const std::string& option, std::string* error_msg);
 
@@ -486,6 +490,8 @@ class CompilerOptions final {
   // compiler-dependant behavior.
   const std::vector<std::string>* passes_to_run_;
 
+  AssumeValueOptions assume_value_options_;
+
   friend class Dex2Oat;
   friend class CommonCompilerDriverTest;
   friend class CommonCompilerTestImpl;
diff --git a/compiler/driver/compiler_options_map-inl.h b/compiler/driver/compiler_options_map-inl.h
index c11b387d1c..97397a579f 100644
--- a/compiler/driver/compiler_options_map-inl.h
+++ b/compiler/driver/compiler_options_map-inl.h
@@ -25,12 +25,61 @@
 #include "android-base/macros.h"
 #include "android-base/stringprintf.h"
 
+#include "assume_value_options.h"
 #include "base/macros.h"
 #include "cmdline_parser.h"
+#include "com_android_art_flags.h"
 #include "compiler_options.h"
 
 namespace art HIDDEN {
 
+template <>
+struct CmdlineType<AssumeValueOptions> : CmdlineTypeParser<AssumeValueOptions> {
+  Result Parse(const std::string& args) {
+    assert(false && "Use AppendValues() for an AssumeValueOptions type");
+    return Result::Failure("Unconditional failure: AssumeValueOptions must be appended: " + args);
+  }
+
+  Result ParseAndAppend(const std::string& args, AssumeValueOptions& assume_value_options) {
+    std::vector<std::string> args_parts;
+    Split(args, ':', &args_parts);
+    if (args_parts.size() != 2) {
+      return Result::Failure(std::string("Invalid --assume-value option: '") + args + "'");
+    }
+
+    static constexpr std::string_view kMemberDelimiter("->");
+    std::string_view signature(args_parts[0]);
+    size_t delimiter_pos = signature.find(kMemberDelimiter);
+    if (delimiter_pos == std::string::npos) {
+      return Result::Failure(std::string("Invalid --assume-value signature: '") + args + "'");
+    }
+    std::string_view class_descriptor = signature.substr(0, delimiter_pos);
+    std::string_view member_name = signature.substr(delimiter_pos + kMemberDelimiter.size());
+
+    // TODO(b/204924812): Make this generic on the predefined and/or provided type.
+    const auto parsed_value = ParseNumeric<uint32_t>(args_parts[1]);
+    if (!parsed_value.IsSuccess()) {
+      return Result::Failure(std::string("Invalid --assume-value value: '") + args + "'");
+    }
+
+    if (!com::android::art::flags::compile_sdk_int_constant()) {
+      // Feature disabled, silently ignore setting the value. Note that if we ever add additional
+      // support beyond for more assumed values beyond SDK_INT, this will need to be adjusted.
+      return Result::SuccessNoValue();
+    }
+
+    if (!assume_value_options.MaybeSetAssumedValue(
+            class_descriptor, member_name, parsed_value.GetValue())) {
+      return Result::Failure(std::string("Invalid --assume-value assignment: '") + args + "'");
+    }
+
+    return Result::SuccessNoValue();
+  }
+
+  static const char* Name() { return "AssumeValueOptions"; }
+  static const char* DescribeType() { return "Lfoo/bar/Baz;->field:value"; }
+};
+
 template <>
 struct CmdlineType<CompilerFilter::Filter> : CmdlineTypeParser<CompilerFilter::Filter> {
   Result Parse(const std::string& option) {
@@ -105,6 +154,8 @@ inline bool ReadCompilerOptions(Base& map, CompilerOptions* options, std::string
     options->dump_stats_ = true;
   }
 
+  map.AssignIfExists(Base::AssumeValueOpts, &options->assume_value_options_);
+
   return true;
 }
 
@@ -238,6 +289,14 @@ NO_INLINE void AddCompilerOptionsArgumentParserOptions(Builder& b) {
           .template WithType<unsigned int>()
           .WithHelp("Maximum solid block size for compressed images.")
           .IntoKey(Map::MaxImageBlockSize)
+
+      .Define("--assume-value=_")
+          .template WithType<AssumeValueOptions>()
+          .AppendValues()
+          .WithHelp("Optional assumed value for compiling a given field.\n"
+                    "E.g.: --assume-value=Landroid/os/Build$VERSION;->SDK_INT:23")
+          .IntoKey(Map::AssumeValueOpts)
+
       // Obsolete flags
       .Ignore({
         "--num-dex-methods=_",
diff --git a/compiler/driver/compiler_options_map.def b/compiler/driver/compiler_options_map.def
index 04e03634bd..43fe76deab 100644
--- a/compiler/driver/compiler_options_map.def
+++ b/compiler/driver/compiler_options_map.def
@@ -63,5 +63,6 @@ COMPILER_OPTIONS_KEY (Unit,                        DumpTimings)
 COMPILER_OPTIONS_KEY (Unit,                        DumpPassTimings)
 COMPILER_OPTIONS_KEY (Unit,                        DumpStats)
 COMPILER_OPTIONS_KEY (unsigned int,                MaxImageBlockSize)
+COMPILER_OPTIONS_KEY (AssumeValueOptions,          AssumeValueOpts)
 
 #undef COMPILER_OPTIONS_KEY
diff --git a/compiler/driver/compiler_options_map.h b/compiler/driver/compiler_options_map.h
index 136af36096..9ab051b0fc 100644
--- a/compiler/driver/compiler_options_map.h
+++ b/compiler/driver/compiler_options_map.h
@@ -20,6 +20,7 @@
 #include <string>
 #include <vector>
 
+#include "assume_value_options.h"
 #include "base/compiler_filter.h"
 #include "base/macros.h"
 #include "base/variant_map.h"
diff --git a/compiler/exception_test.cc b/compiler/exception_test.cc
index 4eae96158e..3de99d5f03 100644
--- a/compiler/exception_test.cc
+++ b/compiler/exception_test.cc
@@ -24,7 +24,7 @@
 #include "base/callee_save_type.h"
 #include "base/leb128.h"
 #include "base/macros.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "base/pointer_size.h"
 #include "class_linker.h"
 #include "common_runtime_test.h"
@@ -78,7 +78,7 @@ class ExceptionTest : public CommonRuntimeTest {
     const uint32_t native_pc_offset = 4u;
     CHECK_ALIGNED_PARAM(native_pc_offset, GetInstructionSetInstructionAlignment(kRuntimeISA));
 
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaStack arena_stack(&pool);
     ScopedArenaAllocator allocator(&arena_stack);
     StackMapStream stack_maps(&allocator, kRuntimeISA);
diff --git a/compiler/jit/jit_compiler.cc b/compiler/jit/jit_compiler.cc
index 4ae3cd1f88..4c81a29ae2 100644
--- a/compiler/jit/jit_compiler.cc
+++ b/compiler/jit/jit_compiler.cc
@@ -24,6 +24,7 @@
 #include "base/systrace.h"
 #include "base/time_utils.h"
 #include "base/timing_logger.h"
+#include "com_android_art_flags.h"
 #include "compiler.h"
 #include "debug/elf_debug_writer.h"
 #include "driver/compiler_options.h"
@@ -33,6 +34,8 @@
 #include "jit/jit_code_cache.h"
 #include "jit/jit_logger.h"
 
+namespace art_flags = com::android::art::flags;
+
 namespace art HIDDEN {
 namespace jit {
 
@@ -125,6 +128,11 @@ void JitCompiler::ParseCompilerOptions() {
     jit_logger_.reset(new JitLogger());
     jit_logger_->OpenLog();
   }
+
+  if (art_flags::compile_sdk_int_constant()) {
+    compiler_options_->GetAssumeValueOptions().MaybeSetAssumedValue(AssumeValueOptions::kSdkInt,
+                                                                    runtime->GetSdkVersion());
+  }
 }
 
 JitCompilerInterface* jit_create() {
diff --git a/compiler/jni/jni_cfi_test.cc b/compiler/jni/jni_cfi_test.cc
index 580f4b7a8a..141017845a 100644
--- a/compiler/jni/jni_cfi_test.cc
+++ b/compiler/jni/jni_cfi_test.cc
@@ -20,7 +20,7 @@
 #include "arch/instruction_set.h"
 #include "base/arena_allocator.h"
 #include "base/macros.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "base/pointer_size.h"
 #include "cfi_test.h"
 #include "gtest/gtest.h"
@@ -60,7 +60,7 @@ class JNICFITest : public CFITest {
     const bool is_synchronized = false;
     const char* shorty = "IIFII";
 
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
 
     std::unique_ptr<JniCallingConvention> jni_conv(
diff --git a/compiler/jni/jni_cfi_test_expected.inc b/compiler/jni/jni_cfi_test_expected.inc
index 47a67c72c0..3b5067f433 100644
--- a/compiler/jni/jni_cfi_test_expected.inc
+++ b/compiler/jni/jni_cfi_test_expected.inc
@@ -1,5 +1,7 @@
-// TODO These arrays should be generated automatically or have instructions for re-creation.
-// For now, the gc_is_marking offset can be adjusted by tweaking the last CL that made a
+// TODO These arrays should be generated automatically. To regenerate an array,
+// manually set JNICFITest::kGenerateExpected to true, and run:
+//     out/host/linux-x86/nativetest64/art_compiler_host_tests/art_compiler_host_tests '--gtest_filter=JNICFITest.*'
+// The gc_is_marking offset can be adjusted by tweaking the last CL that made a
 // similar change.
 
 static constexpr uint8_t expected_asm_kThumb2[] = {
@@ -21,78 +23,79 @@ static constexpr uint8_t expected_cfi_kThumb2[] = {
     0x5B, 0x06, 0x5C, 0x06, 0x5D, 0x06, 0x5E, 0x06, 0x5F, 0x44, 0x0E, 0x00,
     0xC5, 0xC6, 0xC7, 0xC8, 0xCA, 0xCB, 0xCE, 0x46, 0x0B, 0x0E, 0x60,
 };
+// 0x00000000: .cfi_def_cfa: R8 +0
 // 0x00000000: push {r5,r6,r7,r8,r10,r11,lr}
-// 0x00000004: .cfi_def_cfa_offset: 28
-// 0x00000004: .cfi_offset: r5 at cfa-28
-// 0x00000004: .cfi_offset: r6 at cfa-24
-// 0x00000004: .cfi_offset: r7 at cfa-20
-// 0x00000004: .cfi_offset: r8 at cfa-16
-// 0x00000004: .cfi_offset: r10 at cfa-12
-// 0x00000004: .cfi_offset: r11 at cfa-8
-// 0x00000004: .cfi_offset: r14 at cfa-4
+// 0x00000004: .cfi_def_cfa_offset: +28
+// 0x00000004: .cfi_offset: R5 -28
+// 0x00000004: .cfi_offset: R6 -24
+// 0x00000004: .cfi_offset: R7 -20
+// 0x00000004: .cfi_offset: R8 -16
+// 0x00000004: .cfi_offset: R10 -12
+// 0x00000004: .cfi_offset: R11 -8
+// 0x00000004: .cfi_offset: LR -4
 // 0x00000004: vpush {s16-s31}
-// 0x00000008: .cfi_def_cfa_offset: 92
-// 0x00000008: .cfi_offset_extended: r80 at cfa-92
-// 0x00000008: .cfi_offset_extended: r81 at cfa-88
-// 0x00000008: .cfi_offset_extended: r82 at cfa-84
-// 0x00000008: .cfi_offset_extended: r83 at cfa-80
-// 0x00000008: .cfi_offset_extended: r84 at cfa-76
-// 0x00000008: .cfi_offset_extended: r85 at cfa-72
-// 0x00000008: .cfi_offset_extended: r86 at cfa-68
-// 0x00000008: .cfi_offset_extended: r87 at cfa-64
-// 0x00000008: .cfi_offset_extended: r88 at cfa-60
-// 0x00000008: .cfi_offset_extended: r89 at cfa-56
-// 0x00000008: .cfi_offset_extended: r90 at cfa-52
-// 0x00000008: .cfi_offset_extended: r91 at cfa-48
-// 0x00000008: .cfi_offset_extended: r92 at cfa-44
-// 0x00000008: .cfi_offset_extended: r93 at cfa-40
-// 0x00000008: .cfi_offset_extended: r94 at cfa-36
-// 0x00000008: .cfi_offset_extended: r95 at cfa-32
+// 0x00000008: .cfi_def_cfa_offset: +92
+// 0x00000008: .cfi_offset_extended: reg80 -92
+// 0x00000008: .cfi_offset_extended: reg81 -88
+// 0x00000008: .cfi_offset_extended: reg82 -84
+// 0x00000008: .cfi_offset_extended: reg83 -80
+// 0x00000008: .cfi_offset_extended: reg84 -76
+// 0x00000008: .cfi_offset_extended: reg85 -72
+// 0x00000008: .cfi_offset_extended: reg86 -68
+// 0x00000008: .cfi_offset_extended: reg87 -64
+// 0x00000008: .cfi_offset_extended: reg88 -60
+// 0x00000008: .cfi_offset_extended: reg89 -56
+// 0x00000008: .cfi_offset_extended: reg90 -52
+// 0x00000008: .cfi_offset_extended: reg91 -48
+// 0x00000008: .cfi_offset_extended: reg92 -44
+// 0x00000008: .cfi_offset_extended: reg93 -40
+// 0x00000008: .cfi_offset_extended: reg94 -36
+// 0x00000008: .cfi_offset_extended: reg95 -32
 // 0x00000008: sub sp, #4
-// 0x0000000a: .cfi_def_cfa_offset: 96
+// 0x0000000a: .cfi_def_cfa_offset: +96
 // 0x0000000a: str r0, [sp]
 // 0x0000000c: str r1, [sp, #100]
 // 0x0000000e: vstr s0, [sp, #104]
 // 0x00000012: str r2, [sp, #108]
 // 0x00000014: str r3, [sp, #112]
 // 0x00000016: sub sp, #32
-// 0x00000018: .cfi_def_cfa_offset: 128
+// 0x00000018: .cfi_def_cfa_offset: +128
 // 0x00000018: add sp, #32
-// 0x0000001a: .cfi_def_cfa_offset: 96
-// 0x0000001a: .cfi_remember_state
+// 0x0000001a: .cfi_def_cfa_offset: +96
+// 0x0000001a: .cfi_remember_state:
 // 0x0000001a: add sp, #4
-// 0x0000001c: .cfi_def_cfa_offset: 92
+// 0x0000001c: .cfi_def_cfa_offset: +92
 // 0x0000001c: vpop {s16-s31}
-// 0x00000020: .cfi_def_cfa_offset: 28
-// 0x00000020: .cfi_restore_extended: r80
-// 0x00000020: .cfi_restore_extended: r81
-// 0x00000020: .cfi_restore_extended: r82
-// 0x00000020: .cfi_restore_extended: r83
-// 0x00000020: .cfi_restore_extended: r84
-// 0x00000020: .cfi_restore_extended: r85
-// 0x00000020: .cfi_restore_extended: r86
-// 0x00000020: .cfi_restore_extended: r87
-// 0x00000020: .cfi_restore_extended: r88
-// 0x00000020: .cfi_restore_extended: r89
-// 0x00000020: .cfi_restore_extended: r90
-// 0x00000020: .cfi_restore_extended: r91
-// 0x00000020: .cfi_restore_extended: r92
-// 0x00000020: .cfi_restore_extended: r93
-// 0x00000020: .cfi_restore_extended: r94
-// 0x00000020: .cfi_restore_extended: r95
+// 0x00000020: .cfi_def_cfa_offset: +28
+// 0x00000020: .cfi_restore_extended: reg80
+// 0x00000020: .cfi_restore_extended: reg81
+// 0x00000020: .cfi_restore_extended: reg82
+// 0x00000020: .cfi_restore_extended: reg83
+// 0x00000020: .cfi_restore_extended: reg84
+// 0x00000020: .cfi_restore_extended: reg85
+// 0x00000020: .cfi_restore_extended: reg86
+// 0x00000020: .cfi_restore_extended: reg87
+// 0x00000020: .cfi_restore_extended: reg88
+// 0x00000020: .cfi_restore_extended: reg89
+// 0x00000020: .cfi_restore_extended: reg90
+// 0x00000020: .cfi_restore_extended: reg91
+// 0x00000020: .cfi_restore_extended: reg92
+// 0x00000020: .cfi_restore_extended: reg93
+// 0x00000020: .cfi_restore_extended: reg94
+// 0x00000020: .cfi_restore_extended: reg95
 // 0x00000020: pop {r5,r6,r7,r8,r10,r11,lr}
-// 0x00000024: .cfi_def_cfa_offset: 0
-// 0x00000024: .cfi_restore: r5
-// 0x00000024: .cfi_restore: r6
-// 0x00000024: .cfi_restore: r7
-// 0x00000024: .cfi_restore: r8
-// 0x00000024: .cfi_restore: r10
-// 0x00000024: .cfi_restore: r11
-// 0x00000024: .cfi_restore: r14
+// 0x00000024: .cfi_def_cfa_offset: +0
+// 0x00000024: .cfi_restore: R5
+// 0x00000024: .cfi_restore: R6
+// 0x00000024: .cfi_restore: R7
+// 0x00000024: .cfi_restore: R8
+// 0x00000024: .cfi_restore: R10
+// 0x00000024: .cfi_restore: R11
+// 0x00000024: .cfi_restore: LR
 // 0x00000024: ldr r8, [tr, #32] ; is_gc_marking
 // 0x00000028: bx lr
-// 0x0000002a: .cfi_restore_state
-// 0x0000002a: .cfi_def_cfa_offset: 112
+// 0x0000002a: .cfi_restore_state:
+// 0x0000002a: .cfi_def_cfa_offset: +96
 
 static constexpr uint8_t expected_asm_kArm64[] = {
     0xFF, 0xC3, 0x02, 0xD1, 0xF3, 0x53, 0x05, 0xA9, 0xF5, 0x5B, 0x06, 0xA9,
@@ -119,84 +122,85 @@ static constexpr uint8_t expected_cfi_kArm64[] = {
     0x4A, 0x06, 0x4B, 0x44, 0x06, 0x4C, 0x06, 0x4D, 0x44, 0x06, 0x4E, 0x06,
     0x4F, 0x48, 0x0E, 0x00, 0x44, 0x0B, 0x0E, 0xB0, 0x01,
 };
+// 0x00000000: .cfi_def_cfa: W8 +0
 // 0x00000000: sub sp, sp, #0xb0 (176)
-// 0x00000004: .cfi_def_cfa_offset: 176
+// 0x00000004: .cfi_def_cfa_offset: +176
 // 0x00000004: stp tr, x20, [sp, #80]
-// 0x00000008: .cfi_offset: r19 at cfa-96
-// 0x00000008: .cfi_offset: r20 at cfa-88
+// 0x00000008: .cfi_offset: W19 -96
+// 0x00000008: .cfi_offset: W20 -88
 // 0x00000008: stp x21, x22, [sp, #96]
-// 0x0000000c: .cfi_offset: r21 at cfa-80
-// 0x0000000c: .cfi_offset: r22 at cfa-72
+// 0x0000000c: .cfi_offset: W21 -80
+// 0x0000000c: .cfi_offset: W22 -72
 // 0x0000000c: stp x23, x24, [sp, #112]
-// 0x00000010: .cfi_offset: r23 at cfa-64
-// 0x00000010: .cfi_offset: r24 at cfa-56
+// 0x00000010: .cfi_offset: W23 -64
+// 0x00000010: .cfi_offset: W24 -56
 // 0x00000010: stp x25, x26, [sp, #128]
-// 0x00000014: .cfi_offset: r25 at cfa-48
-// 0x00000014: .cfi_offset: r26 at cfa-40
+// 0x00000014: .cfi_offset: W25 -48
+// 0x00000014: .cfi_offset: W26 -40
 // 0x00000014: stp x27, x28, [sp, #144]
-// 0x00000018: .cfi_offset: r27 at cfa-32
-// 0x00000018: .cfi_offset: r28 at cfa-24
+// 0x00000018: .cfi_offset: W27 -32
+// 0x00000018: .cfi_offset: W28 -24
 // 0x00000018: stp x29, lr, [sp, #160]
-// 0x0000001c: .cfi_offset: r29 at cfa-16
-// 0x0000001c: .cfi_offset: r30 at cfa-8
+// 0x0000001c: .cfi_offset: W29 -16
+// 0x0000001c: .cfi_offset: W30 -8
 // 0x0000001c: stp d8, d9, [sp, #16]
-// 0x00000020: .cfi_offset_extended: r72 at cfa-160
-// 0x00000020: .cfi_offset_extended: r73 at cfa-152
+// 0x00000020: .cfi_offset_extended: B8 -160
+// 0x00000020: .cfi_offset_extended: B9 -152
 // 0x00000020: stp d10, d11, [sp, #32]
-// 0x00000024: .cfi_offset_extended: r74 at cfa-144
-// 0x00000024: .cfi_offset_extended: r75 at cfa-136
+// 0x00000024: .cfi_offset_extended: B10 -144
+// 0x00000024: .cfi_offset_extended: B11 -136
 // 0x00000024: stp d12, d13, [sp, #48]
-// 0x00000028: .cfi_offset_extended: r76 at cfa-128
-// 0x00000028: .cfi_offset_extended: r77 at cfa-120
+// 0x00000028: .cfi_offset_extended: B12 -128
+// 0x00000028: .cfi_offset_extended: B13 -120
 // 0x00000028: stp d14, d15, [sp, #64]
-// 0x0000002c: .cfi_offset_extended: r78 at cfa-112
-// 0x0000002c: .cfi_offset_extended: r79 at cfa-104
+// 0x0000002c: .cfi_offset_extended: B14 -112
+// 0x0000002c: .cfi_offset_extended: B15 -104
 // 0x0000002c: str x0, [sp]
 // 0x00000030: str w1, [sp, #184]
 // 0x00000034: str s0, [sp, #188]
 // 0x00000038: str w2, [sp, #192]
 // 0x0000003c: str w3, [sp, #196]
 // 0x00000040: sub sp, sp, #0x20 (32)
-// 0x00000044: .cfi_def_cfa_offset: 208
+// 0x00000044: .cfi_def_cfa_offset: +208
 // 0x00000044: add sp, sp, #0x20 (32)
-// 0x00000048: .cfi_def_cfa_offset: 176
-// 0x00000048: .cfi_remember_state
+// 0x00000048: .cfi_def_cfa_offset: +176
+// 0x00000048: .cfi_remember_state:
 // 0x00000048: ldp tr, x20, [sp, #80]
-// 0x0000004c: .cfi_restore: r19
-// 0x0000004c: .cfi_restore: r20
+// 0x0000004c: .cfi_restore: W19
+// 0x0000004c: .cfi_restore: W20
 // 0x0000004c: ldp x21, x22, [sp, #96]
-// 0x00000050: .cfi_restore: r21
-// 0x00000050: .cfi_restore: r22
+// 0x00000050: .cfi_restore: W21
+// 0x00000050: .cfi_restore: W22
 // 0x00000050: ldp x23, x24, [sp, #112]
-// 0x00000054: .cfi_restore: r23
-// 0x00000054: .cfi_restore: r24
+// 0x00000054: .cfi_restore: W23
+// 0x00000054: .cfi_restore: W24
 // 0x00000054: ldp x25, x26, [sp, #128]
-// 0x00000058: .cfi_restore: r25
-// 0x00000058: .cfi_restore: r26
+// 0x00000058: .cfi_restore: W25
+// 0x00000058: .cfi_restore: W26
 // 0x00000058: ldp x27, x28, [sp, #144]
-// 0x0000005c: .cfi_restore: r27
-// 0x0000005c: .cfi_restore: r28
+// 0x0000005c: .cfi_restore: W27
+// 0x0000005c: .cfi_restore: W28
 // 0x0000005c: ldp x29, lr, [sp, #160]
-// 0x00000060: .cfi_restore: r29
-// 0x00000060: .cfi_restore: r30
+// 0x00000060: .cfi_restore: W29
+// 0x00000060: .cfi_restore: W30
 // 0x00000060: ldp d8, d9, [sp, #16]
-// 0x00000064: .cfi_restore_extended: r72
-// 0x00000064: .cfi_restore_extended: r73
+// 0x00000064: .cfi_restore_extended: B8
+// 0x00000064: .cfi_restore_extended: B9
 // 0x00000064: ldp d10, d11, [sp, #32]
-// 0x00000068: .cfi_restore_extended: r74
-// 0x00000068: .cfi_restore_extended: r75
+// 0x00000068: .cfi_restore_extended: B10
+// 0x00000068: .cfi_restore_extended: B11
 // 0x00000068: ldp d12, d13, [sp, #48]
-// 0x0000006c: .cfi_restore_extended: r76
-// 0x0000006c: .cfi_restore_extended: r77
+// 0x0000006c: .cfi_restore_extended: B12
+// 0x0000006c: .cfi_restore_extended: B13
 // 0x0000006c: ldp d14, d15, [sp, #64]
-// 0x00000070: .cfi_restore_extended: r78
-// 0x00000070: .cfi_restore_extended: r79
+// 0x00000070: .cfi_restore_extended: B14
+// 0x00000070: .cfi_restore_extended: B15
 // 0x00000070: ldr w20, [tr, #32] ; is_gc_marking
 // 0x00000074: add sp, sp, #0xb0 (176)
-// 0x00000078: .cfi_def_cfa_offset: 0
+// 0x00000078: .cfi_def_cfa_offset: +0
 // 0x00000078: ret
-// 0x0000007c: .cfi_restore_state
-// 0x0000007c: .cfi_def_cfa_offset: 176
+// 0x0000007c: .cfi_restore_state:
+// 0x0000007c: .cfi_def_cfa_offset: +176
 
 static constexpr uint8_t expected_asm_kX86[] = {
     0x57, 0x56, 0x55, 0x83, 0xC4, 0xF4, 0x50, 0x89, 0x4C, 0x24, 0x24, 0xF3,
@@ -210,42 +214,43 @@ static constexpr uint8_t expected_cfi_kX86[] = {
     0x43, 0x0E, 0x20, 0x0A, 0x43, 0x0E, 0x10, 0x41, 0x0E, 0x0C, 0xC5, 0x41,
     0x0E, 0x08, 0xC6, 0x41, 0x0E, 0x04, 0xC7, 0x41, 0x0B, 0x0E, 0x20,
 };
+// 0x00000000: .cfi_def_cfa: EIP +0
 // 0x00000000: push edi
-// 0x00000001: .cfi_def_cfa_offset: 8
-// 0x00000001: .cfi_offset: r7 at cfa-8
+// 0x00000001: .cfi_def_cfa_offset: +8
+// 0x00000001: .cfi_offset: EDI -8
 // 0x00000001: push esi
-// 0x00000002: .cfi_def_cfa_offset: 12
-// 0x00000002: .cfi_offset: r6 at cfa-12
+// 0x00000002: .cfi_def_cfa_offset: +12
+// 0x00000002: .cfi_offset: ESI -12
 // 0x00000002: push ebp
-// 0x00000003: .cfi_def_cfa_offset: 16
-// 0x00000003: .cfi_offset: r5 at cfa-16
+// 0x00000003: .cfi_def_cfa_offset: +16
+// 0x00000003: .cfi_offset: EBP -16
 // 0x00000003: add esp, -12
-// 0x00000006: .cfi_def_cfa_offset: 28
+// 0x00000006: .cfi_def_cfa_offset: +28
 // 0x00000006: push eax
-// 0x00000007: .cfi_def_cfa_offset: 32
+// 0x00000007: .cfi_def_cfa_offset: +32
 // 0x00000007: mov [esp + 36], ecx
 // 0x0000000b: movss [esp + 40], xmm0
 // 0x00000011: mov [esp + 44], edx
 // 0x00000015: mov [esp + 48], ebx
 // 0x00000019: add esp, -32
-// 0x0000001c: .cfi_def_cfa_offset: 64
+// 0x0000001c: .cfi_def_cfa_offset: +64
 // 0x0000001c: add esp, 32
-// 0x0000001f: .cfi_def_cfa_offset: 32
-// 0x0000001f: .cfi_remember_state
+// 0x0000001f: .cfi_def_cfa_offset: +32
+// 0x0000001f: .cfi_remember_state:
 // 0x0000001f: add esp, 16
-// 0x00000022: .cfi_def_cfa_offset: 16
+// 0x00000022: .cfi_def_cfa_offset: +16
 // 0x00000022: pop ebp
-// 0x00000023: .cfi_def_cfa_offset: 12
-// 0x00000023: .cfi_restore: r5
+// 0x00000023: .cfi_def_cfa_offset: +12
+// 0x00000023: .cfi_restore: EBP
 // 0x00000023: pop esi
-// 0x00000024: .cfi_def_cfa_offset: 8
-// 0x00000024: .cfi_restore: r6
+// 0x00000024: .cfi_def_cfa_offset: +8
+// 0x00000024: .cfi_restore: ESI
 // 0x00000024: pop edi
-// 0x00000025: .cfi_def_cfa_offset: 4
-// 0x00000025: .cfi_restore: r7
+// 0x00000025: .cfi_def_cfa_offset: +4
+// 0x00000025: .cfi_restore: EDI
 // 0x00000025: ret
-// 0x00000026: .cfi_restore_state
-// 0x00000026: .cfi_def_cfa_offset: 32
+// 0x00000026: .cfi_restore_state:
+// 0x00000026: .cfi_def_cfa_offset: +32
 
 static constexpr uint8_t expected_asm_kX86_64[] = {
     0x41, 0x57, 0x41, 0x56, 0x41, 0x55, 0x41, 0x54, 0x55, 0x53, 0x48, 0x83,
@@ -269,73 +274,74 @@ static constexpr uint8_t expected_cfi_kX86_64[] = {
     0x42, 0x0E, 0x20, 0xCC, 0x42, 0x0E, 0x18, 0xCD, 0x42, 0x0E, 0x10, 0xCE,
     0x42, 0x0E, 0x08, 0xCF, 0x41, 0x0B, 0x0E, 0x60,
 };
+// 0x00000000: .cfi_def_cfa: R8 +0
 // 0x00000000: push r15
-// 0x00000002: .cfi_def_cfa_offset: 16
-// 0x00000002: .cfi_offset: r15 at cfa-16
+// 0x00000002: .cfi_def_cfa_offset: +16
+// 0x00000002: .cfi_offset: R15 -16
 // 0x00000002: push r14
-// 0x00000004: .cfi_def_cfa_offset: 24
-// 0x00000004: .cfi_offset: r14 at cfa-24
+// 0x00000004: .cfi_def_cfa_offset: +24
+// 0x00000004: .cfi_offset: R14 -24
 // 0x00000004: push r13
-// 0x00000006: .cfi_def_cfa_offset: 32
-// 0x00000006: .cfi_offset: r13 at cfa-32
+// 0x00000006: .cfi_def_cfa_offset: +32
+// 0x00000006: .cfi_offset: R13 -32
 // 0x00000006: push r12
-// 0x00000008: .cfi_def_cfa_offset: 40
-// 0x00000008: .cfi_offset: r12 at cfa-40
+// 0x00000008: .cfi_def_cfa_offset: +40
+// 0x00000008: .cfi_offset: R12 -40
 // 0x00000008: push rbp
-// 0x00000009: .cfi_def_cfa_offset: 48
-// 0x00000009: .cfi_offset: r6 at cfa-48
+// 0x00000009: .cfi_def_cfa_offset: +48
+// 0x00000009: .cfi_offset: RBP -48
 // 0x00000009: push rbx
-// 0x0000000a: .cfi_def_cfa_offset: 56
-// 0x0000000a: .cfi_offset: r3 at cfa-56
+// 0x0000000a: .cfi_def_cfa_offset: +56
+// 0x0000000a: .cfi_offset: RBX -56
 // 0x0000000a: subq rsp, 40
-// 0x0000000e: .cfi_def_cfa_offset: 96
+// 0x0000000e: .cfi_def_cfa_offset: +96
 // 0x0000000e: movsd [rsp + 32], xmm15
-// 0x00000015: .cfi_offset: r32 at cfa-64
+// 0x00000015: .cfi_offset: XMM15 -64
 // 0x00000015: movsd [rsp + 24], xmm14
-// 0x0000001c: .cfi_offset: r31 at cfa-72
+// 0x0000001c: .cfi_offset: XMM14 -72
 // 0x0000001c: movsd [rsp + 16], xmm13
-// 0x00000023: .cfi_offset: r30 at cfa-80
+// 0x00000023: .cfi_offset: XMM13 -80
 // 0x00000023: movsd [rsp + 8], xmm12
-// 0x0000002a: .cfi_offset: r29 at cfa-88
+// 0x0000002a: .cfi_offset: XMM12 -88
 // 0x0000002a: movq [rsp], rdi
 // 0x0000002e: mov [rsp + 104], esi
 // 0x00000032: movss [rsp + 108], xmm0
 // 0x00000038: mov [rsp + 112], edx
 // 0x0000003c: mov [rsp + 116], ecx
 // 0x00000040: addq rsp, -32
-// 0x00000044: .cfi_def_cfa_offset: 128
+// 0x00000044: .cfi_def_cfa_offset: +128
 // 0x00000044: addq rsp, 32
-// 0x00000048: .cfi_def_cfa_offset: 96
-// 0x00000048: .cfi_remember_state
+// 0x00000048: .cfi_def_cfa_offset: +96
+// 0x00000048: .cfi_remember_state:
 // 0x00000048: movsd xmm12, [rsp + 8]
-// 0x0000004f: .cfi_restore: r29
+// 0x0000004f: .cfi_restore: XMM12
 // 0x0000004f: movsd xmm13, [rsp + 16]
-// 0x00000056: .cfi_restore: r30
+// 0x00000056: .cfi_restore: XMM13
 // 0x00000056: movsd xmm14, [rsp + 24]
-// 0x0000005d: .cfi_restore: r31
+// 0x0000005d: .cfi_restore: XMM14
 // 0x0000005d: movsd xmm15, [rsp + 32]
-// 0x00000064: .cfi_restore: r32
+// 0x00000064: .cfi_restore: XMM15
 // 0x00000064: addq rsp, 40
-// 0x00000068: .cfi_def_cfa_offset: 56
+// 0x00000068: .cfi_def_cfa_offset: +56
 // 0x00000068: pop rbx
-// 0x00000069: .cfi_def_cfa_offset: 48
-// 0x00000069: .cfi_restore: r3
+// 0x00000069: .cfi_def_cfa_offset: +48
+// 0x00000069: .cfi_restore: RBX
 // 0x00000069: pop rbp
-// 0x0000006a: .cfi_def_cfa_offset: 40
-// 0x0000006a: .cfi_restore: r6
+// 0x0000006a: .cfi_def_cfa_offset: +40
+// 0x0000006a: .cfi_restore: RBP
 // 0x0000006a: pop r12
-// 0x0000006c: .cfi_def_cfa_offset: 32
-// 0x0000006c: .cfi_restore: r12
+// 0x0000006c: .cfi_def_cfa_offset: +32
+// 0x0000006c: .cfi_restore: R12
 // 0x0000006c: pop r13
-// 0x0000006e: .cfi_def_cfa_offset: 24
-// 0x0000006e: .cfi_restore: r13
+// 0x0000006e: .cfi_def_cfa_offset: +24
+// 0x0000006e: .cfi_restore: R13
 // 0x0000006e: pop r14
-// 0x00000070: .cfi_def_cfa_offset: 16
-// 0x00000070: .cfi_restore: r14
+// 0x00000070: .cfi_def_cfa_offset: +16
+// 0x00000070: .cfi_restore: R14
 // 0x00000070: pop r15
-// 0x00000072: .cfi_def_cfa_offset: 8
-// 0x00000072: .cfi_restore: r15
+// 0x00000072: .cfi_def_cfa_offset: +8
+// 0x00000072: .cfi_restore: R15
 // 0x00000072: ret
-// 0x00000073: .cfi_restore_state
-// 0x00000073: .cfi_def_cfa_offset: 96
+// 0x00000073: .cfi_restore_state:
+// 0x00000073: .cfi_def_cfa_offset: +96
 
diff --git a/compiler/oat/jni_stub_hash_map_test.cc b/compiler/oat/jni_stub_hash_map_test.cc
index 170381d988..a62aefad58 100644
--- a/compiler/oat/jni_stub_hash_map_test.cc
+++ b/compiler/oat/jni_stub_hash_map_test.cc
@@ -49,7 +49,6 @@
 #include "obj_ptr.h"
 #include "runtime.h"
 #include "scoped_thread_state_change.h"
-#include "strstream"
 
 namespace art HIDDEN {
 
diff --git a/compiler/optimizing/block_builder.cc b/compiler/optimizing/block_builder.cc
index 12c260a146..7e1dddee79 100644
--- a/compiler/optimizing/block_builder.cc
+++ b/compiler/optimizing/block_builder.cc
@@ -48,7 +48,7 @@ HBasicBlock* HBasicBlockBuilder::MaybeCreateBlockAt(uint32_t semantic_dex_pc,
                                                     uint32_t store_dex_pc) {
   HBasicBlock* block = branch_targets_[store_dex_pc];
   if (block == nullptr) {
-    block = new (allocator_) HBasicBlock(graph_, semantic_dex_pc);
+    block = HBasicBlock::Create(allocator_, graph_, semantic_dex_pc);
     branch_targets_[store_dex_pc] = block;
   }
   DCHECK_EQ(block->GetDexPc(), semantic_dex_pc);
@@ -323,7 +323,7 @@ void HBasicBlockBuilder::InsertTryBoundaryBlocks() {
       HBasicBlock* catch_block = GetBlockAt(address);
       bool is_try_block = (try_block_info.find(catch_block->GetBlockId()) != try_block_info.end());
       if (is_try_block || MightHaveLiveNormalPredecessors(catch_block)) {
-        HBasicBlock* new_catch_block = new (allocator_) HBasicBlock(graph_, address);
+        HBasicBlock* new_catch_block = HBasicBlock::Create(allocator_, graph_, address);
         new_catch_block->AddInstruction(new (allocator_) HGoto(address));
         new_catch_block->AddSuccessor(catch_block);
         graph_->AddBlock(new_catch_block);
@@ -414,7 +414,7 @@ void HBasicBlockBuilder::InsertSynthesizedLoopsForOsr() {
   // Insert synthesized loops before the collected blocks.
   for (uint32_t block_id : targets) {
     HBasicBlock* block = graph_->GetBlocks()[block_id];
-    HBasicBlock* loop_block = new (allocator_) HBasicBlock(graph_, block->GetDexPc());
+    HBasicBlock* loop_block = HBasicBlock::Create(allocator_, graph_, block->GetDexPc());
     graph_->AddBlock(loop_block);
     while (!block->GetPredecessors().empty()) {
       block->GetPredecessors()[0]->ReplaceSuccessor(block, loop_block);
@@ -431,8 +431,8 @@ bool HBasicBlockBuilder::Build() {
   DCHECK(code_item_accessor_.HasCodeItem());
   DCHECK(graph_->GetBlocks().empty());
 
-  graph_->SetEntryBlock(new (allocator_) HBasicBlock(graph_, kNoDexPc));
-  graph_->SetExitBlock(new (allocator_) HBasicBlock(graph_, kNoDexPc));
+  graph_->SetEntryBlock(HBasicBlock::Create(allocator_, graph_, kNoDexPc));
+  graph_->SetExitBlock(HBasicBlock::Create(allocator_, graph_, kNoDexPc));
 
   // TODO(dbrazdil): Do CreateBranchTargets and ConnectBasicBlocks in one pass.
   if (!CreateBranchTargets()) {
@@ -454,8 +454,8 @@ void HBasicBlockBuilder::BuildIntrinsic() {
   DCHECK(graph_->GetBlocks().empty());
 
   // Create blocks.
-  HBasicBlock* entry_block = new (allocator_) HBasicBlock(graph_, kNoDexPc);
-  HBasicBlock* exit_block = new (allocator_) HBasicBlock(graph_, kNoDexPc);
+  HBasicBlock* entry_block = HBasicBlock::Create(allocator_, graph_, kNoDexPc);
+  HBasicBlock* exit_block = HBasicBlock::Create(allocator_, graph_, kNoDexPc);
   HBasicBlock* body = MaybeCreateBlockAt(/* semantic_dex_pc= */ kNoDexPc, /* store_dex_pc= */ 0u);
 
   // Add blocks to the graph.
diff --git a/compiler/optimizing/bounds_check_elimination.cc b/compiler/optimizing/bounds_check_elimination.cc
index 1ef4d751a2..b723f045c5 100644
--- a/compiler/optimizing/bounds_check_elimination.cc
+++ b/compiler/optimizing/bounds_check_elimination.cc
@@ -18,9 +18,11 @@
 
 #include <limits>
 
+#include "base/arena_allocator.h"
 #include "base/scoped_arena_allocator.h"
 #include "base/scoped_arena_containers.h"
 #include "induction_var_range.h"
+#include "loop_information-inl.h"
 #include "nodes.h"
 #include "side_effects_analysis.h"
 
@@ -539,13 +541,13 @@ class BCEVisitor final : public HGraphVisitor {
     for (HInstruction* instruction = block->GetFirstPhi(); instruction != nullptr;) {
       DCHECK(instruction->IsInBlock());
       next_ = instruction->GetNext();
-      instruction->Accept(this);
+      VisitPhi(instruction->AsPhi());
       instruction = next_;
     }
     for (HInstruction* instruction = block->GetFirstInstruction(); instruction != nullptr;) {
       DCHECK(instruction->IsInBlock());
       next_ = instruction->GetNext();
-      instruction->Accept(this);
+      Dispatch(instruction);
       instruction = next_;
     }
     // We should never deoptimize from an osr method, otherwise we might wrongly optimize
@@ -569,7 +571,8 @@ class BCEVisitor final : public HGraphVisitor {
     // TODO(solanes): Do this without a linear pass of the graph?
     GetGraph()->SetHasBoundsChecks(false);
     for (HBasicBlock* block : GetGraph()->GetReversePostOrder()) {
-      for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+      for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done();
+           it.Advance()) {
         HInstruction* instruction = it.Current();
         if (instruction->IsBoundsCheck()) {
           GetGraph()->SetHasBoundsChecks(true);
@@ -1715,9 +1718,9 @@ class BCEVisitor final : public HGraphVisitor {
     }
     // First time early-exit analysis for this loop. Since analysis requires scanning
     // the full loop-body, results of the analysis is stored for subsequent queries.
-    HBlocksInLoopReversePostOrderIterator it_loop(*loop);
-    for (it_loop.Advance(); !it_loop.Done(); it_loop.Advance()) {
-      for (HBasicBlock* successor : it_loop.Current()->GetSuccessors()) {
+    auto loop_blocks = loop->GetBlocksReversePostOrder();
+    for (auto loop_it = ++loop_blocks.begin(), end = loop_blocks.end(); loop_it != end; ++loop_it) {
+      for (HBasicBlock* successor : (*loop_it)->GetSuccessors()) {
         if (!loop->Contains(*successor)) {
           early_exit_loop_.Put(loop_id, true);
           return true;
@@ -1951,7 +1954,8 @@ class BCEVisitor final : public HGraphVisitor {
       HBasicBlock* true_block = entry.second;
       HBasicBlock* new_preheader = true_block->GetSingleSuccessor();
       // Scan all instructions in a new deoptimization block.
-      for (HInstructionIterator it(true_block->GetInstructions()); !it.Done(); it.Advance()) {
+      for (HInstructionIteratorPrefetchNext it(true_block->GetInstructions()); !it.Done();
+           it.Advance()) {
         HInstruction* instruction = it.Current();
         DataType::Type type = instruction->GetType();
         HPhi* phi = nullptr;
@@ -2069,12 +2073,15 @@ bool BoundsCheckElimination::Run() {
     return false;
   }
 
+  SideEffectsAnalysis side_effects(graph_);
+  side_effects.Run();
+
   // Reverse post order guarantees a node's dominators are visited first.
   // We want to visit in the dominator-based order since if a value is known to
   // be bounded by a range at one instruction, it must be true that all uses of
   // that value dominated by that instruction fits in that range. Range of that
   // value can be narrowed further down in the dominator tree.
-  BCEVisitor visitor(graph_, side_effects_, induction_analysis_);
+  BCEVisitor visitor(graph_, side_effects, induction_analysis_);
   for (size_t i = 0, size = graph_->GetReversePostOrder().size(); i != size; ++i) {
     HBasicBlock* current = graph_->GetReversePostOrder()[i];
     if (visitor.IsAddedBlock(current)) {
diff --git a/compiler/optimizing/bounds_check_elimination.h b/compiler/optimizing/bounds_check_elimination.h
index f210fa9127..ade7535282 100644
--- a/compiler/optimizing/bounds_check_elimination.h
+++ b/compiler/optimizing/bounds_check_elimination.h
@@ -22,17 +22,14 @@
 
 namespace art HIDDEN {
 
-class SideEffectsAnalysis;
 class HInductionVarAnalysis;
 
 class BoundsCheckElimination : public HOptimization {
  public:
   BoundsCheckElimination(HGraph* graph,
-                         const SideEffectsAnalysis& side_effects,
                          HInductionVarAnalysis* induction_analysis,
                          const char* name = kBoundsCheckEliminationPassName)
       : HOptimization(graph, name),
-        side_effects_(side_effects),
         induction_analysis_(induction_analysis) {}
 
   bool Run() override;
@@ -40,7 +37,6 @@ class BoundsCheckElimination : public HOptimization {
   static constexpr const char* kBoundsCheckEliminationPassName = "BCE";
 
  private:
-  const SideEffectsAnalysis& side_effects_;
   HInductionVarAnalysis* induction_analysis_;
 
   DISALLOW_COPY_AND_ASSIGN(BoundsCheckElimination);
diff --git a/compiler/optimizing/bounds_check_elimination_test.cc b/compiler/optimizing/bounds_check_elimination_test.cc
index 246cd10ca7..02cfe7e3fe 100644
--- a/compiler/optimizing/bounds_check_elimination_test.cc
+++ b/compiler/optimizing/bounds_check_elimination_test.cc
@@ -41,15 +41,12 @@ class BoundsCheckEliminationTest : public OptimizingUnitTest {
 
     InstructionSimplifier(graph_, /* codegen= */ nullptr).Run();
 
-    SideEffectsAnalysis side_effects(graph_);
-    side_effects.Run();
-
-    GVNOptimization(graph_, side_effects).Run();
+    GVNOptimization(graph_).Run();
 
     HInductionVarAnalysis induction(graph_);
     induction.Run();
 
-    BoundsCheckElimination(graph_, side_effects, &induction).Run();
+    BoundsCheckElimination(graph_, &induction).Run();
   }
 
   HInstruction* BuildSSAGraph1(int initial, int increment, IfCondition cond = kCondGE);
@@ -101,11 +98,10 @@ TEST_F(BoundsCheckEliminationTest, NarrowingRangeArrayBoundsElimination) {
   HBoundsCheck* bounds_check5 = MakeBoundsCheck(block5, parameter2, array_length);
   MakeArraySet(block5, null_check, bounds_check5, constant_1, DataType::Type::kInt32);
 
-  HBasicBlock* exit = AddNewBlock();
+  HBasicBlock* exit = AddExitBlock();
   block2->AddSuccessor(exit);
   block4->AddSuccessor(exit);
   block5->AddSuccessor(exit);
-  MakeExit(exit);
 
   block1->AddSuccessor(block3);  // True successor
   block1->AddSuccessor(block2);  // False successor
@@ -152,13 +148,16 @@ TEST_F(BoundsCheckEliminationTest, OverflowArrayBoundsElimination) {
   HBoundsCheck* bounds_check = MakeBoundsCheck(block3, add, array_length);
   MakeArraySet(block3, null_check, bounds_check, constant_1, DataType::Type::kInt32);
 
-  HBasicBlock* exit = AddNewBlock();
-  MakeExit(exit);
-  block1->AddSuccessor(exit);    // true successor
-  block1->AddSuccessor(block2);  // false successor
-  block2->AddSuccessor(exit);    // true successor
-  block2->AddSuccessor(block3);  // false successor
-  block3->AddSuccessor(exit);
+  HBasicBlock* return_block = AddNewBlock();
+  MakeReturnVoid(return_block);
+  HBasicBlock* exit = AddExitBlock();
+  return_block->AddSuccessor(exit);
+
+  block1->AddSuccessor(return_block);  // true successor
+  block1->AddSuccessor(block2);        // false successor
+  block2->AddSuccessor(return_block);  // true successor
+  block2->AddSuccessor(block3);        // false successor
+  block3->AddSuccessor(return_block);
 
   RunBCE();
 
@@ -199,13 +198,16 @@ TEST_F(BoundsCheckEliminationTest, UnderflowArrayBoundsElimination) {
   HBoundsCheck* bounds_check = MakeBoundsCheck(block3, sub2, array_length);
   MakeArraySet(block3, null_check, bounds_check, constant_1, DataType::Type::kInt32);
 
-  HBasicBlock* exit = AddNewBlock();
-  MakeExit(exit);
-  block1->AddSuccessor(exit);    // true successor
-  block1->AddSuccessor(block2);  // false successor
-  block2->AddSuccessor(exit);    // true successor
-  block2->AddSuccessor(block3);  // false successor
-  block3->AddSuccessor(exit);
+  HBasicBlock* return_block = AddNewBlock();
+  MakeReturnVoid(return_block);
+  HBasicBlock* exit = AddExitBlock();
+  return_block->AddSuccessor(exit);
+
+  block1->AddSuccessor(return_block);  // true successor
+  block1->AddSuccessor(block2);        // false successor
+  block2->AddSuccessor(return_block);  // true successor
+  block2->AddSuccessor(block3);        // false successor
+  block3->AddSuccessor(return_block);
 
   RunBCE();
 
diff --git a/compiler/optimizing/cha_guard_optimization.cc b/compiler/optimizing/cha_guard_optimization.cc
index fb9d220a7b..c723d7debc 100644
--- a/compiler/optimizing/cha_guard_optimization.cc
+++ b/compiler/optimizing/cha_guard_optimization.cc
@@ -16,6 +16,8 @@
 
 #include "cha_guard_optimization.h"
 
+#include "nodes.h"
+
 namespace art HIDDEN {
 
 // Note we can only do CHA guard elimination/motion in a single pass, since
@@ -64,7 +66,7 @@ class CHAGuardVisitor final : public HGraphVisitor {
 
   // The iterator that's being used for this visitor. Need it to manually
   // advance the iterator due to removing/moving more than one instruction.
-  HInstructionIterator* instruction_iterator_;
+  HInstructionIteratorPrefetchNext* instruction_iterator_;
 
   // Used to short-circuit the pass when there is no more guards left to visit.
   uint32_t number_of_guards_to_visit_;
@@ -77,11 +79,11 @@ void CHAGuardVisitor::VisitBasicBlock(HBasicBlock* block) {
     return;
   }
   // Skip phis, just iterate through instructions.
-  HInstructionIterator it(block->GetInstructions());
+  HInstructionIteratorPrefetchNext it(block->GetInstructions());
   instruction_iterator_ = &it;
   for (; !it.Done(); it.Advance()) {
     DCHECK(it.Current()->IsInBlock());
-    it.Current()->Accept(this);
+    Dispatch(it.Current());
   }
 }
 
diff --git a/compiler/optimizing/code_generator.cc b/compiler/optimizing/code_generator.cc
index d63b0abcc7..34db23200a 100644
--- a/compiler/optimizing/code_generator.cc
+++ b/compiler/optimizing/code_generator.cc
@@ -247,10 +247,18 @@ bool CodeGenerator::GoesToNextBlock(HBasicBlock* current, HBasicBlock* next) con
   return GetNextBlockToEmit() == FirstNonEmptyBlock(next);
 }
 
+// Returns true if the `block` emits nothing but a jump.
+inline bool IsSingleJump(HBasicBlock* block) {
+  HLoopInformation* loop_info = block->GetLoopInformation();
+  return (block->IsSingleGoto() || block->IsSingleTryBoundary())
+         // Back edges generate a suspend check.
+         && (loop_info == nullptr || !loop_info->IsBackEdge(*block));
+}
+
 HBasicBlock* CodeGenerator::GetNextBlockToEmit() const {
   for (size_t i = current_block_index_ + 1; i < block_order_->size(); ++i) {
     HBasicBlock* block = (*block_order_)[i];
-    if (!block->IsSingleJump()) {
+    if (!IsSingleJump(block)) {
       return block;
     }
   }
@@ -258,7 +266,7 @@ HBasicBlock* CodeGenerator::GetNextBlockToEmit() const {
 }
 
 HBasicBlock* CodeGenerator::FirstNonEmptyBlock(HBasicBlock* block) const {
-  while (block->IsSingleJump()) {
+  while (IsSingleJump(block)) {
     block = block->GetSuccessors()[0];
   }
   return block;
@@ -343,13 +351,15 @@ void CodeGenerator::Compile() {
     // Don't generate code for an empty block. Its predecessors will branch to its successor
     // directly. Also, the label of that block will not be emitted, so this helps catch
     // errors where we reference that label.
-    if (block->IsSingleJump()) continue;
+    if (IsSingleJump(block)) {
+      continue;
+    }
     Bind(block);
     // This ensures that we have correct native line mapping for all native instructions.
     // It is necessary to make stepping over a statement work. Otherwise, any initial
     // instructions (e.g. moves) would be assumed to be the start of next statement.
     MaybeRecordNativeDebugInfoForBlockEntry(block->GetDexPc());
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       HInstruction* current = it.Current();
       if (current->HasEnvironment()) {
         // Catch StackMaps are dealt with later on in `RecordCatchBlockInfo`.
@@ -365,7 +375,7 @@ void CodeGenerator::Compile() {
       }
       DisassemblyScope disassembly_scope(current, *this);
       DCHECK(CheckTypeConsistency(current));
-      current->Accept(instruction_visitor);
+      instruction_visitor->Dispatch(current);
     }
   }
 
@@ -434,8 +444,8 @@ void CodeGenerator::InitializeCodeGeneration(size_t number_of_spill_slots,
 void CodeGenerator::CreateCommonInvokeLocationSummary(
     HInvoke* invoke, InvokeDexCallingConventionVisitor* visitor) {
   ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
-  LocationSummary* locations = new (allocator) LocationSummary(invoke,
-                                                               LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnMainOnly);
 
   for (size_t i = 0; i < invoke->GetNumberOfArguments(); i++) {
     HInstruction* input = invoke->InputAt(i);
@@ -592,7 +602,7 @@ void CodeGenerator::CreateStringBuilderAppendLocations(HStringBuilderAppend* ins
                                                        Location out) {
   ArenaAllocator* allocator = GetGraph()->GetAllocator();
   LocationSummary* locations =
-      new (allocator) LocationSummary(instruction, LocationSummary::kCallOnMainOnly);
+      LocationSummary::Create(allocator, instruction, LocationSummary::kCallOnMainOnly);
   locations->SetOut(out);
   instruction->GetLocations()->SetInAt(instruction->FormatIndex(),
                                        Location::ConstantLocation(instruction->GetFormat()));
@@ -647,7 +657,7 @@ void CodeGenerator::CreateUnresolvedFieldLocationSummary(
 
   ArenaAllocator* allocator = GetGraph()->GetAllocator();
   LocationSummary* locations =
-      new (allocator) LocationSummary(field_access, LocationSummary::kCallOnMainOnly);
+      LocationSummary::Create(allocator, field_access, LocationSummary::kCallOnMainOnly);
 
   locations->AddTemp(calling_convention.GetFieldIndexLocation());
 
@@ -765,8 +775,9 @@ void CodeGenerator::CreateLoadClassRuntimeCallLocationSummary(HLoadClass* cls,
                                                               Location runtime_return_location) {
   DCHECK_EQ(cls->GetLoadKind(), HLoadClass::LoadKind::kRuntimeCall);
   DCHECK_EQ(cls->InputCount(), 1u);
-  LocationSummary* locations = new (cls->GetBlock()->GetGraph()->GetAllocator()) LocationSummary(
-      cls, LocationSummary::kCallOnMainOnly);
+  ArenaAllocator* allocator = cls->GetBlock()->GetGraph()->GetAllocator();
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, cls, LocationSummary::kCallOnMainOnly);
   locations->SetInAt(0, Location::NoLocation());
   locations->AddTemp(runtime_type_index_location);
   locations->SetOut(runtime_return_location);
@@ -791,9 +802,9 @@ void CodeGenerator::CreateLoadMethodHandleRuntimeCallLocationSummary(
     Location runtime_proto_index_location,
     Location runtime_return_location) {
   DCHECK_EQ(method_handle->InputCount(), 1u);
+  ArenaAllocator* allocator = method_handle->GetBlock()->GetGraph()->GetAllocator();
   LocationSummary* locations =
-      new (method_handle->GetBlock()->GetGraph()->GetAllocator()) LocationSummary(
-          method_handle, LocationSummary::kCallOnMainOnly);
+      LocationSummary::Create(allocator, method_handle, LocationSummary::kCallOnMainOnly);
   locations->SetInAt(0, Location::NoLocation());
   locations->AddTemp(runtime_proto_index_location);
   locations->SetOut(runtime_return_location);
@@ -811,9 +822,9 @@ void CodeGenerator::CreateLoadMethodTypeRuntimeCallLocationSummary(
     Location runtime_proto_index_location,
     Location runtime_return_location) {
   DCHECK_EQ(method_type->InputCount(), 1u);
+  ArenaAllocator* allocator = method_type->GetBlock()->GetGraph()->GetAllocator();
   LocationSummary* locations =
-      new (method_type->GetBlock()->GetGraph()->GetAllocator()) LocationSummary(
-          method_type, LocationSummary::kCallOnMainOnly);
+      LocationSummary::Create(allocator, method_type, LocationSummary::kCallOnMainOnly);
   locations->SetInAt(0, Location::NoLocation());
   locations->AddTemp(runtime_proto_index_location);
   locations->SetOut(runtime_return_location);
@@ -913,7 +924,7 @@ void CodeGenerator::AllocateLocations(HInstruction* instruction) {
   for (HEnvironment* env = instruction->GetEnvironment(); env != nullptr; env = env->GetParent()) {
     env->AllocateLocations(allocator);
   }
-  instruction->Accept(GetLocationBuilder());
+  GetLocationBuilder()->Dispatch(instruction);
   DCHECK(CheckTypeConsistency(instruction));
   LocationSummary* locations = instruction->GetLocations();
   if (!instruction->IsSuspendCheckEntry()) {
@@ -1169,19 +1180,24 @@ void CodeGenerator::RecordPcInfo(HInstruction* instruction,
   DCHECK_IMPLIES(!native_debug_info, instruction->HasEnvironment()) << *instruction;
 
   LocationSummary* locations = instruction->GetLocations();
-  uint32_t register_mask = locations->GetRegisterMask();
-  DCHECK_EQ(register_mask & ~locations->GetLiveRegisters()->GetCoreRegisters(), 0u);
-  if (locations->OnlyCallsOnSlowPath()) {
-    // In case of slow path, we currently set the location of caller-save registers
-    // to register (instead of their stack location when pushed before the slow-path
-    // call). Therefore register_mask contains both callee-save and caller-save
-    // registers that hold objects. We must remove the spilled caller-save from the
-    // mask, since they will be overwritten by the callee.
-    uint32_t spills = GetSlowPathSpills(locations, /* core_registers= */ true);
-    register_mask &= ~spills;
-  } else {
-    // The register mask must be a subset of callee-save registers.
-    DCHECK_EQ(register_mask & core_callee_save_mask_, register_mask);
+  uint32_t register_mask = 0u;
+  BitVector* stack_mask = nullptr;
+  if (locations->CanCall()) {
+    stack_mask = locations->GetStackMask();
+    register_mask = locations->GetRegisterMask();
+    DCHECK_EQ(register_mask & ~locations->GetLiveRegisters()->GetCoreRegisters(), 0u);
+    if (locations->OnlyCallsOnSlowPath()) {
+      // In case of slow path, we currently set the location of caller-save registers
+      // to register (instead of their stack location when pushed before the slow-path
+      // call). Therefore register_mask contains both callee-save and caller-save
+      // registers that hold objects. We must remove the spilled caller-save from the
+      // mask, since they will be overwritten by the callee.
+      uint32_t spills = GetSlowPathSpills(locations, /* core_registers= */ true);
+      register_mask &= ~spills;
+    } else {
+      // The register mask must be a subset of callee-save registers.
+      DCHECK_EQ(register_mask & core_callee_save_mask_, register_mask);
+    }
   }
 
   uint32_t outer_dex_pc = dex_pc;
@@ -1207,12 +1223,8 @@ void CodeGenerator::RecordPcInfo(HInstruction* instruction,
       : (osr ? StackMap::Kind::OSR : StackMap::Kind::Default);
   bool needs_vreg_info = NeedsVregInfo(instruction, osr);
   StackMapStream* stack_map_stream = GetStackMapStream();
-  stack_map_stream->BeginStackMapEntry(outer_dex_pc,
-                                       native_pc,
-                                       register_mask,
-                                       locations->GetStackMask(),
-                                       kind,
-                                       needs_vreg_info);
+  stack_map_stream->BeginStackMapEntry(
+      outer_dex_pc, native_pc, register_mask, stack_mask, kind, needs_vreg_info);
 
   EmitEnvironment(environment, slow_path, needs_vreg_info);
   stack_map_stream->EndStackMapEntry();
@@ -1588,7 +1600,7 @@ LocationSummary* CodeGenerator::CreateThrowingSlowPathLocations(HInstruction* in
     call_kind = LocationSummary::kCallOnSlowPath;
   }
   LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+      LocationSummary::Create(GetGraph()->GetAllocator(), instruction, call_kind);
   if (can_throw_into_catch_block && compiler_options_.GetImplicitNullChecks()) {
     locations->SetCustomSlowPathCallerSaves(caller_saves);  // Default: no caller-save registers.
   }
@@ -1812,9 +1824,8 @@ LocationSummary* CodeGenerator::CreateSystemArrayCopyLocationSummary(
   }
 
   ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
-  LocationSummary* locations = new (allocator) LocationSummary(invoke,
-                                                               LocationSummary::kCallOnSlowPath,
-                                                               kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   // arraycopy(Object src, int src_pos, Object dest, int dest_pos, int length).
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RegisterOrConstant(invoke->InputAt(1)));
diff --git a/compiler/optimizing/code_generator_arm64.cc b/compiler/optimizing/code_generator_arm64.cc
index 5992f8385b..0d374c296b 100644
--- a/compiler/optimizing/code_generator_arm64.cc
+++ b/compiler/optimizing/code_generator_arm64.cc
@@ -1267,8 +1267,8 @@ void ParallelMoveResolverARM64::EmitMove(size_t index) {
 }
 
 void LocationsBuilderARM64::VisitMethodExitHook(HMethodExitHook* method_hook) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(method_hook, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, method_hook, LocationSummary::kCallOnSlowPath);
   DataType::Type return_type = method_hook->InputAt(0)->GetType();
   locations->SetInAt(0, ARM64ReturnLocation(return_type));
 }
@@ -1345,7 +1345,7 @@ void InstructionCodeGeneratorARM64::VisitMethodExitHook(HMethodExitHook* instruc
 }
 
 void LocationsBuilderARM64::VisitMethodEntryHook(HMethodEntryHook* method_hook) {
-  new (GetGraph()->GetAllocator()) LocationSummary(method_hook, LocationSummary::kCallOnSlowPath);
+  LocationSummary::Create(allocator_, method_hook, LocationSummary::kCallOnSlowPath);
 }
 
 void InstructionCodeGeneratorARM64::VisitMethodEntryHook(HMethodEntryHook* instruction) {
@@ -1412,6 +1412,49 @@ void CodeGeneratorARM64::MaybeIncrementHotness(HSuspendCheck* suspend_check, boo
   }
 }
 
+void CodeGeneratorARM64::GenerateFrame(Arm64Assembler* assembler,
+                                       int32_t frame_size,
+                                       CPURegList preserved_core_registers,
+                                       CPURegList preserved_fp_registers,
+                                       bool requires_current_method) {
+  // Stack layout:
+  //      sp[frame_size - 8]        : lr.
+  //      ...                       : other preserved core registers.
+  //      ...                       : other preserved fp registers.
+  //      ...                       : reserved frame space.
+  //      sp[0]                     : current method.
+  DCHECK(!preserved_core_registers.IsEmpty());
+  uint32_t core_spill_size = preserved_core_registers.GetTotalSizeInBytes();
+  uint32_t frame_entry_spill_size = preserved_fp_registers.GetTotalSizeInBytes() + core_spill_size;
+  uint32_t core_spills_offset = frame_size - core_spill_size;
+  uint32_t fp_spills_offset = frame_size - frame_entry_spill_size;
+  vixl::aarch64::MacroAssembler* vixl_assembler = assembler->GetVIXLAssembler();
+
+  // Save the current method if we need it, or if using STP reduces code
+  // size. Note that we do not do this in HCurrentMethod, as the
+  // instruction might have been removed in the SSA graph.
+  CPURegister lowest_spill;
+  if (core_spills_offset == kXRegSizeInBytes) {
+    // If there is no gap between the method and the lowest core spill, use
+    // aligned STP pre-index to store both. Max difference is 512. We do
+    // that to reduce code size even if we do not have to save the method.
+    DCHECK_LE(frame_size, 512);  // 32 core registers are only 256 bytes.
+    lowest_spill = preserved_core_registers.PopLowestIndex();
+    vixl_assembler->Stp(kArtMethodRegister, lowest_spill, MemOperand(sp, -frame_size, PreIndex));
+  } else if (requires_current_method) {
+    vixl_assembler->Str(kArtMethodRegister, MemOperand(sp, -frame_size, PreIndex));
+  } else {
+    vixl_assembler->Claim(frame_size);
+  }
+  assembler->cfi().AdjustCFAOffset(frame_size);
+  if (lowest_spill.IsValid()) {
+    assembler->cfi().RelOffset(DWARFReg(lowest_spill), core_spills_offset);
+    core_spills_offset += kXRegSizeInBytes;
+  }
+  assembler->SpillRegisters(preserved_core_registers, core_spills_offset);
+  assembler->SpillRegisters(preserved_fp_registers, fp_spills_offset);
+}
+
 void CodeGeneratorARM64::GenerateFrameEntry() {
   MacroAssembler* masm = GetVIXLAssembler();
 
@@ -1485,42 +1528,11 @@ void CodeGeneratorARM64::GenerateFrameEntry() {
     // Make sure the frame size isn't unreasonably large.
     DCHECK_LE(GetFrameSize(), GetMaximumFrameSize());
 
-    // Stack layout:
-    //      sp[frame_size - 8]        : lr.
-    //      ...                       : other preserved core registers.
-    //      ...                       : other preserved fp registers.
-    //      ...                       : reserved frame space.
-    //      sp[0]                     : current method.
-    int32_t frame_size = dchecked_integral_cast<int32_t>(GetFrameSize());
-    uint32_t core_spills_offset = frame_size - GetCoreSpillSize();
-    CPURegList preserved_core_registers = GetFramePreservedCoreRegisters();
-    DCHECK(!preserved_core_registers.IsEmpty());
-    uint32_t fp_spills_offset = frame_size - FrameEntrySpillSize();
-    CPURegList preserved_fp_registers = GetFramePreservedFPRegisters();
-
-    // Save the current method if we need it, or if using STP reduces code
-    // size. Note that we do not do this in HCurrentMethod, as the
-    // instruction might have been removed in the SSA graph.
-    CPURegister lowest_spill;
-    if (core_spills_offset == kXRegSizeInBytes) {
-      // If there is no gap between the method and the lowest core spill, use
-      // aligned STP pre-index to store both. Max difference is 512. We do
-      // that to reduce code size even if we do not have to save the method.
-      DCHECK_LE(frame_size, 512);  // 32 core registers are only 256 bytes.
-      lowest_spill = preserved_core_registers.PopLowestIndex();
-      __ Stp(kArtMethodRegister, lowest_spill, MemOperand(sp, -frame_size, PreIndex));
-    } else if (RequiresCurrentMethod()) {
-      __ Str(kArtMethodRegister, MemOperand(sp, -frame_size, PreIndex));
-    } else {
-      __ Claim(frame_size);
-    }
-    GetAssembler()->cfi().AdjustCFAOffset(frame_size);
-    if (lowest_spill.IsValid()) {
-      GetAssembler()->cfi().RelOffset(DWARFReg(lowest_spill), core_spills_offset);
-      core_spills_offset += kXRegSizeInBytes;
-    }
-    GetAssembler()->SpillRegisters(preserved_core_registers, core_spills_offset);
-    GetAssembler()->SpillRegisters(preserved_fp_registers, fp_spills_offset);
+    GenerateFrame(GetAssembler(),
+                  dchecked_integral_cast<int32_t>(GetFrameSize()),
+                  GetFramePreservedCoreRegisters(),
+                  GetFramePreservedFPRegisters(),
+                  RequiresCurrentMethod());
 
     if (GetGraph()->HasShouldDeoptimizeFlag()) {
       // Initialize should_deoptimize flag to 0.
@@ -2290,7 +2302,7 @@ InstructionCodeGeneratorARM64::InstructionCodeGeneratorARM64(HGraph* graph,
 
 void LocationsBuilderARM64::HandleBinaryOp(HBinaryOperation* instr) {
   DCHECK_EQ(instr->InputCount(), 2U);
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instr);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instr);
   DataType::Type type = instr->GetResultType();
   switch (type) {
     case DataType::Type::kInt32:
@@ -2318,11 +2330,11 @@ void LocationsBuilderARM64::HandleFieldGet(HInstruction* instruction,
 
   bool object_field_get_with_read_barrier =
       (instruction->GetType() == DataType::Type::kReference) && codegen_->EmitReadBarrier();
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction,
-                                                       object_field_get_with_read_barrier
-                                                           ? LocationSummary::kCallOnSlowPath
-                                                           : LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
+      instruction,
+      object_field_get_with_read_barrier ? LocationSummary::kCallOnSlowPath
+                                         : LocationSummary::kNoCall);
   if (object_field_get_with_read_barrier && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
     // We need a temporary register for the read barrier load in
@@ -2403,8 +2415,7 @@ void InstructionCodeGeneratorARM64::HandleFieldGet(HInstruction* instruction,
 }
 
 void LocationsBuilderARM64::HandleFieldSet(HInstruction* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   HInstruction* value = instruction->InputAt(1);
   if (IsZeroBitPattern(value)) {
@@ -2545,7 +2556,7 @@ void InstructionCodeGeneratorARM64::HandleBinaryOp(HBinaryOperation* instr) {
 void LocationsBuilderARM64::HandleShift(HBinaryOperation* instr) {
   DCHECK(instr->IsShl() || instr->IsShr() || instr->IsUShr());
 
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instr);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instr);
   DataType::Type type = instr->GetResultType();
   switch (type) {
     case DataType::Type::kInt32:
@@ -2616,7 +2627,7 @@ void InstructionCodeGeneratorARM64::VisitAnd(HAnd* instruction) {
 
 void LocationsBuilderARM64::VisitBitwiseNegatedRight(HBitwiseNegatedRight* instr) {
   DCHECK(DataType::IsIntegralType(instr->GetType())) << instr->GetType();
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instr);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instr);
   locations->SetInAt(0, Location::RequiresRegister());
   // There is no immediate variant of negated bitwise instructions in AArch64.
   locations->SetInAt(1, Location::RequiresRegister());
@@ -2647,8 +2658,7 @@ void LocationsBuilderARM64::VisitDataProcWithShifterOp(
     HDataProcWithShifterOp* instruction) {
   DCHECK(instruction->GetType() == DataType::Type::kInt32 ||
          instruction->GetType() == DataType::Type::kInt64);
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   if (instruction->GetInstrKind() == HInstruction::kNeg) {
     locations->SetInAt(0, Location::ConstantLocation(instruction->InputAt(0)));
   } else {
@@ -2718,8 +2728,7 @@ void InstructionCodeGeneratorARM64::VisitDataProcWithShifterOp(
 }
 
 void LocationsBuilderARM64::VisitIntermediateAddress(HIntermediateAddress* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, ARM64EncodableConstantOrRegister(instruction->GetOffset(), instruction));
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
@@ -2732,8 +2741,7 @@ void InstructionCodeGeneratorARM64::VisitIntermediateAddress(HIntermediateAddres
 }
 
 void LocationsBuilderARM64::VisitIntermediateAddressIndex(HIntermediateAddressIndex* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
 
   HIntConstant* shift = instruction->GetShift()->AsIntConstant();
 
@@ -2764,8 +2772,7 @@ void InstructionCodeGeneratorARM64::VisitIntermediateAddressIndex(
 }
 
 void LocationsBuilderARM64::VisitMultiplyAccumulate(HMultiplyAccumulate* instr) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instr, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instr);
   HInstruction* accumulator = instr->InputAt(HMultiplyAccumulate::kInputAccumulatorIndex);
   if (instr->GetOpKind() == HInstruction::kSub &&
       accumulator->IsConstant() &&
@@ -2818,11 +2825,11 @@ void InstructionCodeGeneratorARM64::VisitMultiplyAccumulate(HMultiplyAccumulate*
 void LocationsBuilderARM64::VisitArrayGet(HArrayGet* instruction) {
   bool object_array_get_with_read_barrier =
       (instruction->GetType() == DataType::Type::kReference) && codegen_->EmitReadBarrier();
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction,
-                                                       object_array_get_with_read_barrier
-                                                           ? LocationSummary::kCallOnSlowPath
-                                                           : LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
+      instruction,
+      object_array_get_with_read_barrier ? LocationSummary::kCallOnSlowPath
+                                         : LocationSummary::kNoCall);
   if (object_array_get_with_read_barrier && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
     if (instruction->GetIndex()->IsConstant()) {
@@ -2989,7 +2996,7 @@ void InstructionCodeGeneratorARM64::VisitArrayGet(HArrayGet* instruction) {
 }
 
 void LocationsBuilderARM64::VisitArrayLength(HArrayLength* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
@@ -3013,7 +3020,8 @@ void LocationsBuilderARM64::VisitArraySet(HArraySet* instruction) {
   DataType::Type value_type = instruction->GetComponentType();
 
   bool needs_type_check = instruction->NeedsTypeCheck();
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
       instruction,
       needs_type_check ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall);
   locations->SetInAt(0, Location::RequiresRegister());
@@ -3282,7 +3290,7 @@ void InstructionCodeGeneratorARM64::VisitBoundsCheck(HBoundsCheck* instruction)
 
 void LocationsBuilderARM64::VisitClinitCheck(HClinitCheck* check) {
   LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(check, LocationSummary::kCallOnSlowPath);
+      LocationSummary::Create(allocator_, check, LocationSummary::kCallOnSlowPath);
   locations->SetInAt(0, Location::RequiresRegister());
   if (check->HasUses()) {
     locations->SetOut(Location::SameAsFirstInput());
@@ -3331,8 +3339,7 @@ void InstructionCodeGeneratorARM64::GenerateFcmp(HInstruction* instruction) {
 }
 
 void LocationsBuilderARM64::VisitCompare(HCompare* compare) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(compare, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, compare);
   DataType::Type compare_type = compare->GetComparisonType();
   HInstruction* rhs = compare->InputAt(1);
   switch (compare_type) {
@@ -3406,7 +3413,7 @@ void InstructionCodeGeneratorARM64::VisitCompare(HCompare* compare) {
 }
 
 void LocationsBuilderARM64::HandleCondition(HCondition* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
 
   HInstruction* rhs = instruction->InputAt(1);
   if (DataType::IsFloatingPointType(instruction->InputAt(0)->GetType())) {
@@ -3777,8 +3784,7 @@ void InstructionCodeGeneratorARM64::GenerateIntDiv(HDiv *instruction) {
 }
 
 void LocationsBuilderARM64::VisitDiv(HDiv* div) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(div, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, div);
   switch (div->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
@@ -3849,8 +3855,7 @@ void InstructionCodeGeneratorARM64::VisitDivZeroCheck(HDivZeroCheck* instruction
 }
 
 void LocationsBuilderARM64::VisitDoubleConstant(HDoubleConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -3866,8 +3871,7 @@ void LocationsBuilderARM64::VisitExit(HExit* exit) {
 void InstructionCodeGeneratorARM64::VisitExit([[maybe_unused]] HExit* exit) {}
 
 void LocationsBuilderARM64::VisitFloatConstant(HFloatConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -4025,7 +4029,7 @@ void InstructionCodeGeneratorARM64::GenerateTestAndBranch(HInstruction* instruct
 }
 
 void LocationsBuilderARM64::VisitIf(HIf* if_instr) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(if_instr);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, if_instr);
   if (IsBooleanValueOrMaterializedCondition(if_instr->InputAt(0))) {
     locations->SetInAt(0, Location::RequiresRegister());
   }
@@ -4075,8 +4079,8 @@ void InstructionCodeGeneratorARM64::VisitIf(HIf* if_instr) {
 }
 
 void LocationsBuilderARM64::VisitDeoptimize(HDeoptimize* deoptimize) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(deoptimize, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, deoptimize, LocationSummary::kCallOnSlowPath);
   InvokeRuntimeCallingConvention calling_convention;
   RegisterSet caller_saves = RegisterSet::Empty();
   caller_saves.Add(Location::RegisterLocation(calling_convention.GetRegisterAt(0).GetCode()));
@@ -4096,8 +4100,7 @@ void InstructionCodeGeneratorARM64::VisitDeoptimize(HDeoptimize* deoptimize) {
 }
 
 void LocationsBuilderARM64::VisitShouldDeoptimizeFlag(HShouldDeoptimizeFlag* flag) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(flag, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, flag);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -4118,7 +4121,7 @@ static inline Condition GetConditionForSelect(HCondition* condition) {
 }
 
 void LocationsBuilderARM64::VisitSelect(HSelect* select) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(select);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, select);
   if (DataType::IsFloatingPointType(select->GetType())) {
     locations->SetInAt(0, Location::RequiresFpuRegister());
     locations->SetInAt(1, Location::RequiresFpuRegister());
@@ -4187,7 +4190,7 @@ void InstructionCodeGeneratorARM64::VisitSelect(HSelect* select) {
 }
 
 void LocationsBuilderARM64::VisitNop(HNop* nop) {
-  new (GetGraph()->GetAllocator()) LocationSummary(nop);
+  LocationSummary::CreateNoCall(allocator_, nop);
 }
 
 void InstructionCodeGeneratorARM64::VisitNop(HNop*) {
@@ -4273,8 +4276,7 @@ void LocationsBuilderARM64::VisitInstanceOf(HInstanceOf* instruction) {
       break;
   }
 
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, instruction, call_kind);
   if (baker_read_barrier_slow_path) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -4567,8 +4569,7 @@ void InstructionCodeGeneratorARM64::VisitInstanceOf(HInstanceOf* instruction) {
 void LocationsBuilderARM64::VisitCheckCast(HCheckCast* instruction) {
   TypeCheckKind type_check_kind = instruction->GetTypeCheckKind();
   LocationSummary::CallKind call_kind = codegen_->GetCheckCastCallKind(instruction);
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, instruction, call_kind);
   locations->SetInAt(0, Location::RequiresRegister());
   if (type_check_kind == TypeCheckKind::kBitstringCheck) {
     locations->SetInAt(1, Location::ConstantLocation(instruction->InputAt(1)));
@@ -4788,7 +4789,7 @@ void InstructionCodeGeneratorARM64::VisitCheckCast(HCheckCast* instruction) {
 }
 
 void LocationsBuilderARM64::VisitIntConstant(HIntConstant* constant) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(constant);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -4797,7 +4798,7 @@ void InstructionCodeGeneratorARM64::VisitIntConstant([[maybe_unused]] HIntConsta
 }
 
 void LocationsBuilderARM64::VisitNullConstant(HNullConstant* constant) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(constant);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -4942,7 +4943,7 @@ void InstructionCodeGeneratorARM64::VisitInvokeInterface(HInvokeInterface* invok
 }
 
 void LocationsBuilderARM64::VisitInvokeVirtual(HInvokeVirtual* invoke) {
-  IntrinsicLocationsBuilderARM64 intrinsic(GetGraph()->GetAllocator(), codegen_);
+  IntrinsicLocationsBuilderARM64 intrinsic(allocator_, codegen_);
   if (intrinsic.TryDispatch(invoke)) {
     return;
   }
@@ -4955,7 +4956,7 @@ void LocationsBuilderARM64::VisitInvokeStaticOrDirect(HInvokeStaticOrDirect* inv
   // art::PrepareForRegisterAllocation.
   DCHECK(!invoke->IsStaticWithExplicitClinitCheck());
 
-  IntrinsicLocationsBuilderARM64 intrinsic(GetGraph()->GetAllocator(), codegen_);
+  IntrinsicLocationsBuilderARM64 intrinsic(allocator_, codegen_);
   if (intrinsic.TryDispatch(invoke)) {
     return;
   }
@@ -5230,7 +5231,7 @@ void CodeGeneratorARM64::MoveFromReturnRegister(Location trg, DataType::Type typ
 }
 
 void LocationsBuilderARM64::VisitInvokePolymorphic(HInvokePolymorphic* invoke) {
-  IntrinsicLocationsBuilderARM64 intrinsic(GetGraph()->GetAllocator(), codegen_);
+  IntrinsicLocationsBuilderARM64 intrinsic(allocator_, codegen_);
   if (intrinsic.TryDispatch(invoke)) {
     return;
   }
@@ -5708,7 +5709,7 @@ void LocationsBuilderARM64::VisitLoadClass(HLoadClass* cls) {
   LocationSummary::CallKind call_kind = (cls->NeedsEnvironment() || requires_read_barrier)
       ? LocationSummary::kCallOnSlowPath
       : LocationSummary::kNoCall;
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(cls, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, cls, call_kind);
   if (kUseBakerReadBarrier && requires_read_barrier && !cls->NeedsEnvironment()) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -5877,7 +5878,7 @@ void LocationsBuilderARM64::VisitLoadMethodType(HLoadMethodType* load) {
     CodeGenerator::CreateLoadMethodTypeRuntimeCallLocationSummary(load, location, location);
   } else {
     LocationSummary* locations =
-        new (GetGraph()->GetAllocator()) LocationSummary(load, LocationSummary::kCallOnSlowPath);
+        LocationSummary::Create(allocator_, load, LocationSummary::kCallOnSlowPath);
     locations->SetOut(Location::RequiresRegister());
     if (load->GetLoadKind() == HLoadMethodType::LoadKind::kBssEntry) {
       if (codegen_->EmitNonBakerReadBarrier()) {
@@ -5942,8 +5943,7 @@ static MemOperand GetExceptionTlsAddress() {
 }
 
 void LocationsBuilderARM64::VisitLoadException(HLoadException* load) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(load, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, load);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -5952,7 +5952,7 @@ void InstructionCodeGeneratorARM64::VisitLoadException(HLoadException* instructi
 }
 
 void LocationsBuilderARM64::VisitClearException(HClearException* clear) {
-  new (GetGraph()->GetAllocator()) LocationSummary(clear, LocationSummary::kNoCall);
+  LocationSummary::CreateNoCall(allocator_, clear);
 }
 
 void InstructionCodeGeneratorARM64::VisitClearException([[maybe_unused]] HClearException* clear) {
@@ -5979,7 +5979,7 @@ HLoadString::LoadKind CodeGeneratorARM64::GetSupportedLoadStringKind(
 
 void LocationsBuilderARM64::VisitLoadString(HLoadString* load) {
   LocationSummary::CallKind call_kind = codegen_->GetLoadStringCallKind(load);
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(load, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, load, call_kind);
   if (load->GetLoadKind() == HLoadString::LoadKind::kRuntimeCall) {
     InvokeRuntimeCallingConvention calling_convention;
     locations->SetOut(calling_convention.GetReturnLocation(load->GetType()));
@@ -6080,7 +6080,7 @@ void InstructionCodeGeneratorARM64::VisitLoadString(HLoadString* load) NO_THREAD
 }
 
 void LocationsBuilderARM64::VisitLongConstant(HLongConstant* constant) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(constant);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -6089,8 +6089,8 @@ void InstructionCodeGeneratorARM64::VisitLongConstant([[maybe_unused]] HLongCons
 }
 
 void LocationsBuilderARM64::VisitMonitorOperation(HMonitorOperation* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, LocationFrom(calling_convention.GetRegisterAt(0)));
 }
@@ -6107,8 +6107,7 @@ void InstructionCodeGeneratorARM64::VisitMonitorOperation(HMonitorOperation* ins
 }
 
 void LocationsBuilderARM64::VisitMul(HMul* mul) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(mul, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, mul);
   switch (mul->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
@@ -6147,8 +6146,7 @@ void InstructionCodeGeneratorARM64::VisitMul(HMul* mul) {
 }
 
 void LocationsBuilderARM64::VisitNeg(HNeg* neg) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(neg, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, neg);
   switch (neg->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
@@ -6185,8 +6183,8 @@ void InstructionCodeGeneratorARM64::VisitNeg(HNeg* neg) {
 }
 
 void LocationsBuilderARM64::VisitNewArray(HNewArray* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetOut(LocationFrom(x0));
   locations->SetInAt(0, LocationFrom(calling_convention.GetRegisterAt(0)));
@@ -6202,8 +6200,8 @@ void InstructionCodeGeneratorARM64::VisitNewArray(HNewArray* instruction) {
 }
 
 void LocationsBuilderARM64::VisitNewInstance(HNewInstance* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, LocationFrom(calling_convention.GetRegisterAt(0)));
   locations->SetOut(calling_convention.GetReturnLocation(DataType::Type::kReference));
@@ -6216,7 +6214,7 @@ void InstructionCodeGeneratorARM64::VisitNewInstance(HNewInstance* instruction)
 }
 
 void LocationsBuilderARM64::VisitNot(HNot* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
@@ -6234,7 +6232,7 @@ void InstructionCodeGeneratorARM64::VisitNot(HNot* instruction) {
 }
 
 void LocationsBuilderARM64::VisitBooleanNot(HBooleanNot* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
@@ -6299,7 +6297,7 @@ void InstructionCodeGeneratorARM64::VisitParallelMove(HParallelMove* instruction
 }
 
 void LocationsBuilderARM64::VisitParameterValue(HParameterValue* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   Location location = parameter_visitor_.GetNextLocation(instruction->GetType());
   if (location.IsStackSlot()) {
     location = Location::StackSlot(location.GetStackIndex() + codegen_->GetFrameSize());
@@ -6315,8 +6313,7 @@ void InstructionCodeGeneratorARM64::VisitParameterValue(
 }
 
 void LocationsBuilderARM64::VisitCurrentMethod(HCurrentMethod* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetOut(LocationFrom(kArtMethodRegister));
 }
 
@@ -6326,7 +6323,7 @@ void InstructionCodeGeneratorARM64::VisitCurrentMethod(
 }
 
 void LocationsBuilderARM64::VisitPhi(HPhi* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   for (size_t i = 0, e = locations->GetInputCount(); i < e; ++i) {
     locations->SetInAt(i, Location::Any());
   }
@@ -6342,7 +6339,7 @@ void LocationsBuilderARM64::VisitRem(HRem* rem) {
   LocationSummary::CallKind call_kind =
       DataType::IsFloatingPointType(type) ? LocationSummary::kCallOnMainOnly
                                            : LocationSummary::kNoCall;
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(rem, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, rem, call_kind);
 
   switch (type) {
     case DataType::Type::kInt32:
@@ -6484,7 +6481,7 @@ void InstructionCodeGeneratorARM64::VisitMax(HMax* max) {
 }
 
 void LocationsBuilderARM64::VisitAbs(HAbs* abs) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(abs);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, abs);
   switch (abs->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
@@ -6541,7 +6538,7 @@ void InstructionCodeGeneratorARM64::VisitMemoryBarrier(HMemoryBarrier* memory_ba
 }
 
 void LocationsBuilderARM64::VisitReturn(HReturn* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   DataType::Type return_type = instruction->InputAt(0)->GetType();
   locations->SetInAt(0, ARM64ReturnLocation(return_type));
 }
@@ -6705,8 +6702,8 @@ void InstructionCodeGeneratorARM64::VisitUnresolvedStaticFieldSet(
 }
 
 void LocationsBuilderARM64::VisitSuspendCheck(HSuspendCheck* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnSlowPath);
   // In suspend check slow path, usually there are no caller-save registers at all.
   // If SIMD instructions are present, however, we force spilling all live SIMD
   // registers in full width (since the runtime only saves/restores lower part).
@@ -6733,8 +6730,8 @@ void InstructionCodeGeneratorARM64::VisitSuspendCheck(HSuspendCheck* instruction
 }
 
 void LocationsBuilderARM64::VisitThrow(HThrow* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, LocationFrom(calling_convention.GetRegisterAt(0)));
 }
@@ -6745,8 +6742,7 @@ void InstructionCodeGeneratorARM64::VisitThrow(HThrow* instruction) {
 }
 
 void LocationsBuilderARM64::VisitTypeConversion(HTypeConversion* conversion) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(conversion, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, conversion);
   DataType::Type input_type = conversion->GetInputType();
   DataType::Type result_type = conversion->GetResultType();
   DCHECK(!DataType::IsTypeConversionImplicit(input_type, result_type))
@@ -6839,8 +6835,7 @@ void InstructionCodeGeneratorARM64::VisitBoundType([[maybe_unused]] HBoundType*
 
 // Simple implementation of packed switch - generate cascaded compare/jumps.
 void LocationsBuilderARM64::VisitPackedSwitch(HPackedSwitch* switch_instr) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(switch_instr, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, switch_instr);
   locations->SetInAt(0, Location::RequiresRegister());
 }
 
@@ -7352,8 +7347,7 @@ void CodeGeneratorARM64::GenerateReadBarrierForRootSlow(HInstruction* instructio
 }
 
 void LocationsBuilderARM64::VisitClassTableGet(HClassTableGet* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister());
 }
diff --git a/compiler/optimizing/code_generator_arm64.h b/compiler/optimizing/code_generator_arm64.h
index 81375871b6..6dbfca48ff 100644
--- a/compiler/optimizing/code_generator_arm64.h
+++ b/compiler/optimizing/code_generator_arm64.h
@@ -485,7 +485,7 @@ class InstructionCodeGeneratorARM64 : public InstructionCodeGenerator {
 class LocationsBuilderARM64 : public HGraphVisitor {
  public:
   LocationsBuilderARM64(HGraph* graph, CodeGeneratorARM64* codegen)
-      : HGraphVisitor(graph), codegen_(codegen) {}
+      : HGraphVisitor(graph), codegen_(codegen), allocator_(graph->GetAllocator()) {}
 
 #define DECLARE_VISIT_INSTRUCTION(name, super) \
   void Visit##name(H##name* instr) override;
@@ -510,6 +510,7 @@ class LocationsBuilderARM64 : public HGraphVisitor {
   void HandleShift(HBinaryOperation* instr);
 
   CodeGeneratorARM64* const codegen_;
+  ArenaAllocator* const allocator_;
   InvokeDexCallingConventionVisitorARM64 parameter_visitor_;
 
   DISALLOW_COPY_AND_ASSIGN(LocationsBuilderARM64);
@@ -658,6 +659,12 @@ class CodeGeneratorARM64 : public CodeGenerator {
                      OptimizingCompilerStats* stats = nullptr);
   virtual ~CodeGeneratorARM64() {}
 
+  static void GenerateFrame(Arm64Assembler* assembler,
+                            int32_t frame_size,
+                            vixl::aarch64::CPURegList preserved_core_registers,
+                            vixl::aarch64::CPURegList preserved_fp_registers,
+                            bool requires_current_method);
+
   void GenerateFrameEntry() override;
   void GenerateFrameExit() override;
 
diff --git a/compiler/optimizing/code_generator_arm_vixl.cc b/compiler/optimizing/code_generator_arm_vixl.cc
index a7854ac886..e4b7740341 100644
--- a/compiler/optimizing/code_generator_arm_vixl.cc
+++ b/compiler/optimizing/code_generator_arm_vixl.cc
@@ -2165,8 +2165,8 @@ void CodeGeneratorARMVIXL::ComputeSpillMask() {
 }
 
 void LocationsBuilderARMVIXL::VisitMethodExitHook(HMethodExitHook* method_hook) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(method_hook, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, method_hook, LocationSummary::kCallOnSlowPath);
   locations->SetInAt(0, parameter_visitor_.GetReturnLocation(method_hook->InputAt(0)->GetType()));
   // We need three temporary registers, two to load the timestamp counter (64-bit value) and one to
   // compute the address to store the timestamp counter.
@@ -2253,8 +2253,8 @@ void InstructionCodeGeneratorARMVIXL::VisitMethodExitHook(HMethodExitHook* instr
 }
 
 void LocationsBuilderARMVIXL::VisitMethodEntryHook(HMethodEntryHook* method_hook) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(method_hook, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, method_hook, LocationSummary::kCallOnSlowPath);
   // We need three temporary registers, two to load the timestamp counter (64-bit value) and one to
   // compute the address to store the timestamp counter.
   locations->AddRegisterTemps(3);
@@ -2994,7 +2994,7 @@ void InstructionCodeGeneratorARMVIXL::GenerateTestAndBranch(HInstruction* instru
 }
 
 void LocationsBuilderARMVIXL::VisitIf(HIf* if_instr) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(if_instr);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, if_instr);
   if (IsBooleanValueOrMaterializedCondition(if_instr->InputAt(0))) {
     locations->SetInAt(0, Location::RequiresRegister());
     if (GetGraph()->IsCompilingBaseline() &&
@@ -3046,8 +3046,8 @@ void InstructionCodeGeneratorARMVIXL::VisitIf(HIf* if_instr) {
 }
 
 void LocationsBuilderARMVIXL::VisitDeoptimize(HDeoptimize* deoptimize) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(deoptimize, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, deoptimize, LocationSummary::kCallOnSlowPath);
   InvokeRuntimeCallingConventionARMVIXL calling_convention;
   RegisterSet caller_saves = RegisterSet::Empty();
   caller_saves.Add(LocationFrom(calling_convention.GetRegisterAt(0)));
@@ -3067,8 +3067,7 @@ void InstructionCodeGeneratorARMVIXL::VisitDeoptimize(HDeoptimize* deoptimize) {
 }
 
 void LocationsBuilderARMVIXL::VisitShouldDeoptimizeFlag(HShouldDeoptimizeFlag* flag) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(flag, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, flag);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -3080,7 +3079,7 @@ void InstructionCodeGeneratorARMVIXL::VisitShouldDeoptimizeFlag(HShouldDeoptimiz
 }
 
 void LocationsBuilderARMVIXL::VisitSelect(HSelect* select) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(select);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, select);
   const bool is_floating_point = DataType::IsFloatingPointType(select->GetType());
 
   if (is_floating_point) {
@@ -3243,7 +3242,7 @@ void InstructionCodeGeneratorARMVIXL::VisitSelect(HSelect* select) {
 }
 
 void LocationsBuilderARMVIXL::VisitNop(HNop* nop) {
-  new (GetGraph()->GetAllocator()) LocationSummary(nop);
+  LocationSummary::CreateNoCall(allocator_, nop);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitNop(HNop*) {
@@ -3342,8 +3341,7 @@ void CodeGeneratorARMVIXL::GenerateConditionWithZero(IfCondition condition,
 }
 
 void LocationsBuilderARMVIXL::HandleCondition(HCondition* cond) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(cond, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, cond);
   const DataType::Type type = cond->InputAt(0)->GetType();
   if (DataType::IsFloatingPointType(type)) {
     locations->SetInAt(0, Location::RequiresFpuRegister());
@@ -3486,8 +3484,7 @@ void InstructionCodeGeneratorARMVIXL::VisitAboveOrEqual(HAboveOrEqual* comp) {
 }
 
 void LocationsBuilderARMVIXL::VisitIntConstant(HIntConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -3496,8 +3493,7 @@ void InstructionCodeGeneratorARMVIXL::VisitIntConstant([[maybe_unused]] HIntCons
 }
 
 void LocationsBuilderARMVIXL::VisitNullConstant(HNullConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -3506,8 +3502,7 @@ void InstructionCodeGeneratorARMVIXL::VisitNullConstant([[maybe_unused]] HNullCo
 }
 
 void LocationsBuilderARMVIXL::VisitLongConstant(HLongConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -3516,8 +3511,7 @@ void InstructionCodeGeneratorARMVIXL::VisitLongConstant([[maybe_unused]] HLongCo
 }
 
 void LocationsBuilderARMVIXL::VisitFloatConstant(HFloatConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -3527,8 +3521,7 @@ void InstructionCodeGeneratorARMVIXL::VisitFloatConstant(
 }
 
 void LocationsBuilderARMVIXL::VisitDoubleConstant(HDoubleConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -3563,8 +3556,7 @@ void InstructionCodeGeneratorARMVIXL::VisitReturnVoid([[maybe_unused]] HReturnVo
 }
 
 void LocationsBuilderARMVIXL::VisitReturn(HReturn* ret) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(ret, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, ret);
   locations->SetInAt(0, parameter_visitor_.GetReturnLocation(ret->InputAt(0)->GetType()));
 }
 
@@ -3823,8 +3815,7 @@ void InstructionCodeGeneratorARMVIXL::VisitInvokeCustom(HInvokeCustom* invoke) {
 }
 
 void LocationsBuilderARMVIXL::VisitNeg(HNeg* neg) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(neg, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, neg);
   switch (neg->GetResultType()) {
     case DataType::Type::kInt32: {
       locations->SetInAt(0, Location::RequiresRegister());
@@ -3895,8 +3886,7 @@ void LocationsBuilderARMVIXL::VisitTypeConversion(HTypeConversion* conversion) {
        || (input_type == DataType::Type::kInt64 && result_type == DataType::Type::kFloat32))
       ? LocationSummary::kCallOnMainOnly
       : LocationSummary::kNoCall;
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(conversion, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, conversion, call_kind);
 
   switch (result_type) {
     case DataType::Type::kUint8:
@@ -4259,8 +4249,7 @@ void InstructionCodeGeneratorARMVIXL::VisitTypeConversion(HTypeConversion* conve
 }
 
 void LocationsBuilderARMVIXL::VisitAdd(HAdd* add) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(add, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, add);
   switch (add->GetResultType()) {
     case DataType::Type::kInt32: {
       locations->SetInAt(0, Location::RequiresRegister());
@@ -4324,8 +4313,7 @@ void InstructionCodeGeneratorARMVIXL::VisitAdd(HAdd* add) {
 }
 
 void LocationsBuilderARMVIXL::VisitSub(HSub* sub) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(sub, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, sub);
   switch (sub->GetResultType()) {
     case DataType::Type::kInt32: {
       locations->SetInAt(0, Location::RequiresRegister());
@@ -4386,8 +4374,7 @@ void InstructionCodeGeneratorARMVIXL::VisitSub(HSub* sub) {
 }
 
 void LocationsBuilderARMVIXL::VisitMul(HMul* mul) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(mul, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, mul);
   switch (mul->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:  {
@@ -4685,7 +4672,7 @@ void LocationsBuilderARMVIXL::VisitDiv(HDiv* div) {
     call_kind = LocationSummary::kCallOnMainOnly;
   }
 
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(div, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, div, call_kind);
 
   switch (div->GetResultType()) {
     case DataType::Type::kInt32: {
@@ -4803,7 +4790,7 @@ void LocationsBuilderARMVIXL::VisitRem(HRem* rem) {
     call_kind = LocationSummary::kNoCall;
   }
 
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(rem, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, rem, call_kind);
 
   switch (type) {
     case DataType::Type::kInt32: {
@@ -4925,7 +4912,7 @@ void InstructionCodeGeneratorARMVIXL::VisitRem(HRem* rem) {
 }
 
 static void CreateMinMaxLocations(ArenaAllocator* allocator, HBinaryOperation* minmax) {
-  LocationSummary* locations = new (allocator) LocationSummary(minmax);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, minmax);
   switch (minmax->GetResultType()) {
     case DataType::Type::kInt32:
       locations->SetInAt(0, Location::RequiresRegister());
@@ -5147,7 +5134,7 @@ void InstructionCodeGeneratorARMVIXL::GenerateMinMax(HBinaryOperation* minmax, b
 }
 
 void LocationsBuilderARMVIXL::VisitMin(HMin* min) {
-  CreateMinMaxLocations(GetGraph()->GetAllocator(), min);
+  CreateMinMaxLocations(allocator_, min);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitMin(HMin* min) {
@@ -5155,7 +5142,7 @@ void InstructionCodeGeneratorARMVIXL::VisitMin(HMin* min) {
 }
 
 void LocationsBuilderARMVIXL::VisitMax(HMax* max) {
-  CreateMinMaxLocations(GetGraph()->GetAllocator(), max);
+  CreateMinMaxLocations(allocator_, max);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitMax(HMax* max) {
@@ -5163,7 +5150,7 @@ void InstructionCodeGeneratorARMVIXL::VisitMax(HMax* max) {
 }
 
 void LocationsBuilderARMVIXL::VisitAbs(HAbs* abs) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(abs);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, abs);
   switch (abs->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
@@ -5391,8 +5378,7 @@ void InstructionCodeGeneratorARMVIXL::HandleLongRotate(HBinaryOperation* rotate)
 }
 
 void LocationsBuilderARMVIXL::HandleRotate(HBinaryOperation* rotate) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(rotate, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, rotate);
   HInstruction* shift = rotate->InputAt(1);
   switch (rotate->GetResultType()) {
     case DataType::Type::kInt32: {
@@ -5459,8 +5445,7 @@ void InstructionCodeGeneratorARMVIXL::VisitRor(HRor* ror) {
 void LocationsBuilderARMVIXL::HandleShift(HBinaryOperation* op) {
   DCHECK(op->IsShl() || op->IsShr() || op->IsUShr());
 
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(op, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, op);
 
   HInstruction* shift = op->InputAt(1);
   switch (op->GetResultType()) {
@@ -5694,8 +5679,8 @@ void InstructionCodeGeneratorARMVIXL::VisitUShr(HUShr* ushr) {
 }
 
 void LocationsBuilderARMVIXL::VisitNewInstance(HNewInstance* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConventionARMVIXL calling_convention;
   locations->SetInAt(0, LocationFrom(calling_convention.GetRegisterAt(0)));
   locations->SetOut(LocationFrom(r0));
@@ -5708,8 +5693,8 @@ void InstructionCodeGeneratorARMVIXL::VisitNewInstance(HNewInstance* instruction
 }
 
 void LocationsBuilderARMVIXL::VisitNewArray(HNewArray* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConventionARMVIXL calling_convention;
   locations->SetOut(LocationFrom(r0));
   locations->SetInAt(0, LocationFrom(calling_convention.GetRegisterAt(0)));
@@ -5726,8 +5711,7 @@ void InstructionCodeGeneratorARMVIXL::VisitNewArray(HNewArray* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitParameterValue(HParameterValue* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   Location location = parameter_visitor_.GetNextLocation(instruction->GetType());
   if (location.IsStackSlot()) {
     location = Location::StackSlot(location.GetStackIndex() + codegen_->GetFrameSize());
@@ -5743,8 +5727,7 @@ void InstructionCodeGeneratorARMVIXL::VisitParameterValue(
 }
 
 void LocationsBuilderARMVIXL::VisitCurrentMethod(HCurrentMethod* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetOut(LocationFrom(kMethodRegister));
 }
 
@@ -5754,8 +5737,7 @@ void InstructionCodeGeneratorARMVIXL::VisitCurrentMethod(
 }
 
 void LocationsBuilderARMVIXL::VisitNot(HNot* not_) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(not_, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, not_);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
@@ -5780,8 +5762,7 @@ void InstructionCodeGeneratorARMVIXL::VisitNot(HNot* not_) {
 }
 
 void LocationsBuilderARMVIXL::VisitBooleanNot(HBooleanNot* bool_not) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(bool_not, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, bool_not);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
@@ -5791,8 +5772,7 @@ void InstructionCodeGeneratorARMVIXL::VisitBooleanNot(HBooleanNot* bool_not) {
 }
 
 void LocationsBuilderARMVIXL::VisitCompare(HCompare* compare) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(compare, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, compare);
   switch (compare->GetComparisonType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -5893,8 +5873,7 @@ void InstructionCodeGeneratorARMVIXL::VisitCompare(HCompare* compare) {
 }
 
 void LocationsBuilderARMVIXL::VisitPhi(HPhi* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   for (size_t i = 0, e = locations->GetInputCount(); i < e; ++i) {
     locations->SetInAt(i, Location::Any());
   }
@@ -5972,8 +5951,7 @@ void LocationsBuilderARMVIXL::HandleFieldSet(HInstruction* instruction,
                                              WriteBarrierKind write_barrier_kind) {
   DCHECK(instruction->IsInstanceFieldSet() || instruction->IsStaticFieldSet());
 
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
 
   DataType::Type field_type = field_info.GetFieldType();
@@ -6148,11 +6126,11 @@ void LocationsBuilderARMVIXL::HandleFieldGet(HInstruction* instruction,
 
   bool object_field_get_with_read_barrier =
       (field_info.GetFieldType() == DataType::Type::kReference) && codegen_->EmitReadBarrier();
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction,
-                                                       object_field_get_with_read_barrier
-                                                           ? LocationSummary::kCallOnSlowPath
-                                                           : LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
+      instruction,
+      object_field_get_with_read_barrier ? LocationSummary::kCallOnSlowPath
+                                         : LocationSummary::kNoCall);
   if (object_field_get_with_read_barrier && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -6597,11 +6575,11 @@ void CodeGeneratorARMVIXL::StoreToShiftedRegOffset(DataType::Type type,
 void LocationsBuilderARMVIXL::VisitArrayGet(HArrayGet* instruction) {
   bool object_array_get_with_read_barrier =
       (instruction->GetType() == DataType::Type::kReference) && codegen_->EmitReadBarrier();
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction,
-                                                       object_array_get_with_read_barrier
-                                                           ? LocationSummary::kCallOnSlowPath
-                                                           : LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
+      instruction,
+      object_array_get_with_read_barrier ? LocationSummary::kCallOnSlowPath
+                                         : LocationSummary::kNoCall);
   if (object_array_get_with_read_barrier && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -6890,7 +6868,8 @@ void LocationsBuilderARMVIXL::VisitArraySet(HArraySet* instruction) {
 
   bool needs_type_check = instruction->NeedsTypeCheck();
 
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
       instruction,
       needs_type_check ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall);
 
@@ -7200,8 +7179,7 @@ void InstructionCodeGeneratorARMVIXL::VisitArraySet(HArraySet* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitArrayLength(HArrayLength* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
@@ -7224,8 +7202,7 @@ void InstructionCodeGeneratorARMVIXL::VisitArrayLength(HArrayLength* instruction
 }
 
 void LocationsBuilderARMVIXL::VisitIntermediateAddress(HIntermediateAddress* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
 
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RegisterOrConstant(instruction->GetOffset()));
@@ -7385,8 +7362,8 @@ void InstructionCodeGeneratorARMVIXL::VisitParallelMove(HParallelMove* instructi
 }
 
 void LocationsBuilderARMVIXL::VisitSuspendCheck(HSuspendCheck* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnSlowPath);
   locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
 }
 
@@ -7731,7 +7708,7 @@ void LocationsBuilderARMVIXL::VisitLoadClass(HLoadClass* cls) {
   LocationSummary::CallKind call_kind = (cls->NeedsEnvironment() || requires_read_barrier)
       ? LocationSummary::kCallOnSlowPath
       : LocationSummary::kNoCall;
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(cls, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, cls, call_kind);
   if (kUseBakerReadBarrier && requires_read_barrier && !cls->NeedsEnvironment()) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -7879,7 +7856,7 @@ void InstructionCodeGeneratorARMVIXL::VisitLoadMethodType(HLoadMethodType* load)
 
 void LocationsBuilderARMVIXL::VisitClinitCheck(HClinitCheck* check) {
   LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(check, LocationSummary::kCallOnSlowPath);
+      LocationSummary::Create(allocator_, check, LocationSummary::kCallOnSlowPath);
   locations->SetInAt(0, Location::RequiresRegister());
   if (check->HasUses()) {
     locations->SetOut(Location::SameAsFirstInput());
@@ -7987,7 +7964,7 @@ HLoadString::LoadKind CodeGeneratorARMVIXL::GetSupportedLoadStringKind(
 
 void LocationsBuilderARMVIXL::VisitLoadString(HLoadString* load) {
   LocationSummary::CallKind call_kind = codegen_->GetLoadStringCallKind(load);
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(load, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, load, call_kind);
   HLoadString::LoadKind load_kind = load->GetLoadKind();
   if (load_kind == HLoadString::LoadKind::kRuntimeCall) {
     locations->SetOut(LocationFrom(r0));
@@ -8074,8 +8051,7 @@ static int32_t GetExceptionTlsOffset() {
 }
 
 void LocationsBuilderARMVIXL::VisitLoadException(HLoadException* load) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(load, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, load);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -8086,7 +8062,7 @@ void InstructionCodeGeneratorARMVIXL::VisitLoadException(HLoadException* load) {
 
 
 void LocationsBuilderARMVIXL::VisitClearException(HClearException* clear) {
-  new (GetGraph()->GetAllocator()) LocationSummary(clear, LocationSummary::kNoCall);
+  LocationSummary::CreateNoCall(allocator_, clear);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitClearException([[maybe_unused]] HClearException* clear) {
@@ -8097,8 +8073,8 @@ void InstructionCodeGeneratorARMVIXL::VisitClearException([[maybe_unused]] HClea
 }
 
 void LocationsBuilderARMVIXL::VisitThrow(HThrow* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConventionARMVIXL calling_convention;
   locations->SetInAt(0, LocationFrom(calling_convention.GetRegisterAt(0)));
 }
@@ -8157,8 +8133,7 @@ void LocationsBuilderARMVIXL::VisitInstanceOf(HInstanceOf* instruction) {
       break;
   }
 
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, instruction, call_kind);
   if (baker_read_barrier_slow_path) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -8503,8 +8478,7 @@ void InstructionCodeGeneratorARMVIXL::VisitInstanceOf(HInstanceOf* instruction)
 void LocationsBuilderARMVIXL::VisitCheckCast(HCheckCast* instruction) {
   TypeCheckKind type_check_kind = instruction->GetTypeCheckKind();
   LocationSummary::CallKind call_kind = codegen_->GetCheckCastCallKind(instruction);
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, instruction, call_kind);
   locations->SetInAt(0, Location::RequiresRegister());
   if (type_check_kind == TypeCheckKind::kBitstringCheck) {
     locations->SetInAt(1, Location::ConstantLocation(instruction->InputAt(1)));
@@ -8731,8 +8705,8 @@ void InstructionCodeGeneratorARMVIXL::VisitCheckCast(HCheckCast* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitMonitorOperation(HMonitorOperation* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConventionARMVIXL calling_convention;
   locations->SetInAt(0, LocationFrom(calling_convention.GetRegisterAt(0)));
 }
@@ -8761,8 +8735,7 @@ void LocationsBuilderARMVIXL::VisitXor(HXor* instruction) {
 }
 
 void LocationsBuilderARMVIXL::HandleBitwiseOperation(HBinaryOperation* instruction, Opcode opcode) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   DCHECK(instruction->GetResultType() == DataType::Type::kInt32
          || instruction->GetResultType() == DataType::Type::kInt64);
   // Note: GVN reorders commutative operations to have the constant on the right hand side.
@@ -8784,8 +8757,7 @@ void InstructionCodeGeneratorARMVIXL::VisitXor(HXor* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitBitwiseNegatedRight(HBitwiseNegatedRight* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   DCHECK(instruction->GetResultType() == DataType::Type::kInt32
          || instruction->GetResultType() == DataType::Type::kInt64);
 
@@ -8852,7 +8824,7 @@ void LocationsBuilderARMVIXL::VisitDataProcWithShifterOp(
   DCHECK(instruction->GetType() == DataType::Type::kInt32 ||
          instruction->GetType() == DataType::Type::kInt64);
   LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+      LocationSummary::CreateNoCall(allocator_, instruction);
   const bool overlap = instruction->GetType() == DataType::Type::kInt64 &&
                        HDataProcWithShifterOp::IsExtensionOp(instruction->GetOpKind());
 
@@ -10060,8 +10032,7 @@ VIXLUInt32Literal* CodeGeneratorARMVIXL::DeduplicateUint32Literal(
 }
 
 void LocationsBuilderARMVIXL::VisitMultiplyAccumulate(HMultiplyAccumulate* instr) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instr, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instr);
   locations->SetInAt(HMultiplyAccumulate::kInputAccumulatorIndex,
                      Location::RequiresRegister());
   locations->SetInAt(HMultiplyAccumulate::kInputMulLeftIndex, Location::RequiresRegister());
@@ -10097,8 +10068,7 @@ void InstructionCodeGeneratorARMVIXL::VisitBoundType([[maybe_unused]] HBoundType
 
 // Simple implementation of packed switch - generate cascaded compare/jumps.
 void LocationsBuilderARMVIXL::VisitPackedSwitch(HPackedSwitch* switch_instr) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(switch_instr, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, switch_instr);
   locations->SetInAt(0, Location::RequiresRegister());
   if (switch_instr->GetNumEntries() > kPackedSwitchCompareJumpThreshold &&
       codegen_->GetAssembler()->GetVIXLAssembler()->IsUsingT32()) {
@@ -10211,8 +10181,7 @@ void CodeGeneratorARMVIXL::MoveFromReturnRegister(Location trg, DataType::Type t
 }
 
 void LocationsBuilderARMVIXL::VisitClassTableGet(HClassTableGet* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister());
 }
diff --git a/compiler/optimizing/code_generator_arm_vixl.h b/compiler/optimizing/code_generator_arm_vixl.h
index bbc519fcf9..a8beced582 100644
--- a/compiler/optimizing/code_generator_arm_vixl.h
+++ b/compiler/optimizing/code_generator_arm_vixl.h
@@ -371,7 +371,7 @@ class ParallelMoveResolverARMVIXL : public ParallelMoveResolverWithSwap {
 class LocationsBuilderARMVIXL : public HGraphVisitor {
  public:
   LocationsBuilderARMVIXL(HGraph* graph, CodeGeneratorARMVIXL* codegen)
-      : HGraphVisitor(graph), codegen_(codegen) {}
+      : HGraphVisitor(graph), codegen_(codegen), allocator_(graph->GetAllocator()) {}
 
 #define DECLARE_VISIT_INSTRUCTION(name, super)     \
   void Visit##name(H##name* instr) override;
@@ -403,6 +403,7 @@ class LocationsBuilderARMVIXL : public HGraphVisitor {
   bool CanEncodeConstantAsImmediate(HConstant* input_cst, Opcode opcode);
 
   CodeGeneratorARMVIXL* const codegen_;
+  ArenaAllocator* const allocator_;
   InvokeDexCallingConventionVisitorARMVIXL parameter_visitor_;
 
   DISALLOW_COPY_AND_ASSIGN(LocationsBuilderARMVIXL);
diff --git a/compiler/optimizing/code_generator_riscv64.cc b/compiler/optimizing/code_generator_riscv64.cc
index ba2e307af5..dd1d96b6b4 100644
--- a/compiler/optimizing/code_generator_riscv64.cc
+++ b/compiler/optimizing/code_generator_riscv64.cc
@@ -2097,7 +2097,7 @@ int32_t InstructionCodeGeneratorRISCV64::VecAddress(LocationSummary* locations,
 
 void LocationsBuilderRISCV64::HandleBinaryOp(HBinaryOperation* instruction) {
   DCHECK_EQ(instruction->InputCount(), 2u);
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   DataType::Type type = instruction->GetResultType();
   switch (type) {
     case DataType::Type::kInt32:
@@ -2245,7 +2245,7 @@ void InstructionCodeGeneratorRISCV64::HandleBinaryOp(HBinaryOperation* instructi
 }
 
 void LocationsBuilderRISCV64::HandleCondition(HCondition* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   switch (instruction->InputAt(0)->GetType()) {
     case DataType::Type::kFloat32:
     case DataType::Type::kFloat64:
@@ -2325,7 +2325,7 @@ void LocationsBuilderRISCV64::HandleShift(HBinaryOperation* instruction) {
          instruction->IsRol() ||
          instruction->IsRor());
 
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   DataType::Type type = instruction->GetResultType();
   switch (type) {
     case DataType::Type::kInt32:
@@ -2493,8 +2493,7 @@ void CodeGeneratorRISCV64::CheckGCCardIsValid(XRegister object) {
 }
 
 void LocationsBuilderRISCV64::HandleFieldSet(HInstruction* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, ValueLocationForStore(instruction->InputAt(1)));
 }
@@ -2540,11 +2539,11 @@ void LocationsBuilderRISCV64::HandleFieldGet(HInstruction* instruction) {
 
   bool object_field_get_with_read_barrier =
       (instruction->GetType() == DataType::Type::kReference) && codegen_->EmitReadBarrier();
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
       instruction,
-      object_field_get_with_read_barrier
-          ? LocationSummary::kCallOnSlowPath
-          : LocationSummary::kNoCall);
+      object_field_get_with_read_barrier ? LocationSummary::kCallOnSlowPath
+                                         : LocationSummary::kNoCall);
 
   // Input for object receiver.
   locations->SetInAt(0, Location::RequiresRegister());
@@ -2700,7 +2699,7 @@ void InstructionCodeGeneratorRISCV64::VisitAboveOrEqual(HAboveOrEqual* instructi
 }
 
 void LocationsBuilderRISCV64::VisitAbs(HAbs* abs) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(abs);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, abs);
   switch (abs->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
@@ -2771,10 +2770,11 @@ void LocationsBuilderRISCV64::VisitArrayGet(HArrayGet* instruction) {
   DataType::Type type = instruction->GetType();
   bool object_array_get_with_read_barrier =
       (type == DataType::Type::kReference) && codegen_->EmitReadBarrier();
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(instruction,
-                      object_array_get_with_read_barrier ? LocationSummary::kCallOnSlowPath :
-                                                           LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
+     instruction,
+     object_array_get_with_read_barrier ? LocationSummary::kCallOnSlowPath
+                                        : LocationSummary::kNoCall);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RegisterOrConstant(instruction->InputAt(1)));
   if (DataType::IsFloatingPointType(type)) {
@@ -2899,7 +2899,7 @@ void InstructionCodeGeneratorRISCV64::VisitArrayGet(HArrayGet* instruction) {
 }
 
 void LocationsBuilderRISCV64::VisitArrayLength(HArrayLength* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
@@ -2919,7 +2919,8 @@ void InstructionCodeGeneratorRISCV64::VisitArrayLength(HArrayLength* instruction
 
 void LocationsBuilderRISCV64::VisitArraySet(HArraySet* instruction) {
   bool needs_type_check = instruction->NeedsTypeCheck();
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
       instruction,
       needs_type_check ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall);
   locations->SetInAt(0, Location::RequiresRegister());
@@ -3079,7 +3080,7 @@ void InstructionCodeGeneratorRISCV64::VisitBelowOrEqual(HBelowOrEqual* instructi
 }
 
 void LocationsBuilderRISCV64::VisitBooleanNot(HBooleanNot* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
@@ -3212,8 +3213,7 @@ static size_t NumberOfCheckCastTemps(bool emit_read_barrier, TypeCheckKind type_
 void LocationsBuilderRISCV64::VisitCheckCast(HCheckCast* instruction) {
   TypeCheckKind type_check_kind = instruction->GetTypeCheckKind();
   LocationSummary::CallKind call_kind = codegen_->GetCheckCastCallKind(instruction);
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, instruction, call_kind);
   locations->SetInAt(0, Location::RequiresRegister());
   if (type_check_kind == TypeCheckKind::kBitstringCheck) {
     locations->SetInAt(1, Location::ConstantLocation(instruction->InputAt(1)));
@@ -3421,8 +3421,7 @@ TypeCheckKind type_check_kind = instruction->GetTypeCheckKind();
 }
 
 void LocationsBuilderRISCV64::VisitClassTableGet(HClassTableGet* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
@@ -3448,7 +3447,7 @@ static int32_t GetExceptionTlsOffset() {
 }
 
 void LocationsBuilderRISCV64::VisitClearException(HClearException* instruction) {
-  new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary::CreateNoCall(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorRISCV64::VisitClearException(
@@ -3457,8 +3456,8 @@ void InstructionCodeGeneratorRISCV64::VisitClearException(
 }
 
 void LocationsBuilderRISCV64::VisitClinitCheck(HClinitCheck* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnSlowPath);
   locations->SetInAt(0, Location::RequiresRegister());
   if (instruction->HasUses()) {
     locations->SetOut(Location::SameAsFirstInput());
@@ -3479,7 +3478,7 @@ void InstructionCodeGeneratorRISCV64::VisitClinitCheck(HClinitCheck* instruction
 void LocationsBuilderRISCV64::VisitCompare(HCompare* instruction) {
   DataType::Type compare_type = instruction->GetComparisonType();
 
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
 
   switch (compare_type) {
     case DataType::Type::kBool:
@@ -3585,8 +3584,7 @@ void InstructionCodeGeneratorRISCV64::VisitConstructorFence(
 }
 
 void LocationsBuilderRISCV64::VisitCurrentMethod(HCurrentMethod* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetOut(Location::RegisterLocation(kArtMethodRegister));
 }
 
@@ -3596,8 +3594,7 @@ void InstructionCodeGeneratorRISCV64::VisitCurrentMethod(
 }
 
 void LocationsBuilderRISCV64::VisitShouldDeoptimizeFlag(HShouldDeoptimizeFlag* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -3609,8 +3606,8 @@ void InstructionCodeGeneratorRISCV64::VisitShouldDeoptimizeFlag(
 }
 
 void LocationsBuilderRISCV64::VisitDeoptimize(HDeoptimize* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(instruction, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnSlowPath);
   InvokeRuntimeCallingConvention calling_convention;
   RegisterSet caller_saves = RegisterSet::Empty();
   caller_saves.Add(Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
@@ -3630,8 +3627,7 @@ void InstructionCodeGeneratorRISCV64::VisitDeoptimize(HDeoptimize* instruction)
 }
 
 void LocationsBuilderRISCV64::VisitDiv(HDiv* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   switch (instruction->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
@@ -3708,8 +3704,7 @@ void InstructionCodeGeneratorRISCV64::VisitDivZeroCheck(HDivZeroCheck* instructi
 }
 
 void LocationsBuilderRISCV64::VisitDoubleConstant(HDoubleConstant* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetOut(Location::ConstantLocation(instruction));
 }
 
@@ -3733,8 +3728,7 @@ void LocationsBuilderRISCV64::VisitExit(HExit* instruction) {
 void InstructionCodeGeneratorRISCV64::VisitExit([[maybe_unused]] HExit* instruction) {}
 
 void LocationsBuilderRISCV64::VisitFloatConstant(HFloatConstant* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetOut(Location::ConstantLocation(instruction));
 }
 
@@ -3768,7 +3762,7 @@ void InstructionCodeGeneratorRISCV64::VisitGreaterThanOrEqual(HGreaterThanOrEqua
 }
 
 void LocationsBuilderRISCV64::VisitIf(HIf* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   if (IsBooleanValueOrMaterializedCondition(instruction->InputAt(0))) {
     locations->SetInAt(0, Location::RequiresRegister());
     if (GetGraph()->IsCompilingBaseline() &&
@@ -3875,8 +3869,7 @@ void LocationsBuilderRISCV64::VisitInstanceOf(HInstanceOf* instruction) {
       break;
   }
 
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, instruction, call_kind);
   if (baker_read_barrier_slow_path) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -4109,7 +4102,7 @@ void InstructionCodeGeneratorRISCV64::VisitInstanceOf(HInstanceOf* instruction)
 }
 
 void LocationsBuilderRISCV64::VisitIntConstant(HIntConstant* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetOut(Location::ConstantLocation(instruction));
 }
 
@@ -4206,7 +4199,7 @@ void LocationsBuilderRISCV64::VisitInvokeStaticOrDirect(HInvokeStaticOrDirect* i
   // art::PrepareForRegisterAllocation.
   DCHECK(!instruction->IsStaticWithExplicitClinitCheck());
 
-  IntrinsicLocationsBuilderRISCV64 intrinsic(GetGraph()->GetAllocator(), codegen_);
+  IntrinsicLocationsBuilderRISCV64 intrinsic(allocator_, codegen_);
   if (intrinsic.TryDispatch(instruction)) {
     return;
   }
@@ -4245,7 +4238,7 @@ void InstructionCodeGeneratorRISCV64::VisitInvokeStaticOrDirect(
 }
 
 void LocationsBuilderRISCV64::VisitInvokeVirtual(HInvokeVirtual* instruction) {
-  IntrinsicLocationsBuilderRISCV64 intrinsic(GetGraph()->GetAllocator(), codegen_);
+  IntrinsicLocationsBuilderRISCV64 intrinsic(allocator_, codegen_);
   if (intrinsic.TryDispatch(instruction)) {
     return;
   }
@@ -4263,7 +4256,7 @@ void InstructionCodeGeneratorRISCV64::VisitInvokeVirtual(HInvokeVirtual* instruc
 }
 
 void LocationsBuilderRISCV64::VisitInvokePolymorphic(HInvokePolymorphic* instruction) {
-  IntrinsicLocationsBuilderRISCV64 intrinsic(GetGraph()->GetAllocator(), codegen_);
+  IntrinsicLocationsBuilderRISCV64 intrinsic(allocator_, codegen_);
   if (intrinsic.TryDispatch(instruction)) {
     return;
   }
@@ -4319,8 +4312,7 @@ void LocationsBuilderRISCV64::VisitLoadClass(HLoadClass* instruction) {
   LocationSummary::CallKind call_kind = (instruction->NeedsEnvironment() || requires_read_barrier)
       ? LocationSummary::kCallOnSlowPath
       : LocationSummary::kNoCall;
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, instruction, call_kind);
   if (kUseBakerReadBarrier && requires_read_barrier && !instruction->NeedsEnvironment()) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -4457,8 +4449,7 @@ void InstructionCodeGeneratorRISCV64::VisitLoadClass(HLoadClass* instruction)
 }
 
 void LocationsBuilderRISCV64::VisitLoadException(HLoadException* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -4490,8 +4481,7 @@ void InstructionCodeGeneratorRISCV64::VisitLoadMethodType(HLoadMethodType* instr
 void LocationsBuilderRISCV64::VisitLoadString(HLoadString* instruction) {
   HLoadString::LoadKind load_kind = instruction->GetLoadKind();
   LocationSummary::CallKind call_kind = codegen_->GetLoadStringCallKind(instruction);
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, instruction, call_kind);
   if (load_kind == HLoadString::LoadKind::kRuntimeCall) {
     InvokeRuntimeCallingConvention calling_convention;
     DCHECK_EQ(DataType::Type::kReference, instruction->GetType());
@@ -4582,7 +4572,7 @@ void InstructionCodeGeneratorRISCV64::VisitLoadString(HLoadString* instruction)
 }
 
 void LocationsBuilderRISCV64::VisitLongConstant(HLongConstant* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetOut(Location::ConstantLocation(instruction));
 }
 
@@ -4608,7 +4598,7 @@ void InstructionCodeGeneratorRISCV64::VisitMemoryBarrier(HMemoryBarrier* instruc
 }
 
 void LocationsBuilderRISCV64::VisitMethodEntryHook(HMethodEntryHook* instruction) {
-  new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kCallOnSlowPath);
+  LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnSlowPath);
 }
 
 void InstructionCodeGeneratorRISCV64::VisitMethodEntryHook(HMethodEntryHook* instruction) {
@@ -4618,8 +4608,8 @@ void InstructionCodeGeneratorRISCV64::VisitMethodEntryHook(HMethodEntryHook* ins
 }
 
 void LocationsBuilderRISCV64::VisitMethodExitHook(HMethodExitHook* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(instruction, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnSlowPath);
   DataType::Type return_type = instruction->InputAt(0)->GetType();
   locations->SetInAt(0, Riscv64ReturnLocation(return_type));
 }
@@ -4639,8 +4629,8 @@ void InstructionCodeGeneratorRISCV64::VisitMin(HMin* instruction) {
 }
 
 void LocationsBuilderRISCV64::VisitMonitorOperation(HMonitorOperation* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
 }
@@ -4656,8 +4646,7 @@ void InstructionCodeGeneratorRISCV64::VisitMonitorOperation(HMonitorOperation* i
 }
 
 void LocationsBuilderRISCV64::VisitMul(HMul* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   switch (instruction->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
@@ -4707,7 +4696,7 @@ void InstructionCodeGeneratorRISCV64::VisitMul(HMul* instruction) {
 }
 
 void LocationsBuilderRISCV64::VisitNeg(HNeg* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   switch (instruction->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
@@ -4752,8 +4741,8 @@ void InstructionCodeGeneratorRISCV64::VisitNeg(HNeg* instruction) {
 }
 
 void LocationsBuilderRISCV64::VisitNewArray(HNewArray* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetOut(calling_convention.GetReturnLocation(DataType::Type::kReference));
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
@@ -4768,8 +4757,8 @@ void InstructionCodeGeneratorRISCV64::VisitNewArray(HNewArray* instruction) {
 }
 
 void LocationsBuilderRISCV64::VisitNewInstance(HNewInstance* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
   locations->SetOut(calling_convention.GetReturnLocation(DataType::Type::kReference));
@@ -4781,7 +4770,7 @@ void InstructionCodeGeneratorRISCV64::VisitNewInstance(HNewInstance* instruction
 }
 
 void LocationsBuilderRISCV64::VisitNop(HNop* instruction) {
-  new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary::CreateNoCall(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorRISCV64::VisitNop([[maybe_unused]] HNop* instruction) {
@@ -4789,7 +4778,7 @@ void InstructionCodeGeneratorRISCV64::VisitNop([[maybe_unused]] HNop* instructio
 }
 
 void LocationsBuilderRISCV64::VisitNot(HNot* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
@@ -4817,7 +4806,7 @@ void InstructionCodeGeneratorRISCV64::VisitNotEqual(HNotEqual* instruction) {
 }
 
 void LocationsBuilderRISCV64::VisitNullConstant(HNullConstant* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetOut(Location::ConstantLocation(instruction));
 }
 
@@ -4844,8 +4833,7 @@ void InstructionCodeGeneratorRISCV64::VisitOr(HOr* instruction) {
 }
 
 void LocationsBuilderRISCV64::VisitPackedSwitch(HPackedSwitch* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
 }
 
@@ -4900,7 +4888,7 @@ void InstructionCodeGeneratorRISCV64::VisitParallelMove(HParallelMove* instructi
 }
 
 void LocationsBuilderRISCV64::VisitParameterValue(HParameterValue* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   Location location = parameter_visitor_.GetNextLocation(instruction->GetType());
   if (location.IsStackSlot()) {
     location = Location::StackSlot(location.GetStackIndex() + codegen_->GetFrameSize());
@@ -4916,7 +4904,7 @@ void InstructionCodeGeneratorRISCV64::VisitParameterValue(
 }
 
 void LocationsBuilderRISCV64::VisitPhi(HPhi* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   for (size_t i = 0, e = locations->GetInputCount(); i < e; ++i) {
     locations->SetInAt(i, Location::Any());
   }
@@ -4932,8 +4920,7 @@ void LocationsBuilderRISCV64::VisitRem(HRem* instruction) {
   LocationSummary::CallKind call_kind =
       DataType::IsFloatingPointType(type) ? LocationSummary::kCallOnMainOnly
                                           : LocationSummary::kNoCall;
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, instruction, call_kind);
 
   switch (type) {
     case DataType::Type::kInt32:
@@ -4986,7 +4973,7 @@ void InstructionCodeGeneratorRISCV64::VisitRem(HRem* instruction) {
 }
 
 void LocationsBuilderRISCV64::VisitReturn(HReturn* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   DataType::Type return_type = instruction->InputAt(0)->GetType();
   DCHECK_NE(return_type, DataType::Type::kVoid);
   locations->SetInAt(0, Riscv64ReturnLocation(return_type));
@@ -5137,7 +5124,7 @@ void InstructionCodeGeneratorRISCV64::VisitUnresolvedStaticFieldSet(
 }
 
 void LocationsBuilderRISCV64::VisitSelect(HSelect* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   if (DataType::IsFloatingPointType(instruction->GetType())) {
     locations->SetInAt(0, FpuRegisterOrZeroBitPatternLocation(instruction->GetFalseValue()));
     locations->SetInAt(1, FpuRegisterOrZeroBitPatternLocation(instruction->GetTrueValue()));
@@ -5243,8 +5230,8 @@ void InstructionCodeGeneratorRISCV64::VisitSub(HSub* instruction) {
 }
 
 void LocationsBuilderRISCV64::VisitSuspendCheck(HSuspendCheck* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(instruction, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnSlowPath);
   // In suspend check slow path, usually there are no caller-save registers at all.
   // If SIMD instructions are present, however, we force spilling all live SIMD
   // registers in full width (since the runtime only saves/restores lower part).
@@ -5267,8 +5254,8 @@ void InstructionCodeGeneratorRISCV64::VisitSuspendCheck(HSuspendCheck* instructi
 }
 
 void LocationsBuilderRISCV64::VisitThrow(HThrow* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
 }
@@ -5300,7 +5287,7 @@ void LocationsBuilderRISCV64::VisitTypeConversion(HTypeConversion* instruction)
     LOG(FATAL) << "Unexpected type conversion from " << input_type << " to " << result_type;
   }
 
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
 
   if (DataType::IsFloatingPointType(input_type)) {
     locations->SetInAt(0, Location::RequiresFpuRegister());
@@ -5431,7 +5418,7 @@ void LocationsBuilderRISCV64::VisitRiscv64ShiftAdd(HRiscv64ShiftAdd* instruction
   DCHECK_EQ(instruction->GetType(), DataType::Type::kInt64)
       << "Unexpected ShiftAdd type: " << instruction->GetType();
 
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
@@ -5465,7 +5452,7 @@ void LocationsBuilderRISCV64::VisitBitwiseNegatedRight(HBitwiseNegatedRight* ins
   DCHECK(codegen_->GetInstructionSetFeatures().HasZbb());
   DCHECK(DataType::IsIntegralType(instruction->GetType())) << instruction->GetType();
 
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
diff --git a/compiler/optimizing/code_generator_riscv64.h b/compiler/optimizing/code_generator_riscv64.h
index dc88296be2..be7b96d7a9 100644
--- a/compiler/optimizing/code_generator_riscv64.h
+++ b/compiler/optimizing/code_generator_riscv64.h
@@ -234,7 +234,7 @@ class FieldAccessCallingConventionRISCV64 : public FieldAccessCallingConvention
 class LocationsBuilderRISCV64 : public HGraphVisitor {
  public:
   LocationsBuilderRISCV64(HGraph* graph, CodeGeneratorRISCV64* codegen)
-      : HGraphVisitor(graph), codegen_(codegen) {}
+      : HGraphVisitor(graph), codegen_(codegen), allocator_(graph->GetAllocator()) {}
 
 #define DECLARE_VISIT_INSTRUCTION(name, super) void Visit##name(H##name* instr) override;
 
@@ -256,9 +256,9 @@ class LocationsBuilderRISCV64 : public HGraphVisitor {
   void HandleFieldSet(HInstruction* instruction);
   void HandleFieldGet(HInstruction* instruction);
 
-  InvokeDexCallingConventionVisitorRISCV64 parameter_visitor_;
-
   CodeGeneratorRISCV64* const codegen_;
+  ArenaAllocator* const allocator_;
+  InvokeDexCallingConventionVisitorRISCV64 parameter_visitor_;
 
   DISALLOW_COPY_AND_ASSIGN(LocationsBuilderRISCV64);
 };
diff --git a/compiler/optimizing/code_generator_vector_arm64_neon.cc b/compiler/optimizing/code_generator_vector_arm64_neon.cc
index 53a2ec720e..ce5952b572 100644
--- a/compiler/optimizing/code_generator_vector_arm64_neon.cc
+++ b/compiler/optimizing/code_generator_vector_arm64_neon.cc
@@ -75,7 +75,7 @@ static bool ShouldEmitDotProductInstructions(const CodeGeneratorARM64* codegen_)
 }
 
 void LocationsBuilderARM64Neon::VisitVecReplicateScalar(HVecReplicateScalar* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   HInstruction* input = instruction->InputAt(0);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
@@ -168,7 +168,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecReplicateScalar(HVecReplicateSca
 }
 
 void LocationsBuilderARM64Neon::VisitVecExtractScalar(HVecExtractScalar* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -217,7 +217,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecExtractScalar(HVecExtractScalar*
 
 // Helper to set up locations for vector unary operations.
 static void CreateVecUnOpLocations(ArenaAllocator* allocator, HVecUnaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
       locations->SetInAt(0, Location::RequiresFpuRegister());
@@ -243,7 +243,7 @@ static void CreateVecUnOpLocations(ArenaAllocator* allocator, HVecUnaryOperation
 }
 
 void LocationsBuilderARM64Neon::VisitVecReduce(HVecReduce* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecReduce(HVecReduce* instruction) {
@@ -283,7 +283,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecReduce(HVecReduce* instruction)
 }
 
 void LocationsBuilderARM64Neon::VisitVecCnv(HVecCnv* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecCnv(HVecCnv* instruction) {
@@ -301,7 +301,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecCnv(HVecCnv* instruction) {
 }
 
 void LocationsBuilderARM64Neon::VisitVecNeg(HVecNeg* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecNeg(HVecNeg* instruction) {
@@ -342,7 +342,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecNeg(HVecNeg* instruction) {
 }
 
 void LocationsBuilderARM64Neon::VisitVecAbs(HVecAbs* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecAbs(HVecAbs* instruction) {
@@ -381,7 +381,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecAbs(HVecAbs* instruction) {
 }
 
 void LocationsBuilderARM64Neon::VisitVecNot(HVecNot* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecNot(HVecNot* instruction) {
@@ -410,7 +410,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecNot(HVecNot* instruction) {
 
 // Helper to set up locations for vector binary operations.
 static void CreateVecBinOpLocations(ArenaAllocator* allocator, HVecBinaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -432,7 +432,7 @@ static void CreateVecBinOpLocations(ArenaAllocator* allocator, HVecBinaryOperati
 }
 
 void LocationsBuilderARM64Neon::VisitVecAdd(HVecAdd* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecAdd(HVecAdd* instruction) {
@@ -474,7 +474,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecAdd(HVecAdd* instruction) {
 }
 
 void LocationsBuilderARM64Neon::VisitVecSaturationAdd(HVecSaturationAdd* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecSaturationAdd(HVecSaturationAdd* instruction) {
@@ -506,7 +506,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecSaturationAdd(HVecSaturationAdd*
 }
 
 void LocationsBuilderARM64Neon::VisitVecHalvingAdd(HVecHalvingAdd* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecHalvingAdd(HVecHalvingAdd* instruction) {
@@ -546,7 +546,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecHalvingAdd(HVecHalvingAdd* instr
 }
 
 void LocationsBuilderARM64Neon::VisitVecSub(HVecSub* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecSub(HVecSub* instruction) {
@@ -588,7 +588,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecSub(HVecSub* instruction) {
 }
 
 void LocationsBuilderARM64Neon::VisitVecSaturationSub(HVecSaturationSub* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecSaturationSub(HVecSaturationSub* instruction) {
@@ -620,7 +620,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecSaturationSub(HVecSaturationSub*
 }
 
 void LocationsBuilderARM64Neon::VisitVecMul(HVecMul* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecMul(HVecMul* instruction) {
@@ -658,7 +658,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecMul(HVecMul* instruction) {
 }
 
 void LocationsBuilderARM64Neon::VisitVecDiv(HVecDiv* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecDiv(HVecDiv* instruction) {
@@ -682,7 +682,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecDiv(HVecDiv* instruction) {
 }
 
 void LocationsBuilderARM64Neon::VisitVecMin(HVecMin* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecMin(HVecMin* instruction) {
@@ -730,7 +730,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecMin(HVecMin* instruction) {
 }
 
 void LocationsBuilderARM64Neon::VisitVecMax(HVecMax* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecMax(HVecMax* instruction) {
@@ -779,7 +779,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecMax(HVecMax* instruction) {
 
 void LocationsBuilderARM64Neon::VisitVecAnd(HVecAnd* instruction) {
   // TODO: Allow constants supported by BIC (vector, immediate).
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecAnd(HVecAnd* instruction) {
@@ -815,7 +815,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecAndNot(HVecAndNot* instruction)
 }
 
 void LocationsBuilderARM64Neon::VisitVecOr(HVecOr* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecOr(HVecOr* instruction) {
@@ -842,7 +842,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecOr(HVecOr* instruction) {
 }
 
 void LocationsBuilderARM64Neon::VisitVecXor(HVecXor* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecXor(HVecXor* instruction) {
@@ -870,7 +870,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecXor(HVecXor* instruction) {
 
 // Helper to set up locations for vector shift operations.
 static void CreateVecShiftLocations(ArenaAllocator* allocator, HVecBinaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
@@ -889,7 +889,7 @@ static void CreateVecShiftLocations(ArenaAllocator* allocator, HVecBinaryOperati
 }
 
 void LocationsBuilderARM64Neon::VisitVecShl(HVecShl* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecShl(HVecShl* instruction) {
@@ -923,7 +923,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecShl(HVecShl* instruction) {
 }
 
 void LocationsBuilderARM64Neon::VisitVecShr(HVecShr* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecShr(HVecShr* instruction) {
@@ -957,7 +957,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecShr(HVecShr* instruction) {
 }
 
 void LocationsBuilderARM64Neon::VisitVecUShr(HVecUShr* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecUShr(HVecUShr* instruction) {
@@ -991,7 +991,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecUShr(HVecUShr* instruction) {
 }
 
 void LocationsBuilderARM64Neon::VisitVecSetScalars(HVecSetScalars* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
 
   DCHECK_EQ(1u, instruction->InputCount());  // only one input currently implemented
 
@@ -1065,7 +1065,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecSetScalars(HVecSetScalars* instr
 
 // Helper to set up locations for vector accumulations.
 static void CreateVecAccumLocations(ArenaAllocator* allocator, HVecOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
@@ -1085,7 +1085,7 @@ static void CreateVecAccumLocations(ArenaAllocator* allocator, HVecOperation* in
 }
 
 void LocationsBuilderARM64Neon::VisitVecMultiplyAccumulate(HVecMultiplyAccumulate* instruction) {
-  CreateVecAccumLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecAccumLocations(allocator_, instruction);
 }
 
 // Some early revisions of the Cortex-A53 have an erratum (835769) whereby it is possible for a
@@ -1133,7 +1133,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecMultiplyAccumulate(HVecMultiplyA
 }
 
 void LocationsBuilderARM64Neon::VisitVecSADAccumulate(HVecSADAccumulate* instruction) {
-  CreateVecAccumLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecAccumLocations(allocator_, instruction);
   // Some conversions require temporary registers.
   LocationSummary* locations = instruction->GetLocations();
   HVecOperation* a = instruction->InputAt(1)->AsVecOperation();
@@ -1315,7 +1315,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecSADAccumulate(HVecSADAccumulate*
 }
 
 void LocationsBuilderARM64Neon::VisitVecDotProd(HVecDotProd* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   DCHECK(instruction->GetPackedType() == DataType::Type::kInt32);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetInAt(1, Location::RequiresFpuRegister());
@@ -1394,7 +1394,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecDotProd(HVecDotProd* instruction
 static void CreateVecMemLocations(ArenaAllocator* allocator,
                                   HVecMemoryOperation* instruction,
                                   bool is_load) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -1420,7 +1420,7 @@ static void CreateVecMemLocations(ArenaAllocator* allocator,
 }
 
 void LocationsBuilderARM64Neon::VisitVecLoad(HVecLoad* instruction) {
-  CreateVecMemLocations(GetGraph()->GetAllocator(), instruction, /*is_load*/ true);
+  CreateVecMemLocations(allocator_, instruction, /*is_load*/ true);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecLoad(HVecLoad* instruction) {
@@ -1480,7 +1480,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecLoad(HVecLoad* instruction) {
 }
 
 void LocationsBuilderARM64Neon::VisitVecStore(HVecStore* instruction) {
-  CreateVecMemLocations(GetGraph()->GetAllocator(), instruction, /*is_load*/ false);
+  CreateVecMemLocations(allocator_, instruction, /*is_load*/ false);
 }
 
 void InstructionCodeGeneratorARM64Neon::VisitVecStore(HVecStore* instruction) {
@@ -1512,7 +1512,7 @@ void InstructionCodeGeneratorARM64Neon::VisitVecStore(HVecStore* instruction) {
 }
 
 void LocationsBuilderARM64Neon::VisitVecPredSetAll(HVecPredSetAll* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   DCHECK(instruction->InputAt(0)->IsIntConstant());
   locations->SetInAt(0, Location::NoLocation());
   locations->SetOut(Location::NoLocation());
diff --git a/compiler/optimizing/code_generator_vector_arm64_sve.cc b/compiler/optimizing/code_generator_vector_arm64_sve.cc
index 3d9bd9187f..8a1ddb5b71 100644
--- a/compiler/optimizing/code_generator_vector_arm64_sve.cc
+++ b/compiler/optimizing/code_generator_vector_arm64_sve.cc
@@ -75,7 +75,7 @@ void InstructionCodeGeneratorARM64Sve::ValidateVectorLength(HVecOperation* instr
 }
 
 void LocationsBuilderARM64Sve::VisitVecReplicateScalar(HVecReplicateScalar* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   HInstruction* input = instruction->InputAt(0);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
@@ -164,7 +164,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecReplicateScalar(HVecReplicateScal
 }
 
 void LocationsBuilderARM64Sve::VisitVecExtractScalar(HVecExtractScalar* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -211,7 +211,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecExtractScalar(HVecExtractScalar*
 
 // Helper to set up locations for vector unary operations.
 static void CreateVecUnOpLocations(ArenaAllocator* allocator, HVecUnaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
       locations->SetInAt(0, Location::RequiresFpuRegister());
@@ -237,7 +237,7 @@ static void CreateVecUnOpLocations(ArenaAllocator* allocator, HVecUnaryOperation
 }
 
 void LocationsBuilderARM64Sve::VisitVecReduce(HVecReduce* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecReduce(HVecReduce* instruction) {
@@ -275,7 +275,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecReduce(HVecReduce* instruction) {
 }
 
 void LocationsBuilderARM64Sve::VisitVecCnv(HVecCnv* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecCnv(HVecCnv* instruction) {
@@ -295,7 +295,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecCnv(HVecCnv* instruction) {
 }
 
 void LocationsBuilderARM64Sve::VisitVecNeg(HVecNeg* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecNeg(HVecNeg* instruction) {
@@ -333,7 +333,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecNeg(HVecNeg* instruction) {
 }
 
 void LocationsBuilderARM64Sve::VisitVecAbs(HVecAbs* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecAbs(HVecAbs* instruction) {
@@ -369,7 +369,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecAbs(HVecAbs* instruction) {
 }
 
 void LocationsBuilderARM64Sve::VisitVecNot(HVecNot* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecNot(HVecNot* instruction) {
@@ -406,7 +406,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecNot(HVecNot* instruction) {
 
 // Helper to set up locations for vector binary operations.
 static void CreateVecBinOpLocations(ArenaAllocator* allocator, HVecBinaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -428,7 +428,7 @@ static void CreateVecBinOpLocations(ArenaAllocator* allocator, HVecBinaryOperati
 }
 
 void LocationsBuilderARM64Sve::VisitVecAdd(HVecAdd* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecAdd(HVecAdd* instruction) {
@@ -487,7 +487,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecHalvingAdd(HVecHalvingAdd* instru
 }
 
 void LocationsBuilderARM64Sve::VisitVecSub(HVecSub* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecSub(HVecSub* instruction) {
@@ -536,7 +536,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecSaturationSub(HVecSaturationSub*
 }
 
 void LocationsBuilderARM64Sve::VisitVecMul(HVecMul* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecMul(HVecMul* instruction) {
@@ -575,7 +575,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecMul(HVecMul* instruction) {
 }
 
 void LocationsBuilderARM64Sve::VisitVecDiv(HVecDiv* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecDiv(HVecDiv* instruction) {
@@ -623,7 +623,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecMax(HVecMax* instruction) {
 
 void LocationsBuilderARM64Sve::VisitVecAnd(HVecAnd* instruction) {
   // TODO: Allow constants supported by BIC (vector, immediate).
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecAnd(HVecAnd* instruction) {
@@ -668,7 +668,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecAndNot(HVecAndNot* instruction) {
 }
 
 void LocationsBuilderARM64Sve::VisitVecOr(HVecOr* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecOr(HVecOr* instruction) {
@@ -704,7 +704,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecOr(HVecOr* instruction) {
 }
 
 void LocationsBuilderARM64Sve::VisitVecXor(HVecXor* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecXor(HVecXor* instruction) {
@@ -741,7 +741,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecXor(HVecXor* instruction) {
 
 // Helper to set up locations for vector shift operations.
 static void CreateVecShiftLocations(ArenaAllocator* allocator, HVecBinaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
@@ -760,7 +760,7 @@ static void CreateVecShiftLocations(ArenaAllocator* allocator, HVecBinaryOperati
 }
 
 void LocationsBuilderARM64Sve::VisitVecShl(HVecShl* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecShl(HVecShl* instruction) {
@@ -793,7 +793,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecShl(HVecShl* instruction) {
 }
 
 void LocationsBuilderARM64Sve::VisitVecShr(HVecShr* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecShr(HVecShr* instruction) {
@@ -826,7 +826,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecShr(HVecShr* instruction) {
 }
 
 void LocationsBuilderARM64Sve::VisitVecUShr(HVecUShr* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecUShr(HVecUShr* instruction) {
@@ -859,7 +859,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecUShr(HVecUShr* instruction) {
 }
 
 void LocationsBuilderARM64Sve::VisitVecSetScalars(HVecSetScalars* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
 
   DCHECK_EQ(2u, instruction->InputCount());  // only one input currently implemented + predicate.
 
@@ -932,7 +932,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecSetScalars(HVecSetScalars* instru
 
 // Helper to set up locations for vector accumulations.
 static void CreateVecAccumLocations(ArenaAllocator* allocator, HVecOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
@@ -952,7 +952,7 @@ static void CreateVecAccumLocations(ArenaAllocator* allocator, HVecOperation* in
 }
 
 void LocationsBuilderARM64Sve::VisitVecMultiplyAccumulate(HVecMultiplyAccumulate* instruction) {
-  CreateVecAccumLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecAccumLocations(allocator_, instruction);
 }
 
 // Some early revisions of the Cortex-A53 have an erratum (835769) whereby it is possible for a
@@ -1011,7 +1011,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecSADAccumulate(HVecSADAccumulate*
 }
 
 void LocationsBuilderARM64Sve::VisitVecDotProd(HVecDotProd* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   DCHECK(instruction->GetPackedType() == DataType::Type::kInt32);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetInAt(1, Location::RequiresFpuRegister());
@@ -1062,7 +1062,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecDotProd(HVecDotProd* instruction)
 static void CreateVecMemLocations(ArenaAllocator* allocator,
                                   HVecMemoryOperation* instruction,
                                   bool is_load) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -1088,7 +1088,7 @@ static void CreateVecMemLocations(ArenaAllocator* allocator,
 }
 
 void LocationsBuilderARM64Sve::VisitVecLoad(HVecLoad* instruction) {
-  CreateVecMemLocations(GetGraph()->GetAllocator(), instruction, /*is_load*/ true);
+  CreateVecMemLocations(allocator_, instruction, /*is_load*/ true);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecLoad(HVecLoad* instruction) {
@@ -1130,7 +1130,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecLoad(HVecLoad* instruction) {
 }
 
 void LocationsBuilderARM64Sve::VisitVecStore(HVecStore* instruction) {
-  CreateVecMemLocations(GetGraph()->GetAllocator(), instruction, /*is_load*/ false);
+  CreateVecMemLocations(allocator_, instruction, /*is_load*/ false);
 }
 
 void InstructionCodeGeneratorARM64Sve::VisitVecStore(HVecStore* instruction) {
@@ -1172,7 +1172,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecStore(HVecStore* instruction) {
 }
 
 void LocationsBuilderARM64Sve::VisitVecPredSetAll(HVecPredSetAll* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   DCHECK(instruction->InputAt(0)->IsIntConstant());
   locations->SetInAt(0, Location::NoLocation());
   locations->SetOut(Location::NoLocation());
@@ -1250,7 +1250,7 @@ void InstructionCodeGeneratorARM64Sve::GenerateIntegerVecComparison(
 }
 
 void LocationsBuilderARM64Sve::HandleVecCondition(HVecCondition* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetInAt(1, Location::RequiresFpuRegister());
   locations->SetOut(Location::RequiresRegister());
@@ -1327,7 +1327,7 @@ FOR_EACH_VEC_CONDITION_INSTRUCTION(DEFINE_VEC_CONDITION_VISITORS)
 #undef FOR_EACH_VEC_CONDITION_INSTRUCTION
 
 void LocationsBuilderARM64Sve::VisitVecPredNot(HVecPredNot* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   DCHECK(instruction->InputAt(0)->IsVecPredSetOperation());
   locations->SetInAt(0, Location::NoLocation());
   locations->SetOut(Location::RequiresRegister());
@@ -1345,7 +1345,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecPredNot(HVecPredNot* instruction)
 }
 
 void LocationsBuilderARM64Sve::VisitVecPredWhile(HVecPredWhile* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   // The instruction doesn't really need a core register as out location; this is a hack
@@ -1401,7 +1401,7 @@ void InstructionCodeGeneratorARM64Sve::VisitVecPredWhile(HVecPredWhile* instruct
 }
 
 void LocationsBuilderARM64Sve::VisitVecPredToBoolean(HVecPredToBoolean* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::NoLocation());
   // Result of the operation - a boolean value in a core register.
   locations->SetOut(Location::RequiresRegister());
diff --git a/compiler/optimizing/code_generator_vector_arm_vixl.cc b/compiler/optimizing/code_generator_vector_arm_vixl.cc
index 5cf34dd500..3384f4139f 100644
--- a/compiler/optimizing/code_generator_vector_arm_vixl.cc
+++ b/compiler/optimizing/code_generator_vector_arm_vixl.cc
@@ -34,7 +34,7 @@ using helpers::RegisterFrom;
 #define __ GetVIXLAssembler()->
 
 void LocationsBuilderARMVIXL::VisitVecReplicateScalar(HVecReplicateScalar* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -77,7 +77,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecReplicateScalar(HVecReplicateScala
 }
 
 void LocationsBuilderARMVIXL::VisitVecExtractScalar(HVecExtractScalar* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kInt32:
       locations->SetInAt(0, Location::RequiresFpuRegister());
@@ -105,7 +105,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecExtractScalar(HVecExtractScalar* i
 
 // Helper to set up locations for vector unary operations.
 static void CreateVecUnOpLocations(ArenaAllocator* allocator, HVecUnaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
       locations->SetInAt(0, Location::RequiresFpuRegister());
@@ -128,7 +128,7 @@ static void CreateVecUnOpLocations(ArenaAllocator* allocator, HVecUnaryOperation
 }
 
 void LocationsBuilderARMVIXL::VisitVecReduce(HVecReduce* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecReduce(HVecReduce* instruction) {
@@ -157,7 +157,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecReduce(HVecReduce* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecCnv(HVecCnv* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecCnv(HVecCnv* instruction) {
@@ -165,7 +165,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecCnv(HVecCnv* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecNeg(HVecNeg* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecNeg(HVecNeg* instruction) {
@@ -194,7 +194,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecNeg(HVecNeg* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecAbs(HVecAbs* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecAbs(HVecAbs* instruction) {
@@ -221,7 +221,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecAbs(HVecAbs* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecNot(HVecNot* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecNot(HVecNot* instruction) {
@@ -249,7 +249,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecNot(HVecNot* instruction) {
 
 // Helper to set up locations for vector binary operations.
 static void CreateVecBinOpLocations(ArenaAllocator* allocator, HVecBinaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -268,7 +268,7 @@ static void CreateVecBinOpLocations(ArenaAllocator* allocator, HVecBinaryOperati
 }
 
 void LocationsBuilderARMVIXL::VisitVecAdd(HVecAdd* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecAdd(HVecAdd* instruction) {
@@ -298,7 +298,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecAdd(HVecAdd* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecSaturationAdd(HVecSaturationAdd* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecSaturationAdd(HVecSaturationAdd* instruction) {
@@ -330,7 +330,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecSaturationAdd(HVecSaturationAdd* i
 }
 
 void LocationsBuilderARMVIXL::VisitVecHalvingAdd(HVecHalvingAdd* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecHalvingAdd(HVecHalvingAdd* instruction) {
@@ -370,7 +370,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecHalvingAdd(HVecHalvingAdd* instruc
 }
 
 void LocationsBuilderARMVIXL::VisitVecSub(HVecSub* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecSub(HVecSub* instruction) {
@@ -400,7 +400,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecSub(HVecSub* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecSaturationSub(HVecSaturationSub* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecSaturationSub(HVecSaturationSub* instruction) {
@@ -432,7 +432,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecSaturationSub(HVecSaturationSub* i
 }
 
 void LocationsBuilderARMVIXL::VisitVecMul(HVecMul* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecMul(HVecMul* instruction) {
@@ -462,7 +462,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecMul(HVecMul* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecDiv(HVecDiv* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecDiv(HVecDiv* instruction) {
@@ -470,7 +470,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecDiv(HVecDiv* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecMin(HVecMin* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecMin(HVecMin* instruction) {
@@ -510,7 +510,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecMin(HVecMin* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecMax(HVecMax* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecMax(HVecMax* instruction) {
@@ -551,7 +551,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecMax(HVecMax* instruction) {
 
 void LocationsBuilderARMVIXL::VisitVecAnd(HVecAnd* instruction) {
   // TODO: Allow constants supported by VAND (immediate).
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecAnd(HVecAnd* instruction) {
@@ -575,7 +575,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecAnd(HVecAnd* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecAndNot(HVecAndNot* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecAndNot(HVecAndNot* instruction) {
@@ -583,7 +583,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecAndNot(HVecAndNot* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecOr(HVecOr* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecOr(HVecOr* instruction) {
@@ -607,7 +607,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecOr(HVecOr* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecXor(HVecXor* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecXor(HVecXor* instruction) {
@@ -632,7 +632,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecXor(HVecXor* instruction) {
 
 // Helper to set up locations for vector shift operations.
 static void CreateVecShiftLocations(ArenaAllocator* allocator, HVecBinaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
@@ -650,7 +650,7 @@ static void CreateVecShiftLocations(ArenaAllocator* allocator, HVecBinaryOperati
 }
 
 void LocationsBuilderARMVIXL::VisitVecShl(HVecShl* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecShl(HVecShl* instruction) {
@@ -680,7 +680,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecShl(HVecShl* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecShr(HVecShr* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecShr(HVecShr* instruction) {
@@ -710,7 +710,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecShr(HVecShr* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecUShr(HVecUShr* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecUShr(HVecUShr* instruction) {
@@ -740,7 +740,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecUShr(HVecUShr* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecSetScalars(HVecSetScalars* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
 
   DCHECK_EQ(1u, instruction->InputCount());  // only one input currently implemented
 
@@ -787,7 +787,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecSetScalars(HVecSetScalars* instruc
 
 // Helper to set up locations for vector accumulations.
 static void CreateVecAccumLocations(ArenaAllocator* allocator, HVecOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
@@ -807,7 +807,7 @@ static void CreateVecAccumLocations(ArenaAllocator* allocator, HVecOperation* in
 }
 
 void LocationsBuilderARMVIXL::VisitVecMultiplyAccumulate(HVecMultiplyAccumulate* instruction) {
-  CreateVecAccumLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecAccumLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecMultiplyAccumulate(HVecMultiplyAccumulate* instruction) {
@@ -815,7 +815,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecMultiplyAccumulate(HVecMultiplyAcc
 }
 
 void LocationsBuilderARMVIXL::VisitVecSADAccumulate(HVecSADAccumulate* instruction) {
-  CreateVecAccumLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecAccumLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecSADAccumulate(HVecSADAccumulate* instruction) {
@@ -872,7 +872,7 @@ static bool IsWordAligned(HVecMemoryOperation* instruction) {
 static void CreateVecMemLocations(ArenaAllocator* allocator,
                                   HVecMemoryOperation* instruction,
                                   bool is_load) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -950,7 +950,7 @@ AlignedMemOperand InstructionCodeGeneratorARMVIXL::VecAddressUnaligned(
 }
 
 void LocationsBuilderARMVIXL::VisitVecLoad(HVecLoad* instruction) {
-  CreateVecMemLocations(GetGraph()->GetAllocator(), instruction, /*is_load*/ true);
+  CreateVecMemLocations(allocator_, instruction, /*is_load*/ true);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecLoad(HVecLoad* instruction) {
@@ -1002,7 +1002,7 @@ void InstructionCodeGeneratorARMVIXL::VisitVecLoad(HVecLoad* instruction) {
 }
 
 void LocationsBuilderARMVIXL::VisitVecStore(HVecStore* instruction) {
-  CreateVecMemLocations(GetGraph()->GetAllocator(), instruction, /*is_load*/ false);
+  CreateVecMemLocations(allocator_, instruction, /*is_load*/ false);
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitVecStore(HVecStore* instruction) {
diff --git a/compiler/optimizing/code_generator_vector_x86.cc b/compiler/optimizing/code_generator_vector_x86.cc
index da61764a7c..b3abf0b113 100644
--- a/compiler/optimizing/code_generator_vector_x86.cc
+++ b/compiler/optimizing/code_generator_vector_x86.cc
@@ -26,7 +26,7 @@ namespace x86 {
 #define __ down_cast<X86Assembler*>(GetAssembler())->  // NOLINT
 
 void LocationsBuilderX86::VisitVecReplicateScalar(HVecReplicateScalar* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   HInstruction* input = instruction->InputAt(0);
   bool is_zero = IsZeroBitPattern(input);
   switch (instruction->GetPackedType()) {
@@ -118,7 +118,7 @@ void InstructionCodeGeneratorX86::VisitVecReplicateScalar(HVecReplicateScalar* i
 }
 
 void LocationsBuilderX86::VisitVecExtractScalar(HVecExtractScalar* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kInt64:
       // Long needs extra temporary to store into the register pair.
@@ -182,7 +182,7 @@ void InstructionCodeGeneratorX86::VisitVecExtractScalar(HVecExtractScalar* instr
 
 // Helper to set up locations for vector unary operations.
 static void CreateVecUnOpLocations(ArenaAllocator* allocator, HVecUnaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -203,7 +203,7 @@ static void CreateVecUnOpLocations(ArenaAllocator* allocator, HVecUnaryOperation
 }
 
 void LocationsBuilderX86::VisitVecReduce(HVecReduce* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
   // Long reduction or min/max require a temporary.
   if (instruction->GetPackedType() == DataType::Type::kInt64 ||
       instruction->GetReductionKind() == HVecReduce::kMin ||
@@ -255,7 +255,7 @@ void InstructionCodeGeneratorX86::VisitVecReduce(HVecReduce* instruction) {
 }
 
 void LocationsBuilderX86::VisitVecCnv(HVecCnv* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86::VisitVecCnv(HVecCnv* instruction) {
@@ -273,7 +273,7 @@ void InstructionCodeGeneratorX86::VisitVecCnv(HVecCnv* instruction) {
 }
 
 void LocationsBuilderX86::VisitVecNeg(HVecNeg* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86::VisitVecNeg(HVecNeg* instruction) {
@@ -320,7 +320,7 @@ void InstructionCodeGeneratorX86::VisitVecNeg(HVecNeg* instruction) {
 }
 
 void LocationsBuilderX86::VisitVecAbs(HVecAbs* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
   // Integral-abs requires a temporary for the comparison.
   if (instruction->GetPackedType() == DataType::Type::kInt32) {
     instruction->GetLocations()->AddTemp(Location::RequiresFpuRegister());
@@ -361,7 +361,7 @@ void InstructionCodeGeneratorX86::VisitVecAbs(HVecAbs* instruction) {
 }
 
 void LocationsBuilderX86::VisitVecNot(HVecNot* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
   // Boolean-not requires a temporary to construct the 16 x one.
   if (instruction->GetPackedType() == DataType::Type::kBool) {
     instruction->GetLocations()->AddTemp(Location::RequiresFpuRegister());
@@ -411,7 +411,7 @@ void InstructionCodeGeneratorX86::VisitVecNot(HVecNot* instruction) {
 
 // Helper to set up locations for vector binary operations.
 static void CreateVecBinOpLocations(ArenaAllocator* allocator, HVecBinaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -433,7 +433,7 @@ static void CreateVecBinOpLocations(ArenaAllocator* allocator, HVecBinaryOperati
 }
 
 static void CreateVecTerOpLocations(ArenaAllocator* allocator, HVecOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -456,9 +456,9 @@ static void CreateVecTerOpLocations(ArenaAllocator* allocator, HVecOperation* in
 
 void LocationsBuilderX86::VisitVecAdd(HVecAdd* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -503,7 +503,7 @@ void InstructionCodeGeneratorX86::VisitVecAdd(HVecAdd* instruction) {
 }
 
 void LocationsBuilderX86::VisitVecSaturationAdd(HVecSaturationAdd* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86::VisitVecSaturationAdd(HVecSaturationAdd* instruction) {
@@ -535,7 +535,7 @@ void InstructionCodeGeneratorX86::VisitVecSaturationAdd(HVecSaturationAdd* instr
 }
 
 void LocationsBuilderX86::VisitVecHalvingAdd(HVecHalvingAdd* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86::VisitVecHalvingAdd(HVecHalvingAdd* instruction) {
@@ -563,9 +563,9 @@ void InstructionCodeGeneratorX86::VisitVecHalvingAdd(HVecHalvingAdd* instruction
 
 void LocationsBuilderX86::VisitVecSub(HVecSub* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -610,7 +610,7 @@ void InstructionCodeGeneratorX86::VisitVecSub(HVecSub* instruction) {
 }
 
 void LocationsBuilderX86::VisitVecSaturationSub(HVecSaturationSub* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86::VisitVecSaturationSub(HVecSaturationSub* instruction) {
@@ -643,9 +643,9 @@ void InstructionCodeGeneratorX86::VisitVecSaturationSub(HVecSaturationSub* instr
 
 void LocationsBuilderX86::VisitVecMul(HVecMul* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -682,9 +682,9 @@ void InstructionCodeGeneratorX86::VisitVecMul(HVecMul* instruction) {
 
 void LocationsBuilderX86::VisitVecDiv(HVecDiv* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -711,7 +711,7 @@ void InstructionCodeGeneratorX86::VisitVecDiv(HVecDiv* instruction) {
 }
 
 void LocationsBuilderX86::VisitVecMin(HVecMin* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86::VisitVecMin(HVecMin* instruction) {
@@ -760,7 +760,7 @@ void InstructionCodeGeneratorX86::VisitVecMin(HVecMin* instruction) {
 }
 
 void LocationsBuilderX86::VisitVecMax(HVecMax* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86::VisitVecMax(HVecMax* instruction) {
@@ -810,9 +810,9 @@ void InstructionCodeGeneratorX86::VisitVecMax(HVecMax* instruction) {
 
 void LocationsBuilderX86::VisitVecAnd(HVecAnd* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -851,9 +851,9 @@ void InstructionCodeGeneratorX86::VisitVecAnd(HVecAnd* instruction) {
 
 void LocationsBuilderX86::VisitVecAndNot(HVecAndNot* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -892,9 +892,9 @@ void InstructionCodeGeneratorX86::VisitVecAndNot(HVecAndNot* instruction) {
 
 void LocationsBuilderX86::VisitVecOr(HVecOr* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -933,9 +933,9 @@ void InstructionCodeGeneratorX86::VisitVecOr(HVecOr* instruction) {
 
 void LocationsBuilderX86::VisitVecXor(HVecXor* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -974,7 +974,7 @@ void InstructionCodeGeneratorX86::VisitVecXor(HVecXor* instruction) {
 
 // Helper to set up locations for vector shift operations.
 static void CreateVecShiftLocations(ArenaAllocator* allocator, HVecBinaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint16:
     case DataType::Type::kInt16:
@@ -991,7 +991,7 @@ static void CreateVecShiftLocations(ArenaAllocator* allocator, HVecBinaryOperati
 }
 
 void LocationsBuilderX86::VisitVecShl(HVecShl* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86::VisitVecShl(HVecShl* instruction) {
@@ -1020,7 +1020,7 @@ void InstructionCodeGeneratorX86::VisitVecShl(HVecShl* instruction) {
 }
 
 void LocationsBuilderX86::VisitVecShr(HVecShr* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86::VisitVecShr(HVecShr* instruction) {
@@ -1045,7 +1045,7 @@ void InstructionCodeGeneratorX86::VisitVecShr(HVecShr* instruction) {
 }
 
 void LocationsBuilderX86::VisitVecUShr(HVecUShr* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86::VisitVecUShr(HVecUShr* instruction) {
@@ -1074,7 +1074,7 @@ void InstructionCodeGeneratorX86::VisitVecUShr(HVecUShr* instruction) {
 }
 
 void LocationsBuilderX86::VisitVecSetScalars(HVecSetScalars* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
 
   DCHECK_EQ(1u, instruction->InputCount());  // only one input currently implemented
 
@@ -1163,7 +1163,7 @@ void InstructionCodeGeneratorX86::VisitVecSetScalars(HVecSetScalars* instruction
 
 // Helper to set up locations for vector accumulations.
 static void CreateVecAccumLocations(ArenaAllocator* allocator, HVecOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
@@ -1183,7 +1183,7 @@ static void CreateVecAccumLocations(ArenaAllocator* allocator, HVecOperation* in
 }
 
 void LocationsBuilderX86::VisitVecMultiplyAccumulate(HVecMultiplyAccumulate* instruction) {
-  CreateVecAccumLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecAccumLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86::VisitVecMultiplyAccumulate(HVecMultiplyAccumulate* instruction) {
@@ -1192,7 +1192,7 @@ void InstructionCodeGeneratorX86::VisitVecMultiplyAccumulate(HVecMultiplyAccumul
 }
 
 void LocationsBuilderX86::VisitVecSADAccumulate(HVecSADAccumulate* instruction) {
-  CreateVecAccumLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecAccumLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86::VisitVecSADAccumulate(HVecSADAccumulate* instruction) {
@@ -1201,7 +1201,7 @@ void InstructionCodeGeneratorX86::VisitVecSADAccumulate(HVecSADAccumulate* instr
 }
 
 void LocationsBuilderX86::VisitVecDotProd(HVecDotProd* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetInAt(1, Location::RequiresFpuRegister());
   locations->SetInAt(2, Location::RequiresFpuRegister());
@@ -1239,7 +1239,7 @@ void InstructionCodeGeneratorX86::VisitVecDotProd(HVecDotProd* instruction) {
 static void CreateVecMemLocations(ArenaAllocator* allocator,
                                   HVecMemoryOperation* instruction,
                                   bool is_load) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -1283,7 +1283,7 @@ static Address VecAddress(LocationSummary* locations, size_t size, bool is_strin
 }
 
 void LocationsBuilderX86::VisitVecLoad(HVecLoad* instruction) {
-  CreateVecMemLocations(GetGraph()->GetAllocator(), instruction, /*is_load*/ true);
+  CreateVecMemLocations(allocator_, instruction, /*is_load*/ true);
   // String load requires a temporary for the compressed load.
   if (mirror::kUseStringCompression && instruction->IsStringCharAt()) {
     instruction->GetLocations()->AddTemp(Location::RequiresFpuRegister());
@@ -1346,7 +1346,7 @@ void InstructionCodeGeneratorX86::VisitVecLoad(HVecLoad* instruction) {
 }
 
 void LocationsBuilderX86::VisitVecStore(HVecStore* instruction) {
-  CreateVecMemLocations(GetGraph()->GetAllocator(), instruction, /*is_load*/ false);
+  CreateVecMemLocations(allocator_, instruction, /*is_load*/ false);
 }
 
 void InstructionCodeGeneratorX86::VisitVecStore(HVecStore* instruction) {
diff --git a/compiler/optimizing/code_generator_vector_x86_64.cc b/compiler/optimizing/code_generator_vector_x86_64.cc
index 76bd5faab3..584bcdd04c 100644
--- a/compiler/optimizing/code_generator_vector_x86_64.cc
+++ b/compiler/optimizing/code_generator_vector_x86_64.cc
@@ -26,7 +26,7 @@ namespace x86_64 {
 #define __ down_cast<X86_64Assembler*>(GetAssembler())->  // NOLINT
 
 void LocationsBuilderX86_64::VisitVecReplicateScalar(HVecReplicateScalar* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   HInstruction* input = instruction->InputAt(0);
   bool is_zero = IsZeroBitPattern(input);
   switch (instruction->GetPackedType()) {
@@ -109,7 +109,7 @@ void InstructionCodeGeneratorX86_64::VisitVecReplicateScalar(HVecReplicateScalar
 }
 
 void LocationsBuilderX86_64::VisitVecExtractScalar(HVecExtractScalar* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -165,7 +165,7 @@ void InstructionCodeGeneratorX86_64::VisitVecExtractScalar(HVecExtractScalar* in
 
 // Helper to set up locations for vector unary operations.
 static void CreateVecUnOpLocations(ArenaAllocator* allocator, HVecUnaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -186,7 +186,7 @@ static void CreateVecUnOpLocations(ArenaAllocator* allocator, HVecUnaryOperation
 }
 
 void LocationsBuilderX86_64::VisitVecReduce(HVecReduce* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
   // Long reduction or min/max require a temporary.
   if (instruction->GetPackedType() == DataType::Type::kInt64 ||
       instruction->GetReductionKind() == HVecReduce::kMin ||
@@ -238,7 +238,7 @@ void InstructionCodeGeneratorX86_64::VisitVecReduce(HVecReduce* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecCnv(HVecCnv* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecCnv(HVecCnv* instruction) {
@@ -256,7 +256,7 @@ void InstructionCodeGeneratorX86_64::VisitVecCnv(HVecCnv* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecNeg(HVecNeg* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecNeg(HVecNeg* instruction) {
@@ -303,7 +303,7 @@ void InstructionCodeGeneratorX86_64::VisitVecNeg(HVecNeg* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecAbs(HVecAbs* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
   // Integral-abs requires a temporary for the comparison.
   if (instruction->GetPackedType() == DataType::Type::kInt32) {
     instruction->GetLocations()->AddTemp(Location::RequiresFpuRegister());
@@ -344,7 +344,7 @@ void InstructionCodeGeneratorX86_64::VisitVecAbs(HVecAbs* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecNot(HVecNot* instruction) {
-  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecUnOpLocations(allocator_, instruction);
   // Boolean-not requires a temporary to construct the 16 x one.
   if (instruction->GetPackedType() == DataType::Type::kBool) {
     instruction->GetLocations()->AddTemp(Location::RequiresFpuRegister());
@@ -394,7 +394,7 @@ void InstructionCodeGeneratorX86_64::VisitVecNot(HVecNot* instruction) {
 
 // Helper to set up locations for vector binary operations.
 static void CreateVecBinOpLocations(ArenaAllocator* allocator, HVecBinaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -416,7 +416,7 @@ static void CreateVecBinOpLocations(ArenaAllocator* allocator, HVecBinaryOperati
 }
 
 static void CreateVecTerOpLocations(ArenaAllocator* allocator, HVecOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -439,9 +439,9 @@ static void CreateVecTerOpLocations(ArenaAllocator* allocator, HVecOperation* in
 
 void LocationsBuilderX86_64::VisitVecAdd(HVecAdd* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -486,7 +486,7 @@ void InstructionCodeGeneratorX86_64::VisitVecAdd(HVecAdd* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecSaturationAdd(HVecSaturationAdd* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecSaturationAdd(HVecSaturationAdd* instruction) {
@@ -518,7 +518,7 @@ void InstructionCodeGeneratorX86_64::VisitVecSaturationAdd(HVecSaturationAdd* in
 }
 
 void LocationsBuilderX86_64::VisitVecHalvingAdd(HVecHalvingAdd* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecHalvingAdd(HVecHalvingAdd* instruction) {
@@ -546,9 +546,9 @@ void InstructionCodeGeneratorX86_64::VisitVecHalvingAdd(HVecHalvingAdd* instruct
 
 void LocationsBuilderX86_64::VisitVecSub(HVecSub* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -593,7 +593,7 @@ void InstructionCodeGeneratorX86_64::VisitVecSub(HVecSub* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecSaturationSub(HVecSaturationSub* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecSaturationSub(HVecSaturationSub* instruction) {
@@ -626,9 +626,9 @@ void InstructionCodeGeneratorX86_64::VisitVecSaturationSub(HVecSaturationSub* in
 
 void LocationsBuilderX86_64::VisitVecMul(HVecMul* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -665,9 +665,9 @@ void InstructionCodeGeneratorX86_64::VisitVecMul(HVecMul* instruction) {
 
 void LocationsBuilderX86_64::VisitVecDiv(HVecDiv* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -694,7 +694,7 @@ void InstructionCodeGeneratorX86_64::VisitVecDiv(HVecDiv* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecMin(HVecMin* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecMin(HVecMin* instruction) {
@@ -743,7 +743,7 @@ void InstructionCodeGeneratorX86_64::VisitVecMin(HVecMin* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecMax(HVecMax* instruction) {
-  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecBinOpLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecMax(HVecMax* instruction) {
@@ -793,9 +793,9 @@ void InstructionCodeGeneratorX86_64::VisitVecMax(HVecMax* instruction) {
 
 void LocationsBuilderX86_64::VisitVecAnd(HVecAnd* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -834,9 +834,9 @@ void InstructionCodeGeneratorX86_64::VisitVecAnd(HVecAnd* instruction) {
 
 void LocationsBuilderX86_64::VisitVecAndNot(HVecAndNot* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -875,9 +875,9 @@ void InstructionCodeGeneratorX86_64::VisitVecAndNot(HVecAndNot* instruction) {
 
 void LocationsBuilderX86_64::VisitVecOr(HVecOr* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -916,9 +916,9 @@ void InstructionCodeGeneratorX86_64::VisitVecOr(HVecOr* instruction) {
 
 void LocationsBuilderX86_64::VisitVecXor(HVecXor* instruction) {
   if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecTerOpLocations(allocator_, instruction);
   } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+    CreateVecBinOpLocations(allocator_, instruction);
   }
 }
 
@@ -957,7 +957,7 @@ void InstructionCodeGeneratorX86_64::VisitVecXor(HVecXor* instruction) {
 
 // Helper to set up locations for vector shift operations.
 static void CreateVecShiftLocations(ArenaAllocator* allocator, HVecBinaryOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint16:
     case DataType::Type::kInt16:
@@ -974,7 +974,7 @@ static void CreateVecShiftLocations(ArenaAllocator* allocator, HVecBinaryOperati
 }
 
 void LocationsBuilderX86_64::VisitVecShl(HVecShl* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecShl(HVecShl* instruction) {
@@ -1003,7 +1003,7 @@ void InstructionCodeGeneratorX86_64::VisitVecShl(HVecShl* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecShr(HVecShr* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecShr(HVecShr* instruction) {
@@ -1028,7 +1028,7 @@ void InstructionCodeGeneratorX86_64::VisitVecShr(HVecShr* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecUShr(HVecUShr* instruction) {
-  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecShiftLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecUShr(HVecUShr* instruction) {
@@ -1057,7 +1057,7 @@ void InstructionCodeGeneratorX86_64::VisitVecUShr(HVecUShr* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecSetScalars(HVecSetScalars* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
 
   DCHECK_EQ(1u, instruction->InputCount());  // only one input currently implemented
 
@@ -1136,7 +1136,7 @@ void InstructionCodeGeneratorX86_64::VisitVecSetScalars(HVecSetScalars* instruct
 
 // Helper to set up locations for vector accumulations.
 static void CreateVecAccumLocations(ArenaAllocator* allocator, HVecOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
@@ -1156,7 +1156,7 @@ static void CreateVecAccumLocations(ArenaAllocator* allocator, HVecOperation* in
 }
 
 void LocationsBuilderX86_64::VisitVecMultiplyAccumulate(HVecMultiplyAccumulate* instruction) {
-  CreateVecAccumLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecAccumLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecMultiplyAccumulate(HVecMultiplyAccumulate* instruction) {
@@ -1165,7 +1165,7 @@ void InstructionCodeGeneratorX86_64::VisitVecMultiplyAccumulate(HVecMultiplyAccu
 }
 
 void LocationsBuilderX86_64::VisitVecSADAccumulate(HVecSADAccumulate* instruction) {
-  CreateVecAccumLocations(GetGraph()->GetAllocator(), instruction);
+  CreateVecAccumLocations(allocator_, instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecSADAccumulate(HVecSADAccumulate* instruction) {
@@ -1174,7 +1174,7 @@ void InstructionCodeGeneratorX86_64::VisitVecSADAccumulate(HVecSADAccumulate* in
 }
 
 void LocationsBuilderX86_64::VisitVecDotProd(HVecDotProd* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetInAt(1, Location::RequiresFpuRegister());
   locations->SetInAt(2, Location::RequiresFpuRegister());
@@ -1212,7 +1212,7 @@ void InstructionCodeGeneratorX86_64::VisitVecDotProd(HVecDotProd* instruction) {
 static void CreateVecMemLocations(ArenaAllocator* allocator,
                                   HVecMemoryOperation* instruction,
                                   bool is_load) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, instruction);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -1256,7 +1256,7 @@ static Address VecAddress(LocationSummary* locations, size_t size, bool is_strin
 }
 
 void LocationsBuilderX86_64::VisitVecLoad(HVecLoad* instruction) {
-  CreateVecMemLocations(GetGraph()->GetAllocator(), instruction, /*is_load*/ true);
+  CreateVecMemLocations(allocator_, instruction, /*is_load*/ true);
   // String load requires a temporary for the compressed load.
   if (mirror::kUseStringCompression && instruction->IsStringCharAt()) {
     instruction->GetLocations()->AddTemp(Location::RequiresFpuRegister());
@@ -1319,7 +1319,7 @@ void InstructionCodeGeneratorX86_64::VisitVecLoad(HVecLoad* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecStore(HVecStore* instruction) {
-  CreateVecMemLocations(GetGraph()->GetAllocator(), instruction, /*is_load*/ false);
+  CreateVecMemLocations(allocator_, instruction, /*is_load*/ false);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecStore(HVecStore* instruction) {
diff --git a/compiler/optimizing/code_generator_x86.cc b/compiler/optimizing/code_generator_x86.cc
index ae31e9f198..c88d46e126 100644
--- a/compiler/optimizing/code_generator_x86.cc
+++ b/compiler/optimizing/code_generator_x86.cc
@@ -1223,8 +1223,8 @@ void SetInForReturnValue(HInstruction* ret, LocationSummary* locations) {
 }
 
 void LocationsBuilderX86::VisitMethodExitHook(HMethodExitHook* method_hook) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(method_hook, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, method_hook, LocationSummary::kCallOnSlowPath);
   SetInForReturnValue(method_hook, locations);
   // We use rdtsc to obtain a timestamp for tracing. rdtsc returns the results in EAX + EDX.
   locations->AddTemp(Location::RegisterLocation(EAX));
@@ -1306,8 +1306,8 @@ void InstructionCodeGeneratorX86::VisitMethodExitHook(HMethodExitHook* instructi
 }
 
 void LocationsBuilderX86::VisitMethodEntryHook(HMethodEntryHook* method_hook) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(method_hook, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, method_hook, LocationSummary::kCallOnSlowPath);
   // We use rdtsc to obtain a timestamp for tracing. rdtsc returns the results in EAX + EDX.
   locations->AddTemp(Location::RegisterLocation(EAX));
   locations->AddTemp(Location::RegisterLocation(EDX));
@@ -2218,7 +2218,7 @@ void InstructionCodeGeneratorX86::GenerateTestAndBranch(HInstruction* instructio
 }
 
 void LocationsBuilderX86::VisitIf(HIf* if_instr) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(if_instr);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, if_instr);
   if (IsBooleanValueOrMaterializedCondition(if_instr->InputAt(0))) {
     if (GetGraph()->IsCompilingBaseline() &&
         codegen_->GetCompilerOptions().ProfileBranches() &&
@@ -2270,8 +2270,8 @@ void InstructionCodeGeneratorX86::VisitIf(HIf* if_instr) {
 }
 
 void LocationsBuilderX86::VisitDeoptimize(HDeoptimize* deoptimize) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(deoptimize, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, deoptimize, LocationSummary::kCallOnSlowPath);
   InvokeRuntimeCallingConvention calling_convention;
   RegisterSet caller_saves = RegisterSet::Empty();
   caller_saves.Add(Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
@@ -2290,8 +2290,7 @@ void InstructionCodeGeneratorX86::VisitDeoptimize(HDeoptimize* deoptimize) {
 }
 
 void LocationsBuilderX86::VisitShouldDeoptimizeFlag(HShouldDeoptimizeFlag* flag) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(flag, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, flag);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -2322,7 +2321,7 @@ static bool SelectCanUseCMOV(HSelect* select) {
 }
 
 void LocationsBuilderX86::VisitSelect(HSelect* select) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(select);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, select);
   if (DataType::IsFloatingPointType(select->GetType())) {
     locations->SetInAt(0, Location::RequiresFpuRegister());
     locations->SetInAt(1, Location::Any());
@@ -2416,7 +2415,7 @@ void InstructionCodeGeneratorX86::VisitSelect(HSelect* select) {
 }
 
 void LocationsBuilderX86::VisitNop(HNop* nop) {
-  new (GetGraph()->GetAllocator()) LocationSummary(nop);
+  LocationSummary::CreateNoCall(allocator_, nop);
 }
 
 void InstructionCodeGeneratorX86::VisitNop(HNop*) {
@@ -2438,8 +2437,7 @@ void CodeGeneratorX86::GenerateNop() {
 }
 
 void LocationsBuilderX86::HandleCondition(HCondition* cond) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(cond, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, cond);
   // Handle the long/FP comparisons made in instruction simplification.
   switch (cond->InputAt(0)->GetType()) {
     case DataType::Type::kInt64: {
@@ -2605,8 +2603,7 @@ void InstructionCodeGeneratorX86::VisitAboveOrEqual(HAboveOrEqual* comp) {
 }
 
 void LocationsBuilderX86::VisitIntConstant(HIntConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -2615,8 +2612,7 @@ void InstructionCodeGeneratorX86::VisitIntConstant([[maybe_unused]] HIntConstant
 }
 
 void LocationsBuilderX86::VisitNullConstant(HNullConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -2625,8 +2621,7 @@ void InstructionCodeGeneratorX86::VisitNullConstant([[maybe_unused]] HNullConsta
 }
 
 void LocationsBuilderX86::VisitLongConstant(HLongConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -2635,8 +2630,7 @@ void InstructionCodeGeneratorX86::VisitLongConstant([[maybe_unused]] HLongConsta
 }
 
 void LocationsBuilderX86::VisitFloatConstant(HFloatConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -2645,8 +2639,7 @@ void InstructionCodeGeneratorX86::VisitFloatConstant([[maybe_unused]] HFloatCons
 }
 
 void LocationsBuilderX86::VisitDoubleConstant(HDoubleConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -2680,8 +2673,7 @@ void InstructionCodeGeneratorX86::VisitReturnVoid([[maybe_unused]] HReturnVoid*
 }
 
 void LocationsBuilderX86::VisitReturn(HReturn* ret) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(ret, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, ret);
   SetInForReturnValue(ret, locations);
 }
 
@@ -2957,8 +2949,7 @@ void InstructionCodeGeneratorX86::VisitInvokeCustom(HInvokeCustom* invoke) {
 }
 
 void LocationsBuilderX86::VisitNeg(HNeg* neg) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(neg, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, neg);
   switch (neg->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
@@ -3038,8 +3029,7 @@ void InstructionCodeGeneratorX86::VisitNeg(HNeg* neg) {
 }
 
 void LocationsBuilderX86::VisitX86FPNeg(HX86FPNeg* neg) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(neg, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, neg);
   DCHECK(DataType::IsFloatingPointType(neg->GetType()));
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetInAt(1, Location::RequiresRegister());
@@ -3080,8 +3070,7 @@ void LocationsBuilderX86::VisitTypeConversion(HTypeConversion* conversion) {
        && result_type == DataType::Type::kInt64)
       ? LocationSummary::kCallOnMainOnly
       : LocationSummary::kNoCall;
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(conversion, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, conversion, call_kind);
 
   switch (result_type) {
     case DataType::Type::kUint8:
@@ -3578,8 +3567,7 @@ void InstructionCodeGeneratorX86::VisitTypeConversion(HTypeConversion* conversio
 }
 
 void LocationsBuilderX86::VisitAdd(HAdd* add) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(add, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, add);
   switch (add->GetResultType()) {
     case DataType::Type::kInt32: {
       locations->SetInAt(0, Location::RequiresRegister());
@@ -3705,8 +3693,7 @@ void InstructionCodeGeneratorX86::VisitAdd(HAdd* add) {
 }
 
 void LocationsBuilderX86::VisitSub(HSub* sub) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(sub, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, sub);
   switch (sub->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64: {
@@ -3811,8 +3798,7 @@ void InstructionCodeGeneratorX86::VisitSub(HSub* sub) {
 }
 
 void LocationsBuilderX86::VisitMul(HMul* mul) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(mul, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, mul);
   switch (mul->GetResultType()) {
     case DataType::Type::kInt32:
       locations->SetInAt(0, Location::RequiresRegister());
@@ -4313,7 +4299,7 @@ void LocationsBuilderX86::VisitDiv(HDiv* div) {
   LocationSummary::CallKind call_kind = (div->GetResultType() == DataType::Type::kInt64)
       ? LocationSummary::kCallOnMainOnly
       : LocationSummary::kNoCall;
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(div, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, div, call_kind);
 
   switch (div->GetResultType()) {
     case DataType::Type::kInt32: {
@@ -4418,7 +4404,7 @@ void LocationsBuilderX86::VisitRem(HRem* rem) {
   LocationSummary::CallKind call_kind = (rem->GetResultType() == DataType::Type::kInt64)
       ? LocationSummary::kCallOnMainOnly
       : LocationSummary::kNoCall;
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(rem, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, rem, call_kind);
 
   switch (type) {
     case DataType::Type::kInt32: {
@@ -4476,7 +4462,7 @@ void InstructionCodeGeneratorX86::VisitRem(HRem* rem) {
 }
 
 static void CreateMinMaxLocations(ArenaAllocator* allocator, HBinaryOperation* minmax) {
-  LocationSummary* locations = new (allocator) LocationSummary(minmax);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, minmax);
   switch (minmax->GetResultType()) {
     case DataType::Type::kInt32:
       locations->SetInAt(0, Location::RequiresRegister());
@@ -4665,7 +4651,7 @@ void InstructionCodeGeneratorX86::GenerateMinMax(HBinaryOperation* minmax, bool
 }
 
 void LocationsBuilderX86::VisitMin(HMin* min) {
-  CreateMinMaxLocations(GetGraph()->GetAllocator(), min);
+  CreateMinMaxLocations(allocator_, min);
 }
 
 void InstructionCodeGeneratorX86::VisitMin(HMin* min) {
@@ -4673,7 +4659,7 @@ void InstructionCodeGeneratorX86::VisitMin(HMin* min) {
 }
 
 void LocationsBuilderX86::VisitMax(HMax* max) {
-  CreateMinMaxLocations(GetGraph()->GetAllocator(), max);
+  CreateMinMaxLocations(allocator_, max);
 }
 
 void InstructionCodeGeneratorX86::VisitMax(HMax* max) {
@@ -4681,7 +4667,7 @@ void InstructionCodeGeneratorX86::VisitMax(HMax* max) {
 }
 
 void LocationsBuilderX86::VisitAbs(HAbs* abs) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(abs);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, abs);
   switch (abs->GetResultType()) {
     case DataType::Type::kInt32:
       locations->SetInAt(0, Location::RegisterLocation(EAX));
@@ -4845,8 +4831,7 @@ void InstructionCodeGeneratorX86::VisitDivZeroCheck(HDivZeroCheck* instruction)
 void LocationsBuilderX86::HandleShift(HBinaryOperation* op) {
   DCHECK(op->IsShl() || op->IsShr() || op->IsUShr());
 
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(op, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, op);
 
   switch (op->GetResultType()) {
     case DataType::Type::kInt32:
@@ -5048,8 +5033,7 @@ void LocationsBuilderX86::VisitRor(HRor* ror) {
 }
 
 void LocationsBuilderX86::HandleRotate(HBinaryOperation* rotate) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(rotate, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, rotate);
 
   switch (rotate->GetResultType()) {
     case DataType::Type::kInt64:
@@ -5186,8 +5170,8 @@ void InstructionCodeGeneratorX86::VisitUShr(HUShr* ushr) {
 }
 
 void LocationsBuilderX86::VisitNewInstance(HNewInstance* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   locations->SetOut(Location::RegisterLocation(EAX));
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
@@ -5200,8 +5184,8 @@ void InstructionCodeGeneratorX86::VisitNewInstance(HNewInstance* instruction) {
 }
 
 void LocationsBuilderX86::VisitNewArray(HNewArray* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   locations->SetOut(Location::RegisterLocation(EAX));
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
@@ -5217,8 +5201,7 @@ void InstructionCodeGeneratorX86::VisitNewArray(HNewArray* instruction) {
 }
 
 void LocationsBuilderX86::VisitParameterValue(HParameterValue* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   Location location = parameter_visitor_.GetNextLocation(instruction->GetType());
   if (location.IsStackSlot()) {
     location = Location::StackSlot(location.GetStackIndex() + codegen_->GetFrameSize());
@@ -5232,8 +5215,7 @@ void InstructionCodeGeneratorX86::VisitParameterValue(
     [[maybe_unused]] HParameterValue* instruction) {}
 
 void LocationsBuilderX86::VisitCurrentMethod(HCurrentMethod* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetOut(Location::RegisterLocation(kMethodRegisterArgument));
 }
 
@@ -5241,8 +5223,7 @@ void InstructionCodeGeneratorX86::VisitCurrentMethod([[maybe_unused]] HCurrentMe
 }
 
 void LocationsBuilderX86::VisitClassTableGet(HClassTableGet* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister());
 }
@@ -5267,8 +5248,7 @@ void InstructionCodeGeneratorX86::VisitClassTableGet(HClassTableGet* instruction
 }
 
 void LocationsBuilderX86::VisitNot(HNot* not_) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(not_, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, not_);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::SameAsFirstInput());
 }
@@ -5294,8 +5274,7 @@ void InstructionCodeGeneratorX86::VisitNot(HNot* not_) {
 }
 
 void LocationsBuilderX86::VisitBooleanNot(HBooleanNot* bool_not) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(bool_not, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, bool_not);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::SameAsFirstInput());
 }
@@ -5309,8 +5288,7 @@ void InstructionCodeGeneratorX86::VisitBooleanNot(HBooleanNot* bool_not) {
 }
 
 void LocationsBuilderX86::VisitCompare(HCompare* compare) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(compare, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, compare);
   switch (compare->GetComparisonType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -5440,8 +5418,7 @@ void InstructionCodeGeneratorX86::VisitCompare(HCompare* compare) {
 }
 
 void LocationsBuilderX86::VisitPhi(HPhi* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   for (size_t i = 0, e = locations->GetInputCount(); i < e; ++i) {
     locations->SetInAt(i, Location::Any());
   }
@@ -6018,11 +5995,11 @@ void LocationsBuilderX86::HandleFieldGet(HInstruction* instruction, const FieldI
 
   bool object_field_get_with_read_barrier =
       (instruction->GetType() == DataType::Type::kReference) && codegen_->EmitReadBarrier();
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction,
-                                                       codegen_->EmitReadBarrier()
-                                                           ? LocationSummary::kCallOnSlowPath
-                                                           : LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
+      instruction,
+      object_field_get_with_read_barrier ? LocationSummary::kCallOnSlowPath
+                                         : LocationSummary::kNoCall);
   if (object_field_get_with_read_barrier && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -6103,8 +6080,7 @@ void LocationsBuilderX86::HandleFieldSet(HInstruction* instruction,
                                          WriteBarrierKind write_barrier_kind) {
   DCHECK(instruction->IsInstanceFieldSet() || instruction->IsStaticFieldSet());
 
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   bool is_volatile = field_info.IsVolatile();
   DataType::Type field_type = field_info.GetFieldType();
@@ -6494,11 +6470,11 @@ void InstructionCodeGeneratorX86::VisitNullCheck(HNullCheck* instruction) {
 void LocationsBuilderX86::VisitArrayGet(HArrayGet* instruction) {
   bool object_array_get_with_read_barrier =
       (instruction->GetType() == DataType::Type::kReference) && codegen_->EmitReadBarrier();
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction,
-                                                       object_array_get_with_read_barrier
-                                                           ? LocationSummary::kCallOnSlowPath
-                                                           : LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
+      instruction,
+      object_array_get_with_read_barrier ? LocationSummary::kCallOnSlowPath
+                                         : LocationSummary::kNoCall);
   if (object_array_get_with_read_barrier && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -6590,7 +6566,8 @@ void LocationsBuilderX86::VisitArraySet(HArraySet* instruction) {
       codegen_->ShouldCheckGCCard(value_type, instruction->GetValue(), write_barrier_kind);
   bool needs_type_check = instruction->NeedsTypeCheck();
 
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
       instruction,
       needs_type_check ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall);
 
@@ -6864,7 +6841,7 @@ void InstructionCodeGeneratorX86::VisitArraySet(HArraySet* instruction) {
 }
 
 void LocationsBuilderX86::VisitArrayLength(HArrayLength* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   if (!instruction->IsEmittedAtUseSite()) {
     locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
@@ -6985,8 +6962,8 @@ void InstructionCodeGeneratorX86::VisitParallelMove(HParallelMove* instruction)
 }
 
 void LocationsBuilderX86::VisitSuspendCheck(HSuspendCheck* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnSlowPath);
   // In suspend check slow path, usually there are no caller-save registers at all.
   // If SIMD instructions are present, however, we force spilling all live SIMD
   // registers in full width (since the runtime only saves/restores lower part).
@@ -7388,7 +7365,7 @@ void LocationsBuilderX86::VisitLoadClass(HLoadClass* cls) {
   LocationSummary::CallKind call_kind = (cls->NeedsEnvironment() || requires_read_barrier)
       ? LocationSummary::kCallOnSlowPath
       : LocationSummary::kNoCall;
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(cls, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, cls, call_kind);
   if (kUseBakerReadBarrier && requires_read_barrier && !cls->NeedsEnvironment()) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -7547,7 +7524,7 @@ void InstructionCodeGeneratorX86::VisitLoadMethodType(HLoadMethodType* load) {
 
 void LocationsBuilderX86::VisitClinitCheck(HClinitCheck* check) {
   LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(check, LocationSummary::kCallOnSlowPath);
+      LocationSummary::Create(allocator_, check, LocationSummary::kCallOnSlowPath);
   locations->SetInAt(0, Location::RequiresRegister());
   if (check->HasUses()) {
     locations->SetOut(Location::SameAsFirstInput());
@@ -7612,7 +7589,7 @@ HLoadString::LoadKind CodeGeneratorX86::GetSupportedLoadStringKind(
 
 void LocationsBuilderX86::VisitLoadString(HLoadString* load) {
   LocationSummary::CallKind call_kind = codegen_->GetLoadStringCallKind(load);
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(load, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, load, call_kind);
   HLoadString::LoadKind load_kind = load->GetLoadKind();
   if (load_kind == HLoadString::LoadKind::kBootImageLinkTimePcRelative ||
       load_kind == HLoadString::LoadKind::kBootImageRelRo ||
@@ -7714,8 +7691,7 @@ static Address GetExceptionTlsAddress() {
 }
 
 void LocationsBuilderX86::VisitLoadException(HLoadException* load) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(load, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, load);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -7724,7 +7700,7 @@ void InstructionCodeGeneratorX86::VisitLoadException(HLoadException* load) {
 }
 
 void LocationsBuilderX86::VisitClearException(HClearException* clear) {
-  new (GetGraph()->GetAllocator()) LocationSummary(clear, LocationSummary::kNoCall);
+  LocationSummary::CreateNoCall(allocator_, clear);
 }
 
 void InstructionCodeGeneratorX86::VisitClearException([[maybe_unused]] HClearException* clear) {
@@ -7732,8 +7708,8 @@ void InstructionCodeGeneratorX86::VisitClearException([[maybe_unused]] HClearExc
 }
 
 void LocationsBuilderX86::VisitThrow(HThrow* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
 }
@@ -7789,8 +7765,7 @@ void LocationsBuilderX86::VisitInstanceOf(HInstanceOf* instruction) {
       break;
   }
 
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, instruction, call_kind);
   if (baker_read_barrier_slow_path) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -8118,8 +8093,7 @@ void InstructionCodeGeneratorX86::VisitInstanceOf(HInstanceOf* instruction) {
 void LocationsBuilderX86::VisitCheckCast(HCheckCast* instruction) {
   TypeCheckKind type_check_kind = instruction->GetTypeCheckKind();
   LocationSummary::CallKind call_kind = codegen_->GetCheckCastCallKind(instruction);
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, instruction, call_kind);
   locations->SetInAt(0, Location::RequiresRegister());
   if (type_check_kind == TypeCheckKind::kInterfaceCheck) {
     // Require a register for the interface check since there is a loop that compares the class to
@@ -8365,8 +8339,8 @@ void InstructionCodeGeneratorX86::VisitCheckCast(HCheckCast* instruction) {
 }
 
 void LocationsBuilderX86::VisitMonitorOperation(HMonitorOperation* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
 }
@@ -8384,7 +8358,7 @@ void InstructionCodeGeneratorX86::VisitMonitorOperation(HMonitorOperation* instr
 void LocationsBuilderX86::VisitX86AndNot(HX86AndNot* instruction) {
   DCHECK(codegen_->GetInstructionSetFeatures().HasAVX2());
   DCHECK(DataType::IsIntOrLongType(instruction->GetType())) << instruction->GetType();
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
@@ -8413,7 +8387,7 @@ void InstructionCodeGeneratorX86::VisitX86AndNot(HX86AndNot* instruction) {
 void LocationsBuilderX86::VisitX86MaskOrResetLeastSetBit(HX86MaskOrResetLeastSetBit* instruction) {
   DCHECK(codegen_->GetInstructionSetFeatures().HasAVX2());
   DCHECK(instruction->GetType() == DataType::Type::kInt32) << instruction->GetType();
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
@@ -8441,8 +8415,7 @@ void LocationsBuilderX86::VisitOr(HOr* instruction) { HandleBitwiseOperation(ins
 void LocationsBuilderX86::VisitXor(HXor* instruction) { HandleBitwiseOperation(instruction); }
 
 void LocationsBuilderX86::HandleBitwiseOperation(HBinaryOperation* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   DCHECK(instruction->GetResultType() == DataType::Type::kInt32
          || instruction->GetResultType() == DataType::Type::kInt64);
   locations->SetInAt(0, Location::RequiresRegister());
@@ -8882,8 +8855,7 @@ void InstructionCodeGeneratorX86::VisitBoundType([[maybe_unused]] HBoundType* in
 
 // Simple implementation of packed switch - generate cascaded compare/jumps.
 void LocationsBuilderX86::VisitPackedSwitch(HPackedSwitch* switch_instr) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(switch_instr, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, switch_instr);
   locations->SetInAt(0, Location::RequiresRegister());
 }
 
@@ -8949,8 +8921,7 @@ void InstructionCodeGeneratorX86::VisitPackedSwitch(HPackedSwitch* switch_instr)
 }
 
 void LocationsBuilderX86::VisitX86PackedSwitch(HX86PackedSwitch* switch_instr) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(switch_instr, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, switch_instr);
   locations->SetInAt(0, Location::RequiresRegister());
 
   // Constant area pointer.
@@ -9004,8 +8975,7 @@ void InstructionCodeGeneratorX86::VisitX86PackedSwitch(HX86PackedSwitch* switch_
 
 void LocationsBuilderX86::VisitX86ComputeBaseMethodAddress(
     HX86ComputeBaseMethodAddress* insn) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(insn, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, insn);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -9028,8 +8998,7 @@ void InstructionCodeGeneratorX86::VisitX86ComputeBaseMethodAddress(
 
 void LocationsBuilderX86::VisitX86LoadFromConstantTable(
     HX86LoadFromConstantTable* insn) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(insn, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, insn);
 
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::ConstantLocation(insn->GetConstant()));
diff --git a/compiler/optimizing/code_generator_x86.h b/compiler/optimizing/code_generator_x86.h
index 2c145b5133..5ed30f11af 100644
--- a/compiler/optimizing/code_generator_x86.h
+++ b/compiler/optimizing/code_generator_x86.h
@@ -229,7 +229,7 @@ class ParallelMoveResolverX86 : public ParallelMoveResolverWithSwap {
 class LocationsBuilderX86 : public HGraphVisitor {
  public:
   LocationsBuilderX86(HGraph* graph, CodeGeneratorX86* codegen)
-      : HGraphVisitor(graph), codegen_(codegen) {}
+      : HGraphVisitor(graph), codegen_(codegen), allocator_(graph->GetAllocator()) {}
 
 #define DECLARE_VISIT_INSTRUCTION(name, super)     \
   void Visit##name(H##name* instr) override;
@@ -259,6 +259,7 @@ class LocationsBuilderX86 : public HGraphVisitor {
   bool CpuHasAvx2FeatureFlag();
 
   CodeGeneratorX86* const codegen_;
+  ArenaAllocator* const allocator_;
   InvokeDexCallingConventionVisitorX86 parameter_visitor_;
 
   DISALLOW_COPY_AND_ASSIGN(LocationsBuilderX86);
diff --git a/compiler/optimizing/code_generator_x86_64.cc b/compiler/optimizing/code_generator_x86_64.cc
index 891bdd72e8..0b522a8f1f 100644
--- a/compiler/optimizing/code_generator_x86_64.cc
+++ b/compiler/optimizing/code_generator_x86_64.cc
@@ -1656,8 +1656,8 @@ static dwarf::Reg DWARFReg(FloatRegister reg) {
 }
 
 void LocationsBuilderX86_64::VisitMethodEntryHook(HMethodEntryHook* method_hook) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(method_hook, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, method_hook, LocationSummary::kCallOnSlowPath);
   // We use rdtsc to record the timestamp for method profiling. rdtsc returns
   // two 32-bit values in EAX + EDX even on 64-bit architectures.
   locations->AddTemp(Location::RegisterLocation(RAX));
@@ -1769,8 +1769,8 @@ void SetInForReturnValue(HInstruction* instr, LocationSummary* locations) {
 }
 
 void LocationsBuilderX86_64::VisitMethodExitHook(HMethodExitHook* method_hook) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(method_hook, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, method_hook, LocationSummary::kCallOnSlowPath);
   SetInForReturnValue(method_hook, locations);
   // We use rdtsc to record the timestamp for method profiling. rdtsc returns
   // two 32-bit values in EAX + EDX even on 64-bit architectures.
@@ -2349,7 +2349,7 @@ void InstructionCodeGeneratorX86_64::GenerateTestAndBranch(HInstruction* instruc
 }
 
 void LocationsBuilderX86_64::VisitIf(HIf* if_instr) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(if_instr);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, if_instr);
   if (IsBooleanValueOrMaterializedCondition(if_instr->InputAt(0))) {
     if (GetGraph()->IsCompilingBaseline() &&
         codegen_->GetCompilerOptions().ProfileBranches() &&
@@ -2400,8 +2400,8 @@ void InstructionCodeGeneratorX86_64::VisitIf(HIf* if_instr) {
 }
 
 void LocationsBuilderX86_64::VisitDeoptimize(HDeoptimize* deoptimize) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(deoptimize, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, deoptimize, LocationSummary::kCallOnSlowPath);
   InvokeRuntimeCallingConvention calling_convention;
   RegisterSet caller_saves = RegisterSet::Empty();
   caller_saves.Add(Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
@@ -2420,8 +2420,7 @@ void InstructionCodeGeneratorX86_64::VisitDeoptimize(HDeoptimize* deoptimize) {
 }
 
 void LocationsBuilderX86_64::VisitShouldDeoptimizeFlag(HShouldDeoptimizeFlag* flag) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator())
-      LocationSummary(flag, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, flag);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -2448,7 +2447,7 @@ static bool SelectCanUseCMOV(HSelect* select) {
 }
 
 void LocationsBuilderX86_64::VisitSelect(HSelect* select) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(select);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, select);
   if (DataType::IsFloatingPointType(select->GetType())) {
     locations->SetInAt(0, Location::RequiresFpuRegister());
     locations->SetInAt(1, Location::Any());
@@ -2528,7 +2527,7 @@ void InstructionCodeGeneratorX86_64::VisitSelect(HSelect* select) {
 }
 
 void LocationsBuilderX86_64::VisitNop(HNop* nop) {
-  new (GetGraph()->GetAllocator()) LocationSummary(nop);
+  LocationSummary::CreateNoCall(allocator_, nop);
 }
 
 void InstructionCodeGeneratorX86_64::VisitNop(HNop*) {
@@ -2550,8 +2549,7 @@ void CodeGeneratorX86_64::GenerateNop() {
 }
 
 void LocationsBuilderX86_64::HandleCondition(HCondition* cond) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(cond, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, cond);
   // Handle the long/FP comparisons made in instruction simplification.
   switch (cond->InputAt(0)->GetType()) {
     case DataType::Type::kInt64:
@@ -2724,8 +2722,7 @@ void InstructionCodeGeneratorX86_64::VisitAboveOrEqual(HAboveOrEqual* comp) {
 }
 
 void LocationsBuilderX86_64::VisitCompare(HCompare* compare) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(compare, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, compare);
   switch (compare->GetComparisonType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -2830,8 +2827,7 @@ void InstructionCodeGeneratorX86_64::VisitCompare(HCompare* compare) {
 }
 
 void LocationsBuilderX86_64::VisitIntConstant(HIntConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -2840,8 +2836,7 @@ void InstructionCodeGeneratorX86_64::VisitIntConstant([[maybe_unused]] HIntConst
 }
 
 void LocationsBuilderX86_64::VisitNullConstant(HNullConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -2850,8 +2845,7 @@ void InstructionCodeGeneratorX86_64::VisitNullConstant([[maybe_unused]] HNullCon
 }
 
 void LocationsBuilderX86_64::VisitLongConstant(HLongConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -2860,8 +2854,7 @@ void InstructionCodeGeneratorX86_64::VisitLongConstant([[maybe_unused]] HLongCon
 }
 
 void LocationsBuilderX86_64::VisitFloatConstant(HFloatConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -2870,8 +2863,7 @@ void InstructionCodeGeneratorX86_64::VisitFloatConstant([[maybe_unused]] HFloatC
 }
 
 void LocationsBuilderX86_64::VisitDoubleConstant(HDoubleConstant* constant) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, constant);
   locations->SetOut(Location::ConstantLocation(constant));
 }
 
@@ -2906,8 +2898,7 @@ void InstructionCodeGeneratorX86_64::VisitReturnVoid([[maybe_unused]] HReturnVoi
 }
 
 void LocationsBuilderX86_64::VisitReturn(HReturn* ret) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(ret, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, ret);
   SetInForReturnValue(ret, locations);
 }
 
@@ -3279,8 +3270,7 @@ void InstructionCodeGeneratorX86_64::VisitInvokeCustom(HInvokeCustom* invoke) {
 }
 
 void LocationsBuilderX86_64::VisitNeg(HNeg* neg) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(neg, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, neg);
   switch (neg->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
@@ -3345,8 +3335,7 @@ void InstructionCodeGeneratorX86_64::VisitNeg(HNeg* neg) {
 }
 
 void LocationsBuilderX86_64::VisitTypeConversion(HTypeConversion* conversion) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(conversion, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, conversion);
   DataType::Type result_type = conversion->GetResultType();
   DataType::Type input_type = conversion->GetInputType();
   DCHECK(!DataType::IsTypeConversionImplicit(input_type, result_type))
@@ -3816,8 +3805,7 @@ void InstructionCodeGeneratorX86_64::VisitTypeConversion(HTypeConversion* conver
 }
 
 void LocationsBuilderX86_64::VisitAdd(HAdd* add) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(add, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, add);
   switch (add->GetResultType()) {
     case DataType::Type::kInt32: {
       locations->SetInAt(0, Location::RequiresRegister());
@@ -3940,8 +3928,7 @@ void InstructionCodeGeneratorX86_64::VisitAdd(HAdd* add) {
 }
 
 void LocationsBuilderX86_64::VisitSub(HSub* sub) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(sub, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, sub);
   switch (sub->GetResultType()) {
     case DataType::Type::kInt32: {
       locations->SetInAt(0, Location::RequiresRegister());
@@ -4031,8 +4018,7 @@ void InstructionCodeGeneratorX86_64::VisitSub(HSub* sub) {
 }
 
 void LocationsBuilderX86_64::VisitMul(HMul* mul) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(mul, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, mul);
   switch (mul->GetResultType()) {
     case DataType::Type::kInt32: {
       locations->SetInAt(0, Location::RequiresRegister());
@@ -4534,8 +4520,7 @@ void InstructionCodeGeneratorX86_64::GenerateDivRemIntegral(HBinaryOperation* in
 }
 
 void LocationsBuilderX86_64::VisitDiv(HDiv* div) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(div, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, div);
   switch (div->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64: {
@@ -4617,8 +4602,7 @@ void InstructionCodeGeneratorX86_64::VisitDiv(HDiv* div) {
 
 void LocationsBuilderX86_64::VisitRem(HRem* rem) {
   DataType::Type type = rem->GetResultType();
-  LocationSummary* locations =
-    new (GetGraph()->GetAllocator()) LocationSummary(rem, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, rem);
 
   switch (type) {
     case DataType::Type::kInt32:
@@ -4669,7 +4653,7 @@ void InstructionCodeGeneratorX86_64::VisitRem(HRem* rem) {
 }
 
 static void CreateMinMaxLocations(ArenaAllocator* allocator, HBinaryOperation* minmax) {
-  LocationSummary* locations = new (allocator) LocationSummary(minmax);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, minmax);
   switch (minmax->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
@@ -4824,7 +4808,7 @@ void InstructionCodeGeneratorX86_64::GenerateMinMax(HBinaryOperation* minmax, bo
 }
 
 void LocationsBuilderX86_64::VisitMin(HMin* min) {
-  CreateMinMaxLocations(GetGraph()->GetAllocator(), min);
+  CreateMinMaxLocations(allocator_, min);
 }
 
 void InstructionCodeGeneratorX86_64::VisitMin(HMin* min) {
@@ -4832,7 +4816,7 @@ void InstructionCodeGeneratorX86_64::VisitMin(HMin* min) {
 }
 
 void LocationsBuilderX86_64::VisitMax(HMax* max) {
-  CreateMinMaxLocations(GetGraph()->GetAllocator(), max);
+  CreateMinMaxLocations(allocator_, max);
 }
 
 void InstructionCodeGeneratorX86_64::VisitMax(HMax* max) {
@@ -4840,7 +4824,7 @@ void InstructionCodeGeneratorX86_64::VisitMax(HMax* max) {
 }
 
 void LocationsBuilderX86_64::VisitAbs(HAbs* abs) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(abs);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, abs);
   switch (abs->GetResultType()) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
@@ -4960,8 +4944,7 @@ void InstructionCodeGeneratorX86_64::VisitDivZeroCheck(HDivZeroCheck* instructio
 void LocationsBuilderX86_64::HandleShift(HBinaryOperation* op) {
   DCHECK(op->IsShl() || op->IsShr() || op->IsUShr());
 
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(op, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, op);
 
   switch (op->GetResultType()) {
     case DataType::Type::kInt32:
@@ -5036,8 +5019,7 @@ void InstructionCodeGeneratorX86_64::HandleShift(HBinaryOperation* op) {
 }
 
 void LocationsBuilderX86_64::HandleRotate(HBinaryOperation* rotate) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(rotate, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, rotate);
 
   switch (rotate->GetResultType()) {
     case DataType::Type::kInt32:
@@ -5145,8 +5127,8 @@ void InstructionCodeGeneratorX86_64::VisitUShr(HUShr* ushr) {
 }
 
 void LocationsBuilderX86_64::VisitNewInstance(HNewInstance* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
   locations->SetOut(Location::RegisterLocation(RAX));
@@ -5159,8 +5141,8 @@ void InstructionCodeGeneratorX86_64::VisitNewInstance(HNewInstance* instruction)
 }
 
 void LocationsBuilderX86_64::VisitNewArray(HNewArray* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetOut(Location::RegisterLocation(RAX));
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
@@ -5176,8 +5158,7 @@ void InstructionCodeGeneratorX86_64::VisitNewArray(HNewArray* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitParameterValue(HParameterValue* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   Location location = parameter_visitor_.GetNextLocation(instruction->GetType());
   if (location.IsStackSlot()) {
     location = Location::StackSlot(location.GetStackIndex() + codegen_->GetFrameSize());
@@ -5193,8 +5174,7 @@ void InstructionCodeGeneratorX86_64::VisitParameterValue(
 }
 
 void LocationsBuilderX86_64::VisitCurrentMethod(HCurrentMethod* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetOut(Location::RegisterLocation(kMethodRegisterArgument));
 }
 
@@ -5204,8 +5184,7 @@ void InstructionCodeGeneratorX86_64::VisitCurrentMethod(
 }
 
 void LocationsBuilderX86_64::VisitClassTableGet(HClassTableGet* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister());
 }
@@ -5229,8 +5208,7 @@ void InstructionCodeGeneratorX86_64::VisitClassTableGet(HClassTableGet* instruct
 }
 
 void LocationsBuilderX86_64::VisitNot(HNot* not_) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(not_, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, not_);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::SameAsFirstInput());
 }
@@ -5255,8 +5233,7 @@ void InstructionCodeGeneratorX86_64::VisitNot(HNot* not_) {
 }
 
 void LocationsBuilderX86_64::VisitBooleanNot(HBooleanNot* bool_not) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(bool_not, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, bool_not);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::SameAsFirstInput());
 }
@@ -5270,8 +5247,7 @@ void InstructionCodeGeneratorX86_64::VisitBooleanNot(HBooleanNot* bool_not) {
 }
 
 void LocationsBuilderX86_64::VisitPhi(HPhi* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   for (size_t i = 0, e = locations->GetInputCount(); i < e; ++i) {
     locations->SetInAt(i, Location::Any());
   }
@@ -5311,11 +5287,11 @@ void LocationsBuilderX86_64::HandleFieldGet(HInstruction* instruction) {
 
   bool object_field_get_with_read_barrier =
       (instruction->GetType() == DataType::Type::kReference) && codegen_->EmitReadBarrier();
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction,
-                                                       object_field_get_with_read_barrier
-                                                           ? LocationSummary::kCallOnSlowPath
-                                                           : LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
+      instruction,
+      object_field_get_with_read_barrier ? LocationSummary::kCallOnSlowPath
+                                         : LocationSummary::kNoCall);
   if (object_field_get_with_read_barrier && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -5382,8 +5358,7 @@ void LocationsBuilderX86_64::HandleFieldSet(HInstruction* instruction,
                                             WriteBarrierKind write_barrier_kind) {
   DCHECK(instruction->IsInstanceFieldSet() || instruction->IsStaticFieldSet());
 
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   DataType::Type field_type = field_info.GetFieldType();
   bool is_volatile = field_info.IsVolatile();
   bool needs_write_barrier =
@@ -5806,11 +5781,11 @@ void InstructionCodeGeneratorX86_64::VisitNullCheck(HNullCheck* instruction) {
 void LocationsBuilderX86_64::VisitArrayGet(HArrayGet* instruction) {
   bool object_array_get_with_read_barrier =
       (instruction->GetType() == DataType::Type::kReference) && codegen_->EmitReadBarrier();
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction,
-                                                       object_array_get_with_read_barrier
-                                                           ? LocationSummary::kCallOnSlowPath
-                                                           : LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
+      instruction,
+      object_array_get_with_read_barrier ? LocationSummary::kCallOnSlowPath
+                                         : LocationSummary::kNoCall);
   if (object_array_get_with_read_barrier && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -5901,7 +5876,8 @@ void LocationsBuilderX86_64::VisitArraySet(HArraySet* instruction) {
       codegen_->ShouldCheckGCCard(value_type, instruction->GetValue(), write_barrier_kind);
   bool needs_type_check = instruction->NeedsTypeCheck();
 
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
       instruction,
       needs_type_check ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall);
 
@@ -6166,8 +6142,7 @@ void InstructionCodeGeneratorX86_64::VisitArraySet(HArraySet* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitArrayLength(HArrayLength* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   if (!instruction->IsEmittedAtUseSite()) {
     locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
@@ -6344,8 +6319,8 @@ void InstructionCodeGeneratorX86_64::VisitParallelMove(HParallelMove* instructio
 }
 
 void LocationsBuilderX86_64::VisitSuspendCheck(HSuspendCheck* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnSlowPath);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnSlowPath);
   // In suspend check slow path, usually there are no caller-save registers at all.
   // If SIMD instructions are present, however, we force spilling all live SIMD
   // registers in full width (since the runtime only saves/restores lower part).
@@ -6707,7 +6682,7 @@ void LocationsBuilderX86_64::VisitLoadClass(HLoadClass* cls) {
   LocationSummary::CallKind call_kind = (cls->NeedsEnvironment() || requires_read_barrier)
       ? LocationSummary::kCallOnSlowPath
       : LocationSummary::kNoCall;
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(cls, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, cls, call_kind);
   if (kUseBakerReadBarrier && requires_read_barrier && !cls->NeedsEnvironment()) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -6846,7 +6821,7 @@ void InstructionCodeGeneratorX86_64::VisitLoadClass(HLoadClass* cls) NO_THREAD_S
 
 void LocationsBuilderX86_64::VisitClinitCheck(HClinitCheck* check) {
   LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(check, LocationSummary::kCallOnSlowPath);
+      LocationSummary::Create(allocator_, check, LocationSummary::kCallOnSlowPath);
   locations->SetInAt(0, Location::RequiresRegister());
   if (check->HasUses()) {
     locations->SetOut(Location::SameAsFirstInput());
@@ -6877,7 +6852,7 @@ Label* CodeGeneratorX86_64::NewJitRootMethodTypePatch(const DexFile& dex_file,
 
 void LocationsBuilderX86_64::VisitLoadMethodType(HLoadMethodType* load) {
   LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(load, LocationSummary::kCallOnSlowPath);
+      LocationSummary::Create(allocator_, load, LocationSummary::kCallOnSlowPath);
   if (load->GetLoadKind() == HLoadMethodType::LoadKind::kRuntimeCall) {
     Location location = Location::RegisterLocation(RAX);
     CodeGenerator::CreateLoadMethodTypeRuntimeCallLocationSummary(load, location, location);
@@ -6963,7 +6938,7 @@ HLoadString::LoadKind CodeGeneratorX86_64::GetSupportedLoadStringKind(
 
 void LocationsBuilderX86_64::VisitLoadString(HLoadString* load) {
   LocationSummary::CallKind call_kind = codegen_->GetLoadStringCallKind(load);
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(load, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, load, call_kind);
   if (load->GetLoadKind() == HLoadString::LoadKind::kRuntimeCall) {
     locations->SetOut(Location::RegisterLocation(RAX));
   } else {
@@ -7059,8 +7034,7 @@ static Address GetExceptionTlsAddress() {
 }
 
 void LocationsBuilderX86_64::VisitLoadException(HLoadException* load) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(load, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, load);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -7069,7 +7043,7 @@ void InstructionCodeGeneratorX86_64::VisitLoadException(HLoadException* load) {
 }
 
 void LocationsBuilderX86_64::VisitClearException(HClearException* clear) {
-  new (GetGraph()->GetAllocator()) LocationSummary(clear, LocationSummary::kNoCall);
+  LocationSummary::CreateNoCall(allocator_, clear);
 }
 
 void InstructionCodeGeneratorX86_64::VisitClearException([[maybe_unused]] HClearException* clear) {
@@ -7077,8 +7051,8 @@ void InstructionCodeGeneratorX86_64::VisitClearException([[maybe_unused]] HClear
 }
 
 void LocationsBuilderX86_64::VisitThrow(HThrow* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
 }
@@ -7134,8 +7108,7 @@ void LocationsBuilderX86_64::VisitInstanceOf(HInstanceOf* instruction) {
       break;
   }
 
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, instruction, call_kind);
   if (baker_read_barrier_slow_path) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -7474,8 +7447,7 @@ void InstructionCodeGeneratorX86_64::VisitInstanceOf(HInstanceOf* instruction) {
 void LocationsBuilderX86_64::VisitCheckCast(HCheckCast* instruction) {
   TypeCheckKind type_check_kind = instruction->GetTypeCheckKind();
   LocationSummary::CallKind call_kind = codegen_->GetCheckCastCallKind(instruction);
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  LocationSummary* locations = LocationSummary::Create(allocator_, instruction, call_kind);
   locations->SetInAt(0, Location::RequiresRegister());
   if (type_check_kind == TypeCheckKind::kInterfaceCheck) {
     // Require a register for the interface check since there is a loop that compares the class to
@@ -7725,8 +7697,8 @@ void InstructionCodeGeneratorX86_64::VisitCheckCast(HCheckCast* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitMonitorOperation(HMonitorOperation* instruction) {
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
-      instruction, LocationSummary::kCallOnMainOnly);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, instruction, LocationSummary::kCallOnMainOnly);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
 }
@@ -7744,7 +7716,7 @@ void InstructionCodeGeneratorX86_64::VisitMonitorOperation(HMonitorOperation* in
 void LocationsBuilderX86_64::VisitX86AndNot(HX86AndNot* instruction) {
   DCHECK(codegen_->GetInstructionSetFeatures().HasAVX2());
   DCHECK(DataType::IsIntOrLongType(instruction->GetType())) << instruction->GetType();
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   // There is no immediate variant of negated bitwise and in X86.
   locations->SetInAt(1, Location::RequiresRegister());
@@ -7754,7 +7726,7 @@ void LocationsBuilderX86_64::VisitX86AndNot(HX86AndNot* instruction) {
 void LocationsBuilderX86_64::VisitX86MaskOrResetLeastSetBit(HX86MaskOrResetLeastSetBit* instruction) {
   DCHECK(codegen_->GetInstructionSetFeatures().HasAVX2());
   DCHECK(DataType::IsIntOrLongType(instruction->GetType())) << instruction->GetType();
-  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
@@ -7788,8 +7760,7 @@ void LocationsBuilderX86_64::VisitOr(HOr* instruction) { HandleBitwiseOperation(
 void LocationsBuilderX86_64::VisitXor(HXor* instruction) { HandleBitwiseOperation(instruction); }
 
 void LocationsBuilderX86_64::HandleBitwiseOperation(HBinaryOperation* instruction) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, instruction);
   DCHECK(instruction->GetResultType() == DataType::Type::kInt32
          || instruction->GetResultType() == DataType::Type::kInt64);
   locations->SetInAt(0, Location::RequiresRegister());
@@ -8213,8 +8184,7 @@ void InstructionCodeGeneratorX86_64::VisitBoundType([[maybe_unused]] HBoundType*
 
 // Simple implementation of packed switch - generate cascaded compare/jumps.
 void LocationsBuilderX86_64::VisitPackedSwitch(HPackedSwitch* switch_instr) {
-  LocationSummary* locations =
-      new (GetGraph()->GetAllocator()) LocationSummary(switch_instr, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, switch_instr);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->AddRegisterTemps(2);
 }
diff --git a/compiler/optimizing/code_generator_x86_64.h b/compiler/optimizing/code_generator_x86_64.h
index 8a514b21f0..91e4100b4b 100644
--- a/compiler/optimizing/code_generator_x86_64.h
+++ b/compiler/optimizing/code_generator_x86_64.h
@@ -226,7 +226,7 @@ class ParallelMoveResolverX86_64 : public ParallelMoveResolverWithSwap {
 class LocationsBuilderX86_64 : public HGraphVisitor {
  public:
   LocationsBuilderX86_64(HGraph* graph, CodeGeneratorX86_64* codegen)
-      : HGraphVisitor(graph), codegen_(codegen) {}
+      : HGraphVisitor(graph), codegen_(codegen), allocator_(graph->GetAllocator()) {}
 
 #define DECLARE_VISIT_INSTRUCTION(name, super)     \
   void Visit##name(H##name* instr) override;
@@ -256,6 +256,7 @@ class LocationsBuilderX86_64 : public HGraphVisitor {
   bool CpuHasAvx2FeatureFlag();
 
   CodeGeneratorX86_64* const codegen_;
+  ArenaAllocator* const allocator_;
   InvokeDexCallingConventionVisitorX86_64 parameter_visitor_;
 
   DISALLOW_COPY_AND_ASSIGN(LocationsBuilderX86_64);
diff --git a/compiler/optimizing/code_sinking.cc b/compiler/optimizing/code_sinking.cc
index 9ba5416697..6bcc2f5a26 100644
--- a/compiler/optimizing/code_sinking.cc
+++ b/compiler/optimizing/code_sinking.cc
@@ -175,10 +175,10 @@ static void AddInputs(HBasicBlock* block,
                       BitVectorView<size_t> processed_instructions,
                       BitVectorView<size_t> discard_blocks,
                       ScopedArenaVector<HInstruction*>* worklist) {
-  for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
     AddInputs(it.Current(), processed_instructions, discard_blocks, worklist);
   }
-  for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
     AddInputs(it.Current(), processed_instructions, discard_blocks, worklist);
   }
 }
@@ -601,7 +601,7 @@ void CodeSinking::ReturnSinking() {
 
   // `new_block` will coalesce the Return instructions into Phi+Return, or the ReturnVoid
   // instructions into a ReturnVoid.
-  HBasicBlock* new_block = new (graph_->GetAllocator()) HBasicBlock(graph_, exit->GetDexPc());
+  HBasicBlock* new_block = HBasicBlock::Create(graph_->GetAllocator(), graph_, exit->GetDexPc());
   if (saw_return) {
     HPhi* new_phi = nullptr;
     for (size_t i = 0; i < exit->GetPredecessors().size(); /*++i in loop*/) {
diff --git a/compiler/optimizing/code_sinking.h b/compiler/optimizing/code_sinking.h
index c743db40d9..88eeccee23 100644
--- a/compiler/optimizing/code_sinking.h
+++ b/compiler/optimizing/code_sinking.h
@@ -18,11 +18,12 @@
 #define ART_COMPILER_OPTIMIZING_CODE_SINKING_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
 
 namespace art HIDDEN {
 
+class HBasicBlock;
+
 /**
  * Optimization pass to move instructions into uncommon branches,
  * when it is safe to do so.
diff --git a/compiler/optimizing/codegen_test.cc b/compiler/optimizing/codegen_test.cc
index 98b0550c4d..188a5c2dbe 100644
--- a/compiler/optimizing/codegen_test.cc
+++ b/compiler/optimizing/codegen_test.cc
@@ -420,27 +420,21 @@ TEST_F(CodegenTest, NonMaterializedCondition) {
   for (CodegenTargetConfig target_config : GetTargetConfigs()) {
     HGraph* graph = CreateGraph();
 
-    HBasicBlock* entry = new (GetAllocator()) HBasicBlock(graph);
-    graph->AddBlock(entry);
+    HBasicBlock* entry = AddNewBlock();
     graph->SetEntryBlock(entry);
     MakeGoto(entry);
 
-    HBasicBlock* first_block = new (GetAllocator()) HBasicBlock(graph);
-    graph->AddBlock(first_block);
+    HBasicBlock* first_block = AddNewBlock();
     entry->AddSuccessor(first_block);
     HIntConstant* constant0 = graph->GetIntConstant(0);
     HIntConstant* constant1 = graph->GetIntConstant(1);
     HInstruction* equal = MakeCondition(first_block, kCondEQ, constant0, constant0);
     MakeIf(first_block, equal);
 
-    HBasicBlock* then_block = new (GetAllocator()) HBasicBlock(graph);
-    HBasicBlock* else_block = new (GetAllocator()) HBasicBlock(graph);
-    HBasicBlock* exit_block = new (GetAllocator()) HBasicBlock(graph);
-    graph->SetExitBlock(exit_block);
+    HBasicBlock* then_block = AddNewBlock();
+    HBasicBlock* else_block = AddNewBlock();
+    HBasicBlock* exit_block = AddExitBlock();
 
-    graph->AddBlock(then_block);
-    graph->AddBlock(else_block);
-    graph->AddBlock(exit_block);
     first_block->AddSuccessor(then_block);
     first_block->AddSuccessor(else_block);
     then_block->AddSuccessor(exit_block);
@@ -479,28 +473,14 @@ TEST_F(CodegenTest, MaterializedCondition1) {
     int rhs[] = {2, 1, 2, -1, 0xabc};
 
     for (size_t i = 0; i < arraysize(lhs); i++) {
-      HGraph* graph = CreateGraph();
-
-      HBasicBlock* entry_block = new (GetAllocator()) HBasicBlock(graph);
-      graph->AddBlock(entry_block);
-      graph->SetEntryBlock(entry_block);
-      MakeGoto(entry_block);
-      HBasicBlock* code_block = new (GetAllocator()) HBasicBlock(graph);
-      graph->AddBlock(code_block);
-      HBasicBlock* exit_block = new (GetAllocator()) HBasicBlock(graph);
-      graph->AddBlock(exit_block);
-      MakeExit(exit_block);
-
-      entry_block->AddSuccessor(code_block);
-      code_block->AddSuccessor(exit_block);
-      graph->SetExitBlock(exit_block);
+      HBasicBlock* code_block = InitEntryMainExitGraph();
 
-      HIntConstant* cst_lhs = graph->GetIntConstant(lhs[i]);
-      HIntConstant* cst_rhs = graph->GetIntConstant(rhs[i]);
+      HIntConstant* cst_lhs = graph_->GetIntConstant(lhs[i]);
+      HIntConstant* cst_rhs = graph_->GetIntConstant(rhs[i]);
       HInstruction* cmp_lt = MakeCondition(code_block, kCondLT, cst_lhs, cst_rhs);
       MakeReturn(code_block, cmp_lt);
 
-      graph->BuildDominatorTree();
+      graph_->BuildDominatorTree();
       auto hook_before_codegen = [](HGraph* graph_in) {
         HBasicBlock* block = graph_in->GetEntryBlock()->GetSuccessors()[0];
         HParallelMove* move =
@@ -509,7 +489,7 @@ TEST_F(CodegenTest, MaterializedCondition1) {
       };
       std::unique_ptr<CompilerOptions> compiler_options =
           CommonCompilerTest::CreateCompilerOptions(target_config.GetInstructionSet(), "default");
-      RunCode(target_config, *compiler_options, graph, hook_before_codegen, true, lhs[i] < rhs[i]);
+      RunCode(target_config, *compiler_options, graph_, hook_before_codegen, true, lhs[i] < rhs[i]);
     }
   }
 }
@@ -528,19 +508,14 @@ TEST_F(CodegenTest, MaterializedCondition2) {
     for (size_t i = 0; i < arraysize(lhs); i++) {
       HGraph* graph = CreateGraph();
 
-      HBasicBlock* entry_block = new (GetAllocator()) HBasicBlock(graph);
-      graph->AddBlock(entry_block);
+      HBasicBlock* entry_block = AddNewBlock();
       graph->SetEntryBlock(entry_block);
       MakeGoto(entry_block);
 
-      HBasicBlock* if_block = new (GetAllocator()) HBasicBlock(graph);
-      graph->AddBlock(if_block);
-      HBasicBlock* if_true_block = new (GetAllocator()) HBasicBlock(graph);
-      graph->AddBlock(if_true_block);
-      HBasicBlock* if_false_block = new (GetAllocator()) HBasicBlock(graph);
-      graph->AddBlock(if_false_block);
-      HBasicBlock* exit_block = new (GetAllocator()) HBasicBlock(graph);
-      graph->AddBlock(exit_block);
+      HBasicBlock* if_block = AddNewBlock();
+      HBasicBlock* if_true_block = AddNewBlock();
+      HBasicBlock* if_false_block = AddNewBlock();
+      HBasicBlock* exit_block = AddExitBlock();
       MakeExit(exit_block);
 
       graph->SetEntryBlock(entry_block);
@@ -549,7 +524,6 @@ TEST_F(CodegenTest, MaterializedCondition2) {
       if_block->AddSuccessor(if_false_block);
       if_true_block->AddSuccessor(exit_block);
       if_false_block->AddSuccessor(exit_block);
-      graph->SetExitBlock(exit_block);
 
       HIntConstant* cst_lhs = graph->GetIntConstant(lhs[i]);
       HIntConstant* cst_rhs = graph->GetIntConstant(rhs[i]);
diff --git a/compiler/optimizing/codegen_test_utils.h b/compiler/optimizing/codegen_test_utils.h
index 130c796652..5b0e7588e7 100644
--- a/compiler/optimizing/codegen_test_utils.h
+++ b/compiler/optimizing/codegen_test_utils.h
@@ -22,7 +22,6 @@
 #include "arch/x86/registers_x86.h"
 #include "base/macros.h"
 #include "code_simulator.h"
-#include "code_simulator_container.h"
 #include "common_compiler_test.h"
 #include "graph_checker.h"
 #include "prepare_for_register_allocation.h"
@@ -184,13 +183,13 @@ static bool CanExecuteOnHardware(const CodeGenerator& codegen) {
 }
 
 static bool CanExecuteISA(InstructionSet target_isa) {
-  CodeSimulatorContainer simulator(target_isa);
-  return DoesHardwareSupportISA(target_isa) || simulator.CanSimulate();
+  std::unique_ptr<CodeSimulator> simulator(CreateCodeSimulator(target_isa));
+  return DoesHardwareSupportISA(target_isa) || bool(simulator);
 }
 
 static bool CanExecute(const CodeGenerator& codegen) {
-  CodeSimulatorContainer simulator(codegen.GetInstructionSet());
-  return CanExecuteOnHardware(codegen) || simulator.CanSimulate();
+  std::unique_ptr<CodeSimulator> simulator(CreateCodeSimulator(codegen.GetInstructionSet()));
+  return CanExecuteOnHardware(codegen) || bool(simulator);
 }
 
 template <typename Expected>
@@ -222,9 +221,9 @@ static void VerifyGeneratedCode(const CodeGenerator& codegen,
   ASSERT_TRUE(CanExecute(codegen)) << "Target isa is not executable.";
 
   // Verify on simulator.
-  CodeSimulatorContainer simulator(codegen.GetInstructionSet());
-  if (simulator.CanSimulate()) {
-    Expected result = SimulatorExecute<Expected>(simulator.Get(), f);
+  std::unique_ptr<CodeSimulator> simulator(CreateCodeSimulator(codegen.GetInstructionSet()));
+  if (simulator) {
+    Expected result = SimulatorExecute<Expected>(simulator.get(), f);
     if (has_result) {
       ASSERT_EQ(expected, result);
     }
diff --git a/compiler/optimizing/constant_folding.cc b/compiler/optimizing/constant_folding.cc
index 66fb04e11a..12cfe08b5e 100644
--- a/compiler/optimizing/constant_folding.cc
+++ b/compiler/optimizing/constant_folding.cc
@@ -205,7 +205,7 @@ void HConstantFoldingVisitor::VisitBinaryOperation(HBinaryOperation* inst) {
     // Already replaced inside TryRemoveBinaryOperationViaSelect.
   } else {
     InstructionWithAbsorbingInputSimplifier simplifier(GetGraph());
-    inst->Accept(&simplifier);
+    simplifier.Dispatch(inst);
   }
 }
 
diff --git a/compiler/optimizing/constant_folding.h b/compiler/optimizing/constant_folding.h
index 73ac3ceb81..5757c8c283 100644
--- a/compiler/optimizing/constant_folding.h
+++ b/compiler/optimizing/constant_folding.h
@@ -18,7 +18,6 @@
 #define ART_COMPILER_OPTIMIZING_CONSTANT_FOLDING_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
 #include "optimizing/optimizing_compiler_stats.h"
 
diff --git a/compiler/optimizing/constructor_fence_redundancy_elimination.cc b/compiler/optimizing/constructor_fence_redundancy_elimination.cc
index c89ec171d9..2783b2c38d 100644
--- a/compiler/optimizing/constructor_fence_redundancy_elimination.cc
+++ b/compiler/optimizing/constructor_fence_redundancy_elimination.cc
@@ -21,6 +21,7 @@
 #include "base/bit_vector-inl.h"
 #include "base/scoped_arena_allocator.h"
 #include "base/scoped_arena_containers.h"
+#include "nodes.h"
 
 namespace art HIDDEN {
 
diff --git a/compiler/optimizing/control_flow_simplifier.cc b/compiler/optimizing/control_flow_simplifier.cc
index 35efed59da..4d1f7f6b6a 100644
--- a/compiler/optimizing/control_flow_simplifier.cc
+++ b/compiler/optimizing/control_flow_simplifier.cc
@@ -39,7 +39,7 @@ static bool IsSimpleBlock(HBasicBlock* block) {
   DCHECK(block->GetPhis().IsEmpty());
 
   size_t num_instructions = 0u;
-  for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
     HInstruction* instruction = it.Current();
     if (instruction->IsControlFlow()) {
       return instruction->IsGoto() || instruction->IsReturn();
@@ -78,7 +78,7 @@ static std::pair<bool, HPhi*> HasAtMostOnePhiWithDifferentInputs(HBasicBlock* bl
   DCHECK_NE(index1, index2);
 
   HPhi* select_phi = nullptr;
-  for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
     HPhi* phi = it.Current()->AsPhi();
     auto&& inputs = phi->GetInputs();
     if (inputs[index1] == inputs[index2]) {
diff --git a/compiler/optimizing/control_flow_simplifier.h b/compiler/optimizing/control_flow_simplifier.h
index 0c74ec299f..1be8fa025e 100644
--- a/compiler/optimizing/control_flow_simplifier.h
+++ b/compiler/optimizing/control_flow_simplifier.h
@@ -60,10 +60,13 @@
 #include "base/macros.h"
 #include "base/scoped_arena_containers.h"
 #include "optimization.h"
-#include "optimizing/nodes.h"
 
 namespace art HIDDEN {
 
+class HBasicBlock;
+class HInstruction;
+class HSelect;
+
 class HControlFlowSimplifier : public HOptimization {
  public:
   HControlFlowSimplifier(HGraph* graph,
diff --git a/compiler/optimizing/control_flow_simplifier_test.cc b/compiler/optimizing/control_flow_simplifier_test.cc
index 2e88c6c77b..bcbe341600 100644
--- a/compiler/optimizing/control_flow_simplifier_test.cc
+++ b/compiler/optimizing/control_flow_simplifier_test.cc
@@ -41,9 +41,6 @@ class ControlFlowSimplifierTest : public OptimizingUnitTest {
   bool CheckGraphAndTryControlFlowSimplifier() {
     graph_->BuildDominatorTree();
     EXPECT_TRUE(CheckGraph());
-
-    SideEffectsAnalysis side_effects(graph_);
-    side_effects.Run();
     return HControlFlowSimplifier(graph_, /*handles*/ nullptr, /*stats*/ nullptr).Run();
   }
 };
@@ -143,7 +140,7 @@ TEST_F(ControlFlowSimplifierTest, testSelectInIrreducibleLoop) {
   for (HBasicBlock* removed_block : {then_block, else_block, body}) {
     ASSERT_BLOCK_REMOVED(removed_block);
     uint32_t removed_block_id = removed_block->GetBlockId();
-    ASSERT_FALSE(loop_info->GetBlocks().IsBitSet(removed_block_id)) << removed_block_id;
+    ASSERT_FALSE(loop_info->GetBlockMask().IsBitSet(removed_block_id)) << removed_block_id;
   }
 }
 
diff --git a/compiler/optimizing/critical_native_abi_fixup_arm.cc b/compiler/optimizing/critical_native_abi_fixup_arm.cc
index 4b1dec05b5..c368de59a1 100644
--- a/compiler/optimizing/critical_native_abi_fixup_arm.cc
+++ b/compiler/optimizing/critical_native_abi_fixup_arm.cc
@@ -51,7 +51,7 @@ bool CriticalNativeAbiFixupArm::Run() {
   }
 
   for (HBasicBlock* block : graph_->GetReversePostOrder()) {
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       HInstruction* instruction = it.Current();
       if (instruction->IsInvokeStaticOrDirect() &&
           instruction->AsInvokeStaticOrDirect()->GetCodePtrLocation() ==
diff --git a/compiler/optimizing/critical_native_abi_fixup_arm.h b/compiler/optimizing/critical_native_abi_fixup_arm.h
index c2068f5e2d..77c2fac410 100644
--- a/compiler/optimizing/critical_native_abi_fixup_arm.h
+++ b/compiler/optimizing/critical_native_abi_fixup_arm.h
@@ -18,7 +18,6 @@
 #define ART_COMPILER_OPTIMIZING_CRITICAL_NATIVE_ABI_FIXUP_ARM_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
 
 namespace art HIDDEN {
diff --git a/compiler/optimizing/critical_native_abi_fixup_riscv64.cc b/compiler/optimizing/critical_native_abi_fixup_riscv64.cc
index c2c98d1df9..54fce78daf 100644
--- a/compiler/optimizing/critical_native_abi_fixup_riscv64.cc
+++ b/compiler/optimizing/critical_native_abi_fixup_riscv64.cc
@@ -55,7 +55,7 @@ bool CriticalNativeAbiFixupRiscv64::Run() {
   }
 
   for (HBasicBlock* block : graph_->GetReversePostOrder()) {
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       HInstruction* instruction = it.Current();
       if (instruction->IsInvokeStaticOrDirect() &&
           instruction->AsInvokeStaticOrDirect()->GetCodePtrLocation() ==
diff --git a/compiler/optimizing/critical_native_abi_fixup_riscv64.h b/compiler/optimizing/critical_native_abi_fixup_riscv64.h
index dc76cff2b8..a04a7eb13e 100644
--- a/compiler/optimizing/critical_native_abi_fixup_riscv64.h
+++ b/compiler/optimizing/critical_native_abi_fixup_riscv64.h
@@ -18,7 +18,6 @@
 #define ART_COMPILER_OPTIMIZING_CRITICAL_NATIVE_ABI_FIXUP_RISCV64_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
 
 namespace art HIDDEN {
diff --git a/compiler/optimizing/data_type.h b/compiler/optimizing/data_type.h
index 40d5e56d08..11ca968035 100644
--- a/compiler/optimizing/data_type.h
+++ b/compiler/optimizing/data_type.h
@@ -122,11 +122,11 @@ class DataType {
     }
   }
 
-  static bool IsFloatingPointType(Type type) {
+  static constexpr bool IsFloatingPointType(Type type) {
     return type == Type::kFloat32 || type == Type::kFloat64;
   }
 
-  static bool IsIntegralType(Type type) {
+  static constexpr bool IsIntegralType(Type type) {
     // The Java language does not allow treating boolean as an integral type but
     // our bit representation makes it safe.
     switch (type) {
@@ -145,19 +145,19 @@ class DataType {
     }
   }
 
-  static bool IsIntOrLongType(Type type) {
+  static constexpr bool IsIntOrLongType(Type type) {
     return type == Type::kInt32 || type == Type::kInt64;
   }
 
-  static bool Is64BitType(Type type) {
+  static constexpr bool Is64BitType(Type type) {
     return type == Type::kUint64 || type == Type::kInt64 || type == Type::kFloat64;
   }
 
-  static bool Is8BitType(Type type) {
+  static constexpr bool Is8BitType(Type type) {
     return type == Type::kInt8 || type == Type::kUint8 || type == Type::kBool;
   }
 
-  static bool IsUnsignedType(Type type) {
+  static constexpr bool IsUnsignedType(Type type) {
     return type == Type::kBool || type == Type::kUint8 || type == Type::kUint16 ||
         type == Type::kUint32 || type == Type::kUint64;
   }
@@ -181,7 +181,7 @@ class DataType {
     }
   }
 
-  static int64_t MinValueOfIntegralType(Type type) {
+  static constexpr int64_t MinValueOfIntegralType(Type type) {
     switch (type) {
       case Type::kBool:
         return std::numeric_limits<bool>::min();
@@ -207,7 +207,7 @@ class DataType {
     return 0;
   }
 
-  static int64_t MaxValueOfIntegralType(Type type) {
+  static constexpr int64_t MaxValueOfIntegralType(Type type) {
     switch (type) {
       case Type::kBool:
         return std::numeric_limits<bool>::max();
@@ -242,7 +242,7 @@ class DataType {
         Size(result_type) > Size(input_type);
   }
 
-  static Type ToSigned(Type type) {
+  static constexpr Type ToSigned(Type type) {
     switch (type) {
       case Type::kUint8:
         return Type::kInt8;
@@ -257,7 +257,7 @@ class DataType {
     }
   }
 
-  static Type ToUnsigned(Type type) {
+  static constexpr Type ToUnsigned(Type type) {
     switch (type) {
       case Type::kInt8:
         return Type::kUint8;
diff --git a/compiler/optimizing/dead_code_elimination.cc b/compiler/optimizing/dead_code_elimination.cc
index c367a20a06..7f0ed33495 100644
--- a/compiler/optimizing/dead_code_elimination.cc
+++ b/compiler/optimizing/dead_code_elimination.cc
@@ -267,7 +267,7 @@ bool HDeadCodeElimination::SimplifyAlwaysThrows() {
     // We iterate to find the first instruction that always throws. If two instructions always
     // throw, the first one will throw and the second one will never be reached.
     HInstruction* throwing_invoke = nullptr;
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       if (it.Current()->IsInvoke() && it.Current()->AsInvoke()->AlwaysThrows()) {
         throwing_invoke = it.Current();
         break;
@@ -291,10 +291,7 @@ bool HDeadCodeElimination::SimplifyAlwaysThrows() {
     // We split the block at the throwing instruction, and the instructions after the throwing
     // instructions will be disconnected from the graph after `block` points to the exit.
     // `RemoveDeadBlocks` will take care of removing this new block and its instructions.
-    // Even though `SplitBefore` doesn't guarantee the graph to remain in SSA form, it is fine
-    // since we do not break it.
-    HBasicBlock* new_block = block->SplitBefore(throwing_invoke->GetNext(),
-                                                /* require_graph_not_in_ssa_form= */ false);
+    HBasicBlock* new_block = block->SplitBefore(throwing_invoke->GetNext());
     DCHECK_EQ(block->GetSingleSuccessor(), new_block);
     block->ReplaceSuccessor(new_block, exit);
 
@@ -605,7 +602,8 @@ struct HDeadCodeElimination::TryBelongingInformation {
 bool HDeadCodeElimination::CanPerformTryRemoval(const TryBelongingInformation& try_belonging_info) {
   const ArenaVector<HBasicBlock*>& blocks = graph_->GetBlocks();
   for (uint32_t i : try_belonging_info.blocks_in_try.Indexes()) {
-    for (HInstructionIterator it(blocks[i]->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(blocks[i]->GetInstructions()); !it.Done();
+         it.Advance()) {
       if (it.Current()->CanThrow()) {
         return false;
       }
@@ -917,7 +915,7 @@ void HDeadCodeElimination::RemoveDeadInstructions() {
   for (HBasicBlock* block : graph_->GetPostOrder()) {
     // Traverse this block's instructions in backward order and remove
     // the unused ones.
-    HBackwardInstructionIterator i(block->GetInstructions());
+    HBackwardInstructionIteratorPrefetchNext i(block->GetInstructions());
     // Skip the first iteration, as the last instruction of a block is
     // a branching instruction.
     DCHECK(i.Current()->IsControlFlow());
@@ -931,7 +929,8 @@ void HDeadCodeElimination::RemoveDeadInstructions() {
     }
 
     // Same for Phis.
-    for (HBackwardInstructionIterator phi_it(block->GetPhis()); !phi_it.Done(); phi_it.Advance()) {
+    for (HBackwardInstructionIteratorPrefetchNext phi_it(block->GetPhis()); !phi_it.Done();
+         phi_it.Advance()) {
       DCHECK(phi_it.Current()->IsPhi());
       HPhi* phi = phi_it.Current()->AsPhi();
       if (phi->IsPhiDeadAndRemovable()) {
@@ -950,7 +949,7 @@ void HDeadCodeElimination::UpdateGraphFlags() {
   bool has_always_throwing_invokes = false;
 
   for (HBasicBlock* block : graph_->GetReversePostOrder()) {
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       HInstruction* instruction = it.Current();
       if (instruction->IsMonitorOperation()) {
         has_monitor_operations = true;
diff --git a/compiler/optimizing/dead_code_elimination.h b/compiler/optimizing/dead_code_elimination.h
index 789962f93c..c688c2890d 100644
--- a/compiler/optimizing/dead_code_elimination.h
+++ b/compiler/optimizing/dead_code_elimination.h
@@ -18,12 +18,13 @@
 #define ART_COMPILER_OPTIMIZING_DEAD_CODE_ELIMINATION_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
-#include "optimizing_compiler_stats.h"
 
 namespace art HIDDEN {
 
+class HBasicBlock;
+class TryBelongingInformation;
+
 /**
  * Optimization pass performing dead code elimination (removal of
  * unused variables/instructions) on the SSA form.
diff --git a/compiler/optimizing/escape.cc b/compiler/optimizing/escape.cc
index cebe94fd0d..93473ff3bb 100644
--- a/compiler/optimizing/escape.cc
+++ b/compiler/optimizing/escape.cc
@@ -61,6 +61,10 @@ void VisitEscapes(HInstruction* reference, EscapeVisitor& escape_visitor) {
       if (!escape_visitor(user)) {
         return;
       }
+    } else if (user->IsInvoke() && user->GetSideEffects().DoesAnyRead()) {
+      if (!escape_visitor(user)) {
+        return;
+      }
     } else if ((user->IsUnresolvedInstanceFieldGet() && (reference == user->InputAt(0))) ||
                (user->IsUnresolvedInstanceFieldSet() && (reference == user->InputAt(0)))) {
       // The field is accessed in an unresolved way. We mark the object as a non-singleton.
@@ -92,18 +96,21 @@ void CalculateEscape(HInstruction* reference,
                      NoEscapeCheck& no_escape,
                      /*out*/ bool* is_singleton,
                      /*out*/ bool* is_singleton_and_not_returned,
-                     /*out*/ bool* is_singleton_and_not_deopt_visible) {
+                     /*out*/ bool* is_singleton_and_not_deopt_visible,
+                     /*out*/ bool* is_singleton_and_not_read_by_invoke) {
   // For references not allocated in the method, don't assume anything.
   if (!reference->IsNewInstance() && !reference->IsNewArray()) {
     *is_singleton = false;
     *is_singleton_and_not_returned = false;
     *is_singleton_and_not_deopt_visible = false;
+    *is_singleton_and_not_read_by_invoke = false;
     return;
   }
   // Assume the best until proven otherwise.
   *is_singleton = true;
   *is_singleton_and_not_returned = true;
   *is_singleton_and_not_deopt_visible = true;
+  *is_singleton_and_not_read_by_invoke = true;
 
   if (reference->IsNewInstance() && reference->AsNewInstance()->IsFinalizable()) {
     // Finalizable reference is treated as being returned in the end.
@@ -126,12 +133,19 @@ void CalculateEscape(HInstruction* reference,
       // value escapes through deopt but might still be singleton. Continue on.
       *is_singleton_and_not_deopt_visible = false;
       return true;
+    } else if (escape->IsInvoke() &&
+               !escape->GetSideEffects().DoesAnyWrite() &&
+               escape->GetSideEffects().DoesAnyRead()) {
+      // value is read by an invocation. Continue on.
+      *is_singleton_and_not_read_by_invoke = false;
+      return true;
     } else {
       // Real escape. All knowledge about what happens to the value lost. We can
       // stop here.
       *is_singleton = false;
       *is_singleton_and_not_returned = false;
       *is_singleton_and_not_deopt_visible = false;
+      *is_singleton_and_not_read_by_invoke = false;
       return false;
     }
   });
@@ -141,12 +155,14 @@ void CalculateEscape(HInstruction* reference,
 bool DoesNotEscape(HInstruction* reference, NoEscapeCheck& no_escape) {
   bool is_singleton = false;
   bool is_singleton_and_not_returned = false;
-  bool is_singleton_and_not_deopt_visible = false;  // not relevant for escape
+  bool is_singleton_and_not_deopt_visible = false;   // not relevant for escape
+  bool is_singleton_and_not_read_by_invoke = false;  // not relevant for escape
   CalculateEscape(reference,
                   no_escape,
                   &is_singleton,
                   &is_singleton_and_not_returned,
-                  &is_singleton_and_not_deopt_visible);
+                  &is_singleton_and_not_deopt_visible,
+                  &is_singleton_and_not_read_by_invoke);
   return is_singleton_and_not_returned;
 }
 
diff --git a/compiler/optimizing/escape.h b/compiler/optimizing/escape.h
index 3b284fbf43..805ad5d0fd 100644
--- a/compiler/optimizing/escape.h
+++ b/compiler/optimizing/escape.h
@@ -110,20 +110,23 @@ void CalculateEscape(HInstruction* reference,
                      NoEscapeCheck& no_escape,
                      /*out*/ bool* is_singleton,
                      /*out*/ bool* is_singleton_and_not_returned,
-                     /*out*/ bool* is_singleton_and_not_deopt_visible);
+                     /*out*/ bool* is_singleton_and_not_deopt_visible,
+                     /*out*/ bool* is_singleton_and_not_read_by_invoke);
 
 inline void CalculateEscape(HInstruction* reference,
                             bool (*no_escape_fn)(HInstruction*, HInstruction*),
                             /*out*/ bool* is_singleton,
                             /*out*/ bool* is_singleton_and_not_returned,
-                            /*out*/ bool* is_singleton_and_not_deopt_visible) {
+                            /*out*/ bool* is_singleton_and_not_deopt_visible,
+                            /*out*/ bool* is_singleton_and_not_read_by_invoke) {
   LambdaNoEscapeCheck esc(no_escape_fn);
   LambdaNoEscapeCheck noop_esc([](HInstruction*, HInstruction*) { return false; });
   CalculateEscape(reference,
                   no_escape_fn == nullptr ? static_cast<NoEscapeCheck&>(noop_esc) : esc,
                   is_singleton,
                   is_singleton_and_not_returned,
-                  is_singleton_and_not_deopt_visible);
+                  is_singleton_and_not_deopt_visible,
+                  is_singleton_and_not_read_by_invoke);
 }
 
 /*
diff --git a/compiler/optimizing/fast_compiler_arm64.cc b/compiler/optimizing/fast_compiler_arm64.cc
index a820fc9422..87128216aa 100644
--- a/compiler/optimizing/fast_compiler_arm64.cc
+++ b/compiler/optimizing/fast_compiler_arm64.cc
@@ -248,6 +248,14 @@ class FastCompilerARM64 : public FastCompiler {
   void DoReadBarrierOn(Register reg, vixl::aarch64::Label* exit = nullptr, bool do_mr_check = true);
   bool CanGenerateCodeFor(ArtField* field, bool can_receiver_be_null)
       REQUIRES_SHARED(Locks::mutator_lock_);
+  bool DoGet(const MemOperand& mem,
+             uint16_t field_index,
+             Instruction::Code code,
+             uint32_t dest_reg,
+             bool can_receiver_be_null,
+             bool is_object,
+             uint32_t dex_pc,
+             const Instruction* next);
 
   // Mark whether dex register `vreg_index` is an object.
   void UpdateRegisterMask(uint32_t vreg_index, bool is_object) {
@@ -431,6 +439,11 @@ void FastCompilerARM64::MoveConstantsToRegisters() {
           CreateNewRegisterLocation(i, DataType::Type::kInt32, /* next= */ nullptr);
       MoveLocation(vreg_locations_[i], location, DataType::Type::kInt32);
       DCHECK(!HitUnimplemented());
+      if (location.GetConstant()->IsArithmeticZero()) {
+        // In case we branch, we need to make sure a null value can be merged
+        // with an object value, so treat the 0 value as an object.
+        UpdateRegisterMask(i, /* is_object= */ true);
+      }
     }
   }
 }
@@ -637,42 +650,14 @@ bool FastCompilerARM64::EnsureHasFrame() {
     RecordPcInfo(0);
   }
 
-  // Stack layout:
-  //      sp[frame_size - 8]        : lr.
-  //      ...                       : other preserved core registers.
-  //      ...                       : other preserved fp registers.
-  //      ...                       : reserved frame space.
-  //      sp[0]                     : current method.
-  int32_t frame_size = GetFrameSize();
-  uint32_t core_spills_offset = frame_size - GetCoreSpillSize();
-  CPURegList preserved_core_registers = GetFramePreservedCoreRegisters();
-  DCHECK(!preserved_core_registers.IsEmpty());
-  uint32_t fp_spills_offset = frame_size - FrameEntrySpillSize();
-  CPURegList preserved_fp_registers = GetFramePreservedFPRegisters();
-
-  // Save the current method if we need it, or if using STP reduces code
-  // size. Note that we do not do this in HCurrentMethod, as the
-  // instruction might have been removed in the SSA graph.
-  CPURegister lowest_spill;
-  if (core_spills_offset == kXRegSizeInBytes) {
-    // If there is no gap between the method and the lowest core spill, use
-    // aligned STP pre-index to store both. Max difference is 512. We do
-    // that to reduce code size even if we do not have to save the method.
-    DCHECK_LE(frame_size, 512);  // 32 core registers are only 256 bytes.
-    lowest_spill = preserved_core_registers.PopLowestIndex();
-    __ Stp(kArtMethodRegister, lowest_spill, MemOperand(sp, -frame_size, PreIndex));
-  } else {
-    __ Str(kArtMethodRegister, MemOperand(sp, -frame_size, PreIndex));
-  }
-  GetAssembler()->cfi().AdjustCFAOffset(frame_size);
-  if (lowest_spill.IsValid()) {
-    GetAssembler()->cfi().RelOffset(DWARFReg(lowest_spill), core_spills_offset);
-    core_spills_offset += kXRegSizeInBytes;
-  }
-  GetAssembler()->SpillRegisters(preserved_core_registers, core_spills_offset);
-  GetAssembler()->SpillRegisters(preserved_fp_registers, fp_spills_offset);
+  CodeGeneratorARM64::GenerateFrame(GetAssembler(),
+                                    GetFrameSize(),
+                                    GetFramePreservedCoreRegisters(),
+                                    GetFramePreservedFPRegisters(),
+                                    /* requires_current_method= */ true);
 
-  // Move registers which are currently allocated from caller-saves to callee-saves.
+  // Move registers which are currently allocated from caller-saves to callee-saves,
+  // and adjust the offsets of stack locations.
   for (int i = 0; i < number_of_vregs; ++i) {
     if (vreg_locations_[i].IsRegister()) {
       Location new_location =
@@ -688,6 +673,16 @@ bool FastCompilerARM64::EnsureHasFrame() {
         return false;
       }
       vreg_locations_[i] = new_location;
+    } else if (vreg_locations_[i].IsStackSlot()) {
+      vreg_locations_[i] = Location::StackSlot(vreg_locations_[i].GetStackIndex() + GetFrameSize());
+    } else if (vreg_locations_[i].IsDoubleStackSlot()) {
+      vreg_locations_[i] =
+          Location::DoubleStackSlot(vreg_locations_[i].GetStackIndex() + GetFrameSize());
+    } else if (vreg_locations_[i].IsConstant() || vreg_locations_[i].IsInvalid()) {
+      // Nothing to do.
+    } else {
+      unimplemented_reason_ = "Unhandled location";
+      return false;
     }
   }
 
@@ -864,7 +859,14 @@ bool FastCompilerARM64::HandleInvoke(const Instruction& instruction,
     } else if (invoke_type == kVirtual) {
       offset = resolved_method->GetVtableIndex();
     } else if (invoke_type == kInterface) {
-      offset = resolved_method->GetImtIndex();
+      if (resolved_method->GetDeclaringClass()->IsObjectClass()) {
+        // If the resolved method is from j.l.Object, emit a virtual call instead.
+        // The IMT conflict stub only handles interface methods.
+        offset = resolved_method->GetVtableIndex();
+        invoke_type = kVirtual;
+      } else {
+        offset = resolved_method->GetImtIndex();
+      }
     }
 
     if (resolved_method->IsStringConstructor()) {
@@ -1241,6 +1243,94 @@ bool FastCompilerARM64::If_21_22t(const Instruction& instruction, uint32_t dex_p
 }
 #undef DO_CASE
 
+bool FastCompilerARM64::DoGet(const MemOperand& mem,
+                              uint16_t field_index,
+                              Instruction::Code opcode,
+                              uint32_t dest_reg,
+                              bool can_receiver_be_null,
+                              bool is_object,
+                              uint32_t dex_pc,
+                              const Instruction* next) {
+  if (is_object) {
+    Register dst = WRegisterFrom(
+        CreateNewRegisterLocation(dest_reg, DataType::Type::kReference, next));
+    if (HitUnimplemented()) {
+      return false;
+    }
+    {
+      // Ensure the pc position is recorded immediately after the load instruction.
+      EmissionCheckScope guard(GetVIXLAssembler(), kMaxMacroInstructionSizeInBytes);
+      __ Ldr(dst, mem);
+      if (can_receiver_be_null) {
+        RecordPcInfo(dex_pc);
+      }
+    }
+    UpdateLocal(dest_reg, /* is_object= */ true);
+    DoReadBarrierOn(dst);
+    return true;
+  }
+
+  // Ensure the pc position is recorded immediately after the load instruction.
+  EmissionCheckScope guard(GetVIXLAssembler(), kMaxMacroInstructionSizeInBytes);
+  switch (opcode) {
+    case Instruction::SGET_BOOLEAN:
+    case Instruction::IGET_BOOLEAN: {
+      Register dst = WRegisterFrom(
+          CreateNewRegisterLocation(dest_reg, DataType::Type::kInt32, next));
+      __ Ldrb(Register(dst), mem);
+      break;
+    }
+    case Instruction::SGET_BYTE:
+    case Instruction::IGET_BYTE: {
+      Register dst = WRegisterFrom(
+          CreateNewRegisterLocation(dest_reg, DataType::Type::kInt32, next));
+      __ Ldrsb(Register(dst), mem);
+      break;
+    }
+    case Instruction::SGET_CHAR:
+    case Instruction::IGET_CHAR: {
+      Register dst = WRegisterFrom(
+          CreateNewRegisterLocation(dest_reg, DataType::Type::kInt32, next));
+      __ Ldrh(Register(dst), mem);
+      break;
+    }
+    case Instruction::SGET_SHORT:
+    case Instruction::IGET_SHORT: {
+      Register dst = WRegisterFrom(
+          CreateNewRegisterLocation(dest_reg, DataType::Type::kInt32, next));
+      __ Ldrsh(Register(dst), mem);
+      break;
+    }
+    case Instruction::SGET:
+    case Instruction::IGET: {
+      const dex::FieldId& field_id = GetDexFile().GetFieldId(field_index);
+      const char* type = GetDexFile().GetFieldTypeDescriptor(field_id);
+      DataType::Type field_type = DataType::FromShorty(type[0]);
+      if (DataType::IsFloatingPointType(field_type)) {
+        VRegister dst = SRegisterFrom(
+            CreateNewRegisterLocation(dest_reg, field_type, next));
+        __ Ldr(dst, mem);
+      } else {
+        Register dst = WRegisterFrom(
+            CreateNewRegisterLocation(dest_reg, DataType::Type::kInt32, next));
+        __ Ldr(dst, mem);
+      }
+      if (HitUnimplemented()) {
+        return false;
+      }
+      break;
+    }
+    default:
+      unimplemented_reason_ = "UnimplementedGet";
+      return false;
+  }
+  UpdateLocal(dest_reg, is_object);
+  if (can_receiver_be_null) {
+    RecordPcInfo(dex_pc);
+  }
+  return true;
+}
+
 bool FastCompilerARM64::ProcessDexInstruction(const Instruction& instruction,
                                               uint32_t dex_pc,
                                               const Instruction* next) {
@@ -1906,7 +1996,6 @@ bool FastCompilerARM64::ProcessDexInstruction(const Instruction& instruction,
           return false;
         }
       }
-
       MemOperand mem = HeapOperand(
           RegisterFrom(GetExistingRegisterLocation(obj_reg, DataType::Type::kReference),
                        DataType::Type::kReference),
@@ -1914,76 +2003,15 @@ bool FastCompilerARM64::ProcessDexInstruction(const Instruction& instruction,
       if (HitUnimplemented()) {
         return false;
       }
-      if (is_object) {
-        Register dst = WRegisterFrom(
-            CreateNewRegisterLocation(source_or_dest_reg, DataType::Type::kReference, next));
-        if (HitUnimplemented()) {
-          return false;
-        }
-        {
-          // Ensure the pc position is recorded immediately after the load instruction.
-          EmissionCheckScope guard(GetVIXLAssembler(), kMaxMacroInstructionSizeInBytes);
-          __ Ldr(dst, mem);
-          if (can_receiver_be_null) {
-            RecordPcInfo(dex_pc);
-          }
-        }
-        UpdateLocal(source_or_dest_reg, /* is_object= */ true);
-        DoReadBarrierOn(dst);
-        return true;
-      }
-      // Ensure the pc position is recorded immediately after the load instruction.
-      EmissionCheckScope guard(GetVIXLAssembler(), kMaxMacroInstructionSizeInBytes);
-      switch (instruction.Opcode()) {
-        case Instruction::IGET_BOOLEAN: {
-          Register dst = WRegisterFrom(
-              CreateNewRegisterLocation(source_or_dest_reg, DataType::Type::kInt32, next));
-          __ Ldrb(Register(dst), mem);
-          break;
-        }
-        case Instruction::IGET_BYTE: {
-          Register dst = WRegisterFrom(
-              CreateNewRegisterLocation(source_or_dest_reg, DataType::Type::kInt32, next));
-          __ Ldrsb(Register(dst), mem);
-          break;
-        }
-        case Instruction::IGET_CHAR: {
-          Register dst = WRegisterFrom(
-              CreateNewRegisterLocation(source_or_dest_reg, DataType::Type::kInt32, next));
-          __ Ldrh(Register(dst), mem);
-          break;
-        }
-        case Instruction::IGET_SHORT: {
-          Register dst = WRegisterFrom(
-              CreateNewRegisterLocation(source_or_dest_reg, DataType::Type::kInt32, next));
-          __ Ldrsh(Register(dst), mem);
-          break;
-        }
-        case Instruction::IGET: {
-          const dex::FieldId& field_id = GetDexFile().GetFieldId(field_index);
-          const char* type = GetDexFile().GetFieldTypeDescriptor(field_id);
-          DataType::Type field_type = DataType::FromShorty(type[0]);
-          if (DataType::IsFloatingPointType(field_type)) {
-            VRegister dst = SRegisterFrom(
-                CreateNewRegisterLocation(source_or_dest_reg, field_type, next));
-            __ Ldr(dst, mem);
-          } else {
-            Register dst = WRegisterFrom(
-                CreateNewRegisterLocation(source_or_dest_reg, DataType::Type::kInt32, next));
-            __ Ldr(dst, mem);
-          }
-          if (HitUnimplemented()) {
-            return false;
-          }
-          break;
-        }
-        default:
-          unimplemented_reason_ = "UnimplementedIGet";
-          return false;
-      }
-      UpdateLocal(source_or_dest_reg, /* is_object= */ false);
-      if (can_receiver_be_null) {
-        RecordPcInfo(dex_pc);
+      if (!DoGet(mem,
+                 field_index,
+                 instruction.Opcode(),
+                 source_or_dest_reg,
+                 can_receiver_be_null,
+                 is_object,
+                 dex_pc,
+                 next)) {
+        return false;
       }
       return true;
     }
@@ -2105,14 +2133,64 @@ bool FastCompilerARM64::ProcessDexInstruction(const Instruction& instruction,
       return true;
     }
 
+    case Instruction::SGET_OBJECT:
+      is_object = true;
+      FALLTHROUGH_INTENDED;
     case Instruction::SGET:
     case Instruction::SGET_WIDE:
-    case Instruction::SGET_OBJECT:
     case Instruction::SGET_BOOLEAN:
     case Instruction::SGET_BYTE:
     case Instruction::SGET_CHAR:
     case Instruction::SGET_SHORT: {
-      break;
+      if (Runtime::Current()->IsAotCompiler()) {
+        unimplemented_reason_ = "AOTSGet";
+        return false;
+      }
+      // We need a frame for the read barrier.
+      if (!EnsureHasFrame()) {
+        return false;
+      }
+      ArtField* field = nullptr;
+      uint16_t field_index = instruction.VRegB_21c();
+      uint32_t source_or_dest_reg = instruction.VRegA_21c();
+      UseScratchRegisterScope temps(GetVIXLAssembler());
+      Register temp = temps.AcquireX();
+      {
+        ScopedObjectAccess soa(Thread::Current());
+        field = ResolveFieldWithAccessChecks(soa.Self(),
+                                             dex_compilation_unit_.GetClassLinker(),
+                                             field_index,
+                                             method_,
+                                             /* is_static= */ true,
+                                             /* is_put= */ false,
+                                             /* resolve_field_type= */ 0u);
+        if (!CanGenerateCodeFor(field, /* can_receiver_be_null= */ false)) {
+          return false;
+        }
+        Handle<mirror::Class> h_klass = handles_->NewHandle(field->GetDeclaringClass());
+        if (!h_klass->IsVisiblyInitialized()) {
+          unimplemented_reason_ = "UninitializedSget";
+          return false;
+        }
+        __ Ldr(temp.W(), jit_patches_.DeduplicateJitClassLiteral(h_klass->GetDexFile(),
+                                                                 h_klass->GetDexTypeIndex(),
+                                                                 h_klass,
+                                                                 code_generation_data_.get()));
+      }
+      __ Ldr(temp.W(), MemOperand(temp.X()));
+      DoReadBarrierOn(temp);
+      MemOperand mem = HeapOperand(temp.W(), field->GetOffset());
+      if (!DoGet(mem,
+                 field_index,
+                 instruction.Opcode(),
+                 source_or_dest_reg,
+                 /* can_receiver_be_null= */ false,
+                 is_object,
+                 dex_pc,
+                 next)) {
+        return false;
+      }
+      return true;
     }
 
     case Instruction::SPUT:
diff --git a/compiler/optimizing/find_loops_test.cc b/compiler/optimizing/find_loops_test.cc
index 8857b2a775..afef0126ff 100644
--- a/compiler/optimizing/find_loops_test.cc
+++ b/compiler/optimizing/find_loops_test.cc
@@ -111,10 +111,10 @@ static void TestBlock(HGraph* graph,
 
   if (blocks_in_loop != nullptr) {
     HLoopInformation* info = block->GetLoopInformation();
-    const BitVector& blocks = info->GetBlocks();
-    ASSERT_EQ(blocks.NumSetBits(), number_of_blocks);
+    const BitVector& block_mask = info->GetBlockMask();
+    ASSERT_EQ(block_mask.NumSetBits(), number_of_blocks);
     for (size_t i = 0; i < number_of_blocks; ++i) {
-      ASSERT_TRUE(blocks.IsBitSet(blocks_in_loop[i]));
+      ASSERT_TRUE(block_mask.IsBitSet(blocks_in_loop[i]));
     }
   } else {
     ASSERT_FALSE(block->IsLoopHeader());
diff --git a/compiler/optimizing/graph_checker.cc b/compiler/optimizing/graph_checker.cc
index 82f98ed5ea..2bd14969f6 100644
--- a/compiler/optimizing/graph_checker.cc
+++ b/compiler/optimizing/graph_checker.cc
@@ -28,6 +28,7 @@
 #include "code_generator.h"
 #include "handle.h"
 #include "intrinsics.h"
+#include "loop_information-inl.h"
 #include "mirror/class.h"
 #include "nodes.h"
 #include "obj_ptr-inl.h"
@@ -279,7 +280,7 @@ void GraphChecker::VisitBasicBlock(HBasicBlock* block) {
   }
 
   // Visit this block's list of phis.
-  for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
     HInstruction* current = it.Current();
     // Ensure this block's list of phis contains only phis.
     if (!current->IsPhi()) {
@@ -292,11 +293,11 @@ void GraphChecker::VisitBasicBlock(HBasicBlock* block) {
                             current_block_->GetBlockId(),
                             current->GetId()));
     }
-    current->Accept(this);
+    Dispatch(current);
   }
 
   // Visit this block's list of instructions.
-  for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
     HInstruction* current = it.Current();
     // Ensure this block's list of instructions does not contains phis.
     if (current->IsPhi()) {
@@ -310,7 +311,7 @@ void GraphChecker::VisitBasicBlock(HBasicBlock* block) {
                        current_block_->GetBlockId(),
                        current->GetId()));
     }
-    current->Accept(this);
+    Dispatch(current);
   }
 
   // Ensure that catch blocks are not normal successors, and normal blocks are
@@ -539,8 +540,9 @@ bool GraphChecker::ContainedInItsBlockList(HInstruction* instruction) {
     const HInstructionList& instruction_list = instruction->IsPhi() ?
                                                    instruction->GetBlock()->GetPhis() :
                                                    instruction->GetBlock()->GetInstructions();
-    for (HInstructionIterator list_it(instruction_list); !list_it.Done(); list_it.Advance()) {
-        map_it->second.insert(list_it.Current());
+    for (HInstructionIteratorPrefetchNext list_it(instruction_list); !list_it.Done();
+         list_it.Advance()) {
+      map_it->second.insert(list_it.Current());
     }
   }
   return map_it->second.find(instruction) != map_it->second.end();
@@ -718,7 +720,8 @@ void GraphChecker::VisitInstruction(HInstruction* instruction) {
     const HTryBoundary& entry = instruction->GetBlock()->GetTryCatchInformation()->GetTryEntry();
     for (HBasicBlock* catch_block : entry.GetExceptionHandlers()) {
       const HEnvironment* environment = catch_block->GetFirstInstruction()->GetEnvironment();
-      for (HInstructionIterator phi_it(catch_block->GetPhis()); !phi_it.Done(); phi_it.Advance()) {
+      for (HInstructionIteratorPrefetchNext phi_it(catch_block->GetPhis()); !phi_it.Done();
+           phi_it.Advance()) {
         HPhi* catch_phi = phi_it.Current()->AsPhi();
         if (environment->GetInstructionAt(catch_phi->GetRegNumber()) == nullptr) {
           AddError(
@@ -951,7 +954,7 @@ void GraphChecker::HandleLoop(HBasicBlock* loop_header) {
     }
   }
 
-  const ArenaBitVector& loop_blocks = loop_information->GetBlocks();
+  const ArenaBitVector& loop_blocks = loop_information->GetBlockMask();
 
   // Ensure back edges belong to the loop.
   if (loop_information->NumberOfBackEdges() == 0) {
@@ -980,7 +983,7 @@ void GraphChecker::HandleLoop(HBasicBlock* loop_header) {
   // If this is a nested loop, ensure the outer loops contain a superset of the blocks.
   for (HLoopInformationOutwardIterator it(*loop_header); !it.Done(); it.Advance()) {
     HLoopInformation* outer_info = it.Current();
-    if (!loop_blocks.IsSubsetOf(&outer_info->GetBlocks())) {
+    if (!loop_blocks.IsSubsetOf(&outer_info->GetBlockMask())) {
       AddError(StringPrintf("Blocks of loop defined by header %d are not a subset of blocks of "
                             "an outer loop defined by header %d.",
                             id,
@@ -1148,7 +1151,7 @@ void GraphChecker::VisitPhi(HPhi* phi) {
   // created for constants which were untyped in DEX. Note that this test can be skipped for
   // a synthetic phi (indicated by lack of a virtual register).
   if (phi->GetRegNumber() != kNoRegNumber) {
-    for (HInstructionIterator phi_it(phi->GetBlock()->GetPhis());
+    for (HInstructionIteratorPrefetchNext phi_it(phi->GetBlock()->GetPhis());
          !phi_it.Done();
          phi_it.Advance()) {
       HPhi* other_phi = phi_it.Current()->AsPhi();
@@ -1365,7 +1368,7 @@ void GraphChecker::CheckWriteBarrier(HInstruction* instruction,
   // B) There's no instruction between them that can trigger a GC.
   HInstruction* object = HuntForOriginalReference(instruction->InputAt(0));
   bool found = false;
-  for (HBackwardInstructionIterator it(instruction); !it.Done(); it.Advance()) {
+  for (HBackwardInstructionIteratorPrefetchNext it(instruction); !it.Done(); it.Advance()) {
     if (instruction->GetKind() == it.Current()->GetKind() &&
         object == HuntForOriginalReference(it.Current()->InputAt(0)) &&
         get_write_barrier_kind(it.Current()) == WriteBarrierKind::kEmitBeingReliedOn) {
diff --git a/compiler/optimizing/graph_checker_test.cc b/compiler/optimizing/graph_checker_test.cc
index cea34b9e2c..4fcd584a3c 100644
--- a/compiler/optimizing/graph_checker_test.cc
+++ b/compiler/optimizing/graph_checker_test.cc
@@ -36,14 +36,11 @@ class GraphCheckerTest : public CommonCompilerTest, public OptimizingUnitTestHel
  */
 HGraph* GraphCheckerTest::CreateSimpleCFG() {
   HGraph* graph = CreateGraph();
-  HBasicBlock* entry_block = new (GetAllocator()) HBasicBlock(graph);
+  HBasicBlock* entry_block = AddNewBlock();
   MakeReturnVoid(entry_block);
-  graph->AddBlock(entry_block);
   graph->SetEntryBlock(entry_block);
-  HBasicBlock* exit_block = new (GetAllocator()) HBasicBlock(graph);
+  HBasicBlock* exit_block = AddExitBlock();
   MakeExit(exit_block);
-  graph->AddBlock(exit_block);
-  graph->SetExitBlock(exit_block);
   entry_block->AddSuccessor(exit_block);
   graph->BuildDominatorTree();
   return graph;
diff --git a/compiler/optimizing/graph_test.cc b/compiler/optimizing/graph_test.cc
index 5b43af3564..0210e02081 100644
--- a/compiler/optimizing/graph_test.cc
+++ b/compiler/optimizing/graph_test.cc
@@ -27,60 +27,47 @@ namespace art HIDDEN {
 
 class GraphTest : public OptimizingUnitTest {
  protected:
-  HBasicBlock* CreateIfBlock(HGraph* graph);
-  HBasicBlock* CreateGotoBlock(HGraph* graph);
-  HBasicBlock* CreateEntryBlock(HGraph* graph);
-  HBasicBlock* CreateReturnBlock(HGraph* graph);
-  HBasicBlock* CreateExitBlock(HGraph* graph);
+  HBasicBlock* CreateIfBlock();
+  HBasicBlock* CreateGotoBlock();
+  HBasicBlock* CreateEntryBlock();
+  HBasicBlock* CreateReturnBlock();
 };
 
-HBasicBlock* GraphTest::CreateIfBlock(HGraph* graph) {
-  HBasicBlock* if_block = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(if_block);
-  HInstruction* instr = graph->GetIntConstant(4);
+HBasicBlock* GraphTest::CreateIfBlock() {
+  HBasicBlock* if_block = AddNewBlock();
+  HInstruction* instr = graph_->GetIntConstant(4);
   HInstruction* equal = MakeCondition(if_block, kCondEQ, instr, instr);
   MakeIf(if_block, equal);
   return if_block;
 }
 
-HBasicBlock* GraphTest::CreateGotoBlock(HGraph* graph) {
-  HBasicBlock* block = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(block);
+HBasicBlock* GraphTest::CreateGotoBlock() {
+  HBasicBlock* block = AddNewBlock();
   MakeGoto(block);
   return block;
 }
 
-HBasicBlock* GraphTest::CreateEntryBlock(HGraph* graph) {
-  HBasicBlock* block = CreateGotoBlock(graph);
-  graph->SetEntryBlock(block);
+HBasicBlock* GraphTest::CreateEntryBlock() {
+  HBasicBlock* block = CreateGotoBlock();
+  graph_->SetEntryBlock(block);
   return block;
 }
 
-HBasicBlock* GraphTest::CreateReturnBlock(HGraph* graph) {
-  HBasicBlock* block = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(block);
-  HInstruction* return_instr = new (GetAllocator()) HReturnVoid();
-  block->AddInstruction(return_instr);
+HBasicBlock* GraphTest::CreateReturnBlock() {
+  HBasicBlock* block = AddNewBlock();
+  MakeReturnVoid(block);
   return block;
 }
 
-HBasicBlock* GraphTest::CreateExitBlock(HGraph* graph) {
-  HBasicBlock* block = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(block);
-  MakeExit(block);
-  return block;
-}
-
-
 // Test that the successors of an if block stay consistent after a SimplifyCFG.
 // This test sets the false block to be the return block.
 TEST_F(GraphTest, IfSuccessorSimpleJoinBlock1) {
   HGraph* graph = CreateGraph();
-  HBasicBlock* entry_block = CreateEntryBlock(graph);
-  HBasicBlock* if_block = CreateIfBlock(graph);
-  HBasicBlock* if_true = CreateGotoBlock(graph);
-  HBasicBlock* return_block = CreateReturnBlock(graph);
-  HBasicBlock* exit_block = CreateExitBlock(graph);
+  HBasicBlock* entry_block = CreateEntryBlock();
+  HBasicBlock* if_block = CreateIfBlock();
+  HBasicBlock* if_true = CreateGotoBlock();
+  HBasicBlock* return_block = CreateReturnBlock();
+  HBasicBlock* exit_block = AddExitBlock();
 
   entry_block->AddSuccessor(if_block);
   if_block->AddSuccessor(if_true);
@@ -108,11 +95,11 @@ TEST_F(GraphTest, IfSuccessorSimpleJoinBlock1) {
 // This test sets the true block to be the return block.
 TEST_F(GraphTest, IfSuccessorSimpleJoinBlock2) {
   HGraph* graph = CreateGraph();
-  HBasicBlock* entry_block = CreateEntryBlock(graph);
-  HBasicBlock* if_block = CreateIfBlock(graph);
-  HBasicBlock* if_false = CreateGotoBlock(graph);
-  HBasicBlock* return_block = CreateReturnBlock(graph);
-  HBasicBlock* exit_block = CreateExitBlock(graph);
+  HBasicBlock* entry_block = CreateEntryBlock();
+  HBasicBlock* if_block = CreateIfBlock();
+  HBasicBlock* if_false = CreateGotoBlock();
+  HBasicBlock* return_block = CreateReturnBlock();
+  HBasicBlock* exit_block = AddExitBlock();
 
   entry_block->AddSuccessor(if_block);
   if_block->AddSuccessor(return_block);
@@ -140,10 +127,10 @@ TEST_F(GraphTest, IfSuccessorSimpleJoinBlock2) {
 // This test sets the true block to be the loop header.
 TEST_F(GraphTest, IfSuccessorMultipleBackEdges1) {
   HGraph* graph = CreateGraph();
-  HBasicBlock* entry_block = CreateEntryBlock(graph);
-  HBasicBlock* if_block = CreateIfBlock(graph);
-  HBasicBlock* return_block = CreateReturnBlock(graph);
-  HBasicBlock* exit_block = CreateExitBlock(graph);
+  HBasicBlock* entry_block = CreateEntryBlock();
+  HBasicBlock* if_block = CreateIfBlock();
+  HBasicBlock* return_block = CreateReturnBlock();
+  HBasicBlock* exit_block = AddExitBlock();
 
   entry_block->AddSuccessor(if_block);
   if_block->AddSuccessor(if_block);
@@ -172,10 +159,10 @@ TEST_F(GraphTest, IfSuccessorMultipleBackEdges1) {
 // This test sets the false block to be the loop header.
 TEST_F(GraphTest, IfSuccessorMultipleBackEdges2) {
   HGraph* graph = CreateGraph();
-  HBasicBlock* entry_block = CreateEntryBlock(graph);
-  HBasicBlock* if_block = CreateIfBlock(graph);
-  HBasicBlock* return_block = CreateReturnBlock(graph);
-  HBasicBlock* exit_block = CreateExitBlock(graph);
+  HBasicBlock* entry_block = CreateEntryBlock();
+  HBasicBlock* if_block = CreateIfBlock();
+  HBasicBlock* return_block = CreateReturnBlock();
+  HBasicBlock* exit_block = AddExitBlock();
 
   entry_block->AddSuccessor(if_block);
   if_block->AddSuccessor(return_block);
@@ -204,11 +191,12 @@ TEST_F(GraphTest, IfSuccessorMultipleBackEdges2) {
 // This test sets the true block to be a loop header with multiple pre headers.
 TEST_F(GraphTest, IfSuccessorMultiplePreHeaders1) {
   HGraph* graph = CreateGraph();
-  HBasicBlock* entry_block = CreateEntryBlock(graph);
-  HBasicBlock* first_if_block = CreateIfBlock(graph);
-  HBasicBlock* if_block = CreateIfBlock(graph);
-  HBasicBlock* loop_block = CreateGotoBlock(graph);
-  HBasicBlock* return_block = CreateReturnBlock(graph);
+  HBasicBlock* entry_block = CreateEntryBlock();
+  HBasicBlock* first_if_block = CreateIfBlock();
+  HBasicBlock* if_block = CreateIfBlock();
+  HBasicBlock* loop_block = CreateGotoBlock();
+  HBasicBlock* return_block = CreateReturnBlock();
+  HBasicBlock* exit_block = AddExitBlock();
 
   entry_block->AddSuccessor(first_if_block);
   first_if_block->AddSuccessor(if_block);
@@ -216,7 +204,7 @@ TEST_F(GraphTest, IfSuccessorMultiplePreHeaders1) {
   loop_block->AddSuccessor(loop_block);
   if_block->AddSuccessor(loop_block);
   if_block->AddSuccessor(return_block);
-
+  return_block->AddSuccessor(exit_block);
 
   ASSERT_EQ(if_block->GetLastInstruction()->AsIf()->IfTrueSuccessor(), loop_block);
   ASSERT_EQ(if_block->GetLastInstruction()->AsIf()->IfFalseSuccessor(), return_block);
@@ -240,11 +228,12 @@ TEST_F(GraphTest, IfSuccessorMultiplePreHeaders1) {
 // This test sets the false block to be a loop header with multiple pre headers.
 TEST_F(GraphTest, IfSuccessorMultiplePreHeaders2) {
   HGraph* graph = CreateGraph();
-  HBasicBlock* entry_block = CreateEntryBlock(graph);
-  HBasicBlock* first_if_block = CreateIfBlock(graph);
-  HBasicBlock* if_block = CreateIfBlock(graph);
-  HBasicBlock* loop_block = CreateGotoBlock(graph);
-  HBasicBlock* return_block = CreateReturnBlock(graph);
+  HBasicBlock* entry_block = CreateEntryBlock();
+  HBasicBlock* first_if_block = CreateIfBlock();
+  HBasicBlock* if_block = CreateIfBlock();
+  HBasicBlock* loop_block = CreateGotoBlock();
+  HBasicBlock* return_block = CreateReturnBlock();
+  HBasicBlock* exit_block = AddExitBlock();
 
   entry_block->AddSuccessor(first_if_block);
   first_if_block->AddSuccessor(if_block);
@@ -252,6 +241,7 @@ TEST_F(GraphTest, IfSuccessorMultiplePreHeaders2) {
   loop_block->AddSuccessor(loop_block);
   if_block->AddSuccessor(return_block);
   if_block->AddSuccessor(loop_block);
+  return_block->AddSuccessor(exit_block);
 
   ASSERT_EQ(if_block->GetLastInstruction()->AsIf()->IfTrueSuccessor(), return_block);
   ASSERT_EQ(if_block->GetLastInstruction()->AsIf()->IfFalseSuccessor(), loop_block);
@@ -273,7 +263,7 @@ TEST_F(GraphTest, IfSuccessorMultiplePreHeaders2) {
 
 TEST_F(GraphTest, InsertInstructionBefore) {
   HGraph* graph = CreateGraph();
-  HBasicBlock* block = CreateGotoBlock(graph);
+  HBasicBlock* block = CreateGotoBlock();
   HInstruction* got = block->GetLastInstruction();
   ASSERT_TRUE(got->IsControlFlow());
 
diff --git a/compiler/optimizing/graph_visualizer.cc b/compiler/optimizing/graph_visualizer.cc
index a8d487e51a..3984ea6115 100644
--- a/compiler/optimizing/graph_visualizer.cc
+++ b/compiler/optimizing/graph_visualizer.cc
@@ -702,7 +702,7 @@ class HGraphVisualizerPrinter final : public HGraphDelegateVisitor {
     HBasicBlock* block = instruction->GetBlock();
     StartAttributeStream("block") << namer_.GetName(block);
 
-    instruction->Accept(this);
+    Dispatch(instruction);
     if (instruction->HasEnvironment()) {
       StringList envs;
       for (HEnvironment* environment = instruction->GetEnvironment();
@@ -813,7 +813,7 @@ class HGraphVisualizerPrinter final : public HGraphDelegateVisitor {
   }
 
   void PrintInstructions(const HInstructionList& list) {
-    for (HInstructionIterator it(list); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(list); !it.Done(); it.Advance()) {
       HInstruction* instruction = it.Current();
       int bci = 0;
       size_t num_uses = instruction->GetUses().SizeSlow();
@@ -955,7 +955,7 @@ class HGraphVisualizerPrinter final : public HGraphDelegateVisitor {
       StartTag("locals");
       PrintInt("size", 0);
       PrintProperty("method", "None");
-      for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+      for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
         AddIndent();
         HInstruction* instruction = it.Current();
         output_ << instruction->GetId() << " " << DataType::TypeId(instruction->GetType())
diff --git a/compiler/optimizing/gvn.cc b/compiler/optimizing/gvn.cc
index 188bfa473d..248c3cbcec 100644
--- a/compiler/optimizing/gvn.cc
+++ b/compiler/optimizing/gvn.cc
@@ -357,10 +357,10 @@ class ValueSet : public ArenaObject<kArenaAllocGvn> {
  */
 class GlobalValueNumberer : public ValueObject {
  public:
-  GlobalValueNumberer(HGraph* graph, const SideEffectsAnalysis& side_effects)
+  explicit GlobalValueNumberer(HGraph* graph)
       : graph_(graph),
         allocator_(graph->GetArenaStack()),
-        side_effects_(side_effects),
+        side_effects_(nullptr),
         sets_(graph->GetBlocks().size(), nullptr, allocator_.Adapter(kArenaAllocGvn)),
         dominated_to_visit_(graph->GetBlocks().size(), allocator_.Adapter(kArenaAllocGvn)),
         successors_to_visit_(graph->GetBlocks().size(), allocator_.Adapter(kArenaAllocGvn)),
@@ -384,7 +384,7 @@ class GlobalValueNumberer : public ValueObject {
 
   HGraph* graph_;
   ScopedArenaAllocator allocator_;
-  const SideEffectsAnalysis& side_effects_;
+  SideEffectsAnalysis* side_effects_;
 
   ValueSet* FindSetFor(HBasicBlock* block) const {
     ValueSet* result = sets_[block->GetBlockId()];
@@ -432,7 +432,13 @@ class GlobalValueNumberer : public ValueObject {
 };
 
 bool GlobalValueNumberer::Run() {
-  DCHECK(side_effects_.HasRun());
+  if (graph_->HasLoops()) {
+    // SideEffectsAnalysis is only used when the graph has loops.
+    side_effects_ = new (&allocator_) SideEffectsAnalysis(graph_);
+    side_effects_->Run();
+    DCHECK(side_effects_->HasRun());
+  }
+
   sets_[graph_->GetEntryBlock()->GetBlockId()] = new (&allocator_) ValueSet(&allocator_);
 
   // Use the reverse post order to ensure the non back-edge predecessors of a block are
@@ -492,7 +498,7 @@ void GlobalValueNumberer::VisitBasicBlock(HBasicBlock* block) {
         } else {
           DCHECK(!block->GetLoopInformation()->IsIrreducible());
           DCHECK_EQ(block->GetDominator(), block->GetLoopInformation()->GetPreHeader());
-          set->Kill(side_effects_.GetLoopEffects(block));
+          set->Kill(side_effects_->GetLoopEffects(block));
         }
       } else if (predecessors.size() > 1) {
         for (HBasicBlock* predecessor : predecessors) {
@@ -602,7 +608,7 @@ HBasicBlock* GlobalValueNumberer::FindVisitedBlockWithRecyclableSet(
 }
 
 bool GVNOptimization::Run() {
-  GlobalValueNumberer gvn(graph_, side_effects_);
+  GlobalValueNumberer gvn(graph_);
   return gvn.Run();
 }
 
diff --git a/compiler/optimizing/gvn.h b/compiler/optimizing/gvn.h
index df4e3a8dbf..b22797b670 100644
--- a/compiler/optimizing/gvn.h
+++ b/compiler/optimizing/gvn.h
@@ -18,27 +18,20 @@
 #define ART_COMPILER_OPTIMIZING_GVN_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
 
 namespace art HIDDEN {
 
-class SideEffectsAnalysis;
-
 class GVNOptimization : public HOptimization {
  public:
-  GVNOptimization(HGraph* graph,
-                  const SideEffectsAnalysis& side_effects,
-                  const char* pass_name = kGlobalValueNumberingPassName)
-      : HOptimization(graph, pass_name), side_effects_(side_effects) {}
+  explicit GVNOptimization(HGraph* graph, const char* pass_name = kGlobalValueNumberingPassName)
+      : HOptimization(graph, pass_name) {}
 
   bool Run() override;
 
   static constexpr const char* kGlobalValueNumberingPassName = "GVN";
 
  private:
-  const SideEffectsAnalysis& side_effects_;
-
   DISALLOW_COPY_AND_ASSIGN(GVNOptimization);
 };
 
diff --git a/compiler/optimizing/gvn_test.cc b/compiler/optimizing/gvn_test.cc
index fba53ee157..5ad5154b96 100644
--- a/compiler/optimizing/gvn_test.cc
+++ b/compiler/optimizing/gvn_test.cc
@@ -47,9 +47,7 @@ TEST_F(GVNTest, LocalFieldElimination) {
   ASSERT_EQ(use_after_kill->GetBlock(), block);
 
   graph_->BuildDominatorTree();
-  SideEffectsAnalysis side_effects(graph_);
-  side_effects.Run();
-  GVNOptimization(graph_, side_effects).Run();
+  GVNOptimization(graph_).Run();
 
   ASSERT_TRUE(to_remove->GetBlock() == nullptr);
   ASSERT_EQ(different_offset->GetBlock(), block);
@@ -71,9 +69,7 @@ TEST_F(GVNTest, GlobalFieldElimination) {
   MakeIFieldGet(join, parameter, DataType::Type::kBool, MemberOffset(42));
 
   graph_->BuildDominatorTree();
-  SideEffectsAnalysis side_effects(graph_);
-  side_effects.Run();
-  GVNOptimization(graph_, side_effects).Run();
+  GVNOptimization(graph_).Run();
 
   // Check that all field get instructions have been GVN'ed.
   ASSERT_TRUE(then->GetFirstInstruction()->IsGoto());
@@ -109,11 +105,7 @@ TEST_F(GVNTest, LoopFieldElimination) {
   ASSERT_EQ(field_get_in_return_block->GetBlock(), return_block);
 
   graph_->BuildDominatorTree();
-  {
-    SideEffectsAnalysis side_effects(graph_);
-    side_effects.Run();
-    GVNOptimization(graph_, side_effects).Run();
-  }
+  GVNOptimization(graph_).Run();
 
   // Check that all field get instructions are still there.
   ASSERT_EQ(field_get_in_loop_header->GetBlock(), loop_header);
@@ -124,11 +116,7 @@ TEST_F(GVNTest, LoopFieldElimination) {
 
   // Now remove the field set, and check that all field get instructions have been GVN'ed.
   loop_body->RemoveInstruction(field_set);
-  {
-    SideEffectsAnalysis side_effects(graph_);
-    side_effects.Run();
-    GVNOptimization(graph_, side_effects).Run();
-  }
+  GVNOptimization(graph_).Run();
 
   ASSERT_TRUE(field_get_in_loop_header->GetBlock() == nullptr);
   ASSERT_TRUE(field_get_in_loop_body->GetBlock() == nullptr);
diff --git a/compiler/optimizing/induction_var_analysis.cc b/compiler/optimizing/induction_var_analysis.cc
index be6c268f5d..6ca42b516b 100644
--- a/compiler/optimizing/induction_var_analysis.cc
+++ b/compiler/optimizing/induction_var_analysis.cc
@@ -18,6 +18,7 @@
 
 #include "base/scoped_arena_containers.h"
 #include "induction_var_range.h"
+#include "loop_information-inl.h"
 
 namespace art HIDDEN {
 
@@ -170,7 +171,8 @@ static bool RewriteBreakLoopBody(const HLoopInformation* loop,
                                  HInstruction* upper,
                                  bool rewrite) {
   // Deal with Phis. Outside use prohibited, except for index (which gets exit value).
-  for (HInstructionIterator it(loop->GetHeader()->GetPhis()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(loop->GetHeader()->GetPhis()); !it.Done();
+       it.Advance()) {
     HInstruction* exit_value = it.Current() == index ? upper : nullptr;
     if (!FixOutsideUse(loop, it.Current(), exit_value, rewrite)) {
       return false;
@@ -251,17 +253,17 @@ void HInductionVarAnalysis::VisitLoop(const HLoopInformation* loop) {
   // Find strongly connected components (SSCs) in the SSA graph of this loop using Tarjan's
   // algorithm. Due to the descendant-first nature, classification happens "on-demand".
   size_t global_depth = 0;
-  for (HBlocksInLoopIterator it_loop(*loop); !it_loop.Done(); it_loop.Advance()) {
-    HBasicBlock* loop_block = it_loop.Current();
+  for (HBasicBlock* loop_block : loop->GetBlocks()) {
     DCHECK(loop_block->IsInLoop());
     if (loop_block->GetLoopInformation() != loop) {
       continue;  // Inner loops visited later.
     }
     // Visit phi-operations and instructions.
-    for (HInstructionIterator it(loop_block->GetPhis()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(loop_block->GetPhis()); !it.Done(); it.Advance()) {
       global_depth = TryVisitNodes(loop, it.Current(), global_depth, &visited_instructions);
     }
-    for (HInstructionIterator it(loop_block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(loop_block->GetInstructions()); !it.Done();
+         it.Advance()) {
       global_depth = TryVisitNodes(loop, it.Current(), global_depth, &visited_instructions);
     }
   }
@@ -270,6 +272,14 @@ void HInductionVarAnalysis::VisitLoop(const HLoopInformation* loop) {
   VisitControl(loop);
 }
 
+void HInductionVarAnalysis::ReVisitLoop(const HLoopInformation* loop) {
+  induction_.erase(loop);
+  for (HInstructionIterator it(loop->GetHeader()->GetPhis()); !it.Done(); it.Advance()) {
+    cycles_.erase(it.Current()->AsPhi());
+  }
+  VisitLoop(loop);
+}
+
 size_t HInductionVarAnalysis::TryVisitNodes(
     const HLoopInformation* loop,
     HInstruction* start_instruction,
@@ -1648,7 +1658,7 @@ bool HInductionVarAnalysis::IsPathologicalCase() {
       continue;
     }
 
-    for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
       DCHECK(it.Current()->IsLoopHeaderPhi());
       HPhi* phi = it.Current()->AsPhi();
       CalculateLoopHeaderPhisInARow(phi, cached_values, local_allocator);
diff --git a/compiler/optimizing/induction_var_analysis.h b/compiler/optimizing/induction_var_analysis.h
index 050950089a..86e0f33f3d 100644
--- a/compiler/optimizing/induction_var_analysis.h
+++ b/compiler/optimizing/induction_var_analysis.h
@@ -157,6 +157,7 @@ class HInductionVarAnalysis : public HOptimization {
 
   // Methods for analysis.
   void VisitLoop(const HLoopInformation* loop);
+  void ReVisitLoop(const HLoopInformation* loop);
   size_t TryVisitNodes(const HLoopInformation* loop,
                        HInstruction* start_instruction,
                        size_t global_depth,
diff --git a/compiler/optimizing/induction_var_range.h b/compiler/optimizing/induction_var_range.h
index ab497414c9..5367f017eb 100644
--- a/compiler/optimizing/induction_var_range.h
+++ b/compiler/optimizing/induction_var_range.h
@@ -137,11 +137,7 @@ class InductionVarRange {
    * Incrementally updates induction information for just the given loop.
    */
   void ReVisit(const HLoopInformation* loop) {
-    induction_analysis_->induction_.erase(loop);
-    for (HInstructionIterator it(loop->GetHeader()->GetPhis()); !it.Done(); it.Advance()) {
-      induction_analysis_->cycles_.erase(it.Current()->AsPhi());
-    }
-    induction_analysis_->VisitLoop(loop);
+    induction_analysis_->ReVisitLoop(loop);
   }
 
   /**
diff --git a/compiler/optimizing/inliner.cc b/compiler/optimizing/inliner.cc
index 251867d512..cda8d498f9 100644
--- a/compiler/optimizing/inliner.cc
+++ b/compiler/optimizing/inliner.cc
@@ -116,7 +116,7 @@ std::string HInliner::DepthString(int line) const {
 static size_t CountNumberOfInstructions(HGraph* graph) {
   size_t number_of_instructions = 0;
   for (HBasicBlock* block : graph->GetReversePostOrderSkipEntryBlock()) {
-    for (HInstructionIterator instr_it(block->GetInstructions());
+    for (HInstructionIteratorPrefetchNext instr_it(block->GetInstructions());
          !instr_it.Done();
          instr_it.Advance()) {
       ++number_of_instructions;
@@ -140,7 +140,9 @@ bool HInliner::Run() {
   if (codegen_->GetCompilerOptions().GetInlineMaxCodeUnits() == 0) {
     // Inlining effectively disabled.
     return false;
-  } else if (graph_->IsDebuggable()) {
+  }
+
+  if (graph_->IsDebuggable()) {
     // For simplicity, we currently never inline when the graph is debuggable. This avoids
     // doing some logic in the runtime to discover if a method could have been inlined.
     return false;
@@ -1589,6 +1591,30 @@ bool HInliner::IsInliningEncouraged(const HInvoke* invoke_instruction,
     return false;
   }
 
+  if (total_number_of_dex_registers_ > kMaximumNumberOfCumulatedDexRegisters) {
+    // Heuristic: Skip building the callee graph for large environments, as we will likely discard
+    // it later.
+    LOG_FAIL(stats_, MethodCompilationStat::kNotInlinedEnvironmentBudget)
+        << "Method " << method->PrettyMethod()
+        << " is not inlined because its block ends with a throw";
+    return false;
+  }
+
+  // The heuristic below tries to prevent inline attempts where the graph is built but discarded
+  // later due to its size being over the budget. A rough estimate of method size (`HInstruction`
+  // count) is 2/3 of code item size (it is not always true - a large code item may result in just
+  // a few `HInstruction`s, but experiments show such methods are relatively rare). We use factor
+  // 3/4 rather than 2/3 as the experiments show that it results in approximately the same number
+  // of prevented-successful inline attempts, but higher prevented-failed attempts.
+  size_t estimated_size = (accessor.InsnsSizeInCodeUnits() * 3u) / 4u;
+  if (estimated_size > inlining_budget_) {
+    LOG_FAIL(stats_, MethodCompilationStat::kNotInlinedCodeItem)
+        << "Method " << method->PrettyMethod()
+        << " is not inlined because its estimated size based on code item exceeds inlining budget: "
+        << estimated_size << " > " << inlining_budget_;
+    return false;
+  }
+
   return true;
 }
 
@@ -1937,7 +1963,8 @@ void HInliner::SubstituteArguments(HGraph* callee_graph,
   ArtMethod* const resolved_method = callee_graph->GetArtMethod();
   size_t parameter_index = 0;
   bool run_rtp = false;
-  for (HInstructionIterator instructions(callee_graph->GetEntryBlock()->GetInstructions());
+  for (HInstructionIteratorPrefetchNext instructions(
+           callee_graph->GetEntryBlock()->GetInstructions());
        !instructions.Done();
        instructions.Advance()) {
     HInstruction* current = instructions.Current();
@@ -2071,8 +2098,6 @@ bool HInliner::CanInlineBody(const HGraph* callee_graph,
     return false;
   }
 
-  const bool too_many_registers =
-      total_number_of_dex_registers_ > kMaximumNumberOfCumulatedDexRegisters;
   bool needs_bss_check = false;
   const bool can_encode_in_stack_map = CanEncodeInlinedMethodInStackMap(
       *outer_compilation_unit_.GetDexFile(), resolved_method, codegen_, &needs_bss_check);
@@ -2099,7 +2124,7 @@ bool HInliner::CanInlineBody(const HGraph* callee_graph,
       }
     }
 
-    for (HInstructionIterator instr_it(block->GetInstructions());
+    for (HInstructionIteratorPrefetchNext instr_it(block->GetInstructions());
          !instr_it.Done();
          instr_it.Advance()) {
       if (++number_of_instructions > inlining_budget_) {
@@ -2111,14 +2136,6 @@ bool HInliner::CanInlineBody(const HGraph* callee_graph,
       }
       HInstruction* current = instr_it.Current();
       if (current->NeedsEnvironment()) {
-        if (too_many_registers) {
-          LOG_FAIL(stats_, MethodCompilationStat::kNotInlinedEnvironmentBudget)
-              << "Method " << resolved_method->PrettyMethod()
-              << " is not inlined because its caller has reached"
-              << " its environment budget limit.";
-          return false;
-        }
-
         if (!can_encode_in_stack_map) {
           LOG_FAIL(stats_, MethodCompilationStat::kNotInlinedStackMaps)
               << "Method " << resolved_method->PrettyMethod() << " could not be inlined because "
@@ -2344,15 +2361,6 @@ void HInliner::RunOptimizations(HGraph* callee_graph,
     optimization->Run();
   }
 
-  // Bail early for pathological cases on the environment (for example recursive calls,
-  // or too large environment).
-  if (total_number_of_dex_registers_ > kMaximumNumberOfCumulatedDexRegisters) {
-    LOG_NOTE() << "Calls in " << callee_graph->GetArtMethod()->PrettyMethod()
-             << " will not be inlined because the outer method has reached"
-             << " its environment budget limit.";
-    return;
-  }
-
   // Bail early if we know we already are over the limit.
   size_t number_of_instructions = CountNumberOfInstructions(callee_graph);
   if (number_of_instructions > inlining_budget_) {
diff --git a/compiler/optimizing/inliner.h b/compiler/optimizing/inliner.h
index 2ca286ea6a..4db94b8d7f 100644
--- a/compiler/optimizing/inliner.h
+++ b/compiler/optimizing/inliner.h
@@ -26,11 +26,21 @@
 
 namespace art HIDDEN {
 
+class ArtMethod;
 class CodeGenerator;
+class CodeItemDataAccessor;
 class DexCompilationUnit;
+template <class T> class Handle;
+class HBasicBlock;
+class HEnvironment;
 class HGraph;
+class HInstanceFieldGet;
+class HInstanceFieldSet;
+class HInstruction;
 class HInvoke;
 class OptimizingCompilerStats;
+class ReferenceTypeInfo;
+template <size_t kNumReferences> class StackHandleScope;
 
 class HInliner : public HOptimization {
  public:
diff --git a/compiler/optimizing/instruction_builder.cc b/compiler/optimizing/instruction_builder.cc
index 8cc79c2424..a4f1b22c44 100644
--- a/compiler/optimizing/instruction_builder.cc
+++ b/compiler/optimizing/instruction_builder.cc
@@ -311,7 +311,7 @@ void HInstructionBuilder::InitializeInstruction(HInstruction* instruction) {
 }
 
 HInstruction* HInstructionBuilder::LoadNullCheckedLocal(uint32_t register_index, uint32_t dex_pc) {
-  HInstruction* ref = LoadLocal(register_index, DataType::Type::kReference);
+  HInstruction* ref = LoadLocal<DataType::Type::kReference>(register_index);
   if (!ref->CanBeNull()) {
     return ref;
   }
@@ -324,7 +324,7 @@ HInstruction* HInstructionBuilder::LoadNullCheckedLocal(uint32_t register_index,
 void HInstructionBuilder::SetLoopHeaderPhiInputs() {
   for (size_t i = loop_headers_.size(); i > 0; --i) {
     HBasicBlock* block = loop_headers_[i - 1];
-    for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
       HPhi* phi = it.Current()->AsPhi();
       size_t vreg = phi->GetRegNumber();
       for (HBasicBlock* predecessor : block->GetPredecessors()) {
@@ -570,15 +570,19 @@ ArenaBitVector* HInstructionBuilder::FindNativeDebugInfoLocations() {
   return locations;
 }
 
+template <bool kCanBeRef, bool kCanBeFp>
+ALWAYS_INLINE inline
 HInstruction* HInstructionBuilder::LoadLocal(uint32_t reg_number, DataType::Type type) const {
   HInstruction* value = (*current_locals_)[reg_number];
   DCHECK(value != nullptr);
 
   // If the operation requests a specific type, we make sure its input is of that type.
-  if (type != value->GetType()) {
-    if (DataType::IsFloatingPointType(type)) {
+  DCHECK_IMPLIES(type == DataType::Type::kReference, kCanBeRef);
+  DCHECK_IMPLIES(DataType::IsFloatingPointType(type), kCanBeFp);
+  if ((kCanBeRef || kCanBeFp) && type != value->GetType()) {
+    if (kCanBeFp && DataType::IsFloatingPointType(type)) {
       value = ssa_builder_->GetFloatOrDoubleEquivalent(value, type);
-    } else if (type == DataType::Type::kReference) {
+    } else if (kCanBeRef && type == DataType::Type::kReference) {
       value = ssa_builder_->GetReferenceTypeEquivalent(value);
     }
     DCHECK(value != nullptr);
@@ -587,6 +591,14 @@ HInstruction* HInstructionBuilder::LoadLocal(uint32_t reg_number, DataType::Type
   return value;
 }
 
+template <DataType::Type kType>
+ALWAYS_INLINE inline
+HInstruction* HInstructionBuilder::LoadLocal(uint32_t reg_number) const {
+  static constexpr bool kCanBeRef = kType == DataType::Type::kReference;
+  static constexpr bool kCanBeFp = DataType::IsFloatingPointType(kType);
+  return LoadLocal<kCanBeRef, kCanBeFp>(reg_number, kType);
+}
+
 void HInstructionBuilder::UpdateLocal(uint32_t reg_number, HInstruction* stored_value) {
   DataType::Type stored_type = stored_value->GetType();
   DCHECK_NE(stored_type, DataType::Type::kVoid);
@@ -673,14 +685,13 @@ template<typename T, bool kCompareWithZero>
 void HInstructionBuilder::If_21_22t(const Instruction& instruction, uint32_t dex_pc) {
   DCHECK_EQ(kCompareWithZero ? Instruction::Format::k21t : Instruction::Format::k22t,
             Instruction::FormatOf(instruction.Opcode()));
-  HInstruction* value = LoadLocal(
-      kCompareWithZero ? instruction.VRegA_21t() : instruction.VRegA_22t(),
-      DataType::Type::kInt32);
+  HInstruction* value = LoadLocal<DataType::Type::kInt32>(
+      kCompareWithZero ? instruction.VRegA_21t() : instruction.VRegA_22t());
   T* comparison = nullptr;
   if (kCompareWithZero) {
     comparison = new (allocator_) T(value, graph_->GetIntConstant(0), dex_pc);
   } else {
-    HInstruction* second = LoadLocal(instruction.VRegB_22t(), DataType::Type::kInt32);
+    HInstruction* second = LoadLocal<DataType::Type::kInt32>(instruction.VRegB_22t());
     comparison = new (allocator_) T(value, second, dex_pc);
   }
   AppendInstruction(comparison);
@@ -705,7 +716,10 @@ template<typename T>
 void HInstructionBuilder::Unop_12x(const Instruction& instruction,
                                    DataType::Type type,
                                    uint32_t dex_pc) {
-  HInstruction* first = LoadLocal(instruction.VRegB_12x(), type);
+  // Unary operations are never on references. Unary ones complement is always integral.
+  static constexpr bool kCanBeRef = false;
+  static constexpr bool kCanBeFp = !std::is_same_v<T, HNot>;
+  HInstruction* first = LoadLocal<kCanBeRef, kCanBeFp>(instruction.VRegB_12x(), type);
   AppendInstruction(new (allocator_) T(type, first, dex_pc));
   UpdateLocal(instruction.VRegA_12x(), current_block_->GetLastInstruction());
 }
@@ -714,7 +728,10 @@ void HInstructionBuilder::Conversion_12x(const Instruction& instruction,
                                          DataType::Type input_type,
                                          DataType::Type result_type,
                                          uint32_t dex_pc) {
-  HInstruction* first = LoadLocal(instruction.VRegB_12x(), input_type);
+  // Conversion are never on references.
+  static constexpr bool kCanBeRef = false;
+  static constexpr bool kCanBeFp = true;
+  HInstruction* first = LoadLocal<kCanBeRef, kCanBeFp>(instruction.VRegB_12x(), input_type);
   AppendInstruction(new (allocator_) HTypeConversion(result_type, first, dex_pc));
   UpdateLocal(instruction.VRegA_12x(), current_block_->GetLastInstruction());
 }
@@ -723,8 +740,12 @@ template<typename T>
 void HInstructionBuilder::Binop_23x(const Instruction& instruction,
                                     DataType::Type type,
                                     uint32_t dex_pc) {
-  HInstruction* first = LoadLocal(instruction.VRegB_23x(), type);
-  HInstruction* second = LoadLocal(instruction.VRegC_23x(), type);
+  // Binary operations are never on references. Bitwise operations are always integral.
+  static constexpr bool kCanBeRef = false;
+  static constexpr bool kCanBeFp =
+      !(std::is_same_v<T, HAnd> || std::is_same_v<T, HOr> || std::is_same_v<T, HXor>);
+  HInstruction* first = LoadLocal<kCanBeRef, kCanBeFp>(instruction.VRegB_23x(), type);
+  HInstruction* second = LoadLocal<kCanBeRef, kCanBeFp>(instruction.VRegC_23x(), type);
   AppendInstruction(new (allocator_) T(type, first, second, dex_pc));
   UpdateLocal(instruction.VRegA_23x(), current_block_->GetLastInstruction());
 }
@@ -733,8 +754,12 @@ template<typename T>
 void HInstructionBuilder::Binop_23x_shift(const Instruction& instruction,
                                           DataType::Type type,
                                           uint32_t dex_pc) {
-  HInstruction* first = LoadLocal(instruction.VRegB_23x(), type);
-  HInstruction* second = LoadLocal(instruction.VRegC_23x(), DataType::Type::kInt32);
+  // Shifts are always integral.
+  static constexpr bool kCanBeFp = false;
+  static constexpr bool kCanBeRef = false;
+  HInstruction* first = LoadLocal<kCanBeRef, kCanBeFp>(instruction.VRegB_23x(), type);
+  HInstruction* second =
+      LoadLocal<kCanBeRef, kCanBeFp>(instruction.VRegC_23x(), DataType::Type::kInt32);
   AppendInstruction(new (allocator_) T(type, first, second, dex_pc));
   UpdateLocal(instruction.VRegA_23x(), current_block_->GetLastInstruction());
 }
@@ -743,8 +768,11 @@ void HInstructionBuilder::Binop_23x_cmp(const Instruction& instruction,
                                         DataType::Type type,
                                         ComparisonBias bias,
                                         uint32_t dex_pc) {
-  HInstruction* first = LoadLocal(instruction.VRegB_23x(), type);
-  HInstruction* second = LoadLocal(instruction.VRegC_23x(), type);
+  // There is no three-way compare for references.
+  static constexpr bool kCanBeRef = false;
+  static constexpr bool kCanBeFp = true;
+  HInstruction* first = LoadLocal<kCanBeRef, kCanBeFp>(instruction.VRegB_23x(), type);
+  HInstruction* second = LoadLocal<kCanBeRef, kCanBeFp>(instruction.VRegC_23x(), type);
   AppendInstruction(new (allocator_) HCompare(type, first, second, bias, dex_pc));
   UpdateLocal(instruction.VRegA_23x(), current_block_->GetLastInstruction());
 }
@@ -753,8 +781,12 @@ template<typename T>
 void HInstructionBuilder::Binop_12x_shift(const Instruction& instruction,
                                           DataType::Type type,
                                           uint32_t dex_pc) {
-  HInstruction* first = LoadLocal(instruction.VRegA_12x(), type);
-  HInstruction* second = LoadLocal(instruction.VRegB_12x(), DataType::Type::kInt32);
+  // Shifts are always integral.
+  static constexpr bool kCanBeFp = false;
+  static constexpr bool kCanBeRef = false;
+  HInstruction* first = LoadLocal<kCanBeRef, kCanBeFp>(instruction.VRegA_12x(), type);
+  HInstruction* second =
+      LoadLocal<kCanBeRef, kCanBeFp>(instruction.VRegB_12x(), DataType::Type::kInt32);
   AppendInstruction(new (allocator_) T(type, first, second, dex_pc));
   UpdateLocal(instruction.VRegA_12x(), current_block_->GetLastInstruction());
 }
@@ -763,15 +795,19 @@ template<typename T>
 void HInstructionBuilder::Binop_12x(const Instruction& instruction,
                                     DataType::Type type,
                                     uint32_t dex_pc) {
-  HInstruction* first = LoadLocal(instruction.VRegA_12x(), type);
-  HInstruction* second = LoadLocal(instruction.VRegB_12x(), type);
+  // Binary operations are never on references. Bitwise operations are always integral.
+  static constexpr bool kCanBeRef = false;
+  static constexpr bool kCanBeFp =
+      !(std::is_same_v<T, HAnd> || std::is_same_v<T, HOr> || std::is_same_v<T, HXor>);
+  HInstruction* first = LoadLocal<kCanBeRef, kCanBeFp>(instruction.VRegA_12x(), type);
+  HInstruction* second = LoadLocal<kCanBeRef, kCanBeFp>(instruction.VRegB_12x(), type);
   AppendInstruction(new (allocator_) T(type, first, second, dex_pc));
   UpdateLocal(instruction.VRegA_12x(), current_block_->GetLastInstruction());
 }
 
 template<typename T>
 void HInstructionBuilder::Binop_22s(const Instruction& instruction, bool reverse, uint32_t dex_pc) {
-  HInstruction* first = LoadLocal(instruction.VRegB_22s(), DataType::Type::kInt32);
+  HInstruction* first = LoadLocal<DataType::Type::kInt32>(instruction.VRegB_22s());
   HInstruction* second = graph_->GetIntConstant(instruction.VRegC_22s());
   if (reverse) {
     std::swap(first, second);
@@ -782,7 +818,7 @@ void HInstructionBuilder::Binop_22s(const Instruction& instruction, bool reverse
 
 template<typename T>
 void HInstructionBuilder::Binop_22b(const Instruction& instruction, bool reverse, uint32_t dex_pc) {
-  HInstruction* first = LoadLocal(instruction.VRegB_22b(), DataType::Type::kInt32);
+  HInstruction* first = LoadLocal<DataType::Type::kInt32>(instruction.VRegB_22b());
   HInstruction* second = graph_->GetIntConstant(instruction.VRegC_22b());
   if (reverse) {
     std::swap(first, second);
@@ -817,7 +853,7 @@ static bool IsFallthroughInstruction(const Instruction& instruction,
 }
 
 void HInstructionBuilder::BuildSwitch(const Instruction& instruction, uint32_t dex_pc) {
-  HInstruction* value = LoadLocal(instruction.VRegA_31t(), DataType::Type::kInt32);
+  HInstruction* value = LoadLocal<DataType::Type::kInt32>(instruction.VRegA_31t());
   DexSwitchTable table(instruction, dex_pc);
 
   if (table.GetNumEntries() == 0) {
@@ -843,22 +879,22 @@ void HInstructionBuilder::BuildSwitch(const Instruction& instruction, uint32_t d
   current_block_ = nullptr;
 }
 
-template <DataType::Type type>
+template <DataType::Type kType>
 ALWAYS_INLINE inline void HInstructionBuilder::BuildMove(uint32_t dest_reg, uint32_t src_reg) {
   // The verifier has no notion of a null type, so a move-object of constant 0
   // will lead to the same constant 0 in the destination register. To mimic
   // this behavior, we just pretend we haven't seen a type change (int to reference)
   // for the 0 constant and phis. We rely on our type propagation to eventually get the
   // types correct.
-  constexpr bool is_reference = type == DataType::Type::kReference;
-  HInstruction* value = is_reference ? (*current_locals_)[src_reg] : /* not needed */ nullptr;
-  if (is_reference && value->IsIntConstant()) {
+  static constexpr bool kIsReference = kType == DataType::Type::kReference;
+  HInstruction* value = kIsReference ? (*current_locals_)[src_reg] : /* not needed */ nullptr;
+  if (kIsReference && value->IsIntConstant()) {
     DCHECK_EQ(value->AsIntConstant()->GetValue(), 0);
-  } else if (is_reference && value->IsPhi()) {
+  } else if (kIsReference && value->IsPhi()) {
     DCHECK(value->GetType() == DataType::Type::kInt32 ||
            value->GetType() == DataType::Type::kReference);
   } else {
-    value = LoadLocal(src_reg, type);
+    value = LoadLocal<kType>(src_reg);
   }
   UpdateLocal(dest_reg, value);
 }
@@ -1885,7 +1921,7 @@ bool HInstructionBuilder::SetupInvokeArguments(HInstruction* invoke,
       if (receiver_arg != ReceiverArg::kIgnored) {
         uint32_t obj_reg = operands.GetOperand(0u);
         HInstruction* arg = (receiver_arg == ReceiverArg::kPlainArg)
-            ? LoadLocal(obj_reg, DataType::Type::kReference)
+            ? LoadLocal<DataType::Type::kReference>(obj_reg)
             : LoadNullCheckedLocal(obj_reg, invoke->GetDexPc());
         if (receiver_arg != ReceiverArg::kNullCheckedOnly) {
           invoke->SetRawInputAt(0u, arg);
@@ -2191,7 +2227,7 @@ bool HInstructionBuilder::HandleStringInit(HInvoke* invoke,
   // This is a StringFactory call, not an actual String constructor. Its result
   // replaces the empty String pre-allocated by NewInstance.
   uint32_t orig_this_reg = operands.GetOperand(0);
-  HInstruction* arg_this = LoadLocal(orig_this_reg, DataType::Type::kReference);
+  HInstruction* arg_this = LoadLocal<DataType::Type::kReference>(orig_this_reg);
 
   // Replacing the NewInstance might render it redundant. Keep a list of these
   // to be visited once it is clear whether it has remaining uses.
@@ -2237,7 +2273,7 @@ bool HInstructionBuilder::BuildInstanceFieldAccess(const Instruction& instructio
   // is unresolved. In that case, we rely on the runtime to perform various
   // checks first, followed by a null check.
   HInstruction* object = (resolved_field == nullptr)
-      ? LoadLocal(obj_reg, DataType::Type::kReference)
+      ? LoadLocal<DataType::Type::kReference>(obj_reg)
       : LoadNullCheckedLocal(obj_reg, dex_pc);
 
   DataType::Type field_type = GetFieldAccessType(*dex_file_, field_index);
@@ -2472,7 +2508,7 @@ void HInstructionBuilder::BuildCheckedDivRem(uint16_t out_vreg,
                                              bool is_div) {
   DCHECK(type == DataType::Type::kInt32 || type == DataType::Type::kInt64);
 
-  HInstruction* first = LoadLocal(first_vreg, type);
+  HInstruction* first = LoadLocal</*kCanBeRef=*/ false, /*kCanBeFp=*/ false>(first_vreg, type);
   HInstruction* second = nullptr;
   if (second_is_constant) {
     if (type == DataType::Type::kInt32) {
@@ -2481,7 +2517,7 @@ void HInstructionBuilder::BuildCheckedDivRem(uint16_t out_vreg,
       second = graph_->GetLongConstant(second_vreg_or_constant);
     }
   } else {
-    second = LoadLocal(second_vreg_or_constant, type);
+    second = LoadLocal</*kCanBeRef=*/ false, /*kCanBeFp=*/ false>(second_vreg_or_constant, type);
   }
 
   if (!second_is_constant ||
@@ -2510,11 +2546,14 @@ void HInstructionBuilder::BuildArrayAccess(const Instruction& instruction,
   HInstruction* object = LoadNullCheckedLocal(array_reg, dex_pc);
   HInstruction* length = new (allocator_) HArrayLength(object, dex_pc);
   AppendInstruction(length);
-  HInstruction* index = LoadLocal(index_reg, DataType::Type::kInt32);
+  HInstruction* index = LoadLocal<DataType::Type::kInt32>(index_reg);
   index = new (allocator_) HBoundsCheck(index, length, dex_pc);
   AppendInstruction(index);
   if (is_put) {
-    HInstruction* value = LoadLocal(source_or_dest_reg, anticipated_type);
+    // The `anticipated_type` can be a reference but it is never floating-point.
+    static constexpr bool kCanBeRef = true;
+    static constexpr bool kCanBeFp = false;
+    HInstruction* value = LoadLocal<kCanBeRef, kCanBeFp>(source_or_dest_reg, anticipated_type);
     // TODO: Insert a type check node if the type is Object.
     HArraySet* aset = new (allocator_) HArraySet(object, index, value, anticipated_type, dex_pc);
     ssa_builder_->MaybeAddAmbiguousArraySet(aset);
@@ -2877,7 +2916,7 @@ void HInstructionBuilder::BuildTypeCheck(const Instruction& instruction,
                                          uint8_t reference,
                                          dex::TypeIndex type_index,
                                          uint32_t dex_pc) {
-  HInstruction* object = LoadLocal(reference, DataType::Type::kReference);
+  HInstruction* object = LoadLocal<DataType::Type::kReference>(reference);
   bool is_instance_of = instruction.Opcode() == Instruction::INSTANCE_OF;
 
   BuildTypeCheck(is_instance_of, object, type_index, dex_pc);
@@ -3710,7 +3749,7 @@ bool HInstructionBuilder::ProcessDexInstruction(const Instruction& instruction,
 
     case Instruction::NEW_ARRAY: {
       dex::TypeIndex type_index(instruction.VRegC_22c());
-      HInstruction* length = LoadLocal(instruction.VRegB_22c(), DataType::Type::kInt32);
+      HInstruction* length = LoadLocal<DataType::Type::kInt32>(instruction.VRegB_22c());
       HNewArray* new_array = BuildNewArray(dex_pc, type_index, length);
 
       UpdateLocal(instruction.VRegA_22c(), current_block_->GetLastInstruction());
@@ -3896,7 +3935,7 @@ bool HInstructionBuilder::ProcessDexInstruction(const Instruction& instruction,
     }
 
     case Instruction::THROW: {
-      HInstruction* exception = LoadLocal(instruction.VRegA_11x(), DataType::Type::kReference);
+      HInstruction* exception = LoadLocal<DataType::Type::kReference>(instruction.VRegA_11x());
       AppendInstruction(new (allocator_) HThrow(exception, dex_pc));
       // We finished building this block. Set the current block to null to avoid
       // adding dead instructions to it.
@@ -3921,7 +3960,7 @@ bool HInstructionBuilder::ProcessDexInstruction(const Instruction& instruction,
 
     case Instruction::MONITOR_ENTER: {
       AppendInstruction(new (allocator_) HMonitorOperation(
-          LoadLocal(instruction.VRegA_11x(), DataType::Type::kReference),
+          LoadLocal<DataType::Type::kReference>(instruction.VRegA_11x()),
           HMonitorOperation::OperationKind::kEnter,
           dex_pc));
       graph_->SetHasMonitorOperations(true);
@@ -3930,7 +3969,7 @@ bool HInstructionBuilder::ProcessDexInstruction(const Instruction& instruction,
 
     case Instruction::MONITOR_EXIT: {
       AppendInstruction(new (allocator_) HMonitorOperation(
-          LoadLocal(instruction.VRegA_11x(), DataType::Type::kReference),
+          LoadLocal<DataType::Type::kReference>(instruction.VRegA_11x()),
           HMonitorOperation::OperationKind::kExit,
           dex_pc));
       graph_->SetHasMonitorOperations(true);
diff --git a/compiler/optimizing/instruction_builder.h b/compiler/optimizing/instruction_builder.h
index 90ab75ec92..2b4ae78869 100644
--- a/compiler/optimizing/instruction_builder.h
+++ b/compiler/optimizing/instruction_builder.h
@@ -80,7 +80,13 @@ class HInstructionBuilder : public ValueObject {
   ScopedArenaVector<HInstruction*>* GetLocalsForWithAllocation(
       HBasicBlock* block, ScopedArenaVector<HInstruction*>* locals, const size_t vregs);
   HInstruction* ValueOfLocalAt(HBasicBlock* block, size_t local);
+
+  template <bool kCanBeRef = true, bool kCanBeFp = true>
   HInstruction* LoadLocal(uint32_t register_index, DataType::Type type) const;
+
+  template <DataType::Type kType>
+  HInstruction* LoadLocal(uint32_t register_index) const;
+
   HInstruction* LoadNullCheckedLocal(uint32_t register_index, uint32_t dex_pc);
   void UpdateLocal(uint32_t register_index, HInstruction* instruction);
 
@@ -132,7 +138,7 @@ class HInstructionBuilder : public ValueObject {
                           bool second_is_lit,
                           bool is_div);
 
-  template <DataType::Type type>
+  template <DataType::Type kType>
   void BuildMove(uint32_t dest_reg, uint32_t src_reg);
 
   void BuildReturn(const Instruction& instruction, DataType::Type type, uint32_t dex_pc);
diff --git a/compiler/optimizing/instruction_list.cc b/compiler/optimizing/instruction_list.cc
new file mode 100644
index 0000000000..8e22e18ba6
--- /dev/null
+++ b/compiler/optimizing/instruction_list.cc
@@ -0,0 +1,158 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "instruction_list.h"
+
+#include "nodes.h"
+
+namespace art HIDDEN {
+
+void HInstructionList::AddInstruction(HInstruction* instruction) {
+  if (first_instruction_ == nullptr) {
+    DCHECK(last_instruction_ == nullptr);
+    first_instruction_ = last_instruction_ = instruction;
+  } else {
+    DCHECK(last_instruction_ != nullptr);
+    last_instruction_->next_ = instruction;
+    instruction->previous_ = last_instruction_;
+    last_instruction_ = instruction;
+  }
+}
+
+void HInstructionList::InsertInstructionBefore(HInstruction* instruction, HInstruction* cursor) {
+  DCHECK(Contains(cursor));
+  if (cursor == first_instruction_) {
+    cursor->previous_ = instruction;
+    instruction->next_ = cursor;
+    first_instruction_ = instruction;
+  } else {
+    instruction->previous_ = cursor->previous_;
+    instruction->next_ = cursor;
+    cursor->previous_ = instruction;
+    instruction->previous_->next_ = instruction;
+  }
+}
+
+void HInstructionList::InsertInstructionAfter(HInstruction* instruction, HInstruction* cursor) {
+  DCHECK(Contains(cursor));
+  if (cursor == last_instruction_) {
+    cursor->next_ = instruction;
+    instruction->previous_ = cursor;
+    last_instruction_ = instruction;
+  } else {
+    instruction->next_ = cursor->next_;
+    instruction->previous_ = cursor;
+    cursor->next_ = instruction;
+    instruction->next_->previous_ = instruction;
+  }
+}
+
+void HInstructionList::RemoveInstruction(HInstruction* instruction) {
+  DCHECK_EQ(instruction->previous_ == nullptr, instruction == first_instruction_);
+  DCHECK_EQ(instruction->next_ == nullptr, instruction == last_instruction_);
+
+  if (instruction == first_instruction_) {
+    first_instruction_ = instruction->next_;
+  } else {
+    instruction->previous_->next_ = instruction->next_;
+  }
+
+  if (instruction == last_instruction_) {
+    last_instruction_ = instruction->previous_;
+  } else {
+    instruction->next_->previous_ = instruction->previous_;
+  }
+}
+
+bool HInstructionList::Contains(HInstruction* instruction) const {
+  for (HInstructionIteratorPrefetchNext it(*this); !it.Done(); it.Advance()) {
+    if (it.Current() == instruction) {
+      return true;
+    }
+  }
+  return false;
+}
+
+bool HInstructionList::FoundBefore(const HInstruction* instruction1,
+                                   const HInstruction* instruction2) const {
+  DCHECK_EQ(instruction1->GetBlock(), instruction2->GetBlock());
+  for (HInstructionIteratorPrefetchNext it(*this); !it.Done(); it.Advance()) {
+    if (it.Current() == instruction2) {
+      return false;
+    }
+    if (it.Current() == instruction1) {
+      return true;
+    }
+  }
+  LOG(FATAL) << "Did not find an order between two instructions of the same block.";
+  UNREACHABLE();
+}
+
+size_t HInstructionList::CountSize() const {
+  size_t size = 0;
+  HInstruction* current = first_instruction_;
+  for (; current != nullptr; current = current->GetNext()) {
+    size++;
+  }
+  return size;
+}
+
+void HInstructionList::SetBlockOfInstructions(HBasicBlock* block) const {
+  for (HInstruction* current = first_instruction_;
+       current != nullptr;
+       current = current->GetNext()) {
+    current->SetBlock(block);
+  }
+}
+
+void HInstructionList::AddAfter(HInstruction* cursor, const HInstructionList& instruction_list) {
+  DCHECK(Contains(cursor));
+  if (!instruction_list.IsEmpty()) {
+    if (cursor == last_instruction_) {
+      last_instruction_ = instruction_list.last_instruction_;
+    } else {
+      cursor->next_->previous_ = instruction_list.last_instruction_;
+    }
+    instruction_list.last_instruction_->next_ = cursor->next_;
+    cursor->next_ = instruction_list.first_instruction_;
+    instruction_list.first_instruction_->previous_ = cursor;
+  }
+}
+
+void HInstructionList::AddBefore(HInstruction* cursor, const HInstructionList& instruction_list) {
+  DCHECK(Contains(cursor));
+  if (!instruction_list.IsEmpty()) {
+    if (cursor == first_instruction_) {
+      first_instruction_ = instruction_list.first_instruction_;
+    } else {
+      cursor->previous_->next_ = instruction_list.first_instruction_;
+    }
+    instruction_list.last_instruction_->next_ = cursor;
+    instruction_list.first_instruction_->previous_ = cursor->previous_;
+    cursor->previous_ = instruction_list.last_instruction_;
+  }
+}
+
+void HInstructionList::Add(const HInstructionList& instruction_list) {
+  if (IsEmpty()) {
+    first_instruction_ = instruction_list.first_instruction_;
+    last_instruction_ = instruction_list.last_instruction_;
+  } else {
+    AddAfter(last_instruction_, instruction_list);
+  }
+}
+
+}  // namespace art
diff --git a/compiler/optimizing/instruction_list.h b/compiler/optimizing/instruction_list.h
new file mode 100644
index 0000000000..c4df86b2ee
--- /dev/null
+++ b/compiler/optimizing/instruction_list.h
@@ -0,0 +1,76 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_COMPILER_OPTIMIZING_INSTRUCTION_LIST_H_
+#define ART_COMPILER_OPTIMIZING_INSTRUCTION_LIST_H_
+
+#include "base/value_object.h"
+#include "base/macros.h"
+
+namespace art HIDDEN {
+
+class HBasicBlock;
+class HInstruction;
+
+class HInstructionList final : public ValueObject {
+ public:
+  HInstructionList() : first_instruction_(nullptr), last_instruction_(nullptr) {}
+
+  void AddInstruction(HInstruction* instruction);
+  void RemoveInstruction(HInstruction* instruction);
+
+  // Insert `instruction` before/after an existing instruction `cursor`.
+  void InsertInstructionBefore(HInstruction* instruction, HInstruction* cursor);
+  void InsertInstructionAfter(HInstruction* instruction, HInstruction* cursor);
+
+  // Return true if this list contains `instruction`.
+  bool Contains(HInstruction* instruction) const;
+
+  // Return true if `instruction1` is found before `instruction2` in this instruction
+  // list and false otherwise.  Abort if none of these instructions is found.
+  bool FoundBefore(const HInstruction* instruction1,
+                   const HInstruction* instruction2) const;
+
+  bool IsEmpty() const { return first_instruction_ == nullptr; }
+  void Clear() { first_instruction_ = last_instruction_ = nullptr; }
+
+  // Update the block of all instructions to be `block`.
+  void SetBlockOfInstructions(HBasicBlock* block) const;
+
+  void AddAfter(HInstruction* cursor, const HInstructionList& instruction_list);
+  void AddBefore(HInstruction* cursor, const HInstructionList& instruction_list);
+  void Add(const HInstructionList& instruction_list);
+
+  // Return the number of instructions in the list. This is an expensive operation.
+  size_t CountSize() const;
+
+ private:
+  HInstruction* first_instruction_;
+  HInstruction* last_instruction_;
+
+  friend class HBasicBlock;
+  friend class HGraph;
+  friend class HInstruction;
+  friend class HInstructionIteratorPrefetchNext;
+  friend class HInstructionIterator;
+  friend class HBackwardInstructionIteratorPrefetchNext;
+
+  DISALLOW_COPY_AND_ASSIGN(HInstructionList);
+};
+
+}  // namespace art
+
+#endif  // ART_COMPILER_OPTIMIZING_INSTRUCTION_LIST_H_
diff --git a/compiler/optimizing/instruction_simplifier.cc b/compiler/optimizing/instruction_simplifier.cc
index 4ef0fc907a..b24119a643 100644
--- a/compiler/optimizing/instruction_simplifier.cc
+++ b/compiler/optimizing/instruction_simplifier.cc
@@ -3038,7 +3038,8 @@ static bool TryReplaceStringBuilderAppend(CodeGenerator* codegen, HInvoke* invok
   uint32_t num_args = 0u;
   bool has_fp_args = false;
   HInstruction* args[StringBuilderAppend::kMaxArgs];  // Added in reverse order.
-  for (HBackwardInstructionIterator iter(block->GetInstructions()); !iter.Done(); iter.Advance()) {
+  for (HBackwardInstructionIteratorPrefetchNext iter(block->GetInstructions()); !iter.Done();
+       iter.Advance()) {
     HInstruction* user = iter.Current();
     // Instructions of interest apply to `sb`, skip those that do not involve `sb`.
     if (user->InputCount() == 0u || user->InputAt(0u) != sb) {
diff --git a/compiler/optimizing/instruction_simplifier.h b/compiler/optimizing/instruction_simplifier.h
index 6f9e1f334e..2012718f33 100644
--- a/compiler/optimizing/instruction_simplifier.h
+++ b/compiler/optimizing/instruction_simplifier.h
@@ -18,13 +18,13 @@
 #define ART_COMPILER_OPTIMIZING_INSTRUCTION_SIMPLIFIER_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
-#include "optimizing_compiler_stats.h"
 
 namespace art HIDDEN {
 
 class CodeGenerator;
+class HBinaryOperation;
+class HSub;
 
 /**
  * Implements optimizations specific to each instruction.
diff --git a/compiler/optimizing/instruction_simplifier_arm.cc b/compiler/optimizing/instruction_simplifier_arm.cc
index 7dccc036b5..25b7405291 100644
--- a/compiler/optimizing/instruction_simplifier_arm.cc
+++ b/compiler/optimizing/instruction_simplifier_arm.cc
@@ -61,10 +61,10 @@ class InstructionSimplifierArmVisitor final : public HGraphVisitor {
    */
   void VisitBasicBlock(HBasicBlock* block) override {
     // TODO: fragile iteration, provide more robust iterators?
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       HInstruction* instruction = it.Current();
       if (instruction->IsInBlock()) {
-        instruction->Accept(this);
+        Dispatch(instruction);
       }
     }
   }
diff --git a/compiler/optimizing/instruction_simplifier_arm.h b/compiler/optimizing/instruction_simplifier_arm.h
index 25cea7c829..b03377313b 100644
--- a/compiler/optimizing/instruction_simplifier_arm.h
+++ b/compiler/optimizing/instruction_simplifier_arm.h
@@ -18,7 +18,6 @@
 #define ART_COMPILER_OPTIMIZING_INSTRUCTION_SIMPLIFIER_ARM_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
 
 namespace art HIDDEN {
diff --git a/compiler/optimizing/instruction_simplifier_arm64.cc b/compiler/optimizing/instruction_simplifier_arm64.cc
index 8dd64e59ee..977372c290 100644
--- a/compiler/optimizing/instruction_simplifier_arm64.cc
+++ b/compiler/optimizing/instruction_simplifier_arm64.cc
@@ -21,6 +21,7 @@
 #include "instruction_simplifier_shared.h"
 #include "mirror/array-inl.h"
 #include "mirror/string.h"
+#include "nodes.h"
 
 namespace art HIDDEN {
 
@@ -63,10 +64,10 @@ class InstructionSimplifierArm64Visitor final : public HGraphVisitor {
    */
   void VisitBasicBlock(HBasicBlock* block) override {
     // TODO: fragile iteration, provide more robust iterators?
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       HInstruction* instruction = it.Current();
       if (instruction->IsInBlock()) {
-        instruction->Accept(this);
+        Dispatch(instruction);
       }
     }
   }
diff --git a/compiler/optimizing/instruction_simplifier_arm64.h b/compiler/optimizing/instruction_simplifier_arm64.h
index 5c57484b24..124dde1cc8 100644
--- a/compiler/optimizing/instruction_simplifier_arm64.h
+++ b/compiler/optimizing/instruction_simplifier_arm64.h
@@ -18,7 +18,6 @@
 #define ART_COMPILER_OPTIMIZING_INSTRUCTION_SIMPLIFIER_ARM64_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
 
 namespace art HIDDEN {
diff --git a/compiler/optimizing/instruction_simplifier_riscv64.cc b/compiler/optimizing/instruction_simplifier_riscv64.cc
index 6dc269f8c5..882796cb23 100644
--- a/compiler/optimizing/instruction_simplifier_riscv64.cc
+++ b/compiler/optimizing/instruction_simplifier_riscv64.cc
@@ -17,6 +17,7 @@
 #include "instruction_simplifier_riscv64.h"
 
 #include "instruction_simplifier.h"
+#include "nodes.h"
 
 namespace art HIDDEN {
 
@@ -33,10 +34,10 @@ class InstructionSimplifierRiscv64Visitor final : public HGraphVisitor {
   }
 
   void VisitBasicBlock(HBasicBlock* block) override {
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       HInstruction* instruction = it.Current();
       if (instruction->IsInBlock()) {
-        instruction->Accept(this);
+        Dispatch(instruction);
       }
     }
   }
diff --git a/compiler/optimizing/instruction_simplifier_riscv64.h b/compiler/optimizing/instruction_simplifier_riscv64.h
index 2fbfedddf4..2805a6cc5e 100644
--- a/compiler/optimizing/instruction_simplifier_riscv64.h
+++ b/compiler/optimizing/instruction_simplifier_riscv64.h
@@ -18,7 +18,6 @@
 #define ART_COMPILER_OPTIMIZING_INSTRUCTION_SIMPLIFIER_RISCV64_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
 
 namespace art HIDDEN {
diff --git a/compiler/optimizing/instruction_simplifier_riscv64_test.cc b/compiler/optimizing/instruction_simplifier_riscv64_test.cc
index 15c81c8957..74426ecb69 100644
--- a/compiler/optimizing/instruction_simplifier_riscv64_test.cc
+++ b/compiler/optimizing/instruction_simplifier_riscv64_test.cc
@@ -19,6 +19,7 @@
 #include <gtest/gtest.h>
 
 #include "base/globals.h"
+#include "optimizing/nodes.h"
 #include "optimizing_unit_test.h"
 
 namespace art HIDDEN {
@@ -27,31 +28,29 @@ namespace riscv64 {
 class InstructionSimplifierRiscv64Test : public OptimizingUnitTest {};
 
 TEST_F(InstructionSimplifierRiscv64Test, SimplifyShiftAdd) {
-  HGraph* graph = CreateGraph();
-  HBasicBlock* entry = AddNewBlock();
-  graph->SetEntryBlock(entry);
-  graph->BuildDominatorTree();
+  HBasicBlock* block = InitEntryMainExitGraphWithReturnVoid();
+  graph_->BuildDominatorTree();
 
   HInstruction* param0 = MakeParam(DataType::Type::kInt64);
   HInstruction* param1 = MakeParam(DataType::Type::kInt64);
-  HInstruction* c0 = graph->GetIntConstant(0);
-  HInstruction* c1 = graph->GetIntConstant(1);
-  HInstruction* c2 = graph->GetIntConstant(2);
-  HInstruction* c3 = graph->GetIntConstant(3);
-  HInstruction* c4 = graph->GetIntConstant(4);
-
-  HInstruction* shl0 = MakeBinOp<HShl>(entry, DataType::Type::kInt64, param0, c0);
-  HInstruction* add_shl0 = MakeBinOp<HAdd>(entry, DataType::Type::kInt64, param1, shl0);
-  HInstruction* shl1 = MakeBinOp<HShl>(entry, DataType::Type::kInt64, param0, c1);
-  HInstruction* add_shl1 = MakeBinOp<HAdd>(entry, DataType::Type::kInt64, param1, shl1);
-  HInstruction* shl2 = MakeBinOp<HShl>(entry, DataType::Type::kInt64, param0, c2);
-  HInstruction* add_shl2 = MakeBinOp<HAdd>(entry, DataType::Type::kInt64, param1, shl2);
-  HInstruction* shl3 = MakeBinOp<HShl>(entry, DataType::Type::kInt64, param0, c3);
-  HInstruction* add_shl3 = MakeBinOp<HAdd>(entry, DataType::Type::kInt64, param1, shl3);
-  HInstruction* shl4 = MakeBinOp<HShl>(entry, DataType::Type::kInt64, param0, c4);
-  HInstruction* add_shl4 = MakeBinOp<HAdd>(entry, DataType::Type::kInt64, param1, shl4);
-
-  InstructionSimplifierRiscv64 simplifier(graph, /*stats=*/ nullptr);
+  HInstruction* c0 = graph_->GetIntConstant(0);
+  HInstruction* c1 = graph_->GetIntConstant(1);
+  HInstruction* c2 = graph_->GetIntConstant(2);
+  HInstruction* c3 = graph_->GetIntConstant(3);
+  HInstruction* c4 = graph_->GetIntConstant(4);
+
+  HInstruction* shl0 = MakeBinOp<HShl>(block, DataType::Type::kInt64, param0, c0);
+  HInstruction* add_shl0 = MakeBinOp<HAdd>(block, DataType::Type::kInt64, param1, shl0);
+  HInstruction* shl1 = MakeBinOp<HShl>(block, DataType::Type::kInt64, param0, c1);
+  HInstruction* add_shl1 = MakeBinOp<HAdd>(block, DataType::Type::kInt64, param1, shl1);
+  HInstruction* shl2 = MakeBinOp<HShl>(block, DataType::Type::kInt64, param0, c2);
+  HInstruction* add_shl2 = MakeBinOp<HAdd>(block, DataType::Type::kInt64, param1, shl2);
+  HInstruction* shl3 = MakeBinOp<HShl>(block, DataType::Type::kInt64, param0, c3);
+  HInstruction* add_shl3 = MakeBinOp<HAdd>(block, DataType::Type::kInt64, param1, shl3);
+  HInstruction* shl4 = MakeBinOp<HShl>(block, DataType::Type::kInt64, param0, c4);
+  HInstruction* add_shl4 = MakeBinOp<HAdd>(block, DataType::Type::kInt64, param1, shl4);
+
+  InstructionSimplifierRiscv64 simplifier(graph_, /*stats=*/ nullptr);
   simplifier.Run();
 
   EXPECT_FALSE(add_shl0->GetBlock() == nullptr);
@@ -62,23 +61,21 @@ TEST_F(InstructionSimplifierRiscv64Test, SimplifyShiftAdd) {
 }
 
 TEST_F(InstructionSimplifierRiscv64Test, SimplifyShiftAddReusedShift) {
-  HGraph* graph = CreateGraph();
-  HBasicBlock* entry = AddNewBlock();
-  graph->SetEntryBlock(entry);
-  graph->BuildDominatorTree();
+  HBasicBlock* block = InitEntryMainExitGraphWithReturnVoid();
+  graph_->BuildDominatorTree();
 
   HInstruction* param0 = MakeParam(DataType::Type::kInt64);
   HInstruction* param1 = MakeParam(DataType::Type::kInt64);
   HInstruction* param2 = MakeParam(DataType::Type::kInt64);
   HInstruction* param3 = MakeParam(DataType::Type::kInt64);
-  HInstruction* c1 = graph->GetIntConstant(1);
+  HInstruction* c1 = graph_->GetIntConstant(1);
 
-  HInstruction* shl1 = MakeBinOp<HShl>(entry, DataType::Type::kInt64, param0, c1);
-  HInstruction* add1 = MakeBinOp<HAdd>(entry, DataType::Type::kInt64, param1, shl1);
-  HInstruction* add2 = MakeBinOp<HAdd>(entry, DataType::Type::kInt64, param2, shl1);
-  HInstruction* add3 = MakeBinOp<HAdd>(entry, DataType::Type::kInt64, param3, shl1);
+  HInstruction* shl1 = MakeBinOp<HShl>(block, DataType::Type::kInt64, param0, c1);
+  HInstruction* add1 = MakeBinOp<HAdd>(block, DataType::Type::kInt64, param1, shl1);
+  HInstruction* add2 = MakeBinOp<HAdd>(block, DataType::Type::kInt64, param2, shl1);
+  HInstruction* add3 = MakeBinOp<HAdd>(block, DataType::Type::kInt64, param3, shl1);
 
-  InstructionSimplifierRiscv64 simplifier(graph, /*stats=*/ nullptr);
+  InstructionSimplifierRiscv64 simplifier(graph_, /*stats=*/ nullptr);
   simplifier.Run();
 
   EXPECT_TRUE(shl1->GetBlock() == nullptr);
diff --git a/compiler/optimizing/instruction_simplifier_x86.cc b/compiler/optimizing/instruction_simplifier_x86.cc
index 9b5fb52509..3885f55475 100644
--- a/compiler/optimizing/instruction_simplifier_x86.cc
+++ b/compiler/optimizing/instruction_simplifier_x86.cc
@@ -14,8 +14,10 @@
  */
 
 #include "instruction_simplifier_x86.h"
-#include "instruction_simplifier_x86_shared.h"
+
 #include "code_generator_x86.h"
+#include "instruction_simplifier_x86_shared.h"
+#include "nodes.h"
 
 namespace art HIDDEN {
 
@@ -39,10 +41,10 @@ class InstructionSimplifierX86Visitor final : public HGraphVisitor {
   }
 
   void VisitBasicBlock(HBasicBlock* block) override {
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       HInstruction* instruction = it.Current();
       if (instruction->IsInBlock()) {
-        instruction->Accept(this);
+        Dispatch(instruction);
       }
     }
   }
diff --git a/compiler/optimizing/instruction_simplifier_x86.h b/compiler/optimizing/instruction_simplifier_x86.h
index 25ebe203b8..52492324dc 100644
--- a/compiler/optimizing/instruction_simplifier_x86.h
+++ b/compiler/optimizing/instruction_simplifier_x86.h
@@ -17,7 +17,6 @@
 #define ART_COMPILER_OPTIMIZING_INSTRUCTION_SIMPLIFIER_X86_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
 
 namespace art HIDDEN {
diff --git a/compiler/optimizing/instruction_simplifier_x86_64.cc b/compiler/optimizing/instruction_simplifier_x86_64.cc
index 9ba1a8a960..816649037a 100644
--- a/compiler/optimizing/instruction_simplifier_x86_64.cc
+++ b/compiler/optimizing/instruction_simplifier_x86_64.cc
@@ -14,8 +14,10 @@
  */
 
 #include "instruction_simplifier_x86_64.h"
-#include "instruction_simplifier_x86_shared.h"
+
 #include "code_generator_x86_64.h"
+#include "instruction_simplifier_x86_shared.h"
+#include "nodes.h"
 
 namespace art HIDDEN {
 
@@ -39,10 +41,10 @@ class InstructionSimplifierX86_64Visitor final : public HGraphVisitor {
   }
 
   void VisitBasicBlock(HBasicBlock* block) override {
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       HInstruction* instruction = it.Current();
       if (instruction->IsInBlock()) {
-        instruction->Accept(this);
+        Dispatch(instruction);
       }
     }
   }
diff --git a/compiler/optimizing/instruction_simplifier_x86_64.h b/compiler/optimizing/instruction_simplifier_x86_64.h
index 1654dc4774..fadacaa740 100644
--- a/compiler/optimizing/instruction_simplifier_x86_64.h
+++ b/compiler/optimizing/instruction_simplifier_x86_64.h
@@ -17,7 +17,6 @@
 #define ART_COMPILER_OPTIMIZING_INSTRUCTION_SIMPLIFIER_X86_64_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
 
 namespace art HIDDEN {
diff --git a/compiler/optimizing/intrinsics.cc b/compiler/optimizing/intrinsics.cc
index edd454c93e..398de61e58 100644
--- a/compiler/optimizing/intrinsics.cc
+++ b/compiler/optimizing/intrinsics.cc
@@ -102,7 +102,7 @@ void IntrinsicVisitor::ComputeValueOfLocations(HInvoke* invoke,
   }
 
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
-  LocationSummary* locations = new (allocator) LocationSummary(invoke, call_kind, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(allocator, invoke, call_kind, kIntrinsified);
   if (call_kind == LocationSummary::kCallOnMainOnly) {
     locations->SetInAt(0, Location::RegisterOrConstant(input));
     locations->AddTemp(first_argument_location);
@@ -187,7 +187,7 @@ void IntrinsicVisitor::CreateReferenceGetReferentLocations(HInvoke* invoke,
 
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister());
 }
@@ -200,7 +200,7 @@ void IntrinsicVisitor::CreateReferenceRefersToLocations(HInvoke* invoke, CodeGen
 
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister());
diff --git a/compiler/optimizing/intrinsics.h b/compiler/optimizing/intrinsics.h
index 4f164e10c6..d9770ad109 100644
--- a/compiler/optimizing/intrinsics.h
+++ b/compiler/optimizing/intrinsics.h
@@ -21,7 +21,6 @@
 #include "code_generator.h"
 #include "intrinsics_list.h"
 #include "nodes.h"
-#include "optimization.h"
 #include "parallel_move_resolver.h"
 
 namespace art HIDDEN {
diff --git a/compiler/optimizing/intrinsics_arm64.cc b/compiler/optimizing/intrinsics_arm64.cc
index 4eb73019fd..f6390f11c3 100644
--- a/compiler/optimizing/intrinsics_arm64.cc
+++ b/compiler/optimizing/intrinsics_arm64.cc
@@ -220,15 +220,13 @@ bool IntrinsicLocationsBuilderARM64::TryDispatch(HInvoke* invoke) {
 #define __ masm->
 
 static void CreateFPToIntLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::RequiresRegister());
 }
 
 static void CreateIntToFPLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresFpuRegister());
 }
@@ -276,15 +274,13 @@ void IntrinsicCodeGeneratorARM64::VisitFloatIntBitsToFloat(HInvoke* invoke) {
 }
 
 static void CreateIntToIntLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
 
 static void CreateIntIntToIntLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
@@ -292,7 +288,7 @@ static void CreateIntIntToIntLocations(ArenaAllocator* allocator, HInvoke* invok
 
 static void CreateIntIntToIntSlowPathCallLocations(ArenaAllocator* allocator, HInvoke* invoke) {
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   // Force kOutputOverlap; see comments in IntrinsicSlowPath::EmitNativeCode.
@@ -541,8 +537,7 @@ void IntrinsicCodeGeneratorARM64::VisitLongLowestOneBit(HInvoke* invoke) {
 }
 
 static void CreateFPToFPLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
 }
@@ -588,8 +583,7 @@ void IntrinsicCodeGeneratorARM64::VisitMathRint(HInvoke* invoke) {
 }
 
 static void CreateFPToIntPlusFPTempLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::RequiresRegister());
   locations->AddTemp(Location::RequiresFpuRegister());
@@ -689,8 +683,7 @@ void IntrinsicCodeGeneratorARM64::VisitMemoryPeekShortNative(HInvoke* invoke) {
 }
 
 static void CreateIntIntToVoidLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
 }
@@ -736,8 +729,7 @@ void IntrinsicCodeGeneratorARM64::VisitMemoryPokeShortNative(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderARM64::VisitThreadCurrentThread(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -839,12 +831,11 @@ static void CreateUnsafeGetLocations(ArenaAllocator* allocator,
                                      CodeGeneratorARM64* codegen,
                                      bool is_volatile = false) {
   bool can_call = codegen->EmitReadBarrier() && IsUnsafeGetReference(invoke);
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke,
-                                      can_call
-                                          ? LocationSummary::kCallOnSlowPath
-                                          : LocationSummary::kNoCall,
-                                      kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
+      invoke,
+      can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
+      kIntrinsified);
   if (can_call && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
     if (ReadBarrierNeedsTemp(is_volatile, invoke)) {
@@ -862,8 +853,7 @@ static void CreateUnsafeGetLocations(ArenaAllocator* allocator,
 
 static void CreateUnsafeGetAbsoluteLocations(ArenaAllocator* allocator,
                                              HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
@@ -987,8 +977,7 @@ void IntrinsicCodeGeneratorARM64::VisitJdkUnsafeGetByte(HInvoke* invoke) {
 }
 
 static void CreateUnsafePutLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   static constexpr int kOffsetIndex = 2;
   static constexpr int kValueIndex = 3;
   // Unused receiver.
@@ -1007,8 +996,7 @@ static void CreateUnsafePutLocations(ArenaAllocator* allocator, HInvoke* invoke)
 }
 
 static void CreateUnsafePutAbsoluteLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   static constexpr int kAddressIndex = 1;
   static constexpr int kValueIndex = 2;
   // Unused receiver.
@@ -1306,12 +1294,11 @@ static void CreateUnsafeCASLocations(ArenaAllocator* allocator,
                                      HInvoke* invoke,
                                      CodeGeneratorARM64* codegen) {
   const bool can_call = codegen->EmitReadBarrier() && IsUnsafeCASReference(invoke);
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke,
-                                      can_call
-                                          ? LocationSummary::kCallOnSlowPath
-                                          : LocationSummary::kNoCall,
-                                      kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
+      invoke,
+      can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
+      kIntrinsified);
   if (can_call && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -1885,12 +1872,11 @@ static void CreateUnsafeGetAndUpdateLocations(ArenaAllocator* allocator,
                                               HInvoke* invoke,
                                               CodeGeneratorARM64* codegen) {
   const bool can_call = codegen->EmitReadBarrier() && IsUnsafeGetAndSetReference(invoke);
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke,
-                                      can_call
-                                          ? LocationSummary::kCallOnSlowPath
-                                          : LocationSummary::kNoCall,
-                                      kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
+      invoke,
+      can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
+      kIntrinsified);
   if (can_call && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -2029,12 +2015,12 @@ void IntrinsicCodeGeneratorARM64::VisitJdkUnsafeGetAndSetReference(HInvoke* invo
 }
 
 void IntrinsicLocationsBuilderARM64::VisitStringCompareTo(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke,
-                                       invoke->InputAt(1)->CanBeNull()
-                                           ? LocationSummary::kCallOnSlowPath
-                                           : LocationSummary::kNoCall,
-                                       kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
+      invoke,
+      invoke->InputAt(1)->CanBeNull() ? LocationSummary::kCallOnSlowPath
+                                      : LocationSummary::kNoCall,
+      kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   locations->AddRegisterTemps(3);
@@ -2259,8 +2245,7 @@ static const char* GetConstString(HInstruction* candidate, uint32_t* utf16_lengt
 }
 
 void IntrinsicLocationsBuilderARM64::VisitStringEquals(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
 
@@ -2493,8 +2478,8 @@ static void GenerateVisitStringIndexOf(HInvoke* invoke,
 }
 
 void IntrinsicLocationsBuilderARM64::VisitStringIndexOf(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   // We have a hand-crafted assembly stub that follows the runtime calling convention. So it's
   // best to align the inputs accordingly.
   InvokeRuntimeCallingConvention calling_convention;
@@ -2511,8 +2496,8 @@ void IntrinsicCodeGeneratorARM64::VisitStringIndexOf(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderARM64::VisitStringIndexOfAfter(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   // We have a hand-crafted assembly stub that follows the runtime calling convention. So it's
   // best to align the inputs accordingly.
   InvokeRuntimeCallingConvention calling_convention;
@@ -2527,8 +2512,8 @@ void IntrinsicCodeGeneratorARM64::VisitStringIndexOfAfter(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderARM64::VisitStringNewStringFromBytes(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, LocationFrom(calling_convention.GetRegisterAt(0)));
   locations->SetInAt(1, LocationFrom(calling_convention.GetRegisterAt(1)));
@@ -2555,7 +2540,7 @@ void IntrinsicCodeGeneratorARM64::VisitStringNewStringFromBytes(HInvoke* invoke)
 
 void IntrinsicLocationsBuilderARM64::VisitStringNewStringFromChars(HInvoke* invoke) {
   LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+      LocationSummary::Create(allocator_, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, LocationFrom(calling_convention.GetRegisterAt(0)));
   locations->SetInAt(1, LocationFrom(calling_convention.GetRegisterAt(1)));
@@ -2575,8 +2560,8 @@ void IntrinsicCodeGeneratorARM64::VisitStringNewStringFromChars(HInvoke* invoke)
 }
 
 void IntrinsicLocationsBuilderARM64::VisitStringNewStringFromString(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, LocationFrom(calling_convention.GetRegisterAt(0)));
   locations->SetOut(calling_convention.GetReturnLocation(DataType::Type::kReference));
@@ -2603,8 +2588,8 @@ static void CreateFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invoke
   DCHECK(DataType::IsFloatingPointType(invoke->InputAt(0)->GetType()));
   DCHECK(DataType::IsFloatingPointType(invoke->GetType()));
 
-  LocationSummary* const locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
 
   locations->SetInAt(0, LocationFrom(calling_convention.GetFpuRegisterAt(0)));
@@ -2617,8 +2602,8 @@ static void CreateFPFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invo
   DCHECK(DataType::IsFloatingPointType(invoke->InputAt(1)->GetType()));
   DCHECK(DataType::IsFloatingPointType(invoke->GetType()));
 
-  LocationSummary* const locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
 
   locations->SetInAt(0, LocationFrom(calling_convention.GetFpuRegisterAt(0)));
@@ -2633,8 +2618,7 @@ static void CreateFPFPFPToFPLocations(ArenaAllocator* allocator, HInvoke* invoke
   DCHECK(DataType::IsFloatingPointType(invoke->InputAt(2)->GetType()));
   DCHECK(DataType::IsFloatingPointType(invoke->GetType()));
 
-  LocationSummary* const locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
 
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetInAt(1, Location::RequiresFpuRegister());
@@ -2793,8 +2777,7 @@ void IntrinsicCodeGeneratorARM64::VisitMathNextAfter(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderARM64::VisitStringGetCharsNoCheck(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetInAt(2, Location::RequiresRegister());
@@ -2965,7 +2948,7 @@ void IntrinsicLocationsBuilderARM64::VisitSystemArrayCopyChar(HInvoke* invoke) {
   }
 
   LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+      LocationSummary::Create(allocator_, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   // arraycopy(char[] src, int src_pos, char[] dst, int dst_pos, int length).
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, LocationForSystemArrayCopyInput(invoke->InputAt(1)));
@@ -3840,8 +3823,7 @@ void IntrinsicCodeGeneratorARM64::VisitReferenceRefersTo(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderARM64::VisitThreadInterrupted(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -3861,8 +3843,7 @@ void IntrinsicCodeGeneratorARM64::VisitThreadInterrupted(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderARM64::VisitReachabilityFence(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::Any());
 }
 
@@ -3873,9 +3854,7 @@ void IntrinsicLocationsBuilderARM64::VisitCRC32Update(HInvoke* invoke) {
     return;
   }
 
-  LocationSummary* locations = new (allocator_) LocationSummary(invoke,
-                                                                LocationSummary::kNoCall,
-                                                                kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
 
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
@@ -4020,9 +3999,7 @@ void IntrinsicLocationsBuilderARM64::VisitCRC32UpdateBytes(HInvoke* invoke) {
   }
 
   LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke,
-                                       LocationSummary::kCallOnSlowPath,
-                                       kIntrinsified);
+      LocationSummary::Create(allocator_, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
 
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
@@ -4075,10 +4052,7 @@ void IntrinsicLocationsBuilderARM64::VisitCRC32UpdateByteBuffer(HInvoke* invoke)
     return;
   }
 
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke,
-                                       LocationSummary::kNoCall,
-                                       kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
 
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
@@ -4118,9 +4092,7 @@ void IntrinsicLocationsBuilderARM64::VisitFP16ToFloat(HInvoke* invoke) {
     return;
   }
 
-  LocationSummary* locations = new (allocator_) LocationSummary(invoke,
-                                                                LocationSummary::kNoCall,
-                                                                kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresFpuRegister());
 }
@@ -4141,9 +4113,7 @@ void IntrinsicLocationsBuilderARM64::VisitFP16ToHalf(HInvoke* invoke) {
     return;
   }
 
-  LocationSummary* locations = new (allocator_) LocationSummary(invoke,
-                                                                LocationSummary::kNoCall,
-                                                                kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::RequiresRegister());
 }
@@ -4929,7 +4899,7 @@ static LocationSummary* CreateVarHandleCommonLocations(HInvoke* invoke,
 
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   // Require coordinates in registers. These are the object holding the value
   // to operate on (except for static fields) and index (for arrays and views).
@@ -5978,8 +5948,8 @@ void VarHandleSlowPathARM64::EmitByteArrayViewCode(CodeGenerator* codegen_in) {
 }
 
 void IntrinsicLocationsBuilderARM64::VisitMethodHandleInvokeExact(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_)
-      LocationSummary(invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
 
   InvokeDexCallingConventionVisitorARM64 calling_convention;
   locations->SetOut(calling_convention.GetReturnLocation(invoke->GetType()));
diff --git a/compiler/optimizing/intrinsics_arm_vixl.cc b/compiler/optimizing/intrinsics_arm_vixl.cc
index 9e60090a03..63bfb16e2b 100644
--- a/compiler/optimizing/intrinsics_arm_vixl.cc
+++ b/compiler/optimizing/intrinsics_arm_vixl.cc
@@ -163,15 +163,13 @@ bool IntrinsicLocationsBuilderARMVIXL::TryDispatch(HInvoke* invoke) {
 }
 
 static void CreateFPToIntLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::RequiresRegister());
 }
 
 static void CreateIntToFPLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresFpuRegister());
 }
@@ -225,15 +223,14 @@ void IntrinsicCodeGeneratorARMVIXL::VisitFloatIntBitsToFloat(HInvoke* invoke) {
 }
 
 static void CreateIntToIntLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
 
 static void CreateIntIntToIntSlowPathCallLocations(ArenaAllocator* allocator, HInvoke* invoke) {
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   // Force kOutputOverlap; see comments in IntrinsicSlowPath::EmitNativeCode.
@@ -241,15 +238,13 @@ static void CreateIntIntToIntSlowPathCallLocations(ArenaAllocator* allocator, HI
 }
 
 static void CreateLongToLongLocationsWithOverlap(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kOutputOverlap);
 }
 
 static void CreateFPToFPLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
 }
@@ -366,8 +361,7 @@ void IntrinsicCodeGeneratorARMVIXL::VisitMathRint(HInvoke* invoke) {
 
 void IntrinsicLocationsBuilderARMVIXL::VisitMathRoundFloat(HInvoke* invoke) {
   if (features_.HasARMv8AInstructions()) {
-    LocationSummary* locations =
-        new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+    LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
     locations->SetInAt(0, Location::RequiresFpuRegister());
     locations->SetOut(Location::RequiresRegister());
     locations->AddTemp(Location::RequiresFpuRegister());
@@ -467,8 +461,7 @@ void IntrinsicCodeGeneratorARMVIXL::VisitMemoryPeekShortNative(HInvoke* invoke)
 }
 
 static void CreateIntIntToVoidLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
 }
@@ -516,7 +509,7 @@ void IntrinsicCodeGeneratorARMVIXL::VisitMemoryPokeShortNative(HInvoke* invoke)
 
 void IntrinsicLocationsBuilderARMVIXL::VisitThreadCurrentThread(HInvoke* invoke) {
   LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+      LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -528,12 +521,12 @@ void IntrinsicCodeGeneratorARMVIXL::VisitThreadCurrentThread(HInvoke* invoke) {
 
 void IntrinsicLocationsBuilderARMVIXL::VisitStringCompareTo(HInvoke* invoke) {
   // The inputs plus one temp.
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke,
-                                       invoke->InputAt(1)->CanBeNull()
-                                           ? LocationSummary::kCallOnSlowPath
-                                           : LocationSummary::kNoCall,
-                                       kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
+      invoke,
+      invoke->InputAt(1)->CanBeNull() ? LocationSummary::kCallOnSlowPath
+                                      : LocationSummary::kNoCall,
+      kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   locations->AddRegisterTemps(3);
@@ -843,8 +836,7 @@ static const char* GetConstString(HInstruction* candidate, uint32_t* utf16_lengt
 }
 
 void IntrinsicLocationsBuilderARMVIXL::VisitStringEquals(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   InvokeRuntimeCallingConventionARMVIXL calling_convention;
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
@@ -1090,8 +1082,8 @@ static void GenerateVisitStringIndexOf(HInvoke* invoke,
 }
 
 void IntrinsicLocationsBuilderARMVIXL::VisitStringIndexOf(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   // We have a hand-crafted assembly stub that follows the runtime calling convention. So it's
   // best to align the inputs accordingly.
   InvokeRuntimeCallingConventionARMVIXL calling_convention;
@@ -1108,8 +1100,8 @@ void IntrinsicCodeGeneratorARMVIXL::VisitStringIndexOf(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderARMVIXL::VisitStringIndexOfAfter(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   // We have a hand-crafted assembly stub that follows the runtime calling convention. So it's
   // best to align the inputs accordingly.
   InvokeRuntimeCallingConventionARMVIXL calling_convention;
@@ -1124,8 +1116,8 @@ void IntrinsicCodeGeneratorARMVIXL::VisitStringIndexOfAfter(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderARMVIXL::VisitStringNewStringFromBytes(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   InvokeRuntimeCallingConventionARMVIXL calling_convention;
   locations->SetInAt(0, LocationFrom(calling_convention.GetRegisterAt(0)));
   locations->SetInAt(1, LocationFrom(calling_convention.GetRegisterAt(1)));
@@ -1150,7 +1142,7 @@ void IntrinsicCodeGeneratorARMVIXL::VisitStringNewStringFromBytes(HInvoke* invok
 
 void IntrinsicLocationsBuilderARMVIXL::VisitStringNewStringFromChars(HInvoke* invoke) {
   LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+      LocationSummary::Create(allocator_, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   InvokeRuntimeCallingConventionARMVIXL calling_convention;
   locations->SetInAt(0, LocationFrom(calling_convention.GetRegisterAt(0)));
   locations->SetInAt(1, LocationFrom(calling_convention.GetRegisterAt(1)));
@@ -1170,8 +1162,8 @@ void IntrinsicCodeGeneratorARMVIXL::VisitStringNewStringFromChars(HInvoke* invok
 }
 
 void IntrinsicLocationsBuilderARMVIXL::VisitStringNewStringFromString(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   InvokeRuntimeCallingConventionARMVIXL calling_convention;
   locations->SetInAt(0, LocationFrom(calling_convention.GetRegisterAt(0)));
   locations->SetOut(LocationFrom(r0));
@@ -1614,7 +1606,7 @@ static void CreateFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invoke
   DCHECK_EQ(invoke->GetType(), DataType::Type::kFloat64);
 
   LocationSummary* const locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   const InvokeRuntimeCallingConventionARMVIXL calling_convention;
 
   locations->SetInAt(0, Location::RequiresFpuRegister());
@@ -1640,7 +1632,7 @@ static void CreateFPFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invo
   DCHECK_EQ(invoke->GetType(), DataType::Type::kFloat64);
 
   LocationSummary* const locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   const InvokeRuntimeCallingConventionARMVIXL calling_convention;
 
   locations->SetInAt(0, Location::RequiresFpuRegister());
@@ -2110,8 +2102,7 @@ void IntrinsicCodeGeneratorARMVIXL::VisitLongLowestOneBit(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderARMVIXL::VisitStringGetCharsNoCheck(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetInAt(2, Location::RequiresRegister());
@@ -2507,8 +2498,7 @@ void IntrinsicCodeGeneratorARMVIXL::VisitReferenceRefersTo(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderARMVIXL::VisitThreadInterrupted(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -2532,8 +2522,7 @@ void IntrinsicCodeGeneratorARMVIXL::VisitThreadInterrupted(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderARMVIXL::VisitReachabilityFence(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::Any());
 }
 
@@ -2691,12 +2680,11 @@ static void CreateUnsafeGetLocations(HInvoke* invoke,
                                      bool atomic) {
   bool can_call = codegen->EmitReadBarrier() && IsUnsafeGetReference(invoke);
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke,
-                                      can_call
-                                          ? LocationSummary::kCallOnSlowPath
-                                          : LocationSummary::kNoCall,
-                                      kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
+      invoke,
+      can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
+      kIntrinsified);
   if (can_call && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -2716,10 +2704,7 @@ static void CreateUnsafeGetLocations(HInvoke* invoke,
 
 static void CreateUnsafeGetAbsoluteLocations(HInvoke* invoke) {
   ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke,
-                                      LocationSummary::kNoCall,
-                                      kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
@@ -3102,8 +3087,7 @@ static void CreateUnsafePutLocations(HInvoke* invoke,
                                      DataType::Type type,
                                      bool atomic) {
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetInAt(2, Location::RequiresRegister());
@@ -3112,12 +3096,11 @@ static void CreateUnsafePutLocations(HInvoke* invoke,
 }
 
 static void CreateUnsafePutAbsoluteLocations(HInvoke* invoke,
-                                     CodeGeneratorARMVIXL* codegen,
-                                     DataType::Type type,
-                                     bool atomic) {
+                                             CodeGeneratorARMVIXL* codegen,
+                                             DataType::Type type,
+                                             bool atomic) {
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetInAt(2, Location::RequiresRegister());
@@ -3753,12 +3736,11 @@ class ReadBarrierCasSlowPathARMVIXL : public SlowPathCodeARMVIXL {
 static void CreateUnsafeCASLocations(HInvoke* invoke, CodeGeneratorARMVIXL* codegen) {
   const bool can_call = codegen->EmitReadBarrier() && IsUnsafeCASReference(invoke);
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke,
-                                      can_call
-                                          ? LocationSummary::kCallOnSlowPath
-                                          : LocationSummary::kNoCall,
-                                      kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
+      invoke,
+      can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
+      kIntrinsified);
   if (can_call && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -4047,12 +4029,11 @@ static void CreateUnsafeGetAndUpdateLocations(HInvoke* invoke,
                                               GetAndUpdateOp get_and_update_op) {
   const bool can_call = codegen->EmitReadBarrier() && IsUnsafeGetAndSetReference(invoke);
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke,
-                                      can_call
-                                          ? LocationSummary::kCallOnSlowPath
-                                          : LocationSummary::kNoCall,
-                                      kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
+      invoke,
+      can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
+      kIntrinsified);
   if (can_call && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -4655,7 +4636,7 @@ static LocationSummary* CreateVarHandleCommonLocations(HInvoke* invoke,
 
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   // Require coordinates in registers. These are the object holding the value
   // to operate on (except for static fields) and index (for arrays and views).
diff --git a/compiler/optimizing/intrinsics_riscv64.cc b/compiler/optimizing/intrinsics_riscv64.cc
index 9379c8d3f6..48add6981d 100644
--- a/compiler/optimizing/intrinsics_riscv64.cc
+++ b/compiler/optimizing/intrinsics_riscv64.cc
@@ -19,6 +19,7 @@
 #include "code_generator_riscv64.h"
 #include "intrinsic_objects.h"
 #include "intrinsics_utils.h"
+#include "mirror/class.h"
 #include "optimizing/locations.h"
 #include "well_known_classes.h"
 
@@ -136,15 +137,13 @@ Riscv64Assembler* IntrinsicCodeGeneratorRISCV64::GetAssembler() {
 }
 
 static void CreateFPToIntLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::RequiresRegister());
 }
 
 static void CreateIntToFPLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresFpuRegister());
 }
@@ -154,8 +153,8 @@ static void CreateFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invoke
   DCHECK(DataType::IsFloatingPointType(invoke->InputAt(0)->GetType()));
   DCHECK(DataType::IsFloatingPointType(invoke->GetType()));
 
-  LocationSummary* const locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
 
   locations->SetInAt(0, Location::FpuRegisterLocation(calling_convention.GetFpuRegisterAt(0)));
@@ -168,8 +167,8 @@ static void CreateFPFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invo
   DCHECK(DataType::IsFloatingPointType(invoke->InputAt(1)->GetType()));
   DCHECK(DataType::IsFloatingPointType(invoke->GetType()));
 
-  LocationSummary* const locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
 
   locations->SetInAt(0, Location::FpuRegisterLocation(calling_convention.GetFpuRegisterAt(0)));
@@ -184,8 +183,7 @@ static void CreateFpFpFpToFpNoOverlapLocations(ArenaAllocator* allocator, HInvok
   DCHECK(DataType::IsFloatingPointType(invoke->InputAt(2)->GetType()));
   DCHECK(DataType::IsFloatingPointType(invoke->GetType()));
 
-  LocationSummary* const locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
 
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetInAt(1, Location::RequiresFpuRegister());
@@ -196,8 +194,7 @@ static void CreateFpFpFpToFpNoOverlapLocations(ArenaAllocator* allocator, HInvok
 static void CreateFPToFPLocations(ArenaAllocator* allocator,
                                   HInvoke* invoke,
                                   Location::OutputOverlap overlaps = Location::kOutputOverlap) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::RequiresFpuRegister(), overlaps);
 }
@@ -269,8 +266,7 @@ void IntrinsicCodeGeneratorRISCV64::VisitFloatIsInfinite(HInvoke* invoke) {
 }
 
 static void CreateIntToIntNoOverlapLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
@@ -318,15 +314,14 @@ void IntrinsicCodeGeneratorRISCV64::VisitMemoryPeekShortNative(HInvoke* invoke)
 }
 
 static void CreateIntIntToVoidLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
 }
 
 static void CreateIntIntToIntSlowPathCallLocations(ArenaAllocator* allocator, HInvoke* invoke) {
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   // Force kOutputOverlap; see comments in IntrinsicSlowPath::EmitNativeCode.
@@ -954,8 +949,8 @@ static void GenerateVisitStringIndexOf(HInvoke* invoke,
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitStringIndexOf(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   // We have a hand-crafted assembly stub that follows the runtime calling convention. So it's
   // best to align the inputs accordingly.
   InvokeRuntimeCallingConvention calling_convention;
@@ -972,8 +967,8 @@ void IntrinsicCodeGeneratorRISCV64::VisitStringIndexOf(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitStringIndexOfAfter(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   // We have a hand-crafted assembly stub that follows the runtime calling convention. So it's
   // best to align the inputs accordingly.
   InvokeRuntimeCallingConvention calling_convention;
@@ -988,8 +983,8 @@ void IntrinsicCodeGeneratorRISCV64::VisitStringIndexOfAfter(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitStringNewStringFromBytes(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
   locations->SetInAt(1, Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
@@ -1015,7 +1010,7 @@ void IntrinsicCodeGeneratorRISCV64::VisitStringNewStringFromBytes(HInvoke* invok
 
 void IntrinsicLocationsBuilderRISCV64::VisitStringNewStringFromChars(HInvoke* invoke) {
   LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+      LocationSummary::Create(allocator_, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
   locations->SetInAt(1, Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
@@ -1035,8 +1030,8 @@ void IntrinsicCodeGeneratorRISCV64::VisitStringNewStringFromChars(HInvoke* invok
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitStringNewStringFromString(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
   locations->SetOut(calling_convention.GetReturnLocation(DataType::Type::kReference));
@@ -1132,8 +1127,7 @@ static void EmitLoadReserved(Riscv64Assembler* assembler,
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitStringEquals(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   locations->AddTemp(Location::RequiresRegister());
@@ -1989,7 +1983,7 @@ static void CreateSystemArrayCopyLocations(HInvoke* invoke, DataType::Type type)
 
   ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   // arraycopy(char[] src, int src_pos, char[] dst, int dst_pos, int length).
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, LocationForSystemArrayCopyInput(invoke->InputAt(1)));
@@ -2351,7 +2345,8 @@ static void CreateUnsafeGetLocations(ArenaAllocator* allocator,
                                      HInvoke* invoke,
                                      CodeGeneratorRISCV64* codegen) {
   bool can_call = codegen->EmitReadBarrier() && IsUnsafeGetReference(invoke);
-  LocationSummary* locations = new (allocator) LocationSummary(
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
       invoke,
       can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
       kIntrinsified);
@@ -2367,8 +2362,7 @@ static void CreateUnsafeGetLocations(ArenaAllocator* allocator,
 
 static void CreateUnsafeGetAbsoluteLocations(ArenaAllocator* allocator,
                                              HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
@@ -2606,8 +2600,7 @@ void IntrinsicCodeGeneratorRISCV64::VisitJdkUnsafeGetByte(HInvoke* invoke) {
 }
 
 static void CreateUnsafePutLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetInAt(2, Location::RequiresRegister());
@@ -2618,8 +2611,7 @@ static void CreateUnsafePutLocations(ArenaAllocator* allocator, HInvoke* invoke)
 }
 
 static void CreateUnsafePutAbsoluteLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetInAt(2, Location::RequiresRegister());
@@ -2868,7 +2860,8 @@ static void CreateUnsafeCASLocations(ArenaAllocator* allocator,
                                      HInvoke* invoke,
                                      CodeGeneratorRISCV64* codegen) {
   const bool can_call = codegen->EmitReadBarrier() && IsUnsafeCASReference(invoke);
-  LocationSummary* locations = new (allocator) LocationSummary(
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
       invoke,
       can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
       kIntrinsified);
@@ -3064,7 +3057,8 @@ static void CreateUnsafeGetAndUpdateLocations(ArenaAllocator* allocator,
                                               HInvoke* invoke,
                                               CodeGeneratorRISCV64* codegen) {
   const bool can_call = codegen->EmitReadBarrier() && IsUnsafeGetAndSetReference(invoke);
-  LocationSummary* locations = new (allocator) LocationSummary(
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
       invoke,
       can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
       kIntrinsified);
@@ -3236,12 +3230,12 @@ void IntrinsicCodeGeneratorRISCV64::VisitJdkUnsafeGetAndSetReference(HInvoke* in
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitStringCompareTo(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke,
-                                       invoke->InputAt(1)->CanBeNull()
-                                           ? LocationSummary::kCallOnSlowPath
-                                           : LocationSummary::kNoCall,
-                                       kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_,
+      invoke,
+      invoke->InputAt(1)->CanBeNull() ? LocationSummary::kCallOnSlowPath
+                                      : LocationSummary::kNoCall,
+      kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   locations->AddRegisterTemps(3);
@@ -3860,7 +3854,7 @@ static LocationSummary* CreateVarHandleCommonLocations(HInvoke* invoke,
 
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   // Require coordinates in registers. These are the object holding the value
   // to operate on (except for static fields) and index (for arrays and views).
@@ -5148,8 +5142,7 @@ void VarHandleSlowPathRISCV64::EmitByteArrayViewCode(CodeGenerator* codegen_in)
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitThreadCurrentThread(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -5160,8 +5153,7 @@ void IntrinsicCodeGeneratorRISCV64::VisitThreadCurrentThread(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitThreadInterrupted(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -5180,8 +5172,7 @@ void IntrinsicCodeGeneratorRISCV64::VisitThreadInterrupted(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitReachabilityFence(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::Any());
 }
 
@@ -5482,8 +5473,7 @@ void IntrinsicCodeGeneratorRISCV64::VisitMathRoundFloat(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathMultiplyHigh(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
@@ -5503,8 +5493,7 @@ void IntrinsicCodeGeneratorRISCV64::VisitMathMultiplyHigh(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitStringGetCharsNoCheck(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
 
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
@@ -5714,8 +5703,7 @@ void GenMathSignum(CodeGeneratorRISCV64* codegen, HInvoke* invoke, DataType::Typ
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathSignumDouble(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::SameAsFirstInput());
 }
@@ -5725,8 +5713,7 @@ void IntrinsicCodeGeneratorRISCV64::VisitMathSignumDouble(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathSignumFloat(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::SameAsFirstInput());
 }
@@ -5766,13 +5753,11 @@ void IntrinsicCodeGeneratorRISCV64::VisitMathCopySignFloat(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMethodHandleInvokeExact(HInvoke* invoke) {
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
-  LocationSummary* locations = new (allocator)
-      LocationSummary(invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
 
   InvokeDexCallingConventionVisitorRISCV64 calling_convention;
   locations->SetOut(calling_convention.GetReturnLocation(invoke->GetType()));
-  locations->SetInAt(0, Location::RequiresRegister());
 
   // Accomodating LocationSummary for underlying invoke-* call.
   uint32_t number_of_args = invoke->GetNumberOfArguments();
@@ -5780,21 +5765,40 @@ void IntrinsicLocationsBuilderRISCV64::VisitMethodHandleInvokeExact(HInvoke* inv
     locations->SetInAt(i, calling_convention.GetNextLocation(invoke->InputAt(i)->GetType()));
   }
 
+  // Passing MethodHandle object as the last parameter: accessors implementation rely on it.
+  DCHECK_EQ(invoke->InputAt(0)->GetType(), DataType::Type::kReference);
+  Location receiver_mh_loc = calling_convention.GetNextLocation(DataType::Type::kReference);
+  locations->SetInAt(0, receiver_mh_loc);
+
   // The last input is MethodType object corresponding to the call-site.
   locations->SetInAt(number_of_args, Location::RequiresRegister());
 
   locations->AddTemp(calling_convention.GetMethodLocation());
   locations->AddRegisterTemps(2);
+
+  if (!receiver_mh_loc.IsRegister()) {
+    locations->AddTemp(Location::RequiresRegister());
+  }
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMethodHandleInvokeExact(HInvoke* invoke) {
   LocationSummary* locations = invoke->GetLocations();
-  XRegister method_handle = locations->InAt(0).AsRegister<XRegister>();
+  Riscv64Assembler* assembler = GetAssembler();
+
+  Location receiver_mh_loc = locations->InAt(0);
+  XRegister method_handle = receiver_mh_loc.IsRegister()
+      ? locations->InAt(0).AsRegister<XRegister>()
+      : locations->GetTemp(3).AsRegister<XRegister>();
+
+  if (!receiver_mh_loc.IsRegister()) {
+    DCHECK(receiver_mh_loc.IsStackSlot());
+    __ Loadwu(method_handle, SP, receiver_mh_loc.GetStackIndex());
+  }
+
   SlowPathCodeRISCV64* slow_path =
       new (codegen_->GetScopedAllocator()) InvokePolymorphicSlowPathRISCV64(invoke, method_handle);
 
   codegen_->AddSlowPath(slow_path);
-  Riscv64Assembler* assembler = GetAssembler();
   XRegister call_site_type =
       locations->InAt(invoke->GetNumberOfArguments()).AsRegister<XRegister>();
 
@@ -5808,10 +5812,18 @@ void IntrinsicCodeGeneratorRISCV64::VisitMethodHandleInvokeExact(HInvoke* invoke
   __ Loadd(method, method_handle, mirror::MethodHandle::ArtFieldOrMethodOffset().Int32Value());
 
   Riscv64Label execute_target_method;
+  Riscv64Label method_dispatch;
 
   XRegister method_handle_kind = locations->GetTemp(2).AsRegister<XRegister>();
-  __ Loadd(method_handle_kind,
-           method_handle, mirror::MethodHandle::HandleKindOffset().Int32Value());
+  __ Loadwu(method_handle_kind,
+            method_handle, mirror::MethodHandle::HandleKindOffset().Int32Value());
+
+  __ Li(temp, mirror::MethodHandle::Kind::kFirstAccessorKind);
+  __ Blt(method_handle_kind, temp, &method_dispatch);
+  __ Loadd(method, method_handle, mirror::MethodHandleImpl::TargetOffset().SizeValue());
+  __ J(&execute_target_method);
+
+  __ Bind(&method_dispatch);
   __ Li(temp, mirror::MethodHandle::Kind::kInvokeStatic);
   __ Beq(method_handle_kind, temp, &execute_target_method);
 
@@ -5830,14 +5842,14 @@ void IntrinsicCodeGeneratorRISCV64::VisitMethodHandleInvokeExact(HInvoke* invoke
     __ Bne(method_handle_kind, temp, &non_virtual_dispatch);
 
     // Skip virtual dispatch if `method` is private.
-    __ Loadd(temp, method, ArtMethod::AccessFlagsOffset().Int32Value());
+    __ Loadwu(temp, method, ArtMethod::AccessFlagsOffset().Int32Value());
     __ Andi(temp, temp, kAccPrivate);
     __ Bnez(temp, &execute_target_method);
 
     XRegister receiver_class = locations->GetTemp(2).AsRegister<XRegister>();
     // If method is defined in the receiver's class, execute it as it is.
-    __ Loadd(temp, method, ArtMethod::DeclaringClassOffset().Int32Value());
-    __ Loadd(receiver_class, receiver, mirror::Object::ClassOffset().Int32Value());
+    __ Loadwu(temp, method, ArtMethod::DeclaringClassOffset().Int32Value());
+    __ Loadwu(receiver_class, receiver, mirror::Object::ClassOffset().Int32Value());
     codegen_->MaybeUnpoisonHeapReference(receiver_class);
 
     // We're not emitting the read barrier for the receiver_class, so false negatives just go
@@ -5852,12 +5864,58 @@ void IntrinsicCodeGeneratorRISCV64::VisitMethodHandleInvokeExact(HInvoke* invoke
     __ Sh3Add(temp, temp, receiver_class);
     __ Loadd(method, temp, vtable_offset);
     __ J(&execute_target_method);
+
     __ Bind(&non_virtual_dispatch);
-  }
+    __ Li(temp, mirror::MethodHandle::Kind::kInvokeInterface);
+    __ Bne(method_handle_kind, temp, slow_path->GetEntryLabel());
+
+    // Skip virtual dispatch if `method` is private.
+    // Re-use method_handle_kind to store access flags.
+    XRegister access_flags = locations->GetTemp(2).AsRegister<XRegister>();
+    __ Loadwu(access_flags, method, ArtMethod::AccessFlagsOffset().Int32Value());
+    __ Andi(temp, access_flags, kAccPrivate);
+    __ Bnez(temp, &execute_target_method);
 
-  // Checks above are jumping to `execute_target_method` is they succeed. If none match, try to
-  // handle in the slow path.
-  __ J(slow_path->GetEntryLabel());
+    // The register T0 is required to be used for the hidden argument in
+    // art_quick_imt_conflict_trampoline. So prevent the assembler from using it.
+    ScratchRegisterScope srs(assembler);
+    srs.ExcludeXRegister(T0);
+
+    // Set the hidden argument.
+    __ Mv(T0, method);
+
+    Riscv64Label get_imt_index_from_method_index;
+    Riscv64Label do_imt_dispatch;
+
+    // Get IMT index.
+    // Not doing default conflict check as IMT index is set for all method which have
+    // kAccAbstract bit.
+    __ Andi(temp, access_flags, kAccAbstract);
+    __ Beqz(temp, &get_imt_index_from_method_index);
+
+    // imt_index is uint16_t
+    __ Loadhu(temp, method, ArtMethod::ImtIndexOffset().Int32Value());
+    __ J(&do_imt_dispatch);
+
+    // Default method, do method->GetMethodIndex() & (ImTable::kSizeTruncToPowerOfTwo - 1);
+    __ Bind(&get_imt_index_from_method_index);
+    __ Loadhu(temp, method, ArtMethod::MethodIndexOffset().Int32Value());
+    __ Andi(temp, temp, ImTable::kSizeTruncToPowerOfTwo - 1);
+
+    __ Bind(&do_imt_dispatch);
+    // Re-using `method` to store receiver class and ImTableEntry.
+    __ Loadd(method, receiver, mirror::Object::ClassOffset().Int32Value());
+    codegen_->MaybeUnpoisonHeapReference(method);
+
+    __ Loadd(method, method, mirror::Class::ImtPtrOffset(PointerSize::k64).Int32Value());
+    __ Sh3Add(temp, temp, method);
+    __ Loadd(method, temp, 0);
+
+    __ J(&execute_target_method);
+  } else {
+    // Not invoke-static and the first argument is not a reference type.
+    __ J(slow_path->GetEntryLabel());
+  }
 
   __ Bind(&execute_target_method);
   Offset entry_point = ArtMethod::EntryPointFromQuickCompiledCodeOffset(kRiscv64PointerSize);
diff --git a/compiler/optimizing/intrinsics_x86.cc b/compiler/optimizing/intrinsics_x86.cc
index 5710ce42bb..3753867a07 100644
--- a/compiler/optimizing/intrinsics_x86.cc
+++ b/compiler/optimizing/intrinsics_x86.cc
@@ -149,8 +149,7 @@ class ReadBarrierSystemArrayCopySlowPathX86 : public SlowPathCode {
 };
 
 static void CreateFPToIntLocations(ArenaAllocator* allocator, HInvoke* invoke, bool is64bit) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::RequiresRegister());
   if (is64bit) {
@@ -159,8 +158,7 @@ static void CreateFPToIntLocations(ArenaAllocator* allocator, HInvoke* invoke, b
 }
 
 static void CreateIntToFPLocations(ArenaAllocator* allocator, HInvoke* invoke, bool is64bit) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresFpuRegister());
   if (is64bit) {
@@ -229,22 +227,19 @@ void IntrinsicCodeGeneratorX86::VisitFloatIntBitsToFloat(HInvoke* invoke) {
 }
 
 static void CreateIntToIntLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::SameAsFirstInput());
 }
 
 static void CreateLongToIntLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister());
 }
 
 static void CreateLongToLongLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kOutputOverlap);
 }
@@ -307,8 +302,7 @@ void IntrinsicCodeGeneratorX86::VisitShortReverseBytes(HInvoke* invoke) {
 }
 
 static void CreateFPToFPLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
 }
@@ -375,8 +369,7 @@ void IntrinsicLocationsBuilderX86::VisitMathRoundFloat(HInvoke* invoke) {
   }
 
   HInvokeStaticOrDirect* static_or_direct = invoke->AsInvokeStaticOrDirect();
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   if (static_or_direct->HasSpecialInput() &&
       invoke->InputAt(
@@ -446,7 +439,7 @@ void IntrinsicCodeGeneratorX86::VisitMathRoundFloat(HInvoke* invoke) {
 
 static void CreateFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invoke) {
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::FpuRegisterLocation(calling_convention.GetFpuRegisterAt(0)));
   locations->SetOut(Location::FpuRegisterLocation(XMM0));
@@ -483,8 +476,7 @@ static void GenFPToFPCall(HInvoke* invoke, CodeGeneratorX86* codegen, QuickEntry
 }
 
 static void CreateLowestOneBitLocations(ArenaAllocator* allocator, bool is_long, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   if (is_long) {
     locations->SetInAt(0, Location::RequiresRegister());
   } else {
@@ -701,7 +693,7 @@ void IntrinsicCodeGeneratorX86::VisitLongLowestOneBit(HInvoke* invoke) {
 
 static void CreateFPFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invoke) {
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::FpuRegisterLocation(calling_convention.GetFpuRegisterAt(0)));
   locations->SetInAt(1, Location::FpuRegisterLocation(calling_convention.GetFpuRegisterAt(1)));
@@ -710,8 +702,7 @@ static void CreateFPFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invo
 
 static void CreateFPFPFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invoke) {
   DCHECK_EQ(invoke->GetNumberOfArguments(), 3U);
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetInAt(1, Location::RequiresFpuRegister());
@@ -786,9 +777,9 @@ static void CreateSystemArrayCopyLocations(HInvoke* invoke) {
   }
 
   // Okay, it is safe to generate inline code.
+  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
   LocationSummary* locations =
-      new (invoke->GetBlock()->GetGraph()->GetAllocator())
-      LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   // arraycopy(Object src, int srcPos, Object dest, int destPos, int length).
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RegisterOrConstant(invoke->InputAt(1)));
@@ -991,8 +982,8 @@ void IntrinsicLocationsBuilderX86::VisitSystemArrayCopyInt(HInvoke* invoke) {
 
 void IntrinsicLocationsBuilderX86::VisitStringCompareTo(HInvoke* invoke) {
   // The inputs plus one temp.
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
   locations->SetInAt(1, Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
@@ -1017,8 +1008,7 @@ void IntrinsicCodeGeneratorX86::VisitStringCompareTo(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86::VisitStringEquals(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
 
@@ -1136,9 +1126,8 @@ void IntrinsicCodeGeneratorX86::VisitStringEquals(HInvoke* invoke) {
 static void CreateStringIndexOfLocations(HInvoke* invoke,
                                          ArenaAllocator* allocator,
                                          bool start_at_zero) {
-  LocationSummary* locations = new (allocator) LocationSummary(invoke,
-                                                               LocationSummary::kCallOnSlowPath,
-                                                               kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   // The data needs to be in EDI for scasw. So request that the string is there, anyways.
   locations->SetInAt(0, Location::RegisterLocation(EDI));
   // If we look for a constant char, we'll still have to copy it into EAX. So just request the
@@ -1331,8 +1320,8 @@ void IntrinsicCodeGeneratorX86::VisitStringIndexOfAfter(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86::VisitStringNewStringFromBytes(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
   locations->SetInAt(1, Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
@@ -1358,7 +1347,7 @@ void IntrinsicCodeGeneratorX86::VisitStringNewStringFromBytes(HInvoke* invoke) {
 
 void IntrinsicLocationsBuilderX86::VisitStringNewStringFromChars(HInvoke* invoke) {
   LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+      LocationSummary::Create(allocator_, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
   locations->SetInAt(1, Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
@@ -1378,8 +1367,8 @@ void IntrinsicCodeGeneratorX86::VisitStringNewStringFromChars(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86::VisitStringNewStringFromString(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
   locations->SetOut(Location::RegisterLocation(EAX));
@@ -1402,8 +1391,7 @@ void IntrinsicCodeGeneratorX86::VisitStringNewStringFromString(HInvoke* invoke)
 
 void IntrinsicLocationsBuilderX86::VisitStringGetCharsNoCheck(HInvoke* invoke) {
   // public void getChars(int srcBegin, int srcEnd, char[] dst, int dstBegin);
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RegisterOrConstant(invoke->InputAt(1)));
   // Place srcEnd in ECX to save a move below.
@@ -1564,8 +1552,7 @@ void IntrinsicCodeGeneratorX86::VisitMemoryPeekShortNative(HInvoke* invoke) {
 static void CreateLongIntToVoidLocations(ArenaAllocator* allocator,
                                          DataType::Type size,
                                          HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   HInstruction* value = invoke->InputAt(1);
   if (size == DataType::Type::kInt8) {
@@ -1654,8 +1641,7 @@ void IntrinsicCodeGeneratorX86::VisitMemoryPokeShortNative(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86::VisitThreadCurrentThread(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -1782,8 +1768,7 @@ static void CreateIntIntToIntLocations(ArenaAllocator* allocator,
                                        HInvoke* invoke,
                                        DataType::Type type,
                                        bool is_volatile) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   if (type == DataType::Type::kInt64) {
@@ -1805,12 +1790,11 @@ static void CreateIntIntIntToIntLocations(ArenaAllocator* allocator,
                                           DataType::Type type,
                                           bool is_volatile) {
   bool can_call = codegen->EmitReadBarrier() && IsUnsafeGetReference(invoke);
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke,
-                                      can_call
-                                          ? LocationSummary::kCallOnSlowPath
-                                          : LocationSummary::kNoCall,
-                                      kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
+      invoke,
+      can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
+      kIntrinsified);
   if (can_call && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -1963,8 +1947,7 @@ static void CreateIntIntIntToVoidPlusTempsLocations(ArenaAllocator* allocator,
                                                     DataType::Type type,
                                                     HInvoke* invoke,
                                                     bool is_volatile) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   if (type == DataType::Type::kInt8 || type == DataType::Type::kUint8) {
@@ -1983,8 +1966,7 @@ static void CreateIntIntIntIntToVoidPlusTempsLocations(ArenaAllocator* allocator
                                                        DataType::Type type,
                                                        HInvoke* invoke,
                                                        bool is_volatile) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetInAt(2, Location::RequiresRegister());
@@ -2281,12 +2263,11 @@ static void CreateIntIntIntIntIntToInt(ArenaAllocator* allocator,
                                        DataType::Type type,
                                        HInvoke* invoke) {
   const bool can_call = codegen->EmitBakerReadBarrier() && IsUnsafeCASReference(invoke);
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke,
-                                      can_call
-                                          ? LocationSummary::kCallOnSlowPath
-                                          : LocationSummary::kNoCall,
-                                      kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
+      invoke,
+      can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
+      kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   // Offset is a long, but in 32 bit mode, we only need the low word.
@@ -2636,12 +2617,11 @@ void CreateUnsafeGetAndUpdateLocations(ArenaAllocator* allocator,
                                        DataType::Type type,
                                        GetAndUpdateOp get_and_unsafe_op) {
   const bool can_call = codegen->EmitReadBarrier() && IsUnsafeGetAndSetReference(invoke);
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke,
-                                      can_call
-                                          ? LocationSummary::kCallOnSlowPath
-                                          : LocationSummary::kNoCall,
-                                      kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
+      invoke,
+      can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
+      kIntrinsified);
   if (can_call && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -2876,8 +2856,7 @@ void IntrinsicCodeGeneratorX86::VisitJdkUnsafeGetAndSetReference(HInvoke* invoke
 }
 
 void IntrinsicLocationsBuilderX86::VisitIntegerReverse(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::SameAsFirstInput());
   locations->AddTemp(Location::RequiresRegister());
@@ -2918,8 +2897,7 @@ void IntrinsicCodeGeneratorX86::VisitIntegerReverse(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86::VisitLongReverse(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::SameAsFirstInput());
   locations->AddTemp(Location::RequiresRegister());
@@ -2960,8 +2938,7 @@ static void CreateBitCountLocations(
     // a call for the intrinsic rather than direct code.
     return;
   }
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   if (is_long) {
     locations->AddTemp(Location::RequiresRegister());
   }
@@ -3026,8 +3003,7 @@ void IntrinsicCodeGeneratorX86::VisitLongBitCount(HInvoke* invoke) {
 }
 
 static void CreateLeadingZeroLocations(ArenaAllocator* allocator, HInvoke* invoke, bool is_long) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   if (is_long) {
     locations->SetInAt(0, Location::RequiresRegister());
   } else {
@@ -3130,8 +3106,7 @@ void IntrinsicCodeGeneratorX86::VisitLongNumberOfLeadingZeros(HInvoke* invoke) {
 }
 
 static void CreateTrailingZeroLocations(ArenaAllocator* allocator, HInvoke* invoke, bool is_long) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   if (is_long) {
     locations->SetInAt(0, Location::RequiresRegister());
   } else {
@@ -3795,8 +3770,7 @@ void IntrinsicCodeGeneratorX86::VisitReferenceRefersTo(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86::VisitThreadInterrupted(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -3814,17 +3788,15 @@ void IntrinsicCodeGeneratorX86::VisitThreadInterrupted(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86::VisitReachabilityFence(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::Any());
 }
 
 void IntrinsicCodeGeneratorX86::VisitReachabilityFence([[maybe_unused]] HInvoke* invoke) {}
 
 void IntrinsicLocationsBuilderX86::VisitIntegerDivideUnsigned(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(invoke,
-                                                                LocationSummary::kCallOnSlowPath,
-                                                                kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator_, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RegisterLocation(EAX));
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetOut(Location::SameAsFirstInput());
@@ -4118,8 +4090,8 @@ static void CreateVarHandleGetLocations(HInvoke* invoke, CodeGeneratorX86* codeg
   }
 
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
-  LocationSummary* locations = new (allocator) LocationSummary(
-      invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   size_t expected_coordinates_count = GetExpectedVarHandleCoordinatesCount(invoke);
   if (expected_coordinates_count == 1u) {
@@ -4254,8 +4226,8 @@ static void CreateVarHandleSetLocations(HInvoke* invoke, CodeGeneratorX86* codeg
   }
 
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
-  LocationSummary* locations = new (allocator) LocationSummary(
-      invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   size_t expected_coordinates_count = GetExpectedVarHandleCoordinatesCount(invoke);
   if (expected_coordinates_count == 1u) {
@@ -4431,8 +4403,8 @@ static void CreateVarHandleGetAndSetLocations(HInvoke* invoke, CodeGeneratorX86*
   }
 
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
-  LocationSummary* locations = new (allocator) LocationSummary(
-      invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->AddRegisterTemps(2);
   // We use this temporary for the card, so we need a byte register
   locations->AddTemp(Location::RegisterLocation(EBX));
@@ -4631,8 +4603,8 @@ static void CreateVarHandleCompareAndSetOrExchangeLocations(HInvoke* invoke,
   }
 
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
-  LocationSummary* locations = new (allocator) LocationSummary(
-      invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->AddRegisterTemps(2);
   // We use this temporary for the card, so we need a byte register
   locations->AddTemp(Location::RegisterLocation(EBX));
@@ -4811,8 +4783,8 @@ static void CreateVarHandleGetAndAddLocations(HInvoke* invoke, CodeGeneratorX86*
   }
 
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
-  LocationSummary* locations = new (allocator) LocationSummary(
-      invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->AddRegisterTemps(2);
   locations->SetInAt(0, Location::RequiresRegister());
   size_t expected_coordinates_count = GetExpectedVarHandleCoordinatesCount(invoke);
@@ -4986,8 +4958,8 @@ static void CreateVarHandleGetAndBitwiseOpLocations(HInvoke* invoke, CodeGenerat
   }
 
   ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
-  LocationSummary* locations = new (allocator) LocationSummary(
-      invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   // We need a byte register temp to store the result of the bitwise operation
   locations->AddTemp(Location::RegisterLocation(EBX));
   locations->AddTemp(Location::RequiresRegister());
diff --git a/compiler/optimizing/intrinsics_x86_64.cc b/compiler/optimizing/intrinsics_x86_64.cc
index d3ee1759e0..22841647a9 100644
--- a/compiler/optimizing/intrinsics_x86_64.cc
+++ b/compiler/optimizing/intrinsics_x86_64.cc
@@ -181,15 +181,13 @@ class InvokePolymorphicSlowPathX86_64 : public SlowPathCode {
 };
 
 static void CreateFPToIntLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::RequiresRegister());
 }
 
 static void CreateIntToFPLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::RequiresFpuRegister());
 }
@@ -253,8 +251,7 @@ void IntrinsicCodeGeneratorX86_64::VisitFloatIntBitsToFloat(HInvoke* invoke) {
 }
 
 static void CreateIntToIntLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::SameAsFirstInput());
 }
@@ -345,8 +342,7 @@ void IntrinsicCodeGeneratorX86_64::VisitDoubleIsInfinite(HInvoke* invoke) {
 }
 
 static void CreateFPToFPLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
 }
@@ -414,8 +410,7 @@ static void CreateSSE41FPToIntLocations(ArenaAllocator* allocator,
     return;
   }
 
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetOut(Location::RequiresRegister());
   locations->AddTemp(Location::RequiresFpuRegister());
@@ -506,7 +501,7 @@ void IntrinsicCodeGeneratorX86_64::VisitMathRoundDouble(HInvoke* invoke) {
 
 static void CreateFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invoke) {
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::FpuRegisterLocation(calling_convention.GetFpuRegisterAt(0)));
   locations->SetOut(Location::FpuRegisterLocation(XMM0));
@@ -637,7 +632,7 @@ void IntrinsicCodeGeneratorX86_64::VisitMathTanh(HInvoke* invoke) {
 
 static void CreateFPFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invoke) {
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::FpuRegisterLocation(calling_convention.GetFpuRegisterAt(0)));
   locations->SetInAt(1, Location::FpuRegisterLocation(calling_convention.GetFpuRegisterAt(1)));
@@ -648,8 +643,7 @@ static void CreateFPFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invo
 
 static void CreateFPFPFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invoke) {
   DCHECK_EQ(invoke->GetNumberOfArguments(), 3U);
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RequiresFpuRegister());
   locations->SetInAt(1, Location::RequiresFpuRegister());
@@ -711,9 +705,9 @@ static void CreateSystemArrayCopyLocations(HInvoke* invoke) {
       return;
     }
   }
+  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
   LocationSummary* locations =
-      new (invoke->GetBlock()->GetGraph()->GetAllocator()) LocationSummary
-      (invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   // arraycopy(Object src, int src_pos, Object dest, int dest_pos, int length).
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RegisterOrConstant(invoke->InputAt(1)));
@@ -1219,8 +1213,8 @@ void IntrinsicCodeGeneratorX86_64::VisitSystemArrayCopy(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86_64::VisitStringCompareTo(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
   locations->SetInAt(1, Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
@@ -1245,8 +1239,7 @@ void IntrinsicCodeGeneratorX86_64::VisitStringCompareTo(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86_64::VisitStringEquals(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RequiresRegister());
 
@@ -1364,9 +1357,8 @@ void IntrinsicCodeGeneratorX86_64::VisitStringEquals(HInvoke* invoke) {
 static void CreateStringIndexOfLocations(HInvoke* invoke,
                                          ArenaAllocator* allocator,
                                          bool start_at_zero) {
-  LocationSummary* locations = new (allocator) LocationSummary(invoke,
-                                                               LocationSummary::kCallOnSlowPath,
-                                                               kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   // The data needs to be in RDI for scasw. So request that the string is there, anyways.
   locations->SetInAt(0, Location::RegisterLocation(RDI));
   // If we look for a constant char, we'll still have to copy it into RAX. So just request the
@@ -1545,8 +1537,8 @@ void IntrinsicCodeGeneratorX86_64::VisitStringIndexOfAfter(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86_64::VisitStringNewStringFromBytes(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
   locations->SetInAt(1, Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
@@ -1572,7 +1564,7 @@ void IntrinsicCodeGeneratorX86_64::VisitStringNewStringFromBytes(HInvoke* invoke
 
 void IntrinsicLocationsBuilderX86_64::VisitStringNewStringFromChars(HInvoke* invoke) {
   LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+      LocationSummary::Create(allocator_, invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
   locations->SetInAt(1, Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
@@ -1592,8 +1584,8 @@ void IntrinsicCodeGeneratorX86_64::VisitStringNewStringFromChars(HInvoke* invoke
 }
 
 void IntrinsicLocationsBuilderX86_64::VisitStringNewStringFromString(HInvoke* invoke) {
-  LocationSummary* locations = new (allocator_) LocationSummary(
-      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
   InvokeRuntimeCallingConvention calling_convention;
   locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
   locations->SetOut(Location::RegisterLocation(RAX));
@@ -1616,8 +1608,7 @@ void IntrinsicCodeGeneratorX86_64::VisitStringNewStringFromString(HInvoke* invok
 
 void IntrinsicLocationsBuilderX86_64::VisitStringGetCharsNoCheck(HInvoke* invoke) {
   // public void getChars(int srcBegin, int srcEnd, char[] dst, int dstBegin);
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RegisterOrConstant(invoke->InputAt(1)));
   locations->SetInAt(2, Location::RequiresRegister());
@@ -1762,8 +1753,7 @@ void IntrinsicCodeGeneratorX86_64::VisitMemoryPeekShortNative(HInvoke* invoke) {
 }
 
 static void CreateIntIntToVoidLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, Location::RegisterOrInt32Constant(invoke->InputAt(1)));
 }
@@ -1847,8 +1837,7 @@ void IntrinsicCodeGeneratorX86_64::VisitMemoryPokeShortNative(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86_64::VisitThreadCurrentThread(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -1938,8 +1927,7 @@ static void GenUnsafeGetAbsolute(HInvoke* invoke,
 }
 
 static void CreateIntIntToIntLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
@@ -1949,12 +1937,11 @@ static void CreateIntIntIntToIntLocations(ArenaAllocator* allocator,
                                           HInvoke* invoke,
                                           CodeGeneratorX86_64* codegen) {
   bool can_call = codegen->EmitReadBarrier() && IsUnsafeGetReference(invoke);
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke,
-                                      can_call
-                                          ? LocationSummary::kCallOnSlowPath
-                                          : LocationSummary::kNoCall,
-                                      kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
+      invoke,
+      can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
+      kIntrinsified);
   if (can_call && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -2086,8 +2073,7 @@ void IntrinsicCodeGeneratorX86_64::VisitJdkUnsafeGetByte(HInvoke* invoke) {
 static void CreateIntIntIntToVoidPlusTempsLocations(ArenaAllocator* allocator,
                                                     [[maybe_unused]] DataType::Type type,
                                                     HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetInAt(2, Location::RequiresRegister());
@@ -2096,8 +2082,7 @@ static void CreateIntIntIntToVoidPlusTempsLocations(ArenaAllocator* allocator,
 static void CreateIntIntIntIntToVoidPlusTempsLocations(ArenaAllocator* allocator,
                                                        DataType::Type type,
                                                        HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetInAt(2, Location::RequiresRegister());
@@ -2335,12 +2320,11 @@ static void CreateUnsafeCASLocations(ArenaAllocator* allocator,
                                      CodeGeneratorX86_64* codegen,
                                      DataType::Type type) {
   const bool can_call = codegen->EmitBakerReadBarrier() && IsUnsafeCASReference(invoke);
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke,
-                                      can_call
-                                          ? LocationSummary::kCallOnSlowPath
-                                          : LocationSummary::kNoCall,
-                                      kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
+      invoke,
+      can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
+      kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetInAt(2, Location::RequiresRegister());
@@ -2735,12 +2719,11 @@ static void CreateUnsafeGetAndUpdateLocations(ArenaAllocator* allocator,
                                               HInvoke* invoke,
                                               CodeGeneratorX86_64* codegen) {
   const bool can_call = codegen->EmitReadBarrier() && IsUnsafeGetAndSetReference(invoke);
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke,
-                                      can_call
-                                          ? LocationSummary::kCallOnSlowPath
-                                          : LocationSummary::kNoCall,
-                                      kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator,
+      invoke,
+      can_call ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall,
+      kIntrinsified);
   if (can_call && kUseBakerReadBarrier) {
     locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
   }
@@ -2933,8 +2916,7 @@ void IntrinsicCodeGeneratorX86_64::VisitJdkUnsafeGetAndSetReference(HInvoke* inv
 }
 
 void IntrinsicLocationsBuilderX86_64::VisitIntegerReverse(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::SameAsFirstInput());
   locations->AddTemp(Location::RequiresRegister());
@@ -2975,8 +2957,7 @@ void IntrinsicCodeGeneratorX86_64::VisitIntegerReverse(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86_64::VisitLongReverse(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetOut(Location::SameAsFirstInput());
   locations->AddRegisterTemps(2);
@@ -3024,8 +3005,7 @@ static void CreateBitCountLocations(
     // a call for the intrinsic rather than direct code.
     return;
   }
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::Any());
   locations->SetOut(Location::RequiresRegister());
 }
@@ -3080,8 +3060,7 @@ void IntrinsicCodeGeneratorX86_64::VisitLongBitCount(HInvoke* invoke) {
 }
 
 static void CreateOneBitLocations(ArenaAllocator* allocator, HInvoke* invoke, bool is_high) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::Any());
   locations->SetOut(Location::RequiresRegister());
   locations->AddTemp(is_high ? Location::RegisterLocation(RCX)  // needs CL
@@ -3215,8 +3194,7 @@ void IntrinsicCodeGeneratorX86_64::VisitLongLowestOneBit(HInvoke* invoke) {
 }
 
 static void CreateLeadingZeroLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::Any());
   locations->SetOut(Location::RequiresRegister());
 }
@@ -3288,8 +3266,7 @@ void IntrinsicCodeGeneratorX86_64::VisitLongNumberOfLeadingZeros(HInvoke* invoke
 }
 
 static void CreateTrailingZeroLocations(ArenaAllocator* allocator, HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator, invoke, kIntrinsified);
   locations->SetInAt(0, Location::Any());
   locations->SetOut(Location::RequiresRegister());
 }
@@ -3567,8 +3544,7 @@ void IntrinsicCodeGeneratorX86_64::VisitReferenceRefersTo(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86_64::VisitThreadInterrupted(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetOut(Location::RequiresRegister());
 }
 
@@ -3587,8 +3563,7 @@ void IntrinsicCodeGeneratorX86_64::VisitThreadInterrupted(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86_64::VisitReachabilityFence(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::Any());
 }
 
@@ -3596,7 +3571,7 @@ void IntrinsicCodeGeneratorX86_64::VisitReachabilityFence([[maybe_unused]] HInvo
 
 static void CreateDivideUnsignedLocations(HInvoke* invoke, ArenaAllocator* allocator) {
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RegisterLocation(RAX));
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetOut(Location::SameAsFirstInput());
@@ -3655,8 +3630,7 @@ void IntrinsicCodeGeneratorX86_64::VisitLongDivideUnsigned(HInvoke* invoke) {
 }
 
 void IntrinsicLocationsBuilderX86_64::VisitMathMultiplyHigh(HInvoke* invoke) {
-  LocationSummary* locations =
-      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  LocationSummary* locations = LocationSummary::CreateNoCall(allocator_, invoke, kIntrinsified);
   locations->SetInAt(0, Location::RegisterLocation(RAX));
   locations->SetInAt(1, Location::RequiresRegister());
   locations->SetOut(Location::RegisterLocation(RDX));
@@ -4142,8 +4116,8 @@ static bool HasVarHandleIntrinsicImplementation(HInvoke* invoke, CodeGeneratorX8
 static LocationSummary* CreateVarHandleCommonLocations(HInvoke* invoke) {
   size_t expected_coordinates_count = GetExpectedVarHandleCoordinatesCount(invoke);
   ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
-  LocationSummary* locations = new (allocator) LocationSummary(
-      invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+  LocationSummary* locations =
+      LocationSummary::Create(allocator, invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
 
   locations->SetInAt(0, Location::RequiresRegister());
   // Require coordinates in registers. These are the object holding the value
@@ -4235,9 +4209,8 @@ static void GenerateVarHandleGet(HInvoke* invoke,
 }
 
 void IntrinsicLocationsBuilderX86_64::VisitMethodHandleInvokeExact(HInvoke* invoke) {
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
-  LocationSummary* locations = new (allocator)
-      LocationSummary(invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  LocationSummary* locations = LocationSummary::Create(
+      allocator_, invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
 
   InvokeDexCallingConventionVisitorX86_64 calling_convention;
   locations->SetOut(calling_convention.GetReturnLocation(invoke->GetType()));
diff --git a/compiler/optimizing/licm.cc b/compiler/optimizing/licm.cc
index 729a277ee0..d8b40aee3e 100644
--- a/compiler/optimizing/licm.cc
+++ b/compiler/optimizing/licm.cc
@@ -16,6 +16,7 @@
 
 #include "licm.h"
 
+#include "loop_information-inl.h"
 #include "side_effects_analysis.h"
 
 namespace art HIDDEN {
@@ -81,8 +82,16 @@ static void UpdateLoopPhisIn(ArenaAllocator* allocator,
 }
 
 bool LICM::Run() {
+  if (!graph_->HasLoops()) {
+    // Nothing to do.
+    return false;
+  }
+
   bool didLICM = false;
-  DCHECK(side_effects_.HasRun());
+  SideEffectsAnalysis side_effects(graph_);
+  side_effects.Run();
+
+  DCHECK(side_effects.HasRun());
 
   // Only used during debug.
   ArenaBitVector* visited = nullptr;
@@ -101,11 +110,10 @@ bool LICM::Run() {
     }
 
     HLoopInformation* loop_info = block->GetLoopInformation();
-    SideEffects loop_effects = side_effects_.GetLoopEffects(block);
+    SideEffects loop_effects = side_effects.GetLoopEffects(block);
     HBasicBlock* pre_header = loop_info->GetPreHeader();
 
-    for (HBlocksInLoopIterator it_loop(*loop_info); !it_loop.Done(); it_loop.Advance()) {
-      HBasicBlock* inner = it_loop.Current();
+    for (HBasicBlock* inner : loop_info->GetBlocks()) {
       DCHECK(inner->IsInLoop());
       if (inner->GetLoopInformation() != loop_info) {
         // Thanks to post order visit, inner loops were already visited.
@@ -128,7 +136,7 @@ bool LICM::Run() {
       // instruction that is not hoisted stops this optimization. Non-throwing instructions,
       // on the other hand, can still be hoisted.
       bool found_first_non_hoisted_visible_instruction_in_loop = !inner->IsLoopHeader();
-      for (HInstructionIterator inst_it(inner->GetInstructions());
+      for (HInstructionIteratorPrefetchNext inst_it(inner->GetInstructions());
            !inst_it.Done();
            inst_it.Advance()) {
         HInstruction* instruction = inst_it.Current();
diff --git a/compiler/optimizing/licm.h b/compiler/optimizing/licm.h
index 1a86b6eb9f..a06bfa9495 100644
--- a/compiler/optimizing/licm.h
+++ b/compiler/optimizing/licm.h
@@ -18,29 +18,22 @@
 #define ART_COMPILER_OPTIMIZING_LICM_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
 
 namespace art HIDDEN {
 
-class SideEffectsAnalysis;
-
 class LICM : public HOptimization {
  public:
   LICM(HGraph* graph,
-       const SideEffectsAnalysis& side_effects,
        OptimizingCompilerStats* stats,
        const char* name = kLoopInvariantCodeMotionPassName)
-      : HOptimization(graph, name, stats),
-        side_effects_(side_effects) {}
+      : HOptimization(graph, name, stats) {}
 
   bool Run() override;
 
   static constexpr const char* kLoopInvariantCodeMotionPassName = "licm";
 
  private:
-  const SideEffectsAnalysis& side_effects_;
-
   DISALLOW_COPY_AND_ASSIGN(LICM);
 };
 
diff --git a/compiler/optimizing/licm_test.cc b/compiler/optimizing/licm_test.cc
index 08a2dcc179..5ae5748ffd 100644
--- a/compiler/optimizing/licm_test.cc
+++ b/compiler/optimizing/licm_test.cc
@@ -59,9 +59,7 @@ class LICMTest : public OptimizingUnitTest {
   // Performs LICM optimizations (after proper set up).
   void PerformLICM() {
     graph_->BuildDominatorTree();
-    SideEffectsAnalysis side_effects(graph_);
-    side_effects.Run();
-    LICM(graph_, side_effects, nullptr).Run();
+    LICM(graph_, nullptr).Run();
   }
 
   // Specific basic blocks.
diff --git a/compiler/optimizing/linear_order.cc b/compiler/optimizing/linear_order.cc
index 25ca866b2c..c669b3fe4b 100644
--- a/compiler/optimizing/linear_order.cc
+++ b/compiler/optimizing/linear_order.cc
@@ -61,7 +61,7 @@ static bool IsLinearOrderWellFormed(const HGraph* graph, ArrayRef<HBasicBlock*>
       continue;
     }
     HLoopInformation* loop = header->GetLoopInformation();
-    size_t num_blocks = loop->GetBlocks().NumSetBits();
+    size_t num_blocks = loop->GetBlockMask().NumSetBits();
     size_t found_blocks = 0u;
     for (HBasicBlock* block : linear_order) {
       if (loop->Contains(*block)) {
diff --git a/compiler/optimizing/live_ranges_test.cc b/compiler/optimizing/live_ranges_test.cc
index fb1a23eef4..0f8208157d 100644
--- a/compiler/optimizing/live_ranges_test.cc
+++ b/compiler/optimizing/live_ranges_test.cc
@@ -30,6 +30,9 @@ namespace art HIDDEN {
 
 class LiveRangesTest : public CommonCompilerTest, public OptimizingUnitTestHelper {
  protected:
+  // Define a shortcut for the `kLivenessPositionsPerInstruction`.
+  static constexpr size_t kLppi = kLivenessPositionsPerInstruction;
+
   HGraph* BuildGraph(const std::vector<uint16_t>& data);
 
   std::unique_ptr<CompilerOptions> compiler_options_;
@@ -52,12 +55,13 @@ TEST_F(LiveRangesTest, CFG1) {
    *  return 0;
    *
    * Which becomes the following graph (numbered by lifetime position):
-   *       2: constant0
-   *       4: goto
+   *       1: constant0
+   *       2: goto
    *           |
-   *       8: return
+   *       4: return
    *           |
-   *       12: exit
+   *       6: exit
+   * (Above positions are multiplied by `kLivenessPositionsPerInstruction`, or `kLppi` for short.)
    */
   const std::vector<uint16_t> data = ONE_REGISTER_CODE_ITEM(
     Instruction::CONST_4 | 0 | 0,
@@ -71,12 +75,12 @@ TEST_F(LiveRangesTest, CFG1) {
 
   LiveInterval* interval = liveness.GetInstructionFromSsaIndex(0)->GetLiveInterval();
   LiveRange* range = interval->GetFirstRange();
-  ASSERT_EQ(2u, range->GetStart());
+  ASSERT_EQ(1u * kLppi, range->GetStart());
   // Last use is the return instruction.
-  ASSERT_EQ(8u, range->GetEnd());
+  ASSERT_EQ(4u * kLppi, range->GetEnd());
   HBasicBlock* block = graph->GetBlocks()[1];
   ASSERT_TRUE(block->GetLastInstruction()->IsReturn());
-  ASSERT_EQ(8u, block->GetLastInstruction()->GetLifetimePosition());
+  ASSERT_EQ(4u * kLppi, block->GetLastInstruction()->GetLifetimePosition());
   ASSERT_TRUE(range->GetNext() == nullptr);
 }
 
@@ -90,17 +94,18 @@ TEST_F(LiveRangesTest, CFG2) {
    *  return a;
    *
    * Which becomes the following graph (numbered by lifetime position):
-   *       2: constant0
-   *       4: goto
+   *       1: constant0
+   *       2: goto
    *           |
-   *       8: equal
-   *       10: if
+   *       4: equal
+   *       5: if
    *       /       \
-   *   14: goto   18: goto
+   *    7: goto   9: goto
    *       \       /
-   *       22: return
+   *       11: return
    *         |
-   *       26: exit
+   *       13: exit
+   * (Above positions are multiplied by `kLivenessPositionsPerInstruction`, or `kLppi` for short.)
    */
   const std::vector<uint16_t> data = ONE_REGISTER_CODE_ITEM(
     Instruction::CONST_4 | 0 | 0,
@@ -115,12 +120,12 @@ TEST_F(LiveRangesTest, CFG2) {
 
   LiveInterval* interval = liveness.GetInstructionFromSsaIndex(0)->GetLiveInterval();
   LiveRange* range = interval->GetFirstRange();
-  ASSERT_EQ(2u, range->GetStart());
+  ASSERT_EQ(1u * kLppi, range->GetStart());
   // Last use is the return instruction.
-  ASSERT_EQ(22u, range->GetEnd());
+  ASSERT_EQ(11u * kLppi, range->GetEnd());
   HBasicBlock* block = graph->GetBlocks()[3];
   ASSERT_TRUE(block->GetLastInstruction()->IsReturn());
-  ASSERT_EQ(22u, block->GetLastInstruction()->GetLifetimePosition());
+  ASSERT_EQ(11u * kLppi, block->GetLastInstruction()->GetLifetimePosition());
   ASSERT_TRUE(range->GetNext() == nullptr);
 }
 
@@ -135,19 +140,20 @@ TEST_F(LiveRangesTest, CFG3) {
    *  return a;
    *
    * Which becomes the following graph (numbered by lifetime position):
-   *       2: constant0
-   *       4: constant4
-   *       6: goto
+   *       1: constant0
+   *       2: constant4
+   *       3: goto
    *           |
-   *       10: equal
-   *       12: if
+   *       5: equal
+   *       6: if
    *       /       \
-   *   16: goto   20: goto
+   *    8: goto   10: goto
    *       \       /
-   *       22: phi
-   *       24: return
+   *       11: phi
+   *       12: return
    *         |
-   *       28: exit
+   *       14: exit
+   * (Above positions are multiplied by `kLivenessPositionsPerInstruction`, or `kLppi` for short.)
    */
   const std::vector<uint16_t> data = ONE_REGISTER_CODE_ITEM(
     Instruction::CONST_4 | 0 | 0,
@@ -163,10 +169,10 @@ TEST_F(LiveRangesTest, CFG3) {
   // Test for the 4 constant.
   LiveInterval* interval = liveness.GetInstructionFromSsaIndex(1)->GetLiveInterval();
   LiveRange* range = interval->GetFirstRange();
-  ASSERT_EQ(4u, range->GetStart());
+  ASSERT_EQ(2u * kLppi, range->GetStart());
   // Last use is the phi at the return block so instruction is live until
   // the end of the then block.
-  ASSERT_EQ(18u, range->GetEnd());
+  ASSERT_EQ(9u * kLppi, range->GetEnd());
   ASSERT_TRUE(range->GetNext() == nullptr);
 
   // Test for the 0 constant.
@@ -174,22 +180,22 @@ TEST_F(LiveRangesTest, CFG3) {
   // The then branch is a hole for this constant, therefore its interval has 2 ranges.
   // First range starts from the definition and ends at the if block.
   range = interval->GetFirstRange();
-  ASSERT_EQ(2u, range->GetStart());
+  ASSERT_EQ(1u * kLppi, range->GetStart());
   // 14 is the end of the if block.
-  ASSERT_EQ(14u, range->GetEnd());
+  ASSERT_EQ(7u * kLppi, range->GetEnd());
   // Second range is the else block.
   range = range->GetNext();
-  ASSERT_EQ(18u, range->GetStart());
+  ASSERT_EQ(9u * kLppi, range->GetStart());
   // Last use is the phi at the return block.
-  ASSERT_EQ(22u, range->GetEnd());
+  ASSERT_EQ(11u * kLppi, range->GetEnd());
   ASSERT_TRUE(range->GetNext() == nullptr);
 
   // Test for the phi.
   interval = liveness.GetInstructionFromSsaIndex(2)->GetLiveInterval();
   range = interval->GetFirstRange();
-  ASSERT_EQ(22u, liveness.GetInstructionFromSsaIndex(2)->GetLifetimePosition());
-  ASSERT_EQ(22u, range->GetStart());
-  ASSERT_EQ(24u, range->GetEnd());
+  ASSERT_EQ(11u * kLppi, liveness.GetInstructionFromSsaIndex(2)->GetLifetimePosition());
+  ASSERT_EQ(11u * kLppi, range->GetStart());
+  ASSERT_EQ(12u * kLppi, range->GetEnd());
   ASSERT_TRUE(range->GetNext() == nullptr);
 }
 
@@ -203,22 +209,23 @@ TEST_F(LiveRangesTest, Loop1) {
    *  return 5;
    *
    * Which becomes the following graph (numbered by lifetime position):
-   *       2: constant0
-   *       4: constant5
-   *       6: constant4
-   *       8: goto
+   *       1: constant0
+   *       2: constant5
+   *       3: constant4
+   *       4: goto
    *           |
-   *       12: goto
+   *       6: goto
    *           |
-   *       14: phi
-   *       16: equal
-   *       18: if +++++
+   *       7: phi
+   *       8: equal
+   *       9: if +++++
    *        |       \ +
-   *        |     22: goto
+   *        |     11: goto
    *        |
-   *       26: return
+   *       13: return
    *         |
-   *       30: exit
+   *       15: exit
+   * (Above positions are multiplied by `kLivenessPositionsPerInstruction`, or `kLppi` for short.)
    */
 
   const std::vector<uint16_t> data = TWO_REGISTERS_CODE_ITEM(
@@ -238,34 +245,34 @@ TEST_F(LiveRangesTest, Loop1) {
   // Test for the 0 constant.
   LiveInterval* interval = graph->GetIntConstant(0)->GetLiveInterval();
   LiveRange* range = interval->GetFirstRange();
-  ASSERT_EQ(2u, range->GetStart());
+  ASSERT_EQ(1u * kLppi, range->GetStart());
   // Last use is the loop phi so instruction is live until
   // the end of the pre loop header.
-  ASSERT_EQ(14u, range->GetEnd());
+  ASSERT_EQ(7u * kLppi, range->GetEnd());
   ASSERT_TRUE(range->GetNext() == nullptr);
 
   // Test for the 4 constant.
   interval = graph->GetIntConstant(4)->GetLiveInterval();
   range = interval->GetFirstRange();
   // The instruction is live until the end of the loop.
-  ASSERT_EQ(6u, range->GetStart());
-  ASSERT_EQ(24u, range->GetEnd());
+  ASSERT_EQ(3u * kLppi, range->GetStart());
+  ASSERT_EQ(12u * kLppi, range->GetEnd());
   ASSERT_TRUE(range->GetNext() == nullptr);
 
   // Test for the 5 constant.
   interval = graph->GetIntConstant(5)->GetLiveInterval();
   range = interval->GetFirstRange();
   // The instruction is live until the return instruction after the loop.
-  ASSERT_EQ(4u, range->GetStart());
-  ASSERT_EQ(26u, range->GetEnd());
+  ASSERT_EQ(2u * kLppi, range->GetStart());
+  ASSERT_EQ(13u * kLppi, range->GetEnd());
   ASSERT_TRUE(range->GetNext() == nullptr);
 
   // Test for the phi.
   interval = liveness.GetInstructionFromSsaIndex(3)->GetLiveInterval();
   range = interval->GetFirstRange();
   // Instruction is input of non-materialized Equal and hence live until If.
-  ASSERT_EQ(14u, range->GetStart());
-  ASSERT_EQ(19u, range->GetEnd());
+  ASSERT_EQ(7u * kLppi, range->GetStart());
+  ASSERT_EQ(9u * kLppi + kLivenessPositionOfNormalUse, range->GetEnd());
   ASSERT_TRUE(range->GetNext() == nullptr);
 }
 
@@ -279,23 +286,23 @@ TEST_F(LiveRangesTest, Loop2) {
    *  return a;
    *
    * Which becomes the following graph (numbered by lifetime position):
-   *       2: constant0
-   *       4: goto
+   *       1: constant0
+   *       2: goto
    *           |
-   *       8: goto
+   *       4: goto
    *           |
-   *       10: phi
-   *       12: equal
-   *       14: if +++++
+   *       5: phi
+   *       6: equal
+   *       7: if +++++
    *        |       \ +
-   *        |     18: add
-   *        |     20: goto
+   *        |     9: add
+   *        |     10: goto
    *        |
-   *       24: return
+   *       12: return
    *         |
-   *       28: exit
-   *
-   * We want to make sure the phi at 10 has a lifetime hole after the add at 20.
+   *       14: exit
+   * We want to make sure the phi at 5 has a lifetime hole after the add at 10.
+   * (Above positions are multiplied by `kLivenessPositionsPerInstruction`, or `kLppi` for short.)
    */
 
   const std::vector<uint16_t> data = ONE_REGISTER_CODE_ITEM(
@@ -314,29 +321,29 @@ TEST_F(LiveRangesTest, Loop2) {
   HIntConstant* constant = liveness.GetInstructionFromSsaIndex(0)->AsIntConstant();
   LiveInterval* interval = constant->GetLiveInterval();
   LiveRange* range = interval->GetFirstRange();
-  ASSERT_EQ(2u, range->GetStart());
+  ASSERT_EQ(1u * kLppi, range->GetStart());
   // Last use is the loop phi so instruction is live until
   // the end of the pre loop header.
-  ASSERT_EQ(10u, range->GetEnd());
+  ASSERT_EQ(5u * kLppi, range->GetEnd());
   ASSERT_TRUE(range->GetNext() == nullptr);
 
   // Test for the loop phi.
   HPhi* phi = liveness.GetInstructionFromSsaIndex(1)->AsPhi();
   interval = phi->GetLiveInterval();
   range = interval->GetFirstRange();
-  ASSERT_EQ(10u, range->GetStart());
-  ASSERT_EQ(19u, range->GetEnd());
+  ASSERT_EQ(5u * kLppi, range->GetStart());
+  ASSERT_EQ(9u * kLppi + kLivenessPositionOfNormalUse, range->GetEnd());
   range = range->GetNext();
   ASSERT_TRUE(range != nullptr);
-  ASSERT_EQ(22u, range->GetStart());
-  ASSERT_EQ(24u, range->GetEnd());
+  ASSERT_EQ(11u * kLppi, range->GetStart());
+  ASSERT_EQ(12u * kLppi, range->GetEnd());
 
   // Test for the add instruction.
   HAdd* add = liveness.GetInstructionFromSsaIndex(2)->AsAdd();
   interval = add->GetLiveInterval();
   range = interval->GetFirstRange();
-  ASSERT_EQ(18u, range->GetStart());
-  ASSERT_EQ(22u, range->GetEnd());
+  ASSERT_EQ(36u, range->GetStart());
+  ASSERT_EQ(44u, range->GetEnd());
   ASSERT_TRUE(range->GetNext() == nullptr);
 }
 
@@ -353,22 +360,22 @@ TEST_F(LiveRangesTest, CFG4) {
    *  return b;
    *
    * Which becomes the following graph (numbered by lifetime position):
-   *       2: constant0
-   *       4: constant4
-   *       6: goto
+   *       1: constant0
+   *       2: constant4
+   *       3: goto
    *           |
-   *       10: equal
-   *       12: if
+   *       5: equal
+   *       6: if
    *       /       \
-   *   16: add    22: add
-   *   18: goto   24: goto
+   *    8: add    11: add
+   *    9: goto   12: goto
    *       \       /
-   *       26: phi
-   *       28: return
+   *       13: phi
+   *       14: return
    *         |
-   *       32: exit
-   *
-   * We want to make sure the constant0 has a lifetime hole after the 16: add.
+   *       16: exit
+   * We want to make sure the constant0 has a lifetime hole after the 8: add.
+   * (Above positions are multiplied by `kLivenessPositionsPerInstruction`, or `kLppi` for short.)
    */
   const std::vector<uint16_t> data = TWO_REGISTERS_CODE_ITEM(
     Instruction::CONST_4 | 0 | 0,
@@ -387,46 +394,46 @@ TEST_F(LiveRangesTest, CFG4) {
   // Test for the 0 constant.
   LiveInterval* interval = liveness.GetInstructionFromSsaIndex(0)->GetLiveInterval();
   LiveRange* range = interval->GetFirstRange();
-  ASSERT_EQ(2u, range->GetStart());
-  ASSERT_EQ(17u, range->GetEnd());
+  ASSERT_EQ(1u * kLppi, range->GetStart());
+  ASSERT_EQ(8u * kLppi + kLivenessPositionOfNormalUse, range->GetEnd());
   range = range->GetNext();
   ASSERT_TRUE(range != nullptr);
-  ASSERT_EQ(20u, range->GetStart());
-  ASSERT_EQ(23u, range->GetEnd());
+  ASSERT_EQ(10u * kLppi, range->GetStart());
+  ASSERT_EQ(11u * kLppi + kLivenessPositionOfNormalUse, range->GetEnd());
   ASSERT_TRUE(range->GetNext() == nullptr);
 
   // Test for the 4 constant.
   interval = liveness.GetInstructionFromSsaIndex(1)->GetLiveInterval();
   range = interval->GetFirstRange();
-  ASSERT_EQ(4u, range->GetStart());
-  ASSERT_EQ(17u, range->GetEnd());
+  ASSERT_EQ(2u * kLppi, range->GetStart());
+  ASSERT_EQ(8u * kLppi + kLivenessPositionOfNormalUse, range->GetEnd());
   range = range->GetNext();
-  ASSERT_EQ(20u, range->GetStart());
-  ASSERT_EQ(23u, range->GetEnd());
+  ASSERT_EQ(10u * kLppi, range->GetStart());
+  ASSERT_EQ(11u * kLppi + kLivenessPositionOfNormalUse, range->GetEnd());
   ASSERT_TRUE(range->GetNext() == nullptr);
 
   // Test for the first add.
   HAdd* add = liveness.GetInstructionFromSsaIndex(2)->AsAdd();
   interval = add->GetLiveInterval();
   range = interval->GetFirstRange();
-  ASSERT_EQ(16u, range->GetStart());
-  ASSERT_EQ(20u, range->GetEnd());
+  ASSERT_EQ(8u * kLppi, range->GetStart());
+  ASSERT_EQ(10u * kLppi, range->GetEnd());
   ASSERT_TRUE(range->GetNext() == nullptr);
 
   // Test for the second add.
   add = liveness.GetInstructionFromSsaIndex(3)->AsAdd();
   interval = add->GetLiveInterval();
   range = interval->GetFirstRange();
-  ASSERT_EQ(22u, range->GetStart());
-  ASSERT_EQ(26u, range->GetEnd());
+  ASSERT_EQ(11u * kLppi, range->GetStart());
+  ASSERT_EQ(13u * kLppi, range->GetEnd());
   ASSERT_TRUE(range->GetNext() == nullptr);
 
   HPhi* phi = liveness.GetInstructionFromSsaIndex(4)->AsPhi();
   ASSERT_TRUE(phi->GetUses().HasExactlyOneElement());
   interval = phi->GetLiveInterval();
   range = interval->GetFirstRange();
-  ASSERT_EQ(26u, range->GetStart());
-  ASSERT_EQ(28u, range->GetEnd());
+  ASSERT_EQ(13u * kLppi, range->GetStart());
+  ASSERT_EQ(14u * kLppi, range->GetEnd());
   ASSERT_TRUE(range->GetNext() == nullptr);
 }
 
diff --git a/compiler/optimizing/load_store_analysis.h b/compiler/optimizing/load_store_analysis.h
index 65c35e8372..c8d54c2b17 100644
--- a/compiler/optimizing/load_store_analysis.h
+++ b/compiler/optimizing/load_store_analysis.h
@@ -39,12 +39,14 @@ class ReferenceInfo : public DeletableArenaObject<kArenaAllocLSA> {
         position_(pos),
         is_singleton_(true),
         is_singleton_and_not_returned_(true),
-        is_singleton_and_not_deopt_visible_(true) {
+        is_singleton_and_not_deopt_visible_(true),
+        is_singleton_and_not_read_by_invoke_(true) {
     CalculateEscape(reference_,
                     nullptr,
                     &is_singleton_,
                     &is_singleton_and_not_returned_,
-                    &is_singleton_and_not_deopt_visible_);
+                    &is_singleton_and_not_deopt_visible_,
+                    &is_singleton_and_not_read_by_invoke_);
   }
 
   HInstruction* GetReference() const {
@@ -62,18 +64,14 @@ class ReferenceInfo : public DeletableArenaObject<kArenaAllocLSA> {
     return is_singleton_;
   }
 
-  // Returns true if reference_ is a singleton and not returned to the caller or
-  // used as an environment local of an HDeoptimize instruction.
-  // The allocation and stores into reference_ may be eliminated for such cases.
   bool IsSingletonAndRemovable() const {
-    return is_singleton_and_not_returned_ && is_singleton_and_not_deopt_visible_;
+    return is_singleton_and_not_returned_ &&
+        is_singleton_and_not_deopt_visible_ &&
+        is_singleton_and_not_read_by_invoke_;
   }
 
-  // Returns true if reference_ is a singleton and returned to the caller or
-  // used as an environment local of an HDeoptimize instruction.
   bool IsSingletonAndNonRemovable() const {
-    return is_singleton_ &&
-           (!is_singleton_and_not_returned_ || !is_singleton_and_not_deopt_visible_);
+    return is_singleton_ && !IsSingletonAndRemovable();
   }
 
  private:
@@ -88,6 +86,8 @@ class ReferenceInfo : public DeletableArenaObject<kArenaAllocLSA> {
   bool is_singleton_and_not_returned_;
   // Is singleton and not used as an environment local of HDeoptimize.
   bool is_singleton_and_not_deopt_visible_;
+  // Is singleton and no invocation is reading it.
+  bool is_singleton_and_not_read_by_invoke_;
 
   DISALLOW_COPY_AND_ASSIGN(ReferenceInfo);
 };
@@ -458,7 +458,7 @@ class HeapLocationCollector final : public HGraphVisitor {
     }
   }
 
-  void VisitFieldAccess(HFieldAccess* instruction) override {
+  void HandleFieldAccess(HFieldAccess* instruction) {
     HInstruction* ref = instruction->InputAt(0);
     const FieldInfo& field_info = instruction->GetFieldInfo();
     DataType::Type type = field_info.GetFieldType();
@@ -489,22 +489,22 @@ class HeapLocationCollector final : public HGraphVisitor {
 
   void VisitInstanceFieldGet(HInstanceFieldGet* instruction) override {
     CreateReferenceInfoForReferenceType(instruction);
-    VisitFieldAccess(instruction);
+    HandleFieldAccess(instruction);
   }
 
   void VisitInstanceFieldSet(HInstanceFieldSet* instruction) override {
     has_heap_stores_ = true;
-    VisitFieldAccess(instruction);
+    HandleFieldAccess(instruction);
   }
 
   void VisitStaticFieldGet(HStaticFieldGet* instruction) override {
     CreateReferenceInfoForReferenceType(instruction);
-    VisitFieldAccess(instruction);
+    HandleFieldAccess(instruction);
   }
 
   void VisitStaticFieldSet(HStaticFieldSet* instruction) override {
     has_heap_stores_ = true;
-    VisitFieldAccess(instruction);
+    HandleFieldAccess(instruction);
   }
 
   // We intentionally don't collect HUnresolvedInstanceField/HUnresolvedStaticField accesses
diff --git a/compiler/optimizing/load_store_elimination.cc b/compiler/optimizing/load_store_elimination.cc
index 799c1dcbf0..2d761afb9c 100644
--- a/compiler/optimizing/load_store_elimination.cc
+++ b/compiler/optimizing/load_store_elimination.cc
@@ -1588,9 +1588,11 @@ void LSEVisitor::MergePredecessorRecords(HBasicBlock* block) {
   for (size_t idx = 0u; idx != num_heap_locations; ++idx) {
     Value merged_value = MergePredecessorValues(block, idx);
     if (kIsDebugBuild) {
-      if (merged_value.NeedsPhi()) {
+      if (merged_value.NeedsNonLoopPhi() || merged_value.NeedsPlainLoopPhi()) {
         uint32_t block_id = merged_value.GetPhiPlaceholder().GetBlockId();
         CHECK(GetGraph()->GetBlocks()[block_id]->Dominates(block));
+      } else if (merged_value.NeedsConvertedLoopPhi()) {
+        CHECK(merged_value.GetLoopPhiConversionLoad()->GetBlock()->Dominates(block));
       } else if (merged_value.IsInstruction()) {
         CHECK(merged_value.GetInstruction()->GetBlock()->Dominates(block));
       }
@@ -1616,7 +1618,8 @@ static HInstruction* FindOrConstructNonLoopPhi(
     HBasicBlock* block,
     const ScopedArenaVector<HInstruction*>& phi_inputs,
     DataType::Type type) {
-  for (HInstructionIterator phi_it(block->GetPhis()); !phi_it.Done(); phi_it.Advance()) {
+  for (HInstructionIteratorPrefetchNext phi_it(block->GetPhis()); !phi_it.Done();
+       phi_it.Advance()) {
     HInstruction* phi = phi_it.Current();
     DCHECK_EQ(phi->InputCount(), phi_inputs.size());
     auto cmp = [](HInstruction* lhs, const HUserRecord<HInstruction*>& rhs) {
@@ -2239,7 +2242,8 @@ bool LSEVisitor::MaterializeLoopPhis(ArrayRef<const size_t> phi_placeholder_inde
     size_t idx = phi_placeholder.GetHeapLocation();
     HBasicBlock* block = GetGraph()->GetBlocks()[phi_placeholder.GetBlockId()];
     ArrayRef<HBasicBlock* const> predecessors(block->GetPredecessors());
-    for (HInstructionIterator phi_it(block->GetPhis()); !phi_it.Done(); phi_it.Advance()) {
+    for (HInstructionIteratorPrefetchNext phi_it(block->GetPhis()); !phi_it.Done();
+         phi_it.Advance()) {
       HInstruction* phi = phi_it.Current();
       DCHECK_EQ(phi->InputCount(), predecessors.size());
       ArrayRef<HUserRecord<HInstruction*>> phi_inputs = phi->GetInputRecords();
@@ -2749,7 +2753,8 @@ void LSEVisitor::UpdateValueRecordForStoreElimination(/*inout*/ValueRecord* valu
     DCHECK(store_record != nullptr);
     *value_record = store_record->old_value_record;
   }
-  if (value_record->stored_by.NeedsPhi() &&
+  DCHECK(!value_record->stored_by.NeedsConvertedLoopPhi());
+  if ((value_record->stored_by.NeedsPlainLoopPhi() || value_record->stored_by.NeedsNonLoopPhi()) &&
       !phi_placeholders_to_search_for_kept_stores_.IsBitSet(
            PhiPlaceholderIndex(value_record->stored_by))) {
     // Some stores feeding this heap location may have been eliminated. Use the `stored_by`
@@ -2757,7 +2762,10 @@ void LSEVisitor::UpdateValueRecordForStoreElimination(/*inout*/ValueRecord* valu
     value_record->value = value_record->stored_by;
   }
   value_record->value = ReplacementOrValue(value_record->value);
-  if (value_record->value.NeedsNonLoopPhi()) {
+  if (value_record->value.NeedsConvertedLoopPhi()) {
+    // The Phi placeholder was unreplaceable. The load must be used as is if the value is needed.
+    value_record->value = Value::ForInstruction(value_record->value.GetLoopPhiConversionLoad());
+  } else if (value_record->value.NeedsNonLoopPhi()) {
     // Treat all Phi placeholders as requiring loop Phis at this point.
     // We do not want MaterializeLoopPhis() to call MaterializeNonLoopPhis().
     value_record->value =
@@ -2835,7 +2843,7 @@ void LSEVisitor::FindStoresWritingOldValues() {
     StoreRecord* store_record = store_records_[store_id];
     DCHECK(store_record != nullptr);
     UpdateValueRecordForStoreElimination(&store_record->old_value_record);
-    if (store_record->old_value_record.value.NeedsPhi()) {
+    if (store_record->old_value_record.value.NeedsPlainLoopPhi()) {
       DataType::Type type = store_record->stored_value->GetType();
       FindOldValueForPhiPlaceholder(store_record->old_value_record.value.GetPhiPlaceholder(), type);
       store_record->old_value_record.value =
diff --git a/compiler/optimizing/load_store_elimination.h b/compiler/optimizing/load_store_elimination.h
index e77168547d..60164e1fde 100644
--- a/compiler/optimizing/load_store_elimination.h
+++ b/compiler/optimizing/load_store_elimination.h
@@ -22,8 +22,6 @@
 
 namespace art HIDDEN {
 
-class SideEffectsAnalysis;
-
 class LoadStoreElimination : public HOptimization {
  public:
   // Controls whether to enable VLOG(compiler) logs explaining the transforms taking place.
diff --git a/compiler/optimizing/load_store_elimination_test.cc b/compiler/optimizing/load_store_elimination_test.cc
index af039229af..788c631d80 100644
--- a/compiler/optimizing/load_store_elimination_test.cc
+++ b/compiler/optimizing/load_store_elimination_test.cc
@@ -1324,6 +1324,8 @@ class TwoTypesConversionsTestGroup : public LoadStoreEliminationTestBase<
     // a `HInstanceFieldGet` after constructing it.
     return (load_type == DataType::Type::kUint8) ? DataType::Type::kInt8 : load_type;
   }
+
+  void MergingTwiceConvertedValueStoreTest(bool extra_diamond);
 };
 
 TEST_P(TwoTypesConversionsTestGroup, StoreLoad) {
@@ -1601,7 +1603,7 @@ TEST_P(TwoTypesConversionsTestGroup, MergingConvertedValueStore) {
   }
 }
 
-TEST_P(TwoTypesConversionsTestGroup, MergingTwiceConvertedValueStore) {
+void TwoTypesConversionsTestGroup::MergingTwiceConvertedValueStoreTest(bool extra_diamond) {
   auto [load_type1, load_type2] = GetParam();
   DataType::Type field_type1 = FieldTypeForLoadType(load_type1);
   DataType::Type field_type2 = FieldTypeForLoadType(load_type2);
@@ -1611,6 +1613,13 @@ TEST_P(TwoTypesConversionsTestGroup, MergingTwiceConvertedValueStore) {
 
   HBasicBlock* return_block = InitEntryMainExitGraph();
   auto [pre_header, loop_header, loop_body] = CreateForLoopWithInstructions(return_block);
+  if (extra_diamond) {
+    // Out-of-date debug check in `MergePredecessorRecords()` used to crash when the merged
+    // values from all incoming paths needed the same phi placeholder with a conversion load.
+    // Create an extra diamond to check such merging. b/405552185
+    HInstruction* bool_param = MakeParam(DataType::Type::kBool);
+    std::tie(loop_body, std::ignore, std::ignore) = CreateDiamondPattern(loop_body, bool_param);
+  }
 
   HInstruction* param = MakeParam(DataType::Type::kInt32);
   HInstruction* object = MakeParam(DataType::Type::kReference);
@@ -1681,6 +1690,64 @@ TEST_P(TwoTypesConversionsTestGroup, MergingTwiceConvertedValueStore) {
   }
 }
 
+TEST_P(TwoTypesConversionsTestGroup, MergingTwiceConvertedValueStore) {
+  MergingTwiceConvertedValueStoreTest(/*extra_diamond=*/ false);
+}
+
+TEST_P(TwoTypesConversionsTestGroup, MergingTwiceConvertedValueStoreWithExtraDiamond) {
+  MergingTwiceConvertedValueStoreTest(/*extra_diamond=*/ true);
+}
+
+TEST_P(TwoTypesConversionsTestGroup, UnreplacedConversionLoadDuringStoreElimination) {
+  auto [load_type1, load_type2] = GetParam();
+  DataType::Type field_type1 = FieldTypeForLoadType(load_type1);
+  // Note: `load_type2` is not actually used for any load, we just write with `field_type2`.
+  DataType::Type field_type2 = FieldTypeForLoadType(load_type2);
+
+  HBasicBlock* return_block = InitEntryMainExitGraph();
+  HInstruction* param = MakeParam(DataType::Type::kInt32);
+  HInstruction* object = MakeParam(DataType::Type::kReference);
+  HInstruction* bool_param = MakeParam(DataType::Type::kBool);
+
+  auto [pre_header1, header1, body1] = CreateForLoopWithInstructions(return_block);
+  auto [pre_header2, header2, body2_end] = CreateForLoopWithInstructions(return_block);
+  auto [body2_start, body2_left, body2_right] = CreateDiamondPattern(body2_end, bool_param);
+
+  // Write the first field in the `pre_header1`, clobber it in `body1` and read it
+  // in `pre_header2`. LSE shall initially mark the load as depending on a loop
+  // phi placeholder but later determine that the load must be retained as is.
+  HInstruction* pre_header1_write1 =
+      MakeIFieldSet(pre_header1, object, param, field_type1, MemberOffset(40));
+  HInvoke* body1_invoke = MakeInvokeStatic(body1, DataType::Type::kVoid, {});
+  HInstanceFieldGet* pre_header2_read1 =
+      MakeIFieldGet(pre_header2, object, field_type1, MemberOffset(40));
+  pre_header2_read1->SetType(load_type1);
+
+  // Write the second field in `pre_header2`, `body2_start` and `body2_right`, making
+  // LSE keep these writes because the loop phi placeholder they feed shall remain
+  // live at the return instruction. The value written in `body2_start` is marked as
+  // requiring a loop phi, with or without type conversion based on the load types.
+  // When looking for stores to eliminate, we used to crash when processing the
+  // `body2_right_write2` because a loop phi requiring a type conversion as the "old
+  // value" was not handled correctly with these writes marked as kept. b/405553153
+  HInstruction* pre_header2_write2 =
+      MakeIFieldSet(pre_header2, object, param, field_type2, MemberOffset(40));
+  HInstruction* body2_start_write2 =
+      MakeIFieldSet(body2_start, object, pre_header2_read1, field_type2, MemberOffset(40));
+  HInstruction* body2_right_write2 =
+      MakeIFieldSet(body2_right, object, param, field_type2, MemberOffset(40));
+
+  HInstruction* ret = MakeReturn(return_block, param);
+  PerformLSE();
+
+  EXPECT_INS_RETAINED(pre_header1_write1);
+  EXPECT_INS_RETAINED(body1_invoke);
+  EXPECT_INS_RETAINED(pre_header2_read1);
+  EXPECT_INS_RETAINED(pre_header2_write2);
+  EXPECT_INS_RETAINED(body2_start_write2);
+  EXPECT_INS_RETAINED(body2_right_write2);
+}
+
 TEST_P(TwoTypesConversionsTestGroup, MergingConvertedValueStorePhiDeduplication) {
   auto [load_type1, load_type2] = GetParam();
   DataType::Type field_type1 = FieldTypeForLoadType(load_type1);
diff --git a/compiler/optimizing/locations.cc b/compiler/optimizing/locations.cc
index f419263f62..013bfe809b 100644
--- a/compiler/optimizing/locations.cc
+++ b/compiler/optimizing/locations.cc
@@ -1,4 +1,4 @@
-  /*
+/*
  * Copyright (C) 2014 The Android Open Source Project
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -26,41 +26,47 @@ namespace art HIDDEN {
 // Verify that Location is trivially copyable.
 static_assert(std::is_trivially_copyable<Location>::value, "Location should be trivially copyable");
 
-static inline ArrayRef<Location> AllocateInputLocations(HInstruction* instruction,
-                                                        ArenaAllocator* allocator) {
-  size_t input_count = instruction->InputCount();
-  Location* array = allocator->AllocArray<Location>(input_count, kArenaAllocLocationSummary);
-  return {array, input_count};
-}
-
-LocationSummary::LocationSummary(HInstruction* instruction,
-                                 CallKind call_kind,
-                                 bool intrinsified,
-                                 ArenaAllocator* allocator)
-    : inputs_(AllocateInputLocations(instruction, allocator)),
-      temps_(allocator->Adapter(kArenaAllocLocationSummary)),
-      stack_mask_(nullptr),
+ALWAYS_INLINE inline LocationSummary::CallData::CallData(ArenaAllocator* allocator)
+    : stack_mask(allocator, /*start_bits=*/ 0, /*expandable=*/ true, kArenaAllocLocationSummary),
+      has_custom_slow_path_calling_convention(false),
+      register_mask(0),
+      live_registers(RegisterSet::Empty()),
+      custom_slow_path_caller_saves(RegisterSet::Empty()) {}
+
+ALWAYS_INLINE inline LocationSummary::LocationSummary(HInstruction* instruction,
+                                                      CallKind call_kind,
+                                                      bool intrinsified,
+                                                      ArenaAllocator* allocator,
+                                                      size_t input_count)
+    : temps_(allocator->Adapter(kArenaAllocLocationSummary)),
+      call_data_(nullptr),
       call_kind_(call_kind),
       intrinsified_(intrinsified),
-      has_custom_slow_path_calling_convention_(false),
       output_overlaps_(Location::kOutputOverlap),
-      register_mask_(0),
-      live_registers_(RegisterSet::Empty()),
-      custom_slow_path_caller_saves_(RegisterSet::Empty()) {
+      input_count_(dchecked_integral_cast<uint32_t>(input_count)) {
   instruction->SetLocations(this);
 
-  if (NeedsSafepoint()) {
-    stack_mask_ = ArenaBitVector::Create(allocator, 0, true, kArenaAllocLocationSummary);
+  if (CanCall()) {
+    call_data_ = new (allocator) CallData(allocator);
   }
 }
 
-LocationSummary::LocationSummary(HInstruction* instruction,
-                                 CallKind call_kind,
-                                 bool intrinsified)
-    : LocationSummary(instruction,
-                      call_kind,
-                      intrinsified,
-                      instruction->GetBlock()->GetGraph()->GetAllocator()) {}
+LocationSummary* LocationSummary::CreateImpl(ArenaAllocator* allocator,
+                                             HInstruction* instruction,
+                                             CallKind call_kind,
+                                             bool intrinsified,
+                                             size_t input_count) {
+  size_t size = offsetof(LocationSummary, inputs_) + input_count * sizeof(inputs_[0]);
+  void* storage = allocator->Alloc(size, kArenaAllocLocationSummary);
+  LocationSummary* locations =
+      new (storage) LocationSummary(instruction, call_kind, intrinsified, allocator, input_count);
+  // Inputs array is zero-initialized, all entries are invalid.
+  static_assert(Location::kInvalid == 0);
+  DCHECK(std::all_of(locations->Inputs().begin(),
+                     locations->Inputs().end(),
+                     [](Location loc) { return loc.IsInvalid(); }));
+  return locations;
+}
 
 Location Location::RegisterOrConstant(HInstruction* instruction) {
   return instruction->IsConstant()
diff --git a/compiler/optimizing/locations.h b/compiler/optimizing/locations.h
index b8fe29c621..cf580f0c05 100644
--- a/compiler/optimizing/locations.h
+++ b/compiler/optimizing/locations.h
@@ -17,6 +17,7 @@
 #ifndef ART_COMPILER_OPTIMIZING_LOCATIONS_H_
 #define ART_COMPILER_OPTIMIZING_LOCATIONS_H_
 
+#include "base/arena_bit_vector.h"
 #include "base/arena_containers.h"
 #include "base/arena_object.h"
 #include "base/array_ref.h"
@@ -542,20 +543,45 @@ class LocationSummary : public ArenaObject<kArenaAllocLocationSummary> {
     kCallOnMainOnly
   };
 
-  explicit LocationSummary(HInstruction* instruction,
-                           CallKind call_kind = kNoCall,
-                           bool intrinsified = false);
+  // The `Create` function is parametrized by the instruction type to allow devirtualizing
+  // the underlying virtual call of the inlineable `instruction->InputCount()`.
+  // Note: Making this template-parameter-dependent also helps us avoid `#include "nodes.h"`
+  // which would lead to a circular dependency we would have to resolve.
+  template <typename InstructionType>
+  ALWAYS_INLINE
+  static LocationSummary* Create(ArenaAllocator* allocator,
+                                 InstructionType* instruction,
+                                 CallKind call_kind,
+                                 bool intrinsified = false) {
+    return CreateImpl(allocator, instruction, call_kind, intrinsified, instruction->InputCount());
+  }
+
+  template <typename InstructionType>
+  ALWAYS_INLINE
+  static LocationSummary* CreateNoCall(ArenaAllocator* allocator,
+                                       InstructionType* instruction,
+                                       bool intrinsified = false) {
+    return Create(allocator, instruction, kNoCall, intrinsified);
+  }
+
+  ArrayRef<Location> Inputs() {
+    return ArrayRef<Location>(&inputs_[0], input_count_);
+  }
+
+  ArrayRef<const Location> Inputs() const {
+    return ArrayRef<const Location>(&inputs_[0], input_count_);
+  }
 
   void SetInAt(uint32_t at, Location location) {
-    inputs_[at] = location;
+    Inputs()[at] = location;
   }
 
   Location InAt(uint32_t at) const {
-    return inputs_[at];
+    return Inputs()[at];
   }
 
   size_t GetInputCount() const {
-    return inputs_.size();
+    return input_count_;
   }
 
   // Set the output location.  Argument `overlaps` tells whether the
@@ -636,53 +662,65 @@ class LocationSummary : public ArenaObject<kArenaAllocLocationSummary> {
 
   void SetCustomSlowPathCallerSaves(const RegisterSet& caller_saves) {
     DCHECK(OnlyCallsOnSlowPath());
-    has_custom_slow_path_calling_convention_ = true;
-    custom_slow_path_caller_saves_ = caller_saves;
+    call_data_->has_custom_slow_path_calling_convention = true;
+    call_data_->custom_slow_path_caller_saves = caller_saves;
   }
 
   bool HasCustomSlowPathCallingConvention() const {
-    return has_custom_slow_path_calling_convention_;
+    // Meaningful only for `kCallOnSlowPath`. Allow checking also for `kCallOnMainAndSlowPath`.
+    DCHECK(CallsOnSlowPath());
+    DCHECK_IMPLIES(CallsOnMainAndSlowPath(), !call_data_->has_custom_slow_path_calling_convention);
+    return call_data_->has_custom_slow_path_calling_convention;
   }
 
   const RegisterSet& GetCustomSlowPathCallerSaves() const {
     DCHECK(HasCustomSlowPathCallingConvention());
-    return custom_slow_path_caller_saves_;
+    return call_data_->custom_slow_path_caller_saves;
   }
 
   void SetStackBit(uint32_t index) {
-    stack_mask_->SetBit(index);
+    DCHECK(CanCall());
+    call_data_->stack_mask.SetBit(index);
   }
 
   void ClearStackBit(uint32_t index) {
-    stack_mask_->ClearBit(index);
+    DCHECK(CanCall());
+    call_data_->stack_mask.ClearBit(index);
   }
 
   void SetRegisterBit(uint32_t reg_id) {
-    register_mask_ |= (1 << reg_id);
+    DCHECK(CanCall());
+    call_data_->register_mask |= (1 << reg_id);
   }
 
   uint32_t GetRegisterMask() const {
-    return register_mask_;
+    DCHECK(CanCall());
+    return call_data_->register_mask;
   }
 
   bool RegisterContainsObject(uint32_t reg_id) {
-    return RegisterSet::Contains(register_mask_, reg_id);
+    DCHECK(CanCall());
+    return RegisterSet::Contains(call_data_->register_mask, reg_id);
   }
 
   void AddLiveRegister(Location location) {
-    live_registers_.Add(location);
+    DCHECK(CanCall());
+    call_data_->live_registers.Add(location);
   }
 
   BitVector* GetStackMask() const {
-    return stack_mask_;
+    DCHECK(CanCall());
+    return &call_data_->stack_mask;
   }
 
   RegisterSet* GetLiveRegisters() {
-    return &live_registers_;
+    DCHECK(CanCall());
+    return &call_data_->live_registers;
   }
 
   size_t GetNumberOfLiveRegisters() const {
-    return live_registers_.GetNumberOfRegisters();
+    DCHECK(CanCall());
+    return call_data_->live_registers.GetNumberOfRegisters();
   }
 
   bool OutputUsesSameAs(uint32_t input_index) const {
@@ -692,7 +730,7 @@ class LocationSummary : public ArenaObject<kArenaAllocLocationSummary> {
   }
 
   bool IsFixedInput(uint32_t input_index) const {
-    Location input = inputs_[input_index];
+    Location input = Inputs()[input_index];
     return input.IsRegister()
         || input.IsFpuRegister()
         || input.IsPair()
@@ -709,36 +747,57 @@ class LocationSummary : public ArenaObject<kArenaAllocLocationSummary> {
   }
 
  private:
+  struct CallData : public ArenaObject<kArenaAllocLocationSummary> {
+   public:
+    explicit CallData(ArenaAllocator* allocator);
+
+    // Mask of objects that live in the stack.
+    ArenaBitVector stack_mask;
+
+    // Whether the slow path has default or custom calling convention.
+    bool has_custom_slow_path_calling_convention;
+
+    // Mask of objects that live in register.
+    uint32_t register_mask;
+
+    // Registers that are in use at this position.
+    RegisterSet live_registers;
+
+    // Custom slow path caller saves. Valid only if indicated by
+    // `has_custom_slow_path_calling_convention`.
+    RegisterSet custom_slow_path_caller_saves;
+  };
+
   LocationSummary(HInstruction* instruction,
                   CallKind call_kind,
                   bool intrinsified,
-                  ArenaAllocator* allocator);
+                  ArenaAllocator* allocator,
+                  size_t input_count);
+
+  static LocationSummary* CreateImpl(ArenaAllocator* allocator,
+                                     HInstruction* instruction,
+                                     CallKind call_kind,
+                                     bool intrinsified,
+                                     size_t input_count);
 
-  ArrayRef<Location> inputs_;
   ArenaVector<Location> temps_;
   Location output_;
 
-  // Mask of objects that live in the stack.
-  BitVector* stack_mask_;
+  // Data asociated with a call, null for `kNoCall`.
+  CallData* call_data_;
 
   const CallKind call_kind_;
   // Whether these are locations for an intrinsified call.
   const bool intrinsified_;
-  // Whether the slow path has default or custom calling convention.
-  bool has_custom_slow_path_calling_convention_;
   // Whether the output overlaps with any of the inputs. If it overlaps, then it cannot
   // share the same register as the inputs.
   Location::OutputOverlap output_overlaps_;
 
-  // Mask of objects that live in register.
-  uint32_t register_mask_;
-
-  // Registers that are in use at this position.
-  RegisterSet live_registers_;
+  // The number of inputs.
+  const uint32_t input_count_;
 
-  // Custom slow path caller saves. Valid only if indicated by
-  // `has_custom_slow_path_calling_convention_`.
-  RegisterSet custom_slow_path_caller_saves_;
+  // Inputs array allocated together with the `LocationSummary`.
+  Location inputs_[0];
 
   ART_FRIEND_TEST(RegisterAllocatorTest, ExpectedInRegisterHint);
   ART_FRIEND_TEST(RegisterAllocatorTest, SameAsFirstInputHint);
diff --git a/compiler/optimizing/loop_analysis.cc b/compiler/optimizing/loop_analysis.cc
index b3f9e835de..b78bfb3c07 100644
--- a/compiler/optimizing/loop_analysis.cc
+++ b/compiler/optimizing/loop_analysis.cc
@@ -19,6 +19,7 @@
 #include "base/bit_vector-inl.h"
 #include "code_generator.h"
 #include "induction_var_range.h"
+#include "loop_information-inl.h"
 
 namespace art HIDDEN {
 
@@ -27,11 +28,7 @@ void LoopAnalysis::CalculateLoopBasicProperties(HLoopInformation* loop_info,
                                                 int64_t trip_count) {
   analysis_results->trip_count_ = trip_count;
 
-  for (HBlocksInLoopIterator block_it(*loop_info);
-       !block_it.Done();
-       block_it.Advance()) {
-    HBasicBlock* block = block_it.Current();
-
+  for (HBasicBlock* block : loop_info->GetBlocks()) {
     // Check whether one of the successor is loop exit.
     for (HBasicBlock* successor : block->GetSuccessors()) {
       if (!loop_info->Contains(*successor)) {
@@ -49,7 +46,7 @@ void LoopAnalysis::CalculateLoopBasicProperties(HLoopInformation* loop_info,
       }
     }
 
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       HInstruction* instruction = it.Current();
       if (it.Current()->GetType() == DataType::Type::kInt64) {
         analysis_results->has_long_type_instructions_ = true;
@@ -358,13 +355,12 @@ class X86_64LoopHelper : public ArchDefaultLoopHelper {
 
 uint32_t X86_64LoopHelper::GetUnrollingFactor(HLoopInformation* loop_info,
                                               HBasicBlock* header) const {
-  uint32_t num_inst = 0, num_inst_header = 0, num_inst_loop_body = 0;
-  for (HBlocksInLoopIterator it(*loop_info); !it.Done(); it.Advance()) {
-    HBasicBlock* block = it.Current();
-    DCHECK(block);
-    num_inst = 0;
+  uint32_t num_inst_header = 0, num_inst_loop_body = 0;
+  for (HBasicBlock* block : loop_info->GetBlocks()) {
+    uint32_t num_inst = 0;
 
-    for (HInstructionIterator it1(block->GetInstructions()); !it1.Done(); it1.Advance()) {
+    for (HInstructionIteratorPrefetchNext it1(block->GetInstructions()); !it1.Done();
+         it1.Advance()) {
       HInstruction* inst = it1.Current();
       DCHECK(inst);
 
diff --git a/compiler/optimizing/loop_information-inl.h b/compiler/optimizing/loop_information-inl.h
new file mode 100644
index 0000000000..0eaa64eae4
--- /dev/null
+++ b/compiler/optimizing/loop_information-inl.h
@@ -0,0 +1,58 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_COMPILER_OPTIMIZING_LOOP_INFORMATION_INL_H_
+#define ART_COMPILER_OPTIMIZING_LOOP_INFORMATION_INL_H_
+
+#include "loop_information.h"
+
+#include "base/array_ref.h"
+#include "base/stl_util.h"
+#include "base/transform_iterator.h"
+#include "nodes.h"
+
+namespace art HIDDEN {
+
+inline auto HLoopInformation::GetBlocks() const {
+  ArrayRef<HBasicBlock* const> blocks(GetHeader()->GetGraph()->GetBlocks());
+  return MakeTransformRange(block_mask_.Indexes(),
+                            [blocks](uint32_t index) {
+                              DCHECK(blocks[index] != nullptr);
+                              return blocks[index];
+                            });
+}
+
+inline auto HLoopInformation::GetBlocksPostOrder() const {
+  return Filter(GetHeader()->GetGraph()->GetPostOrder(),
+                [this](HBasicBlock* block) { return block_mask_.IsBitSet(block->GetBlockId()); });
+}
+
+inline auto HLoopInformation::GetBlocksReversePostOrder() const {
+  return Filter(GetHeader()->GetGraph()->GetReversePostOrder(),
+                [this](HBasicBlock* block) { return block_mask_.IsBitSet(block->GetBlockId()); });
+}
+
+inline HLoopInformationOutwardIterator::HLoopInformationOutwardIterator(const HBasicBlock& block)
+    : current_(block.GetLoopInformation()) {}
+
+inline void HLoopInformationOutwardIterator::Advance() {
+  DCHECK(!Done());
+  current_ = current_->GetPreHeader()->GetLoopInformation();
+}
+
+}  // namespace art
+
+#endif  // ART_COMPILER_OPTIMIZING_LOOP_INFORMATION_INL_H_
diff --git a/compiler/optimizing/loop_information.cc b/compiler/optimizing/loop_information.cc
new file mode 100644
index 0000000000..4d7fdd5829
--- /dev/null
+++ b/compiler/optimizing/loop_information.cc
@@ -0,0 +1,271 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "loop_information-inl.h"
+
+#include "base/bit_vector-inl.h"
+#include "nodes.h"
+
+namespace art HIDDEN {
+
+static const int kDefaultNumberOfBackEdges = 1;
+
+HLoopInformation::HLoopInformation(HBasicBlock* header, HGraph* graph)
+    : header_(header),
+      suspend_check_(nullptr),
+      irreducible_(false),
+      contains_irreducible_loop_(false),
+      back_edges_(graph->GetAllocator()->Adapter(kArenaAllocLoopInfoBackEdges)),
+      // Make bit vector growable, as the number of blocks may change.
+      block_mask_(graph->GetAllocator(),
+                  graph->GetBlocks().size(),
+                  /*expandable=*/ true,
+                  kArenaAllocLoopInfoBackEdges) {
+  back_edges_.reserve(kDefaultNumberOfBackEdges);
+}
+
+void HLoopInformation::Dump(std::ostream& os) {
+  os << "header: " << header_->GetBlockId() << std::endl;
+  os << "pre header: " << GetPreHeader()->GetBlockId() << std::endl;
+  for (HBasicBlock* block : back_edges_) {
+    os << "back edge: " << block->GetBlockId() << std::endl;
+  }
+  for (HBasicBlock* block : header_->GetPredecessors()) {
+    os << "predecessor: " << block->GetBlockId() << std::endl;
+  }
+  for (uint32_t idx : block_mask_.Indexes()) {
+    os << "  in loop: " << idx << std::endl;
+  }
+}
+
+void HLoopInformation::Add(HBasicBlock* block) {
+  block_mask_.SetBit(block->GetBlockId());
+}
+
+void HLoopInformation::Remove(HBasicBlock* block) {
+  block_mask_.ClearBit(block->GetBlockId());
+}
+
+void HLoopInformation::PopulateRecursive(HBasicBlock* block) {
+  if (block_mask_.IsBitSet(block->GetBlockId())) {
+    return;
+  }
+
+  block_mask_.SetBit(block->GetBlockId());
+  MarkInLoop(block);
+  if (block->IsLoopHeader()) {
+    // We're visiting loops in post-order, so inner loops must have been
+    // populated already.
+    DCHECK(block->GetLoopInformation()->IsPopulated());
+    if (block->GetLoopInformation()->IsIrreducible()) {
+      contains_irreducible_loop_ = true;
+    }
+  }
+  for (HBasicBlock* predecessor : block->GetPredecessors()) {
+    PopulateRecursive(predecessor);
+  }
+}
+
+void HLoopInformation::PopulateIrreducibleRecursive(HBasicBlock* block, ArenaBitVector* finalized) {
+  size_t block_id = block->GetBlockId();
+
+  // If `block` is in `finalized`, we know its membership in the loop has been
+  // decided and it does not need to be revisited.
+  if (finalized->IsBitSet(block_id)) {
+    return;
+  }
+
+  bool is_finalized = false;
+  if (block->IsLoopHeader()) {
+    // If we hit a loop header in an irreducible loop, we first check if the
+    // pre header of that loop belongs to the currently analyzed loop. If it does,
+    // then we visit the back edges.
+    // Note that we cannot use GetPreHeader, as the loop may have not been populated
+    // yet.
+    HBasicBlock* pre_header = block->GetPredecessors()[0];
+    PopulateIrreducibleRecursive(pre_header, finalized);
+    if (block_mask_.IsBitSet(pre_header->GetBlockId())) {
+      MarkInLoop(block);
+      block_mask_.SetBit(block_id);
+      finalized->SetBit(block_id);
+      is_finalized = true;
+
+      HLoopInformation* info = block->GetLoopInformation();
+      for (HBasicBlock* back_edge : info->GetBackEdges()) {
+        PopulateIrreducibleRecursive(back_edge, finalized);
+      }
+    }
+  } else {
+    // Visit all predecessors. If one predecessor is part of the loop, this
+    // block is also part of this loop.
+    for (HBasicBlock* predecessor : block->GetPredecessors()) {
+      PopulateIrreducibleRecursive(predecessor, finalized);
+      if (!is_finalized && block_mask_.IsBitSet(predecessor->GetBlockId())) {
+        MarkInLoop(block);
+        block_mask_.SetBit(block_id);
+        finalized->SetBit(block_id);
+        is_finalized = true;
+      }
+    }
+  }
+
+  // All predecessors have been recursively visited. Mark finalized if not marked yet.
+  if (!is_finalized) {
+    finalized->SetBit(block_id);
+  }
+}
+
+void HLoopInformation::Populate() {
+  DCHECK_EQ(block_mask_.NumSetBits(), 0u) << "Loop information has already been populated";
+  // Populate this loop: starting with the back edge, recursively add predecessors
+  // that are not already part of that loop. Set the header as part of the loop
+  // to end the recursion.
+  // This is a recursive implementation of the algorithm described in
+  // "Advanced Compiler Design & Implementation" (Muchnick) p192.
+  HGraph* graph = header_->GetGraph();
+  block_mask_.SetBit(header_->GetBlockId());
+  MarkInLoop(header_);
+
+  bool is_irreducible_loop = HasBackEdgeNotDominatedByHeader();
+
+  if (is_irreducible_loop) {
+    // Allocate memory from local ScopedArenaAllocator.
+    ScopedArenaAllocator allocator(graph->GetArenaStack());
+    ArenaBitVector visited(&allocator,
+                           graph->GetBlocks().size(),
+                           /* expandable= */ false,
+                           kArenaAllocGraphBuilder);
+    // Stop marking blocks at the loop header.
+    visited.SetBit(header_->GetBlockId());
+
+    for (HBasicBlock* back_edge : GetBackEdges()) {
+      PopulateIrreducibleRecursive(back_edge, &visited);
+    }
+  } else {
+    for (HBasicBlock* back_edge : GetBackEdges()) {
+      PopulateRecursive(back_edge);
+    }
+  }
+
+  if (!is_irreducible_loop && graph->IsCompilingOsr()) {
+    // When compiling in OSR mode, all loops in the compiled method may be entered
+    // from the interpreter. We treat this OSR entry point just like an extra entry
+    // to an irreducible loop, so we need to mark the method's loops as irreducible.
+    // This does not apply to inlined loops which do not act as OSR entry points.
+    if (suspend_check_ == nullptr) {
+      // Just building the graph in OSR mode, this loop is not inlined. We never build an
+      // inner graph in OSR mode as we can do OSR transition only from the outer method.
+      is_irreducible_loop = true;
+    } else {
+      // Look at the suspend check's environment to determine if the loop was inlined.
+      DCHECK(suspend_check_->HasEnvironment());
+      if (!suspend_check_->GetEnvironment()->IsFromInlinedInvoke()) {
+        is_irreducible_loop = true;
+      }
+    }
+  }
+  if (is_irreducible_loop) {
+    irreducible_ = true;
+    contains_irreducible_loop_ = true;
+    graph->SetHasIrreducibleLoops(true);
+  }
+  graph->SetHasLoops(true);
+}
+
+void HLoopInformation::PopulateInnerLoopUpwards(HLoopInformation* inner_loop) {
+  DCHECK(inner_loop->GetPreHeader()->GetLoopInformation() == this);
+  block_mask_.Union(&inner_loop->block_mask_);
+  HLoopInformation* outer_loop = GetPreHeader()->GetLoopInformation();
+  if (outer_loop != nullptr) {
+    outer_loop->PopulateInnerLoopUpwards(this);
+  }
+}
+
+HBasicBlock* HLoopInformation::GetPreHeader() const {
+  HBasicBlock* block = header_->GetPredecessors()[0];
+  DCHECK(irreducible_ || (block == header_->GetDominator()));
+  return block;
+}
+
+bool HLoopInformation::Contains(const HBasicBlock& block) const {
+  return block_mask_.IsBitSet(block.GetBlockId());
+}
+
+bool HLoopInformation::IsIn(const HLoopInformation& other) const {
+  return other.block_mask_.IsBitSet(header_->GetBlockId());
+}
+
+bool HLoopInformation::IsDefinedOutOfTheLoop(HInstruction* instruction) const {
+  return !block_mask_.IsBitSet(instruction->GetBlock()->GetBlockId());
+}
+
+size_t HLoopInformation::GetLifetimeEnd() const {
+  size_t last_position = 0;
+  for (HBasicBlock* back_edge : GetBackEdges()) {
+    last_position = std::max(back_edge->GetLifetimeEnd(), last_position);
+  }
+  return last_position;
+}
+
+bool HLoopInformation::HasBackEdgeNotDominatedByHeader() const {
+  for (HBasicBlock* back_edge : GetBackEdges()) {
+    DCHECK(back_edge->GetDominator() != nullptr);
+    if (!header_->Dominates(back_edge)) {
+      return true;
+    }
+  }
+  return false;
+}
+
+bool HLoopInformation::DominatesAllBackEdges(HBasicBlock* block) {
+  for (HBasicBlock* back_edge : GetBackEdges()) {
+    if (!block->Dominates(back_edge)) {
+      return false;
+    }
+  }
+  return true;
+}
+
+bool HLoopInformation::HasExitEdge() const {
+  // Determine if this loop has at least one exit edge.
+  for (HBasicBlock* block : GetBlocks()) {
+    for (HBasicBlock* successor : block->GetSuccessors()) {
+      if (!Contains(*successor)) {
+        return true;
+      }
+    }
+  }
+  return false;
+}
+
+inline void HLoopInformation::MarkInLoop(HBasicBlock* block) {
+  if (!block->IsInLoop()) {
+    block->SetLoopInformation(this);
+  } else if (block->IsLoopHeader()) {
+    // Nothing to do. This just means `*this` is an outer loop.
+  } else if (block->GetLoopInformation()->Contains(*GetHeader())) {
+    // The `block` is currently part of an outer loop. Make it part of this inner loop.
+    // Note that a non loop header having a loop information means this loop information
+    // has already been populated
+    block->SetLoopInformation(this);
+  } else {
+    // The `block` is part of an inner loop. Do not update the loop information.
+    // Note that we cannot do the check `Contains(block->GetLoopInformation()->GetHeader())`
+    // at this point, because this function is being called while populating `*this`.
+  }
+}
+
+}  // namespace art
diff --git a/compiler/optimizing/loop_information.h b/compiler/optimizing/loop_information.h
new file mode 100644
index 0000000000..d4ad0b20f6
--- /dev/null
+++ b/compiler/optimizing/loop_information.h
@@ -0,0 +1,177 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_COMPILER_OPTIMIZING_LOOP_INFORMATION_H_
+#define ART_COMPILER_OPTIMIZING_LOOP_INFORMATION_H_
+
+#include <iosfwd>
+
+#include "base/macros.h"
+
+#include "base/arena_bit_vector.h"
+#include "base/arena_containers.h"
+#include "base/arena_object.h"
+#include "base/value_object.h"
+#include "base/stl_util.h"
+
+namespace art HIDDEN {
+
+class HBasicBlock;
+class HGraph;
+class HInstruction;
+class HSuspendCheck;
+
+class HLoopInformation final : public ArenaObject<kArenaAllocLoopInfo> {
+ public:
+  HLoopInformation(HBasicBlock* header, HGraph* graph);
+
+  bool IsIrreducible() const { return irreducible_; }
+  bool ContainsIrreducibleLoop() const { return contains_irreducible_loop_; }
+
+  void Dump(std::ostream& os);
+
+  HBasicBlock* GetHeader() const {
+    return header_;
+  }
+
+  void SetHeader(HBasicBlock* block) {
+    header_ = block;
+  }
+
+  HSuspendCheck* GetSuspendCheck() const { return suspend_check_; }
+  void SetSuspendCheck(HSuspendCheck* check) { suspend_check_ = check; }
+  bool HasSuspendCheck() const { return suspend_check_ != nullptr; }
+
+  void AddBackEdge(HBasicBlock* back_edge) {
+    back_edges_.push_back(back_edge);
+  }
+
+  void RemoveBackEdge(HBasicBlock* back_edge) {
+    RemoveElement(back_edges_, back_edge);
+  }
+
+  bool IsBackEdge(const HBasicBlock& block) const {
+    return ContainsElement(back_edges_, &block);
+  }
+
+  size_t NumberOfBackEdges() const {
+    return back_edges_.size();
+  }
+
+  HBasicBlock* GetPreHeader() const;
+
+  const ArenaVector<HBasicBlock*>& GetBackEdges() const {
+    return back_edges_;
+  }
+
+  // Returns the lifetime position of the back edge that has the
+  // greatest lifetime position.
+  size_t GetLifetimeEnd() const;
+
+  void ReplaceBackEdge(HBasicBlock* existing, HBasicBlock* new_back_edge) {
+    ReplaceElement(back_edges_, existing, new_back_edge);
+  }
+
+  // Finds blocks that are part of this loop.
+  void Populate();
+
+  // Updates blocks population of the loop and all of its outer' ones recursively after the
+  // population of the inner loop is updated.
+  void PopulateInnerLoopUpwards(HLoopInformation* inner_loop);
+
+  // Returns whether this loop information contains `block`.
+  // Note that this loop information *must* be populated before entering this function.
+  bool Contains(const HBasicBlock& block) const;
+
+  // Returns whether this loop information is an inner loop of `other`.
+  // Note that `other` *must* be populated before entering this function.
+  bool IsIn(const HLoopInformation& other) const;
+
+  // Returns true if instruction is not defined within this loop.
+  bool IsDefinedOutOfTheLoop(HInstruction* instruction) const;
+
+  const ArenaBitVector& GetBlockMask() const { return block_mask_; }
+
+  void Add(HBasicBlock* block);
+  void Remove(HBasicBlock* block);
+
+  void ClearAllBlocks() {
+    block_mask_.ClearAllBits();
+  }
+
+  bool HasBackEdgeNotDominatedByHeader() const;
+
+  bool IsPopulated() const {
+    return block_mask_.GetHighestBitSet() != -1;
+  }
+
+  bool DominatesAllBackEdges(HBasicBlock* block);
+
+  bool HasExitEdge() const;
+
+  // Resets back edge and blocks-in-loop data.
+  void ResetBasicBlockData() {
+    back_edges_.clear();
+    ClearAllBlocks();
+  }
+
+  auto GetBlocks() const;  // In block id order.
+  auto GetBlocksPostOrder() const;
+  auto GetBlocksReversePostOrder() const;
+
+ private:
+  // Internal recursive implementation of `Populate`.
+  void PopulateRecursive(HBasicBlock* block);
+  void PopulateIrreducibleRecursive(HBasicBlock* block, ArenaBitVector* finalized);
+
+  // Set the loop information in the `block`. Overrides the `block`'s current
+  // loop information if it is an outer loop of the loop information `*this`.
+  void MarkInLoop(HBasicBlock* block);
+
+  HBasicBlock* header_;
+  HSuspendCheck* suspend_check_;
+  bool irreducible_;
+  bool contains_irreducible_loop_;
+  ArenaVector<HBasicBlock*> back_edges_;
+  ArenaBitVector block_mask_;
+
+  DISALLOW_COPY_AND_ASSIGN(HLoopInformation);
+};
+
+// Iterates over the LoopInformation of all loops which contain 'block'
+// from the innermost to the outermost.
+class HLoopInformationOutwardIterator final : public ValueObject {
+ public:
+  explicit HLoopInformationOutwardIterator(const HBasicBlock& block);
+
+  bool Done() const { return current_ == nullptr; }
+
+  void Advance();
+
+  HLoopInformation* Current() const {
+    DCHECK(!Done());
+    return current_;
+  }
+
+ private:
+  HLoopInformation* current_;
+
+  DISALLOW_COPY_AND_ASSIGN(HLoopInformationOutwardIterator);
+};
+
+}  // namespace art
+
+#endif  // ART_COMPILER_OPTIMIZING_LOOP_INFORMATION_H_
diff --git a/compiler/optimizing/loop_optimization.cc b/compiler/optimizing/loop_optimization.cc
index 18b2b56d00..7447b4c947 100644
--- a/compiler/optimizing/loop_optimization.cc
+++ b/compiler/optimizing/loop_optimization.cc
@@ -24,6 +24,7 @@
 #include "code_generator.h"
 #include "driver/compiler_options.h"
 #include "linear_order.h"
+#include "loop_information-inl.h"
 #include "mirror/array-inl.h"
 #include "mirror/string.h"
 
@@ -71,9 +72,9 @@ static bool IsGotoBlock(HBasicBlock* block, /*out*/ HBasicBlock** succ) {
 
 // Detect an early exit loop.
 static bool IsEarlyExit(HLoopInformation* loop_info) {
-  HBlocksInLoopReversePostOrderIterator it_loop(*loop_info);
-  for (it_loop.Advance(); !it_loop.Done(); it_loop.Advance()) {
-    for (HBasicBlock* successor : it_loop.Current()->GetSuccessors()) {
+  auto loop_blocks = loop_info->GetBlocksReversePostOrder();
+  for (auto loop_it = ++loop_blocks.begin(), end = loop_blocks.end(); loop_it != end; ++loop_it) {
+    for (HBasicBlock* successor : (*loop_it)->GetSuccessors()) {
       if (!loop_info->Contains(*successor)) {
         return true;
       }
@@ -492,12 +493,12 @@ static bool HasLoopDiamondStructure(HLoopInformation* loop_info) {
     return false;
   }
 
-  DCHECK_EQ(loop_info->GetBlocks().NumSetBits(), 5u);
+  DCHECK_EQ(loop_info->GetBlockMask().NumSetBits(), 5u);
   return true;
 }
 
 static bool IsPredicatedLoopControlFlowSupported(HLoopInformation* loop_info) {
-  size_t num_of_blocks = loop_info->GetBlocks().NumSetBits();
+  size_t num_of_blocks = loop_info->GetBlockMask().NumSetBits();
   return num_of_blocks == 2 || HasLoopDiamondStructure(loop_info);
 }
 
@@ -721,8 +722,7 @@ void HLoopOptimization::CalculateAndSetTryCatchKind(LoopNode* node) {
     }
   }
 
-  for (HBlocksInLoopIterator it_loop(*node->loop_info); !it_loop.Done(); it_loop.Advance()) {
-    HBasicBlock* block = it_loop.Current();
+  for (HBasicBlock* block : node->loop_info->GetBlocks()) {
     if (block->GetTryCatchInformation() != nullptr) {
       node->try_catch_kind = LoopNode::TryCatchKind::kHasTryCatch;
       return;
@@ -788,7 +788,7 @@ void HLoopOptimization::SimplifyInduction(LoopNode* node) {
   // the last value and remove the induction cycle.
   // Examples: for (int i = 0; x != null;   i++) { .... no i .... }
   //           for (int i = 0; i < 10; i++, k++) { .... no k .... } return k;
-  for (HInstructionIterator it(header->GetPhis()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(header->GetPhis()); !it.Done(); it.Advance()) {
     HPhi* phi = it.Current()->AsPhi();
     if (TrySetPhiInduction(phi, /*restrict_uses*/ true) &&
         TryAssignLastValue(node->loop_info, phi, preheader, /*collect_loop_uses*/ false)) {
@@ -809,8 +809,13 @@ void HLoopOptimization::SimplifyInduction(LoopNode* node) {
 
 void HLoopOptimization::SimplifyBlocks(LoopNode* node) {
   // Iterate over all basic blocks in the loop-body.
-  for (HBlocksInLoopIterator it(*node->loop_info); !it.Done(); it.Advance()) {
-    HBasicBlock* block = it.Current();
+  //
+  // Note that when we remove blocks, the corresponding bit in the loop information's
+  // block mask shall be cleared. The underlying bit vector iterator shall then skip
+  // such cleared bits because it looks at the bit vector storage and does not cache
+  // the bits, not even the currently processed word. This is rather error prone as
+  // future optimizations of the iterator could break this code.
+  for (HBasicBlock* block : node->loop_info->GetBlocks()) {
     // Remove dead instructions from the loop-body.
     RemoveDeadInstructions(block->GetPhis());
     RemoveDeadInstructions(block->GetInstructions());
@@ -851,11 +856,7 @@ void HLoopOptimization::SimplifyBlocks(LoopNode* node) {
 // In that case returns single exit basic block (outside the loop); otherwise nullptr.
 static HBasicBlock* GetInnerLoopFiniteSingleExit(HLoopInformation* loop_info) {
   HBasicBlock* exit = nullptr;
-  for (HBlocksInLoopIterator block_it(*loop_info);
-       !block_it.Done();
-       block_it.Advance()) {
-    HBasicBlock* block = block_it.Current();
-
+  for (HBasicBlock* block : loop_info->GetBlocks()) {
     // Check whether one of the successor is loop exit.
     for (HBasicBlock* successor : block->GetSuccessors()) {
       if (!loop_info->Contains(*successor)) {
@@ -896,7 +897,7 @@ bool HLoopOptimization::TryOptimizeInnerLoopFinite(LoopNode* node) {
   // a trivial loop (just iterating once). Replace subsequent index uses, if any,
   // with the last value and remove the loop, possibly after unrolling its body.
   HPhi* main_phi = nullptr;
-  size_t num_of_blocks = header->GetLoopInformation()->GetBlocks().NumSetBits();
+  size_t num_of_blocks = header->GetLoopInformation()->GetBlockMask().NumSetBits();
 
   if (num_of_blocks == 2 && TrySetSimpleLoopHeader(header, &main_phi)) {
     bool is_empty = IsEmptyBody(body);
@@ -952,7 +953,7 @@ bool HLoopOptimization::TryVectorizePredicated(LoopNode* node,
   //
   // TODO: Support array disambiguation tests for CF loops.
   if (NeedsArrayRefsDisambiguationTest() &&
-      node->loop_info->GetBlocks().NumSetBits() != 2) {
+      node->loop_info->GetBlockMask().NumSetBits() != 2) {
     return false;
   }
 
@@ -968,7 +969,7 @@ bool HLoopOptimization::TryVectorizedTraditional(LoopNode* node,
                                                  HPhi* main_phi,
                                                  int64_t trip_count) {
   HBasicBlock* header = node->loop_info->GetHeader();
-  size_t num_of_blocks = header->GetLoopInformation()->GetBlocks().NumSetBits();
+  size_t num_of_blocks = header->GetLoopInformation()->GetBlockMask().NumSetBits();
 
   if (num_of_blocks != 2 || !ShouldVectorizeCommon(node, main_phi, trip_count)) {
     return false;
@@ -1143,11 +1144,7 @@ bool HLoopOptimization::CanVectorizeDataFlow(LoopNode* node,
   vector_runtime_test_b_ = nullptr;
 
   // Traverse the data flow of the loop, in the original program order.
-  for (HBlocksInLoopReversePostOrderIterator block_it(*header->GetLoopInformation());
-       !block_it.Done();
-       block_it.Advance()) {
-    HBasicBlock* block = block_it.Current();
-
+  for (HBasicBlock* block : header->GetLoopInformation()->GetBlocksReversePostOrder()) {
     if (block == header) {
       // The header is of a certain structure (TrySetSimpleLoopHeader) and doesn't need to be
       // processed here.
@@ -1162,7 +1159,7 @@ bool HLoopOptimization::CanVectorizeDataFlow(LoopNode* node,
 
     // Scan the loop-body instructions, starting a right-hand-side tree traversal at each
     // left-hand-side occurrence, which allows passing down attributes down the use tree.
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       if (!VectorizeDef(node, it.Current(), /*generate_code*/ false)) {
         return false;  // failure to vectorize a left-hand-side
       }
@@ -1536,10 +1533,12 @@ void HLoopOptimization::FinalizeVectorization(LoopNode* node) {
   }
 
   // Remove the original loop.
-  for (HBlocksInLoopPostOrderIterator it_loop(*node->loop_info);
-       !it_loop.Done();
-       it_loop.Advance()) {
-    HBasicBlock* cur_block = it_loop.Current();
+  auto loop_blocks = node->loop_info->GetBlocksPostOrder();
+  for (auto it = loop_blocks.begin(), end = loop_blocks.end(); it != end;) {
+    HBasicBlock* cur_block = *it;
+    // Advance the iterator early to avoid potential issues with iterator validity when
+    // removing the block below and clearing the corresponding bit in the loop's block mask.
+    ++it;
     if (cur_block == node->loop_info->GetHeader()) {
       continue;
     }
@@ -1634,12 +1633,9 @@ void HLoopOptimization::GenerateNewLoopPredicated(LoopNode* node,
   InitPredicateInfoMap(node, pred_while);
 
   // Assign governing predicates for instructions in the loop; the traversal order doesn't matter.
-  for (HBlocksInLoopIterator block_it(*node->loop_info);
-       !block_it.Done();
-       block_it.Advance()) {
-    HBasicBlock* cur_block = block_it.Current();
-
-    for (HInstructionIterator it(cur_block->GetInstructions()); !it.Done(); it.Advance()) {
+  for (HBasicBlock* cur_block : node->loop_info->GetBlocks()) {
+    for (HInstructionIteratorPrefetchNext it(cur_block->GetInstructions()); !it.Done();
+         it.Advance()) {
       auto i = vector_map_->find(it.Current());
       if (i != vector_map_->end()) {
         HInstruction* instr = i->second;
@@ -1677,16 +1673,13 @@ void HLoopOptimization::GenerateNewLoopBodyOnce(LoopNode* node,
   HLoopInformation* loop_info = node->loop_info;
 
   // Traverse the data flow of the loop, in the original program order.
-  for (HBlocksInLoopReversePostOrderIterator block_it(*loop_info);
-      !block_it.Done();
-      block_it.Advance()) {
-    HBasicBlock* cur_block = block_it.Current();
-
+  for (HBasicBlock* cur_block : loop_info->GetBlocksReversePostOrder()) {
     if (cur_block == loop_info->GetHeader()) {
       continue;
     }
 
-    for (HInstructionIterator it(cur_block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(cur_block->GetInstructions()); !it.Done();
+         it.Advance()) {
       bool vectorized_def = VectorizeDef(node, it.Current(), /*generate_code*/ true);
       DCHECK(vectorized_def);
     }
@@ -1694,16 +1687,13 @@ void HLoopOptimization::GenerateNewLoopBodyOnce(LoopNode* node,
 
   // Generate body from the instruction map, in the original program order.
   HEnvironment* env = vector_header_->GetFirstInstruction()->GetEnvironment();
-  for (HBlocksInLoopReversePostOrderIterator block_it(*loop_info);
-        !block_it.Done();
-        block_it.Advance()) {
-    HBasicBlock* cur_block = block_it.Current();
-
+  for (HBasicBlock* cur_block : loop_info->GetBlocksReversePostOrder()) {
     if (cur_block == loop_info->GetHeader()) {
       continue;
     }
 
-    for (HInstructionIterator it(cur_block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(cur_block->GetInstructions()); !it.Done();
+         it.Advance()) {
       auto i = vector_map_->find(it.Current());
       if (i != vector_map_->end() && !i->second->IsInBlock()) {
         Insert(vector_body_, i->second);
@@ -2978,7 +2968,7 @@ bool HLoopOptimization::TrySetSimpleLoopHeader(HBasicBlock* block, /*out*/ HPhi*
   // (1) optional reductions in loop,
   // (2) the main induction, used in loop control.
   HPhi* phi = nullptr;
-  for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
     if (TrySetPhiReduction(it.Current()->AsPhi())) {
       continue;
     } else if (phi == nullptr) {
@@ -3018,7 +3008,7 @@ bool HLoopOptimization::IsEmptyBody(HBasicBlock* block) {
   if (!block->GetPhis().IsEmpty()) {
     return false;
   }
-  for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
     HInstruction* instruction = it.Current();
     if (!instruction->IsGoto() && iset_->find(instruction) == iset_->end()) {
       return false;
@@ -3118,7 +3108,7 @@ bool HLoopOptimization::TryAssignLastValue(HLoopInformation* loop_info,
 }
 
 void HLoopOptimization::RemoveDeadInstructions(const HInstructionList& list) {
-  for (HBackwardInstructionIterator i(list); !i.Done(); i.Advance()) {
+  for (HBackwardInstructionIteratorPrefetchNext i(list); !i.Done(); i.Advance()) {
     HInstruction* instruction = i.Current();
     if (instruction->IsDeadAndRemovable()) {
       simplified_ = true;
@@ -3149,10 +3139,7 @@ void HLoopOptimization::PreparePredicateInfoMap(LoopNode* node) {
 
   DCHECK(IsPredicatedLoopControlFlowSupported(loop_info));
 
-  for (HBlocksInLoopIterator block_it(*loop_info);
-      !block_it.Done();
-      block_it.Advance()) {
-    HBasicBlock* cur_block = block_it.Current();
+  for (HBasicBlock* cur_block : loop_info->GetBlocks()) {
     BlockPredicateInfo* pred_info = new (loop_allocator_) BlockPredicateInfo();
 
     predicate_info_map_->Put(cur_block, pred_info);
@@ -3168,7 +3155,7 @@ void HLoopOptimization::InitPredicateInfoMap(LoopNode* node,
   // would just exit the loop then.
   header_info->SetControlFlowInfo(loop_main_pred, loop_main_pred);
 
-  size_t blocks_in_loop = header->GetLoopInformation()->GetBlocks().NumSetBits();
+  size_t blocks_in_loop = header->GetLoopInformation()->GetBlockMask().NumSetBits();
   if (blocks_in_loop == 2) {
     for (HBasicBlock* successor : header->GetSuccessors()) {
       if (loop_info->Contains(*successor)) {
diff --git a/compiler/optimizing/loop_optimization_test.cc b/compiler/optimizing/loop_optimization_test.cc
index c60b66b6d7..0139b9a5c4 100644
--- a/compiler/optimizing/loop_optimization_test.cc
+++ b/compiler/optimizing/loop_optimization_test.cc
@@ -101,10 +101,8 @@ class LoopOptimizationTest : public LoopOptimizationTestBase {
 
   /** Adds a loop nest at given position before successor. */
   HBasicBlock* AddLoop(HBasicBlock* position, HBasicBlock* successor) {
-    HBasicBlock* header = new (GetAllocator()) HBasicBlock(graph_);
-    HBasicBlock* body = new (GetAllocator()) HBasicBlock(graph_);
-    graph_->AddBlock(header);
-    graph_->AddBlock(body);
+    HBasicBlock* header = AddNewBlock();
+    HBasicBlock* body = AddNewBlock();
     // Control flow.
     position->ReplaceSuccessor(successor, header);
     header->AddSuccessor(body);
@@ -317,10 +315,8 @@ TEST_F(LoopOptimizationTest, LoopNestWithSequence) {
 // This is a test for nodes.cc functionality - HGraph::SimplifyLoop.
 TEST_F(LoopOptimizationTest, SimplifyLoopReoderPredecessors) {
   // Can't use AddLoop as we want special order for blocks predecessors.
-  HBasicBlock* header = new (GetAllocator()) HBasicBlock(graph_);
-  HBasicBlock* body = new (GetAllocator()) HBasicBlock(graph_);
-  graph_->AddBlock(header);
-  graph_->AddBlock(body);
+  HBasicBlock* header = AddNewBlock();
+  HBasicBlock* body = AddNewBlock();
 
   // Control flow: make a loop back edge first in the list of predecessors.
   entry_block_->RemoveSuccessor(return_block_);
@@ -366,13 +362,9 @@ TEST_F(LoopOptimizationTest, SimplifyLoopSinglePreheader) {
       new (GetAllocator()) HSuspendCheck(), header->GetLastInstruction());
 
   // Insert an if construct before the loop so it will have two preheaders.
-  HBasicBlock* if_block = new (GetAllocator()) HBasicBlock(graph_);
-  HBasicBlock* preheader0 = new (GetAllocator()) HBasicBlock(graph_);
-  HBasicBlock* preheader1 = new (GetAllocator()) HBasicBlock(graph_);
-
-  graph_->AddBlock(if_block);
-  graph_->AddBlock(preheader0);
-  graph_->AddBlock(preheader1);
+  HBasicBlock* if_block = AddNewBlock();
+  HBasicBlock* preheader0 = AddNewBlock();
+  HBasicBlock* preheader1 = AddNewBlock();
 
   // Fix successors/predecessors.
   entry_block_->ReplaceSuccessor(header, if_block);
diff --git a/compiler/optimizing/nodes.cc b/compiler/optimizing/nodes.cc
index 6b74e7246e..99253f063f 100644
--- a/compiler/optimizing/nodes.cc
+++ b/compiler/optimizing/nodes.cc
@@ -28,7 +28,7 @@
 #include "base/bit_vector.h"
 #include "base/iteration_range.h"
 #include "base/logging.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "base/scoped_arena_allocator.h"
 #include "base/scoped_arena_containers.h"
 #include "base/stl_util.h"
@@ -39,6 +39,7 @@
 #include "intrinsic_objects.h"
 #include "intrinsics.h"
 #include "intrinsics_list.h"
+#include "loop_information-inl.h"
 #include "mirror/class-inl.h"
 #include "scoped_thread_state_change-inl.h"
 #include "ssa_builder.h"
@@ -61,6 +62,17 @@ inline int32_t HGraph::AllocateInstructionId() {
   return current_instruction_id_++;
 }
 
+// Register a back edge; if the block was not a loop header before the call,
+// associate a newly created loop info with it.
+void AddBackEdge(HBasicBlock* block, HBasicBlock* back_edge) {
+  if (block->GetLoopInformation() == nullptr) {
+    HGraph* graph = block->GetGraph();
+    block->SetLoopInformation(new (graph->GetAllocator()) HLoopInformation(block, graph));
+  }
+  DCHECK_EQ(block->GetLoopInformation()->GetHeader(), block);
+  block->GetLoopInformation()->AddBackEdge(back_edge);
+}
+
 void HGraph::FindBackEdges(/*out*/ BitVectorView<size_t> visited) {
   // "visited" must be empty on entry, it's an output argument for all visited (i.e. live) blocks.
   DCHECK(!visited.IsAnyBitSet());
@@ -93,7 +105,7 @@ void HGraph::FindBackEdges(/*out*/ BitVectorView<size_t> visited) {
       uint32_t successor_id = successor->GetBlockId();
       if (visiting.IsBitSet(successor_id)) {
         DCHECK(ContainsElement(worklist, successor));
-        successor->AddBackEdge(current);
+        AddBackEdge(successor, current);
       } else if (!visited.IsBitSet(successor_id)) {
         visited.SetBit(successor_id);
         visiting.SetBit(successor_id);
@@ -158,10 +170,11 @@ void HGraph::RemoveDeadBlocksInstructionsAsUsersAndDisconnect(
       if (block == nullptr) continue;
 
       // Remove as user.
-      for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+      for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
         RemoveAsUser(it.Current());
       }
-      for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+      for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done();
+           it.Advance()) {
         RemoveAsUser(it.Current());
       }
 
@@ -183,7 +196,8 @@ static void RemoveCatchPhiUsesOfDeadInstruction(HInstruction* insn) {
     HBasicBlock* user_block = use.GetUser()->GetBlock();
     DCHECK(use.GetUser()->IsPhi());
     DCHECK(user_block->IsCatchBlock());
-    for (HInstructionIterator phi_it(user_block->GetPhis()); !phi_it.Done(); phi_it.Advance()) {
+    for (HInstructionIteratorPrefetchNext phi_it(user_block->GetPhis()); !phi_it.Done();
+         phi_it.Advance()) {
       phi_it.Current()->AsPhi()->RemoveInputAt(use_index);
     }
   }
@@ -309,38 +323,48 @@ static bool UpdateDominatorOfSuccessor(HBasicBlock* block, HBasicBlock* successo
 
 void HGraph::ComputeDominanceInformation() {
   DCHECK(reverse_post_order_.empty());
-  reverse_post_order_.reserve(blocks_.size());
+  const size_t size = blocks_.size();
+  reverse_post_order_.reserve(size);
   reverse_post_order_.push_back(entry_block_);
 
-  // Allocate memory from local ScopedArenaAllocator.
-  ScopedArenaAllocator allocator(GetArenaStack());
-  // Number of visits of a given node, indexed by block id.
-  ScopedArenaVector<size_t> visits(blocks_.size(), 0u, allocator.Adapter(kArenaAllocGraphBuilder));
-  // Number of successors visited from a given node, indexed by block id.
-  ScopedArenaVector<size_t> successors_visited(blocks_.size(),
-                                               0u,
-                                               allocator.Adapter(kArenaAllocGraphBuilder));
-  // Nodes for which we need to visit successors.
-  ScopedArenaVector<HBasicBlock*> worklist(allocator.Adapter(kArenaAllocGraphBuilder));
-  constexpr size_t kDefaultWorklistSize = 8;
-  worklist.reserve(kDefaultWorklistSize);
-  worklist.push_back(entry_block_);
+  {
+    // Allocate memory from local ScopedArenaAllocator.
+    ScopedArenaAllocator allocator(GetArenaStack());
+    // Number of visits of a given node, indexed by block id.
+    ScopedArenaVector<size_t> visits(size, 0u, allocator.Adapter(kArenaAllocGraphBuilder));
+    // Number of successors visited from a given node, indexed by block id.
+    ScopedArenaVector<size_t> successors_visited(
+        size, 0u, allocator.Adapter(kArenaAllocGraphBuilder));
+    // Nodes for which we need to visit successors.
+    ScopedArenaVector<HBasicBlock*> worklist(allocator.Adapter(kArenaAllocGraphBuilder));
+    worklist.reserve(size);
+    worklist.push_back(entry_block_);
+
+    // Cached for the check below.
+    HBasicBlock* exit = GetExitBlock();
 
-  while (!worklist.empty()) {
-    HBasicBlock* current = worklist.back();
-    uint32_t current_id = current->GetBlockId();
-    if (successors_visited[current_id] == current->GetSuccessors().size()) {
-      worklist.pop_back();
-    } else {
+    while (!worklist.empty()) {
+      HBasicBlock* current = worklist.back();
+      uint32_t current_id = current->GetBlockId();
+      DCHECK_LT(successors_visited[current_id], current->GetSuccessors().size());
       HBasicBlock* successor = current->GetSuccessors()[successors_visited[current_id]++];
+      if (successors_visited[current_id] == current->GetSuccessors().size()) {
+        worklist.pop_back();
+      }
       UpdateDominatorOfSuccessor(current, successor);
 
       // Once all the forward edges have been visited, we know the immediate
       // dominator of the block. We can then start visiting its successors.
-      if (++visits[successor->GetBlockId()] ==
-          successor->GetPredecessors().size() - successor->NumberOfBackEdges()) {
+      size_t successor_visits_needed =
+          successor->GetPredecessors().size() -
+          (successor->IsLoopHeader() ? successor->GetLoopInformation()->NumberOfBackEdges() : 0u);
+      if (++visits[successor->GetBlockId()] == successor_visits_needed) {
         reverse_post_order_.push_back(successor);
-        worklist.push_back(successor);
+        // The exit block is the only one with no successors. Will be encountered only one time per
+        // graph, at the end.
+        if (LIKELY(successor != exit)) {
+          worklist.push_back(successor);
+        }
       }
     }
   }
@@ -391,7 +415,7 @@ void HGraph::ComputeDominanceInformation() {
 }
 
 HBasicBlock* HGraph::SplitEdge(HBasicBlock* block, HBasicBlock* successor) {
-  HBasicBlock* new_block = new (allocator_) HBasicBlock(this, successor->GetDexPc());
+  HBasicBlock* new_block = HBasicBlock::Create(allocator_, this, successor->GetDexPc());
   AddBlock(new_block);
   // Use `InsertBetween` to ensure the predecessor index and successor index of
   // `block` and `successor` are preserved.
@@ -414,18 +438,9 @@ void HGraph::SplitCriticalEdge(HBasicBlock* block, HBasicBlock* successor) {
   }
 }
 
-HBasicBlock* HGraph::SplitEdgeAndUpdateRPO(HBasicBlock* block, HBasicBlock* successor) {
-  HBasicBlock* new_block = SplitEdge(block, successor);
-  // In the RPO we have {... , block, ... , successor}. We want to insert `new_block` right after
-  // `block` to have a consistent RPO without recomputing the whole graph's RPO.
-  reverse_post_order_.insert(
-      reverse_post_order_.begin() + IndexOfElement(reverse_post_order_, block) + 1, new_block);
-  return new_block;
-}
-
 // Reorder phi inputs to match reordering of the block's predecessors.
 static void FixPhisAfterPredecessorsReodering(HBasicBlock* block, size_t first, size_t second) {
-  for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
     HPhi* phi = it.Current()->AsPhi();
     HInstruction* first_instr = phi->InputAt(first);
     HInstruction* second_instr = phi->InputAt(second);
@@ -498,7 +513,7 @@ static void FixControlForNewSinglePreheader(HBasicBlock* header, HBasicBlock* ne
 void HGraph::TransformLoopToSinglePreheaderFormat(HBasicBlock* header) {
   HLoopInformation* loop_info = header->GetLoopInformation();
 
-  HBasicBlock* preheader = new (allocator_) HBasicBlock(this, header->GetDexPc());
+  HBasicBlock* preheader = HBasicBlock::Create(allocator_, this, header->GetDexPc());
   AddBlock(preheader);
   preheader->AddInstruction(new (allocator_) HGoto(header->GetDexPc()));
 
@@ -524,7 +539,7 @@ void HGraph::TransformLoopToSinglePreheaderFormat(HBasicBlock* header) {
   DCHECK(found);
 
   // Fix the data-flow.
-  for (HInstructionIterator it(header->GetPhis()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(header->GetPhis()); !it.Done(); it.Advance()) {
     HPhi* header_phi = it.Current()->AsPhi();
 
     HPhi* preheader_phi = new (GetAllocator()) HPhi(GetAllocator(),
@@ -677,20 +692,6 @@ GraphAnalysisResult HGraph::AnalyzeLoops() const {
   return kAnalysisSuccess;
 }
 
-void HLoopInformation::Dump(std::ostream& os) {
-  os << "header: " << header_->GetBlockId() << std::endl;
-  os << "pre header: " << GetPreHeader()->GetBlockId() << std::endl;
-  for (HBasicBlock* block : back_edges_) {
-    os << "back edge: " << block->GetBlockId() << std::endl;
-  }
-  for (HBasicBlock* block : header_->GetPredecessors()) {
-    os << "predecessor: " << block->GetBlockId() << std::endl;
-  }
-  for (uint32_t idx : blocks_.Indexes()) {
-    os << "  in loop: " << idx << std::endl;
-  }
-}
-
 template <class InstructionType, typename ValueType>
 InstructionType* HGraph::CreateConstant(ValueType value,
                                         ArenaSafeMap<ValueType, InstructionType*>* cache) {
@@ -827,208 +828,6 @@ void HGraph::CacheDoubleConstant(HDoubleConstant* constant) {
   cached_double_constants_.Overwrite(value, constant);
 }
 
-void HLoopInformation::Add(HBasicBlock* block) {
-  blocks_.SetBit(block->GetBlockId());
-}
-
-void HLoopInformation::Remove(HBasicBlock* block) {
-  blocks_.ClearBit(block->GetBlockId());
-}
-
-void HLoopInformation::PopulateRecursive(HBasicBlock* block) {
-  if (blocks_.IsBitSet(block->GetBlockId())) {
-    return;
-  }
-
-  blocks_.SetBit(block->GetBlockId());
-  block->SetInLoop(this);
-  if (block->IsLoopHeader()) {
-    // We're visiting loops in post-order, so inner loops must have been
-    // populated already.
-    DCHECK(block->GetLoopInformation()->IsPopulated());
-    if (block->GetLoopInformation()->IsIrreducible()) {
-      contains_irreducible_loop_ = true;
-    }
-  }
-  for (HBasicBlock* predecessor : block->GetPredecessors()) {
-    PopulateRecursive(predecessor);
-  }
-}
-
-void HLoopInformation::PopulateIrreducibleRecursive(HBasicBlock* block, ArenaBitVector* finalized) {
-  size_t block_id = block->GetBlockId();
-
-  // If `block` is in `finalized`, we know its membership in the loop has been
-  // decided and it does not need to be revisited.
-  if (finalized->IsBitSet(block_id)) {
-    return;
-  }
-
-  bool is_finalized = false;
-  if (block->IsLoopHeader()) {
-    // If we hit a loop header in an irreducible loop, we first check if the
-    // pre header of that loop belongs to the currently analyzed loop. If it does,
-    // then we visit the back edges.
-    // Note that we cannot use GetPreHeader, as the loop may have not been populated
-    // yet.
-    HBasicBlock* pre_header = block->GetPredecessors()[0];
-    PopulateIrreducibleRecursive(pre_header, finalized);
-    if (blocks_.IsBitSet(pre_header->GetBlockId())) {
-      block->SetInLoop(this);
-      blocks_.SetBit(block_id);
-      finalized->SetBit(block_id);
-      is_finalized = true;
-
-      HLoopInformation* info = block->GetLoopInformation();
-      for (HBasicBlock* back_edge : info->GetBackEdges()) {
-        PopulateIrreducibleRecursive(back_edge, finalized);
-      }
-    }
-  } else {
-    // Visit all predecessors. If one predecessor is part of the loop, this
-    // block is also part of this loop.
-    for (HBasicBlock* predecessor : block->GetPredecessors()) {
-      PopulateIrreducibleRecursive(predecessor, finalized);
-      if (!is_finalized && blocks_.IsBitSet(predecessor->GetBlockId())) {
-        block->SetInLoop(this);
-        blocks_.SetBit(block_id);
-        finalized->SetBit(block_id);
-        is_finalized = true;
-      }
-    }
-  }
-
-  // All predecessors have been recursively visited. Mark finalized if not marked yet.
-  if (!is_finalized) {
-    finalized->SetBit(block_id);
-  }
-}
-
-void HLoopInformation::Populate() {
-  DCHECK_EQ(blocks_.NumSetBits(), 0u) << "Loop information has already been populated";
-  // Populate this loop: starting with the back edge, recursively add predecessors
-  // that are not already part of that loop. Set the header as part of the loop
-  // to end the recursion.
-  // This is a recursive implementation of the algorithm described in
-  // "Advanced Compiler Design & Implementation" (Muchnick) p192.
-  HGraph* graph = header_->GetGraph();
-  blocks_.SetBit(header_->GetBlockId());
-  header_->SetInLoop(this);
-
-  bool is_irreducible_loop = HasBackEdgeNotDominatedByHeader();
-
-  if (is_irreducible_loop) {
-    // Allocate memory from local ScopedArenaAllocator.
-    ScopedArenaAllocator allocator(graph->GetArenaStack());
-    ArenaBitVector visited(&allocator,
-                           graph->GetBlocks().size(),
-                           /* expandable= */ false,
-                           kArenaAllocGraphBuilder);
-    // Stop marking blocks at the loop header.
-    visited.SetBit(header_->GetBlockId());
-
-    for (HBasicBlock* back_edge : GetBackEdges()) {
-      PopulateIrreducibleRecursive(back_edge, &visited);
-    }
-  } else {
-    for (HBasicBlock* back_edge : GetBackEdges()) {
-      PopulateRecursive(back_edge);
-    }
-  }
-
-  if (!is_irreducible_loop && graph->IsCompilingOsr()) {
-    // When compiling in OSR mode, all loops in the compiled method may be entered
-    // from the interpreter. We treat this OSR entry point just like an extra entry
-    // to an irreducible loop, so we need to mark the method's loops as irreducible.
-    // This does not apply to inlined loops which do not act as OSR entry points.
-    if (suspend_check_ == nullptr) {
-      // Just building the graph in OSR mode, this loop is not inlined. We never build an
-      // inner graph in OSR mode as we can do OSR transition only from the outer method.
-      is_irreducible_loop = true;
-    } else {
-      // Look at the suspend check's environment to determine if the loop was inlined.
-      DCHECK(suspend_check_->HasEnvironment());
-      if (!suspend_check_->GetEnvironment()->IsFromInlinedInvoke()) {
-        is_irreducible_loop = true;
-      }
-    }
-  }
-  if (is_irreducible_loop) {
-    irreducible_ = true;
-    contains_irreducible_loop_ = true;
-    graph->SetHasIrreducibleLoops(true);
-  }
-  graph->SetHasLoops(true);
-}
-
-void HLoopInformation::PopulateInnerLoopUpwards(HLoopInformation* inner_loop) {
-  DCHECK(inner_loop->GetPreHeader()->GetLoopInformation() == this);
-  blocks_.Union(&inner_loop->blocks_);
-  HLoopInformation* outer_loop = GetPreHeader()->GetLoopInformation();
-  if (outer_loop != nullptr) {
-    outer_loop->PopulateInnerLoopUpwards(this);
-  }
-}
-
-HBasicBlock* HLoopInformation::GetPreHeader() const {
-  HBasicBlock* block = header_->GetPredecessors()[0];
-  DCHECK(irreducible_ || (block == header_->GetDominator()));
-  return block;
-}
-
-bool HLoopInformation::Contains(const HBasicBlock& block) const {
-  return blocks_.IsBitSet(block.GetBlockId());
-}
-
-bool HLoopInformation::IsIn(const HLoopInformation& other) const {
-  return other.blocks_.IsBitSet(header_->GetBlockId());
-}
-
-bool HLoopInformation::IsDefinedOutOfTheLoop(HInstruction* instruction) const {
-  return !blocks_.IsBitSet(instruction->GetBlock()->GetBlockId());
-}
-
-size_t HLoopInformation::GetLifetimeEnd() const {
-  size_t last_position = 0;
-  for (HBasicBlock* back_edge : GetBackEdges()) {
-    last_position = std::max(back_edge->GetLifetimeEnd(), last_position);
-  }
-  return last_position;
-}
-
-bool HLoopInformation::HasBackEdgeNotDominatedByHeader() const {
-  for (HBasicBlock* back_edge : GetBackEdges()) {
-    DCHECK(back_edge->GetDominator() != nullptr);
-    if (!header_->Dominates(back_edge)) {
-      return true;
-    }
-  }
-  return false;
-}
-
-bool HLoopInformation::DominatesAllBackEdges(HBasicBlock* block) {
-  for (HBasicBlock* back_edge : GetBackEdges()) {
-    if (!block->Dominates(back_edge)) {
-      return false;
-    }
-  }
-  return true;
-}
-
-
-bool HLoopInformation::HasExitEdge() const {
-  // Determine if this loop has at least one exit edge.
-  HBlocksInLoopReversePostOrderIterator it_loop(*this);
-  for (; !it_loop.Done(); it_loop.Advance()) {
-    for (HBasicBlock* successor : it_loop.Current()->GetSuccessors()) {
-      if (!Contains(*successor)) {
-        return true;
-      }
-    }
-  }
-  return false;
-}
-
 bool HBasicBlock::Dominates(const HBasicBlock* other) const {
   // Walk up the dominator tree from `other`, to find out if `this`
   // is an ancestor.
@@ -1252,7 +1051,7 @@ std::ostream& HInstruction::Dump(std::ostream& os, bool dump_args) {
   HGraphVisualizer::DumpInstruction(&os, graph, this);
   if (dump_args) {
     // Allocate memory from local ScopedArenaAllocator.
-    std::optional<MallocArenaPool> local_arena_pool;
+    std::optional<CallocArenaPool> local_arena_pool;
     std::optional<ArenaStack> local_arena_stack;
     if (UNLIKELY(graph == nullptr)) {
       local_arena_pool.emplace();
@@ -1309,87 +1108,6 @@ HInstruction* HInstruction::GetPreviousDisregardingMoves() const {
   return previous;
 }
 
-void HInstructionList::AddInstruction(HInstruction* instruction) {
-  if (first_instruction_ == nullptr) {
-    DCHECK(last_instruction_ == nullptr);
-    first_instruction_ = last_instruction_ = instruction;
-  } else {
-    DCHECK(last_instruction_ != nullptr);
-    last_instruction_->next_ = instruction;
-    instruction->previous_ = last_instruction_;
-    last_instruction_ = instruction;
-  }
-}
-
-void HInstructionList::InsertInstructionBefore(HInstruction* instruction, HInstruction* cursor) {
-  DCHECK(Contains(cursor));
-  if (cursor == first_instruction_) {
-    cursor->previous_ = instruction;
-    instruction->next_ = cursor;
-    first_instruction_ = instruction;
-  } else {
-    instruction->previous_ = cursor->previous_;
-    instruction->next_ = cursor;
-    cursor->previous_ = instruction;
-    instruction->previous_->next_ = instruction;
-  }
-}
-
-void HInstructionList::InsertInstructionAfter(HInstruction* instruction, HInstruction* cursor) {
-  DCHECK(Contains(cursor));
-  if (cursor == last_instruction_) {
-    cursor->next_ = instruction;
-    instruction->previous_ = cursor;
-    last_instruction_ = instruction;
-  } else {
-    instruction->next_ = cursor->next_;
-    instruction->previous_ = cursor;
-    cursor->next_ = instruction;
-    instruction->next_->previous_ = instruction;
-  }
-}
-
-void HInstructionList::RemoveInstruction(HInstruction* instruction) {
-  DCHECK_EQ(instruction->previous_ == nullptr, instruction == first_instruction_);
-  DCHECK_EQ(instruction->next_ == nullptr, instruction == last_instruction_);
-
-  if (instruction == first_instruction_) {
-    first_instruction_ = instruction->next_;
-  } else {
-    instruction->previous_->next_ = instruction->next_;
-  }
-
-  if (instruction == last_instruction_) {
-    last_instruction_ = instruction->previous_;
-  } else {
-    instruction->next_->previous_ = instruction->previous_;
-  }
-}
-
-bool HInstructionList::Contains(HInstruction* instruction) const {
-  for (HInstructionIterator it(*this); !it.Done(); it.Advance()) {
-    if (it.Current() == instruction) {
-      return true;
-    }
-  }
-  return false;
-}
-
-bool HInstructionList::FoundBefore(const HInstruction* instruction1,
-                                   const HInstruction* instruction2) const {
-  DCHECK_EQ(instruction1->GetBlock(), instruction2->GetBlock());
-  for (HInstructionIterator it(*this); !it.Done(); it.Advance()) {
-    if (it.Current() == instruction2) {
-      return false;
-    }
-    if (it.Current() == instruction1) {
-      return true;
-    }
-  }
-  LOG(FATAL) << "Did not find an order between two instructions of the same block.";
-  UNREACHABLE();
-}
-
 bool HInstruction::Dominates(HInstruction* other_instruction) const {
   return other_instruction == this || StrictlyDominates(other_instruction);
 }
@@ -1466,21 +1184,21 @@ void HInstruction::ReplaceUsesDominatedBy(HInstruction* dominator,
       return;
     }
     HGraph* graph = GetBlock()->GetGraph();
-    visited_blocks = ArenaBitVector::CreateFixedSize(
-        graph->GetAllocator(), graph->GetBlocks().size(), kArenaAllocMisc);
+    const size_t size = graph->GetBlocks().size();
+    visited_blocks = ArenaBitVector::CreateFixedSize(graph->GetAllocator(), size, kArenaAllocMisc);
     ScopedArenaAllocator allocator(graph->GetArenaStack());
-    ScopedArenaQueue<const HBasicBlock*> worklist(allocator.Adapter(kArenaAllocMisc));
-    worklist.push(dominator_block);
+    ScopedArenaVector<const HBasicBlock*> worklist(allocator.Adapter(kArenaAllocMisc));
+    worklist.reserve(size);
+    worklist.push_back(dominator_block);
+    visited_blocks.SetBit(dominator_block->GetBlockId());
 
     while (!worklist.empty()) {
-      const HBasicBlock* current = worklist.front();
-      worklist.pop();
-      visited_blocks.SetBit(current->GetBlockId());
+      const HBasicBlock* current = worklist.back();
+      worklist.pop_back();
       for (HBasicBlock* dominated : current->GetDominatedBlocks()) {
-        if (visited_blocks.IsBitSet(dominated->GetBlockId())) {
-          continue;
-        }
-        worklist.push(dominated);
+        DCHECK(!visited_blocks.IsBitSet(dominated->GetBlockId()));
+        visited_blocks.SetBit(dominated->GetBlockId());
+        worklist.push_back(dominated);
       }
     }
   };
@@ -1716,15 +1434,6 @@ HInstruction* HConstructorFence::GetAssociatedAllocation(bool ignore_inputs) {
   return nullptr;
 }
 
-#define DEFINE_ACCEPT(name, super)                                             \
-void H##name::Accept(HGraphVisitor* visitor) {                                 \
-  visitor->Visit##name(this);                                                  \
-}
-
-FOR_EACH_CONCRETE_INSTRUCTION(DEFINE_ACCEPT)
-
-#undef DEFINE_ACCEPT
-
 void HGraphVisitor::VisitInsertionOrder() {
   for (HBasicBlock* block : graph_->GetActiveBlocks()) {
     VisitBasicBlock(block);
@@ -1743,23 +1452,23 @@ void HGraphVisitor::VisitBasicBlock(HBasicBlock* block) {
 }
 
 void HGraphVisitor::VisitPhis(HBasicBlock* block) {
-  for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
     DCHECK(it.Current()->IsPhi());
     VisitPhi(it.Current()->AsPhi());
   }
 }
 
 void HGraphVisitor::VisitNonPhiInstructions(HBasicBlock* block) {
-  for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
     DCHECK(!it.Current()->IsPhi());
-    it.Current()->Accept(this);
+    Dispatch(it.Current());
   }
 }
 
 void HGraphVisitor::VisitNonPhiInstructionsHandleChanges(HBasicBlock* block) {
-  for (HInstructionIteratorHandleChanges it(block->GetInstructions()); !it.Done(); it.Advance()) {
+  for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
     DCHECK(!it.Current()->IsPhi());
-    it.Current()->Accept(this);
+    Dispatch(it.Current());
   }
 }
 
@@ -2133,13 +1842,11 @@ void HInstruction::MoveBeforeFirstUserAndOutOfLoops() {
   MoveBefore(insert_pos);
 }
 
-HBasicBlock* HBasicBlock::SplitBefore(HInstruction* cursor, bool require_graph_not_in_ssa_form) {
-  DCHECK_IMPLIES(require_graph_not_in_ssa_form, !graph_->IsInSsaForm())
-      << "Support for SSA form not implemented.";
+HBasicBlock* HBasicBlock::SplitBefore(HInstruction* cursor) {
   DCHECK_EQ(cursor->GetBlock(), this);
 
   HBasicBlock* new_block =
-      new (GetGraph()->GetAllocator()) HBasicBlock(GetGraph(), cursor->GetDexPc());
+      HBasicBlock::Create(GetGraph()->GetAllocator(), GetGraph(), cursor->GetDexPc());
   new_block->instructions_.first_instruction_ = cursor;
   new_block->instructions_.last_instruction_ = instructions_.last_instruction_;
   instructions_.last_instruction_ = cursor->previous_;
@@ -2168,7 +1875,8 @@ HBasicBlock* HBasicBlock::CreateImmediateDominator() {
   DCHECK(!graph_->IsInSsaForm()) << "Support for SSA form not implemented.";
   DCHECK(!IsCatchBlock()) << "Support for updating try/catch information not implemented.";
 
-  HBasicBlock* new_block = new (GetGraph()->GetAllocator()) HBasicBlock(GetGraph(), GetDexPc());
+  HBasicBlock* new_block =
+      HBasicBlock::Create(GetGraph()->GetAllocator(), GetGraph(), GetDexPc());
 
   for (HBasicBlock* predecessor : GetPredecessors()) {
     predecessor->successors_[predecessor->GetSuccessorIndexOf(this)] = new_block;
@@ -2185,7 +1893,7 @@ HBasicBlock* HBasicBlock::SplitBeforeForInlining(HInstruction* cursor) {
   DCHECK_EQ(cursor->GetBlock(), this);
 
   HBasicBlock* new_block =
-      new (GetGraph()->GetAllocator()) HBasicBlock(GetGraph(), cursor->GetDexPc());
+      HBasicBlock::Create(GetGraph()->GetAllocator(), GetGraph(), cursor->GetDexPc());
   new_block->instructions_.first_instruction_ = cursor;
   new_block->instructions_.last_instruction_ = instructions_.last_instruction_;
   instructions_.last_instruction_ = cursor->previous_;
@@ -2217,7 +1925,8 @@ HBasicBlock* HBasicBlock::SplitAfterForInlining(HInstruction* cursor) {
   DCHECK_NE(instructions_.last_instruction_, cursor);
   DCHECK_EQ(cursor->GetBlock(), this);
 
-  HBasicBlock* new_block = new (GetGraph()->GetAllocator()) HBasicBlock(GetGraph(), GetDexPc());
+  HBasicBlock* new_block =
+      HBasicBlock::Create(GetGraph()->GetAllocator(), GetGraph(), GetDexPc());
   new_block->instructions_.first_instruction_ = cursor->GetNext();
   new_block->instructions_.last_instruction_ = instructions_.last_instruction_;
   cursor->next_->previous_ = nullptr;
@@ -2258,7 +1967,7 @@ const HTryBoundary* HBasicBlock::ComputeTryEntryOfSuccessors() const {
 }
 
 bool HBasicBlock::HasThrowingInstructions() const {
-  for (HInstructionIterator it(GetInstructions()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(GetInstructions()); !it.Done(); it.Advance()) {
     if (it.Current()->CanThrow()) {
       return true;
     }
@@ -2348,60 +2057,6 @@ bool HTryBoundary::HasSameExceptionHandlersAs(const HTryBoundary& other) const {
   return true;
 }
 
-size_t HInstructionList::CountSize() const {
-  size_t size = 0;
-  HInstruction* current = first_instruction_;
-  for (; current != nullptr; current = current->GetNext()) {
-    size++;
-  }
-  return size;
-}
-
-void HInstructionList::SetBlockOfInstructions(HBasicBlock* block) const {
-  for (HInstruction* current = first_instruction_;
-       current != nullptr;
-       current = current->GetNext()) {
-    current->SetBlock(block);
-  }
-}
-
-void HInstructionList::AddAfter(HInstruction* cursor, const HInstructionList& instruction_list) {
-  DCHECK(Contains(cursor));
-  if (!instruction_list.IsEmpty()) {
-    if (cursor == last_instruction_) {
-      last_instruction_ = instruction_list.last_instruction_;
-    } else {
-      cursor->next_->previous_ = instruction_list.last_instruction_;
-    }
-    instruction_list.last_instruction_->next_ = cursor->next_;
-    cursor->next_ = instruction_list.first_instruction_;
-    instruction_list.first_instruction_->previous_ = cursor;
-  }
-}
-
-void HInstructionList::AddBefore(HInstruction* cursor, const HInstructionList& instruction_list) {
-  DCHECK(Contains(cursor));
-  if (!instruction_list.IsEmpty()) {
-    if (cursor == first_instruction_) {
-      first_instruction_ = instruction_list.first_instruction_;
-    } else {
-      cursor->previous_->next_ = instruction_list.first_instruction_;
-    }
-    instruction_list.last_instruction_->next_ = cursor;
-    instruction_list.first_instruction_->previous_ = cursor->previous_;
-    cursor->previous_ = instruction_list.last_instruction_;
-  }
-}
-
-void HInstructionList::Add(const HInstructionList& instruction_list) {
-  if (IsEmpty()) {
-    first_instruction_ = instruction_list.first_instruction_;
-    last_instruction_ = instruction_list.last_instruction_;
-  } else {
-    AddAfter(last_instruction_, instruction_list);
-  }
-}
-
 void HBasicBlock::DisconnectAndDelete() {
   // Dominators must be removed after all the blocks they dominate. This way
   // a loop header is removed last, a requirement for correct loop information
@@ -2420,8 +2075,8 @@ void HBasicBlock::DisconnectAndDelete() {
     // was their dominator.
     // Note that we do not remove `this` from `loop_info` as it is unreachable.
     DCHECK(!loop_info->IsIrreducible());
-    DCHECK_EQ(loop_info->GetBlocks().NumSetBits(), 1u);
-    DCHECK_EQ(static_cast<uint32_t>(loop_info->GetBlocks().GetHighestBitSet()), GetBlockId());
+    DCHECK_EQ(loop_info->GetBlockMask().NumSetBits(), 1u);
+    DCHECK_EQ(static_cast<uint32_t>(loop_info->GetBlockMask().GetHighestBitSet()), GetBlockId());
     loop_update_start = loop_info->GetPreHeader();
   }
 
@@ -2527,13 +2182,15 @@ void HBasicBlock::DisconnectFromSuccessors(BitVectorView<const size_t> visited)
       if (successor->predecessors_.size() == 1u) {
         // The successor has just one predecessor left. Replace phis with the only
         // remaining input.
-        for (HInstructionIterator phi_it(successor->GetPhis()); !phi_it.Done(); phi_it.Advance()) {
+        for (HInstructionIteratorPrefetchNext phi_it(successor->GetPhis()); !phi_it.Done();
+             phi_it.Advance()) {
           HPhi* phi = phi_it.Current()->AsPhi();
           phi->ReplaceWith(phi->InputAt(1 - this_index));
           successor->RemovePhi(phi);
         }
       } else {
-        for (HInstructionIterator phi_it(successor->GetPhis()); !phi_it.Done(); phi_it.Advance()) {
+        for (HInstructionIteratorPrefetchNext phi_it(successor->GetPhis()); !phi_it.Done();
+             phi_it.Advance()) {
           phi_it.Current()->AsPhi()->RemoveInputAt(this_index);
         }
       }
@@ -2543,7 +2200,7 @@ void HBasicBlock::DisconnectFromSuccessors(BitVectorView<const size_t> visited)
 }
 
 void HBasicBlock::RemoveCatchPhiUsesAndInstruction(bool building_dominator_tree) {
-  for (HBackwardInstructionIterator it(GetInstructions()); !it.Done(); it.Advance()) {
+  for (HBackwardInstructionIteratorPrefetchNext it(GetInstructions()); !it.Done(); it.Advance()) {
     HInstruction* insn = it.Current();
     RemoveCatchPhiUsesOfDeadInstruction(insn);
 
@@ -2556,7 +2213,7 @@ void HBasicBlock::RemoveCatchPhiUsesAndInstruction(bool building_dominator_tree)
     }
     RemoveInstruction(insn, /* ensure_safety= */ !building_dominator_tree);
   }
-  for (HInstructionIterator it(GetPhis()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(GetPhis()); !it.Done(); it.Advance()) {
     HPhi* insn = it.Current()->AsPhi();
     RemoveCatchPhiUsesOfDeadInstruction(insn);
 
@@ -2740,7 +2397,7 @@ HInstruction* HGraph::InlineInto(HGraph* outer_graph, HInvoke* invoke) {
   {
     // Skip the entry block, we do not need to update the entry's suspend check.
     for (HBasicBlock* block : GetReversePostOrderSkipEntryBlock()) {
-      for (HInstructionIterator instr_it(block->GetInstructions());
+      for (HInstructionIteratorPrefetchNext instr_it(block->GetInstructions());
            !instr_it.Done();
            instr_it.Advance()) {
         HInstruction* current = instr_it.Current();
@@ -2980,7 +2637,8 @@ HInstruction* HGraph::InlineInto(HGraph* outer_graph, HInvoke* invoke) {
   // We must do this after the other blocks have been inlined, otherwise ids of
   // constants could overlap with the inner graph.
   size_t parameter_index = 0;
-  for (HInstructionIterator it(entry_block_->GetInstructions()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(entry_block_->GetInstructions()); !it.Done();
+       it.Advance()) {
     HInstruction* current = it.Current();
     HInstruction* replacement = nullptr;
     if (current->IsNullConstant()) {
@@ -3042,10 +2700,10 @@ void HGraph::TransformLoopHeaderForBCE(HBasicBlock* header) {
   HBasicBlock* old_pre_header = header->GetDominator();
 
   // Need extra block to avoid critical edge.
-  HBasicBlock* if_block = new (allocator_) HBasicBlock(this, header->GetDexPc());
-  HBasicBlock* true_block = new (allocator_) HBasicBlock(this, header->GetDexPc());
-  HBasicBlock* false_block = new (allocator_) HBasicBlock(this, header->GetDexPc());
-  HBasicBlock* new_pre_header = new (allocator_) HBasicBlock(this, header->GetDexPc());
+  HBasicBlock* if_block = HBasicBlock::Create(allocator_, this, header->GetDexPc());
+  HBasicBlock* true_block = HBasicBlock::Create(allocator_, this, header->GetDexPc());
+  HBasicBlock* false_block = HBasicBlock::Create(allocator_, this, header->GetDexPc());
+  HBasicBlock* new_pre_header = HBasicBlock::Create(allocator_, this, header->GetDexPc());
   AddBlock(if_block);
   AddBlock(true_block);
   AddBlock(false_block);
@@ -3102,9 +2760,9 @@ HBasicBlock* HGraph::TransformLoopForVectorization(HBasicBlock* header,
   HLoopInformation* loop = header->GetLoopInformation();
 
   // Add new loop blocks.
-  HBasicBlock* new_pre_header = new (allocator_) HBasicBlock(this, header->GetDexPc());
-  HBasicBlock* new_header = new (allocator_) HBasicBlock(this, header->GetDexPc());
-  HBasicBlock* new_body = new (allocator_) HBasicBlock(this, header->GetDexPc());
+  HBasicBlock* new_pre_header = HBasicBlock::Create(allocator_, this, header->GetDexPc());
+  HBasicBlock* new_header = HBasicBlock::Create(allocator_, this, header->GetDexPc());
+  HBasicBlock* new_body = HBasicBlock::Create(allocator_, this, header->GetDexPc());
   AddBlock(new_pre_header);
   AddBlock(new_header);
   AddBlock(new_body);
@@ -3145,7 +2803,7 @@ HBasicBlock* HGraph::TransformLoopForVectorization(HBasicBlock* header,
       loop->GetSuspendCheck()->GetEnvironment(), header);
 
   // Update loop information.
-  new_header->AddBackEdge(new_body);
+  AddBackEdge(new_header, new_body);
   new_header->GetLoopInformation()->SetSuspendCheck(suspend_check);
   new_header->GetLoopInformation()->Populate();
   new_pre_header->SetLoopInformation(loop->GetPreHeader()->GetLoopInformation());  // outward
diff --git a/compiler/optimizing/nodes.h b/compiler/optimizing/nodes.h
index 7b86695670..2c3b54aa7a 100644
--- a/compiler/optimizing/nodes.h
+++ b/compiler/optimizing/nodes.h
@@ -46,8 +46,10 @@
 #include "entrypoints/quick/quick_entrypoints_enum.h"
 #include "handle.h"
 #include "handle_cache.h"
+#include "instruction_list.h"
 #include "intrinsics_enum.h"
 #include "locations.h"
+#include "loop_information.h"
 #include "mirror/class.h"
 #include "mirror/method_type.h"
 #include "offsets.h"
@@ -93,7 +95,6 @@ static const int kDefaultNumberOfSuccessors = 2;
 static const int kDefaultNumberOfPredecessors = 2;
 static const int kDefaultNumberOfExceptionalPredecessors = 0;
 static const int kDefaultNumberOfDominatedBlocks = 1;
-static const int kDefaultNumberOfBackEdges = 1;
 
 // The maximum (meaningful) distance (31) that can be used in an integer shift/rotate operation.
 static constexpr int32_t kMaxIntShiftDistance = 0x1f;
@@ -153,53 +154,6 @@ static inline typename std::make_unsigned<T>::type MakeUnsigned(T x) {
   return static_cast<typename std::make_unsigned<T>::type>(x);
 }
 
-class HInstructionList : public ValueObject {
- public:
-  HInstructionList() : first_instruction_(nullptr), last_instruction_(nullptr) {}
-
-  void AddInstruction(HInstruction* instruction);
-  void RemoveInstruction(HInstruction* instruction);
-
-  // Insert `instruction` before/after an existing instruction `cursor`.
-  void InsertInstructionBefore(HInstruction* instruction, HInstruction* cursor);
-  void InsertInstructionAfter(HInstruction* instruction, HInstruction* cursor);
-
-  // Return true if this list contains `instruction`.
-  bool Contains(HInstruction* instruction) const;
-
-  // Return true if `instruction1` is found before `instruction2` in
-  // this instruction list and false otherwise.  Abort if none
-  // of these instructions is found.
-  bool FoundBefore(const HInstruction* instruction1,
-                   const HInstruction* instruction2) const;
-
-  bool IsEmpty() const { return first_instruction_ == nullptr; }
-  void Clear() { first_instruction_ = last_instruction_ = nullptr; }
-
-  // Update the block of all instructions to be `block`.
-  void SetBlockOfInstructions(HBasicBlock* block) const;
-
-  void AddAfter(HInstruction* cursor, const HInstructionList& instruction_list);
-  void AddBefore(HInstruction* cursor, const HInstructionList& instruction_list);
-  void Add(const HInstructionList& instruction_list);
-
-  // Return the number of instructions in the list. This is an expensive operation.
-  size_t CountSize() const;
-
- private:
-  HInstruction* first_instruction_;
-  HInstruction* last_instruction_;
-
-  friend class HBasicBlock;
-  friend class HGraph;
-  friend class HInstruction;
-  friend class HInstructionIterator;
-  friend class HInstructionIteratorHandleChanges;
-  friend class HBackwardInstructionIterator;
-
-  DISALLOW_COPY_AND_ASSIGN(HInstructionList);
-};
-
 // Control-flow graph of a method. Contains a list of basic blocks.
 class HGraph : public ArenaObject<kArenaAllocGraph> {
  public:
@@ -341,10 +295,6 @@ class HGraph : public ArenaObject<kArenaAllocGraph> {
 
   void SplitCriticalEdge(HBasicBlock* block, HBasicBlock* successor);
 
-  // Splits the edge between `block` and `successor` and then updates the graph's RPO to keep
-  // consistency without recomputing the whole graph.
-  HBasicBlock* SplitEdgeAndUpdateRPO(HBasicBlock* block, HBasicBlock* successor);
-
   void OrderLoopHeaderPredecessors(HBasicBlock* header);
 
   // Transform a loop into a format with a single preheader.
@@ -693,131 +643,10 @@ class HGraph : public ArenaObject<kArenaAllocGraph> {
   DISALLOW_COPY_AND_ASSIGN(HGraph);
 };
 
-class HLoopInformation : public ArenaObject<kArenaAllocLoopInfo> {
- public:
-  HLoopInformation(HBasicBlock* header, HGraph* graph)
-      : header_(header),
-        suspend_check_(nullptr),
-        irreducible_(false),
-        contains_irreducible_loop_(false),
-        back_edges_(graph->GetAllocator()->Adapter(kArenaAllocLoopInfoBackEdges)),
-        // Make bit vector growable, as the number of blocks may change.
-        blocks_(graph->GetAllocator(),
-                graph->GetBlocks().size(),
-                true,
-                kArenaAllocLoopInfoBackEdges) {
-    back_edges_.reserve(kDefaultNumberOfBackEdges);
-  }
-
-  bool IsIrreducible() const { return irreducible_; }
-  bool ContainsIrreducibleLoop() const { return contains_irreducible_loop_; }
-
-  void Dump(std::ostream& os);
-
-  HBasicBlock* GetHeader() const {
-    return header_;
-  }
-
-  void SetHeader(HBasicBlock* block) {
-    header_ = block;
-  }
-
-  HSuspendCheck* GetSuspendCheck() const { return suspend_check_; }
-  void SetSuspendCheck(HSuspendCheck* check) { suspend_check_ = check; }
-  bool HasSuspendCheck() const { return suspend_check_ != nullptr; }
-
-  void AddBackEdge(HBasicBlock* back_edge) {
-    back_edges_.push_back(back_edge);
-  }
-
-  void RemoveBackEdge(HBasicBlock* back_edge) {
-    RemoveElement(back_edges_, back_edge);
-  }
-
-  bool IsBackEdge(const HBasicBlock& block) const {
-    return ContainsElement(back_edges_, &block);
-  }
-
-  size_t NumberOfBackEdges() const {
-    return back_edges_.size();
-  }
-
-  HBasicBlock* GetPreHeader() const;
-
-  const ArenaVector<HBasicBlock*>& GetBackEdges() const {
-    return back_edges_;
-  }
-
-  // Returns the lifetime position of the back edge that has the
-  // greatest lifetime position.
-  size_t GetLifetimeEnd() const;
-
-  void ReplaceBackEdge(HBasicBlock* existing, HBasicBlock* new_back_edge) {
-    ReplaceElement(back_edges_, existing, new_back_edge);
-  }
-
-  // Finds blocks that are part of this loop.
-  void Populate();
-
-  // Updates blocks population of the loop and all of its outer' ones recursively after the
-  // population of the inner loop is updated.
-  void PopulateInnerLoopUpwards(HLoopInformation* inner_loop);
-
-  // Returns whether this loop information contains `block`.
-  // Note that this loop information *must* be populated before entering this function.
-  bool Contains(const HBasicBlock& block) const;
-
-  // Returns whether this loop information is an inner loop of `other`.
-  // Note that `other` *must* be populated before entering this function.
-  bool IsIn(const HLoopInformation& other) const;
-
-  // Returns true if instruction is not defined within this loop.
-  bool IsDefinedOutOfTheLoop(HInstruction* instruction) const;
-
-  const ArenaBitVector& GetBlocks() const { return blocks_; }
-
-  void Add(HBasicBlock* block);
-  void Remove(HBasicBlock* block);
-
-  void ClearAllBlocks() {
-    blocks_.ClearAllBits();
-  }
-
-  bool HasBackEdgeNotDominatedByHeader() const;
-
-  bool IsPopulated() const {
-    return blocks_.GetHighestBitSet() != -1;
-  }
-
-  bool DominatesAllBackEdges(HBasicBlock* block);
-
-  bool HasExitEdge() const;
-
-  // Resets back edge and blocks-in-loop data.
-  void ResetBasicBlockData() {
-    back_edges_.clear();
-    ClearAllBlocks();
-  }
-
- private:
-  // Internal recursive implementation of `Populate`.
-  void PopulateRecursive(HBasicBlock* block);
-  void PopulateIrreducibleRecursive(HBasicBlock* block, ArenaBitVector* finalized);
-
-  HBasicBlock* header_;
-  HSuspendCheck* suspend_check_;
-  bool irreducible_;
-  bool contains_irreducible_loop_;
-  ArenaVector<HBasicBlock*> back_edges_;
-  ArenaBitVector blocks_;
-
-  DISALLOW_COPY_AND_ASSIGN(HLoopInformation);
-};
-
 // Stores try/catch information for basic blocks.
 // Note that HGraph is constructed so that catch blocks cannot simultaneously
 // be try blocks.
-class TryCatchInformation : public ArenaObject<kArenaAllocTryCatchInfo> {
+class TryCatchInformation final : public ArenaObject<kArenaAllocTryCatchInfo> {
  public:
   // Try block information constructor.
   explicit TryCatchInformation(const HTryBoundary& try_entry)
@@ -878,23 +707,10 @@ static constexpr uint32_t kInvalidBlockId = static_cast<uint32_t>(-1);
 // as a double linked list. Each block knows its predecessors and
 // successors.
 
-class HBasicBlock : public ArenaObject<kArenaAllocBasicBlock> {
+class HBasicBlock final : public ArenaObject<kArenaAllocBasicBlock> {
  public:
-  explicit HBasicBlock(HGraph* graph, uint32_t dex_pc = kNoDexPc)
-      : graph_(graph),
-        predecessors_(graph->GetAllocator()->Adapter(kArenaAllocPredecessors)),
-        successors_(graph->GetAllocator()->Adapter(kArenaAllocSuccessors)),
-        loop_information_(nullptr),
-        dominator_(nullptr),
-        dominated_blocks_(graph->GetAllocator()->Adapter(kArenaAllocDominated)),
-        block_id_(kInvalidBlockId),
-        dex_pc_(dex_pc),
-        lifetime_start_(kNoLifetime),
-        lifetime_end_(kNoLifetime),
-        try_catch_information_(nullptr) {
-    predecessors_.reserve(kDefaultNumberOfPredecessors);
-    successors_.reserve(kDefaultNumberOfSuccessors);
-    dominated_blocks_.reserve(kDefaultNumberOfDominatedBlocks);
+  static HBasicBlock* Create(ArenaAllocator* allocator, HGraph* graph, uint32_t dex_pc = kNoDexPc) {
+    return new (allocator) HBasicBlock(allocator, graph, dex_pc);
   }
 
   const ArenaVector<HBasicBlock*>& GetPredecessors() const {
@@ -933,34 +749,6 @@ class HBasicBlock : public ArenaObject<kArenaAllocBasicBlock> {
   bool IsSingleReturnOrReturnVoidAllowingPhis() const;
   bool IsSingleTryBoundary() const;
 
-  // Returns true if this block emits nothing but a jump.
-  bool IsSingleJump() const {
-    HLoopInformation* loop_info = GetLoopInformation();
-    return (IsSingleGoto() || IsSingleTryBoundary())
-           // Back edges generate a suspend check.
-           && (loop_info == nullptr || !loop_info->IsBackEdge(*this));
-  }
-
-  void AddBackEdge(HBasicBlock* back_edge) {
-    if (loop_information_ == nullptr) {
-      loop_information_ = new (graph_->GetAllocator()) HLoopInformation(this, graph_);
-    }
-    DCHECK_EQ(loop_information_->GetHeader(), this);
-    loop_information_->AddBackEdge(back_edge);
-  }
-
-  // Registers a back edge; if the block was not a loop header before the call associates a newly
-  // created loop info with it.
-  //
-  // Used in SuperblockCloner to preserve LoopInformation object instead of reseting loop
-  // info for all blocks during back edges recalculation.
-  void AddBackEdgeWhileUpdating(HBasicBlock* back_edge) {
-    if (loop_information_ == nullptr || loop_information_->GetHeader() != this) {
-      loop_information_ = new (graph_->GetAllocator()) HLoopInformation(this, graph_);
-    }
-    loop_information_->AddBackEdge(back_edge);
-  }
-
   HGraph* GetGraph() const { return graph_; }
   void SetGraph(HGraph* graph) { graph_ = graph; }
 
@@ -982,10 +770,6 @@ class HBasicBlock : public ArenaObject<kArenaAllocBasicBlock> {
 
   void ClearDominanceInformation();
 
-  int NumberOfBackEdges() const {
-    return IsLoopHeader() ? loop_information_->NumberOfBackEdges() : 0;
-  }
-
   HInstruction* GetFirstInstruction() const { return instructions_.first_instruction_; }
   HInstruction* GetLastInstruction() const { return instructions_.last_instruction_; }
   const HInstructionList& GetInstructions() const { return instructions_; }
@@ -1089,7 +873,7 @@ class HBasicBlock : public ArenaObject<kArenaAllocBasicBlock> {
   // graph, create a Goto at the end of the former block and will create an edge
   // between the blocks. It will not, however, update the reverse post order or
   // loop and try/catch information.
-  HBasicBlock* SplitBefore(HInstruction* cursor, bool require_graph_not_in_ssa_form = true);
+  HBasicBlock* SplitBefore(HInstruction* cursor);
 
   // Split the block into two blocks just before `cursor`. Returns the newly
   // created block. Note that this method just updates raw block information,
@@ -1180,26 +964,6 @@ class HBasicBlock : public ArenaObject<kArenaAllocBasicBlock> {
     return loop_information_;
   }
 
-  // Set the loop_information_ on this block. Overrides the current
-  // loop_information if it is an outer loop of the passed loop information.
-  // Note that this method is called while creating the loop information.
-  void SetInLoop(HLoopInformation* info) {
-    if (IsLoopHeader()) {
-      // Nothing to do. This just means `info` is an outer loop.
-    } else if (!IsInLoop()) {
-      loop_information_ = info;
-    } else if (loop_information_->Contains(*info->GetHeader())) {
-      // Block is currently part of an outer loop. Make it part of this inner loop.
-      // Note that a non loop header having a loop information means this loop information
-      // has already been populated
-      loop_information_ = info;
-    } else {
-      // Block is part of an inner loop. Do not update the loop information.
-      // Note that we cannot do the check `info->Contains(loop_information_)->GetHeader()`
-      // at this point, because this method is being called while populating `info`.
-    }
-  }
-
   // Raw update of the loop information.
   void SetLoopInformation(HLoopInformation* info) {
     loop_information_ = info;
@@ -1244,6 +1008,23 @@ class HBasicBlock : public ArenaObject<kArenaAllocBasicBlock> {
   bool HasSinglePhi() const;
 
  private:
+  HBasicBlock(ArenaAllocator* allocator, HGraph* graph, uint32_t dex_pc)
+      : graph_(graph),
+        predecessors_(allocator->Adapter(kArenaAllocPredecessors)),
+        successors_(allocator->Adapter(kArenaAllocSuccessors)),
+        loop_information_(nullptr),
+        dominator_(nullptr),
+        dominated_blocks_(allocator->Adapter(kArenaAllocDominated)),
+        block_id_(kInvalidBlockId),
+        dex_pc_(dex_pc),
+        lifetime_start_(kNoLifetime),
+        lifetime_end_(kNoLifetime),
+        try_catch_information_(nullptr) {
+    predecessors_.reserve(kDefaultNumberOfPredecessors);
+    successors_.reserve(kDefaultNumberOfSuccessors);
+    dominated_blocks_.reserve(kDefaultNumberOfDominatedBlocks);
+  }
+
   HGraph* graph_;
   ArenaVector<HBasicBlock*> predecessors_;
   ArenaVector<HBasicBlock*> successors_;
@@ -1267,31 +1048,6 @@ class HBasicBlock : public ArenaObject<kArenaAllocBasicBlock> {
   DISALLOW_COPY_AND_ASSIGN(HBasicBlock);
 };
 
-// Iterates over the LoopInformation of all loops which contain 'block'
-// from the innermost to the outermost.
-class HLoopInformationOutwardIterator : public ValueObject {
- public:
-  explicit HLoopInformationOutwardIterator(const HBasicBlock& block)
-      : current_(block.GetLoopInformation()) {}
-
-  bool Done() const { return current_ == nullptr; }
-
-  void Advance() {
-    DCHECK(!Done());
-    current_ = current_->GetPreHeader()->GetLoopInformation();
-  }
-
-  HLoopInformation* Current() const {
-    DCHECK(!Done());
-    return current_;
-  }
-
- private:
-  HLoopInformation* current_;
-
-  DISALLOW_COPY_AND_ASSIGN(HLoopInformationOutwardIterator);
-};
-
 #define FOR_EACH_CONCRETE_INSTRUCTION_SCALAR_COMMON(M)                  \
   M(Above, Condition)                                                   \
   M(AboveOrEqual, Condition)                                            \
@@ -1519,8 +1275,7 @@ FOR_EACH_INSTRUCTION(FORWARD_DECLARATION)
   HInstruction* Clone(ArenaAllocator* arena) const override {             \
     DCHECK(IsClonable());                                                 \
     return new (arena) H##type(*this);                                    \
-  }                                                                       \
-  void Accept(HGraphVisitor* visitor) override
+  }
 
 #define DECLARE_ABSTRACT_INSTRUCTION(type)                              \
   private:                                                              \
@@ -1559,7 +1314,7 @@ using HUseList = IntrusiveForwardList<HUseListNode<T>>;
 // instructions they use and pointers to the corresponding HUseListNodes kept
 // by the used instructions.
 template <typename T>
-class HUserRecord : public ValueObject {
+class HUserRecord final : public ValueObject {
  public:
   HUserRecord() : instruction_(nullptr), before_use_node_() {}
   explicit HUserRecord(HInstruction* instruction) : instruction_(instruction), before_use_node_() {}
@@ -1642,7 +1397,7 @@ using HConstInputsRef = TransformArrayRef<const HUserRecord<HInstruction*>, HInp
  * Note that, to ease the implementation, 'changes' bits are least significant
  * bits, while 'dependency' bits are most significant bits.
  */
-class SideEffects : public ValueObject {
+class SideEffects final : public ValueObject {
  public:
   SideEffects() : flags_(0) {}
 
@@ -1858,7 +1613,7 @@ class SideEffects : public ValueObject {
 };
 
 // A HEnvironment object contains the values of virtual registers at a given location.
-class HEnvironment : public ArenaObject<kArenaAllocEnvironment> {
+class HEnvironment final : public ArenaObject<kArenaAllocEnvironment> {
  public:
   static HEnvironment* Create(ArenaAllocator* allocator,
                               size_t number_of_vregs,
@@ -2019,7 +1774,7 @@ class HEnvironment : public ArenaObject<kArenaAllocEnvironment> {
 std::ostream& operator<<(std::ostream& os, const HInstruction& rhs);
 
 // Iterates over the Environments
-class HEnvironmentIterator : public ValueObject {
+class HEnvironmentIterator final : public ValueObject {
  public:
   using iterator_category = std::forward_iterator_tag;
   using value_type = HEnvironment*;
@@ -2118,9 +1873,6 @@ class HInstruction : public ArenaObject<kArenaAllocInstruction> {
   bool IsInBlock() const { return block_ != nullptr; }
   bool IsInLoop() const { return block_->IsInLoop(); }
   bool IsLoopHeaderPhi() const { return IsPhi() && block_->IsLoopHeader(); }
-  bool IsIrreducibleLoopHeaderPhi() const {
-    return IsLoopHeaderPhi() && GetBlock()->GetLoopInformation()->IsIrreducible();
-  }
 
   virtual ArrayRef<HUserRecord<HInstruction*>> GetInputRecords() = 0;
 
@@ -2154,7 +1906,6 @@ class HInstruction : public ArenaObject<kArenaAllocInstruction> {
     SetRawInputRecordAt(index, HUserRecord<HInstruction*>(input));
   }
 
-  virtual void Accept(HGraphVisitor* visitor) = 0;
   virtual const char* DebugName() const = 0;
 
   DataType::Type GetType() const {
@@ -2168,7 +1919,24 @@ class HInstruction : public ArenaObject<kArenaAllocInstruction> {
 
   uint32_t GetDexPc() const { return dex_pc_; }
 
-  virtual bool IsControlFlow() const { return false; }
+  bool IsControlFlow() const {
+    switch (GetKind()) {
+      case kExit:
+      case kGoto:
+      case kIf:
+      case kPackedSwitch:
+      case kReturn:
+      case kReturnVoid:
+      case kThrow:
+      case kTryBoundary:
+#if defined(ART_ENABLE_CODEGEN_x86)
+      case kX86PackedSwitch:
+#endif
+        return true;
+      default:
+        return false;
+    }
+  }
 
   // Can the instruction throw?
   // TODO: We should rename to CanVisiblyThrow, as some instructions (like HNewInstance),
@@ -2294,18 +2062,29 @@ class HInstruction : public ArenaObject<kArenaAllocInstruction> {
   }
 
   bool IsRemovable() const {
-    return
-        !DoesAnyWrite() &&
-        // TODO(solanes): Merge calls from IsSuspendCheck to IsControlFlow into one that doesn't
-        // do virtual dispatching.
-        !IsSuspendCheck() &&
-        !IsNop() &&
-        !IsParameterValue() &&
-        // If we added an explicit barrier then we should keep it.
-        !IsMemoryBarrier() &&
-        !IsConstructorFence() &&
-        !IsControlFlow() &&
-        !CanThrow();
+    switch (GetKind()) {
+      case kConstructorFence:
+      case kMemoryBarrier:
+      case kNop:
+      case kParameterValue:
+      case kSuspendCheck:
+      // Control flow HInstructions. This has to be kept in sync with IsControlFlow.
+      case kExit:
+      case kGoto:
+      case kIf:
+      case kPackedSwitch:
+      case kReturn:
+      case kReturnVoid:
+      case kThrow:
+      case kTryBoundary:
+#if defined(ART_ENABLE_CODEGEN_x86)
+      case kX86PackedSwitch:
+#endif
+        return false;
+      default:
+        DCHECK(!IsControlFlow());
+        return !DoesAnyWrite() && !CanThrow();
+    }
   }
 
   bool IsDeadAndRemovable() const {
@@ -2691,9 +2470,9 @@ template <typename InnerIter> struct HSTLInstructionIterator;
 // Iterates over the instructions, while preserving the next instruction
 // in case the current instruction gets removed from the list by the user
 // of this iterator.
-class HInstructionIterator : public ValueObject {
+class HInstructionIteratorPrefetchNext final : public ValueObject {
  public:
-  explicit HInstructionIterator(const HInstructionList& instructions)
+  explicit HInstructionIteratorPrefetchNext(const HInstructionList& instructions)
       : instruction_(instructions.first_instruction_) {
     next_ = Done() ? nullptr : instruction_->GetNext();
   }
@@ -2706,20 +2485,20 @@ class HInstructionIterator : public ValueObject {
   }
 
  private:
-  HInstructionIterator() : instruction_(nullptr), next_(nullptr) {}
+  HInstructionIteratorPrefetchNext() : instruction_(nullptr), next_(nullptr) {}
 
   HInstruction* instruction_;
   HInstruction* next_;
 
-  friend struct HSTLInstructionIterator<HInstructionIterator>;
+  friend struct HSTLInstructionIterator<HInstructionIteratorPrefetchNext>;
 };
 
 // Iterates over the instructions without saving the next instruction,
 // therefore handling changes in the graph potentially made by the user
 // of this iterator.
-class HInstructionIteratorHandleChanges : public ValueObject {
+class HInstructionIterator final : public ValueObject {
  public:
-  explicit HInstructionIteratorHandleChanges(const HInstructionList& instructions)
+  explicit HInstructionIterator(const HInstructionList& instructions)
       : instruction_(instructions.first_instruction_) {
   }
 
@@ -2730,22 +2509,22 @@ class HInstructionIteratorHandleChanges : public ValueObject {
   }
 
  private:
-  HInstructionIteratorHandleChanges() : instruction_(nullptr) {}
+  HInstructionIterator() : instruction_(nullptr) {}
 
   HInstruction* instruction_;
 
-  friend struct HSTLInstructionIterator<HInstructionIteratorHandleChanges>;
+  friend struct HSTLInstructionIterator<HInstructionIterator>;
 };
 
-
-class HBackwardInstructionIterator : public ValueObject {
+class HBackwardInstructionIteratorPrefetchNext final : public ValueObject {
  public:
-  explicit HBackwardInstructionIterator(const HInstructionList& instructions)
+  explicit HBackwardInstructionIteratorPrefetchNext(const HInstructionList& instructions)
       : instruction_(instructions.last_instruction_) {
     next_ = Done() ? nullptr : instruction_->GetPrevious();
   }
 
-  explicit HBackwardInstructionIterator(HInstruction* instruction) : instruction_(instruction) {
+  explicit HBackwardInstructionIteratorPrefetchNext(HInstruction* instruction)
+      : instruction_(instruction) {
     next_ = Done() ? nullptr : instruction_->GetPrevious();
   }
 
@@ -2757,12 +2536,12 @@ class HBackwardInstructionIterator : public ValueObject {
   }
 
  private:
-  HBackwardInstructionIterator() : instruction_(nullptr), next_(nullptr) {}
+  HBackwardInstructionIteratorPrefetchNext() : instruction_(nullptr), next_(nullptr) {}
 
   HInstruction* instruction_;
   HInstruction* next_;
 
-  friend struct HSTLInstructionIterator<HBackwardInstructionIterator>;
+  friend struct HSTLInstructionIterator<HBackwardInstructionIteratorPrefetchNext>;
 };
 
 template <typename InnerIter>
@@ -2774,9 +2553,9 @@ struct HSTLInstructionIterator : public ValueObject {
   using pointer = void;
   using reference = void;
 
-  static_assert(std::is_same_v<InnerIter, HBackwardInstructionIterator> ||
-                    std::is_same_v<InnerIter, HInstructionIterator> ||
-                    std::is_same_v<InnerIter, HInstructionIteratorHandleChanges>,
+  static_assert(std::is_same_v<InnerIter, HBackwardInstructionIteratorPrefetchNext> ||
+                    std::is_same_v<InnerIter, HInstructionIteratorPrefetchNext> ||
+                    std::is_same_v<InnerIter, HInstructionIterator>,
                 "Unknown wrapped iterator!");
 
   explicit HSTLInstructionIterator(InnerIter inner) : inner_(inner) {}
@@ -2904,7 +2683,7 @@ class HExpression<0, Base> : public Base {
   friend class SsaBuilder;
 };
 
-class HMethodEntryHook : public HExpression<0> {
+class HMethodEntryHook final : public HExpression<0> {
  public:
   explicit HMethodEntryHook(uint32_t dex_pc)
       : HExpression(kMethodEntryHook, SideEffects::All(), dex_pc) {}
@@ -2921,7 +2700,7 @@ class HMethodEntryHook : public HExpression<0> {
   DEFAULT_COPY_CONSTRUCTOR(MethodEntryHook);
 };
 
-class HMethodExitHook : public HExpression<1> {
+class HMethodExitHook final : public HExpression<1> {
  public:
   HMethodExitHook(HInstruction* value, uint32_t dex_pc)
       : HExpression(kMethodExitHook, SideEffects::All(), dex_pc) {
@@ -2948,8 +2727,6 @@ class HReturnVoid final : public HExpression<0> {
       : HExpression(kReturnVoid, SideEffects::None(), dex_pc) {
   }
 
-  bool IsControlFlow() const override { return true; }
-
   DECLARE_INSTRUCTION(ReturnVoid);
 
  protected:
@@ -2965,8 +2742,6 @@ class HReturn final : public HExpression<1> {
     SetRawInputAt(0, value);
   }
 
-  bool IsControlFlow() const override { return true; }
-
   DECLARE_INSTRUCTION(Return);
 
  protected:
@@ -3081,8 +2856,6 @@ class HExit final : public HExpression<0> {
       : HExpression(kExit, SideEffects::None(), dex_pc) {
   }
 
-  bool IsControlFlow() const override { return true; }
-
   DECLARE_INSTRUCTION(Exit);
 
  protected:
@@ -3097,8 +2870,6 @@ class HGoto final : public HExpression<0> {
   }
 
   bool IsClonable() const override { return true; }
-  bool IsControlFlow() const override { return true; }
-
   HBasicBlock* GetSuccessor() const {
     return GetBlock()->GetSingleSuccessor();
   }
@@ -3370,7 +3141,6 @@ class HIf final : public HExpression<1> {
   }
 
   bool IsClonable() const override { return true; }
-  bool IsControlFlow() const override { return true; }
 
   HBasicBlock* IfTrueSuccessor() const {
     return GetBlock()->GetSuccessors()[0];
@@ -3422,8 +3192,6 @@ class HTryBoundary final : public HExpression<0> {
     SetPackedField<BoundaryKindField>(kind);
   }
 
-  bool IsControlFlow() const override { return true; }
-
   // Returns the block's non-exceptional successor (index zero).
   HBasicBlock* GetNormalFlowSuccessor() const { return GetBlock()->GetSuccessors()[0]; }
 
@@ -3669,8 +3437,6 @@ class HPackedSwitch final : public HExpression<1> {
 
   bool IsClonable() const override { return true; }
 
-  bool IsControlFlow() const override { return true; }
-
   int32_t GetStartValue() const { return start_value_; }
 
   uint32_t GetNumEntries() const { return num_entries_; }
@@ -3762,7 +3528,22 @@ class HBinaryOperation : public HExpression<2> {
   HInstruction* GetRight() const { return InputAt(1); }
   DataType::Type GetResultType() const { return GetType(); }
 
-  virtual bool IsCommutative() const { return false; }
+  bool IsCommutative() const {
+    switch (GetKind()) {
+      case kAdd:
+      case kAnd:
+      case kEqual:
+      case kMax:
+      case kMin:
+      case kMul:
+      case kNotEqual:
+      case kOr:
+      case kXor:
+        return true;
+      default:
+        return false;
+    }
+  }
 
   // Put constant on the right.
   // Returns whether order is changed.
@@ -3966,8 +3747,6 @@ class HEqual final : public HCondition {
       : HCondition(kEqual, first, second, dex_pc) {
   }
 
-  bool IsCommutative() const override { return true; }
-
   HConstant* Evaluate([[maybe_unused]] HNullConstant* x,
                       [[maybe_unused]] HNullConstant* y) const override {
     return MakeConstantCondition(true);
@@ -4011,8 +3790,6 @@ class HNotEqual final : public HCondition {
       : HCondition(kNotEqual, first, second, dex_pc) {
   }
 
-  bool IsCommutative() const override { return true; }
-
   HConstant* Evaluate([[maybe_unused]] HNullConstant* x,
                       [[maybe_unused]] HNullConstant* y) const override {
     return MakeConstantCondition(false);
@@ -5285,8 +5062,6 @@ class HAdd final : public HBinaryOperation {
       : HBinaryOperation(kAdd, result_type, left, right, SideEffects::None(), dex_pc) {
   }
 
-  bool IsCommutative() const override { return true; }
-
   template <typename T> static T Compute(T x, T y) { return x + y; }
 
   HConstant* Evaluate(HIntConstant* x, HIntConstant* y) const override {
@@ -5347,8 +5122,6 @@ class HMul final : public HBinaryOperation {
       : HBinaryOperation(kMul, result_type, left, right, SideEffects::None(), dex_pc) {
   }
 
-  bool IsCommutative() const override { return true; }
-
   template <typename T> static T Compute(T x, T y) { return x * y; }
 
   HConstant* Evaluate(HIntConstant* x, HIntConstant* y) const override {
@@ -5466,8 +5239,6 @@ class HMin final : public HBinaryOperation {
        uint32_t dex_pc)
       : HBinaryOperation(kMin, result_type, left, right, SideEffects::None(), dex_pc) {}
 
-  bool IsCommutative() const override { return true; }
-
   // Evaluation for integral values.
   template <typename T> static T ComputeIntegral(T x, T y) {
     return (x <= y) ? x : y;
@@ -5503,8 +5274,6 @@ class HMax final : public HBinaryOperation {
        uint32_t dex_pc)
       : HBinaryOperation(kMax, result_type, left, right, SideEffects::None(), dex_pc) {}
 
-  bool IsCommutative() const override { return true; }
-
   // Evaluation for integral values.
   template <typename T> static T ComputeIntegral(T x, T y) {
     return (x >= y) ? x : y;
@@ -5701,8 +5470,6 @@ class HAnd final : public HBinaryOperation {
       : HBinaryOperation(kAnd, result_type, left, right, SideEffects::None(), dex_pc) {
   }
 
-  bool IsCommutative() const override { return true; }
-
   template <typename T> static T Compute(T x, T y) { return x & y; }
 
   HConstant* Evaluate(HIntConstant* x, HIntConstant* y) const override {
@@ -5727,8 +5494,6 @@ class HOr final : public HBinaryOperation {
       : HBinaryOperation(kOr, result_type, left, right, SideEffects::None(), dex_pc) {
   }
 
-  bool IsCommutative() const override { return true; }
-
   template <typename T> static T Compute(T x, T y) { return x | y; }
 
   HConstant* Evaluate(HIntConstant* x, HIntConstant* y) const override {
@@ -5753,8 +5518,6 @@ class HXor final : public HBinaryOperation {
       : HBinaryOperation(kXor, result_type, left, right, SideEffects::None(), dex_pc) {
   }
 
-  bool IsCommutative() const override { return true; }
-
   template <typename T> static T Compute(T x, T y) { return x ^ y; }
 
   HConstant* Evaluate(HIntConstant* x, HIntConstant* y) const override {
@@ -5993,7 +5756,7 @@ class HNullCheck final : public HExpression<1> {
 
 // Embeds an ArtField and all the information required by the compiler. We cache
 // that information to avoid requiring the mutator lock every time we need it.
-class FieldInfo : public ValueObject {
+class FieldInfo final : public ValueObject {
  public:
   FieldInfo(ArtField* field,
             MemberOffset field_offset,
@@ -6065,7 +5828,7 @@ class HFieldAccess : public HInstruction {
                uint16_t declaring_class_def_index,
                const DexFile& dex_file,
                uint32_t dex_pc)
-      : HInstruction(kind, field_type, side_effects, dex_pc),
+      : HInstruction(kind, side_effects, dex_pc),
         field_info_(field,
                     field_offset,
                     field_type,
@@ -6109,6 +5872,7 @@ class HInstanceFieldGet final : public HExpression<1, HFieldAccess> {
                     declaring_class_def_index,
                     dex_file,
                     dex_pc) {
+    SetPackedField<TypeField>(field_type);
     SetRawInputAt(0, object);
   }
 
@@ -7243,6 +7007,7 @@ class HStaticFieldGet final : public HExpression<1, HFieldAccess> {
                     declaring_class_def_index,
                     dex_file,
                     dex_pc) {
+    SetPackedField<TypeField>(field_type);
     SetRawInputAt(0, cls);
   }
 
@@ -7572,8 +7337,6 @@ class HThrow final : public HExpression<1> {
     SetRawInputAt(0, exception);
   }
 
-  bool IsControlFlow() const override { return true; }
-
   bool NeedsEnvironment() const override { return true; }
 
   bool CanThrow() const override { return true; }
@@ -8137,7 +7900,7 @@ class HSelect final : public HExpression<3> {
   DEFAULT_COPY_CONSTRUCTOR(Select);
 };
 
-class MoveOperands : public ArenaObject<kArenaAllocMoveOperands> {
+class MoveOperands final : public ArenaObject<kArenaAllocMoveOperands> {
  public:
   MoveOperands(Location source,
                Location destination,
@@ -8402,11 +8165,33 @@ class HGraphVisitor : public ValueObject {
   // Visit functions for instruction classes.
 #define DECLARE_VISIT_INSTRUCTION(name, super)                                        \
   virtual void Visit##name(H##name* instr) { VisitInstruction(instr); }
-
-  FOR_EACH_INSTRUCTION(DECLARE_VISIT_INSTRUCTION)
-
+  FOR_EACH_CONCRETE_INSTRUCTION(DECLARE_VISIT_INSTRUCTION)
 #undef DECLARE_VISIT_INSTRUCTION
 
+  ALWAYS_INLINE void Dispatch(HInstruction* insn) {
+    HInstruction::InstructionKind kind;
+    // Use `asm volatile` to prevent clang++ from optimizing the `kind = insn->GetKind()`
+    // together with the `switch`. The simple expression can somehow derail the
+    // `switch` optimization and result in a much worse compiled code. b/413605257
+    asm volatile("" : "=r"(kind) : "0"(insn->GetKind()));
+
+    switch (kind) {
+    #define DEFINE_DISPATCH_CASE(kind, super)                 \
+      case HInstruction::k##kind:                             \
+        Visit##kind(insn->As##kind());                        \
+        break;
+      FOR_EACH_CONCRETE_INSTRUCTION(DEFINE_DISPATCH_CASE)
+    #undef DEFINE_DISPATCH_CASE
+      default:
+        // Note: clang++ can optimize this `switch` to a virtual dispatch with indexed
+        // load from the vtable using an adjusted `invoke->GetKind()` as the index.
+        // However, a non-empty `default` or `case` causes clang++ to produce much
+        // worse code, so we want to limit this check to debug builds only.
+        DCHECK(false) << "UNREACHABLE";
+        UNREACHABLE();
+    }
+  }
+
  protected:
   void VisitPhis(HBasicBlock* block);
   void VisitNonPhiInstructions(HBasicBlock* block);
@@ -8426,13 +8211,16 @@ class HGraphDelegateVisitor : public HGraphVisitor {
       : HGraphVisitor(graph, stats) {}
   virtual ~HGraphDelegateVisitor() {}
 
-  // Visit functions that delegate to to super class.
-#define DECLARE_VISIT_INSTRUCTION(name, super)                                        \
-  void Visit##name(H##name* instr) override { Visit##super(instr); }
-
-  FOR_EACH_INSTRUCTION(DECLARE_VISIT_INSTRUCTION)
+  // Visit functions that delegate to super class.
+#define DECLARE_VISIT_ABSTRACT_INSTRUCTION(name, super)               \
+  virtual void Visit##name(H##name* instr) { Visit##super(instr); }
+  FOR_EACH_ABSTRACT_INSTRUCTION(DECLARE_VISIT_ABSTRACT_INSTRUCTION)
+#undef DECLARE_VISIT_ABSTRACT_INSTRUCTION
 
-#undef DECLARE_VISIT_INSTRUCTION
+#define DECLARE_VISIT_CONCRETE_INSTRUCTION(name, super)               \
+  void Visit##name(H##name* instr) override { Visit##super(instr); }
+  FOR_EACH_CONCRETE_INSTRUCTION(DECLARE_VISIT_CONCRETE_INSTRUCTION)
+#undef DECLARE_VISIT_CONCRETE_INSTRUCTION
 
  private:
   DISALLOW_COPY_AND_ASSIGN(HGraphDelegateVisitor);
@@ -8465,105 +8253,6 @@ class CloneAndReplaceInstructionVisitor final : public HGraphDelegateVisitor {
   DISALLOW_COPY_AND_ASSIGN(CloneAndReplaceInstructionVisitor);
 };
 
-// Iterator over the blocks that are part of the loop; includes blocks which are part
-// of an inner loop. The order in which the blocks are iterated is on their
-// block id.
-class HBlocksInLoopIterator : public ValueObject {
- public:
-  explicit HBlocksInLoopIterator(const HLoopInformation& info)
-      : blocks_in_loop_(info.GetBlocks()),
-        blocks_(info.GetHeader()->GetGraph()->GetBlocks()),
-        index_(0) {
-    if (!blocks_in_loop_.IsBitSet(index_)) {
-      Advance();
-    }
-  }
-
-  bool Done() const { return index_ == blocks_.size(); }
-  HBasicBlock* Current() const { return blocks_[index_]; }
-  void Advance() {
-    ++index_;
-    for (size_t e = blocks_.size(); index_ < e; ++index_) {
-      if (blocks_in_loop_.IsBitSet(index_)) {
-        break;
-      }
-    }
-  }
-
- private:
-  const BitVector& blocks_in_loop_;
-  const ArenaVector<HBasicBlock*>& blocks_;
-  size_t index_;
-
-  DISALLOW_COPY_AND_ASSIGN(HBlocksInLoopIterator);
-};
-
-// Iterator over the blocks that are part of the loop; includes blocks which are part
-// of an inner loop. The order in which the blocks are iterated is reverse
-// post order.
-class HBlocksInLoopReversePostOrderIterator : public ValueObject {
- public:
-  explicit HBlocksInLoopReversePostOrderIterator(const HLoopInformation& info)
-      : blocks_in_loop_(info.GetBlocks()),
-        blocks_(info.GetHeader()->GetGraph()->GetReversePostOrder()),
-        index_(0) {
-    if (!blocks_in_loop_.IsBitSet(blocks_[index_]->GetBlockId())) {
-      Advance();
-    }
-  }
-
-  bool Done() const { return index_ == blocks_.size(); }
-  HBasicBlock* Current() const { return blocks_[index_]; }
-  void Advance() {
-    ++index_;
-    for (size_t e = blocks_.size(); index_ < e; ++index_) {
-      if (blocks_in_loop_.IsBitSet(blocks_[index_]->GetBlockId())) {
-        break;
-      }
-    }
-  }
-
- private:
-  const BitVector& blocks_in_loop_;
-  const ArenaVector<HBasicBlock*>& blocks_;
-  size_t index_;
-
-  DISALLOW_COPY_AND_ASSIGN(HBlocksInLoopReversePostOrderIterator);
-};
-
-// Iterator over the blocks that are part of the loop; includes blocks which are part
-// of an inner loop. The order in which the blocks are iterated is post order.
-class HBlocksInLoopPostOrderIterator : public ValueObject {
- public:
-  explicit HBlocksInLoopPostOrderIterator(const HLoopInformation& info)
-      : blocks_in_loop_(info.GetBlocks()),
-        blocks_(info.GetHeader()->GetGraph()->GetReversePostOrder()),
-        index_(blocks_.size() - 1) {
-    if (!blocks_in_loop_.IsBitSet(blocks_[index_]->GetBlockId())) {
-      Advance();
-    }
-  }
-
-  bool Done() const { return index_ < 0; }
-  HBasicBlock* Current() const { return blocks_[index_]; }
-  void Advance() {
-    --index_;
-    for (; index_ >= 0; --index_) {
-      if (blocks_in_loop_.IsBitSet(blocks_[index_]->GetBlockId())) {
-        break;
-      }
-    }
-  }
-
- private:
-  const BitVector& blocks_in_loop_;
-  const ArenaVector<HBasicBlock*>& blocks_;
-
-  int32_t index_;
-
-  DISALLOW_COPY_AND_ASSIGN(HBlocksInLoopPostOrderIterator);
-};
-
 // Returns int64_t value of a properly typed constant.
 inline int64_t Int64FromConstant(HConstant* constant) {
   if (constant->IsIntConstant()) {
diff --git a/compiler/optimizing/nodes_riscv64.h b/compiler/optimizing/nodes_riscv64.h
index 51e8f7a1d1..fc8a7b12dc 100644
--- a/compiler/optimizing/nodes_riscv64.h
+++ b/compiler/optimizing/nodes_riscv64.h
@@ -35,7 +35,6 @@ class HRiscv64ShiftAdd final : public HBinaryOperation {
 
   uint32_t GetDistance() const { return GetPackedField<DistanceField>(); }
 
-  bool IsCommutative() const override { return false; }
   bool InstructionDataEquals(const HInstruction* other) const override {
     return GetPackedFields() == other->AsRiscv64ShiftAdd()->GetPackedFields();
   }
diff --git a/compiler/optimizing/nodes_test.cc b/compiler/optimizing/nodes_test.cc
index 97f74cb7b9..b5787b7bf0 100644
--- a/compiler/optimizing/nodes_test.cc
+++ b/compiler/optimizing/nodes_test.cc
@@ -163,8 +163,7 @@ TEST_F(NodeTest, RemoveInstruction) {
  */
 TEST_F(NodeTest, InsertInstruction) {
   HGraph* graph = CreateGraph();
-  HBasicBlock* entry = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(entry);
+  HBasicBlock* entry = AddNewBlock();
   graph->SetEntryBlock(entry);
   HInstruction* parameter1 = MakeParam(DataType::Type::kReference);
   HInstruction* parameter2 = MakeParam(DataType::Type::kReference);
@@ -184,8 +183,7 @@ TEST_F(NodeTest, InsertInstruction) {
  */
 TEST_F(NodeTest, AddInstruction) {
   HGraph* graph = CreateGraph();
-  HBasicBlock* entry = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(entry);
+  HBasicBlock* entry = AddNewBlock();
   graph->SetEntryBlock(entry);
   HInstruction* parameter = MakeParam(DataType::Type::kReference);
 
@@ -229,8 +227,7 @@ TEST_F(NodeTest, InsertDuplicateInstructionAt) {
 
 TEST_F(NodeTest, ParentEnvironment) {
   HGraph* graph = CreateGraph();
-  HBasicBlock* entry = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(entry);
+  HBasicBlock* entry = AddNewBlock();
   graph->SetEntryBlock(entry);
   HInstruction* parameter1 = MakeParam(DataType::Type::kReference);
   HInstruction* with_environment = MakeNullCheck(entry, parameter1, /*env=*/ {parameter1});
diff --git a/compiler/optimizing/nodes_vector_test.cc b/compiler/optimizing/nodes_vector_test.cc
index 6120c11d45..4ef6fcfc8a 100644
--- a/compiler/optimizing/nodes_vector_test.cc
+++ b/compiler/optimizing/nodes_vector_test.cc
@@ -26,32 +26,19 @@ namespace art HIDDEN {
  */
 class NodesVectorTest : public OptimizingUnitTest {
  public:
-  NodesVectorTest()
-      : graph_(CreateGraph()) {
-    BuildGraph();
-  }
-
-  ~NodesVectorTest() { }
-
-  void BuildGraph() {
+  NodesVectorTest() {
+    CreateGraph();
     graph_->SetNumberOfVRegs(1);
-    entry_block_ = new (GetAllocator()) HBasicBlock(graph_);
-    exit_block_ = new (GetAllocator()) HBasicBlock(graph_);
-    graph_->AddBlock(entry_block_);
-    graph_->AddBlock(exit_block_);
-    graph_->SetEntryBlock(entry_block_);
-    graph_->SetExitBlock(exit_block_);
+    graph_->SetEntryBlock(AddNewBlock());
+    AddExitBlock();
     int8_parameter_ = MakeParam(DataType::Type::kInt8);
     int16_parameter_ = MakeParam(DataType::Type::kInt16);
     int32_parameter_ = MakeParam(DataType::Type::kInt32);
   }
 
-  // General building fields.
-  HGraph* graph_;
-
-  HBasicBlock* entry_block_;
-  HBasicBlock* exit_block_;
+  ~NodesVectorTest() { }
 
+  // General building fields.
   HInstruction* int8_parameter_;
   HInstruction* int16_parameter_;
   HInstruction* int32_parameter_;
diff --git a/compiler/optimizing/nodes_x86.h b/compiler/optimizing/nodes_x86.h
index 491045de99..f99af377fb 100644
--- a/compiler/optimizing/nodes_x86.h
+++ b/compiler/optimizing/nodes_x86.h
@@ -105,8 +105,6 @@ class HX86PackedSwitch final : public HExpression<2> {
     SetRawInputAt(1, method_base);
   }
 
-  bool IsControlFlow() const override { return true; }
-
   int32_t GetStartValue() const { return start_value_; }
 
   int32_t GetNumEntries() const { return num_entries_; }
@@ -139,8 +137,6 @@ class HX86AndNot final : public HBinaryOperation {
       : HBinaryOperation(kX86AndNot, result_type, left, right, SideEffects::None(), dex_pc) {
   }
 
-  bool IsCommutative() const override { return false; }
-
   template <typename T> static T Compute(T x, T y) { return ~x & y; }
 
   HConstant* Evaluate(HIntConstant* x, HIntConstant* y) const override {
diff --git a/compiler/optimizing/optimization.cc b/compiler/optimizing/optimization.cc
index 31780739bc..0fb8126850 100644
--- a/compiler/optimizing/optimization.cc
+++ b/compiler/optimizing/optimization.cc
@@ -68,8 +68,6 @@ namespace art HIDDEN {
 
 const char* OptimizationPassName(OptimizationPass pass) {
   switch (pass) {
-    case OptimizationPass::kSideEffectsAnalysis:
-      return SideEffectsAnalysis::kSideEffectsAnalysisPassName;
     case OptimizationPass::kInductionVarAnalysis:
       return HInductionVarAnalysis::kInductionPassName;
     case OptimizationPass::kGlobalValueNumbering:
@@ -143,7 +141,7 @@ const char* OptimizationPassName(OptimizationPass pass) {
 
 #define X(x) if (pass_name == OptimizationPassName((x))) return (x)
 
-OptimizationPass OptimizationPassByName(const std::string& pass_name) {
+OptimizationPass OptimizationPassByName(std::string_view pass_name) {
   X(OptimizationPass::kBoundsCheckElimination);
   X(OptimizationPass::kCHAGuardOptimization);
   X(OptimizationPass::kCodeSinking);
@@ -160,7 +158,6 @@ OptimizationPass OptimizationPassByName(const std::string& pass_name) {
   X(OptimizationPass::kLoopOptimization);
   X(OptimizationPass::kReferenceTypePropagation);
   X(OptimizationPass::kScheduling);
-  X(OptimizationPass::kSideEffectsAnalysis);
 #ifdef ART_ENABLE_CODEGEN_arm
   X(OptimizationPass::kInstructionSimplifierArm);
   X(OptimizationPass::kCriticalNativeAbiFixupArm);
@@ -192,10 +189,9 @@ ArenaVector<HOptimization*> ConstructOptimizations(
     const DexCompilationUnit& dex_compilation_unit) {
   ArenaVector<HOptimization*> optimizations(allocator->Adapter());
 
-  // Some optimizations require SideEffectsAnalysis or HInductionVarAnalysis
+  // Some optimizations require HInductionVarAnalysis
   // instances. This method uses the nearest instance preceeding it in the pass
   // name list or fails fatally if no such analysis can be found.
-  SideEffectsAnalysis* most_recent_side_effects = nullptr;
   HInductionVarAnalysis* most_recent_induction = nullptr;
 
   // Loop over the requested optimizations.
@@ -211,9 +207,6 @@ ArenaVector<HOptimization*> ConstructOptimizations(
       //
       // Analysis passes (kept in most recent for subsequent passes).
       //
-      case OptimizationPass::kSideEffectsAnalysis:
-        opt = most_recent_side_effects = new (allocator) SideEffectsAnalysis(graph, pass_name);
-        break;
       case OptimizationPass::kInductionVarAnalysis:
         opt = most_recent_induction =
             new (allocator) HInductionVarAnalysis(graph, stats, pass_name);
@@ -222,12 +215,10 @@ ArenaVector<HOptimization*> ConstructOptimizations(
       // Passes that need prior analysis.
       //
       case OptimizationPass::kGlobalValueNumbering:
-        CHECK(most_recent_side_effects != nullptr);
-        opt = new (allocator) GVNOptimization(graph, *most_recent_side_effects, pass_name);
+        opt = new (allocator) GVNOptimization(graph, pass_name);
         break;
       case OptimizationPass::kInvariantCodeMotion:
-        CHECK(most_recent_side_effects != nullptr);
-        opt = new (allocator) LICM(graph, *most_recent_side_effects, stats, pass_name);
+        opt = new (allocator) LICM(graph, stats, pass_name);
         break;
       case OptimizationPass::kLoopOptimization:
         CHECK(most_recent_induction != nullptr);
@@ -235,9 +226,8 @@ ArenaVector<HOptimization*> ConstructOptimizations(
             graph, *codegen, most_recent_induction, stats, pass_name);
         break;
       case OptimizationPass::kBoundsCheckElimination:
-        CHECK(most_recent_side_effects != nullptr && most_recent_induction != nullptr);
-        opt = new (allocator) BoundsCheckElimination(
-            graph, *most_recent_side_effects, most_recent_induction, pass_name);
+        CHECK(most_recent_induction != nullptr);
+        opt = new (allocator) BoundsCheckElimination(graph, most_recent_induction, pass_name);
         break;
       //
       // Regular passes.
diff --git a/compiler/optimizing/optimization.h b/compiler/optimizing/optimization.h
index 0f0a15f7c9..bdf84dc6d1 100644
--- a/compiler/optimizing/optimization.h
+++ b/compiler/optimizing/optimization.h
@@ -17,15 +17,19 @@
 #ifndef ART_COMPILER_OPTIMIZING_OPTIMIZATION_H_
 #define ART_COMPILER_OPTIMIZING_OPTIMIZATION_H_
 
+#include <string_view>
+
+#include "base/arena_containers.h"
 #include "base/arena_object.h"
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimizing_compiler_stats.h"
 
 namespace art HIDDEN {
 
 class CodeGenerator;
 class DexCompilationUnit;
+class HGraph;
+class OptimizingCompilerStats;
 
 /**
  * Abstraction to implement an optimization pass.
@@ -84,7 +88,6 @@ enum class OptimizationPass {
   kLoopOptimization,
   kReferenceTypePropagation,
   kScheduling,
-  kSideEffectsAnalysis,
   kWriteBarrierElimination,
 #ifdef ART_ENABLE_CODEGEN_arm
   kInstructionSimplifierArm,
@@ -115,23 +118,23 @@ enum class OptimizationPass {
 const char* OptimizationPassName(OptimizationPass pass);
 
 // Lookup optimization pass by name.
-OptimizationPass OptimizationPassByName(const std::string& pass_name);
+OptimizationPass OptimizationPassByName(std::string_view pass_name);
 
 // Optimization definition consisting of an optimization pass
 // an optional alternative name (nullptr denotes default), and
 // an optional pass dependence (kNone denotes no dependence).
 struct OptimizationDef {
-  OptimizationDef(OptimizationPass p, const char* pn, OptimizationPass d)
-      : pass(p), pass_name(pn), depends_on(d) {}
+  constexpr OptimizationDef(OptimizationPass p, const char* pn, OptimizationPass d)
+      : pass(p), depends_on(d), pass_name(pn) {}
   OptimizationPass pass;
-  const char* pass_name;
   OptimizationPass depends_on;
+  const char* pass_name;
 };
 
 // Helper method for optimization definition array entries.
-inline OptimizationDef OptDef(OptimizationPass pass,
-                              const char* pass_name = nullptr,
-                              OptimizationPass depends_on = OptimizationPass::kNone) {
+constexpr OptimizationDef OptDef(OptimizationPass pass,
+                                 const char* pass_name = nullptr,
+                                 OptimizationPass depends_on = OptimizationPass::kNone) {
   return OptimizationDef(pass, pass_name, depends_on);
 }
 
diff --git a/compiler/optimizing/optimizing_compiler.cc b/compiler/optimizing/optimizing_compiler.cc
index 18b9413c22..1fea4cca73 100644
--- a/compiler/optimizing/optimizing_compiler.cc
+++ b/compiler/optimizing/optimizing_compiler.cc
@@ -452,7 +452,7 @@ bool OptimizingCompiler::RunRequiredPasses(HGraph* graph,
 #if defined(ART_ENABLE_CODEGEN_arm)
     case InstructionSet::kThumb2:
     case InstructionSet::kArm: {
-      OptimizationDef arm_optimizations[] = {
+      static constexpr OptimizationDef arm_optimizations[] = {
           OptDef(OptimizationPass::kCriticalNativeAbiFixupArm),
       };
       return RunOptimizations(graph,
@@ -464,7 +464,7 @@ bool OptimizingCompiler::RunRequiredPasses(HGraph* graph,
 #endif
 #if defined(ART_ENABLE_CODEGEN_riscv64)
     case InstructionSet::kRiscv64: {
-      OptimizationDef riscv64_optimizations[] = {
+      static constexpr OptimizationDef riscv64_optimizations[] = {
           OptDef(OptimizationPass::kCriticalNativeAbiFixupRiscv64),
       };
       return RunOptimizations(graph,
@@ -476,7 +476,7 @@ bool OptimizingCompiler::RunRequiredPasses(HGraph* graph,
 #endif
 #ifdef ART_ENABLE_CODEGEN_x86
     case InstructionSet::kX86: {
-      OptimizationDef x86_optimizations[] = {
+      static constexpr OptimizationDef x86_optimizations[] = {
           OptDef(OptimizationPass::kPcRelativeFixupsX86),
       };
       return RunOptimizations(graph,
@@ -503,9 +503,8 @@ bool OptimizingCompiler::RunArchOptimizations(HGraph* graph,
 #if defined(ART_ENABLE_CODEGEN_arm)
     case InstructionSet::kThumb2:
     case InstructionSet::kArm: {
-      OptimizationDef arm_optimizations[] = {
+      static constexpr OptimizationDef arm_optimizations[] = {
           OptDef(OptimizationPass::kInstructionSimplifierArm),
-          OptDef(OptimizationPass::kSideEffectsAnalysis),
           OptDef(OptimizationPass::kGlobalValueNumbering, "GVN$after_arch"),
           OptDef(OptimizationPass::kCriticalNativeAbiFixupArm),
           OptDef(OptimizationPass::kScheduling)
@@ -519,9 +518,8 @@ bool OptimizingCompiler::RunArchOptimizations(HGraph* graph,
 #endif
 #ifdef ART_ENABLE_CODEGEN_arm64
     case InstructionSet::kArm64: {
-      OptimizationDef arm64_optimizations[] = {
+      static constexpr OptimizationDef arm64_optimizations[] = {
           OptDef(OptimizationPass::kInstructionSimplifierArm64),
-          OptDef(OptimizationPass::kSideEffectsAnalysis),
           OptDef(OptimizationPass::kGlobalValueNumbering, "GVN$after_arch"),
           OptDef(OptimizationPass::kScheduling)
       };
@@ -534,9 +532,8 @@ bool OptimizingCompiler::RunArchOptimizations(HGraph* graph,
 #endif
 #if defined(ART_ENABLE_CODEGEN_riscv64)
     case InstructionSet::kRiscv64: {
-      OptimizationDef riscv64_optimizations[] = {
+      static constexpr OptimizationDef riscv64_optimizations[] = {
           OptDef(OptimizationPass::kInstructionSimplifierRiscv64),
-          OptDef(OptimizationPass::kSideEffectsAnalysis),
           OptDef(OptimizationPass::kGlobalValueNumbering, "GVN$after_arch"),
           OptDef(OptimizationPass::kCriticalNativeAbiFixupRiscv64)
       };
@@ -549,9 +546,8 @@ bool OptimizingCompiler::RunArchOptimizations(HGraph* graph,
 #endif
 #ifdef ART_ENABLE_CODEGEN_x86
     case InstructionSet::kX86: {
-      OptimizationDef x86_optimizations[] = {
+      static constexpr OptimizationDef x86_optimizations[] = {
           OptDef(OptimizationPass::kInstructionSimplifierX86),
-          OptDef(OptimizationPass::kSideEffectsAnalysis),
           OptDef(OptimizationPass::kGlobalValueNumbering, "GVN$after_arch"),
           OptDef(OptimizationPass::kPcRelativeFixupsX86),
           OptDef(OptimizationPass::kX86MemoryOperandGeneration)
@@ -565,9 +561,8 @@ bool OptimizingCompiler::RunArchOptimizations(HGraph* graph,
 #endif
 #ifdef ART_ENABLE_CODEGEN_x86_64
     case InstructionSet::kX86_64: {
-      OptimizationDef x86_64_optimizations[] = {
+      static constexpr OptimizationDef x86_64_optimizations[] = {
           OptDef(OptimizationPass::kInstructionSimplifierX86_64),
-          OptDef(OptimizationPass::kSideEffectsAnalysis),
           OptDef(OptimizationPass::kGlobalValueNumbering, "GVN$after_arch"),
           OptDef(OptimizationPass::kX86MemoryOperandGeneration)
       };
@@ -642,7 +637,7 @@ void OptimizingCompiler::RunOptimizations(HGraph* graph,
     return;
   }
 
-  OptimizationDef optimizations[] = {
+  static constexpr OptimizationDef optimizations[] = {
       // Initial optimizations.
       OptDef(OptimizationPass::kConstantFolding),
       OptDef(OptimizationPass::kInstructionSimplifier),
@@ -661,8 +656,6 @@ void OptimizingCompiler::RunOptimizations(HGraph* graph,
              "dead_code_elimination$after_inlining",
              OptimizationPass::kInliner),
       // GVN.
-      OptDef(OptimizationPass::kSideEffectsAnalysis,
-             "side_effects$before_gvn"),
       OptDef(OptimizationPass::kGlobalValueNumbering),
       OptDef(OptimizationPass::kReferenceTypePropagation,
              "reference_type_propagation$after_gvn",
@@ -676,8 +669,6 @@ void OptimizingCompiler::RunOptimizations(HGraph* graph,
       OptDef(OptimizationPass::kDeadCodeElimination,
              "dead_code_elimination$after_gvn"),
       // High-level optimizations.
-      OptDef(OptimizationPass::kSideEffectsAnalysis,
-             "side_effects$before_licm"),
       OptDef(OptimizationPass::kInvariantCodeMotion),
       OptDef(OptimizationPass::kInductionVarAnalysis),
       OptDef(OptimizationPass::kBoundsCheckElimination),
@@ -1089,7 +1080,7 @@ CodeGenerator* OptimizingCompiler::TryCompileIntrinsic(
     builder.BuildIntrinsicGraph(method);
   }
 
-  OptimizationDef optimizations[] = {
+  static constexpr OptimizationDef optimizations[] = {
       // The codegen has a few assumptions that only the instruction simplifier
       // can satisfy.
       OptDef(OptimizationPass::kInstructionSimplifier),
diff --git a/compiler/optimizing/optimizing_unit_test.h b/compiler/optimizing/optimizing_unit_test.h
index 8dc17f1618..983d479a71 100644
--- a/compiler/optimizing/optimizing_unit_test.h
+++ b/compiler/optimizing/optimizing_unit_test.h
@@ -27,7 +27,7 @@
 
 #include "base/macros.h"
 #include "base/indenter.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "base/scoped_arena_allocator.h"
 #include "builder.h"
 #include "common_compiler_test.h"
@@ -118,7 +118,8 @@ inline void RemoveSuspendChecks(HGraph* graph) {
       if (block->GetLoopInformation() != nullptr) {
         block->GetLoopInformation()->SetSuspendCheck(nullptr);
       }
-      for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+      for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done();
+           it.Advance()) {
         HInstruction* current = it.Current();
         if (current->IsSuspendCheck()) {
           current->GetBlock()->RemoveInstruction(current);
@@ -138,7 +139,7 @@ class ArenaPoolAndAllocator {
   ScopedArenaAllocator* GetScopedAllocator() { return &scoped_allocator_; }
 
  private:
-  MallocArenaPool pool_;
+  CallocArenaPool pool_;
   ArenaAllocator allocator_;
   ArenaStack arena_stack_;
   ScopedArenaAllocator scoped_allocator_;
@@ -154,7 +155,7 @@ class AdjacencyListGraph {
       const std::string_view exit_name,
       const std::vector<Edge>& adj) : graph_(graph) {
     auto create_block = [&]() {
-      HBasicBlock* blk = new (alloc) HBasicBlock(graph_);
+      HBasicBlock* blk = HBasicBlock::Create(alloc, graph_);
       graph_->AddBlock(blk);
       return blk;
     };
@@ -425,11 +426,18 @@ class OptimizingUnitTestHelper {
   }
 
   HBasicBlock* AddNewBlock() {
-    HBasicBlock* block = new (GetAllocator()) HBasicBlock(graph_);
+    HBasicBlock* block = HBasicBlock::Create(GetAllocator(), graph_);
     graph_->AddBlock(block);
     return block;
   }
 
+  HBasicBlock* AddExitBlock() {
+    HBasicBlock* block = AddNewBlock();
+    MakeExit(block);
+    graph_->SetExitBlock(block);
+    return block;
+  }
+
   // Run GraphChecker with all checks.
   //
   // Return: the status whether the run is successful.
@@ -775,6 +783,19 @@ class OptimizingUnitTestHelper {
     return invoke;
   }
 
+  template <typename Type>
+  Type* MakeUnOp(HBasicBlock* block,
+                 DataType::Type result_type,
+                 HInstruction* input,
+                 uint32_t dex_pc = kNoDexPc) {
+    static_assert(std::is_base_of_v<HUnaryOperation, Type> ||
+                  // TODO: Make `HTypeConversion` inherit `HUnaryOperation`.
+                  std::is_same_v<HTypeConversion, Type>);
+    Type* insn = new (GetAllocator()) Type(result_type, input, dex_pc);
+    AddOrInsertInstruction(block, insn);
+    return insn;
+  }
+
   template <typename Type>
   Type* MakeBinOp(HBasicBlock* block,
                   DataType::Type result_type,
diff --git a/compiler/optimizing/parallel_move_test.cc b/compiler/optimizing/parallel_move_test.cc
index d2b993280d..9bf111b8ce 100644
--- a/compiler/optimizing/parallel_move_test.cc
+++ b/compiler/optimizing/parallel_move_test.cc
@@ -16,7 +16,7 @@
 
 #include "base/arena_allocator.h"
 #include "base/macros.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "nodes.h"
 #include "parallel_move_resolver.h"
 
@@ -182,7 +182,7 @@ TYPED_TEST_CASE(ParallelMoveTest, ParallelMoveResolverTestTypes);
 
 
 TYPED_TEST(ParallelMoveTest, Dependency) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaAllocator allocator(&pool);
 
   {
@@ -209,7 +209,7 @@ TYPED_TEST(ParallelMoveTest, Dependency) {
 }
 
 TYPED_TEST(ParallelMoveTest, Cycle) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaAllocator allocator(&pool);
 
   {
@@ -259,7 +259,7 @@ TYPED_TEST(ParallelMoveTest, Cycle) {
 }
 
 TYPED_TEST(ParallelMoveTest, ConstantLast) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaAllocator allocator(&pool);
   TypeParam resolver(&allocator);
   HParallelMove* moves = new (&allocator) HParallelMove(&allocator);
@@ -278,7 +278,7 @@ TYPED_TEST(ParallelMoveTest, ConstantLast) {
 }
 
 TYPED_TEST(ParallelMoveTest, Pairs) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaAllocator allocator(&pool);
 
   {
@@ -455,7 +455,7 @@ TYPED_TEST(ParallelMoveTest, Pairs) {
 }
 
 TYPED_TEST(ParallelMoveTest, MultiCycles) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaAllocator allocator(&pool);
 
   {
@@ -553,7 +553,7 @@ TYPED_TEST(ParallelMoveTest, MultiCycles) {
 
 // Test that we do 64bits moves before 32bits moves.
 TYPED_TEST(ParallelMoveTest, CyclesWith64BitsMoves) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaAllocator allocator(&pool);
 
   {
@@ -612,7 +612,7 @@ TYPED_TEST(ParallelMoveTest, CyclesWith64BitsMoves) {
 }
 
 TYPED_TEST(ParallelMoveTest, CyclesWith64BitsMoves2) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaAllocator allocator(&pool);
 
   {
diff --git a/compiler/optimizing/pc_relative_fixups_x86.h b/compiler/optimizing/pc_relative_fixups_x86.h
index 45578d8050..9d7b9f6db3 100644
--- a/compiler/optimizing/pc_relative_fixups_x86.h
+++ b/compiler/optimizing/pc_relative_fixups_x86.h
@@ -18,7 +18,6 @@
 #define ART_COMPILER_OPTIMIZING_PC_RELATIVE_FIXUPS_X86_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
 
 namespace art HIDDEN {
diff --git a/compiler/optimizing/prepare_for_register_allocation.cc b/compiler/optimizing/prepare_for_register_allocation.cc
index c5dbab5f79..8a9e09a899 100644
--- a/compiler/optimizing/prepare_for_register_allocation.cc
+++ b/compiler/optimizing/prepare_for_register_allocation.cc
@@ -61,9 +61,9 @@ bool PrepareForRegisterAllocation::Run() {
   // Order does not matter.
   for (HBasicBlock* block : graph_->GetReversePostOrder()) {
     // No need to visit the phis.
-    for (HInstructionIteratorHandleChanges inst_it(block->GetInstructions()); !inst_it.Done();
+    for (HInstructionIterator inst_it(block->GetInstructions()); !inst_it.Done();
          inst_it.Advance()) {
-      inst_it.Current()->Accept(&visitor);
+      visitor.Dispatch(inst_it.Current());
     }
   }
   return true;
diff --git a/compiler/optimizing/reference_type_propagation.cc b/compiler/optimizing/reference_type_propagation.cc
index 6d74b0c3f7..3b3324b813 100644
--- a/compiler/optimizing/reference_type_propagation.cc
+++ b/compiler/optimizing/reference_type_propagation.cc
@@ -124,7 +124,7 @@ ReferenceTypePropagation::ReferenceTypePropagation(HGraph* graph,
 
 void ReferenceTypePropagation::Visit(HInstruction* instruction) {
   RTPVisitor visitor(graph_, hint_dex_cache_, is_first_run_);
-  instruction->Accept(&visitor);
+  visitor.Dispatch(instruction);
 }
 
 void ReferenceTypePropagation::Visit(ArrayRef<HInstruction* const> instructions) {
@@ -136,7 +136,7 @@ void ReferenceTypePropagation::Visit(ArrayRef<HInstruction* const> instructions)
     }
   }
   for (HInstruction* instruction : instructions) {
-    instruction->Accept(&visitor);
+    visitor.Dispatch(instruction);
     // We don't know if the instruction list is ordered in the same way normal
     // visiting would be so we need to process every instruction manually.
     if (RTPVisitor::IsUpdateable(instruction)) {
@@ -316,7 +316,7 @@ void ReferenceTypePropagation::RTPVisitor::VisitBasicBlock(HBasicBlock* block) {
   VisitPhis(block);
 
   // Handle instructions. Since RTP may add HBoundType instructions just after the
-  // last visited instruction, use `HInstructionIteratorHandleChanges` iterator.
+  // last visited instruction, use `HInstructionIterator` iterator.
   VisitNonPhiInstructionsHandleChanges(block);
 
   // Add extra nodes to bound types.
@@ -714,7 +714,7 @@ void ReferenceTypePropagation::RTPVisitor::VisitCheckCast(HCheckCast* check_cast
   } else {
     // This is the first run of RTP and class is unresolved. Remove the binding.
     // The instruction itself is removed in VisitBoundType so as to not
-    // invalidate HInstructionIterator.
+    // invalidate HInstructionIteratorPrefetchNext.
     bound_type->ReplaceWith(bound_type->InputAt(0));
   }
 }
diff --git a/compiler/optimizing/register_allocation_resolver.cc b/compiler/optimizing/register_allocation_resolver.cc
index e847755978..bd4e3f588e 100644
--- a/compiler/optimizing/register_allocation_resolver.cc
+++ b/compiler/optimizing/register_allocation_resolver.cc
@@ -55,8 +55,7 @@ void RegisterAllocationResolver::Resolve(ArrayRef<HInstruction* const> safepoint
 
   // Resolve outputs, including stack locations.
   // TODO: Use pointers of Location inside LiveInterval to avoid doing another iteration.
-  for (size_t i = 0, e = liveness_.GetNumberOfSsaValues(); i < e; ++i) {
-    HInstruction* instruction = liveness_.GetInstructionFromSsaIndex(i);
+  for (HInstruction* instruction : liveness_.GetInstructionsFromSsaIndexes()) {
     LiveInterval* current = instruction->GetLiveInterval();
     LocationSummary* locations = instruction->GetLocations();
     Location location = locations->Out();
@@ -143,8 +142,7 @@ void RegisterAllocationResolver::Resolve(ArrayRef<HInstruction* const> safepoint
   }
 
   // Connect siblings and resolve inputs.
-  for (size_t i = 0, e = liveness_.GetNumberOfSsaValues(); i < e; ++i) {
-    HInstruction* instruction = liveness_.GetInstructionFromSsaIndex(i);
+  for (HInstruction* instruction : liveness_.GetInstructionsFromSsaIndexes()) {
     ConnectSiblings(instruction->GetLiveInterval());
   }
 
@@ -184,14 +182,13 @@ void RegisterAllocationResolver::Resolve(ArrayRef<HInstruction* const> safepoint
       // Catch phi values are set at runtime by the exception delivery mechanism.
     } else {
       for (HInstructionIterator inst_it(block->GetPhis()); !inst_it.Done(); inst_it.Advance()) {
-        HInstruction* phi = inst_it.Current();
-        for (size_t i = 0, e = block->GetPredecessors().size(); i < e; ++i) {
-          HBasicBlock* predecessor = block->GetPredecessors()[i];
+        HPhi* phi = inst_it.Current()->AsPhi();
+        HInputsRef inputs = phi->GetInputs();
+        Location destination = phi->GetLiveInterval()->ToLocation();
+        for (auto [predecessor, input_index] : ZipCount(block->GetPredecessors())) {
           DCHECK_EQ(predecessor->GetNormalSuccessors().size(), 1u);
-          HInstruction* input = phi->InputAt(i);
-          Location source = input->GetLiveInterval()->GetLocationAt(
+          Location source = inputs[input_index]->GetLiveInterval()->GetLocationAt(
               predecessor->GetLifetimeEnd() - 1);
-          Location destination = phi->GetLiveInterval()->ToLocation();
           InsertParallelMoveAtExitOf(predecessor, phi, source, destination);
         }
       }
@@ -205,7 +202,7 @@ void RegisterAllocationResolver::Resolve(ArrayRef<HInstruction* const> safepoint
       continue;
     }
     HInstruction* at = liveness_.GetTempUser(temp);
-    size_t temp_index = liveness_.GetTempIndex(temp);
+    size_t temp_index = temp->GetTempIndex();
     LocationSummary* locations = at->GetLocations();
     switch (temp->GetType()) {
       case DataType::Type::kInt32:
@@ -230,8 +227,7 @@ void RegisterAllocationResolver::Resolve(ArrayRef<HInstruction* const> safepoint
 }
 
 void RegisterAllocationResolver::UpdateSafepointLiveRegisters() {
-  for (size_t i = 0, e = liveness_.GetNumberOfSsaValues(); i < e; ++i) {
-    HInstruction* instruction = liveness_.GetInstructionFromSsaIndex(i);
+  for (HInstruction* instruction : liveness_.GetInstructionsFromSsaIndexes()) {
     for (LiveInterval* current = instruction->GetLiveInterval();
          current != nullptr;
          current = current->GetNextSibling()) {
@@ -239,11 +235,9 @@ void RegisterAllocationResolver::UpdateSafepointLiveRegisters() {
         continue;
       }
       Location source = current->ToLocation();
-      for (SafepointPosition* safepoint_position = current->GetFirstSafepoint();
-           safepoint_position != nullptr;
-           safepoint_position = safepoint_position->GetNext()) {
-        DCHECK(current->CoversSlow(safepoint_position->GetPosition()));
-        LocationSummary* locations = safepoint_position->GetLocations();
+      for (const SafepointPosition& safepoint_position : current->GetSafepoints()) {
+        DCHECK(current->CoversSlow(safepoint_position.GetPosition()));
+        LocationSummary* locations = safepoint_position.GetLocations();
         switch (source.GetKind()) {
           case Location::kRegister:
           case Location::kFpuRegister: {
@@ -381,18 +375,16 @@ void RegisterAllocationResolver::ConnectSiblings(LiveInterval* interval) {
       InsertParallelMoveAt(current->GetEnd(), interval->GetDefinedBy(), source, destination);
     }
 
-    for (SafepointPosition* safepoint_position = current->GetFirstSafepoint();
-         safepoint_position != nullptr;
-         safepoint_position = safepoint_position->GetNext()) {
-      DCHECK(current->CoversSlow(safepoint_position->GetPosition()));
+    for (const SafepointPosition& safepoint_position : current->GetSafepoints()) {
+      DCHECK(current->CoversSlow(safepoint_position.GetPosition()));
 
       if (current->GetType() == DataType::Type::kReference) {
         DCHECK(interval->GetDefinedBy()->IsActualObject())
             << interval->GetDefinedBy()->DebugName()
             << '(' << interval->GetDefinedBy()->GetId() << ')'
-            << "@" << safepoint_position->GetInstruction()->DebugName()
-            << '(' << safepoint_position->GetInstruction()->GetId() << ')';
-        LocationSummary* locations = safepoint_position->GetLocations();
+            << "@" << safepoint_position.GetInstruction()->DebugName()
+            << '(' << safepoint_position.GetInstruction()->GetId() << ')';
+        LocationSummary* locations = safepoint_position.GetLocations();
         if (current->GetParent()->HasSpillSlot()) {
           locations->SetStackBit(current->GetParent()->GetSpillSlot() / kVRegSize);
         }
@@ -544,30 +536,35 @@ void RegisterAllocationResolver::AddInputMoveFor(HInstruction* input,
   AddMove(move, source, destination, nullptr, input->GetType());
 }
 
-static bool IsInstructionStart(size_t position) {
-  return (position & 1) == 0;
+static bool IsMovePositionAtInstructionStart(size_t position) {
+  static_assert(IsPowerOfTwo(kLivenessPositionsPerInstruction));
+  return (position & (kLivenessPositionsPerInstruction - 1)) == 0;
 }
 
-static bool IsInstructionEnd(size_t position) {
-  return (position & 1) == 1;
+static bool IsMovePositionAfterInstruction(size_t position) {
+  static_assert(IsPowerOfTwo(kLivenessPositionsPerInstruction));
+  return (position & (kLivenessPositionsPerInstruction - 1)) == kLivenessPositionForMoveAfter;
 }
 
 void RegisterAllocationResolver::InsertParallelMoveAt(size_t position,
                                                       HInstruction* instruction,
                                                       Location source,
                                                       Location destination) const {
+  DCHECK(IsMovePositionAtInstructionStart(position) ||
+         IsMovePositionAfterInstruction(position)) << position;
   DCHECK(IsValidDestination(destination)) << destination;
   if (source.Equals(destination)) return;
 
-  HInstruction* at = liveness_.GetInstructionFromPosition(position / 2);
+  HInstruction* at =
+      liveness_.GetInstructionFromPosition(position / kLivenessPositionsPerInstruction);
   HParallelMove* move;
   if (at == nullptr) {
-    if (IsInstructionStart(position)) {
+    if (IsMovePositionAtInstructionStart(position)) {
       // Block boundary, don't do anything the connection of split siblings will handle it.
       return;
     } else {
       // Move must happen before the first instruction of the block.
-      at = liveness_.GetInstructionFromPosition((position + 1) / 2);
+      at = liveness_.GetInstructionFromPosition(position / kLivenessPositionsPerInstruction + 1);
       // Note that parallel moves may have already been inserted, so we explicitly
       // ask for the first instruction of the block: `GetInstructionFromPosition` does
       // not contain the `HParallelMove` instructions.
@@ -590,7 +587,7 @@ void RegisterAllocationResolver::InsertParallelMoveAt(size_t position,
         move = at->AsParallelMove();
       }
     }
-  } else if (IsInstructionEnd(position)) {
+  } else if (IsMovePositionAfterInstruction(position)) {
     // Move must happen after the instruction.
     DCHECK(!at->IsControlFlow());
     move = at->GetNext()->AsParallelMoveOrNull();
@@ -686,7 +683,7 @@ void RegisterAllocationResolver::InsertMoveAfter(HInstruction* instruction,
     return;
   }
 
-  size_t position = instruction->GetLifetimePosition() + 1;
+  size_t position = instruction->GetLifetimePosition() + kLivenessPositionForMoveAfter;
   HParallelMove* move = instruction->GetNext()->AsParallelMoveOrNull();
   // This is a parallel move for moving the output of an instruction. We need
   // to differentiate with input moves, moves for connecting siblings in a
diff --git a/compiler/optimizing/register_allocator-inl.h b/compiler/optimizing/register_allocator-inl.h
new file mode 100644
index 0000000000..754006c4a1
--- /dev/null
+++ b/compiler/optimizing/register_allocator-inl.h
@@ -0,0 +1,54 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_COMPILER_OPTIMIZING_REGISTER_ALLOCATOR_INL_H_
+#define ART_COMPILER_OPTIMIZING_REGISTER_ALLOCATOR_INL_H_
+
+#include "register_allocator.h"
+
+#include "base/bit_utils.h"
+#include "data_type.h"
+#include "ssa_liveness_analysis.h"
+
+namespace art HIDDEN {
+
+inline uint32_t RegisterAllocator::GetSingleRegisterMask(LiveInterval* interval,
+                                                         RegisterType register_type) {
+  DCHECK(interval->HasRegister());
+  DCHECK_EQ(register_type == RegisterType::kFpRegister,
+            DataType::IsFloatingPointType(interval->GetType()));
+  DCHECK_LE(static_cast<size_t>(interval->GetRegister()), BitSizeOf<uint32_t>());
+  return 1u << interval->GetRegister();
+}
+
+inline uint32_t RegisterAllocator::GetBlockedRegistersMask(
+    LiveInterval* interval,
+    ArrayRef<HInstruction* const> instructions_from_positions,
+    size_t number_of_registers,
+    uint32_t registers_blocked_for_call) {
+  DCHECK(!interval->HasRegister());
+  DCHECK(interval->IsFixed());
+  DCHECK_EQ(interval->GetType(), DataType::Type::kVoid);
+  DCHECK(interval->GetFirstRange() != nullptr);
+  size_t start = interval->GetFirstRange()->GetStart();
+  bool blocked_for_call =
+      instructions_from_positions[start / kLivenessPositionsPerInstruction] != nullptr;
+  return blocked_for_call ? registers_blocked_for_call : MaxInt<uint32_t>(number_of_registers);
+}
+
+}  // namespace art
+
+#endif  // ART_COMPILER_OPTIMIZING_REGISTER_ALLOCATOR_INL_H_
diff --git a/compiler/optimizing/register_allocator.cc b/compiler/optimizing/register_allocator.cc
index a318d43412..aa98b98241 100644
--- a/compiler/optimizing/register_allocator.cc
+++ b/compiler/optimizing/register_allocator.cc
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-#include "register_allocator.h"
+#include "register_allocator-inl.h"
 
 #include <iostream>
 #include <sstream>
@@ -24,6 +24,7 @@
 #include "base/bit_utils_iterator.h"
 #include "base/bit_vector-inl.h"
 #include "code_generator.h"
+#include "loop_information-inl.h"
 #include "register_allocator_linear_scan.h"
 #include "ssa_liveness_analysis.h"
 
@@ -75,10 +76,11 @@ RegisterAllocator::~RegisterAllocator() {
     // Poison live interval pointers with "Error: BAD 71ve1nt3rval."
     LiveInterval* bad_live_interval = reinterpret_cast<LiveInterval*>(0xebad7113u);
     for (HBasicBlock* block : codegen_->GetGraph()->GetLinearOrder()) {
-      for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+      for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
         it.Current()->SetLiveInterval(bad_live_interval);
       }
-      for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+      for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done();
+           it.Advance()) {
         it.Current()->SetLiveInterval(bad_live_interval);
       }
     }
@@ -129,23 +131,24 @@ void RegisterAllocator::DumpRegister(std::ostream& stream,
 uint32_t RegisterAllocator::GetRegisterMask(LiveInterval* interval,
                                             RegisterType register_type) const {
   if (interval->HasRegister()) {
-    DCHECK_EQ(register_type == RegisterType::kFpRegister,
-              DataType::IsFloatingPointType(interval->GetType()));
-    DCHECK_LE(static_cast<size_t>(interval->GetRegister()), BitSizeOf<uint32_t>());
-    return 1u << interval->GetRegister();
+    return GetSingleRegisterMask(interval, register_type);
   } else if (interval->IsFixed()) {
-    DCHECK_EQ(interval->GetType(), DataType::Type::kVoid);
-    DCHECK(interval->GetFirstRange() != nullptr);
-    size_t start = interval->GetFirstRange()->GetStart();
-    bool blocked_for_call = liveness_.GetInstructionFromPosition(start / 2u) != nullptr;
+    size_t num_registers;
+    uint32_t registers_blocked_for_call;
     switch (register_type) {
       case RegisterType::kCoreRegister:
-        return blocked_for_call ? core_registers_blocked_for_call_
-                                : MaxInt<uint32_t>(num_core_registers_);
+        num_registers = num_core_registers_;
+        registers_blocked_for_call = core_registers_blocked_for_call_;
+        break;
       case RegisterType::kFpRegister:
-        return blocked_for_call ? fp_registers_blocked_for_call_
-                                : MaxInt<uint32_t>(num_fp_registers_);
+        num_registers = num_fp_registers_;
+        registers_blocked_for_call = fp_registers_blocked_for_call_;
+        break;
     }
+    return GetBlockedRegistersMask(interval,
+                                   liveness_.GetInstructionsFromPositions(),
+                                   num_registers,
+                                   registers_blocked_for_call);
   } else {
     return 0u;
   }
@@ -170,18 +173,13 @@ bool RegisterAllocator::ValidateIntervals(ArrayRef<LiveInterval* const> interval
   // that we cannot use in this static member function.
   auto get_register_mask = [&](LiveInterval* interval) {
     if (interval->HasRegister()) {
-      DCHECK_EQ(register_type == RegisterType::kFpRegister,
-                DataType::IsFloatingPointType(interval->GetType()));
-      DCHECK_LE(static_cast<size_t>(interval->GetRegister()), BitSizeOf<uint32_t>());
-      return 1u << interval->GetRegister();
+      return GetSingleRegisterMask(interval, register_type);
     } else if (interval->IsFixed()) {
-      DCHECK_EQ(interval->GetType(), DataType::Type::kVoid);
-      DCHECK(interval->GetFirstRange() != nullptr);
-      size_t start = interval->GetFirstRange()->GetStart();
-      CHECK(liveness != nullptr);
-      bool blocked_for_call = liveness->GetInstructionFromPosition(start / 2u) != nullptr;
-      return blocked_for_call ? registers_blocked_for_call
-                              : MaxInt<uint32_t>(number_of_registers);
+      DCHECK(liveness != nullptr);
+      return GetBlockedRegistersMask(interval,
+                                     liveness->GetInstructionsFromPositions(),
+                                     number_of_registers,
+                                     registers_blocked_for_call);
     } else {
       return 0u;
     }
@@ -304,9 +302,15 @@ LiveInterval* RegisterAllocator::Split(LiveInterval* interval, size_t position)
   }
 }
 
-LiveInterval* RegisterAllocator::SplitBetween(LiveInterval* interval, size_t from, size_t to) {
-  HBasicBlock* block_from = liveness_.GetBlockFromPosition(from / 2);
-  HBasicBlock* block_to = liveness_.GetBlockFromPosition(to / 2);
+LiveInterval* RegisterAllocator::SplitBetween(
+    LiveInterval* interval,
+    size_t from,
+    size_t to,
+    ArrayRef<HInstruction* const> instructions_from_positions) {
+  HBasicBlock* block_from = SsaLivenessAnalysis::GetBlockFromPosition(
+      from / kLivenessPositionsPerInstruction, instructions_from_positions);
+  HBasicBlock* block_to = SsaLivenessAnalysis::GetBlockFromPosition(
+      to / kLivenessPositionsPerInstruction, instructions_from_positions);
   DCHECK(block_from != nullptr);
   DCHECK(block_to != nullptr);
 
diff --git a/compiler/optimizing/register_allocator.h b/compiler/optimizing/register_allocator.h
index 6d59b1686c..eb355624e8 100644
--- a/compiler/optimizing/register_allocator.h
+++ b/compiler/optimizing/register_allocator.h
@@ -80,7 +80,10 @@ class RegisterAllocator : public DeletableArenaObject<kArenaAllocRegisterAllocat
 
   // Split `interval` at a position between `from` and `to`. The method will try
   // to find an optimal split position.
-  LiveInterval* SplitBetween(LiveInterval* interval, size_t from, size_t to);
+  static LiveInterval* SplitBetween(LiveInterval* interval,
+                                    size_t from,
+                                    size_t to,
+                                    ArrayRef<HInstruction* const> instructions_from_positions);
 
   // Helper for calling the right typed codegen function for dumping a register.
   void DumpRegister(std::ostream& stream, int reg, RegisterType register_type) const {
@@ -95,6 +98,15 @@ class RegisterAllocator : public DeletableArenaObject<kArenaAllocRegisterAllocat
   // blocks and irreducible loop headers to save memory and improve performance.
   uint32_t GetRegisterMask(LiveInterval* interval, RegisterType register_type) const;
 
+  // Helper function for `GetRegisterMask()` specialized for intervals holding a register.
+  static uint32_t GetSingleRegisterMask(LiveInterval* interval, RegisterType register_type);
+
+  // Helper function for `GetRegisterMask()` specialized for intervals holding blocked registers.
+  static uint32_t GetBlockedRegistersMask(LiveInterval* interval,
+                                          ArrayRef<HInstruction* const> instructions_from_positions,
+                                          size_t number_of_registers,
+                                          uint32_t registers_blocked_for_call);
+
   ScopedArenaAllocator* const allocator_;
   CodeGenerator* const codegen_;
   const SsaLivenessAnalysis& liveness_;
diff --git a/compiler/optimizing/register_allocator_linear_scan.cc b/compiler/optimizing/register_allocator_linear_scan.cc
index d75f1b8b46..40cb1a50cd 100644
--- a/compiler/optimizing/register_allocator_linear_scan.cc
+++ b/compiler/optimizing/register_allocator_linear_scan.cc
@@ -23,8 +23,10 @@
 #include "base/bit_vector-inl.h"
 #include "base/pointer_size.h"
 #include "code_generator.h"
+#include "com_android_art_flags.h"
 #include "linear_order.h"
 #include "register_allocation_resolver.h"
+#include "register_allocator-inl.h"
 #include "ssa_liveness_analysis.h"
 
 namespace art HIDDEN {
@@ -41,16 +43,290 @@ static bool IsLowOfUnalignedPairInterval(LiveInterval* low) {
   return GetHighForLowRegister(low->GetRegister()) != low->GetHighInterval()->GetRegister();
 }
 
+class RegisterAllocatorLinearScan::SpillSlotData {
+ public:
+  SpillSlotData(LiveInterval* interval, size_t end)
+      : end_(end),
+        gap_start_(interval->GetFirstRange()->GetEnd()),
+        interval_(interval),
+        range_(interval->GetFirstRange()) {
+    DCHECK_EQ(end, interval->GetLastSibling()->GetEnd());
+  }
+
+  size_t GetEnd() const {
+    return end_;
+  }
+
+  // Determine if the spill slot can be used for another interval `parent`.
+  // Returns `true` if the spill slot can be reused, `false` otherwise.
+  // The `parent`'s `start` and `end` are passed as arguments as a performance optimization.
+  //
+  // This is a heuristic which does not find all spill slot reuse opportunities. For that,
+  // we would need to keep track of all lifetime positions used for the spill slot, either as
+  // a bit mask, or as a list of all intervals using it, and that could take a lot of memory.
+  //
+  // Instead, we keep only the `interval_` with the longest lifetime (ending at `end_`) and
+  // the `gap_start_`, the earlier position that can be reused. When we reuse the slot for
+  // another interval that falls within `[gap_start_, end_)` and does not overlap the current
+  // `interval_`'s lifetime, we update `gap_start_` to the end of that interval.
+  bool CanUseFor(LiveInterval* parent, size_t start, size_t end) {
+    DCHECK(!parent->IsSplit());
+    DCHECK_EQ(start, parent->GetStart());
+    DCHECK_EQ(end, parent->GetLastSibling()->GetEnd());
+    // Check if the spill slot use has ended at the `start` position.
+    if (start >= end_) {
+      return true;
+    }
+    // Check if the `parent` interval can fit in the gap.
+    if (start < gap_start_ || end > end_) {
+      return false;
+    }
+    // Update search start position based on `gap_start_`.
+    while (gap_start_ >= interval_->GetEnd()) {
+      if (interval_->GetNextSibling() == nullptr) {
+        return true;  // The entire range `[gap_start_, end_)` can be used.
+      }
+      interval_ = interval_->GetNextSibling();
+      range_ = interval_->GetFirstRange();
+    }
+    while (gap_start_ >= range_->GetEnd()) {
+      range_ = range_->GetNext();
+      DCHECK(range_ != nullptr);
+    }
+    // Check if there are any overlapping ranges.
+    // This is similar to `LiveInterval::FirstIntersectionWith()` but covers sibling intervals.
+    auto move_to_next_range = [](LiveInterval*& interval, LiveRange*& range) ALWAYS_INLINE {
+      range = range->GetNext();
+      if (range == nullptr) {
+        if (interval->GetNextSibling() == nullptr) {
+          return false;
+        }
+        interval = interval->GetNextSibling();
+        range = interval->GetFirstRange();
+      }
+      return true;
+    };
+    LiveInterval* interval = interval_;
+    LiveRange* range = range_;
+    LiveInterval* other_interval = parent;
+    LiveRange* other_range = parent->GetFirstRange();
+    while (true) {
+      if (range->IsBefore(*other_range)) {
+        if (!move_to_next_range(interval, range)) {
+          return true;  // No more ranges to check.
+        }
+      } else if (other_range->IsBefore(*range)) {
+        if (!move_to_next_range(other_interval, other_range)) {
+          return true;  // No more ranges to check.
+        }
+      } else {
+        DCHECK(range->IntersectsWith(*other_range));
+        return false;
+      }
+    }
+  }
+
+  void UseFor(LiveInterval* interval, size_t end) {
+    DCHECK_EQ(end, interval->GetLastSibling()->GetEnd());
+    if (end > end_) {
+      DCHECK_GE(interval->GetParent()->GetStart(), end_);
+      *this = SpillSlotData(interval, end);
+    } else {
+      DCHECK(CanUseFor(interval->GetParent(), interval->GetParent()->GetStart(), end));
+      DCHECK_GT(end, gap_start_);
+      gap_start_ = end;
+    }
+  }
+
+ private:
+  size_t end_;
+  size_t gap_start_;
+  LiveInterval* interval_;
+  LiveRange* range_;
+};
+
+class RegisterAllocatorLinearScan::LinearScan {
+ public:
+  LinearScan(RegisterAllocatorLinearScan* register_allocator, RegisterType register_type)
+      : LinearScan(register_allocator,
+                   register_type,
+                   register_allocator->codegen_,
+                   register_allocator->allocator_) {}
+
+  void Run();
+
+ private:
+  LinearScan(RegisterAllocatorLinearScan* register_allocator,
+             RegisterType register_type,
+             CodeGenerator* codegen,
+             ScopedArenaAllocator* allocator);
+
+  static size_t GetNumberOfRegisters(CodeGenerator* codegen, RegisterType register_type) {
+    return register_type == RegisterType::kCoreRegister
+        ? codegen->GetNumberOfCoreRegisters()
+        : codegen->GetNumberOfFloatingPointRegisters();
+  }
+
+  static size_t GetRegistersBlockedForCall(
+      RegisterAllocatorLinearScan* register_allocator, RegisterType register_type) {
+    return register_type == RegisterType::kCoreRegister
+        ? register_allocator->core_registers_blocked_for_call_
+        : register_allocator->fp_registers_blocked_for_call_;
+  }
+
+  static const bool* GetBlockedRegisters(CodeGenerator* codegen, RegisterType register_type) {
+    return register_type == RegisterType::kCoreRegister
+        ? codegen->GetBlockedCoreRegisters()
+        : codegen->GetBlockedFloatingPointRegisters();
+  }
+
+  static ScopedArenaVector<SpillSlotData>* GetSpillSlots(
+      RegisterAllocatorLinearScan* register_allocator, RegisterType register_type) {
+    return register_type == RegisterType::kCoreRegister
+        ? &register_allocator->int_spill_slots_
+        : &register_allocator->float_spill_slots_;
+  }
+
+  static ScopedArenaVector<SpillSlotData>* GetWideSpillSlots(
+      RegisterAllocatorLinearScan* register_allocator, RegisterType register_type) {
+    return register_type == RegisterType::kCoreRegister
+        ? &register_allocator->long_spill_slots_
+        : &register_allocator->double_spill_slots_;
+  }
+
+  static ScopedArenaVector<LiveInterval*> TakeUnhandledIntervals(
+      RegisterAllocatorLinearScan* register_allocator, RegisterType register_type) {
+    ScopedArenaVector<LiveInterval*>* source = register_type == RegisterType::kCoreRegister
+        ? &register_allocator->unhandled_core_intervals_
+        : &register_allocator->unhandled_fp_intervals_;
+    ScopedArenaVector<LiveInterval*> result(source->get_allocator());
+    result.swap(*source);
+    return result;
+  }
+
+  static ScopedArenaVector<LiveInterval*>* GetPhysicalRegisterIntervals(
+      RegisterAllocatorLinearScan* register_allocator, RegisterType register_type) {
+    return register_type == RegisterType::kCoreRegister
+        ? &register_allocator->physical_core_register_intervals_
+        : &register_allocator->physical_fp_register_intervals_;
+  }
+
+  ALWAYS_INLINE ScopedArenaVector<SpillSlotData>* GetSpillSlotsForType(DataType::Type type) {
+    switch (type) {
+      case DataType::Type::kFloat64:
+      case DataType::Type::kInt64:
+        return wide_spill_slots_;
+      case DataType::Type::kUint32:
+      case DataType::Type::kUint64:
+      case DataType::Type::kVoid:
+        // Let the compiler optimize this away in release build.
+        DCHECK(false) << "Unexpected type for interval " << type;
+        FALLTHROUGH_INTENDED;
+      case DataType::Type::kFloat32:
+      case DataType::Type::kReference:
+      case DataType::Type::kInt32:
+      case DataType::Type::kUint16:
+      case DataType::Type::kUint8:
+      case DataType::Type::kInt8:
+      case DataType::Type::kBool:
+      case DataType::Type::kInt16:
+        return spill_slots_;
+    }
+  }
+
+  bool TryUsingSpillSlotHint(LiveInterval* interval);
+  bool TryAllocateFreeReg(LiveInterval* interval);
+  bool AllocateBlockedReg(LiveInterval* interval);
+  int FindAvailableRegisterPair(ArrayRef<size_t> next_use, size_t starting_at) const;
+  int FindAvailableRegister(ArrayRef<size_t> next_use, LiveInterval* current) const;
+
+  // Allocate a spill slot for the given interval. Should be called in linear
+  // order of interval starting positions.
+  void AllocateSpillSlotFor(LiveInterval* interval);
+
+  void DumpInterval(std::ostream& stream, LiveInterval* interval) const;
+  void DumpAllIntervals(std::ostream& stream) const;
+
+  // Try splitting an active non-pair or unaligned pair interval at the given `position`.
+  // Returns whether it was successful at finding such an interval.
+  bool TrySplitNonPairOrUnalignedPairIntervalAt(size_t position,
+                                                size_t first_register_use,
+                                                ArrayRef<size_t> next_use);
+
+  LiveInterval* SplitBetween(LiveInterval* interval, size_t from, size_t to) const {
+    return RegisterAllocator::SplitBetween(interval, from, to, instructions_from_positions_);
+  }
+
+  bool IsBlocked(int reg) const {
+    DCHECK_LT(static_cast<size_t>(reg), number_of_registers_);
+    return blocked_registers_[reg];
+  }
+
+  bool IsCallerSaveRegister(int reg) const {
+    DCHECK_LT(static_cast<size_t>(reg), BitSizeOf<uint32_t>());
+    return (registers_blocked_for_call_ & (1u << reg)) != 0u;
+  }
+
+  ArrayRef<size_t> GetRegistersArray() {
+    return ArrayRef<size_t>(registers_array_, number_of_registers_);
+  }
+
+  uint32_t GetRegisterMask(LiveInterval* interval) {
+    return interval->HasRegister()
+        ? RegisterAllocator::GetSingleRegisterMask(interval, register_type_)
+        : RegisterAllocator::GetBlockedRegistersMask(interval,
+                                                     instructions_from_positions_,
+                                                     number_of_registers_,
+                                                     registers_blocked_for_call_);
+  }
+
+  CodeGenerator* const codegen_;
+
+  // Number of registers for the current register kind (core or floating point).
+  size_t number_of_registers_;
+
+  // The register type processed by this `LinearScan` object.
+  const RegisterType register_type_;
+
+  // Mask of registers blocked for a call.
+  uint32_t registers_blocked_for_call_;
+
+  // Blocked registers, as decided by the code generator.
+  const bool* const blocked_registers_;
+
+  // Spill slots for normal and wide intervals, pointing to appropriately typed slots
+  // in the `RegisterAllocatorLinearScan`.
+  ScopedArenaVector<SpillSlotData>* const spill_slots_;
+  ScopedArenaVector<SpillSlotData>* const wide_spill_slots_;
+
+  // Currently processed list of unhandled intervals. Retrieved either from
+  // `unhandled_core_intervals_` or `unhandled_fp_intervals_`.
+  ScopedArenaVector<LiveInterval*> unhandled_;
+
+  // List of intervals that have been processed.
+  ScopedArenaVector<LiveInterval*> handled_;
+
+  // List of intervals that are currently active when processing a new live interval.
+  // That is, they have a live range that spans the start of the new interval.
+  ScopedArenaVector<LiveInterval*> active_;
+
+  // List of intervals that are currently inactive when processing a new live interval.
+  // That is, they have a lifetime hole that spans the start of the new interval.
+  ScopedArenaVector<LiveInterval*> inactive_;
+
+  // Instructions indexed by lifetime positions, cached from `SsaLivenessAnalysis`.
+  const ArrayRef<HInstruction* const> instructions_from_positions_;
+
+  // Temporary array, allocated ahead of time for simplicity.
+  size_t* registers_array_;
+};
+
 RegisterAllocatorLinearScan::RegisterAllocatorLinearScan(ScopedArenaAllocator* allocator,
                                                          CodeGenerator* codegen,
                                                          const SsaLivenessAnalysis& liveness)
       : RegisterAllocator(allocator, codegen, liveness),
         unhandled_core_intervals_(allocator->Adapter(kArenaAllocRegisterAllocator)),
         unhandled_fp_intervals_(allocator->Adapter(kArenaAllocRegisterAllocator)),
-        unhandled_(nullptr),
-        handled_(allocator->Adapter(kArenaAllocRegisterAllocator)),
-        active_(allocator->Adapter(kArenaAllocRegisterAllocator)),
-        inactive_(allocator->Adapter(kArenaAllocRegisterAllocator)),
         physical_core_register_intervals_(allocator->Adapter(kArenaAllocRegisterAllocator)),
         physical_fp_register_intervals_(allocator->Adapter(kArenaAllocRegisterAllocator)),
         block_registers_for_call_interval_(
@@ -64,11 +340,6 @@ RegisterAllocatorLinearScan::RegisterAllocatorLinearScan(ScopedArenaAllocator* a
         double_spill_slots_(allocator->Adapter(kArenaAllocRegisterAllocator)),
         catch_phi_spill_slots_(0),
         safepoints_(allocator->Adapter(kArenaAllocRegisterAllocator)),
-        current_register_type_(RegisterType::kCoreRegister),
-        number_of_registers_(-1),
-        registers_array_(nullptr),
-        blocked_core_registers_(codegen->GetBlockedCoreRegisters()),
-        blocked_fp_registers_(codegen->GetBlockedFloatingPointRegisters()),
         reserved_out_slots_(0) {
   temp_intervals_.reserve(4);
   int_spill_slots_.reserve(kDefaultNumberOfSpillSlots);
@@ -96,21 +367,20 @@ void RegisterAllocatorLinearScan::AllocateRegisters() {
                ArrayRef<LiveInterval* const>(temp_intervals_));
 
   if (kIsDebugBuild) {
-    current_register_type_ = RegisterType::kCoreRegister;
-    ValidateInternal(true);
-    current_register_type_ = RegisterType::kFpRegister;
-    ValidateInternal(true);
+    ValidateInternal(RegisterType::kCoreRegister, /*log_fatal_on_failure=*/ true);
+    ValidateInternal(RegisterType::kFpRegister, /*log_fatal_on_failure=*/ true);
     // Check that the linear order is still correct with regards to lifetime positions.
     // Since only parallel moves have been inserted during the register allocation,
     // these checks are mostly for making sure these moves have been added correctly.
     size_t current_liveness = 0;
     for (HBasicBlock* block : codegen_->GetGraph()->GetLinearOrder()) {
-      for (HInstructionIterator inst_it(block->GetPhis()); !inst_it.Done(); inst_it.Advance()) {
+      for (HInstructionIteratorPrefetchNext inst_it(block->GetPhis()); !inst_it.Done();
+           inst_it.Advance()) {
         HInstruction* instruction = inst_it.Current();
         DCHECK_LE(current_liveness, instruction->GetLifetimePosition());
         current_liveness = instruction->GetLifetimePosition();
       }
-      for (HInstructionIterator inst_it(block->GetInstructions());
+      for (HInstructionIteratorPrefetchNext inst_it(block->GetInstructions());
            !inst_it.Done();
            inst_it.Advance()) {
         HInstruction* instruction = inst_it.Current();
@@ -121,6 +391,11 @@ void RegisterAllocatorLinearScan::AllocateRegisters() {
   }
 }
 
+bool RegisterAllocatorLinearScan::Validate(bool log_fatal_on_failure) {
+  return ValidateInternal(RegisterType::kCoreRegister, log_fatal_on_failure) &&
+         ValidateInternal(RegisterType::kFpRegister, log_fatal_on_failure);
+}
+
 void RegisterAllocatorLinearScan::BlockRegister(Location location,
                                                 size_t position,
                                                 bool will_call) {
@@ -150,18 +425,20 @@ void RegisterAllocatorLinearScan::BlockRegister(Location location,
     }
   }
   DCHECK(interval->GetRegister() == reg);
-  interval->AddRange(position, position + 1u);
+  interval->AddRange(position, position + kLivenessPositionsToBlock);
 }
 
 void RegisterAllocatorLinearScan::AllocateRegistersInternal() {
   // Iterate post-order, to ensure the list is sorted, and the last added interval
   // is the one with the lowest start position.
   for (HBasicBlock* block : codegen_->GetGraph()->GetLinearPostOrder()) {
-    for (HBackwardInstructionIterator back_it(block->GetInstructions()); !back_it.Done();
+    for (HBackwardInstructionIteratorPrefetchNext back_it(block->GetInstructions());
+         !back_it.Done();
          back_it.Advance()) {
       ProcessInstruction(back_it.Current());
     }
-    for (HInstructionIterator inst_it(block->GetPhis()); !inst_it.Done(); inst_it.Advance()) {
+    for (HInstructionIteratorPrefetchNext inst_it(block->GetPhis()); !inst_it.Done();
+         inst_it.Advance()) {
       ProcessInstruction(inst_it.Current());
     }
 
@@ -171,8 +448,9 @@ void RegisterAllocatorLinearScan::AllocateRegistersInternal() {
       // intervals belonging to the live-in set of the catch/header block to be spilled.
       // TODO(ngeoffray): Phis in this block could be allocated in register.
       size_t position = block->GetLifetimeStart();
-      DCHECK_EQ(liveness_.GetInstructionFromPosition(position / 2u), nullptr);
-      block_registers_special_interval_->AddRange(position, position + 1u);
+      DCHECK_EQ(liveness_.GetInstructionFromPosition(position / kLivenessPositionsPerInstruction),
+                nullptr);
+      block_registers_special_interval_->AddRange(position, position + kLivenessPositionsToBlock);
     }
   }
 
@@ -180,50 +458,47 @@ void RegisterAllocatorLinearScan::AllocateRegistersInternal() {
   PointerSize pointer_size = InstructionSetPointerSize(codegen_->GetInstructionSet());
   reserved_out_slots_ += static_cast<size_t>(pointer_size) / kVRegSize;
 
-  number_of_registers_ = codegen_->GetNumberOfCoreRegisters();
-  registers_array_ = allocator_->AllocArray<size_t>(number_of_registers_,
-                                                    kArenaAllocRegisterAllocator);
-  current_register_type_ = RegisterType::kCoreRegister;
-  unhandled_ = &unhandled_core_intervals_;
-  // Add intervals representing groups of physical registers blocked for calls,
-  // catch blocks and irreducible loop headers.
-  for (LiveInterval* block_registers_interval : { block_registers_for_call_interval_,
-                                                  block_registers_special_interval_ }) {
-    if (block_registers_interval->GetFirstRange() != nullptr) {
-      block_registers_interval->ResetSearchCache();
-      inactive_.push_back(block_registers_interval);
-    }
-  }
-  for (LiveInterval* fixed : physical_core_register_intervals_) {
-    if (fixed != nullptr) {
-      // Fixed interval is added to inactive_ instead of unhandled_.
-      // It's also the only type of inactive interval whose start position
-      // can be after the current interval during linear scan.
-      // Fixed interval is never split and never moves to unhandled_.
-      inactive_.push_back(fixed);
-    }
+  // Most methods have some core register intervals, so run the core register pass unconditionally.
+  LinearScan(this, RegisterType::kCoreRegister).Run();
+  // Most methods do not have any FP register intervals, so try to avoid the overhead
+  // of constructing the `LinearScan` object for the FP registers pass.
+  if (!unhandled_fp_intervals_.empty()) {
+    LinearScan(this, RegisterType::kFpRegister).Run();
   }
-  LinearScan();
-
-  inactive_.clear();
-  active_.clear();
-  handled_.clear();
+}
 
-  number_of_registers_ = codegen_->GetNumberOfFloatingPointRegisters();
-  registers_array_ = allocator_->AllocArray<size_t>(number_of_registers_,
-                                                    kArenaAllocRegisterAllocator);
-  current_register_type_ = RegisterType::kFpRegister;
-  unhandled_ = &unhandled_fp_intervals_;
+RegisterAllocatorLinearScan::LinearScan::LinearScan(
+    RegisterAllocatorLinearScan* register_allocator,
+    RegisterType register_type,
+    CodeGenerator* codegen,
+    ScopedArenaAllocator* allocator)
+    : codegen_(codegen),
+      number_of_registers_(GetNumberOfRegisters(codegen, register_type)),
+      register_type_(register_type),
+      registers_blocked_for_call_(GetRegistersBlockedForCall(register_allocator, register_type)),
+      blocked_registers_(GetBlockedRegisters(codegen, register_type)),
+      spill_slots_(GetSpillSlots(register_allocator, register_type)),
+      wide_spill_slots_(GetWideSpillSlots(register_allocator, register_type)),
+      unhandled_(TakeUnhandledIntervals(register_allocator, register_type)),
+      handled_(allocator->Adapter(kArenaAllocRegisterAllocator)),
+      active_(allocator->Adapter(kArenaAllocRegisterAllocator)),
+      inactive_(allocator->Adapter(kArenaAllocRegisterAllocator)),
+      instructions_from_positions_(register_allocator->liveness_.GetInstructionsFromPositions()),
+      registers_array_(
+          allocator->AllocArray<size_t>(number_of_registers_, kArenaAllocRegisterAllocator)) {
   // Add intervals representing groups of physical registers blocked for calls,
   // catch blocks and irreducible loop headers.
-  for (LiveInterval* block_registers_interval : { block_registers_for_call_interval_,
-                                                  block_registers_special_interval_ }) {
+  LiveInterval* block_registers_intervals[] = {
+      register_allocator->block_registers_for_call_interval_,
+      register_allocator->block_registers_special_interval_
+  };
+  for (LiveInterval* block_registers_interval : block_registers_intervals) {
     if (block_registers_interval->GetFirstRange() != nullptr) {
       block_registers_interval->ResetSearchCache();
       inactive_.push_back(block_registers_interval);
     }
   }
-  for (LiveInterval* fixed : physical_fp_register_intervals_) {
+  for (LiveInterval* fixed : *GetPhysicalRegisterIntervals(register_allocator, register_type)) {
     if (fixed != nullptr) {
       // Fixed interval is added to inactive_ instead of unhandled_.
       // It's also the only type of inactive interval whose start position
@@ -232,7 +507,6 @@ void RegisterAllocatorLinearScan::AllocateRegistersInternal() {
       inactive_.push_back(fixed);
     }
   }
-  LinearScan();
 }
 
 void RegisterAllocatorLinearScan::ProcessInstruction(HInstruction* instruction) {
@@ -262,8 +536,9 @@ void RegisterAllocatorLinearScan::ProcessInstruction(HInstruction* instruction)
     // If a call will happen, add the range to a fixed interval that represents all the
     // caller-save registers blocked at call sites.
     const size_t position = instruction->GetLifetimePosition();
-    DCHECK_NE(liveness_.GetInstructionFromPosition(position / 2u), nullptr);
-    block_registers_for_call_interval_->AddRange(position, position + 1u);
+    DCHECK_NE(liveness_.GetInstructionFromPosition(position / kLivenessPositionsPerInstruction),
+              nullptr);
+    block_registers_for_call_interval_->AddRange(position, position + kLivenessPositionsToBlock);
   }
   CheckForTempLiveIntervals(instruction, will_call);
   CheckForSafepoint(instruction);
@@ -296,7 +571,10 @@ void RegisterAllocatorLinearScan::ProcessInstruction(HInstruction* instruction)
     // Split just before first register use.
     size_t first_register_use = current->FirstRegisterUse();
     if (first_register_use != kNoLifetime) {
-      LiveInterval* split = SplitBetween(current, current->GetStart(), first_register_use - 1);
+      LiveInterval* split = SplitBetween(current,
+                                         current->GetStart(),
+                                         first_register_use - 1,
+                                         liveness_.GetInstructionsFromPositions());
       // Don't add directly to `unhandled`, it needs to be sorted and the start
       // of this new interval might be after intervals already in the list.
       AddSorted(&unhandled, split);
@@ -341,20 +619,18 @@ void RegisterAllocatorLinearScan::CheckForTempLiveIntervals(HInstruction* instru
       switch (temp.GetPolicy()) {
         case Location::kRequiresRegister: {
           LiveInterval* interval =
-              LiveInterval::MakeTempInterval(allocator_, DataType::Type::kInt32);
+              LiveInterval::MakeTempInterval(allocator_, DataType::Type::kInt32, i, position);
           temp_intervals_.push_back(interval);
-          interval->AddTempUse(instruction, i);
           unhandled_core_intervals_.push_back(interval);
           break;
         }
 
         case Location::kRequiresFpuRegister: {
           LiveInterval* interval =
-              LiveInterval::MakeTempInterval(allocator_, DataType::Type::kFloat64);
+              LiveInterval::MakeTempInterval(allocator_, DataType::Type::kFloat64, i, position);
           temp_intervals_.push_back(interval);
-          interval->AddTempUse(instruction, i);
           if (codegen_->NeedsTwoRegisters(DataType::Type::kFloat64)) {
-            interval->AddHighInterval(/* is_temp= */ true);
+            interval->AddHighTempInterval();
             LiveInterval* high = interval->GetHighInterval();
             temp_intervals_.push_back(high);
             unhandled_fp_intervals_.push_back(high);
@@ -398,6 +674,8 @@ void RegisterAllocatorLinearScan::CheckForFixedInputs(HInstruction* instruction,
 
 void RegisterAllocatorLinearScan::AddSafepointsFor(HInstruction* instruction) {
   LiveInterval* current = instruction->GetLiveInterval();
+  SafepointPositionList list;
+  auto before = list.before_begin();
   for (size_t safepoint_index = safepoints_.size(); safepoint_index > 0; --safepoint_index) {
     HInstruction* safepoint = safepoints_[safepoint_index - 1u];
     size_t safepoint_position = SafepointPosition::ComputePosition(safepoint);
@@ -418,8 +696,9 @@ void RegisterAllocatorLinearScan::AddSafepointsFor(HInstruction* instruction) {
       // Hole in the interval.
       continue;
     }
-    current->AddSafepoint(safepoint);
+    before = list.insert_after(before, *current->CreateSafepointPosition(safepoint));
   }
+  current->SetSafepointPositions(std::move(list));
 }
 
 void RegisterAllocatorLinearScan::CheckForFixedOutput(HInstruction* instruction, bool will_call) {
@@ -437,28 +716,28 @@ void RegisterAllocatorLinearScan::CheckForFixedOutput(HInstruction* instruction,
   if (output.IsUnallocated() && output.GetPolicy() == Location::kSameAsFirstInput) {
     Location first = locations->InAt(0);
     if (first.IsRegister() || first.IsFpuRegister()) {
-      current->SetFrom(position + 1u);
+      current->SetFrom(position + kLivenessPositionOfFixedOutput);
       current->SetRegister(first.reg());
     } else if (first.IsPair()) {
-      current->SetFrom(position + 1u);
+      current->SetFrom(position + kLivenessPositionOfFixedOutput);
       current->SetRegister(first.low());
       LiveInterval* high = current->GetHighInterval();
       high->SetRegister(first.high());
-      high->SetFrom(position + 1u);
+      high->SetFrom(position + kLivenessPositionOfFixedOutput);
     }
   } else if (output.IsRegister() || output.IsFpuRegister()) {
     // Shift the interval's start by one to account for the blocked register.
-    current->SetFrom(position + 1u);
+    current->SetFrom(position + kLivenessPositionOfFixedOutput);
     current->SetRegister(output.reg());
     BlockRegister(output, position, will_call);
     // Ensure that an explicit output register is marked as being allocated.
     codegen_->AddAllocatedRegister(output);
   } else if (output.IsPair()) {
-    current->SetFrom(position + 1u);
+    current->SetFrom(position + kLivenessPositionOfFixedOutput);
     current->SetRegister(output.low());
     LiveInterval* high = current->GetHighInterval();
     high->SetRegister(output.high());
-    high->SetFrom(position + 1u);
+    high->SetFrom(position + kLivenessPositionOfFixedOutput);
     BlockRegister(output.ToLow(), position, will_call);
     BlockRegister(output.ToHigh(), position, will_call);
     // Ensure that an explicit output register pair is marked as being allocated.
@@ -498,8 +777,17 @@ class AllRangesIterator : public ValueObject {
   DISALLOW_COPY_AND_ASSIGN(AllRangesIterator);
 };
 
-bool RegisterAllocatorLinearScan::ValidateInternal(bool log_fatal_on_failure) const {
-  auto should_process = [](RegisterType current_register_type, LiveInterval* interval) {
+inline size_t RegisterAllocatorLinearScan::GetNumberOfSpillSlots() const {
+  return int_spill_slots_.size() +
+         long_spill_slots_.size() +
+         float_spill_slots_.size() +
+         double_spill_slots_.size() +
+         catch_phi_spill_slots_;
+}
+
+bool RegisterAllocatorLinearScan::ValidateInternal(RegisterType current_register_type,
+                                                   bool log_fatal_on_failure) const {
+  auto should_process = [current_register_type](LiveInterval* interval) {
     if (interval == nullptr) {
       return false;
     }
@@ -516,7 +804,7 @@ bool RegisterAllocatorLinearScan::ValidateInternal(bool log_fatal_on_failure) co
       allocator.Adapter(kArenaAllocRegisterAllocatorValidate));
   for (size_t i = 0; i < liveness_.GetNumberOfSsaValues(); ++i) {
     HInstruction* instruction = liveness_.GetInstructionFromSsaIndex(i);
-    if (should_process(current_register_type_, instruction->GetLiveInterval())) {
+    if (should_process(instruction->GetLiveInterval())) {
       intervals.push_back(instruction->GetLiveInterval());
     }
   }
@@ -528,7 +816,7 @@ bool RegisterAllocatorLinearScan::ValidateInternal(bool log_fatal_on_failure) co
     }
   }
   const ScopedArenaVector<LiveInterval*>* physical_register_intervals =
-      (current_register_type_ == RegisterType::kCoreRegister)
+      (current_register_type == RegisterType::kCoreRegister)
           ? &physical_core_register_intervals_
           : &physical_fp_register_intervals_;
   for (LiveInterval* fixed : *physical_register_intervals) {
@@ -538,7 +826,7 @@ bool RegisterAllocatorLinearScan::ValidateInternal(bool log_fatal_on_failure) co
   }
 
   for (LiveInterval* temp : temp_intervals_) {
-    if (should_process(current_register_type_, temp)) {
+    if (should_process(temp)) {
       intervals.push_back(temp);
     }
   }
@@ -548,11 +836,12 @@ bool RegisterAllocatorLinearScan::ValidateInternal(bool log_fatal_on_failure) co
                            reserved_out_slots_,
                            *codegen_,
                            &liveness_,
-                           current_register_type_,
+                           current_register_type,
                            log_fatal_on_failure);
 }
 
-void RegisterAllocatorLinearScan::DumpInterval(std::ostream& stream, LiveInterval* interval) const {
+void RegisterAllocatorLinearScan::LinearScan::DumpInterval(std::ostream& stream,
+                                                           LiveInterval* interval) const {
   interval->Dump(stream);
   stream << ": ";
   if (interval->HasRegister()) {
@@ -563,16 +852,17 @@ void RegisterAllocatorLinearScan::DumpInterval(std::ostream& stream, LiveInterva
     }
   } else if (interval->IsFixed()) {
     DCHECK_EQ(interval->GetType(), DataType::Type::kVoid);
-    DCHECK(interval == block_registers_for_call_interval_ ||
-           interval == block_registers_special_interval_);
-    stream << (interval == block_registers_for_call_interval_ ? "block-for-call" : "block-special");
+    size_t start = interval->GetFirstRange()->GetStart();
+    bool blocked_for_call =
+        instructions_from_positions_[start / kLivenessPositionsPerInstruction] != nullptr;
+    stream << (blocked_for_call ? "block-for-call" : "block-special");
   } else {
     stream << "spilled";
   }
   stream << std::endl;
 }
 
-void RegisterAllocatorLinearScan::DumpAllIntervals(std::ostream& stream) const {
+void RegisterAllocatorLinearScan::LinearScan::DumpAllIntervals(std::ostream& stream) const {
   stream << "inactive: " << std::endl;
   for (LiveInterval* inactive_interval : inactive_) {
     DumpInterval(stream, inactive_interval);
@@ -582,9 +872,7 @@ void RegisterAllocatorLinearScan::DumpAllIntervals(std::ostream& stream) const {
     DumpInterval(stream, active_interval);
   }
   stream << "unhandled: " << std::endl;
-  auto unhandled = (unhandled_ != nullptr) ?
-      unhandled_ : &unhandled_core_intervals_;
-  for (LiveInterval* unhandled_interval : *unhandled) {
+  for (LiveInterval* unhandled_interval : unhandled_) {
     DumpInterval(stream, unhandled_interval);
   }
   stream << "handled: " << std::endl;
@@ -594,23 +882,23 @@ void RegisterAllocatorLinearScan::DumpAllIntervals(std::ostream& stream) const {
 }
 
 // By the book implementation of a linear scan register allocator.
-void RegisterAllocatorLinearScan::LinearScan() {
+void RegisterAllocatorLinearScan::LinearScan::Run() {
   size_t last_position = std::numeric_limits<size_t>::max();
-  while (!unhandled_->empty()) {
-    // (1) Remove interval with the lowest start position from unhandled.
-    LiveInterval* current = unhandled_->back();
-    unhandled_->pop_back();
+  while (!unhandled_.empty()) {
+    // Remove interval with the lowest start position from unhandled.
+    LiveInterval* current = unhandled_.back();
+    unhandled_.pop_back();
 
     // Make sure the interval is an expected state.
     DCHECK(!current->IsFixed() && !current->HasSpillSlot());
     // Make sure we are going in the right order.
-    DCHECK(unhandled_->empty() || unhandled_->back()->GetStart() >= current->GetStart());
+    DCHECK(unhandled_.empty() || unhandled_.back()->GetStart() >= current->GetStart());
     // Make sure a low interval is always with a high.
-    DCHECK_IMPLIES(current->IsLowInterval(), unhandled_->back()->IsHighInterval());
+    DCHECK_IMPLIES(current->IsLowInterval(), unhandled_.back()->IsHighInterval());
     // Make sure a high interval is always with a low.
     DCHECK(current->IsLowInterval() ||
-           unhandled_->empty() ||
-           !unhandled_->back()->IsHighInterval());
+           unhandled_.empty() ||
+           !unhandled_.back()->IsHighInterval());
 
     size_t position = current->GetStart();
     if (position != last_position) {
@@ -618,9 +906,8 @@ void RegisterAllocatorLinearScan::LinearScan() {
       // active_ below shouldn't need to be re-checked.
       size_t inactive_intervals_to_handle = inactive_.size();
 
-      // (2) Remove currently active intervals that are dead at this position.
-      //     Move active intervals that have a lifetime hole at this position
-      //     to inactive.
+      // Remove currently active intervals that are dead at this position.
+      // Move active intervals that have a lifetime hole at this position to inactive.
       auto active_kept_end = std::remove_if(
           active_.begin(),
           active_.end(),
@@ -637,8 +924,8 @@ void RegisterAllocatorLinearScan::LinearScan() {
           });
       active_.erase(active_kept_end, active_.end());
 
-      // (3) Remove currently inactive intervals that are dead at this position.
-      //     Move inactive intervals that cover this position to active.
+      // Remove currently inactive intervals that are dead at this position.
+      // Move inactive intervals that cover this position to active.
       auto inactive_to_handle_end = inactive_.begin() + inactive_intervals_to_handle;
       auto inactive_kept_end = std::remove_if(
           inactive_.begin(),
@@ -679,18 +966,24 @@ void RegisterAllocatorLinearScan::LinearScan() {
       continue;
     }
 
-    // (4) Try to find an available register.
+    // For a Phi which has all inputs in the same spill slot as its spill slot hint, use that hint.
+    if (com::android::art::flags::reg_alloc_spill_slot_reuse() &&
+        current->HasSpillSlotHint() &&
+        TryUsingSpillSlotHint(current)) {
+      continue;
+    }
+
+    // Try to find an available register.
     bool success = TryAllocateFreeReg(current);
 
-    // (5) If no register could be found, we need to spill.
+    // If no register could be found, we need to spill.
     if (!success) {
       success = AllocateBlockedReg(current);
     }
 
-    // (6) If the interval had a register allocated, add it to the list of active
-    //     intervals.
+    // If the interval had a register allocated, add it to the list of active intervals.
     if (success) {
-      codegen_->AddAllocatedRegister((current_register_type_ == RegisterType::kCoreRegister)
+      codegen_->AddAllocatedRegister((register_type_ == RegisterType::kCoreRegister)
           ? Location::RegisterLocation(current->GetRegister())
           : Location::FpuRegisterLocation(current->GetRegister()));
       active_.push_back(current);
@@ -701,7 +994,7 @@ void RegisterAllocatorLinearScan::LinearScan() {
   }
 }
 
-static void FreeIfNotCoverAt(LiveInterval* interval, size_t position, size_t* free_until) {
+static void FreeIfNotCoverAt(LiveInterval* interval, size_t position, ArrayRef<size_t> free_until) {
   DCHECK(!interval->IsHighInterval());
   // Note that the same instruction may occur multiple times in the input list,
   // so `free_until` may have changed already.
@@ -725,20 +1018,69 @@ static void FreeIfNotCoverAt(LiveInterval* interval, size_t position, size_t* fr
   }
 }
 
+bool RegisterAllocatorLinearScan::LinearScan::TryUsingSpillSlotHint(LiveInterval* current) {
+  DCHECK(current->HasSpillSlotHint());
+  int hint = current->GetSpillSlotHint();
+  DCHECK(current->GetDefinedBy() != nullptr);
+  HBasicBlock* block = current->GetDefinedBy()->GetBlock();
+  DCHECK(current->GetDefinedBy()->IsPhi());
+  auto inputs = current->GetDefinedBy()->AsPhi()->GetInputs();
+  DCHECK_EQ(inputs.size(), block->GetPredecessors().size());
+
+  // Check if all inputs have the same spill slot as `hint` and that none of them
+  // has a register allocated before the incoming edge.
+  for (auto [predecessor, input_index] : ZipCount(block->GetPredecessors())) {
+    DCHECK_EQ(predecessor->GetNormalSuccessors().size(), 1u);
+    LiveInterval* input_li = inputs[input_index]->GetLiveInterval();
+    if (input_li->GetSpillSlot() != hint ||
+        input_li->GetSiblingAt(predecessor->GetLifetimeEnd() - 1)->HasRegister()) {
+      return false;
+    }
+  }
+
+  // Check that the required spill slots are available at the start of the `current` interval.
+  ScopedArenaVector<SpillSlotData>* spill_slots = GetSpillSlotsForType(current->GetType());
+  size_t number_of_spill_slots_needed = current->NumberOfSpillSlotsNeeded();
+  DCHECK_LE(hint + number_of_spill_slots_needed, spill_slots->size());
+  DCHECK(current->GetParent() == current);
+  size_t start = current->GetStart();
+  size_t end = current->GetLastSibling()->GetEnd();
+  ArrayRef<SpillSlotData> range =
+      ArrayRef<SpillSlotData>(*spill_slots).SubArray(hint, number_of_spill_slots_needed);
+  if (std::any_of(range.begin(),
+                  range.end(),
+                  [=](const SpillSlotData& data) { return data.GetEnd() > start; })) {
+    return false;
+  }
+
+  // Use the spill slots and split the `current` interval if there is any register use.
+  SpillSlotData new_data(current, end);
+  for (SpillSlotData& data : range) {
+    DCHECK_LE(data.GetEnd(), start);
+    data = new_data;
+  }
+  current->SetSpillSlot(hint);
+  size_t first_register_use = current->FirstRegisterUse();
+  if (first_register_use != kNoLifetime) {
+    LiveInterval* split = SplitBetween(current, current->GetStart(), first_register_use - 1);
+    DCHECK(current != split);
+    AddSorted(&unhandled_, split);
+  }
+  handled_.push_back(current);
+  return true;
+}
+
 // Find a free register. If multiple are found, pick the register that
 // is free the longest.
-bool RegisterAllocatorLinearScan::TryAllocateFreeReg(LiveInterval* current) {
-  size_t* free_until = registers_array_;
-
+bool RegisterAllocatorLinearScan::LinearScan::TryAllocateFreeReg(LiveInterval* current) {
   // First set all registers to be free.
-  for (size_t i = 0; i < number_of_registers_; ++i) {
-    free_until[i] = kMaxLifetimePosition;
-  }
+  ArrayRef<size_t> free_until = GetRegistersArray();
+  std::fill_n(free_until.begin(), free_until.size(), kMaxLifetimePosition);
 
   // For each active interval, set its register(s) to not free.
   for (LiveInterval* interval : active_) {
     DCHECK(interval->HasRegister() || interval->IsFixed());
-    uint32_t register_mask = GetRegisterMask(interval, current_register_type_);
+    uint32_t register_mask = GetRegisterMask(interval);
     DCHECK_NE(register_mask, 0u);
     for (uint32_t reg : LowToHighBits(register_mask)) {
       free_until[reg] = 0;
@@ -766,7 +1108,7 @@ bool RegisterAllocatorLinearScan::TryAllocateFreeReg(LiveInterval* current) {
             // position to check whether the input is dead or is inactive after
             // `defined_by`.
             DCHECK(interval->CoversSlow(defined_by->GetLifetimePosition()));
-            size_t position = defined_by->GetLifetimePosition() + 1;
+            size_t position = defined_by->GetLifetimePosition() + kLivenessPositionOfNormalUse;
             FreeIfNotCoverAt(interval, position, free_until);
           }
         }
@@ -789,7 +1131,7 @@ bool RegisterAllocatorLinearScan::TryAllocateFreeReg(LiveInterval* current) {
     }
 
     DCHECK(inactive->HasRegister() || inactive->IsFixed());
-    uint32_t register_mask = GetRegisterMask(inactive, current_register_type_);
+    uint32_t register_mask = GetRegisterMask(inactive);
     DCHECK_NE(register_mask, 0u);
     for (uint32_t reg : LowToHighBits(register_mask)) {
       if (free_until[reg] == 0) {
@@ -818,7 +1160,7 @@ bool RegisterAllocatorLinearScan::TryAllocateFreeReg(LiveInterval* current) {
     }
   } else {
     DCHECK(!current->IsHighInterval());
-    int hint = current->FindFirstRegisterHint(free_until, liveness_);
+    int hint = current->FindFirstRegisterHint(free_until, instructions_from_positions_);
     if ((hint != kNoRegister)
         // For simplicity, if the hint we are getting for a pair cannot be used,
         // we are just going to allocate a new pair.
@@ -856,21 +1198,16 @@ bool RegisterAllocatorLinearScan::TryAllocateFreeReg(LiveInterval* current) {
     // the register is not available anymore.
     LiveInterval* split = SplitBetween(current, current->GetStart(), free_until[reg]);
     DCHECK(split != nullptr);
-    AddSorted(unhandled_, split);
+    AddSorted(&unhandled_, split);
   }
   return true;
 }
 
-bool RegisterAllocatorLinearScan::IsBlocked(int reg) const {
-  return (current_register_type_ == RegisterType::kCoreRegister)
-      ? blocked_core_registers_[reg]
-      : blocked_fp_registers_[reg];
-}
-
-int RegisterAllocatorLinearScan::FindAvailableRegisterPair(size_t* next_use, size_t starting_at) const {
+int RegisterAllocatorLinearScan::LinearScan::FindAvailableRegisterPair(ArrayRef<size_t> next_use,
+                                                                       size_t starting_at) const {
   int reg = kNoRegister;
   // Pick the register pair that is used the last.
-  for (size_t i = 0; i < number_of_registers_; ++i) {
+  for (size_t i : Range(number_of_registers_)) {
     if (IsBlocked(i)) continue;
     if (!IsLowRegister(i)) continue;
     int high_register = GetHighForLowRegister(i);
@@ -892,21 +1229,14 @@ int RegisterAllocatorLinearScan::FindAvailableRegisterPair(size_t* next_use, siz
   return reg;
 }
 
-bool RegisterAllocatorLinearScan::IsCallerSaveRegister(int reg) const {
-  uint32_t registers_blocked_for_call = (current_register_type_ == RegisterType::kCoreRegister)
-      ? core_registers_blocked_for_call_
-      : fp_registers_blocked_for_call_;
-  DCHECK_LT(static_cast<size_t>(reg), BitSizeOf<uint32_t>());
-  return (registers_blocked_for_call & (1u << reg)) != 0u;
-}
-
-int RegisterAllocatorLinearScan::FindAvailableRegister(size_t* next_use, LiveInterval* current) const {
+int RegisterAllocatorLinearScan::LinearScan::FindAvailableRegister(ArrayRef<size_t> next_use,
+                                                                   LiveInterval* current) const {
   // We special case intervals that do not span a safepoint to try to find a caller-save
   // register if one is available. We iterate from 0 to the number of registers,
   // so if there are caller-save registers available at the end, we continue the iteration.
   bool prefers_caller_save = !current->HasWillCallSafepoint();
   int reg = kNoRegister;
-  for (size_t i = 0; i < number_of_registers_; ++i) {
+  for (size_t i : Range(number_of_registers_)) {
     if (IsBlocked(i)) {
       // Register cannot be used. Continue.
       continue;
@@ -963,9 +1293,8 @@ static ArenaVector<LiveInterval*>::iterator RemoveIntervalAndPotentialOtherHalf(
   }
 }
 
-bool RegisterAllocatorLinearScan::TrySplitNonPairOrUnalignedPairIntervalAt(size_t position,
-                                                                           size_t first_register_use,
-                                                                           size_t* next_use) {
+bool RegisterAllocatorLinearScan::LinearScan::TrySplitNonPairOrUnalignedPairIntervalAt(
+    size_t position, size_t first_register_use, ArrayRef<size_t> next_use) {
   for (auto it = active_.begin(), end = active_.end(); it != end; ++it) {
     LiveInterval* active = *it;
     // Special fixed intervals that represent multiple registers do not report having a register.
@@ -986,7 +1315,7 @@ bool RegisterAllocatorLinearScan::TrySplitNonPairOrUnalignedPairIntervalAt(size_
         handled_.push_back(active);
       }
       RemoveIntervalAndPotentialOtherHalf(&active_, it);
-      AddSorted(unhandled_, split);
+      AddSorted(&unhandled_, split);
       return true;
     }
   }
@@ -996,7 +1325,7 @@ bool RegisterAllocatorLinearScan::TrySplitNonPairOrUnalignedPairIntervalAt(size_
 // Find the register that is used the last, and spill the interval
 // that holds it. If the first use of `current` is after that register
 // we spill `current` instead.
-bool RegisterAllocatorLinearScan::AllocateBlockedReg(LiveInterval* current) {
+bool RegisterAllocatorLinearScan::LinearScan::AllocateBlockedReg(LiveInterval* current) {
   size_t first_register_use = current->FirstRegisterUse();
   if (current->HasRegister()) {
     DCHECK(current->IsHighInterval());
@@ -1013,25 +1342,29 @@ bool RegisterAllocatorLinearScan::AllocateBlockedReg(LiveInterval* current) {
   }
 
   // First set all registers as not being used.
-  size_t* next_use = registers_array_;
-  for (size_t i = 0; i < number_of_registers_; ++i) {
-    next_use[i] = kMaxLifetimePosition;
-  }
+  ArrayRef<size_t> next_use = GetRegistersArray();
+  std::fill_n(next_use.begin(), next_use.size(), kMaxLifetimePosition);
 
   // For each active interval, find the next use of its register after the
   // start of current.
   for (LiveInterval* active : active_) {
-    if (active->IsFixed()) {
-      uint32_t register_mask = GetRegisterMask(active, current_register_type_);
-      DCHECK_NE(register_mask, 0u);
-      for (uint32_t reg : LowToHighBits(register_mask)) {
-        next_use[reg] = current->GetStart();
+    size_t use = current->GetStart();
+    if (active->HasRegister()) {
+      size_t reg = active->GetRegister();
+      bool has_use_after = true;
+      if (!active->IsFixed() && !active->IsTemp()) {
+        use = active->FirstRegisterUseAfter(use);
+        has_use_after = use != kNoLifetime;
+      }
+      if (has_use_after) {
+        next_use[reg] = use;
       }
     } else {
-      DCHECK(active->HasRegister());
-      size_t use = active->FirstRegisterUseAfter(current->GetStart());
-      if (use != kNoLifetime) {
-        next_use[active->GetRegister()] = use;
+      DCHECK(active->IsFixed());
+      uint32_t register_mask = GetRegisterMask(active);
+      DCHECK_NE(register_mask, 0u);
+      for (uint32_t reg : LowToHighBits(register_mask)) {
+        next_use[reg] = use;
       }
     }
   }
@@ -1053,7 +1386,7 @@ bool RegisterAllocatorLinearScan::AllocateBlockedReg(LiveInterval* current) {
     size_t next_intersection = inactive->FirstIntersectionWith(current);
     if (next_intersection != kNoLifetime) {
       if (inactive->IsFixed()) {
-        uint32_t register_mask = GetRegisterMask(inactive, current_register_type_);
+        uint32_t register_mask = GetRegisterMask(inactive);
         DCHECK_NE(register_mask, 0u);
         for (uint32_t reg : LowToHighBits(register_mask)) {
           next_use[reg] = std::min(next_intersection, next_use[reg]);
@@ -1094,7 +1427,8 @@ bool RegisterAllocatorLinearScan::AllocateBlockedReg(LiveInterval* current) {
         DumpInterval(std::cerr, current);
         DumpAllIntervals(std::cerr);
         // This situation has the potential to infinite loop, so we make it a non-debug CHECK.
-        HInstruction* at = liveness_.GetInstructionFromPosition(first_register_use / 2);
+        HInstruction* at =
+            instructions_from_positions_[first_register_use / kLivenessPositionsPerInstruction];
         CHECK(false) << "There is not enough registers available for "
           << current->GetParent()->GetDefinedBy()->DebugName() << " "
           << current->GetParent()->GetDefinedBy()->GetId()
@@ -1111,17 +1445,17 @@ bool RegisterAllocatorLinearScan::AllocateBlockedReg(LiveInterval* current) {
                                                               first_register_use,
                                                               next_use);
       DCHECK(success);
-      LiveInterval* existing = unhandled_->back();
+      LiveInterval* existing = unhandled_.back();
       DCHECK(existing->IsHighInterval());
       DCHECK_EQ(existing->GetLowInterval(), current);
-      unhandled_->push_back(current);
+      unhandled_.push_back(current);
     } else {
       // If the first use of that instruction is after the last use of the found
       // register, we split this interval just before its first register use.
       AllocateSpillSlotFor(current);
       LiveInterval* split = SplitBetween(current, current->GetStart(), first_register_use - 1);
       DCHECK(current != split);
-      AddSorted(unhandled_, split);
+      AddSorted(&unhandled_, split);
     }
     return false;
   } else {
@@ -1131,8 +1465,7 @@ bool RegisterAllocatorLinearScan::AllocateBlockedReg(LiveInterval* current) {
 
     for (auto it = active_.begin(), end = active_.end(); it != end; ++it) {
       LiveInterval* active = *it;
-      DCHECK_IMPLIES(active->IsFixed(),
-                     (GetRegisterMask(active, current_register_type_) & (1u << reg)) == 0u);
+      DCHECK_IMPLIES(active->IsFixed(), (GetRegisterMask(active) & (1u << reg)) == 0u);
       if (active->GetRegister() == reg) {
         DCHECK(!active->IsFixed());
         LiveInterval* split = Split(active, current->GetStart());
@@ -1140,7 +1473,7 @@ bool RegisterAllocatorLinearScan::AllocateBlockedReg(LiveInterval* current) {
           handled_.push_back(active);
         }
         RemoveIntervalAndPotentialOtherHalf(&active_, it);
-        AddSorted(unhandled_, split);
+        AddSorted(&unhandled_, split);
         break;
       }
     }
@@ -1149,8 +1482,9 @@ bool RegisterAllocatorLinearScan::AllocateBlockedReg(LiveInterval* current) {
     for (auto it = inactive_.begin(); it != inactive_.end(); ) {
       LiveInterval* inactive = *it;
       bool erased = false;
-      if ((GetRegisterMask(inactive, current_register_type_) & (1u << reg)) != 0u) {
-        if (!current->IsSplit() && !inactive->IsFixed()) {
+      if ((inactive->HasRegister() || inactive->IsFixed()) &&
+          (GetRegisterMask(inactive) & (1u << reg)) != 0u) {
+        if (!inactive->IsFixed() && !current->IsSplit()) {
           // Neither current nor inactive are fixed.
           // Thanks to SSA, a non-split interval starting in a hole of an
           // inactive interval should never intersect with that inactive interval.
@@ -1162,7 +1496,7 @@ bool RegisterAllocatorLinearScan::AllocateBlockedReg(LiveInterval* current) {
             if (inactive->IsFixed()) {
               LiveInterval* split = Split(current, next_intersection);
               DCHECK_NE(split, current);
-              AddSorted(unhandled_, split);
+              AddSorted(&unhandled_, split);
             } else {
               // Split at the start of `current`, which will lead to splitting
               // at the end of the lifetime hole of `inactive`.
@@ -1172,7 +1506,7 @@ bool RegisterAllocatorLinearScan::AllocateBlockedReg(LiveInterval* current) {
               it = RemoveIntervalAndPotentialOtherHalf(&inactive_, it);
               erased = true;
               handled_.push_back(inactive);
-              AddSorted(unhandled_, split);
+              AddSorted(&unhandled_, split);
             }
           }
         }
@@ -1212,7 +1546,7 @@ void RegisterAllocatorLinearScan::AddSorted(ScopedArenaVector<LiveInterval*>* ar
   }
 }
 
-void RegisterAllocatorLinearScan::AllocateSpillSlotFor(LiveInterval* interval) {
+void RegisterAllocatorLinearScan::LinearScan::AllocateSpillSlotFor(LiveInterval* interval) {
   if (interval->IsHighInterval()) {
     // The low interval already took care of allocating the spill slot.
     DCHECK(!interval->GetLowInterval()->HasRegister());
@@ -1247,62 +1581,80 @@ void RegisterAllocatorLinearScan::AllocateSpillSlotFor(LiveInterval* interval) {
     return;
   }
 
-  ScopedArenaVector<size_t>* spill_slots = nullptr;
-  switch (interval->GetType()) {
-    case DataType::Type::kFloat64:
-      spill_slots = &double_spill_slots_;
-      break;
-    case DataType::Type::kInt64:
-      spill_slots = &long_spill_slots_;
-      break;
-    case DataType::Type::kFloat32:
-      spill_slots = &float_spill_slots_;
-      break;
-    case DataType::Type::kReference:
-    case DataType::Type::kInt32:
-    case DataType::Type::kUint16:
-    case DataType::Type::kUint8:
-    case DataType::Type::kInt8:
-    case DataType::Type::kBool:
-    case DataType::Type::kInt16:
-      spill_slots = &int_spill_slots_;
-      break;
-    case DataType::Type::kUint32:
-    case DataType::Type::kUint64:
-    case DataType::Type::kVoid:
-      LOG(FATAL) << "Unexpected type for interval " << interval->GetType();
+  ScopedArenaVector<SpillSlotData>* spill_slots = GetSpillSlotsForType(interval->GetType());
+  size_t number_of_spill_slots_needed = parent->NumberOfSpillSlotsNeeded();
+  size_t start = parent->GetStart();
+  size_t end = interval->GetLastSibling()->GetEnd();
+  size_t slot = 0;
+  bool used_hint = false;
+
+  if (com::android::art::flags::reg_alloc_spill_slot_reuse()) {
+    LiveInterval* hint_phi_interval = parent->GetHintPhiInterval();
+    // If the immediate hint Phi does not have a spill hint, we could try to follow the
+    // hint Phi chain to a Phi that does. However, we would need to make sure we don't go
+    // over a Phi loop forever. And we would need to investigate if the additional spill
+    // slot sharing we can find this way is worth the increase in compilation time.
+    if (hint_phi_interval != nullptr && hint_phi_interval->HasSpillSlotOrHint()) {
+      size_t hint = hint_phi_interval->GetSpillSlotHint();
+      DCHECK_LE(hint + number_of_spill_slots_needed, spill_slots->size());
+      ArrayRef<SpillSlotData> range =
+          ArrayRef<SpillSlotData>(*spill_slots).SubArray(hint, number_of_spill_slots_needed);
+      if (std::all_of(range.begin(),
+                      range.end(),
+                      [=](SpillSlotData& data) { return data.CanUseFor(parent, start, end); })) {
+        // Update slots and use the hint.
+        for (SpillSlotData& data : range) {
+          data.UseFor(interval, end);
+        }
+        used_hint = true;
+        slot = hint;
+      }
+    }
   }
 
   // Find first available spill slots.
-  size_t number_of_spill_slots_needed = parent->NumberOfSpillSlotsNeeded();
-  size_t slot = 0;
-  for (size_t e = spill_slots->size(); slot < e; ++slot) {
-    bool found = true;
-    for (size_t s = slot, u = std::min(slot + number_of_spill_slots_needed, e); s < u; s++) {
-      if ((*spill_slots)[s] > parent->GetStart()) {
-        found = false;  // failure
-        break;
+  if (!used_hint) {
+    for (size_t e = spill_slots->size(); slot < e; ++slot) {
+      bool found = true;
+      for (size_t s = slot, u = std::min(slot + number_of_spill_slots_needed, e); s < u; s++) {
+        if ((*spill_slots)[s].GetEnd() > start) {
+          found = false;  // failure
+          break;
+        }
+      }
+      if (found) {
+        break;  // success
       }
     }
-    if (found) {
-      break;  // success
+
+    // Need new spill slots?
+    SpillSlotData new_data(interval, end);
+    size_t num_old_slots = spill_slots->size() - slot;
+    if (num_old_slots < number_of_spill_slots_needed) {
+      spill_slots->resize(slot + number_of_spill_slots_needed, new_data);
+      // Update only old slots below.
+      number_of_spill_slots_needed = num_old_slots;
     }
-  }
 
-  // Need new spill slots?
-  size_t upper = slot + number_of_spill_slots_needed;
-  if (upper > spill_slots->size()) {
-    spill_slots->resize(upper);
-  }
-  // Set slots to end.
-  size_t end = interval->GetLastSibling()->GetEnd();
-  for (size_t s = slot; s < upper; s++) {
-    (*spill_slots)[s] = end;
+    // Set slots to end.
+    for (size_t s : Range(slot, slot + number_of_spill_slots_needed)) {
+      SpillSlotData& data = (*spill_slots)[s];
+      DCHECK_LE(data.GetEnd(), start);
+      data = new_data;
+    }
   }
 
   // Note that the exact spill slot location will be computed when we resolve,
   // that is when we know the number of spill slots for each type.
   parent->SetSpillSlot(slot);
+
+  if (com::android::art::flags::reg_alloc_spill_slot_reuse()) {
+    LiveInterval* hint_phi_interval = parent->GetHintPhiInterval();
+    while (hint_phi_interval != nullptr && !hint_phi_interval->HasSpillSlotOrHint()) {
+      hint_phi_interval->SetSpillSlotHint(slot);
+      hint_phi_interval = hint_phi_interval->GetHintPhiInterval();
+    }
+  }
 }
 
 void RegisterAllocatorLinearScan::AllocateSpillSlotForCatchPhi(HPhi* phi) {
diff --git a/compiler/optimizing/register_allocator_linear_scan.h b/compiler/optimizing/register_allocator_linear_scan.h
index 296c41d073..c54a709138 100644
--- a/compiler/optimizing/register_allocator_linear_scan.h
+++ b/compiler/optimizing/register_allocator_linear_scan.h
@@ -46,42 +46,19 @@ class RegisterAllocatorLinearScan : public RegisterAllocator {
 
   void AllocateRegisters() override;
 
-  bool Validate(bool log_fatal_on_failure) override {
-    current_register_type_ = RegisterType::kCoreRegister;
-    if (!ValidateInternal(log_fatal_on_failure)) {
-      return false;
-    }
-    current_register_type_ = RegisterType::kFpRegister;
-    return ValidateInternal(log_fatal_on_failure);
-  }
-
-  size_t GetNumberOfSpillSlots() const {
-    return int_spill_slots_.size()
-        + long_spill_slots_.size()
-        + float_spill_slots_.size()
-        + double_spill_slots_.size()
-        + catch_phi_spill_slots_;
-  }
+  bool Validate(bool log_fatal_on_failure) override;
+
+  size_t GetNumberOfSpillSlots() const;
 
  private:
-  // Main methods of the allocator.
-  void LinearScan();
-  bool TryAllocateFreeReg(LiveInterval* interval);
-  bool AllocateBlockedReg(LiveInterval* interval);
+  class LinearScan;
 
   // Add `interval` in the given sorted list.
   static void AddSorted(ScopedArenaVector<LiveInterval*>* array, LiveInterval* interval);
 
-  // Returns whether `reg` is blocked by the code generator.
-  bool IsBlocked(int reg) const;
-
   // Update the interval for the register in `location` to cover [start, end).
   void BlockRegister(Location location, size_t position, bool will_call);
 
-  // Allocate a spill slot for the given interval. Should be called in linear
-  // order of interval starting positions.
-  void AllocateSpillSlotFor(LiveInterval* interval);
-
   // Allocate a spill slot for the given catch phi. Will allocate the same slot
   // for phis which share the same vreg. Must be called in reverse linear order
   // of lifetime positions and ascending vreg numbers for correctness.
@@ -90,12 +67,7 @@ class RegisterAllocatorLinearScan : public RegisterAllocator {
   // Helper methods.
   void AllocateRegistersInternal();
   void ProcessInstruction(HInstruction* instruction);
-  bool ValidateInternal(bool log_fatal_on_failure) const;
-  void DumpInterval(std::ostream& stream, LiveInterval* interval) const;
-  void DumpAllIntervals(std::ostream& stream) const;
-  int FindAvailableRegisterPair(size_t* next_use, size_t starting_at) const;
-  int FindAvailableRegister(size_t* next_use, LiveInterval* current) const;
-  bool IsCallerSaveRegister(int reg) const;
+  bool ValidateInternal(RegisterType current_register_type, bool log_fatal_on_failure) const;
 
   // If any inputs require specific registers, block those registers
   // at the position of this instruction.
@@ -120,12 +92,6 @@ class RegisterAllocatorLinearScan : public RegisterAllocator {
   // Try to remove the SuspendCheck at function entry. Returns true if it was successful.
   bool TryRemoveSuspendCheckEntry(HInstruction* instruction);
 
-  // Try splitting an active non-pair or unaligned pair interval at the given `position`.
-  // Returns whether it was successful at finding such an interval.
-  bool TrySplitNonPairOrUnalignedPairIntervalAt(size_t position,
-                                                size_t first_register_use,
-                                                size_t* next_use);
-
   // List of intervals for core registers that must be processed, ordered by start
   // position. Last entry is the interval that has the lowest start position.
   // This list is initially populated before doing the linear scan.
@@ -134,21 +100,6 @@ class RegisterAllocatorLinearScan : public RegisterAllocator {
   // List of intervals for floating-point registers. Same comments as above.
   ScopedArenaVector<LiveInterval*> unhandled_fp_intervals_;
 
-  // Currently processed list of unhandled intervals. Either `unhandled_core_intervals_`
-  // or `unhandled_fp_intervals_`.
-  ScopedArenaVector<LiveInterval*>* unhandled_;
-
-  // List of intervals that have been processed.
-  ScopedArenaVector<LiveInterval*> handled_;
-
-  // List of intervals that are currently active when processing a new live interval.
-  // That is, they have a live range that spans the start of the new interval.
-  ScopedArenaVector<LiveInterval*> active_;
-
-  // List of intervals that are currently inactive when processing a new live interval.
-  // That is, they have a lifetime hole that spans the start of the new interval.
-  ScopedArenaVector<LiveInterval*> inactive_;
-
   // Fixed intervals for physical registers. Such intervals cover the positions
   // where an instruction requires a specific register.
   ScopedArenaVector<LiveInterval*> physical_core_register_intervals_;
@@ -164,10 +115,11 @@ class RegisterAllocatorLinearScan : public RegisterAllocator {
   // are typed to avoid (1) doing moves and swaps between two different kinds
   // of registers, and (2) swapping between a single stack slot and a double
   // stack slot. This simplifies the parallel move resolver.
-  ScopedArenaVector<size_t> int_spill_slots_;
-  ScopedArenaVector<size_t> long_spill_slots_;
-  ScopedArenaVector<size_t> float_spill_slots_;
-  ScopedArenaVector<size_t> double_spill_slots_;
+  class SpillSlotData;
+  ScopedArenaVector<SpillSlotData> int_spill_slots_;
+  ScopedArenaVector<SpillSlotData> long_spill_slots_;
+  ScopedArenaVector<SpillSlotData> float_spill_slots_;
+  ScopedArenaVector<SpillSlotData> double_spill_slots_;
 
   // Spill slots allocated to catch phis. This category is special-cased because
   // (1) slots are allocated prior to linear scan and in reverse linear order,
@@ -177,24 +129,10 @@ class RegisterAllocatorLinearScan : public RegisterAllocator {
   // Instructions that need a safepoint.
   ScopedArenaVector<HInstruction*> safepoints_;
 
-  // The register type we're currently processing.
-  RegisterType current_register_type_;
-
-  // Number of registers for the current register kind (core or floating point).
-  size_t number_of_registers_;
-
-  // Temporary array, allocated ahead of time for simplicity.
-  size_t* registers_array_;
-
-  // Blocked registers, as decided by the code generator.
-  bool* const blocked_core_registers_;
-  bool* const blocked_fp_registers_;
-
   // Slots reserved for out arguments.
   size_t reserved_out_slots_;
 
-  ART_FRIEND_TEST(RegisterAllocatorTest, FreeUntil);
-  ART_FRIEND_TEST(RegisterAllocatorTest, SpillInactive);
+  friend class RegisterAllocatorTest;
 
   DISALLOW_COPY_AND_ASSIGN(RegisterAllocatorLinearScan);
 };
diff --git a/compiler/optimizing/register_allocator_test.cc b/compiler/optimizing/register_allocator_test.cc
index e5eb54c2c2..94353fc54e 100644
--- a/compiler/optimizing/register_allocator_test.cc
+++ b/compiler/optimizing/register_allocator_test.cc
@@ -22,6 +22,7 @@
 #include "builder.h"
 #include "code_generator.h"
 #include "code_generator_x86.h"
+#include "com_android_art_flags.h"
 #include "dex/dex_file.h"
 #include "dex/dex_file_types.h"
 #include "dex/dex_instruction.h"
@@ -47,10 +48,10 @@ class RegisterAllocatorTest : public CommonCompilerTest, public OptimizingUnitTe
 
   // Helper functions that make use of the OptimizingUnitTest's members.
   bool Check(const std::vector<uint16_t>& data);
-  HGraph* BuildIfElseWithPhi(HPhi** phi, HInstruction** input1, HInstruction** input2);
-  HGraph* BuildFieldReturn(HInstruction** field, HInstruction** ret);
-  HGraph* BuildTwoSubs(HInstruction** first_sub, HInstruction** second_sub);
-  HGraph* BuildDiv(HInstruction** div);
+  void BuildIfElseWithPhi(HPhi** phi, HInstruction** input1, HInstruction** input2);
+  void BuildFieldReturn(HInstruction** field, HInstruction** ret);
+  void BuildTwoSubs(HInstruction** first_sub, HInstruction** second_sub);
+  void BuildDiv(HInstruction** div);
 
   bool ValidateIntervals(const ScopedArenaVector<LiveInterval*>& intervals,
                          const CodeGenerator& codegen) {
@@ -63,6 +64,9 @@ class RegisterAllocatorTest : public CommonCompilerTest, public OptimizingUnitTe
                                                 /* log_fatal_on_failure= */ false);
   }
 
+  void TestFreeUntil(bool special_first);
+  void TestSpillInactive();
+
   std::unique_ptr<CompilerOptions> compiler_options_;
 };
 
@@ -398,101 +402,92 @@ TEST_F(RegisterAllocatorTest, DeadPhi) {
  * allocating for at the minimum lifetime position between the two inactive intervals.
  * This test only applies to the linear scan allocator.
  */
-TEST_F(RegisterAllocatorTest, FreeUntil) {
-  const std::vector<uint16_t> data = TWO_REGISTERS_CODE_ITEM(
-    Instruction::CONST_4 | 0 | 0,
-    Instruction::RETURN);
-
-  HGraph* graph = CreateCFG(data);
-  SsaDeadPhiElimination(graph).Run();
-  x86::CodeGeneratorX86 codegen(graph, *compiler_options_);
-  SsaLivenessAnalysis liveness(graph, &codegen, GetScopedAllocator());
+void RegisterAllocatorTest::TestFreeUntil(bool special_first) {
+  HBasicBlock* block = InitEntryMainExitGraphWithReturnVoid();
+  HInstruction* const0 = graph_->GetIntConstant(0);
+
+  HAdd* add = MakeBinOp<HAdd>(block, DataType::Type::kInt32, const0, const0);
+  HInstruction* placeholder1 = MakeUnOp<HNeg>(block, DataType::Type::kInt32, const0);
+  HInstruction* placeholder2 = MakeUnOp<HNeg>(block, DataType::Type::kInt32, const0);
+  HInstruction* ret = MakeReturn(block, add);
+
+  graph_->ComputeDominanceInformation();
+  x86::CodeGeneratorX86 codegen(graph_, *compiler_options_);
+  SsaLivenessAnalysis liveness(graph_, &codegen, GetScopedAllocator());
   liveness.Analyze();
-  RegisterAllocatorLinearScan register_allocator(GetScopedAllocator(), &codegen, liveness);
-
-  // Add an artifical range to cover the temps that will be put in the unhandled list.
-  LiveInterval* unhandled = graph->GetEntryBlock()->GetFirstInstruction()->GetLiveInterval();
-  unhandled->AddLoopRange(0, 60);
 
-  // Populate the instructions in the liveness object, to please the register allocator.
-  for (size_t i = 0; i < 60; ++i) {
-    liveness.instructions_from_lifetime_position_.push_back(
-        graph->GetEntryBlock()->GetFirstInstruction());
+  // Avoid allocating the register for the `const0` when used by the `add`.
+  add->GetLocations()->SetInAt(0, Location::ConstantLocation(const0));
+  ASSERT_TRUE(add->GetLocations()->InAt(1).IsConstant());
+
+  // Record placeholder positions for blocking intervals and remove placeholders.
+  size_t blocking_pos1 = placeholder1->GetLiveInterval()->GetStart();
+  size_t blocking_pos2 = placeholder2->GetLiveInterval()->GetStart();
+  auto& const0_uses = const0->GetLiveInterval()->uses_;
+  ASSERT_EQ(4, std::distance(const0_uses.begin(), const0_uses.end()));
+  auto add_it = std::next(const0_uses.begin());
+  ASSERT_TRUE(add_it->GetUser() == add);
+  for (HInstruction* placeholder : {placeholder1, placeholder2}) {
+    block->RemoveInstruction(placeholder);
+    ASSERT_TRUE(std::next(add_it)->GetUser() == placeholder);
+    const0_uses.erase_after(add_it);
+    // Set the block again in the dead placeholders to allow `liveness` to retrieve the block.
+    placeholder->SetBlock(block);
   }
 
-  // For SSA value intervals, only an interval resulted from a split may intersect
-  // with inactive intervals.
-  unhandled = register_allocator.Split(unhandled, 5);
-
-  // Add three temps holding the same register, and starting at different positions.
-  // Put the one that should be picked in the middle of the inactive list to ensure
-  // we do not depend on an order.
-  LiveInterval* interval =
-      LiveInterval::MakeFixedInterval(GetScopedAllocator(), 0, DataType::Type::kInt32);
-  interval->AddRange(40, 50);
-  register_allocator.inactive_.push_back(interval);
-
-  interval = LiveInterval::MakeFixedInterval(GetScopedAllocator(), 0, DataType::Type::kInt32);
-  interval->AddRange(20, 30);
-  register_allocator.inactive_.push_back(interval);
-
-  interval = LiveInterval::MakeFixedInterval(GetScopedAllocator(), 0, DataType::Type::kInt32);
-  interval->AddRange(60, 70);
-  register_allocator.inactive_.push_back(interval);
-
-  register_allocator.number_of_registers_ = 1;
-  register_allocator.registers_array_ = GetAllocator()->AllocArray<size_t>(1);
-  register_allocator.current_register_type_ = RegisterAllocator::RegisterType::kCoreRegister;
-  register_allocator.unhandled_ = &register_allocator.unhandled_core_intervals_;
-
-  ASSERT_TRUE(register_allocator.TryAllocateFreeReg(unhandled));
-
-  // Check that we have split the interval.
-  ASSERT_EQ(1u, register_allocator.unhandled_->size());
-  // Check that we know need to find a new register where the next interval
-  // that uses the register starts.
-  ASSERT_EQ(20u, register_allocator.unhandled_->front()->GetStart());
-}
+  RegisterAllocatorLinearScan register_allocator(GetScopedAllocator(), &codegen, liveness);
 
-HGraph* RegisterAllocatorTest::BuildIfElseWithPhi(HPhi** phi,
-                                                  HInstruction** input1,
-                                                  HInstruction** input2) {
-  HGraph* graph = CreateGraph();
-  HBasicBlock* entry = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(entry);
-  graph->SetEntryBlock(entry);
-  HInstruction* parameter = MakeParam(DataType::Type::kReference);
+  // Test two variants, so that we hit the desired configuration once, no matter the order
+  // in which the register allocator inserts the blocking intervals into inactive intervals.
+  size_t call_pos = special_first ? blocking_pos2 : blocking_pos1;
+  size_t special_pos = special_first ? blocking_pos1 : blocking_pos2;
+  register_allocator.block_registers_for_call_interval_->AddRange(call_pos, call_pos + 1);
+  register_allocator.block_registers_special_interval_->AddRange(special_pos, special_pos + 1);
 
-  HBasicBlock* block = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(block);
-  entry->AddSuccessor(block);
+  // Set just one register available to make all intervals compete for the same.
+  bool* blocked_registers = codegen.GetBlockedCoreRegisters();
+  std::fill_n(blocked_registers + 1, codegen.GetNumberOfCoreRegisters() - 1, true);
+
+  register_allocator.AllocateRegistersInternal();
+
+  std::pair<size_t, int> expected_add_start_and_reg[] = {
+    {add->GetLifetimePosition(), 0},
+    {blocking_pos1, -1},
+  };
+  LiveInterval* li = add->GetLiveInterval();
+  for (const std::pair<size_t, int>& expected_start_and_reg : expected_add_start_and_reg) {
+    ASSERT_TRUE(li != nullptr);
+    ASSERT_EQ(expected_start_and_reg.first, li->GetStart());
+    ASSERT_EQ(expected_start_and_reg.second, li->GetRegister());
+    li = li->GetNextSibling();
+  }
+  ASSERT_TRUE(li == nullptr);
+}
 
-  HInstruction* test = MakeIFieldGet(block, parameter, DataType::Type::kBool, MemberOffset(22));
-  MakeIf(block, test);
+TEST_F(RegisterAllocatorTest, FreeUntilCallFirst) {
+  TestFreeUntil(/*special_first=*/ false);
+}
 
-  HBasicBlock* then = new (GetAllocator()) HBasicBlock(graph);
-  HBasicBlock* else_ = new (GetAllocator()) HBasicBlock(graph);
-  HBasicBlock* join = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(then);
-  graph->AddBlock(else_);
-  graph->AddBlock(join);
+TEST_F(RegisterAllocatorTest, FreeUntilSpecialFirst) {
+  TestFreeUntil(/*special_first=*/ true);
+}
 
-  block->AddSuccessor(then);
-  block->AddSuccessor(else_);
-  then->AddSuccessor(join);
-  else_->AddSuccessor(join);
-  MakeGoto(then);
-  MakeGoto(else_);
+void RegisterAllocatorTest::BuildIfElseWithPhi(HPhi** phi,
+                                               HInstruction** input1,
+                                               HInstruction** input2) {
+  HBasicBlock* join = InitEntryMainExitGraph();
+  auto [if_block, then, else_] = CreateDiamondPattern(join);
+  HInstruction* parameter = MakeParam(DataType::Type::kReference);
+  HInstruction* test = MakeIFieldGet(if_block, parameter, DataType::Type::kBool, MemberOffset(22));
+  MakeIf(if_block, test);
 
   *input1 = MakeIFieldGet(then, parameter, DataType::Type::kInt32, MemberOffset(42));
   *input2 = MakeIFieldGet(else_, parameter, DataType::Type::kInt32, MemberOffset(42));
-
   *phi = MakePhi(join, {*input1, *input2});
-  MakeExit(join);
+  MakeReturn(join, *phi);
 
-  graph->BuildDominatorTree();
-  graph->AnalyzeLoops();
-  return graph;
+  graph_->BuildDominatorTree();
+  graph_->AnalyzeLoops();
 }
 
 TEST_F(RegisterAllocatorTest, PhiHint) {
@@ -500,9 +495,9 @@ TEST_F(RegisterAllocatorTest, PhiHint) {
   HInstruction *input1, *input2;
 
   {
-    HGraph* graph = BuildIfElseWithPhi(&phi, &input1, &input2);
-    x86::CodeGeneratorX86 codegen(graph, *compiler_options_);
-    SsaLivenessAnalysis liveness(graph, &codegen, GetScopedAllocator());
+    BuildIfElseWithPhi(&phi, &input1, &input2);
+    x86::CodeGeneratorX86 codegen(graph_, *compiler_options_);
+    SsaLivenessAnalysis liveness(graph_, &codegen, GetScopedAllocator());
     liveness.Analyze();
 
     // Check that the register allocator is deterministic.
@@ -516,9 +511,9 @@ TEST_F(RegisterAllocatorTest, PhiHint) {
   }
 
   {
-    HGraph* graph = BuildIfElseWithPhi(&phi, &input1, &input2);
-    x86::CodeGeneratorX86 codegen(graph, *compiler_options_);
-    SsaLivenessAnalysis liveness(graph, &codegen, GetScopedAllocator());
+    BuildIfElseWithPhi(&phi, &input1, &input2);
+    x86::CodeGeneratorX86 codegen(graph_, *compiler_options_);
+    SsaLivenessAnalysis liveness(graph_, &codegen, GetScopedAllocator());
     liveness.Analyze();
 
     // Set the phi to a specific register, and check that the inputs get allocated
@@ -534,9 +529,9 @@ TEST_F(RegisterAllocatorTest, PhiHint) {
   }
 
   {
-    HGraph* graph = BuildIfElseWithPhi(&phi, &input1, &input2);
-    x86::CodeGeneratorX86 codegen(graph, *compiler_options_);
-    SsaLivenessAnalysis liveness(graph, &codegen, GetScopedAllocator());
+    BuildIfElseWithPhi(&phi, &input1, &input2);
+    x86::CodeGeneratorX86 codegen(graph_, *compiler_options_);
+    SsaLivenessAnalysis liveness(graph_, &codegen, GetScopedAllocator());
     liveness.Analyze();
 
     // Set input1 to a specific register, and check that the phi and other input get allocated
@@ -552,9 +547,9 @@ TEST_F(RegisterAllocatorTest, PhiHint) {
   }
 
   {
-    HGraph* graph = BuildIfElseWithPhi(&phi, &input1, &input2);
-    x86::CodeGeneratorX86 codegen(graph, *compiler_options_);
-    SsaLivenessAnalysis liveness(graph, &codegen, GetScopedAllocator());
+    BuildIfElseWithPhi(&phi, &input1, &input2);
+    x86::CodeGeneratorX86 codegen(graph_, *compiler_options_);
+    SsaLivenessAnalysis liveness(graph_, &codegen, GetScopedAllocator());
     liveness.Analyze();
 
     // Set input2 to a specific register, and check that the phi and other input get allocated
@@ -570,36 +565,23 @@ TEST_F(RegisterAllocatorTest, PhiHint) {
   }
 }
 
-HGraph* RegisterAllocatorTest::BuildFieldReturn(HInstruction** field, HInstruction** ret) {
-  HGraph* graph = CreateGraph();
-  HBasicBlock* entry = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(entry);
-  graph->SetEntryBlock(entry);
+void RegisterAllocatorTest::BuildFieldReturn(HInstruction** field, HInstruction** ret) {
+  HBasicBlock* block = InitEntryMainExitGraph();
   HInstruction* parameter = MakeParam(DataType::Type::kReference);
 
-  HBasicBlock* block = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(block);
-  entry->AddSuccessor(block);
-
   *field = MakeIFieldGet(block, parameter, DataType::Type::kInt32, MemberOffset(42));
   *ret = MakeReturn(block, *field);
 
-  HBasicBlock* exit = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(exit);
-  block->AddSuccessor(exit);
-  MakeExit(exit);
-
-  graph->BuildDominatorTree();
-  return graph;
+  graph_->BuildDominatorTree();
 }
 
 TEST_F(RegisterAllocatorTest, ExpectedInRegisterHint) {
   HInstruction *field, *ret;
 
   {
-    HGraph* graph = BuildFieldReturn(&field, &ret);
-    x86::CodeGeneratorX86 codegen(graph, *compiler_options_);
-    SsaLivenessAnalysis liveness(graph, &codegen, GetScopedAllocator());
+    BuildFieldReturn(&field, &ret);
+    x86::CodeGeneratorX86 codegen(graph_, *compiler_options_);
+    SsaLivenessAnalysis liveness(graph_, &codegen, GetScopedAllocator());
     liveness.Analyze();
 
     std::unique_ptr<RegisterAllocator> register_allocator =
@@ -611,14 +593,14 @@ TEST_F(RegisterAllocatorTest, ExpectedInRegisterHint) {
   }
 
   {
-    HGraph* graph = BuildFieldReturn(&field, &ret);
-    x86::CodeGeneratorX86 codegen(graph, *compiler_options_);
-    SsaLivenessAnalysis liveness(graph, &codegen, GetScopedAllocator());
+    BuildFieldReturn(&field, &ret);
+    x86::CodeGeneratorX86 codegen(graph_, *compiler_options_);
+    SsaLivenessAnalysis liveness(graph_, &codegen, GetScopedAllocator());
     liveness.Analyze();
 
     // Check that the field gets put in the register expected by its use.
     // Don't use SetInAt because we are overriding an already allocated location.
-    ret->GetLocations()->inputs_[0] = Location::RegisterLocation(2);
+    ret->GetLocations()->Inputs()[0] = Location::RegisterLocation(2);
 
     std::unique_ptr<RegisterAllocator> register_allocator =
         RegisterAllocator::Create(GetScopedAllocator(), &codegen, liveness);
@@ -628,38 +610,28 @@ TEST_F(RegisterAllocatorTest, ExpectedInRegisterHint) {
   }
 }
 
-HGraph* RegisterAllocatorTest::BuildTwoSubs(HInstruction** first_sub, HInstruction** second_sub) {
-  HGraph* graph = CreateGraph();
-  HBasicBlock* entry = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(entry);
-  graph->SetEntryBlock(entry);
+void RegisterAllocatorTest::BuildTwoSubs(HInstruction** first_sub, HInstruction** second_sub) {
+  HBasicBlock* block = InitEntryMainExitGraph();
   HInstruction* parameter = MakeParam(DataType::Type::kInt32);
-
-  HInstruction* constant1 = graph->GetIntConstant(1);
-  HInstruction* constant2 = graph->GetIntConstant(2);
-
-  HBasicBlock* block = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(block);
-  entry->AddSuccessor(block);
+  HInstruction* constant1 = graph_->GetIntConstant(1);
+  HInstruction* constant2 = graph_->GetIntConstant(2);
 
   *first_sub = new (GetAllocator()) HSub(DataType::Type::kInt32, parameter, constant1);
   block->AddInstruction(*first_sub);
   *second_sub = new (GetAllocator()) HSub(DataType::Type::kInt32, *first_sub, constant2);
   block->AddInstruction(*second_sub);
+  MakeReturn(block, *second_sub);
 
-  MakeExit(block);
-
-  graph->BuildDominatorTree();
-  return graph;
+  graph_->BuildDominatorTree();
 }
 
 TEST_F(RegisterAllocatorTest, SameAsFirstInputHint) {
   HInstruction *first_sub, *second_sub;
 
   {
-    HGraph* graph = BuildTwoSubs(&first_sub, &second_sub);
-    x86::CodeGeneratorX86 codegen(graph, *compiler_options_);
-    SsaLivenessAnalysis liveness(graph, &codegen, GetScopedAllocator());
+    BuildTwoSubs(&first_sub, &second_sub);
+    x86::CodeGeneratorX86 codegen(graph_, *compiler_options_);
+    SsaLivenessAnalysis liveness(graph_, &codegen, GetScopedAllocator());
     liveness.Analyze();
 
     std::unique_ptr<RegisterAllocator> register_allocator =
@@ -672,9 +644,9 @@ TEST_F(RegisterAllocatorTest, SameAsFirstInputHint) {
   }
 
   {
-    HGraph* graph = BuildTwoSubs(&first_sub, &second_sub);
-    x86::CodeGeneratorX86 codegen(graph, *compiler_options_);
-    SsaLivenessAnalysis liveness(graph, &codegen, GetScopedAllocator());
+    BuildTwoSubs(&first_sub, &second_sub);
+    x86::CodeGeneratorX86 codegen(graph_, *compiler_options_);
+    SsaLivenessAnalysis liveness(graph_, &codegen, GetScopedAllocator());
     liveness.Analyze();
 
     // check that both adds get the same register.
@@ -692,33 +664,24 @@ TEST_F(RegisterAllocatorTest, SameAsFirstInputHint) {
   }
 }
 
-HGraph* RegisterAllocatorTest::BuildDiv(HInstruction** div) {
-  HGraph* graph = CreateGraph();
-  HBasicBlock* entry = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(entry);
-  graph->SetEntryBlock(entry);
+void RegisterAllocatorTest::BuildDiv(HInstruction** div) {
+  HBasicBlock* block = InitEntryMainExitGraph();
   HInstruction* first = MakeParam(DataType::Type::kInt32);
   HInstruction* second = MakeParam(DataType::Type::kInt32);
 
-  HBasicBlock* block = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(block);
-  entry->AddSuccessor(block);
-
   *div = new (GetAllocator()) HDiv(
       DataType::Type::kInt32, first, second, 0);  // don't care about dex_pc.
   block->AddInstruction(*div);
+  MakeReturn(block, *div);
 
-  MakeExit(block);
-
-  graph->BuildDominatorTree();
-  return graph;
+  graph_->BuildDominatorTree();
 }
 
 TEST_F(RegisterAllocatorTest, ExpectedExactInRegisterAndSameOutputHint) {
   HInstruction *div;
-  HGraph* graph = BuildDiv(&div);
-  x86::CodeGeneratorX86 codegen(graph, *compiler_options_);
-  SsaLivenessAnalysis liveness(graph, &codegen, GetScopedAllocator());
+  BuildDiv(&div);
+  x86::CodeGeneratorX86 codegen(graph_, *compiler_options_);
+  SsaLivenessAnalysis liveness(graph_, &codegen, GetScopedAllocator());
   liveness.Analyze();
 
   std::unique_ptr<RegisterAllocator> register_allocator =
@@ -733,50 +696,43 @@ TEST_F(RegisterAllocatorTest, ExpectedExactInRegisterAndSameOutputHint) {
 // register would lead to spilling an inactive interval at the wrong
 // position.
 // This test only applies to the linear scan allocator.
-TEST_F(RegisterAllocatorTest, SpillInactive) {
-  // Create a synthesized graph to please the register_allocator and
-  // ssa_liveness_analysis code.
-  HGraph* graph = CreateGraph();
-  HBasicBlock* entry = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(entry);
-  graph->SetEntryBlock(entry);
+void RegisterAllocatorTest::TestSpillInactive() {
+  // Define a shortcut for the `kLivenessPositionsPerInstruction`.
+  static constexpr size_t kLppi = kLivenessPositionsPerInstruction;
+
+  HBasicBlock* block = InitEntryMainExitGraphWithReturnVoid();
   HInstruction* one = MakeParam(DataType::Type::kInt32);
   HInstruction* two = MakeParam(DataType::Type::kInt32);
   HInstruction* three = MakeParam(DataType::Type::kInt32);
   HInstruction* four = MakeParam(DataType::Type::kInt32);
 
-  HBasicBlock* block = new (GetAllocator()) HBasicBlock(graph);
-  graph->AddBlock(block);
-  entry->AddSuccessor(block);
-  MakeExit(block);
-
   // We create a synthesized user requesting a register, to avoid just spilling the
   // intervals.
   HPhi* user = new (GetAllocator()) HPhi(GetAllocator(), 0, 1, DataType::Type::kInt32);
   user->SetBlock(block);
   user->AddInput(one);
-  LocationSummary* locations = new (GetAllocator()) LocationSummary(user, LocationSummary::kNoCall);
+  LocationSummary* locations = LocationSummary::CreateNoCall(GetAllocator(), user);
   locations->SetInAt(0, Location::RequiresRegister());
-  static constexpr size_t phi_ranges[][2] = {{20, 30}};
+  static constexpr size_t phi_ranges[][2] = {{10 * kLppi, 15 * kLppi}};
   BuildInterval(phi_ranges, arraysize(phi_ranges), GetScopedAllocator(), -1, user);
 
   // Create an interval with lifetime holes.
-  static constexpr size_t ranges1[][2] = {{0, 2}, {4, 6}, {8, 10}};
+  static constexpr size_t ranges1[][2] =
+      {{0u * kLppi, 2u * kLppi}, {4u * kLppi, 5u * kLppi}, {7u * kLppi, 8u * kLppi}};
   LiveInterval* first = BuildInterval(ranges1, arraysize(ranges1), GetScopedAllocator(), -1, one);
-  first->uses_.push_front(*new (GetScopedAllocator()) UsePosition(user, 0u, 8));
-  first->uses_.push_front(*new (GetScopedAllocator()) UsePosition(user, 0u, 7));
-  first->uses_.push_front(*new (GetScopedAllocator()) UsePosition(user, 0u, 6));
+  first->uses_.push_front(*new (GetScopedAllocator()) UsePosition(user, 0u, 7u * kLppi));
+  first->uses_.push_front(*new (GetScopedAllocator()) UsePosition(user, 0u, 6u * kLppi));
+  first->uses_.push_front(*new (GetScopedAllocator()) UsePosition(user, 0u, 5u * kLppi));
 
-  locations = new (GetAllocator()) LocationSummary(first->GetDefinedBy(), LocationSummary::kNoCall);
+  locations = LocationSummary::CreateNoCall(GetAllocator(), first->GetDefinedBy());
   locations->SetOut(Location::RequiresRegister());
-  first = first->SplitAt(1);
+  first = first->SplitAt(1u * kLppi);
 
   // Create an interval that conflicts with the next interval, to force the next
   // interval to call `AllocateBlockedReg`.
-  static constexpr size_t ranges2[][2] = {{2, 4}};
+  static constexpr size_t ranges2[][2] = {{2u * kLppi, 4u * kLppi}};
   LiveInterval* second = BuildInterval(ranges2, arraysize(ranges2), GetScopedAllocator(), -1, two);
-  locations =
-      new (GetAllocator()) LocationSummary(second->GetDefinedBy(), LocationSummary::kNoCall);
+  locations = LocationSummary::CreateNoCall(GetAllocator(), second->GetDefinedBy());
   locations->SetOut(Location::RequiresRegister());
 
   // Create an interval that will lead to splitting the first interval. The bug occured
@@ -784,50 +740,265 @@ TEST_F(RegisterAllocatorTest, SpillInactive) {
   // this interval and the first interval. We would have then put the interval with ranges
   // "[0, 2(, [4, 6(" in the list of handled intervals, even though we haven't processed intervals
   // before lifetime position 6 yet.
-  static constexpr size_t ranges3[][2] = {{2, 4}, {8, 10}};
+  static constexpr size_t ranges3[][2] = {{2u * kLppi, 4u * kLppi}, {7u * kLppi, 8u * kLppi}};
   LiveInterval* third = BuildInterval(ranges3, arraysize(ranges3), GetScopedAllocator(), -1, three);
-  third->uses_.push_front(*new (GetScopedAllocator()) UsePosition(user, 0u, 8));
-  third->uses_.push_front(*new (GetScopedAllocator()) UsePosition(user, 0u, 4));
-  third->uses_.push_front(*new (GetScopedAllocator()) UsePosition(user, 0u, 3));
-  locations = new (GetAllocator()) LocationSummary(third->GetDefinedBy(), LocationSummary::kNoCall);
+  third->uses_.push_front(*new (GetScopedAllocator()) UsePosition(user, 0u, 7u * kLppi));
+  third->uses_.push_front(*new (GetScopedAllocator()) UsePosition(user, 0u, 4u * kLppi));
+  third->uses_.push_front(*new (GetScopedAllocator()) UsePosition(user, 0u, 3u * kLppi));
+  locations = LocationSummary::CreateNoCall(GetAllocator(), third->GetDefinedBy());
   locations->SetOut(Location::RequiresRegister());
-  third = third->SplitAt(3);
+  third = third->SplitAt(3u * kLppi);
 
   // Because the first part of the split interval was considered handled, this interval
   // was free to allocate the same register, even though it conflicts with it.
-  static constexpr size_t ranges4[][2] = {{4, 6}};
+  static constexpr size_t ranges4[][2] = {{4u * kLppi, 5u * kLppi}};
   LiveInterval* fourth = BuildInterval(ranges4, arraysize(ranges4), GetScopedAllocator(), -1, four);
-  locations =
-      new (GetAllocator()) LocationSummary(fourth->GetDefinedBy(), LocationSummary::kNoCall);
+  locations = LocationSummary::CreateNoCall(GetAllocator(), fourth->GetDefinedBy());
   locations->SetOut(Location::RequiresRegister());
 
-  x86::CodeGeneratorX86 codegen(graph, *compiler_options_);
-  SsaLivenessAnalysis liveness(graph, &codegen, GetScopedAllocator());
+  x86::CodeGeneratorX86 codegen(graph_, *compiler_options_);
+  SsaLivenessAnalysis liveness(graph_, &codegen, GetScopedAllocator());
   // Populate the instructions in the liveness object, to please the register allocator.
-  for (size_t i = 0; i < 32; ++i) {
-    liveness.instructions_from_lifetime_position_.push_back(user);
-  }
+  liveness.instructions_from_lifetime_position_.assign(16, user);
 
   RegisterAllocatorLinearScan register_allocator(GetScopedAllocator(), &codegen, liveness);
-  register_allocator.unhandled_core_intervals_.push_back(fourth);
-  register_allocator.unhandled_core_intervals_.push_back(third);
-  register_allocator.unhandled_core_intervals_.push_back(second);
-  register_allocator.unhandled_core_intervals_.push_back(first);
+  register_allocator.unhandled_core_intervals_.assign({fourth, third, second, first});
 
   // Set just one register available to make all intervals compete for the same.
-  register_allocator.number_of_registers_ = 1;
-  register_allocator.registers_array_ = GetAllocator()->AllocArray<size_t>(1);
-  register_allocator.current_register_type_ = RegisterAllocator::RegisterType::kCoreRegister;
-  register_allocator.unhandled_ = &register_allocator.unhandled_core_intervals_;
-  register_allocator.LinearScan();
+  bool* blocked_registers = codegen.GetBlockedCoreRegisters();
+  std::fill_n(blocked_registers + 1, codegen.GetNumberOfCoreRegisters() - 1, true);
+
+  // We have set up all intervals manually and we want `AllocateRegistersInternal()` to run
+  // the linear scan without processing instructions - check that the linear order is empty.
+  ASSERT_TRUE(codegen.GetGraph()->GetLinearOrder().empty());
+  register_allocator.AllocateRegistersInternal();
 
   // Test that there is no conflicts between intervals.
-  ScopedArenaVector<LiveInterval*> intervals(GetScopedAllocator()->Adapter());
-  intervals.push_back(first);
-  intervals.push_back(second);
-  intervals.push_back(third);
-  intervals.push_back(fourth);
+  ScopedArenaVector<LiveInterval*> intervals({first, second, third, fourth},
+                                             GetScopedAllocator()->Adapter());
   ASSERT_TRUE(ValidateIntervals(intervals, codegen));
 }
 
+TEST_F(RegisterAllocatorTest, SpillInactive) {
+  TestSpillInactive();
+}
+
+TEST_F(RegisterAllocatorTest, ReuseSpillSlots) {
+  if (!com::android::art::flags::reg_alloc_spill_slot_reuse()) {
+    GTEST_SKIP() << "Improved spill slot reuse disabled.";
+  }
+  HBasicBlock* return_block = InitEntryMainExitGraph();
+  auto [start, left, right] = CreateDiamondPattern(return_block);
+  HInstruction* obj = MakeParam(DataType::Type::kReference);
+  HInstruction* cond = MakeIFieldGet(start, obj, DataType::Type::kBool, MemberOffset(32));
+  MakeIf(start, cond);
+
+  // Load two values from fields. Both shall be used as Phi inputs later.
+  HInstruction* left_get1 = MakeIFieldGet(left, obj, DataType::Type::kInt32, MemberOffset(36));
+  HInstruction* left_get2 = MakeIFieldGet(left, obj, DataType::Type::kInt32, MemberOffset(40));
+  // Convert one of the values to `Int64` to spill the loaded values.
+  HInstruction* left_conv1 = MakeUnOp<HTypeConversion>(left, DataType::Type::kInt64, left_get1);
+  // Convert the `Int64` value back to `Int32`. x86 codegen uses EAX and EDX for conversion
+  // which is not a normal pair, so avoid using this odd explicit pair for a Phi.
+  HInstruction* left_conv2 = MakeUnOp<HTypeConversion>(left, DataType::Type::kInt32, left_conv1);
+
+  // Repeat the sequence from `left` block in the `right` block (with different offsets). Without
+  // spill slot hints, spill slots should be assigned the same way as in the `left` block.
+  HInstruction* right_get1 = MakeIFieldGet(right, obj, DataType::Type::kInt32, MemberOffset(44));
+  HInstruction* right_get2 = MakeIFieldGet(right, obj, DataType::Type::kInt32, MemberOffset(48));
+  HInstruction* right_conv1 = MakeUnOp<HTypeConversion>(right, DataType::Type::kInt64, right_get1);
+  HInstruction* right_conv2 = MakeUnOp<HTypeConversion>(right, DataType::Type::kInt32, right_conv1);
+
+  // Add Phis that tie the first field load in `left` to the second field load in `right` and
+  // vice versa, to check that the hints can align the spill slots assigned to inputs.
+  HPhi* phi1 = MakePhi(return_block, {left_get1, right_get2});
+  HPhi* phi2 = MakePhi(return_block, {left_get2, right_get1});
+
+  // Add some instructions that use the `phi1`, `phi2` and even the converted values
+  // to derive some return value.
+  HPhi* phi_conv = MakePhi(return_block, {left_conv2, right_conv2});
+  HMin* min1 = MakeBinOp<HMin>(return_block, DataType::Type::kInt32, phi1, phi2);
+  HMin* min2 = MakeBinOp<HMin>(return_block, DataType::Type::kInt32, min1, phi_conv);
+  MakeReturn(return_block, min2);
+
+  graph_->ComputeDominanceInformation();
+  x86::CodeGeneratorX86 codegen(graph_, *compiler_options_);
+  SsaLivenessAnalysis liveness(graph_, &codegen, GetScopedAllocator());
+  liveness.Analyze();
+
+  // Set just two registers available to make it easy to force spills.
+  // Choose EAX and EDX which are used by type conversion from Int32 to Int64, so that
+  // we can use the type conversion to spill all live intervals wherever we want.
+  // Note that the `obj` parameter comes in the blocked ECX which works fine for the test.
+  bool* blocked_registers = codegen.GetBlockedCoreRegisters();
+  std::fill_n(blocked_registers, codegen.GetNumberOfCoreRegisters(), true);
+  blocked_registers[x86::EAX] = blocked_registers[x86::EDX] = false;
+
+  std::unique_ptr<RegisterAllocator> register_allocator =
+      RegisterAllocator::Create(GetScopedAllocator(), &codegen, liveness);
+  register_allocator->AllocateRegisters();
+
+  // Field loads would be spilled even without using spill slot hints.
+  ASSERT_TRUE(left_get1->GetLiveInterval()->HasSpillSlot());
+  ASSERT_TRUE(left_get2->GetLiveInterval()->HasSpillSlot());
+  ASSERT_TRUE(right_get1->GetLiveInterval()->HasSpillSlot());
+  ASSERT_TRUE(right_get2->GetLiveInterval()->HasSpillSlot());
+
+  // Input spill slots are aligned thanks to the spill slot hints.
+  EXPECT_EQ(left_get1->GetLiveInterval()->GetSpillSlot(),
+            right_get2->GetLiveInterval()->GetSpillSlot());
+  EXPECT_EQ(left_get2->GetLiveInterval()->GetSpillSlot(),
+            right_get1->GetLiveInterval()->GetSpillSlot());
+
+  // Check that `phi1` and `phi2` use the spill slots used by their inputs.
+  EXPECT_TRUE(phi1->GetLiveInterval()->HasSpillSlot());
+  EXPECT_EQ(left_get1->GetLiveInterval()->GetSpillSlot(), phi1->GetLiveInterval()->GetSpillSlot());
+  EXPECT_TRUE(phi2->GetLiveInterval()->HasSpillSlot());
+  EXPECT_EQ(left_get2->GetLiveInterval()->GetSpillSlot(), phi2->GetLiveInterval()->GetSpillSlot());
+
+  // Check that `phi1` and `phi2` are split and don't have a register in the first sibling.
+  EXPECT_TRUE(phi1->GetLiveInterval()->GetNextSibling() != nullptr);
+  EXPECT_TRUE(!phi1->GetLiveInterval()->HasRegister());
+  EXPECT_TRUE(phi2->GetLiveInterval()->GetNextSibling() != nullptr);
+  EXPECT_TRUE(!phi2->GetLiveInterval()->HasRegister());
+}
+
+TEST_F(RegisterAllocatorTest, ReuseSpillSlotGaps) {
+  if (!com::android::art::flags::reg_alloc_spill_slot_reuse()) {
+    GTEST_SKIP() << "Improved spill slot reuse disabled.";
+  }
+  HBasicBlock* return_block = InitEntryMainExitGraph();
+  auto [pre_header, header, body] = CreateWhileLoop(return_block);
+
+  HInstruction* const0 = graph_->GetIntConstant(0);
+  HInstruction* const10 = graph_->GetIntConstant(10);
+
+  HPhi* phi1 = MakePhi(header, {const0, /* placeholder */ const0});
+  HNeg* neg1 = MakeUnOp<HNeg>(body, DataType::Type::kInt32, phi1);
+  phi1->ReplaceInput(neg1, 1u);  // Update back-edge input.
+
+  HPhi* phi2 = MakePhi(header, {const0, /* placeholder */ const0});
+  HNeg* neg2 = MakeUnOp<HNeg>(body, DataType::Type::kInt32, phi2);
+  phi2->ReplaceInput(neg2, 1u);  // Update back-edge input.
+
+  // Loop variable and condition. This is added after `neg1` and `neg2` to spill both.
+  HPhi* phi = MakePhi(header, {const0, /* placeholder */ const0});
+  HNeg* neg = MakeUnOp<HNeg>(body, DataType::Type::kInt32, phi);
+  phi->ReplaceInput(neg, 1u);  // Update back-edge input.
+  HCondition* cond = MakeCondition(header, kCondGE, phi, const10);
+  MakeIf(header, cond);
+
+  // Add an environment use of `phi1` and a normal use of `phi2`.
+  HCondition* deopt_cond = MakeCondition(header, kCondLT, phi, const0);
+  HDeoptimize* deopt = new (GetAllocator()) HDeoptimize(
+      GetAllocator(), deopt_cond, DeoptimizationKind::kDebugging, /*dex_pc=*/ 0u);
+  AddOrInsertInstruction(return_block, deopt);
+  ManuallyBuildEnvFor(deopt, {phi1});
+  HReturn* ret = MakeReturn(return_block, phi2);
+
+  graph_->BuildDominatorTree();
+  x86::CodeGeneratorX86 codegen(graph_, *compiler_options_);
+  SsaLivenessAnalysis liveness(graph_, &codegen, GetScopedAllocator());
+  liveness.Analyze();
+
+  // Set just one register available to make all intervals compete for the same.
+  bool* blocked_registers = codegen.GetBlockedCoreRegisters();
+  std::fill_n(blocked_registers + 1, codegen.GetNumberOfCoreRegisters() - 1, true);
+
+  std::unique_ptr<RegisterAllocator> register_allocator =
+      RegisterAllocator::Create(GetScopedAllocator(), &codegen, liveness);
+  register_allocator->AllocateRegisters();
+
+  ASSERT_TRUE(phi1->GetLiveInterval()->HasSpillSlot());
+  ASSERT_TRUE(neg1->GetLiveInterval()->HasSpillSlot());
+  EXPECT_EQ(phi1->GetLiveInterval()->GetSpillSlot(), neg1->GetLiveInterval()->GetSpillSlot());
+  ASSERT_TRUE(phi2->GetLiveInterval()->HasSpillSlot());
+  ASSERT_TRUE(neg2->GetLiveInterval()->HasSpillSlot());
+  EXPECT_EQ(phi2->GetLiveInterval()->GetSpillSlot(), neg2->GetLiveInterval()->GetSpillSlot());
+}
+
+// Regression test for wrongly assuming that a Phi interval with a spill slot hint
+// is not split when checking if the spill slot can be used. Indeed, it can be split
+// and we must use the sibling to determine the lifetime end.
+TEST_F(RegisterAllocatorTest, ReuseSpillSlotsUnavailableWithSplitPhiInterval) {
+  if (!com::android::art::flags::reg_alloc_spill_slot_reuse()) {
+    GTEST_SKIP() << "Improved spill slot reuse disabled.";
+  }
+  HBasicBlock* return_block = InitEntryMainExitGraph();
+  auto [start, left, right] = CreateDiamondPattern(return_block);
+  HInstruction* const0 = graph_->GetIntConstant(0);
+  HInstruction* obj = MakeParam(DataType::Type::kReference);
+  HInstruction* cond = MakeIFieldGet(start, obj, DataType::Type::kBool, MemberOffset(32));
+  MakeIf(start, cond);
+
+  // Add a load followed by `HNeg`, so that the loaded value is spilled.
+  HInstruction* left_get = MakeIFieldGet(left, obj, DataType::Type::kInt32, MemberOffset(36));
+  HNeg* left_neg = MakeUnOp<HNeg>(left, DataType::Type::kInt32, left_get);
+
+  // Repeat the sequence from `left` block in the `right` block (with a different offset).
+  HInstruction* right_get = MakeIFieldGet(right, obj, DataType::Type::kInt32, MemberOffset(40));
+  HNeg* right_neg = MakeUnOp<HNeg>(right, DataType::Type::kInt32, right_get);
+
+  // Phis shall be processed in the order in which they are inserted.
+  // The first Phi shall initially be allocated the only available register.
+  HPhi* phi1 = MakePhi(return_block, {const0, right_neg});
+  // The second Phi has no register use, so it shall be spilled.
+  // The spill slot used by `left_get` and `right_get` shall be reused for this unrelated Phi.
+  HPhi* phi2 = MakePhi(return_block, {left_neg, const0});
+  // The third Phi has a hint that would put it to the same spill slot as both of its inputs
+  // if it was not already taken by `phi2`. But the spill slot is no longer available, so we
+  // try to allocate a register. Since `get_phi`'s first register use is before the `phi1`'s
+  // first register use, we allocate the register for `get_phi` and re-insert `phi1` to the
+  // unhandled intervals. However, since the last use of `get_phi` is after the `invoke`
+  // which blocks the register, `get_phi`'s interval shall be split in the process.
+  HPhi* get_phi = MakePhi(return_block, {left_get, right_get});
+  // The fourth Phi has an earlier register use than `get_phi`, so it shall be allocated
+  // the register and `get_phi`'s interval shall be re-inserted to unhandled intervals.
+  HPhi* neg_phi = MakePhi(return_block, {left_neg, right_neg});
+  // Then we shall re-process `phi1`, assigning the next spill slot.
+  // Then we shall re-process `get_phi` and try to assign the hint slot where we previously
+  // wrongly assumed that it has no sibling and triggered a `DCHECK()`.
+
+  // Add a register use for the `neg_phi`.
+  HNeg* neg_neg = MakeUnOp<HNeg>(return_block, DataType::Type::kInt32, neg_phi);
+  // Add a register use for the `get_phi`.
+  // Use `HSub` which can have the second operand on the stack for x86.
+  HSub* sub1 = MakeBinOp<HSub>(return_block, DataType::Type::kInt32, get_phi, neg_neg);
+  // Add an invoke that forces the `get_phi` interval to be split when initially allocated.
+  HInvoke* invoke = MakeInvokeStatic(return_block, DataType::Type::kVoid, {}, {});
+  // Add another register use for the `get_phi` after the `invoke`.
+  HSub* sub2 = MakeBinOp<HSub>(return_block, DataType::Type::kInt32, get_phi, sub1);
+
+  // Add some instructions that use all the values to derive some return value.
+  HSub* sub3 = MakeBinOp<HSub>(return_block, DataType::Type::kInt32, sub2, phi1);
+  HSub* sub4 = MakeBinOp<HSub>(return_block, DataType::Type::kInt32, sub3, phi2);
+  MakeReturn(return_block, sub4);
+
+  graph_->ComputeDominanceInformation();
+  x86::CodeGeneratorX86 codegen(graph_, *compiler_options_);
+  SsaLivenessAnalysis liveness(graph_, &codegen, GetScopedAllocator());
+  liveness.Analyze();
+
+  // Set just one register available to make all intervals compete for the same.
+  // Note that the `obj` parameter comes in the blocked ECX which works fine for the test.
+  bool* blocked_registers = codegen.GetBlockedCoreRegisters();
+  std::fill_n(blocked_registers + 1, codegen.GetNumberOfCoreRegisters() - 1, true);
+
+  std::unique_ptr<RegisterAllocator> register_allocator =
+      RegisterAllocator::Create(GetScopedAllocator(), &codegen, liveness);
+  register_allocator->AllocateRegisters();
+
+  ASSERT_TRUE(left_get->GetLiveInterval()->HasSpillSlot());
+  ASSERT_TRUE(right_get->GetLiveInterval()->HasSpillSlot());
+  ASSERT_EQ(left_get->GetLiveInterval()->GetSpillSlot(),
+            right_get->GetLiveInterval()->GetSpillSlot());
+
+  ASSERT_TRUE(phi2->GetLiveInterval()->HasSpillSlot());
+  ASSERT_EQ(left_get->GetLiveInterval()->GetSpillSlot(), phi2->GetLiveInterval()->GetSpillSlot());
+
+  ASSERT_TRUE(get_phi->GetLiveInterval()->HasSpillSlot());
+  ASSERT_NE(left_get->GetLiveInterval()->GetSpillSlot(),
+            get_phi->GetLiveInterval()->GetSpillSlot());
+}
+
 }  // namespace art
diff --git a/compiler/optimizing/scheduler.cc b/compiler/optimizing/scheduler.cc
index 9deb37d106..0b7c67d2f9 100644
--- a/compiler/optimizing/scheduler.cc
+++ b/compiler/optimizing/scheduler.cc
@@ -744,7 +744,7 @@ bool HScheduler::IsSchedulable(const HBasicBlock* block) const {
     return false;
   }
   // Check whether all instructions in this block are schedulable.
-  for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
     if (!IsSchedulable(it.Current())) {
       return false;
     }
diff --git a/compiler/optimizing/scheduler.h b/compiler/optimizing/scheduler.h
index a9672ea732..f7811904bb 100644
--- a/compiler/optimizing/scheduler.h
+++ b/compiler/optimizing/scheduler.h
@@ -414,14 +414,10 @@ class SchedulingLatencyVisitor : public HGraphDelegateVisitor {
     UNREACHABLE();
   }
 
-  void Visit(HInstruction* instruction) {
-    instruction->Accept(this);
-  }
-
   void CalculateLatency(SchedulingNode* node) {
     // By default nodes have no internal latency.
     last_visited_internal_latency_ = 0;
-    Visit(node->GetInstruction());
+    Dispatch(node->GetInstruction());
   }
 
   uint32_t GetLastVisitedLatency() const { return last_visited_latency_; }
@@ -524,7 +520,8 @@ class HScheduler {
       LatencyVisitor* latency_visitor) ALWAYS_INLINE {
     SchedulingGraph scheduling_graph(allocator, heap_location_collector);
     ScopedArenaVector<SchedulingNode*> scheduling_nodes(allocator->Adapter(kArenaAllocScheduler));
-    for (HBackwardInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HBackwardInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done();
+         it.Advance()) {
       HInstruction* instruction = it.Current();
       CHECK_EQ(instruction->GetBlock(), block)
           << instruction->DebugName()
diff --git a/compiler/optimizing/scheduler_test.cc b/compiler/optimizing/scheduler_test.cc
index 6359119a87..5aa0b9c55c 100644
--- a/compiler/optimizing/scheduler_test.cc
+++ b/compiler/optimizing/scheduler_test.cc
@@ -72,10 +72,8 @@ class SchedulerTest : public CommonCompilerTest, public OptimizingUnitTestHelper
 
   // Build scheduling graph, and run target specific scheduling on it.
   void TestBuildDependencyGraphAndSchedule(HScheduler* scheduler) {
-    HBasicBlock* entry = new (GetAllocator()) HBasicBlock(graph_);
-    HBasicBlock* block1 = new (GetAllocator()) HBasicBlock(graph_);
-    graph_->AddBlock(entry);
-    graph_->AddBlock(block1);
+    HBasicBlock* entry = AddNewBlock();
+    HBasicBlock* block1 = AddNewBlock();
     graph_->SetEntryBlock(entry);
 
     // entry:
@@ -124,7 +122,8 @@ class SchedulerTest : public CommonCompilerTest, public OptimizingUnitTestHelper
 
     TestSchedulingGraph scheduling_graph(GetScopedAllocator());
     // Instructions must be inserted in reverse order into the scheduling graph.
-    for (HBackwardInstructionIterator it(block1->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HBackwardInstructionIteratorPrefetchNext it(block1->GetInstructions()); !it.Done();
+         it.Advance()) {
       scheduling_graph.AddNode(it.Current());
     }
 
@@ -181,10 +180,8 @@ class SchedulerTest : public CommonCompilerTest, public OptimizingUnitTestHelper
   }
 
   void TestDependencyGraphOnAliasingArrayAccesses(HScheduler* scheduler) {
-    HBasicBlock* entry = new (GetAllocator()) HBasicBlock(graph_);
-    HBasicBlock* block1 = new (GetAllocator()) HBasicBlock(graph_);
-    graph_->AddBlock(entry);
-    graph_->AddBlock(block1);
+    HBasicBlock* entry = AddNewBlock();
+    HBasicBlock* block1 = AddNewBlock();
     graph_->SetEntryBlock(entry);
 
     HInstruction* arr = MakeParam(DataType::Type::kReference);
@@ -213,7 +210,8 @@ class SchedulerTest : public CommonCompilerTest, public OptimizingUnitTestHelper
     heap_location_collector.BuildAliasingMatrix();
     TestSchedulingGraph scheduling_graph(GetScopedAllocator(), &heap_location_collector);
 
-    for (HBackwardInstructionIterator it(block1->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HBackwardInstructionIteratorPrefetchNext it(block1->GetInstructions()); !it.Done();
+         it.Advance()) {
       // Build scheduling graph with memory access aliasing information
       // from LSA/heap_location_collector.
       scheduling_graph.AddNode(it.Current());
diff --git a/compiler/optimizing/side_effects_analysis.cc b/compiler/optimizing/side_effects_analysis.cc
index 56719b100e..729556423f 100644
--- a/compiler/optimizing/side_effects_analysis.cc
+++ b/compiler/optimizing/side_effects_analysis.cc
@@ -19,11 +19,6 @@
 namespace art HIDDEN {
 
 bool SideEffectsAnalysis::Run() {
-  // Inlining might have created more blocks, so we need to increase the size
-  // if needed.
-  block_effects_.resize(graph_->GetBlocks().size());
-  loop_effects_.resize(graph_->GetBlocks().size());
-
   // In DEBUG mode, ensure side effects are properly initialized to empty.
   if (kIsDebugBuild) {
     for (HBasicBlock* block : graph_->GetReversePostOrder()) {
@@ -40,7 +35,7 @@ bool SideEffectsAnalysis::Run() {
   for (HBasicBlock* block : graph_->GetPostOrder()) {
     SideEffects effects = SideEffects::None();
     // Update `effects` with the side effects of all instructions in this block.
-    for (HInstructionIterator inst_it(block->GetInstructions()); !inst_it.Done();
+    for (HInstructionIteratorPrefetchNext inst_it(block->GetInstructions()); !inst_it.Done();
          inst_it.Advance()) {
       HInstruction* instruction = inst_it.Current();
       effects = effects.Union(instruction->GetSideEffects());
diff --git a/compiler/optimizing/side_effects_analysis.h b/compiler/optimizing/side_effects_analysis.h
index bb2d7948e4..b512cee80b 100644
--- a/compiler/optimizing/side_effects_analysis.h
+++ b/compiler/optimizing/side_effects_analysis.h
@@ -20,17 +20,17 @@
 #include "base/arena_containers.h"
 #include "base/macros.h"
 #include "nodes.h"
-#include "optimization.h"
 
 namespace art HIDDEN {
 
-class SideEffectsAnalysis : public HOptimization {
+class SideEffectsAnalysis : public ArenaObject<kArenaAllocOptimization> {
  public:
-  explicit SideEffectsAnalysis(HGraph* graph, const char* pass_name = kSideEffectsAnalysisPassName)
-      : HOptimization(graph, pass_name),
-        graph_(graph),
-        block_effects_(graph->GetAllocator()->Adapter(kArenaAllocSideEffectsAnalysis)),
-        loop_effects_(graph->GetAllocator()->Adapter(kArenaAllocSideEffectsAnalysis)) {}
+  explicit SideEffectsAnalysis(HGraph* graph)
+      : graph_(graph),
+        block_effects_(graph_->GetBlocks().size(),
+                       graph->GetAllocator()->Adapter(kArenaAllocSideEffectsAnalysis)),
+        loop_effects_(graph_->GetBlocks().size(),
+                      graph->GetAllocator()->Adapter(kArenaAllocSideEffectsAnalysis)) {}
 
   SideEffects GetLoopEffects(HBasicBlock* block) const;
   SideEffects GetBlockEffects(HBasicBlock* block) const;
@@ -40,8 +40,6 @@ class SideEffectsAnalysis : public HOptimization {
 
   bool HasRun() const { return has_run_; }
 
-  static constexpr const char* kSideEffectsAnalysisPassName = "side_effects";
-
  private:
   void UpdateLoopEffects(HLoopInformation* info, SideEffects effects);
 
diff --git a/compiler/optimizing/ssa_builder.cc b/compiler/optimizing/ssa_builder.cc
index 2179bf50b5..f972e518fc 100644
--- a/compiler/optimizing/ssa_builder.cc
+++ b/compiler/optimizing/ssa_builder.cc
@@ -32,7 +32,7 @@ namespace art HIDDEN {
 void SsaBuilder::FixNullConstantType() {
   // The order doesn't matter here.
   for (HBasicBlock* block : graph_->GetReversePostOrder()) {
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       HInstruction* equality_instr = it.Current();
       if (!equality_instr->IsEqual() && !equality_instr->IsNotEqual()) {
         continue;
@@ -65,7 +65,7 @@ void SsaBuilder::FixNullConstantType() {
 void SsaBuilder::EquivalentPhisCleanup() {
   // The order doesn't matter here.
   for (HBasicBlock* block : graph_->GetReversePostOrder()) {
-    for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
       HPhi* phi = it.Current()->AsPhi();
       HPhi* next = phi->GetNextEquivalentPhiWithSameType();
       if (next != nullptr) {
@@ -87,7 +87,8 @@ void SsaBuilder::EquivalentPhisCleanup() {
 
 void SsaBuilder::FixEnvironmentPhis() {
   for (HBasicBlock* block : graph_->GetReversePostOrder()) {
-    for (HInstructionIterator it_phis(block->GetPhis()); !it_phis.Done(); it_phis.Advance()) {
+    for (HInstructionIteratorPrefetchNext it_phis(block->GetPhis()); !it_phis.Done();
+         it_phis.Advance()) {
       HPhi* phi = it_phis.Current()->AsPhi();
       // If the phi is not dead, or has no environment uses, there is nothing to do.
       if (!phi->IsDead() || !phi->HasEnvironmentUses()) continue;
@@ -240,14 +241,16 @@ void SsaBuilder::RunPrimitiveTypePropagation() {
 
   for (HBasicBlock* block : graph_->GetReversePostOrder()) {
     if (block->IsLoopHeader()) {
-      for (HInstructionIterator phi_it(block->GetPhis()); !phi_it.Done(); phi_it.Advance()) {
+      for (HInstructionIteratorPrefetchNext phi_it(block->GetPhis()); !phi_it.Done();
+           phi_it.Advance()) {
         HPhi* phi = phi_it.Current()->AsPhi();
         if (phi->IsLive()) {
           worklist.push_back(phi);
         }
       }
     } else {
-      for (HInstructionIterator phi_it(block->GetPhis()); !phi_it.Done(); phi_it.Advance()) {
+      for (HInstructionIteratorPrefetchNext phi_it(block->GetPhis()); !phi_it.Done();
+           phi_it.Advance()) {
         // Eagerly compute the type of the phi, for quicker convergence. Note
         // that we don't need to add users to the worklist because we are
         // doing a reverse post-order visit, therefore either the phi users are
@@ -502,7 +505,7 @@ static bool HasPhiEquivalentAtLoopEntry(HGraph* graph) {
   // map.
   for (HBasicBlock* block : graph->GetReversePostOrder()) {
     if (block->IsLoopHeader()) {
-      for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+      for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
         if (it.Current()->AsPhi()->HasEquivalentPhi()) {
           return true;
         }
diff --git a/compiler/optimizing/ssa_builder.h b/compiler/optimizing/ssa_builder.h
index 99a5469932..6aa936481f 100644
--- a/compiler/optimizing/ssa_builder.h
+++ b/compiler/optimizing/ssa_builder.h
@@ -21,7 +21,6 @@
 #include "base/scoped_arena_allocator.h"
 #include "base/scoped_arena_containers.h"
 #include "nodes.h"
-#include "optimization.h"
 
 namespace art HIDDEN {
 
diff --git a/compiler/optimizing/ssa_liveness_analysis.cc b/compiler/optimizing/ssa_liveness_analysis.cc
index 8d727a660a..c9df0d1917 100644
--- a/compiler/optimizing/ssa_liveness_analysis.cc
+++ b/compiler/optimizing/ssa_liveness_analysis.cc
@@ -16,13 +16,25 @@
 
 #include "ssa_liveness_analysis.h"
 
+#include "base/arena_bit_vector.h"
 #include "base/bit_vector-inl.h"
 #include "code_generator.h"
+#include "com_android_art_flags.h"
 #include "linear_order.h"
+#include "loop_information-inl.h"
 #include "nodes.h"
 
 namespace art HIDDEN {
 
+inline BlockInfo::BlockInfo(ScopedArenaAllocator* allocator, size_t number_of_ssa_values)
+    : live_in_(ArenaBitVector::CreateFixedSize(
+          allocator, number_of_ssa_values, kArenaAllocSsaLiveness)),
+      live_out_(ArenaBitVector::CreateFixedSize(
+          allocator, number_of_ssa_values, kArenaAllocSsaLiveness)),
+      kill_(ArenaBitVector::CreateFixedSize(
+          allocator, number_of_ssa_values, kArenaAllocSsaLiveness)) {
+}
+
 void SsaLivenessAnalysis::Analyze() {
   // Compute the linear order directly in the graph's data structure
   // (there are no more following graph mutations).
@@ -34,7 +46,7 @@ void SsaLivenessAnalysis::Analyze() {
 }
 
 void SsaLivenessAnalysis::NumberInstructions() {
-  int ssa_index = 0;
+  size_t ssa_index = 0;
   size_t lifetime_position = 0;
   // Each instruction gets a lifetime position, and a block gets a lifetime
   // start and end position. Non-phi instructions have a distinct lifetime position than
@@ -48,7 +60,8 @@ void SsaLivenessAnalysis::NumberInstructions() {
   for (HBasicBlock* block : graph_->GetLinearOrder()) {
     block->SetLifetimeStart(lifetime_position);
 
-    for (HInstructionIterator inst_it(block->GetPhis()); !inst_it.Done(); inst_it.Advance()) {
+    for (HInstructionIteratorPrefetchNext inst_it(block->GetPhis()); !inst_it.Done();
+         inst_it.Advance()) {
       HInstruction* current = inst_it.Current();
       codegen_->AllocateLocations(current);
       LocationSummary* locations = current->GetLocations();
@@ -60,12 +73,12 @@ void SsaLivenessAnalysis::NumberInstructions() {
       }
       current->SetLifetimePosition(lifetime_position);
     }
-    lifetime_position += 2;
+    lifetime_position += kLivenessPositionsPerInstruction;
 
     // Add a null marker to notify we are starting a block.
     instructions_from_lifetime_position_.push_back(nullptr);
 
-    for (HInstructionIterator inst_it(block->GetInstructions()); !inst_it.Done();
+    for (HInstructionIteratorPrefetchNext inst_it(block->GetInstructions()); !inst_it.Done();
          inst_it.Advance()) {
       HInstruction* current = inst_it.Current();
       codegen_->AllocateLocations(current);
@@ -78,18 +91,19 @@ void SsaLivenessAnalysis::NumberInstructions() {
       }
       instructions_from_lifetime_position_.push_back(current);
       current->SetLifetimePosition(lifetime_position);
-      lifetime_position += 2;
+      lifetime_position += kLivenessPositionsPerInstruction;
     }
 
     block->SetLifetimeEnd(lifetime_position);
   }
-  number_of_ssa_values_ = ssa_index;
+  DCHECK_EQ(GetNumberOfSsaValues(), ssa_index);
 }
 
 void SsaLivenessAnalysis::ComputeLiveness() {
+  size_t number_of_ssa_values = GetNumberOfSsaValues();
   for (HBasicBlock* block : graph_->GetLinearOrder()) {
     block_infos_[block->GetBlockId()] =
-        new (allocator_) BlockInfo(allocator_, *block, number_of_ssa_values_);
+        new (allocator_) BlockInfo(allocator_, number_of_ssa_values);
   }
 
   // Compute the live ranges, as well as the initial live_in, live_out, and kill sets.
@@ -187,9 +201,18 @@ void SsaLivenessAnalysis::ComputeLiveRanges() {
         // therefore be the same and we only need to keep alive one.
       } else {
         size_t phi_input_index = successor->GetPredecessorIndexOf(block);
-        for (HInstructionIterator phi_it(successor->GetPhis()); !phi_it.Done(); phi_it.Advance()) {
+        for (HInstructionIteratorPrefetchNext phi_it(successor->GetPhis()); !phi_it.Done();
+             phi_it.Advance()) {
           HInstruction* phi = phi_it.Current();
           HInstruction* input = phi->InputAt(phi_input_index);
+          if (com::android::art::flags::reg_alloc_spill_slot_reuse() &&
+              input->GetLiveInterval()->GetUses().empty()) {
+            // If the `input` has no recorded uses yet, the `phi` use shall be its last use
+            // (we visit blocks in reverse linear order) and the `input` dies at the end of
+            // the `block`. Record the `phi` interval as a hint to try using the same spill
+            // slot in order to avoid excessive moves if both `input` and `phi` get spilled.
+            input->GetLiveInterval()->SetHintPhiInterval(phi->GetLiveInterval());
+          }
           input->GetLiveInterval()->AddPhiUse(phi, phi_input_index, block);
           // A phi input whose last user is the phi dies at the end of the predecessor block,
           // and not at the phi's lifetime position.
@@ -205,7 +228,8 @@ void SsaLivenessAnalysis::ComputeLiveRanges() {
       current->GetLiveInterval()->AddRange(block->GetLifetimeStart(), block->GetLifetimeEnd());
     }
 
-    for (HBackwardInstructionIterator back_it(block->GetInstructions()); !back_it.Done();
+    for (HBackwardInstructionIteratorPrefetchNext back_it(block->GetInstructions());
+         !back_it.Done();
          back_it.Advance()) {
       HInstruction* current = back_it.Current();
       if (current->HasSsaIndex()) {
@@ -218,13 +242,31 @@ void SsaLivenessAnalysis::ComputeLiveRanges() {
       // Process inputs of instructions.
       if (current->IsEmittedAtUseSite()) {
         if (kIsDebugBuild) {
-          DCHECK(!current->GetLocations()->Out().IsValid());
-          for (const HUseListNode<HInstruction*>& use : current->GetUses()) {
-            HInstruction* user = use.GetUser();
-            size_t index = use.GetIndex();
-            DCHECK(!user->GetLocations()->InAt(index).IsValid());
+          CHECK(!current->GetLocations()->Out().IsValid());
+          CHECK(!current->HasEnvironmentUses());
+          if (current->IsNullCheck()) {
+            // Implicit null check is replaced by its input in all users before register
+            // allocation, so it does not have any uses at this point.
+            CHECK(current->GetUses().empty());
+          } else {
+            // TODO: Should we allow dead instructions marked as "emitted at use site"?
+            CHECK(!current->GetUses().empty());
+            for (const HUseListNode<HInstruction*>& use : current->GetUses()) {
+              HInstruction* user = use.GetUser();
+              size_t index = use.GetIndex();
+              CHECK(!user->GetLocations()->InAt(index).IsValid());
+            }
+            if (!current->GetUses().HasExactlyOneElement()) {
+              // If there is more than one user, there can be no unallocated locations.
+              // We do not have a way to record different locations for different use sites.
+              for (size_t i : Range(current->GetLocations()->GetInputCount())) {
+                CHECK(!current->GetLocations()->InAt(i).IsUnallocated());
+              }
+              for (size_t i : Range(current->GetLocations()->GetTempCount())) {
+                CHECK(!current->GetLocations()->GetTemp(i).IsUnallocated());
+              }
+            }
           }
-          DCHECK(!current->HasEnvironmentUses());
         }
       } else {
         // Process the environment first, because we know their uses come after
@@ -242,7 +284,8 @@ void SsaLivenessAnalysis::ComputeLiveRanges() {
     }
 
     // Kill phis defined in this block.
-    for (HInstructionIterator inst_it(block->GetPhis()); !inst_it.Done(); inst_it.Advance()) {
+    for (HInstructionIteratorPrefetchNext inst_it(block->GetPhis()); !inst_it.Done();
+         inst_it.Advance()) {
       HInstruction* current = inst_it.Current();
       if (current->HasSsaIndex()) {
         kill.SetBit(current->GetSsaIndex());
@@ -326,6 +369,195 @@ void SsaLivenessAnalysis::DoCheckNoLiveInIrreducibleLoop(const HBasicBlock& bloc
   }
 }
 
+void LiveInterval::AddUse(HInstruction* instruction,
+                          HEnvironment* environment,
+                          size_t input_index,
+                          HInstruction* actual_user) {
+  bool is_environment = (environment != nullptr);
+  LocationSummary* locations = instruction->GetLocations();
+  if (actual_user == nullptr) {
+    actual_user = instruction;
+  }
+
+  // Set the use within the instruction.
+  size_t position = actual_user->GetLifetimePosition() + kLivenessPositionOfNormalUse;
+  if (!is_environment) {
+    if (locations->IsFixedInput(input_index) || locations->OutputUsesSameAs(input_index)) {
+      // For fixed inputs and output same as input, the register allocator
+      // requires to have inputs die at the instruction, so that input moves use the
+      // location of the input just before that instruction (and not potential moves due
+      // to splitting).
+      DCHECK_EQ(instruction, actual_user);
+      position = actual_user->GetLifetimePosition();
+    } else if (!locations->InAt(input_index).IsValid()) {
+      return;
+    }
+  }
+
+  if (!is_environment && instruction->IsInLoop()) {
+    AddBackEdgeUses(*instruction->GetBlock());
+  }
+
+  if ((!uses_.empty()) &&
+      (uses_.front().GetUser() == actual_user) &&
+      (uses_.front().GetPosition() < position)) {
+    // The user uses the instruction multiple times, and one use dies before the other.
+    // We update the use list so that the latter is first.
+    DCHECK(!is_environment);
+    DCHECK(uses_.front().GetPosition() + kLivenessPositionOfNormalUse == position);
+    UsePositionList::iterator next_pos = uses_.begin();
+    UsePositionList::iterator insert_pos;
+    do {
+      insert_pos = next_pos;
+      ++next_pos;
+    } while (next_pos != uses_.end() && next_pos->GetPosition() < position);
+    UsePosition* new_use = new (allocator_) UsePosition(instruction, input_index, position);
+    uses_.insert_after(insert_pos, *new_use);
+    if (first_range_->GetEnd() == uses_.front().GetPosition()) {
+      first_range_->end_ = position;
+    }
+    return;
+  }
+
+  if (is_environment) {
+    DCHECK(env_uses_.empty() || position <= env_uses_.front().GetPosition());
+    EnvUsePosition* new_env_use =
+        new (allocator_) EnvUsePosition(environment, input_index, position);
+    env_uses_.push_front(*new_env_use);
+  } else {
+    DCHECK(uses_.empty() || position <= uses_.front().GetPosition());
+    UsePosition* new_use = new (allocator_) UsePosition(instruction, input_index, position);
+    uses_.push_front(*new_use);
+  }
+
+  size_t start_block_position = instruction->GetBlock()->GetLifetimeStart();
+  if (first_range_ == nullptr) {
+    // First time we see a use of that interval.
+    first_range_ = last_range_ = range_search_start_ =
+        new (allocator_) LiveRange(start_block_position, position, nullptr);
+  } else if (first_range_->GetStart() == start_block_position) {
+    // There is a use later in the same block or in a following block.
+    // Note that in such a case, `AddRange` for the whole blocks has been called
+    // before arriving in this method, and this is the reason the start of
+    // `first_range_` is before the given `position`.
+    DCHECK_LE(position, first_range_->GetEnd());
+  } else {
+    DCHECK(first_range_->GetStart() > position);
+    // There is a hole in the interval. Create a new range.
+    // Note that the start of `first_range_` can be equal to `end`: two blocks
+    // having adjacent lifetime positions are not necessarily
+    // predecessor/successor. When two blocks are predecessor/successor, the
+    // liveness algorithm has called `AddRange` before arriving in this method,
+    // and the check line 205 would succeed.
+    first_range_ = range_search_start_ =
+        new (allocator_) LiveRange(start_block_position, position, first_range_);
+  }
+}
+
+LiveInterval* LiveInterval::SplitAt(size_t position) {
+  DCHECK(!IsTemp());
+  DCHECK(!IsFixed());
+  DCHECK_GT(position, GetStart());
+
+  if (GetEnd() <= position) {
+    // This range dies before `position`, no need to split.
+    return nullptr;
+  }
+
+  LiveInterval* new_interval = new (allocator_) LiveInterval(allocator_, type_);
+
+  SafepointPositionList::const_iterator before = safepoints_.before_begin();
+  for (auto it = safepoints_.begin(), end = safepoints_.end(); it != end; ++it) {
+    if (it->GetPosition() >= position) {
+      break;
+    }
+    before = it;
+  }
+  new_interval->safepoints_.splice_after(
+      new_interval->safepoints_.before_begin(), safepoints_, before, safepoints_.end());
+
+  new_interval->next_sibling_ = next_sibling_;
+  next_sibling_ = new_interval;
+  new_interval->parent_ = parent_;
+
+  LiveRange* current = first_range_;
+  LiveRange* previous = nullptr;
+  // Iterate over the ranges, and either find a range that covers this position, or
+  // two ranges in between this position (that is, the position is in a lifetime hole).
+  do {
+    if (position >= current->GetEnd()) {
+      // Move to next range.
+      previous = current;
+      current = current->next_;
+    } else if (position <= current->GetStart()) {
+      // If the previous range did not cover this position, we know position is in
+      // a lifetime hole. We can just break the first_range_ and last_range_ links
+      // and return the new interval.
+      DCHECK(previous != nullptr);
+      DCHECK(current != first_range_);
+      new_interval->last_range_ = last_range_;
+      last_range_ = previous;
+      previous->next_ = nullptr;
+      new_interval->first_range_ = current;
+      if (range_search_start_ != nullptr && range_search_start_->GetEnd() >= current->GetEnd()) {
+        // Search start point is inside `new_interval`. Change it to null
+        // (i.e. the end of the interval) in the original interval.
+        range_search_start_ = nullptr;
+      }
+      new_interval->range_search_start_ = new_interval->first_range_;
+      return new_interval;
+    } else {
+      // This range covers position. We create a new last_range_ for this interval
+      // that covers last_range_->Start() and position. We also shorten the current
+      // range and make it the first range of the new interval.
+      DCHECK(position < current->GetEnd() && position > current->GetStart());
+      new_interval->last_range_ = last_range_;
+      last_range_ = new (allocator_) LiveRange(current->start_, position, nullptr);
+      if (previous != nullptr) {
+        previous->next_ = last_range_;
+      } else {
+        first_range_ = last_range_;
+      }
+      new_interval->first_range_ = current;
+      current->start_ = position;
+      if (range_search_start_ != nullptr && range_search_start_->GetEnd() >= current->GetEnd()) {
+        // Search start point is inside `new_interval`. Change it to `last_range`
+        // in the original interval. This is conservative but always correct.
+        range_search_start_ = last_range_;
+      }
+      new_interval->range_search_start_ = new_interval->first_range_;
+      return new_interval;
+    }
+  } while (current != nullptr);
+
+  LOG(FATAL) << "Unreachable";
+  return nullptr;
+}
+
+void LiveInterval::Dump(std::ostream& stream) const {
+  stream << "ranges: { ";
+  LiveRange* current = first_range_;
+  while (current != nullptr) {
+    current->Dump(stream);
+    stream << " ";
+    current = current->GetNext();
+  }
+  stream << "}, uses: { ";
+  for (const UsePosition& use : GetUses()) {
+    use.Dump(stream);
+    stream << " ";
+  }
+  stream << "}, { ";
+  for (const EnvUsePosition& env_use : GetEnvironmentUses()) {
+    env_use.Dump(stream);
+    stream << " ";
+  }
+  stream << "}";
+  stream << " is_fixed: " << is_fixed_ << ", is_split: " << IsSplit();
+  stream << " is_low: " << IsLowInterval();
+  stream << " is_high: " << IsHighInterval();
+}
+
 void LiveInterval::DumpWithContext(std::ostream& stream,
                                    const CodeGenerator& codegen) const {
   Dump(stream);
@@ -351,8 +583,8 @@ static int RegisterOrLowRegister(Location location) {
   return location.IsPair() ? location.low() : location.reg();
 }
 
-int LiveInterval::FindFirstRegisterHint(size_t* free_until,
-                                        const SsaLivenessAnalysis& liveness) const {
+int LiveInterval::FindFirstRegisterHint(
+    ArrayRef<size_t> free_until, ArrayRef<HInstruction* const> instructions_from_positions) const {
   DCHECK(!IsHighInterval());
   if (IsTemp()) return kNoRegister;
 
@@ -366,12 +598,15 @@ int LiveInterval::FindFirstRegisterHint(size_t* free_until,
     }
   }
 
-  if (IsSplit() && liveness.IsAtBlockBoundary(GetStart() / 2)) {
+  if (IsSplit() &&
+      SsaLivenessAnalysis::IsAtBlockBoundary(
+          GetStart() / kLivenessPositionsPerInstruction, instructions_from_positions)) {
     // If the start of this interval is at a block boundary, we look at the
     // location of the interval in blocks preceding the block this interval
     // starts at. If one location is a register we return it as a hint. This
     // will avoid a move between the two blocks.
-    HBasicBlock* block = liveness.GetBlockFromPosition(GetStart() / 2);
+    HBasicBlock* block = SsaLivenessAnalysis::GetBlockFromPosition(
+        GetStart() / kLivenessPositionsPerInstruction, instructions_from_positions);
     size_t next_register_use = FirstRegisterUse();
     for (HBasicBlock* predecessor : block->GetPredecessors()) {
       size_t position = predecessor->GetLifetimeEnd() - 1;
@@ -560,4 +795,49 @@ LiveInterval* LiveInterval::GetSiblingAt(size_t position) {
   return current;
 }
 
+void LiveInterval::AddBackEdgeUses(const HBasicBlock& block_at_use) {
+  DCHECK(block_at_use.IsInLoop());
+  if (block_at_use.GetGraph()->HasIrreducibleLoops()) {
+    // Linear order may not be well formed when irreducible loops are present,
+    // i.e. loop blocks may not be adjacent and a back edge may not be last,
+    // which violates assumptions made in this method.
+    return;
+  }
+
+  // Add synthesized uses at the back edge of loops to help the register allocator.
+  // Note that this method is called in decreasing liveness order, to facilitate adding
+  // uses at the head of the `uses_` list. Because below
+  // we iterate from inner-most to outer-most, which is in increasing liveness order,
+  // we need to add subsequent entries after the last inserted entry.
+  const UsePositionList::iterator old_begin = uses_.begin();
+  UsePositionList::iterator insert_pos = uses_.before_begin();
+  for (HLoopInformationOutwardIterator it(block_at_use); !it.Done(); it.Advance()) {
+    HLoopInformation* current = it.Current();
+    if (GetDefinedBy()->GetLifetimePosition() >= current->GetHeader()->GetLifetimeStart()) {
+      // This interval is defined in the loop. We can stop going outward.
+      break;
+    }
+
+    // We're only adding a synthesized use at the last back edge. Adding synthesized uses on
+    // all back edges is not necessary: anything used in the loop will have its use at the
+    // last back edge. If we want branches in a loop to have better register allocation than
+    // another branch, then it is the linear order we should change.
+    size_t back_edge_use_position = current->GetLifetimeEnd();
+    if ((old_begin != uses_.end()) && (old_begin->GetPosition() <= back_edge_use_position)) {
+      // There was a use already seen in this loop. Therefore the previous call to `AddUse`
+      // already inserted the backedge use. We can stop going outward.
+      DCHECK(HasSynthesizeUseAt(back_edge_use_position));
+      break;
+    }
+
+    DCHECK(insert_pos != uses_.before_begin()
+           ? back_edge_use_position > insert_pos->GetPosition()
+           : current == block_at_use.GetLoopInformation())
+        << std::distance(uses_.before_begin(), insert_pos);
+
+    UsePosition* new_use = new (allocator_) UsePosition(back_edge_use_position);
+    insert_pos = uses_.insert_after(insert_pos, *new_use);
+  }
+}
+
 }  // namespace art
diff --git a/compiler/optimizing/ssa_liveness_analysis.h b/compiler/optimizing/ssa_liveness_analysis.h
index 5a6ad84915..6f9bddd628 100644
--- a/compiler/optimizing/ssa_liveness_analysis.h
+++ b/compiler/optimizing/ssa_liveness_analysis.h
@@ -19,7 +19,7 @@
 
 #include <iostream>
 
-#include "base/arena_bit_vector.h"
+#include "base/array_ref.h"
 #include "base/bit_vector.h"
 #include "base/intrusive_forward_list.h"
 #include "base/iteration_range.h"
@@ -35,21 +35,19 @@ class SsaLivenessAnalysis;
 
 static constexpr int kNoRegister = -1;
 
+// Constants describing positions assigned to various data for an instruction.
+static constexpr size_t kLivenessPositionsPerInstruction = 4u;
+static constexpr size_t kLivenessPositionsForTemp = kLivenessPositionsPerInstruction - 1u;
+static constexpr size_t kLivenessPositionsToBlock = kLivenessPositionsPerInstruction - 1u;
+static constexpr size_t kLivenessPositionOfNormalUse = 1u;  // Inside instruction.
+static constexpr size_t kLivenessPositionOfFixedOutput = kLivenessPositionsPerInstruction - 1u;
+static constexpr size_t kLivenessPositionForMoveAfter = kLivenessPositionsPerInstruction - 1u;
+
 class BlockInfo : public ArenaObject<kArenaAllocSsaLiveness> {
  public:
-  BlockInfo(ScopedArenaAllocator* allocator, const HBasicBlock& block, size_t number_of_ssa_values)
-      : block_(block),
-        live_in_(ArenaBitVector::CreateFixedSize(
-            allocator, number_of_ssa_values, kArenaAllocSsaLiveness)),
-        live_out_(ArenaBitVector::CreateFixedSize(
-            allocator, number_of_ssa_values, kArenaAllocSsaLiveness)),
-        kill_(ArenaBitVector::CreateFixedSize(
-            allocator, number_of_ssa_values, kArenaAllocSsaLiveness)) {
-    UNUSED(block_);
-  }
+  BlockInfo(ScopedArenaAllocator* allocator, size_t number_of_ssa_values);
 
  private:
-  const HBasicBlock& block_;
   BitVectorView<size_t> live_in_;
   BitVectorView<size_t> live_out_;
   BitVectorView<size_t> kill_;
@@ -227,11 +225,11 @@ inline IterationRange<Iterator> FindMatchingUseRange(Iterator first,
   return MakeIterationRange(begin, end);
 }
 
-class SafepointPosition : public ArenaObject<kArenaAllocSsaLiveness> {
+class SafepointPosition : public ArenaObject<kArenaAllocSsaLiveness>,
+                          public IntrusiveForwardListNode<SafepointPosition> {
  public:
   explicit SafepointPosition(HInstruction* instruction)
-      : instruction_(instruction),
-        next_(nullptr) {}
+      : instruction_(instruction) {}
 
   static size_t ComputePosition(HInstruction* instruction) {
     // We special case instructions emitted at use site, as their
@@ -240,24 +238,16 @@ class SafepointPosition : public ArenaObject<kArenaAllocSsaLiveness> {
       // Currently only applies to implicit null checks, which are emitted
       // at the next instruction.
       DCHECK(instruction->IsNullCheck()) << instruction->DebugName();
-      return instruction->GetLifetimePosition() + 2;
+      return instruction->GetLifetimePosition() + kLivenessPositionsPerInstruction;
     } else {
       return instruction->GetLifetimePosition();
     }
   }
 
-  void SetNext(SafepointPosition* next) {
-    next_ = next;
-  }
-
   size_t GetPosition() const {
     return ComputePosition(instruction_);
   }
 
-  SafepointPosition* GetNext() const {
-    return next_;
-  }
-
   LocationSummary* GetLocations() const {
     return instruction_->GetLocations();
   }
@@ -268,11 +258,12 @@ class SafepointPosition : public ArenaObject<kArenaAllocSsaLiveness> {
 
  private:
   HInstruction* const instruction_;
-  SafepointPosition* next_;
 
   DISALLOW_COPY_AND_ASSIGN(SafepointPosition);
 };
 
+using SafepointPositionList = IntrusiveForwardList<SafepointPosition>;
+
 /**
  * An interval is a list of disjoint live ranges where an instruction is live.
  * Each instruction that has uses gets an interval.
@@ -288,115 +279,45 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
   static LiveInterval* MakeFixedInterval(ScopedArenaAllocator* allocator,
                                          int reg,
                                          DataType::Type type) {
-    return new (allocator) LiveInterval(allocator, type, nullptr, true, reg, false);
+    return new (allocator) LiveInterval(allocator, type, nullptr, true, reg);
   }
 
-  static LiveInterval* MakeTempInterval(ScopedArenaAllocator* allocator, DataType::Type type) {
-    return new (allocator) LiveInterval(allocator, type, nullptr, false, kNoRegister, true);
+  static LiveInterval* MakeTempInterval(ScopedArenaAllocator* allocator,
+                                        DataType::Type type,
+                                        size_t temp_index,
+                                        size_t position) {
+    int8_t checked_index = dchecked_integral_cast<int8_t>(temp_index);
+    LiveInterval* temp = new (allocator) LiveInterval(allocator,
+                                                      type,
+                                                      /*defined_by*/ nullptr,
+                                                      /*is_fixed=*/ false,
+                                                      /*reg=*/ kNoRegister,
+                                                      checked_index);
+    temp->AddRange(position, position + kLivenessPositionsForTemp);
+    return temp;
   }
 
-  bool IsFixed() const { return is_fixed_; }
-  bool IsTemp() const { return is_temp_; }
-  // This interval is the result of a split.
-  bool IsSplit() const { return parent_ != this; }
+  bool IsTemp() const {
+    static_assert(kNoTempIndex < 0);
+    return temp_index_ >= 0;
+  }
 
-  void AddTempUse(HInstruction* instruction, size_t temp_index) {
+  size_t GetTempIndex() const {
     DCHECK(IsTemp());
-    DCHECK(GetUses().empty()) << "A temporary can only have one user";
-    DCHECK(GetEnvironmentUses().empty()) << "A temporary cannot have environment user";
-    size_t position = instruction->GetLifetimePosition();
-    UsePosition* new_use = new (allocator_) UsePosition(instruction, temp_index, position);
-    uses_.push_front(*new_use);
-    AddRange(position, position + 1);
+    return dchecked_integral_cast<size_t>(temp_index_);
   }
 
+  bool IsFixed() const { return is_fixed_; }
+  // This interval is the result of a split.
+  bool IsSplit() const { return parent_ != this; }
+
   // Record use of an input. The use will be recorded as an environment use if
   // `environment` is not null and as register use otherwise. If `actual_user`
   // is specified, the use will be recorded at `actual_user`'s lifetime position.
   void AddUse(HInstruction* instruction,
               HEnvironment* environment,
               size_t input_index,
-              HInstruction* actual_user = nullptr) {
-    bool is_environment = (environment != nullptr);
-    LocationSummary* locations = instruction->GetLocations();
-    if (actual_user == nullptr) {
-      actual_user = instruction;
-    }
-
-    // Set the use within the instruction.
-    size_t position = actual_user->GetLifetimePosition() + 1;
-    if (!is_environment) {
-      if (locations->IsFixedInput(input_index) || locations->OutputUsesSameAs(input_index)) {
-        // For fixed inputs and output same as input, the register allocator
-        // requires to have inputs die at the instruction, so that input moves use the
-        // location of the input just before that instruction (and not potential moves due
-        // to splitting).
-        DCHECK_EQ(instruction, actual_user);
-        position = actual_user->GetLifetimePosition();
-      } else if (!locations->InAt(input_index).IsValid()) {
-        return;
-      }
-    }
-
-    if (!is_environment && instruction->IsInLoop()) {
-      AddBackEdgeUses(*instruction->GetBlock());
-    }
-
-    if ((!uses_.empty()) &&
-        (uses_.front().GetUser() == actual_user) &&
-        (uses_.front().GetPosition() < position)) {
-      // The user uses the instruction multiple times, and one use dies before the other.
-      // We update the use list so that the latter is first.
-      DCHECK(!is_environment);
-      DCHECK(uses_.front().GetPosition() + 1 == position);
-      UsePositionList::iterator next_pos = uses_.begin();
-      UsePositionList::iterator insert_pos;
-      do {
-        insert_pos = next_pos;
-        ++next_pos;
-      } while (next_pos != uses_.end() && next_pos->GetPosition() < position);
-      UsePosition* new_use = new (allocator_) UsePosition(instruction, input_index, position);
-      uses_.insert_after(insert_pos, *new_use);
-      if (first_range_->GetEnd() == uses_.front().GetPosition()) {
-        first_range_->end_ = position;
-      }
-      return;
-    }
-
-    if (is_environment) {
-      DCHECK(env_uses_.empty() || position <= env_uses_.front().GetPosition());
-      EnvUsePosition* new_env_use =
-          new (allocator_) EnvUsePosition(environment, input_index, position);
-      env_uses_.push_front(*new_env_use);
-    } else {
-      DCHECK(uses_.empty() || position <= uses_.front().GetPosition());
-      UsePosition* new_use = new (allocator_) UsePosition(instruction, input_index, position);
-      uses_.push_front(*new_use);
-    }
-
-    size_t start_block_position = instruction->GetBlock()->GetLifetimeStart();
-    if (first_range_ == nullptr) {
-      // First time we see a use of that interval.
-      first_range_ = last_range_ = range_search_start_ =
-          new (allocator_) LiveRange(start_block_position, position, nullptr);
-    } else if (first_range_->GetStart() == start_block_position) {
-      // There is a use later in the same block or in a following block.
-      // Note that in such a case, `AddRange` for the whole blocks has been called
-      // before arriving in this method, and this is the reason the start of
-      // `first_range_` is before the given `position`.
-      DCHECK_LE(position, first_range_->GetEnd());
-    } else {
-      DCHECK(first_range_->GetStart() > position);
-      // There is a hole in the interval. Create a new range.
-      // Note that the start of `first_range_` can be equal to `end`: two blocks
-      // having adjacent lifetime positions are not necessarily
-      // predecessor/successor. When two blocks are predecessor/successor, the
-      // liveness algorithm has called `AddRange` before arriving in this method,
-      // and the check line 205 would succeed.
-      first_range_ = range_search_start_ =
-          new (allocator_) LiveRange(start_block_position, position, first_range_);
-    }
-  }
+              HInstruction* actual_user = nullptr);
 
   void AddPhiUse(HInstruction* instruction, size_t input_index, HBasicBlock* block) {
     DCHECK(instruction->IsPhi());
@@ -452,13 +373,34 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
     }
   }
 
-  bool HasSpillSlot() const { return spill_slot_ != kNoSpillSlot; }
+  bool HasSpillSlot() const {
+    static_assert(kNoSpillSlot == -1);
+    return spill_slot_or_hint_ >= 0;
+  }
   void SetSpillSlot(int slot) {
-    DCHECK(!is_fixed_);
-    DCHECK(!is_temp_);
-    spill_slot_ = slot;
+    DCHECK(!IsFixed());
+    DCHECK(!IsTemp());
+    spill_slot_or_hint_ = slot;
+    DCHECK(HasSpillSlot());
+  }
+  int GetSpillSlot() const { return spill_slot_or_hint_; }
+
+  bool HasSpillSlotOrHint() const {
+    return spill_slot_or_hint_ != kNoSpillSlot;
+  }
+  bool HasSpillSlotHint() const {
+    return spill_slot_or_hint_ < kNoSpillSlot;
+  }
+  void SetSpillSlotHint(int hint) {
+    static_assert(kNoSpillSlot == -1);
+    DCHECK(!HasSpillSlotOrHint());
+    DCHECK_GE(hint, 0);
+    spill_slot_or_hint_ = -2 - hint;
+  }
+  int GetSpillSlotHint() const {
+    DCHECK(HasSpillSlotOrHint());
+    return HasSpillSlot() ? GetSpillSlot() : -(spill_slot_or_hint_ + 2);
   }
-  int GetSpillSlot() const { return spill_slot_; }
 
   void SetFrom(size_t from) {
     if (first_range_ != nullptr) {
@@ -467,8 +409,12 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
       // Instruction without uses.
       DCHECK(uses_.empty());
       DCHECK(from == defined_by_->GetLifetimePosition());
+      // TODO: The `kLivenessPositionsPerInstruction` below looks like a bug for calls coming
+      // from `RegisterAllocatorLinearScan::CheckForFixedOutput()` as the new range reaches
+      // into the next instruction. However, those call always take the `first_range_ != nullptr`
+      // path above. We should use another, simpler function for that.
       first_range_ = last_range_ = range_search_start_ =
-          new (allocator_) LiveRange(from, from + 2, nullptr);
+          new (allocator_) LiveRange(from, from + kLivenessPositionsPerInstruction, nullptr);
     }
   }
 
@@ -555,10 +501,7 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
   }
 
   size_t FirstRegisterUseAfter(size_t position) const {
-    if (is_temp_) {
-      return position == GetStart() ? position : kNoLifetime;
-    }
-
+    DCHECK(!IsTemp());
     if (IsDefiningPosition(position) && DefinitionRequiresRegister()) {
       return position;
     }
@@ -581,7 +524,8 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
   // Returns the location of the first register use for this live interval,
   // including a register definition if applicable.
   size_t FirstRegisterUse() const {
-    return FirstRegisterUseAfter(GetStart());
+    size_t start = GetStart();
+    return IsTemp() ? start : FirstRegisterUseAfter(start);
   }
 
   // Whether the interval requires a register rather than a stack location.
@@ -591,10 +535,7 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
   }
 
   size_t FirstUseAfter(size_t position) const {
-    if (is_temp_) {
-      return position == GetStart() ? position : kNoLifetime;
-    }
-
+    DCHECK(!IsTemp());
     if (IsDefiningPosition(position)) {
       DCHECK(defined_by_->GetLocations()->Out().IsValid());
       return position;
@@ -630,21 +571,10 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
   }
 
   bool HasWillCallSafepoint() const {
-    for (SafepointPosition* safepoint = first_safepoint_;
-         safepoint != nullptr;
-         safepoint = safepoint->GetNext()) {
-      if (safepoint->GetLocations()->WillCall()) return true;
-    }
-    return false;
-  }
-
-  SafepointPosition* FindSafepointJustBefore(size_t position) const {
-    for (SafepointPosition* safepoint = first_safepoint_, *previous = nullptr;
-         safepoint != nullptr;
-         previous = safepoint, safepoint = safepoint->GetNext()) {
-      if (safepoint->GetPosition() >= position) return previous;
-    }
-    return last_safepoint_;
+    return std::any_of(
+        safepoints_.begin(),
+        safepoints_.end(),
+        [](const SafepointPosition& safepoint) { return safepoint.GetLocations()->WillCall(); });
   }
 
   /**
@@ -654,87 +584,7 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
    * The new interval covers:
    * [position ... end)
    */
-  LiveInterval* SplitAt(size_t position) {
-    DCHECK(!is_temp_);
-    DCHECK(!is_fixed_);
-    DCHECK_GT(position, GetStart());
-
-    if (GetEnd() <= position) {
-      // This range dies before `position`, no need to split.
-      return nullptr;
-    }
-
-    LiveInterval* new_interval = new (allocator_) LiveInterval(allocator_, type_);
-    SafepointPosition* new_last_safepoint = FindSafepointJustBefore(position);
-    if (new_last_safepoint == nullptr) {
-      new_interval->first_safepoint_ = first_safepoint_;
-      new_interval->last_safepoint_ = last_safepoint_;
-      first_safepoint_ = last_safepoint_ = nullptr;
-    } else if (last_safepoint_ != new_last_safepoint) {
-      new_interval->last_safepoint_ = last_safepoint_;
-      new_interval->first_safepoint_ = new_last_safepoint->GetNext();
-      DCHECK(new_interval->first_safepoint_ != nullptr);
-      last_safepoint_ = new_last_safepoint;
-      last_safepoint_->SetNext(nullptr);
-    }
-
-    new_interval->next_sibling_ = next_sibling_;
-    next_sibling_ = new_interval;
-    new_interval->parent_ = parent_;
-
-    LiveRange* current = first_range_;
-    LiveRange* previous = nullptr;
-    // Iterate over the ranges, and either find a range that covers this position, or
-    // two ranges in between this position (that is, the position is in a lifetime hole).
-    do {
-      if (position >= current->GetEnd()) {
-        // Move to next range.
-        previous = current;
-        current = current->next_;
-      } else if (position <= current->GetStart()) {
-        // If the previous range did not cover this position, we know position is in
-        // a lifetime hole. We can just break the first_range_ and last_range_ links
-        // and return the new interval.
-        DCHECK(previous != nullptr);
-        DCHECK(current != first_range_);
-        new_interval->last_range_ = last_range_;
-        last_range_ = previous;
-        previous->next_ = nullptr;
-        new_interval->first_range_ = current;
-        if (range_search_start_ != nullptr && range_search_start_->GetEnd() >= current->GetEnd()) {
-          // Search start point is inside `new_interval`. Change it to null
-          // (i.e. the end of the interval) in the original interval.
-          range_search_start_ = nullptr;
-        }
-        new_interval->range_search_start_ = new_interval->first_range_;
-        return new_interval;
-      } else {
-        // This range covers position. We create a new last_range_ for this interval
-        // that covers last_range_->Start() and position. We also shorten the current
-        // range and make it the first range of the new interval.
-        DCHECK(position < current->GetEnd() && position > current->GetStart());
-        new_interval->last_range_ = last_range_;
-        last_range_ = new (allocator_) LiveRange(current->start_, position, nullptr);
-        if (previous != nullptr) {
-          previous->next_ = last_range_;
-        } else {
-          first_range_ = last_range_;
-        }
-        new_interval->first_range_ = current;
-        current->start_ = position;
-        if (range_search_start_ != nullptr && range_search_start_->GetEnd() >= current->GetEnd()) {
-          // Search start point is inside `new_interval`. Change it to `last_range`
-          // in the original interval. This is conservative but always correct.
-          range_search_start_ = last_range_;
-        }
-        new_interval->range_search_start_ = new_interval->first_range_;
-        return new_interval;
-      }
-    } while (current != nullptr);
-
-    LOG(FATAL) << "Unreachable";
-    return nullptr;
-  }
+  LiveInterval* SplitAt(size_t position);
 
   bool StartsBeforeOrAt(LiveInterval* other) const {
     return GetStart() <= other->GetStart();
@@ -744,29 +594,7 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
     return GetStart() > other->GetStart();
   }
 
-  void Dump(std::ostream& stream) const {
-    stream << "ranges: { ";
-    LiveRange* current = first_range_;
-    while (current != nullptr) {
-      current->Dump(stream);
-      stream << " ";
-      current = current->GetNext();
-    }
-    stream << "}, uses: { ";
-    for (const UsePosition& use : GetUses()) {
-      use.Dump(stream);
-      stream << " ";
-    }
-    stream << "}, { ";
-    for (const EnvUsePosition& env_use : GetEnvironmentUses()) {
-      env_use.Dump(stream);
-      stream << " ";
-    }
-    stream << "}";
-    stream << " is_fixed: " << is_fixed_ << ", is_split: " << IsSplit();
-    stream << " is_low: " << IsLowInterval();
-    stream << " is_high: " << IsHighInterval();
-  }
+  void Dump(std::ostream& stream) const;
 
   // Same as Dump, but adds context such as the instruction defining this interval, and
   // the register currently assigned to this interval.
@@ -784,7 +612,8 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
   // Returns the first register hint that is at least free before
   // the value contained in `free_until`. If none is found, returns
   // `kNoRegister`.
-  int FindFirstRegisterHint(size_t* free_until, const SsaLivenessAnalysis& liveness) const;
+  int FindFirstRegisterHint(ArrayRef<size_t> free_until,
+                            ArrayRef<HInstruction* const> instructions_from_positions) const;
 
   // If there is enough at the definition site to find a register (for example
   // it uses the same input as the first input), returns the register as a hint.
@@ -850,29 +679,56 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
     high_or_low_interval_ = high;
   }
 
-  void AddHighInterval(bool is_temp = false) {
+  void AddHighTempInterval() {
     DCHECK(IsParent());
     DCHECK(!HasHighInterval());
     DCHECK(!HasLowInterval());
-    high_or_low_interval_ = new (allocator_) LiveInterval(
-        allocator_, type_, defined_by_, false, kNoRegister, is_temp, true);
-    high_or_low_interval_->high_or_low_interval_ = this;
+    DCHECK(IsTemp());
+    LiveInterval* high = new (allocator_) LiveInterval(allocator_,
+                                                       type_,
+                                                       /*defined_by=*/ nullptr,
+                                                       /*is_fixed=*/ false,
+                                                       /*reg=*/ kNoRegister,
+                                                       temp_index_,
+                                                       /*is_high_interval=*/ true);
+    DCHECK(first_range_ != nullptr);
+    DCHECK(first_range_->GetNext() == nullptr);
+    high->AddRange(GetStart(), GetStart() + kLivenessPositionsForTemp);
+    DCHECK(uses_.empty());
+    DCHECK(env_uses_.empty());
+    high_or_low_interval_ = high;
+    high->high_or_low_interval_ = this;
+  }
+
+  void AddHighInterval() {
+    DCHECK(IsParent());
+    DCHECK(!HasHighInterval());
+    DCHECK(!HasLowInterval());
+    DCHECK(!IsTemp());
+    LiveInterval* high = new (allocator_) LiveInterval(allocator_,
+                                                       type_,
+                                                       defined_by_,
+                                                       /*is_fixed=*/ false,
+                                                       /*reg=*/ kNoRegister,
+                                                       kNoTempIndex,
+                                                       /*is_high_interval=*/ true);
     if (first_range_ != nullptr) {
-      high_or_low_interval_->first_range_ = first_range_->Dup(allocator_);
-      high_or_low_interval_->last_range_ = high_or_low_interval_->first_range_->GetLastRange();
-      high_or_low_interval_->range_search_start_ = high_or_low_interval_->first_range_;
+      high->first_range_ = first_range_->Dup(allocator_);
+      high->last_range_ = high->first_range_->GetLastRange();
+      high->range_search_start_ = high->first_range_;
     }
-    auto pos = high_or_low_interval_->uses_.before_begin();
+    auto pos = high->uses_.before_begin();
     for (const UsePosition& use : uses_) {
       UsePosition* new_use = use.Clone(allocator_);
-      pos = high_or_low_interval_->uses_.insert_after(pos, *new_use);
+      pos = high->uses_.insert_after(pos, *new_use);
     }
-
-    auto env_pos = high_or_low_interval_->env_uses_.before_begin();
+    auto env_pos = high->env_uses_.before_begin();
     for (const EnvUsePosition& env_use : env_uses_) {
       EnvUsePosition* new_env_use = env_use.Clone(allocator_);
-      env_pos = high_or_low_interval_->env_uses_.insert_after(env_pos, *new_env_use);
+      env_pos = high->env_uses_.insert_after(env_pos, *new_env_use);
     }
+    high_or_low_interval_ = high;
+    high->high_or_low_interval_ = this;
   }
 
   // Returns whether an interval, when it is non-split, is using
@@ -924,8 +780,9 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
             && interval->SameRegisterKind(*this)
             && interval->GetRegister() == GetRegister()) {
           // We found the input that has the same register. Check if it is live after
-          // `defined_by`_.
-          return !interval->CoversSlow(defined_by_->GetLifetimePosition() + 1);
+          // `defined_by_`.
+          return !interval->CoversSlow(
+              defined_by_->GetLifetimePosition() + kLivenessPositionOfNormalUse);
         }
       }
     }
@@ -933,19 +790,17 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
     UNREACHABLE();
   }
 
-  void AddSafepoint(HInstruction* instruction) {
-    SafepointPosition* safepoint = new (allocator_) SafepointPosition(instruction);
-    if (first_safepoint_ == nullptr) {
-      first_safepoint_ = last_safepoint_ = safepoint;
-    } else {
-      DCHECK_LE(last_safepoint_->GetPosition(), safepoint->GetPosition());
-      last_safepoint_->SetNext(safepoint);
-      last_safepoint_ = safepoint;
-    }
+  SafepointPosition* CreateSafepointPosition(HInstruction* instruction) {
+    return new (allocator_) SafepointPosition(instruction);
+  }
+
+  void SetSafepointPositions(SafepointPositionList&& list) {
+    DCHECK(safepoints_.empty());
+    safepoints_.swap(list);
   }
 
-  SafepointPosition* GetFirstSafepoint() const {
-    return first_safepoint_;
+  const SafepointPositionList& GetSafepoints() const {
+    return safepoints_;
   }
 
   // Resets the starting point for range-searching queries to the first range.
@@ -981,32 +836,42 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
     return false;
   }
 
+  void SetHintPhiInterval(LiveInterval* hint_phi_interval) {
+    DCHECK(hint_phi_interval->GetDefinedBy() != nullptr);
+    DCHECK(hint_phi_interval->GetDefinedBy()->IsPhi());
+    hint_phi_interval_ = hint_phi_interval;
+  }
+
+  LiveInterval* GetHintPhiInterval() {
+    return hint_phi_interval_;
+  }
+
  private:
   LiveInterval(ScopedArenaAllocator* allocator,
                DataType::Type type,
                HInstruction* defined_by = nullptr,
                bool is_fixed = false,
                int reg = kNoRegister,
-               bool is_temp = false,
+               int8_t temp_index = kNoTempIndex,
                bool is_high_interval = false)
       : allocator_(allocator),
         first_range_(nullptr),
         last_range_(nullptr),
         range_search_start_(nullptr),
-        first_safepoint_(nullptr),
-        last_safepoint_(nullptr),
+        safepoints_(),
         uses_(),
         env_uses_(),
-        type_(type),
         next_sibling_(nullptr),
         parent_(this),
+        defined_by_(defined_by),
+        high_or_low_interval_(nullptr),
+        hint_phi_interval_(nullptr),
         register_(reg),
-        spill_slot_(kNoSpillSlot),
+        spill_slot_or_hint_(kNoSpillSlot),
+        type_(type),
+        temp_index_(temp_index),
         is_fixed_(is_fixed),
-        is_temp_(is_temp),
-        is_high_interval_(is_high_interval),
-        high_or_low_interval_(nullptr),
-        defined_by_(defined_by) {}
+        is_high_interval_(is_high_interval) {}
 
   // Searches for a LiveRange that either covers the given position or is the
   // first next LiveRange. Returns null if no such LiveRange exists. Ranges
@@ -1048,52 +913,7 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
     return false;
   }
 
-  void AddBackEdgeUses(const HBasicBlock& block_at_use) {
-    DCHECK(block_at_use.IsInLoop());
-    if (block_at_use.GetGraph()->HasIrreducibleLoops()) {
-      // Linear order may not be well formed when irreducible loops are present,
-      // i.e. loop blocks may not be adjacent and a back edge may not be last,
-      // which violates assumptions made in this method.
-      return;
-    }
-
-    // Add synthesized uses at the back edge of loops to help the register allocator.
-    // Note that this method is called in decreasing liveness order, to faciliate adding
-    // uses at the head of the `uses_` list. Because below
-    // we iterate from inner-most to outer-most, which is in increasing liveness order,
-    // we need to add subsequent entries after the last inserted entry.
-    const UsePositionList::iterator old_begin = uses_.begin();
-    UsePositionList::iterator insert_pos = uses_.before_begin();
-    for (HLoopInformationOutwardIterator it(block_at_use);
-         !it.Done();
-         it.Advance()) {
-      HLoopInformation* current = it.Current();
-      if (GetDefinedBy()->GetLifetimePosition() >= current->GetHeader()->GetLifetimeStart()) {
-        // This interval is defined in the loop. We can stop going outward.
-        break;
-      }
-
-      // We're only adding a synthesized use at the last back edge. Adding synthesized uses on
-      // all back edges is not necessary: anything used in the loop will have its use at the
-      // last back edge. If we want branches in a loop to have better register allocation than
-      // another branch, then it is the linear order we should change.
-      size_t back_edge_use_position = current->GetLifetimeEnd();
-      if ((old_begin != uses_.end()) && (old_begin->GetPosition() <= back_edge_use_position)) {
-        // There was a use already seen in this loop. Therefore the previous call to `AddUse`
-        // already inserted the backedge use. We can stop going outward.
-        DCHECK(HasSynthesizeUseAt(back_edge_use_position));
-        break;
-      }
-
-      DCHECK(insert_pos != uses_.before_begin()
-             ? back_edge_use_position > insert_pos->GetPosition()
-             : current == block_at_use.GetLoopInformation())
-          << std::distance(uses_.before_begin(), insert_pos);
-
-      UsePosition* new_use = new (allocator_) UsePosition(back_edge_use_position);
-      insert_pos = uses_.insert_after(insert_pos, *new_use);
-    }
-  }
+  void AddBackEdgeUses(const HBasicBlock& block_at_use);
 
   ScopedArenaAllocator* const allocator_;
 
@@ -1107,48 +927,57 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
   LiveRange* range_search_start_;
 
   // Safepoints where this interval is live.
-  SafepointPosition* first_safepoint_;
-  SafepointPosition* last_safepoint_;
+  SafepointPositionList safepoints_;
 
   // Uses of this interval. Only the parent interval keeps these lists.
   UsePositionList uses_;
   EnvUsePositionList env_uses_;
 
-  // The instruction type this interval corresponds to.
-  const DataType::Type type_;
-
   // Live interval that is the result of a split.
   LiveInterval* next_sibling_;
 
   // The first interval from which split intervals come from.
   LiveInterval* parent_;
 
+  // The instruction represented by this interval.
+  HInstruction* const defined_by_;
+
+  // If this interval needs a register pair, the high or low equivalent.
+  // `is_high_interval_` tells whether this holds the low or the high.
+  LiveInterval* high_or_low_interval_;
+
+  // If the last use of the instruction is a Phi, keep a record of that Phi's interval
+  // for hints, except if the Phi is a loop Phi in an irreducible loop.
+  LiveInterval* hint_phi_interval_;
+
   // The register allocated to this interval.
   int register_;
 
-  // The spill slot allocated to this interval.
-  int spill_slot_;
+  // The spill slot allocated to this interval, or a spill slot hint, `kNoSpillSlot` if neither.
+  //
+  // Values >= 0 represent an actual spill slot, -1 is reserved for `kNoSpillSlot`
+  // and values <= -2 encode a non-negative spill slot hint as `-2 - hint`.
+  int spill_slot_or_hint_;
+
+  // The instruction type this interval corresponds to.
+  const DataType::Type type_;
+
+  // The index of the temporary, `kNoTempIndex` if not a temporary.
+  // Currently, we support only 32 core and 32 FP registers. We should never request more
+  // temps than that, so `int8_t` is enough. (Even if we added another register type.)
+  const int8_t temp_index_;
 
   // Whether the interval is for a fixed register.
   const bool is_fixed_;
 
-  // Whether the interval is for a temporary.
-  const bool is_temp_;
-
   // Whether this interval is a synthesized interval for register pair.
   const bool is_high_interval_;
 
-  // If this interval needs a register pair, the high or low equivalent.
-  // `is_high_interval_` tells whether this holds the low or the high.
-  LiveInterval* high_or_low_interval_;
-
-  // The instruction represented by this interval.
-  HInstruction* const defined_by_;
-
   static constexpr int kNoRegister = -1;
   static constexpr int kNoSpillSlot = -1;
+  static constexpr int8_t kNoTempIndex = -1;
 
-  ART_FRIEND_TEST(RegisterAllocatorTest, SpillInactive);
+  friend class RegisterAllocatorTest;
 
   DISALLOW_COPY_AND_ASSIGN(LiveInterval);
 };
@@ -1183,8 +1012,7 @@ class SsaLivenessAnalysis : public ValueObject {
                      nullptr,
                      allocator_->Adapter(kArenaAllocSsaLiveness)),
         instructions_from_ssa_index_(allocator_->Adapter(kArenaAllocSsaLiveness)),
-        instructions_from_lifetime_position_(allocator_->Adapter(kArenaAllocSsaLiveness)),
-        number_of_ssa_values_(0) {
+        instructions_from_lifetime_position_(allocator_->Adapter(kArenaAllocSsaLiveness)) {
   }
 
   void Analyze();
@@ -1205,43 +1033,45 @@ class SsaLivenessAnalysis : public ValueObject {
     return instructions_from_ssa_index_[index];
   }
 
+  ArrayRef<HInstruction* const> GetInstructionsFromSsaIndexes() const {
+    return ArrayRef<HInstruction* const>(instructions_from_ssa_index_);
+  }
+
   HInstruction* GetInstructionFromPosition(size_t index) const {
     return instructions_from_lifetime_position_[index];
   }
 
-  HBasicBlock* GetBlockFromPosition(size_t index) const {
-    HInstruction* instruction = GetInstructionFromPosition(index);
+  ArrayRef<HInstruction* const> GetInstructionsFromPositions() const {
+    return ArrayRef<HInstruction* const>(instructions_from_lifetime_position_);
+  }
+
+  static HBasicBlock* GetBlockFromPosition(
+      size_t index, ArrayRef<HInstruction* const> instructions_from_positions) {
+    HInstruction* instruction = instructions_from_positions[index];
     if (instruction == nullptr) {
       // If we are at a block boundary, get the block following.
-      instruction = GetInstructionFromPosition(index + 1);
+      instruction = instructions_from_positions[index + 1];
     }
     return instruction->GetBlock();
   }
 
-  bool IsAtBlockBoundary(size_t index) const {
-    return GetInstructionFromPosition(index) == nullptr;
+  static bool IsAtBlockBoundary(
+      size_t index, ArrayRef<HInstruction* const> instructions_from_positions) {
+    return instructions_from_positions[index] == nullptr;
   }
 
   HInstruction* GetTempUser(LiveInterval* temp) const {
     // A temporary shares the same lifetime start as the instruction that requires it.
     DCHECK(temp->IsTemp());
-    HInstruction* user = GetInstructionFromPosition(temp->GetStart() / 2);
-    DCHECK_EQ(user, temp->GetUses().front().GetUser());
+    HInstruction* user =
+        GetInstructionFromPosition(temp->GetStart() / kLivenessPositionsPerInstruction);
+    DCHECK(user != nullptr);
+    DCHECK_EQ(temp->GetStart(), user->GetLifetimePosition());
     return user;
   }
 
-  size_t GetTempIndex(LiveInterval* temp) const {
-    // We use the input index to store the index of the temporary in the user's temporary list.
-    DCHECK(temp->IsTemp());
-    return temp->GetUses().front().GetInputIndex();
-  }
-
-  size_t GetMaxLifetimePosition() const {
-    return instructions_from_lifetime_position_.size() * 2 - 1;
-  }
-
   size_t GetNumberOfSsaValues() const {
-    return number_of_ssa_values_;
+    return instructions_from_ssa_index_.size();
   }
 
   static constexpr const char* kLivenessPassName = "liveness";
@@ -1316,12 +1146,11 @@ class SsaLivenessAnalysis : public ValueObject {
   // Temporary array used when computing live_in, live_out, and kill sets.
   ScopedArenaVector<HInstruction*> instructions_from_ssa_index_;
 
-  // Temporary array used when inserting moves in the graph.
+  // Compressed map from lifetime position to instruction (nullptr for block start).
+  // Indexed by the lifetime position divided by `kLivenessPositionsPerInstruction`.
   ScopedArenaVector<HInstruction*> instructions_from_lifetime_position_;
-  size_t number_of_ssa_values_;
 
-  ART_FRIEND_TEST(RegisterAllocatorTest, SpillInactive);
-  ART_FRIEND_TEST(RegisterAllocatorTest, FreeUntil);
+  friend class RegisterAllocatorTest;
 
   DISALLOW_COPY_AND_ASSIGN(SsaLivenessAnalysis);
 };
diff --git a/compiler/optimizing/ssa_liveness_analysis_test.cc b/compiler/optimizing/ssa_liveness_analysis_test.cc
index d896aa88f4..2a51c849a5 100644
--- a/compiler/optimizing/ssa_liveness_analysis_test.cc
+++ b/compiler/optimizing/ssa_liveness_analysis_test.cc
@@ -37,16 +37,14 @@ class SsaLivenessAnalysisTest : public OptimizingUnitTest {
     codegen_ = CodeGenerator::Create(graph_, *compiler_options_);
     CHECK(codegen_ != nullptr);
     // Create entry block.
-    entry_ = new (GetAllocator()) HBasicBlock(graph_);
-    graph_->AddBlock(entry_);
+    entry_ = AddNewBlock();
     graph_->SetEntryBlock(entry_);
   }
 
  protected:
   HBasicBlock* CreateSuccessor(HBasicBlock* block) {
     HGraph* graph = block->GetGraph();
-    HBasicBlock* successor = new (GetAllocator()) HBasicBlock(graph);
-    graph->AddBlock(successor);
+    HBasicBlock* successor = AddNewBlock();
     block->AddSuccessor(successor);
     return successor;
   }
@@ -62,7 +60,8 @@ TEST_F(SsaLivenessAnalysisTest, TestReturnArg) {
 
   HBasicBlock* block = CreateSuccessor(entry_);
   MakeReturn(block, arg);
-  MakeExit(block);
+  HBasicBlock* exit = AddExitBlock();
+  block->AddSuccessor(exit);
 
   graph_->BuildDominatorTree();
   SsaLivenessAnalysis ssa_analysis(graph_, codegen_.get(), GetScopedAllocator());
@@ -70,8 +69,9 @@ TEST_F(SsaLivenessAnalysisTest, TestReturnArg) {
 
   std::ostringstream arg_dump;
   arg->GetLiveInterval()->Dump(arg_dump);
-  EXPECT_STREQ("ranges: { [2,6) }, uses: { 6 }, { } is_fixed: 0, is_split: 0 is_low: 0 is_high: 0",
-               arg_dump.str().c_str());
+  EXPECT_STREQ(
+      "ranges: { [4,12) }, uses: { 12 }, { } is_fixed: 0, is_split: 0 is_low: 0 is_high: 0",
+      arg_dump.str().c_str());
 }
 
 TEST_F(SsaLivenessAnalysisTest, TestAput) {
@@ -87,24 +87,26 @@ TEST_F(SsaLivenessAnalysisTest, TestAput) {
   HInstruction* length = MakeArrayLength(block, array);
   HInstruction* bounds_check = MakeBoundsCheck(block, index, length, /*env=*/ args);
   MakeArraySet(block, array, index, value, DataType::Type::kInt32);
+  HBasicBlock* exit = AddExitBlock();
+  block->AddSuccessor(exit);
 
   graph_->BuildDominatorTree();
   SsaLivenessAnalysis ssa_analysis(graph_, codegen_.get(), GetScopedAllocator());
   ssa_analysis.Analyze();
 
   EXPECT_FALSE(graph_->IsDebuggable());
-  EXPECT_EQ(18u, bounds_check->GetLifetimePosition());
+  EXPECT_EQ(36u, bounds_check->GetLifetimePosition());
   static const char* const expected[] = {
-      "ranges: { [2,21) }, uses: { 15 17 21 }, { 15 19 } is_fixed: 0, is_split: 0 is_low: 0 "
+      "ranges: { [4,41) }, uses: { 29 33 41 }, { 29 37 } is_fixed: 0, is_split: 0 is_low: 0 "
           "is_high: 0",
-      "ranges: { [4,21) }, uses: { 19 21 }, { } is_fixed: 0, is_split: 0 is_low: 0 "
+      "ranges: { [8,41) }, uses: { 37 41 }, { } is_fixed: 0, is_split: 0 is_low: 0 "
           "is_high: 0",
-      "ranges: { [6,21) }, uses: { 21 }, { } is_fixed: 0, is_split: 0 is_low: 0 "
+      "ranges: { [12,41) }, uses: { 41 }, { } is_fixed: 0, is_split: 0 is_low: 0 "
           "is_high: 0",
       // Environment uses do not keep the non-reference argument alive.
-      "ranges: { [8,10) }, uses: { }, { } is_fixed: 0, is_split: 0 is_low: 0 is_high: 0",
+      "ranges: { [16,20) }, uses: { }, { } is_fixed: 0, is_split: 0 is_low: 0 is_high: 0",
       // Environment uses keep the reference argument alive.
-      "ranges: { [10,19) }, uses: { }, { 15 19 } is_fixed: 0, is_split: 0 is_low: 0 is_high: 0",
+      "ranges: { [20,37) }, uses: { }, { 29 37 } is_fixed: 0, is_split: 0 is_low: 0 is_high: 0",
   };
   CHECK_EQ(arraysize(expected), args.size());
   size_t arg_index = 0u;
@@ -134,23 +136,25 @@ TEST_F(SsaLivenessAnalysisTest, TestDeoptimize) {
   block->AddInstruction(deoptimize);
   ManuallyBuildEnvFor(deoptimize, /*env=*/ args);
   MakeArraySet(block, array, index, value, DataType::Type::kInt32);
+  HBasicBlock* exit = AddExitBlock();
+  block->AddSuccessor(exit);
 
   graph_->BuildDominatorTree();
   SsaLivenessAnalysis ssa_analysis(graph_, codegen_.get(), GetScopedAllocator());
   ssa_analysis.Analyze();
 
   EXPECT_FALSE(graph_->IsDebuggable());
-  EXPECT_EQ(20u, deoptimize->GetLifetimePosition());
+  EXPECT_EQ(40u, deoptimize->GetLifetimePosition());
   static const char* const expected[] = {
-      "ranges: { [2,23) }, uses: { 15 17 23 }, { 15 21 } is_fixed: 0, is_split: 0 is_low: 0 "
+      "ranges: { [4,45) }, uses: { 29 33 45 }, { 29 41 } is_fixed: 0, is_split: 0 is_low: 0 "
           "is_high: 0",
-      "ranges: { [4,23) }, uses: { 19 23 }, { 21 } is_fixed: 0, is_split: 0 is_low: 0 "
+      "ranges: { [8,45) }, uses: { 37 45 }, { 41 } is_fixed: 0, is_split: 0 is_low: 0 "
           "is_high: 0",
-      "ranges: { [6,23) }, uses: { 23 }, { 21 } is_fixed: 0, is_split: 0 is_low: 0 is_high: 0",
+      "ranges: { [12,45) }, uses: { 45 }, { 41 } is_fixed: 0, is_split: 0 is_low: 0 is_high: 0",
       // Environment use in HDeoptimize keeps even the non-reference argument alive.
-      "ranges: { [8,21) }, uses: { }, { 21 } is_fixed: 0, is_split: 0 is_low: 0 is_high: 0",
+      "ranges: { [16,41) }, uses: { }, { 41 } is_fixed: 0, is_split: 0 is_low: 0 is_high: 0",
       // Environment uses keep the reference argument alive.
-      "ranges: { [10,21) }, uses: { }, { 15 21 } is_fixed: 0, is_split: 0 is_low: 0 is_high: 0",
+      "ranges: { [20,41) }, uses: { }, { 29 41 } is_fixed: 0, is_split: 0 is_low: 0 is_high: 0",
   };
   CHECK_EQ(arraysize(expected), args.size());
   size_t arg_index = 0u;
diff --git a/compiler/optimizing/ssa_phi_elimination.cc b/compiler/optimizing/ssa_phi_elimination.cc
index b2a3846dc4..1cf2b5f4c5 100644
--- a/compiler/optimizing/ssa_phi_elimination.cc
+++ b/compiler/optimizing/ssa_phi_elimination.cc
@@ -20,6 +20,8 @@
 #include "base/scoped_arena_allocator.h"
 #include "base/scoped_arena_containers.h"
 #include "base/bit_vector-inl.h"
+#include "loop_information.h"
+#include "nodes.h"
 
 namespace art HIDDEN {
 
@@ -44,7 +46,8 @@ void SsaDeadPhiElimination::MarkDeadPhis() {
 
   // Add to the worklist phis referenced by non-phi instructions.
   for (HBasicBlock* block : graph_->GetReversePostOrder()) {
-    for (HInstructionIterator inst_it(block->GetPhis()); !inst_it.Done(); inst_it.Advance()) {
+    for (HInstructionIteratorPrefetchNext inst_it(block->GetPhis()); !inst_it.Done();
+         inst_it.Advance()) {
       HPhi* phi = inst_it.Current()->AsPhi();
       if (phi->IsDead()) {
         continue;
@@ -123,6 +126,10 @@ void SsaDeadPhiElimination::EliminateDeadPhis() {
   }
 }
 
+inline bool IsIrreducibleLoopHeaderPhi(HPhi* phi) {
+  return phi->GetBlock()->IsLoopHeader() && phi->GetBlock()->GetLoopInformation()->IsIrreducible();
+}
+
 bool SsaRedundantPhiElimination::Run() {
   // Use local allocator for allocating memory used by this optimization.
   ScopedArenaAllocator allocator(graph_->GetArenaStack());
@@ -134,7 +141,8 @@ bool SsaRedundantPhiElimination::Run() {
   // Add all phis in the worklist. Order does not matter for correctness, and
   // neither will necessarily converge faster.
   for (HBasicBlock* block : graph_->GetReversePostOrder()) {
-    for (HInstructionIterator inst_it(block->GetPhis()); !inst_it.Done(); inst_it.Advance()) {
+    for (HInstructionIteratorPrefetchNext inst_it(block->GetPhis()); !inst_it.Done();
+         inst_it.Advance()) {
       worklist.push_back(inst_it.Current()->AsPhi());
     }
   }
@@ -165,7 +173,7 @@ bool SsaRedundantPhiElimination::Run() {
     cycle_worklist.push_back(phi);
     visited_phis_in_cycle.SetBit(phi->GetId());
     bool catch_phi_in_cycle = phi->IsCatchPhi();
-    bool irreducible_loop_phi_in_cycle = phi->IsIrreducibleLoopHeaderPhi();
+    bool irreducible_loop_phi_in_cycle = IsIrreducibleLoopHeaderPhi(phi);
 
     // First do a simple loop over inputs and check if they are all the same.
     for (HInstruction* input : phi->GetInputs()) {
@@ -197,7 +205,7 @@ bool SsaRedundantPhiElimination::Run() {
               cycle_worklist.push_back(input->AsPhi());
               visited_phis_in_cycle.SetBit(input->GetId());
               catch_phi_in_cycle |= input->AsPhi()->IsCatchPhi();
-              irreducible_loop_phi_in_cycle |= input->IsIrreducibleLoopHeaderPhi();
+              irreducible_loop_phi_in_cycle |= IsIrreducibleLoopHeaderPhi(input->AsPhi());
             } else {
               // Already visited, nothing to do.
             }
diff --git a/compiler/optimizing/ssa_phi_elimination.h b/compiler/optimizing/ssa_phi_elimination.h
index f606f928fa..8641014e81 100644
--- a/compiler/optimizing/ssa_phi_elimination.h
+++ b/compiler/optimizing/ssa_phi_elimination.h
@@ -18,7 +18,6 @@
 #define ART_COMPILER_OPTIMIZING_SSA_PHI_ELIMINATION_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
 
 namespace art HIDDEN {
diff --git a/compiler/optimizing/ssa_test.cc b/compiler/optimizing/ssa_test.cc
index 980493db34..fec5ccb3a9 100644
--- a/compiler/optimizing/ssa_test.cc
+++ b/compiler/optimizing/ssa_test.cc
@@ -72,10 +72,10 @@ class SsaPrettyPrinter : public HPrettyPrinter {
 static void ReNumberInstructions(HGraph* graph) {
   int id = 0;
   for (HBasicBlock* block : graph->GetBlocks()) {
-    for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
       it.Current()->SetId(id++);
     }
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       it.Current()->SetId(id++);
     }
   }
@@ -90,7 +90,7 @@ void SsaTest::TestCode(const std::vector<uint16_t>& data, const char* expected)
 
   // Test that phis had their type set.
   for (HBasicBlock* block : graph->GetBlocks()) {
-    for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
       ASSERT_NE(it.Current()->GetType(), DataType::Type::kVoid);
     }
   }
diff --git a/compiler/optimizing/stack_map_test.cc b/compiler/optimizing/stack_map_test.cc
index 6eb4fd9cf5..6b3c92c06b 100644
--- a/compiler/optimizing/stack_map_test.cc
+++ b/compiler/optimizing/stack_map_test.cc
@@ -19,7 +19,7 @@
 #include "art_method.h"
 #include "base/arena_bit_vector.h"
 #include "base/macros.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "stack_map_stream.h"
 
 #include "gtest/gtest.h"
@@ -49,7 +49,7 @@ using Kind = DexRegisterLocation::Kind;
 constexpr static uint32_t kPcAlign = GetInstructionSetInstructionAlignment(kRuntimeISA);
 
 TEST(StackMapTest, Test1) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
   StackMapStream stream(&allocator, kRuntimeISA);
@@ -108,7 +108,7 @@ TEST(StackMapTest, Test1) {
 }
 
 TEST(StackMapTest, Test2) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
   StackMapStream stream(&allocator, kRuntimeISA);
@@ -307,7 +307,7 @@ TEST(StackMapTest, Test2) {
 }
 
 TEST(StackMapTest, TestDeduplicateInlineInfoDexRegisterMap) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
   StackMapStream stream(&allocator, kRuntimeISA);
@@ -375,7 +375,7 @@ TEST(StackMapTest, TestDeduplicateInlineInfoDexRegisterMap) {
 }
 
 TEST(StackMapTest, TestNonLiveDexRegisters) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
   StackMapStream stream(&allocator, kRuntimeISA);
@@ -428,7 +428,7 @@ TEST(StackMapTest, TestNonLiveDexRegisters) {
 }
 
 TEST(StackMapTest, TestShareDexRegisterMap) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
   StackMapStream stream(&allocator, kRuntimeISA);
@@ -489,7 +489,7 @@ TEST(StackMapTest, TestShareDexRegisterMap) {
 }
 
 TEST(StackMapTest, TestNoDexRegisterMap) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
   StackMapStream stream(&allocator, kRuntimeISA);
@@ -539,7 +539,7 @@ TEST(StackMapTest, TestNoDexRegisterMap) {
 }
 
 TEST(StackMapTest, InlineTest) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
   StackMapStream stream(&allocator, kRuntimeISA);
@@ -734,7 +734,7 @@ TEST(StackMapTest, PackedNativePcTest) {
 }
 
 TEST(StackMapTest, TestDeduplicateStackMask) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
   StackMapStream stream(&allocator, kRuntimeISA);
diff --git a/compiler/optimizing/superblock_cloner.cc b/compiler/optimizing/superblock_cloner.cc
index 5ab34fb1a8..98263f09d8 100644
--- a/compiler/optimizing/superblock_cloner.cc
+++ b/compiler/optimizing/superblock_cloner.cc
@@ -174,7 +174,7 @@ void SuperblockCloner::RemapOrigInternalOrIncomingEdge(HBasicBlock* orig_block,
   // in the end all of the phis in the copy successor have the same number of inputs - the number
   // of copy successor's predecessors.
   bool first_phi_met = false;
-  for (HInstructionIterator it(orig_succ->GetPhis()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(orig_succ->GetPhis()); !it.Done(); it.Advance()) {
     HPhi* orig_phi = it.Current()->AsPhi();
     HPhi* copy_phi = GetInstrCopy(orig_phi)->AsPhi();
     HInstruction* orig_phi_input = orig_phi->InputAt(this_index);
@@ -204,7 +204,7 @@ void SuperblockCloner::AddCopyInternalEdge(HBasicBlock* orig_block,
   copy_block->AddSuccessor(copy_succ);
 
   size_t orig_index = orig_succ->GetPredecessorIndexOf(orig_block);
-  for (HInstructionIterator it(orig_succ->GetPhis()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(orig_succ->GetPhis()); !it.Done(); it.Advance()) {
     HPhi* orig_phi = it.Current()->AsPhi();
     HPhi* copy_phi = GetInstrCopy(orig_phi)->AsPhi();
     HInstruction* orig_phi_input = orig_phi->InputAt(orig_index);
@@ -220,7 +220,7 @@ void SuperblockCloner::RemapCopyInternalEdge(HBasicBlock* orig_block,
   DCHECK(copy_block->HasSuccessor(orig_succ));
 
   size_t orig_index = orig_succ->GetPredecessorIndexOf(orig_block);
-  for (HInstructionIterator it(orig_succ->GetPhis()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(orig_succ->GetPhis()); !it.Done(); it.Advance()) {
     HPhi* orig_phi = it.Current()->AsPhi();
     HInstruction* orig_phi_input = orig_phi->InputAt(orig_index);
     orig_phi->AddInput(orig_phi_input);
@@ -269,7 +269,13 @@ void SuperblockCloner::FindBackEdgesLocal(HBasicBlock* entry_block, ArenaBitVect
 
       if (visiting.IsBitSet(successor_id)) {
         DCHECK(ContainsElement(worklist, successor));
-        successor->AddBackEdgeWhileUpdating(current);
+        // Register a back edge; if the `successor` was not a loop header, or if its loop info
+        // points to the cloned source loop header, associate a newly created loop info with it.
+        if (successor->GetLoopInformation() == nullptr ||
+            successor->GetLoopInformation()->GetHeader() != successor) {
+          successor->SetLoopInformation(new (arena_) HLoopInformation(successor, graph_));
+        }
+        successor->GetLoopInformation()->AddBackEdge(current);
       } else if (!visited.IsBitSet(successor_id)) {
         visited.SetBit(successor_id);
         visiting.SetBit(successor_id);
@@ -445,7 +451,7 @@ void SuperblockCloner::FindAndSetLocalAreaForAdjustments() {
 
   if (outer_loop_ != nullptr) {
     // Save the loop population info as it will be changed later.
-    outer_loop_bb_set_.Copy(&outer_loop_->GetBlocks());
+    outer_loop_bb_set_.Copy(&outer_loop_->GetBlockMask());
   }
 }
 
@@ -504,7 +510,7 @@ void SuperblockCloner::ResolveDataFlow() {
   for (auto entry : *bb_map_) {
     HBasicBlock* orig_block = entry.first;
 
-    for (HInstructionIterator it(orig_block->GetPhis()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(orig_block->GetPhis()); !it.Done(); it.Advance()) {
       HPhi* orig_phi = it.Current()->AsPhi();
       HPhi* copy_phi = GetInstrCopy(orig_phi)->AsPhi();
       ResolvePhi(orig_phi);
@@ -512,7 +518,8 @@ void SuperblockCloner::ResolveDataFlow() {
     }
     if (kIsDebugBuild) {
       // Inputs of instruction copies must be already mapped to correspondent inputs copies.
-      for (HInstructionIterator it(orig_block->GetInstructions()); !it.Done(); it.Advance()) {
+      for (HInstructionIteratorPrefetchNext it(orig_block->GetInstructions()); !it.Done();
+           it.Advance()) {
         CheckInstructionInputsRemapping(it.Current());
       }
     }
@@ -528,7 +535,7 @@ bool SuperblockCloner::CollectLiveOutsAndCheckClonable(HInstructionMap* live_out
   for (uint32_t idx : orig_bb_set_.Indexes()) {
     HBasicBlock* block = GetBlockById(idx);
 
-    for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetPhis()); !it.Done(); it.Advance()) {
       HInstruction* instr = it.Current();
       DCHECK(instr->IsClonable());
 
@@ -537,7 +544,7 @@ bool SuperblockCloner::CollectLiveOutsAndCheckClonable(HInstructionMap* live_out
       }
     }
 
-    for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    for (HInstructionIteratorPrefetchNext it(block->GetInstructions()); !it.Done(); it.Advance()) {
       HInstruction* instr = it.Current();
       if (!instr->IsClonable()) {
         return false;
@@ -875,7 +882,7 @@ bool SuperblockCloner::IsFastCase() const {
   }
 
   // Check that orig_bb_set_ corresponds to loop peeling/unrolling.
-  if (common_loop_info == nullptr || !orig_bb_set_.SameBitsSet(&common_loop_info->GetBlocks())) {
+  if (common_loop_info == nullptr || !orig_bb_set_.SameBitsSet(&common_loop_info->GetBlockMask())) {
     return false;
   }
 
@@ -967,7 +974,8 @@ void SuperblockCloner::CleanUp() {
   // As this is needed to be processed we also simplify phis with multiple same inputs here.
   for (auto entry : *bb_map_) {
     for (HBasicBlock* block : {entry.first, entry.second}) {
-      for (HInstructionIterator inst_it(block->GetPhis()); !inst_it.Done(); inst_it.Advance()) {
+      for (HInstructionIteratorPrefetchNext inst_it(block->GetPhis()); !inst_it.Done();
+           inst_it.Advance()) {
         HPhi* phi = inst_it.Current()->AsPhi();
         if (ArePhiInputsTheSame(phi)) {
           phi->ReplaceWith(phi->InputAt(0));
@@ -984,11 +992,11 @@ void SuperblockCloner::CleanUp() {
 
 HBasicBlock* SuperblockCloner::CloneBasicBlock(const HBasicBlock* orig_block) {
   HGraph* graph = orig_block->GetGraph();
-  HBasicBlock* copy_block = new (arena_) HBasicBlock(graph, orig_block->GetDexPc());
+  HBasicBlock* copy_block = HBasicBlock::Create(arena_, graph, orig_block->GetDexPc());
   graph->AddBlock(copy_block);
 
   // Clone all the phis and add them to the map.
-  for (HInstructionIterator it(orig_block->GetPhis()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(orig_block->GetPhis()); !it.Done(); it.Advance()) {
     HInstruction* orig_instr = it.Current();
     HInstruction* copy_instr = orig_instr->Clone(arena_);
     copy_block->AddPhi(copy_instr->AsPhi());
@@ -998,7 +1006,8 @@ HBasicBlock* SuperblockCloner::CloneBasicBlock(const HBasicBlock* orig_block) {
   }
 
   // Clone all the instructions and add them to the map.
-  for (HInstructionIterator it(orig_block->GetInstructions()); !it.Done(); it.Advance()) {
+  for (HInstructionIteratorPrefetchNext it(orig_block->GetInstructions()); !it.Done();
+       it.Advance()) {
     HInstruction* orig_instr = it.Current();
     HInstruction* copy_instr = orig_instr->Clone(arena_);
     ReplaceInputsWithCopies(copy_instr);
diff --git a/compiler/optimizing/superblock_cloner.h b/compiler/optimizing/superblock_cloner.h
index d4db0b3852..1bc6d131b5 100644
--- a/compiler/optimizing/superblock_cloner.h
+++ b/compiler/optimizing/superblock_cloner.h
@@ -374,7 +374,8 @@ class LoopClonerHelper : public ValueObject {
                    SuperblockCloner::HInstructionMap* hir_map,
                    InductionVarRange* induction_range) :
       loop_info_(info),
-      cloner_(info->GetHeader()->GetGraph(), &info->GetBlocks(), bb_map, hir_map, induction_range) {
+      cloner_(
+          info->GetHeader()->GetGraph(), &info->GetBlockMask(), bb_map, hir_map, induction_range) {
     // For now do transformations only for natural loops.
     DCHECK(!info->IsIrreducible());
   }
diff --git a/compiler/optimizing/superblock_cloner_test.cc b/compiler/optimizing/superblock_cloner_test.cc
index 1bef8a4e9d..1928658e68 100644
--- a/compiler/optimizing/superblock_cloner_test.cc
+++ b/compiler/optimizing/superblock_cloner_test.cc
@@ -113,7 +113,7 @@ TEST_F(SuperblockClonerTest, CloneBasicBlocks) {
   HInstructionMap hir_map(std::less<HInstruction*>(), arena->Adapter(kArenaAllocSuperblockCloner));
 
   HLoopInformation* loop_info = header->GetLoopInformation();
-  orig_bb_set.Union(&loop_info->GetBlocks());
+  orig_bb_set.Union(&loop_info->GetBlockMask());
 
   SuperblockCloner cloner(graph_,
                           &orig_bb_set,
@@ -190,7 +190,7 @@ TEST_F(SuperblockClonerTest, AdjustControlFlowInfo) {
       arena, graph_->GetBlocks().size(), false, kArenaAllocSuperblockCloner);
 
   HLoopInformation* loop_info = header->GetLoopInformation();
-  orig_bb_set.Union(&loop_info->GetBlocks());
+  orig_bb_set.Union(&loop_info->GetBlockMask());
 
   SuperblockCloner cloner(graph_,
                           &orig_bb_set,
@@ -319,7 +319,7 @@ TEST_F(SuperblockClonerTest, LoopPeelingMultipleBackEdges) {
 
   MakeIf(if_block, param_);
 
-  HInstructionIterator it(header->GetPhis());
+  HInstructionIteratorPrefetchNext it(header->GetPhis());
   DCHECK(!it.Done());
   HPhi* loop_phi = it.Current()->AsPhi();
   HInstruction* temp_add =
@@ -501,7 +501,7 @@ TEST_F(SuperblockClonerTest, FastCaseCheck) {
 
   ArenaBitVector orig_bb_set(
       arena, graph_->GetBlocks().size(), false, kArenaAllocSuperblockCloner);
-  orig_bb_set.Union(&loop_info->GetBlocks());
+  orig_bb_set.Union(&loop_info->GetBlockMask());
 
   HEdgeSet remap_orig_internal(graph_->GetAllocator()->Adapter(kArenaAllocSuperblockCloner));
   HEdgeSet remap_copy_internal(graph_->GetAllocator()->Adapter(kArenaAllocSuperblockCloner));
diff --git a/compiler/optimizing/x86_memory_gen.h b/compiler/optimizing/x86_memory_gen.h
index 1cae1a5d3a..cb251d04d0 100644
--- a/compiler/optimizing/x86_memory_gen.h
+++ b/compiler/optimizing/x86_memory_gen.h
@@ -18,7 +18,6 @@
 #define ART_COMPILER_OPTIMIZING_X86_MEMORY_GEN_H_
 
 #include "base/macros.h"
-#include "nodes.h"
 #include "optimization.h"
 
 namespace art HIDDEN {
diff --git a/compiler/trampolines/trampoline_compiler.cc b/compiler/trampolines/trampoline_compiler.cc
index b3fc688cc9..35025205d9 100644
--- a/compiler/trampolines/trampoline_compiler.cc
+++ b/compiler/trampolines/trampoline_compiler.cc
@@ -17,7 +17,7 @@
 #include "trampoline_compiler.h"
 
 #include "base/arena_allocator.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "jni/jni_env_ext.h"
 
 #ifdef ART_ENABLE_CODEGEN_arm
@@ -200,7 +200,7 @@ static std::unique_ptr<const std::vector<uint8_t>> CreateTrampoline(ArenaAllocat
 std::unique_ptr<const std::vector<uint8_t>> CreateTrampoline64(InstructionSet isa,
                                                                EntryPointCallingConvention abi,
                                                                ThreadOffset64 offset) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaAllocator allocator(&pool);
   switch (isa) {
 #ifdef ART_ENABLE_CODEGEN_arm64
@@ -226,7 +226,7 @@ std::unique_ptr<const std::vector<uint8_t>> CreateTrampoline64(InstructionSet is
 std::unique_ptr<const std::vector<uint8_t>> CreateTrampoline32(InstructionSet isa,
                                                                EntryPointCallingConvention abi,
                                                                ThreadOffset32 offset) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaAllocator allocator(&pool);
   switch (isa) {
 #ifdef ART_ENABLE_CODEGEN_arm
diff --git a/compiler/utils/assembler_test.h b/compiler/utils/assembler_test.h
index e90187ccbc..f80cc3e19b 100644
--- a/compiler/utils/assembler_test.h
+++ b/compiler/utils/assembler_test.h
@@ -28,12 +28,16 @@
 
 #include "base/array_ref.h"
 #include "base/macros.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "assembler_test_base.h"
 #include "common_runtime_test.h"  // For ScratchFile
 
 namespace art HIDDEN {
 
+typedef void (*DriverFnPtr)(const std::vector<uint8_t>& art_code,
+                            const std::string& assembly_text,
+                            const std::string& test_name);
+
 // Helper for a constexpr string length.
 constexpr size_t ConstexprStrLen(char const* str, size_t count = 0) {
   return ('\0' == str[0]) ? count : ConstexprStrLen(str+1, count+1);
@@ -72,6 +76,13 @@ class AssemblerTest : public AssemblerTestBase {
     DriverWrapper(assembly_string, test_name);
   }
 
+  // Since it's a string arg, the driver assumes assembler is already called.
+  void CustomDriverStr(DriverFnPtr custom_driver,
+                       const std::string& assembly_string,
+                       const std::string& test_name) {
+    DriverWrapper(assembly_string, test_name, custom_driver);
+  }
+
   //
   // Register repeats.
   //
@@ -214,9 +225,8 @@ class AssemblerTest : public AssemblerTestBase {
     for (auto reg1 : reg1_registers) {
       for (auto reg2 : reg2_registers) {
         for (int64_t imm : imms) {
-          ImmType new_imm = CreateImmediate(imm);
           if (f != nullptr) {
-            (assembler_.get()->*f)(reg1, reg2, new_imm * multiplier + bias);
+            (assembler_.get()->*f)(reg1, reg2, CreateImmediate(imm * multiplier + bias));
           }
           std::string base = fmt;
 
@@ -771,6 +781,42 @@ class AssemblerTest : public AssemblerTestBase {
         fmt);
   }
 
+  // Repeats over addresses and vec-registers provided by fixture.
+  std::string RepeatAV(void (Ass::*f)(const Addr&, VecReg), const std::string& fmt) {
+    return RepeatAV(f, GetAddresses(), fmt);
+  }
+
+  // Variant that takes explicit vector of address
+  // (to test restricted addressing modes set).
+  std::string RepeatAV(void (Ass::*f)(const Addr&, VecReg),
+                       const std::vector<Addr>& a,
+                       const std::string& fmt) {
+    return RepeatTemplatedMemReg<Addr, VecReg>(f,
+                                               a,
+                                               GetVectorRegisters(),
+                                               &AssemblerTest::GetAddrName,
+                                               &AssemblerTest::GetVecRegName,
+                                               fmt);
+  }
+
+  // Repeats over vec-registers and addresses provided by fixture.
+  std::string RepeatVA(void (Ass::*f)(VecReg, const Addr&), const std::string& fmt) {
+    return RepeatVA(f, GetAddresses(), fmt);
+  }
+
+  // Variant that takes explicit vector of address
+  // (to test restricted addressing modes set).
+  std::string RepeatVA(void (Ass::*f)(VecReg, const Addr&),
+                       const std::vector<Addr>& a,
+                       const std::string& fmt) {
+    return RepeatTemplatedRegMem<VecReg, Addr>(f,
+                                               GetVectorRegisters(),
+                                               a,
+                                               &AssemblerTest::GetVecRegName,
+                                               &AssemblerTest::GetAddrName,
+                                               fmt);
+  }
+
   template <typename ImmType>
   std::string RepeatVIb(void (Ass::*f)(VecReg, ImmType),
                         int imm_bits,
@@ -1635,19 +1681,25 @@ class AssemblerTest : public AssemblerTestBase {
   // Override this to pad the code with NOPs to a certain size if needed.
   virtual void Pad([[maybe_unused]] std::vector<uint8_t>& data) {}
 
-  void DriverWrapper(const std::string& assembly_text, const std::string& test_name) {
+  void DriverWrapper(const std::string& assembly_text,
+                     const std::string& test_name,
+                     DriverFnPtr custom_driver = nullptr) {
     assembler_->FinalizeCode();
     size_t cs = assembler_->CodeSize();
     std::unique_ptr<std::vector<uint8_t>> data(new std::vector<uint8_t>(cs));
     MemoryRegion code(&(*data)[0], data->size());
     assembler_->CopyInstructions(code);
     Pad(*data);
-    Driver(*data, assembly_text, test_name);
+    if (custom_driver != nullptr) {
+      (*custom_driver)(*data, assembly_text, test_name);
+    } else {
+      Driver(*data, assembly_text, test_name);
+    }
   }
 
   static constexpr size_t kWarnManyCombinationsThreshold = 500;
 
-  MallocArenaPool pool_;
+  CallocArenaPool pool_;
   std::unique_ptr<ArenaAllocator> allocator_;
   std::unique_ptr<Ass> assembler_;
 
diff --git a/compiler/utils/assembler_thumb_test.cc b/compiler/utils/assembler_thumb_test.cc
index 53cb3d6f8e..823541b3bf 100644
--- a/compiler/utils/assembler_thumb_test.cc
+++ b/compiler/utils/assembler_thumb_test.cc
@@ -31,7 +31,7 @@
 
 #include "base/hex_dump.h"
 #include "base/macros.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "common_runtime_test.h"
 
 namespace art HIDDEN {
@@ -88,7 +88,7 @@ class ArmVIXLAssemblerTest : public AssemblerTestBase {
 
 #define __ assembler.
 
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaAllocator allocator;
   ArmVIXLJNIMacroAssembler assembler;
 };
diff --git a/compiler/utils/jni_macro_assembler_test.h b/compiler/utils/jni_macro_assembler_test.h
index ff182e6146..7ecf547ae9 100644
--- a/compiler/utils/jni_macro_assembler_test.h
+++ b/compiler/utils/jni_macro_assembler_test.h
@@ -21,7 +21,7 @@
 
 #include "assembler_test_base.h"
 #include "base/macros.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "common_runtime_test.h"  // For ScratchFile
 
 #include <sys/stat.h>
@@ -89,7 +89,7 @@ class JNIMacroAssemblerTest : public AssemblerTestBase {
     Driver(*data, assembly_text, test_name);
   }
 
-  MallocArenaPool pool_;
+  CallocArenaPool pool_;
   std::unique_ptr<ArenaAllocator> allocator_;
   std::unique_ptr<Ass> assembler_;
 
diff --git a/compiler/utils/riscv64/jni_macro_assembler_riscv64_test.cc b/compiler/utils/riscv64/jni_macro_assembler_riscv64_test.cc
index be6feeb9de..db718b319d 100644
--- a/compiler/utils/riscv64/jni_macro_assembler_riscv64_test.cc
+++ b/compiler/utils/riscv64/jni_macro_assembler_riscv64_test.cc
@@ -32,7 +32,7 @@
 #include "utils/assembler_test_base.h"
 
 #include "base/macros.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 
 namespace art HIDDEN {
 namespace riscv64 {
@@ -73,7 +73,7 @@ class JniMacroAssemblerRiscv64Test : public AssemblerTestBase {
   static const size_t kWordSize = 4u;
   static const size_t kDoubleWordSize = 8u;
 
-  MallocArenaPool pool_;
+  CallocArenaPool pool_;
   ArenaAllocator allocator_;
   Riscv64JNIMacroAssembler assembler_;
 };
diff --git a/compiler/utils/x86/assembler_x86_test.cc b/compiler/utils/x86/assembler_x86_test.cc
index b9205a9b31..b925164835 100644
--- a/compiler/utils/x86/assembler_x86_test.cc
+++ b/compiler/utils/x86/assembler_x86_test.cc
@@ -18,14 +18,14 @@
 
 #include "base/arena_allocator.h"
 #include "base/macros.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "base/stl_util.h"
 #include "utils/assembler_test.h"
 
 namespace art HIDDEN {
 
 TEST(AssemblerX86, CreateBuffer) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaAllocator allocator(&pool);
   AssemblerBuffer buffer(&allocator);
   AssemblerBuffer::EnsureCapacity ensured(&buffer);
diff --git a/compiler/utils/x86_64/assembler_x86_64.cc b/compiler/utils/x86_64/assembler_x86_64.cc
index 6330cc5d62..46258c9749 100644
--- a/compiler/utils/x86_64/assembler_x86_64.cc
+++ b/compiler/utils/x86_64/assembler_x86_64.cc
@@ -24,12 +24,20 @@
 namespace art HIDDEN {
 namespace x86_64 {
 
+inline uint8_t GetEncodedVexLen(XmmRegister xmmReg) {
+  return xmmReg.IsYMM() ? SET_VEX_L_256 : SET_VEX_L_128;
+}
+
 std::ostream& operator<<(std::ostream& os, const CpuRegister& reg) {
   return os << reg.AsRegister();
 }
 
 std::ostream& operator<<(std::ostream& os, const XmmRegister& reg) {
-  return os << reg.AsFloatRegister();
+  if (reg.IsYMM()) {
+    return os << "ymm" << static_cast<int>(reg.AsFloatRegister());
+  } else {
+    return os << reg.AsFloatRegister();
+  }
 }
 
 std::ostream& operator<<(std::ostream& os, const X87Register& reg) {
@@ -64,13 +72,8 @@ std::ostream& operator<<(std::ostream& os, const Address& addr) {
   }
 }
 
-bool X86_64Assembler::CpuHasAVXorAVX2FeatureFlag() {
-  if (has_AVX_ || has_AVX2_) {
-    return true;
-  }
-  return false;
-}
-
+bool X86_64Assembler::CpuHasAVXFeatureFlag() { return has_AVX_; }
+bool X86_64Assembler::CpuHasAVX2FeatureFlag() { return has_AVX2_; }
 
 void X86_64Assembler::call(CpuRegister reg) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -414,7 +417,7 @@ void X86_64Assembler::leal(CpuRegister dst, const Address& src) {
 
 
 void X86_64Assembler::movaps(XmmRegister dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovaps(dst, src);
     return;
   }
@@ -426,13 +429,15 @@ void X86_64Assembler::movaps(XmmRegister dst, XmmRegister src) {
 }
 
 
-/**VEX.128.0F.WIG 28 /r VMOVAPS xmm1, xmm2 */
+/* VEX.128.0F.WIG 28 /r VMOVAPS xmm1, xmm2
+   VEX.256.0F.WIG 28 /r VMOVAPS ymm1, ymm2 */
 void X86_64Assembler::vmovaps(XmmRegister dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   uint8_t byte_zero, byte_one, byte_two;
   bool is_twobyte_form = true;
-  bool load = dst.NeedsRex();
-  bool store = !load;
+  bool store = src.NeedsRex() && !dst.NeedsRex();
+  bool load = !store;
+  uint8_t vex_len = GetEncodedVexLen(dst);
 
   if (src.NeedsRex()&& dst.NeedsRex()) {
     is_twobyte_form = false;
@@ -443,18 +448,13 @@ void X86_64Assembler::vmovaps(XmmRegister dst, XmmRegister src) {
   X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
   if (is_twobyte_form) {
     bool rex_bit = (load) ? dst.NeedsRex() : src.NeedsRex();
-    byte_one = EmitVexPrefixByteOne(rex_bit,
-                                    vvvv_reg,
-                                    SET_VEX_L_128,
-                                    SET_VEX_PP_NONE);
+    byte_one = EmitVexPrefixByteOne(rex_bit, vvvv_reg, vex_len, SET_VEX_PP_NONE);
   } else {
     byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
                                     /*X=*/ false,
                                     src.NeedsRex(),
                                     SET_VEX_M_0F);
-    byte_two = EmitVexPrefixByteTwo(/*W=*/ false,
-                                    SET_VEX_L_128,
-                                    SET_VEX_PP_NONE);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, vex_len, SET_VEX_PP_NONE);
   }
   EmitUint8(byte_zero);
   EmitUint8(byte_one);
@@ -462,13 +462,13 @@ void X86_64Assembler::vmovaps(XmmRegister dst, XmmRegister src) {
     EmitUint8(byte_two);
   }
   // Instruction Opcode
-  if (is_twobyte_form && store) {
+  if (store) {
     EmitUint8(0x29);
   } else {
     EmitUint8(0x28);
   }
   // Instruction Operands
-  if (is_twobyte_form && store) {
+  if (store) {
     EmitXmmRegisterOperand(src.LowBits(), dst);
   } else {
     EmitXmmRegisterOperand(dst.LowBits(), src);
@@ -476,7 +476,7 @@ void X86_64Assembler::vmovaps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::movaps(XmmRegister dst, const Address& src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovaps(dst, src);
     return;
   }
@@ -487,12 +487,14 @@ void X86_64Assembler::movaps(XmmRegister dst, const Address& src) {
   EmitOperand(dst.LowBits(), src);
 }
 
-/**VEX.128.0F.WIG 28 /r VMOVAPS xmm1, m128 */
+/* VEX.128.0F.WIG 28 /r VMOVAPS xmm1, m128
+   VEX.256.0F.WIG 28 /r VMOVAPS ymm1, m128 */
 void X86_64Assembler::vmovaps(XmmRegister dst, const Address& src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t vex_len = GetEncodedVexLen(dst);
   // Instruction VEX Prefix
-  EmitVexPrefixForAddress(src, dst.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_NONE);
+  EmitVexPrefixForAddress(src, dst.NeedsRex(), vex_len, SET_VEX_PP_NONE);
   // Instruction Opcode
   EmitUint8(0x28);
   // Instruction Operands
@@ -500,7 +502,7 @@ void X86_64Assembler::vmovaps(XmmRegister dst, const Address& src) {
 }
 
 void X86_64Assembler::movups(XmmRegister dst, const Address& src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovups(dst, src);
     return;
   }
@@ -511,12 +513,14 @@ void X86_64Assembler::movups(XmmRegister dst, const Address& src) {
   EmitOperand(dst.LowBits(), src);
 }
 
-/** VEX.128.0F.WIG 10 /r VMOVUPS xmm1, m128 */
+/* VEX.128.0F.WIG 10 /r VMOVUPS xmm1, m128
+   VEX.256.0F.WIG 10 /r VMOVUPS ymm1, m128 */
 void X86_64Assembler::vmovups(XmmRegister dst, const Address& src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t vex_len = GetEncodedVexLen(dst);
   // Instruction VEX Prefix
-  EmitVexPrefixForAddress(src, dst.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_NONE);
+  EmitVexPrefixForAddress(src, dst.NeedsRex(), vex_len, SET_VEX_PP_NONE);
   // Instruction Opcode
   EmitUint8(0x10);
   // Instruction Operands
@@ -525,7 +529,7 @@ void X86_64Assembler::vmovups(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::movaps(const Address& dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (src.IsYMM()) {
     vmovaps(dst, src);
     return;
   }
@@ -536,12 +540,14 @@ void X86_64Assembler::movaps(const Address& dst, XmmRegister src) {
   EmitOperand(src.LowBits(), dst);
 }
 
-/** VEX.128.0F.WIG 29 /r VMOVAPS m128, xmm1 */
+/* VEX.128.0F.WIG 29 /r VMOVAPS m128, xmm1
+   VEX.256.0F.WIG 29 /r VMOVAPS m128, ymm1 */
 void X86_64Assembler::vmovaps(const Address& dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t vex_len = GetEncodedVexLen(src);
   // Instruction VEX Prefix
-  EmitVexPrefixForAddress(dst, src.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_NONE);
+  EmitVexPrefixForAddress(dst, src.NeedsRex(), vex_len, SET_VEX_PP_NONE);
   // Instruction Opcode
   EmitUint8(0x29);
   // Instruction Operands
@@ -549,7 +555,7 @@ void X86_64Assembler::vmovaps(const Address& dst, XmmRegister src) {
 }
 
 void X86_64Assembler::movups(const Address& dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (src.IsYMM()) {
     vmovups(dst, src);
     return;
   }
@@ -560,12 +566,14 @@ void X86_64Assembler::movups(const Address& dst, XmmRegister src) {
   EmitOperand(src.LowBits(), dst);
 }
 
-/** VEX.128.0F.WIG 11 /r VMOVUPS m128, xmm1 */
+/* VEX.128.0F.WIG 11 /r VMOVUPS m128, xmm1
+   VEX.128.0F.WIG 11 /r VMOVUPS m128, ymm1 */
 void X86_64Assembler::vmovups(const Address& dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t vex_len = GetEncodedVexLen(src);
   // Instruction VEX Prefix
-  EmitVexPrefixForAddress(dst, src.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_NONE);
+  EmitVexPrefixForAddress(dst, src.NeedsRex(), vex_len, SET_VEX_PP_NONE);
   // Instruction Opcode
   EmitUint8(0x11);
   // Instruction Operands
@@ -574,6 +582,10 @@ void X86_64Assembler::vmovups(const Address& dst, XmmRegister src) {
 
 
 void X86_64Assembler::movss(XmmRegister dst, const Address& src) {
+  if (dst.IsYMM()) {
+    vmovss(dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF3);
   EmitOptionalRex32(dst, src);
@@ -584,6 +596,10 @@ void X86_64Assembler::movss(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::movss(const Address& dst, XmmRegister src) {
+  if (src.IsYMM()) {
+    vmovss(dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF3);
   EmitOptionalRex32(src, dst);
@@ -594,6 +610,10 @@ void X86_64Assembler::movss(const Address& dst, XmmRegister src) {
 
 
 void X86_64Assembler::movss(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vmovss(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF3);
   EmitOptionalRex32(src, dst);  // Movss is MR encoding instead of the usual RM.
@@ -602,6 +622,34 @@ void X86_64Assembler::movss(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(src.LowBits(), dst);
 }
 
+/* VEX.LIG.F3.0F.WIG 10 /r VMOVSS xmm1, m32 */
+void X86_64Assembler::vmovss(XmmRegister dst, const Address& src) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  // Instruction VEX Prefix
+  EmitVexPrefixForAddress(src, dst.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_F3);
+  // Instruction Opcode
+  EmitUint8(0x10);
+  // Instruction Operands
+  EmitOperand(dst.LowBits(), src);
+}
+
+/* VEX.LIG.F3.0F.WIG 11 /r VMOVSS m32, xmm1 */
+void X86_64Assembler::vmovss(const Address& dst, XmmRegister src) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  // Instruction VEX Prefix
+  EmitVexPrefixForAddress(dst, src.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_F3);
+  // Instruction Opcode
+  EmitUint8(0x11);
+  // Instruction Operands
+  EmitOperand(src.LowBits(), dst);
+}
+
+/* VEX.LIG.F3.0F.WIG 10/11 /r VMOVSS xmm1, xmm2, xmm3 */
+void X86_64Assembler::vmovss(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  EmitVecMergeFPRegs(dst, src1, src2, SET_VEX_PP_F3);
+}
 
 void X86_64Assembler::movsxd(CpuRegister dst, CpuRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -620,25 +668,61 @@ void X86_64Assembler::movsxd(CpuRegister dst, const Address& src) {
 
 
 void X86_64Assembler::movq(XmmRegister dst, CpuRegister src) {
+  if (dst.IsYMM()) {
+    vmovq(dst, src);
+    return;
+  }
   EmitMovCpuFpu(dst, src, /*is64bit=*/ true, /*opcode=*/ 0x6E);
 }
 
 
 void X86_64Assembler::movq(CpuRegister dst, XmmRegister src) {
+  if (src.IsYMM()) {
+    vmovq(dst, src);
+    return;
+  }
   EmitMovCpuFpu(src, dst, /*is64bit=*/ true, /*opcode=*/ 0x7E);
 }
 
 
 void X86_64Assembler::movd(XmmRegister dst, CpuRegister src) {
+  if (dst.IsYMM()) {
+    vmovd(dst, src);
+    return;
+  }
   EmitMovCpuFpu(dst, src, /*is64bit=*/ false, /*opcode=*/ 0x6E);
 }
 
 
 void X86_64Assembler::movd(CpuRegister dst, XmmRegister src) {
+  if (src.IsYMM()) {
+    vmovd(dst, src);
+    return;
+  }
   EmitMovCpuFpu(src, dst, /*is64bit=*/ false, /*opcode=*/ 0x7E);
 }
 
 
+/* VEX.128.66.0F.W1 6E /r VMOVQ xmm1, r64/m64 */
+void X86_64Assembler::vmovq(XmmRegister dst, CpuRegister src) {
+  EmitVecMoveCpuFpu(dst, src, /*is64bit=*/ true, /*opcode=*/ 0x6E);
+}
+
+/* VEX.128.66.0F.W1 7E /r VMOVQ r64/m64, xmm1 */
+void X86_64Assembler::vmovq(CpuRegister dst, XmmRegister src) {
+  EmitVecMoveCpuFpu(src, dst, /*is64bit=*/ true, /*opcode=*/ 0x7E);
+}
+
+/* VEX.128.66.0F.W0 6E /r VMOVD xmm1, r32/m32 */
+void X86_64Assembler::vmovd(XmmRegister dst, CpuRegister src) {
+  EmitVecMoveCpuFpu(dst, src, /*is64bit=*/ false, /*opcode=*/ 0x6E);
+}
+
+/* VEX.128.66.0F.W0 7E /r VMOVD r32/m32, xmm1 */
+void X86_64Assembler::vmovd(CpuRegister dst, XmmRegister src) {
+  EmitVecMoveCpuFpu(src, dst, /*is64bit=*/ false, /*opcode=*/ 0x7E);
+}
+
 void X86_64Assembler::addss(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF3);
@@ -719,6 +803,10 @@ void X86_64Assembler::divss(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::addps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vaddps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -728,6 +816,10 @@ void X86_64Assembler::addps(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::subps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vsubps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -746,6 +838,10 @@ void X86_64Assembler::vsubps(XmmRegister dst, XmmRegister src1, XmmRegister src2
 
 
 void X86_64Assembler::mulps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vmulps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -759,6 +855,10 @@ void X86_64Assembler::vmulps(XmmRegister dst, XmmRegister src1, XmmRegister src2
 }
 
 void X86_64Assembler::divps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vdivps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -771,7 +871,7 @@ void X86_64Assembler::vdivps(XmmRegister dst, XmmRegister src1, XmmRegister src2
 }
 
 void X86_64Assembler::vfmadd213ss(XmmRegister acc, XmmRegister left, XmmRegister right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
   ByteZero = EmitVexPrefixByteZero(/*is_twobyte_form=*/ false);
@@ -790,7 +890,7 @@ void X86_64Assembler::vfmadd213ss(XmmRegister acc, XmmRegister left, XmmRegister
 }
 
 void X86_64Assembler::vfmadd213sd(XmmRegister acc, XmmRegister left, XmmRegister right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
   ByteZero = EmitVexPrefixByteZero(/*is_twobyte_form=*/ false);
@@ -829,7 +929,7 @@ void X86_64Assembler::fstps(const Address& dst) {
 
 
 void X86_64Assembler::movapd(XmmRegister dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovapd(dst, src);
     return;
   }
@@ -841,34 +941,32 @@ void X86_64Assembler::movapd(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
-/** VEX.128.66.0F.WIG 28 /r VMOVAPD xmm1, xmm2 */
+/* VEX.128.66.0F.WIG 28/29 /r VMOVAPD xmm1, xmm2
+   VEX.256.66.0F.WIG 28/29 /r VMOVAPD ymm1, ymm2 */
 void X86_64Assembler::vmovapd(XmmRegister dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = true;
+  uint8_t vex_len = GetEncodedVexLen(dst);
 
   if (src.NeedsRex() && dst.NeedsRex()) {
     is_twobyte_form = false;
   }
   // Instruction VEX Prefix
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  bool load = dst.NeedsRex();
+  bool store = (src.NeedsRex() && !dst.NeedsRex());
+  bool load = !store;
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     bool rex_bit = load ? dst.NeedsRex() : src.NeedsRex();
-    ByteOne = EmitVexPrefixByteOne(rex_bit,
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(rex_bit, vvvv_reg, vex_len, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
                                    src.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, vex_len, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -876,13 +974,13 @@ void X86_64Assembler::vmovapd(XmmRegister dst, XmmRegister src) {
     EmitUint8(ByteTwo);
   }
   // Instruction Opcode
-  if (is_twobyte_form && !load) {
+  if (store) {
     EmitUint8(0x29);
   } else {
     EmitUint8(0x28);
   }
   // Instruction Operands
-  if (is_twobyte_form && !load) {
+  if (store) {
     EmitXmmRegisterOperand(src.LowBits(), dst);
   } else {
     EmitXmmRegisterOperand(dst.LowBits(), src);
@@ -890,7 +988,7 @@ void X86_64Assembler::vmovapd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::movapd(XmmRegister dst, const Address& src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovapd(dst, src);
     return;
   }
@@ -902,12 +1000,14 @@ void X86_64Assembler::movapd(XmmRegister dst, const Address& src) {
   EmitOperand(dst.LowBits(), src);
 }
 
-/** VEX.128.66.0F.WIG 28 /r VMOVAPD xmm1, m128 */
+/* VEX.128.66.0F.WIG 28 /r VMOVAPD xmm1, m128
+   VEX.256.66.0F.WIG 28 /r VMOVAPD ymm1, m256 */
 void X86_64Assembler::vmovapd(XmmRegister dst, const Address& src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t vex_len = GetEncodedVexLen(dst);
   // Instruction VEX Prefix
-  EmitVexPrefixForAddress(src, dst.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_66);
+  EmitVexPrefixForAddress(src, dst.NeedsRex(), vex_len, SET_VEX_PP_66);
   // Instruction Opcode
   EmitUint8(0x28);
   // Instruction Operands
@@ -915,7 +1015,7 @@ void X86_64Assembler::vmovapd(XmmRegister dst, const Address& src) {
 }
 
 void X86_64Assembler::movupd(XmmRegister dst, const Address& src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovupd(dst, src);
     return;
   }
@@ -927,12 +1027,14 @@ void X86_64Assembler::movupd(XmmRegister dst, const Address& src) {
   EmitOperand(dst.LowBits(), src);
 }
 
-/** VEX.128.66.0F.WIG 10 /r VMOVUPD xmm1, m128 */
+/* VEX.128.66.0F.WIG 10 /r VMOVUPD xmm1, m128
+   VEX.256.66.0F.WIG 10 /r VMOVUPD ymm1, m256 */
 void X86_64Assembler::vmovupd(XmmRegister dst, const Address& src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t vex_len = GetEncodedVexLen(dst);
   // Instruction VEX Prefix
-  EmitVexPrefixForAddress(src, dst.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_66);
+  EmitVexPrefixForAddress(src, dst.NeedsRex(), vex_len, SET_VEX_PP_66);
   // Instruction Opcode
   EmitUint8(0x10);
   // Instruction Operands
@@ -940,7 +1042,7 @@ void X86_64Assembler::vmovupd(XmmRegister dst, const Address& src) {
 }
 
 void X86_64Assembler::movapd(const Address& dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (src.IsYMM()) {
     vmovapd(dst, src);
     return;
   }
@@ -952,12 +1054,14 @@ void X86_64Assembler::movapd(const Address& dst, XmmRegister src) {
   EmitOperand(src.LowBits(), dst);
 }
 
-/** VEX.128.66.0F.WIG 29 /r VMOVAPD m128, xmm1 */
+/* VEX.128.66.0F.WIG 29 /r VMOVAPD m128, xmm1
+   VEX.256.66.0F.WIG 29 /r VMOVAPD m256, ymm1 */
 void X86_64Assembler::vmovapd(const Address& dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t vex_len = GetEncodedVexLen(src);
   // Instruction VEX Prefix
-  EmitVexPrefixForAddress(dst, src.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_66);
+  EmitVexPrefixForAddress(dst, src.NeedsRex(), vex_len, SET_VEX_PP_66);
   // Instruction Opcode
   EmitUint8(0x29);
   // Instruction Operands
@@ -965,7 +1069,7 @@ void X86_64Assembler::vmovapd(const Address& dst, XmmRegister src) {
 }
 
 void X86_64Assembler::movupd(const Address& dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (src.IsYMM()) {
     vmovupd(dst, src);
     return;
   }
@@ -977,12 +1081,14 @@ void X86_64Assembler::movupd(const Address& dst, XmmRegister src) {
   EmitOperand(src.LowBits(), dst);
 }
 
-/** VEX.128.66.0F.WIG 11 /r VMOVUPD m128, xmm1 */
+/* VEX.128.66.0F.WIG 11 /r VMOVUPD m128, xmm1
+   VEX.256.66.0F.WIG 11 /r VMOVUPD m256, ymm1 */
 void X86_64Assembler::vmovupd(const Address& dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t vex_len = GetEncodedVexLen(src);
   // Instruction VEX Prefix
-  EmitVexPrefixForAddress(dst, src.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_66);
+  EmitVexPrefixForAddress(dst, src.NeedsRex(), vex_len, SET_VEX_PP_66);
   // Instruction Opcode
   EmitUint8(0x11);
   // Instruction Operands
@@ -991,6 +1097,10 @@ void X86_64Assembler::vmovupd(const Address& dst, XmmRegister src) {
 
 
 void X86_64Assembler::movsd(XmmRegister dst, const Address& src) {
+  if (dst.IsYMM()) {
+    vmovsd(dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF2);
   EmitOptionalRex32(dst, src);
@@ -1001,6 +1111,10 @@ void X86_64Assembler::movsd(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::movsd(const Address& dst, XmmRegister src) {
+  if (src.IsYMM()) {
+    vmovsd(dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF2);
   EmitOptionalRex32(src, dst);
@@ -1011,6 +1125,10 @@ void X86_64Assembler::movsd(const Address& dst, XmmRegister src) {
 
 
 void X86_64Assembler::movsd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vmovsd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF2);
   EmitOptionalRex32(src, dst);  // Movsd is MR encoding instead of the usual RM.
@@ -1019,6 +1137,34 @@ void X86_64Assembler::movsd(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(src.LowBits(), dst);
 }
 
+/* VEX.LIG.F2.0F.WIG 10 /r VMOVSD xmm1, m64 */
+void X86_64Assembler::vmovsd(XmmRegister dst, const Address& src) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  // Instruction VEX Prefix
+  EmitVexPrefixForAddress(src, dst.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_F2);
+  // Instruction Opcode
+  EmitUint8(0x10);
+  // Instruction Operands
+  EmitOperand(dst.LowBits(), src);
+}
+
+/* VEX.LIG.F2.0F.WIG 11 /r VMOVSD m64, xmm1 */
+void X86_64Assembler::vmovsd(const Address& dst, XmmRegister src) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  // Instruction VEX Prefix
+  EmitVexPrefixForAddress(dst, src.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_F2);
+  // Instruction Opcode
+  EmitUint8(0x11);
+  // Instruction Operands
+  EmitOperand(src.LowBits(), dst);
+}
+
+/* VEX.LIG.F2.0F.WIG 10/11 /r VMOVSD xmm1, xmm2, xmm3 */
+void X86_64Assembler::vmovsd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  EmitVecMergeFPRegs(dst, src1, src2, SET_VEX_PP_F2);
+}
 
 void X86_64Assembler::addsd(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -1101,6 +1247,10 @@ void X86_64Assembler::divsd(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::addpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vaddpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1117,6 +1267,10 @@ void X86_64Assembler::vaddpd(XmmRegister dst, XmmRegister add_left, XmmRegister
 
 
 void X86_64Assembler::subpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vsubpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1132,6 +1286,10 @@ void X86_64Assembler::vsubpd(XmmRegister dst, XmmRegister src1, XmmRegister src2
 
 
 void X86_64Assembler::mulpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vmulpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1146,6 +1304,10 @@ void X86_64Assembler::vmulpd(XmmRegister dst, XmmRegister src1, XmmRegister src2
 }
 
 void X86_64Assembler::divpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vdivpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1161,7 +1323,7 @@ void X86_64Assembler::vdivpd(XmmRegister dst, XmmRegister src1, XmmRegister src2
 
 
 void X86_64Assembler::movdqa(XmmRegister dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovdqa(dst, src);
     return;
   }
@@ -1173,34 +1335,32 @@ void X86_64Assembler::movdqa(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
-/** VEX.128.66.0F.WIG 6F /r VMOVDQA xmm1, xmm2 */
+/* VEX.128.66.0F.WIG 6F/7F /r VMOVDQA xmm1, xmm2
+   VEX.256.66.0F.WIG 6F/7F /r VMOVDQA ymm1, ymm2 */
 void X86_64Assembler::vmovdqa(XmmRegister dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = true;
+  uint8_t vex_len = GetEncodedVexLen(dst);
 
   // Instruction VEX Prefix
   if (src.NeedsRex() && dst.NeedsRex()) {
     is_twobyte_form = false;
   }
-  bool load = dst.NeedsRex();
+  bool store = (src.NeedsRex() && !dst.NeedsRex());
+  bool load = !store;
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     bool rex_bit = load ? dst.NeedsRex() : src.NeedsRex();
-    ByteOne = EmitVexPrefixByteOne(rex_bit,
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(rex_bit, vvvv_reg, vex_len, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
                                    src.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, vex_len, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1208,13 +1368,13 @@ void X86_64Assembler::vmovdqa(XmmRegister dst, XmmRegister src) {
     EmitUint8(ByteTwo);
   }
   // Instruction Opcode
-  if (is_twobyte_form && !load) {
+  if (store) {
     EmitUint8(0x7F);
   } else {
     EmitUint8(0x6F);
   }
   // Instruction Operands
-  if (is_twobyte_form && !load) {
+  if (store) {
     EmitXmmRegisterOperand(src.LowBits(), dst);
   } else {
     EmitXmmRegisterOperand(dst.LowBits(), src);
@@ -1222,7 +1382,7 @@ void X86_64Assembler::vmovdqa(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::movdqa(XmmRegister dst, const Address& src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovdqa(dst, src);
     return;
   }
@@ -1234,12 +1394,14 @@ void X86_64Assembler::movdqa(XmmRegister dst, const Address& src) {
   EmitOperand(dst.LowBits(), src);
 }
 
-/** VEX.128.66.0F.WIG 6F /r VMOVDQA xmm1, m128 */
+/* VEX.128.66.0F.WIG 6F /r VMOVDQA xmm1, m128
+   VEX.256.66.0F.WIG 6F /r VMOVDQA ymm1, m256 */
 void X86_64Assembler::vmovdqa(XmmRegister dst, const Address& src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t vex_len = GetEncodedVexLen(dst);
   // Instruction VEX Prefix
-  EmitVexPrefixForAddress(src, dst.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_66);
+  EmitVexPrefixForAddress(src, dst.NeedsRex(), vex_len, SET_VEX_PP_66);
   // Instruction Opcode
   EmitUint8(0x6F);
   // Instruction Operands
@@ -1247,7 +1409,7 @@ void X86_64Assembler::vmovdqa(XmmRegister dst, const Address& src) {
 }
 
 void X86_64Assembler::movdqu(XmmRegister dst, const Address& src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovdqu(dst, src);
     return;
   }
@@ -1259,13 +1421,14 @@ void X86_64Assembler::movdqu(XmmRegister dst, const Address& src) {
   EmitOperand(dst.LowBits(), src);
 }
 
-/** VEX.128.F3.0F.WIG 6F /r VMOVDQU xmm1, m128
-Load Unaligned */
+/* VEX.128.F3.0F.WIG 6F /r VMOVDQU xmm1, m128
+   VEX.256.F3.0F.WIG 6F /r VMOVDQU ymm1, m256 */
 void X86_64Assembler::vmovdqu(XmmRegister dst, const Address& src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t vex_len = GetEncodedVexLen(dst);
   // Instruction VEX Prefix
-  EmitVexPrefixForAddress(src, dst.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_F3);
+  EmitVexPrefixForAddress(src, dst.NeedsRex(), vex_len, SET_VEX_PP_F3);
   // Instruction Opcode
   EmitUint8(0x6F);
   // Instruction Operands
@@ -1273,7 +1436,7 @@ void X86_64Assembler::vmovdqu(XmmRegister dst, const Address& src) {
 }
 
 void X86_64Assembler::movdqa(const Address& dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (src.IsYMM()) {
     vmovdqa(dst, src);
     return;
   }
@@ -1285,12 +1448,14 @@ void X86_64Assembler::movdqa(const Address& dst, XmmRegister src) {
   EmitOperand(src.LowBits(), dst);
 }
 
-/** VEX.128.66.0F.WIG 7F /r VMOVDQA m128, xmm1 */
+/* VEX.128.66.0F.WIG 7F /r VMOVDQA m128, xmm1
+   VEX.256.66.0F.WIG 7F /r VMOVDQA m256, ymm1 */
 void X86_64Assembler::vmovdqa(const Address& dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t vex_len = GetEncodedVexLen(src);
   // Instruction VEX Prefix
-  EmitVexPrefixForAddress(dst, src.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_66);
+  EmitVexPrefixForAddress(dst, src.NeedsRex(), vex_len, SET_VEX_PP_66);
   // Instruction Opcode
   EmitUint8(0x7F);
   // Instruction Operands
@@ -1298,7 +1463,7 @@ void X86_64Assembler::vmovdqa(const Address& dst, XmmRegister src) {
 }
 
 void X86_64Assembler::movdqu(const Address& dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (src.IsYMM()) {
     vmovdqu(dst, src);
     return;
   }
@@ -1310,12 +1475,14 @@ void X86_64Assembler::movdqu(const Address& dst, XmmRegister src) {
   EmitOperand(src.LowBits(), dst);
 }
 
-/** VEX.128.F3.0F.WIG 7F /r VMOVDQU m128, xmm1 */
+/* VEX.128.F3.0F.WIG 7F /r VMOVDQU m128, xmm1
+   VEX.256.F3.0F.WIG 7F /r VMOVDQU m256, ymm1 */
 void X86_64Assembler::vmovdqu(const Address& dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t vex_len = GetEncodedVexLen(src);
   // Instruction VEX Prefix
-  EmitVexPrefixForAddress(dst, src.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_F3);
+  EmitVexPrefixForAddress(dst, src.NeedsRex(), vex_len, SET_VEX_PP_F3);
   // Instruction Opcode
   EmitUint8(0x7F);
   // Instruction Operands
@@ -1323,6 +1490,10 @@ void X86_64Assembler::vmovdqu(const Address& dst, XmmRegister src) {
 }
 
 void X86_64Assembler::paddb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1333,12 +1504,17 @@ void X86_64Assembler::paddb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vpaddb(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   EmitVecArithAndLogicalOperation(
       dst, add_left, add_right, /*opcode=*/ 0xFC, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 
 void X86_64Assembler::psubb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1349,11 +1525,16 @@ void X86_64Assembler::psubb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vpsubb(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   EmitVecArithAndLogicalOperation(dst, add_left, add_right, /*opcode=*/ 0xF8, SET_VEX_PP_66);
 }
 
 
 void X86_64Assembler::paddw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1363,12 +1544,17 @@ void X86_64Assembler::paddw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpaddw(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   EmitVecArithAndLogicalOperation(
       dst, add_left, add_right, /*opcode=*/ 0xFD, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 
 void X86_64Assembler::psubw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1378,11 +1564,16 @@ void X86_64Assembler::psubw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpsubw(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   EmitVecArithAndLogicalOperation(dst, add_left, add_right, /*opcode=*/ 0xF9, SET_VEX_PP_66);
 }
 
 
 void X86_64Assembler::pmullw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmullw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1392,11 +1583,16 @@ void X86_64Assembler::pmullw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpmullw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   EmitVecArithAndLogicalOperation(
       dst, src1, src2, /*opcode=*/ 0xD5, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 void X86_64Assembler::paddd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1406,11 +1602,16 @@ void X86_64Assembler::paddd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpaddd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   EmitVecArithAndLogicalOperation(
       dst, add_left, add_right, /*opcode=*/ 0xFE, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 void X86_64Assembler::psubd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1421,6 +1622,10 @@ void X86_64Assembler::psubd(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::pmulld(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmulld(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1431,9 +1636,10 @@ void X86_64Assembler::pmulld(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpmulld(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  uint8_t vex_len = GetEncodedVexLen(dst);
   ByteZero = EmitVexPrefixByteZero(/*is_twobyte_form*/ false);
   X86_64ManagedRegister vvvv_reg =
       X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
@@ -1441,7 +1647,7 @@ void X86_64Assembler::vpmulld(XmmRegister dst, XmmRegister src1, XmmRegister src
                                    /*X=*/ false,
                                    src2.NeedsRex(),
                                    SET_VEX_M_0F_38);
-  ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+  ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, vex_len, SET_VEX_PP_66);
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
   EmitUint8(ByteTwo);
@@ -1450,6 +1656,10 @@ void X86_64Assembler::vpmulld(XmmRegister dst, XmmRegister src1, XmmRegister src
 }
 
 void X86_64Assembler::paddq(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddq(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1460,12 +1670,17 @@ void X86_64Assembler::paddq(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vpaddq(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   EmitVecArithAndLogicalOperation(
       dst, add_left, add_right, /*opcode=*/ 0xD4, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 
 void X86_64Assembler::psubq(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubq(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1475,11 +1690,16 @@ void X86_64Assembler::psubq(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpsubq(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   EmitVecArithAndLogicalOperation(dst, add_left, add_right, /*opcode=*/ 0xFB, SET_VEX_PP_66);
 }
 
 
 void X86_64Assembler::paddusb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddusb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1490,6 +1710,10 @@ void X86_64Assembler::paddusb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::paddsb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddsb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1500,6 +1724,10 @@ void X86_64Assembler::paddsb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::paddusw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddusw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1510,6 +1738,10 @@ void X86_64Assembler::paddusw(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::paddsw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddsw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1520,6 +1752,10 @@ void X86_64Assembler::paddsw(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::psubusb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubusb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1530,6 +1766,10 @@ void X86_64Assembler::psubusb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::psubsb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubsb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1540,11 +1780,16 @@ void X86_64Assembler::psubsb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vpsubd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   EmitVecArithAndLogicalOperation(dst, add_left, add_right, /*opcode=*/ 0xFA, SET_VEX_PP_66);
 }
 
 
 void X86_64Assembler::psubusw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubusw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1555,6 +1800,10 @@ void X86_64Assembler::psubusw(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::psubsw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubsw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1563,6 +1812,65 @@ void X86_64Assembler::psubsw(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+/* VEX.128.66.0F.WIG DC /r VPADDUSB xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG DC /r VPADDUSB ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpaddusb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0xDC, SET_VEX_PP_66, /*is_commutative=*/ true);
+}
+
+/* VEX.128.66.0F.WIG EC /r VPADDSB xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG EC /r VPADDSB ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpaddsb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0xEC, SET_VEX_PP_66, /*is_commutative=*/ true);
+}
+
+/* VEX.128.66.0F.WIG DD /r VPADDUSW xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG DD /r VPADDUSW ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpaddusw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0xDD, SET_VEX_PP_66, /*is_commutative=*/ true);
+}
+
+/* VEX.128.66.0F.WIG ED /r VPADDSW xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG ED /r VPADDSW ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpaddsw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0xED, SET_VEX_PP_66, /*is_commutative=*/ true);
+}
+
+/* VEX.128.66.0F.WIG D8 /r VPSUBUSB xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG D8 /r VPSUBUSB ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpsubusb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, /*opcode=*/ 0xD8, SET_VEX_PP_66);
+}
+
+/* VEX.128.66.0F.WIG E8 /r VPSUBSB xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG E8 /r VPSUBSB ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpsubsb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, /*opcode=*/ 0xE8, SET_VEX_PP_66);
+}
+
+/* VEX.128.66.0F.WIG D9 /r VPSUBUSW xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG D9 /r VPSUBUSW ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpsubusw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, /*opcode=*/ 0xD9, SET_VEX_PP_66);
+}
+
+/* VEX.128.66.0F.WIG E9 /r VPSUBSW xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG E9 /r VPSUBSW ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpsubsw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, /*opcode=*/ 0xE9, SET_VEX_PP_66);
+}
 
 void X86_64Assembler::cvtsi2ss(XmmRegister dst, CpuRegister src) {
   cvtsi2ss(dst, src, false);
@@ -1735,6 +2043,10 @@ void X86_64Assembler::cvtsd2ss(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::cvtdq2ps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vcvtdq2ps(dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -1742,6 +2054,33 @@ void X86_64Assembler::cvtdq2ps(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+/* VEX.128.0F.WIG 5B /r VCVTDQ2PS xmm1, xmm2/m128
+   VEX.256.0F.WIG 5B /r VCVTDQ2PS ymm1, ymm2/m256 */
+void X86_64Assembler::vcvtdq2ps(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t vex_len = GetEncodedVexLen(dst);
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  bool is_twobyte_form = !src.NeedsRex();
+  uint8_t byte_zero = 0x00, byte_one = 0x00, byte_two = 0x00;
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, vex_len, SET_VEX_PP_NONE);
+  } else {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                    /*X=*/false,
+                                    src.NeedsRex(),
+                                    SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, vex_len, SET_VEX_PP_NONE);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+  EmitUint8(0x5B);
+  EmitXmmRegisterOperand(dst.LowBits(), src);
+}
 
 void X86_64Assembler::cvtdq2pd(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -1884,6 +2223,10 @@ void X86_64Assembler::xorpd(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::xorpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vxorpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1903,6 +2246,10 @@ void X86_64Assembler::xorps(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::xorps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vxorps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -1911,6 +2258,10 @@ void X86_64Assembler::xorps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pxor(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpxor(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1919,19 +2270,23 @@ void X86_64Assembler::pxor(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
-/* VEX.128.66.0F.WIG EF /r VPXOR xmm1, xmm2, xmm3/m128 */
+/* VEX.128.66.0F.WIG EF /r VPXOR xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG EF /r VPXOR ymm1, ymm2, ymm3/m256 */
 void X86_64Assembler::vpxor(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   EmitVecArithAndLogicalOperation(
       dst, src1, src2, /*opcode=*/ 0xEF, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
-/* VEX.128.0F.WIG 57 /r VXORPS xmm1,xmm2, xmm3/m128 */
+/* VEX.128.0F.WIG 57 /r VXORPS xmm1,xmm2, xmm3/m128
+   VEX.256.0F.WIG 57 /r VXORPS ymm1,ymm2, ymm3/m256 */
 void X86_64Assembler::vxorps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
   EmitVecArithAndLogicalOperation(
       dst, src1, src2, /*opcode=*/ 0x57, SET_VEX_PP_NONE, /*is_commutative=*/ true);
 }
 
-/* VEX.128.66.0F.WIG 57 /r VXORPD xmm1,xmm2, xmm3/m128 */
+/* VEX.128.66.0F.WIG 57 /r VXORPD xmm1,xmm2, xmm3/m128
+   VEX.256.66.0F.WIG 57 /r VXORPD ymm1,ymm2, ymm3/m256 */
 void X86_64Assembler::vxorpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
   EmitVecArithAndLogicalOperation(
       dst, src1, src2, /*opcode=*/ 0x57, SET_VEX_PP_66, /*is_commutative=*/ true);
@@ -1947,6 +2302,10 @@ void X86_64Assembler::andpd(XmmRegister dst, const Address& src) {
 }
 
 void X86_64Assembler::andpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vandpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1956,6 +2315,10 @@ void X86_64Assembler::andpd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::andps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vandps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -1964,6 +2327,10 @@ void X86_64Assembler::andps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pand(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpand(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1972,19 +2339,23 @@ void X86_64Assembler::pand(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
-/* VEX.128.66.0F.WIG DB /r VPAND xmm1, xmm2, xmm3/m128 */
+/* VEX.128.66.0F.WIG DB /r VPAND xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG DB /r VPAND ymm1, ymm2, ymm3/m256 */
 void X86_64Assembler::vpand(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   EmitVecArithAndLogicalOperation(
       dst, src1, src2, /*opcode=*/ 0xDB, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
-/* VEX.128.0F 54 /r VANDPS xmm1,xmm2, xmm3/m128 */
+/* VEX.128.0F 54 /r VANDPS xmm1, xmm2, xmm3/m128
+   VEX.256.0F 54 /r VANDPS ymm1, ymm2, ymm3/m256 */
 void X86_64Assembler::vandps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
   EmitVecArithAndLogicalOperation(
       dst, src1, src2, /*opcode=*/ 0x54, SET_VEX_PP_NONE, /*is_commutative=*/ true);
 }
 
-/* VEX.128.66.0F 54 /r VANDPD xmm1, xmm2, xmm3/m128 */
+/* VEX.128.66.0F 54 /r VANDPD xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F 54 /r VANDPD ymm1, ymm2, ymm3/m256 */
 void X86_64Assembler::vandpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
   EmitVecArithAndLogicalOperation(
       dst, src1, src2, /*opcode=*/ 0x54, SET_VEX_PP_66, /*is_commutative=*/ true);
@@ -2010,6 +2381,10 @@ void X86_64Assembler::andn(CpuRegister dst, CpuRegister src1, CpuRegister src2)
 }
 
 void X86_64Assembler::andnpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vandnpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2019,6 +2394,10 @@ void X86_64Assembler::andnpd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::andnps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vandnps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -2027,6 +2406,10 @@ void X86_64Assembler::andnps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pandn(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpandn(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2035,22 +2418,30 @@ void X86_64Assembler::pandn(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
-/* VEX.128.66.0F.WIG DF /r VPANDN xmm1, xmm2, xmm3/m128 */
+/* VEX.128.66.0F.WIG DF /r VPANDN xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG DF /r VPANDN ymm1, ymm2, ymm3/m256 */
 void X86_64Assembler::vpandn(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   EmitVecArithAndLogicalOperation(dst, src1, src2, /*opcode=*/ 0xDF, SET_VEX_PP_66);
 }
 
-/* VEX.128.0F 55 /r VANDNPS xmm1, xmm2, xmm3/m128 */
+/* VEX.128.0F 55 /r VANDNPS xmm1, xmm2, xmm3/m128
+   VEX.256.0F 55 /r VANDNPS ymm1, ymm2, ymm3/m256 */
 void X86_64Assembler::vandnps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
   EmitVecArithAndLogicalOperation(dst, src1, src2, /*opcode=*/ 0x55, SET_VEX_PP_NONE);
 }
 
-/* VEX.128.66.0F 55 /r VANDNPD xmm1, xmm2, xmm3/m128 */
+/* VEX.128.66.0F 55 /r VANDNPD xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F 55 /r VANDNPD ymm1, ymm2, ymm3/m256 */
 void X86_64Assembler::vandnpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
   EmitVecArithAndLogicalOperation(dst, src1, src2, /*opcode=*/ 0x55, SET_VEX_PP_66);
 }
 
 void X86_64Assembler::orpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vorpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2060,6 +2451,10 @@ void X86_64Assembler::orpd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::orps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vorps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -2068,6 +2463,10 @@ void X86_64Assembler::orps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::por(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpor(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2076,25 +2475,33 @@ void X86_64Assembler::por(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
-/* VEX.128.66.0F.WIG EB /r VPOR xmm1, xmm2, xmm3/m128 */
+/* VEX.128.66.0F.WIG EB /r VPOR xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG EB /r VPOR ymm1, ymm2, ymm3/m256 */
 void X86_64Assembler::vpor(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   EmitVecArithAndLogicalOperation(
       dst, src1, src2, /*opcode=*/ 0xEB, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
-/* VEX.128.0F 56 /r VORPS xmm1,xmm2, xmm3/m128 */
+/* VEX.128.0F 56 /r VORPS xmm1, xmm2, xmm3/m128
+   VEX.256.0F 56 /r VORPS ymm1, ymm2, ymm3/m256 */
 void X86_64Assembler::vorps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
   EmitVecArithAndLogicalOperation(
       dst, src1, src2, /*opcode=*/ 0x56, SET_VEX_PP_NONE, /*is_commutative=*/ true);
 }
 
-/* VEX.128.66.0F 56 /r VORPD xmm1,xmm2, xmm3/m128 */
+/* VEX.128.66.0F 56 /r VORPD xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F 56 /r VORPD ymm1, ymm2, ymm3/m256 */
 void X86_64Assembler::vorpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
   EmitVecArithAndLogicalOperation(
       dst, src1, src2, /*opcode=*/ 0x56, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 void X86_64Assembler::pavgb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpavgb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2104,6 +2511,10 @@ void X86_64Assembler::pavgb(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pavgw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpavgw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2112,6 +2523,22 @@ void X86_64Assembler::pavgw(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+/* VEX.128.66.0F.WIG E0 /r VPAVGB xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG E0 /r VPAVGB ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpavgb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0xE0, SET_VEX_PP_66, /*is_commutative=*/ true);
+}
+
+/* VEX.128.66.0F.WIG E3 /r VPAVGW xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG E3 /r VPAVGW ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpavgw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0xE3, SET_VEX_PP_66, /*is_commutative=*/ true);
+}
+
 void X86_64Assembler::psadbw(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
@@ -2122,6 +2549,10 @@ void X86_64Assembler::psadbw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pmaddwd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmaddwd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2131,6 +2562,7 @@ void X86_64Assembler::pmaddwd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpmaddwd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   EmitVecArithAndLogicalOperation(
       dst, src1, src2, /*opcode=*/ 0xF5, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
@@ -2146,6 +2578,10 @@ void X86_64Assembler::phaddw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::phaddd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vphaddd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2155,6 +2591,31 @@ void X86_64Assembler::phaddd(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+/* VEX.128.66.0F38.WIG 02 /r VPHADDD xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F38.WIG 02 /r VPHADDD ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vphaddd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  uint8_t byte_zero, byte_one, byte_two;
+  uint8_t vex_len = GetEncodedVexLen(dst);
+  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(/*is_twobyte_form*/ false);
+  byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                  /*X=*/false,
+                                  src2.NeedsRex(),
+                                  SET_VEX_M_0F_38);
+  byte_two = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, vex_len, SET_VEX_PP_66);
+
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  EmitUint8(byte_two);
+  // Instruction Opcode
+  EmitUint8(0x02);
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
+}
+
 void X86_64Assembler::haddps(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF2);
@@ -2212,6 +2673,10 @@ void X86_64Assembler::hsubpd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pminsb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpminsb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2222,6 +2687,10 @@ void X86_64Assembler::pminsb(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pmaxsb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmaxsb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2232,6 +2701,10 @@ void X86_64Assembler::pmaxsb(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pminsw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpminsw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2241,6 +2714,10 @@ void X86_64Assembler::pminsw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pmaxsw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmaxsw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2250,6 +2727,10 @@ void X86_64Assembler::pmaxsw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pminsd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpminsd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2260,6 +2741,10 @@ void X86_64Assembler::pminsd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pmaxsd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmaxsd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2270,6 +2755,10 @@ void X86_64Assembler::pmaxsd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pminub(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpminub(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2279,6 +2768,10 @@ void X86_64Assembler::pminub(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pmaxub(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmaxub(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2288,6 +2781,10 @@ void X86_64Assembler::pmaxub(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pminuw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpminuw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2298,6 +2795,10 @@ void X86_64Assembler::pminuw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pmaxuw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmaxuw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2308,6 +2809,10 @@ void X86_64Assembler::pmaxuw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pminud(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpminud(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2318,6 +2823,10 @@ void X86_64Assembler::pminud(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pmaxud(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmaxud(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2328,6 +2837,10 @@ void X86_64Assembler::pmaxud(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::minps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vminps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -2336,6 +2849,10 @@ void X86_64Assembler::minps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::maxps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vmaxps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -2344,6 +2861,10 @@ void X86_64Assembler::maxps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::minpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vminpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2353,6 +2874,10 @@ void X86_64Assembler::minpd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::maxpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vmaxpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2362,6 +2887,10 @@ void X86_64Assembler::maxpd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pcmpeqb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpcmpeqb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2398,6 +2927,14 @@ void X86_64Assembler::pcmpeqq(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+/* VEX.128.66.0F.WIG 74 /r VPCMPEQB xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG 74 /r VPCMPEQB ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpcmpeqb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0x74, SET_VEX_PP_66, /*is_commutative=*/ true);
+}
+
 void X86_64Assembler::pcmpgtb(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
@@ -2426,6 +2963,10 @@ void X86_64Assembler::pcmpgtd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pcmpgtq(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpcmpgtq(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2435,6 +2976,13 @@ void X86_64Assembler::pcmpgtq(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+/* VEX.128.66.0F38.WIG 37 /r VPCMPGTQ xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F38.WIG 37 /r VPCMPGTQ ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpcmpgtq(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte=*/ true, /*opcode=*/ 0x37);
+}
+
 void X86_64Assembler::shufpd(XmmRegister dst, XmmRegister src, const Immediate& imm) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
@@ -2468,6 +3016,10 @@ void X86_64Assembler::pshufd(XmmRegister dst, XmmRegister src, const Immediate&
 
 
 void X86_64Assembler::punpcklbw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpunpcklbw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2549,6 +3101,10 @@ void X86_64Assembler::punpckhqdq(XmmRegister dst, XmmRegister src) {
 
 void X86_64Assembler::psllw(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpsllw(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -2561,6 +3117,10 @@ void X86_64Assembler::psllw(XmmRegister reg, const Immediate& shift_count) {
 
 void X86_64Assembler::pslld(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpslld(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -2573,6 +3133,10 @@ void X86_64Assembler::pslld(XmmRegister reg, const Immediate& shift_count) {
 
 void X86_64Assembler::psllq(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpsllq(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -2582,9 +3146,33 @@ void X86_64Assembler::psllq(XmmRegister reg, const Immediate& shift_count) {
   EmitUint8(shift_count.value());
 }
 
+/* VEX.128.66.0F.WIG 71 /6 ib VPSLLW xmm1, xmm2, imm8
+   VEX.256.66.0F.WIG 71 /6 ib VPSLLW ymm1, ymm2, imm8 */
+void X86_64Assembler::vpsllw(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x71, 6);
+}
+
+/* VEX.128.66.0F.WIG 72 /6 ib VPSLLD xmm1, xmm2, imm8
+   VEX.256.66.0F.WIG 72 /6 ib VPSLLD ymm1, ymm2, imm8 */
+void X86_64Assembler::vpslld(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x72, 6);
+}
+
+/* VEX.128.66.0F.WIG 73 /6 ib VPSLLQ xmm1, xmm2, imm8
+   VEX.256.66.0F.WIG 73 /6 ib VPSLLQ ymm1, ymm2, imm8 */
+void X86_64Assembler::vpsllq(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x73, 6);
+}
 
 void X86_64Assembler::psraw(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpsraw(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -2597,6 +3185,10 @@ void X86_64Assembler::psraw(XmmRegister reg, const Immediate& shift_count) {
 
 void X86_64Assembler::psrad(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpsrad(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -2606,9 +3198,26 @@ void X86_64Assembler::psrad(XmmRegister reg, const Immediate& shift_count) {
   EmitUint8(shift_count.value());
 }
 
+/* VEX.128.66.0F.WIG 71 /4 ib VPSRAW xmm1, xmm2, imm8
+   VEX.256.66.0F.WIG 71 /4 ib VPSRAW ymm1, ymm2, imm8 */
+void X86_64Assembler::vpsraw(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x71, 4);
+}
+
+/* VEX.128.66.0F.WIG 72 /4 ib VPSRAD xmm1, xmm2, imm8
+   VEX.256.66.0F.WIG 72 /4 ib VPSRAD ymm1, ymm2, imm8 */
+void X86_64Assembler::vpsrad(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x72, 4);
+}
 
 void X86_64Assembler::psrlw(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpsrlw(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -2621,6 +3230,10 @@ void X86_64Assembler::psrlw(XmmRegister reg, const Immediate& shift_count) {
 
 void X86_64Assembler::psrld(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpsrld(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -2633,6 +3246,10 @@ void X86_64Assembler::psrld(XmmRegister reg, const Immediate& shift_count) {
 
 void X86_64Assembler::psrlq(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpsrlq(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -2654,6 +3271,340 @@ void X86_64Assembler::psrldq(XmmRegister reg, const Immediate& shift_count) {
   EmitUint8(shift_count.value());
 }
 
+/* VEX.128.66.0F.WIG 71 /2 ib VPSRLW xmm1, xmm2, imm8
+   VEX.256.66.0F.WIG 71 /2 ib VPSRLW ymm1, ymm2, imm8 */
+void X86_64Assembler::vpsrlw(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x71, 2);
+}
+
+/* VEX.128.66.0F.WIG 72 /2 ib VPSRLD xmm1, xmm2, imm8
+   VEX.256.66.0F.WIG 72 /2 ib VPSRLD ymm1, ymm2, imm8 */
+void X86_64Assembler::vpsrld(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x72, 2);
+}
+
+/* VEX.128.66.0F.WIG 73 /2 ib VPSRLQ xmm1, xmm2, imm8
+   VEX.256.66.0F.WIG 73 /2 ib VPSRLQ ymm1, ymm2, imm8 */
+void X86_64Assembler::vpsrlq(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x73, 2);
+}
+
+/* VEX.256.66.0F3A.W1 01 /r ib VPERMPD ymm1, ymm2/m256, imm8 */
+void X86_64Assembler::vpermpd(XmmRegister dst, XmmRegister src, const Immediate& indices) {
+  DCHECK(CpuHasAVX2FeatureFlag());
+  DCHECK(dst.IsYMM());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t vex_len = GetEncodedVexLen(dst);
+  uint8_t byte_zero = EmitVexPrefixByteZero(/*is_twobyte_form*/ false);
+  uint8_t byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                          /*X=*/false,
+                                          src.NeedsRex(),
+                                          SET_VEX_M_0F_3A);
+  uint8_t byte_two = EmitVexPrefixByteTwo(/*W=*/true, vex_len, SET_VEX_PP_66);
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  EmitUint8(byte_two);
+  EmitUint8(0x01);
+  EmitXmmRegisterOperand(dst.LowBits(), src);
+  EmitUint8(indices.value());
+}
+
+/* VEX.128.66.0F38 38 /r VPMINSB xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F38 38 /r VPMINSB ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpminsb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  // Although commutative, there is no advantage in encoding. We end up emitting 3-byte VEX prefix,
+  // if any of the source registers needs rex. So, ignore is_commutative
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte=*/ true, /*opcode=*/ 0x38);
+}
+
+/* VEX.128.66.0F38.WIG 3C /r VPMAXSB xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F38.WIG 3C /r VPMAXSB ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpmaxsb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  // Although commutative, there is no advantage in encoding. We end up emitting 3-byte VEX prefix,
+  // if any of the source registers needs rex. So, ignore is_commutative
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte=*/ true, /*opcode=*/ 0x3C);
+}
+
+/* VEX.128.66.0F EA /r VPMINSW xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F EA /r VPMINSW ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpminsw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst,
+                         src1,
+                         src2,
+                         SET_VEX_PP_66,
+                         /*is_vex_3byte=*/ false,
+                         /*opcode=*/ 0xEA,
+                         /*is_commutative=*/ true);
+}
+
+/* VEX.128.66.0F.WIG EE /r VPMAXSW xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG EE /r VPMAXSW ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpmaxsw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst,
+                         src1,
+                         src2,
+                         SET_VEX_PP_66,
+                         /*is_vex_3byte=*/ false,
+                         /*opcode=*/ 0xEE,
+                         /*is_commutative=*/ true);
+}
+
+/* VEX.128.66.0F38.WIG 39 /r VPMINSD xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F38.WIG 39 /r VPMINSD ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpminsd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  // Although commutative, there is no advantage in encoding. We end up emitting 3-byte VEX prefix,
+  // if any of the source registers needs rex. So, ignore is_commutative
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte=*/ true, /*opcode=*/ 0x39);
+}
+
+/* VEX.128.66.0F38.WIG 3D /r VPMAXSD xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F38.WIG 3D /r VPMAXSD ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpmaxsd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  // Although commutative, there is no advantage in encoding. We end up emitting 3-byte VEX prefix,
+  // if any of the source registers needs rex. So, ignore is_commutative
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte=*/ true, /*opcode=*/ 0x3D);
+}
+
+/* VEX.128.66.0F DA /r VPMINUB xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F DA /r VPMINUB ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpminub(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst,
+                         src1,
+                         src2,
+                         SET_VEX_PP_66,
+                         /*is_vex_3byte=*/ false,
+                         /*opcode=*/ 0xDA,
+                         /*is_commutative=*/ true);
+}
+
+/* VEX.128.66.0F DE /r VPMAXUB xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F DE /r VPMAXUB ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpmaxub(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst,
+                         src1,
+                         src2,
+                         SET_VEX_PP_66,
+                         /*is_vex_3byte=*/ false,
+                         /*opcode=*/ 0xDE,
+                         /*is_commutative=*/ true);
+}
+
+/* VEX.128.66.0F38 3A/r VPMINUW xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F38 3A/r VPMINUW ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpminuw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  // Although commutative, there is no advantage in encoding. We end up emitting 3-byte VEX prefix,
+  // if any of the source registers needs rex. So, ignore is_commutative
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte=*/ true, /*opcode=*/ 0x3A);
+}
+
+/* VEX.128.66.0F38 3E/r VPMAXUW xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F38 3E/r VPMAXUW ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpmaxuw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  // Although commutative, there is no advantage in encoding. We end up emitting 3-byte VEX prefix,
+  // if any of the source registers needs rex. So, ignore is_commutative
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte=*/ true, /*opcode=*/ 0x3E);
+}
+
+/* VEX.128.66.0F38.WIG 3B /r VPMINUD xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F38.WIG 3B /r VPMINUD ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpminud(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  // Although commutative, there is no advantage in encoding. We end up emitting 3-byte VEX prefix,
+  // if any of the source registers needs rex. So, ignore is_commutative
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte=*/ true, /*opcode=*/ 0x3B);
+}
+
+/* VEX.128.66.0F38.WIG 3F /r VPMAXUD xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F38.WIG 3F /r VPMAXUD ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpmaxud(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  // Although commutative, there is no advantage in encoding. We end up emitting 3-byte VEX prefix,
+  // if any of the source registers needs rex. So, ignore is_commutative
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte=*/ true, /*opcode=*/ 0x3F);
+}
+
+/* VEX.128.0F.WIG 5D /r VMINPS xmm1, xmm2, xmm3/m128
+   VEX.256.0F.WIG 5D /r VMINPS ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vminps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  // Not commutative, as this is FP and has special handling for 0.0/NaN
+  EmitVecMinMaxOperation(
+      dst, src1, src2, SET_VEX_PP_NONE, /*is_vex_3byte=*/ false, /*opcode=*/ 0x5D);
+}
+
+/* VEX.128.0F.WIG 5F /r VMAXPS xmm1, xmm2, xmm3/m128
+   VEX.256.0F.WIG 5F /r VMAXPS ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vmaxps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  // Not commutative, as this is FP and has special handling for 0.0/NaN
+  EmitVecMinMaxOperation(
+      dst, src1, src2, SET_VEX_PP_NONE, /*is_vex_3byte=*/ false, /*opcode=*/ 0x5F);
+}
+
+/* VEX.128.66.0F.WIG 5D /r VMINPD xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG 5D /r VMINPD ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vminpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  // Not commutative, as this is FP and has special handling for 0.0/NaN
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte=*/ false, /*opcode=*/ 0x5D);
+}
+
+/* VEX.128.66.0F.WIG 5F /r VMAXPD xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG 5F /r VMAXPD ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vmaxpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  // Not commutative, as this is FP and has special handling for 0.0/NaN
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte=*/ false, /*opcode=*/ 0x5F);
+}
+
+/* VEX.128.66.0F38.W0 18 /r VBROADCASTSS xmm1, xmm2
+   VEX.256.66.0F38.W0 18 /r VBROADCASTSS ymm1, xmm2 */
+void X86_64Assembler::vbroadcastss(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag());
+  EmitVecBroadcastInstruction(dst, src, 0x18);
+}
+
+/* VEX.256.66.0F38.W0 19 /r VBROADCASTSD ymm1, xmm2 */
+void X86_64Assembler::vbroadcastsd(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag());
+  EmitVecBroadcastInstruction(dst, src, 0x19);
+}
+
+/* VEX.128.66.0F38.W0 78 /r VPBROADCASTB xmm1, xmm2/m8
+   VEX.256.66.0F38.W0 78 /r VPBROADCASTB ymm1, xmm2/m8 */
+void X86_64Assembler::vpbroadcastb(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag());
+  EmitVecBroadcastInstruction(dst, src, 0x78);
+}
+
+/* VEX.128.66.0F38.W0 79 /r VPBROADCASTW xmm1, xmm2/m16
+   VEX.256.66.0F38.W0 79 /r VPBROADCASTW ymm1, xmm2/m16 */
+void X86_64Assembler::vpbroadcastw(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag());
+  EmitVecBroadcastInstruction(dst, src, 0x79);
+}
+
+/* VEX.128.66.0F38.W0 58 /r VPBROADCASTD xmm1, xmm2/m32
+   VEX.256.66.0F38.W0 58 /r VPBROADCASTD ymm1, xmm2/m32 */
+void X86_64Assembler::vpbroadcastd(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag());
+  EmitVecBroadcastInstruction(dst, src, 0x58);
+}
+
+/* VEX.128.66.0F38.W0 59 /r VPBROADCASTQ xmm1, xmm2/m64
+   VEX.256.66.0F38.W0 59 /r VPBROADCASTQ ymm1, xmm2/m64 */
+void X86_64Assembler::vpbroadcastq(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag());
+  EmitVecBroadcastInstruction(dst, src, 0x59);
+}
+
+/* 66 0F 38 1C /r PABSB xmm1, xmm2/m128 */
+void X86_64Assembler::pabsb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpabsb(dst, src);
+    return;
+  }
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  EmitUint8(0x66);
+  EmitOptionalRex(false, false, dst.NeedsRex(), false, src.NeedsRex());
+  EmitUint8(0x0F);
+  EmitUint8(0x38);
+  EmitUint8(0x1C);
+  EmitRegisterOperand(dst.LowBits(), src.LowBits());
+}
+
+/* 66 0F 38 1D /r PABSW xmm1, xmm2/m128 */
+void X86_64Assembler::pabsw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpabsw(dst, src);
+    return;
+  }
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  EmitUint8(0x66);
+  EmitOptionalRex(false, false, dst.NeedsRex(), false, src.NeedsRex());
+  EmitUint8(0x0F);
+  EmitUint8(0x38);
+  EmitUint8(0x1D);
+  EmitRegisterOperand(dst.LowBits(), src.LowBits());
+}
+
+/* 66 0F 38 1E /r PABSD xmm1, xmm2/m128 */
+void X86_64Assembler::pabsd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpabsd(dst, src);
+    return;
+  }
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  EmitUint8(0x66);
+  EmitOptionalRex(false, false, dst.NeedsRex(), false, src.NeedsRex());
+  EmitUint8(0x0F);
+  EmitUint8(0x38);
+  EmitUint8(0x1E);
+  EmitRegisterOperand(dst.LowBits(), src.LowBits());
+}
+
+/* VEX.128.66.0F38.WIG 1C /r VPABSB xmm1, xmm2/m128
+   VEX.256.66.0F38.WIG 1C /r VPABSB ymm1, ymm2/m256 */
+void X86_64Assembler::vpabsb(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecBroadcastInstruction(dst, src, 0x1C);
+}
+
+/* VEX.128.66.0F38.WIG 1D /r VPABSW xmm1, xmm2/m128
+   VEX.256.66.0F38.WIG 1D /r VPABSW ymm1, ymm2/m256 */
+void X86_64Assembler::vpabsw(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecBroadcastInstruction(dst, src, 0x1D);
+}
+
+/* VEX.128.66.0F38.WIG 1E /r VPABSD xmm1, xmm2/m128
+   VEX.256.66.0F38.WIG 1E /r VPABSD ymm1, ymm2/m256 */
+void X86_64Assembler::vpabsd(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecBroadcastInstruction(dst, src, 0x1E);
+}
+
+/* VEX.128.66.0F.WIG 60/r VPUNPCKLBW xmm1, xmm2, xmm3/m128
+   VEX.256.66.0F.WIG 60/r VPUNPCKLBW ymm1, ymm2, ymm3/m256 */
+void X86_64Assembler::vpunpcklbw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = !src2.NeedsRex();
+  uint8_t vex_len = GetEncodedVexLen(dst);
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, vex_len, SET_VEX_PP_66);
+  } else {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                    /*X=*/false,
+                                    src2.NeedsRex(),
+                                    SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, vex_len, SET_VEX_PP_66);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+
+  // Instruction Opcode
+  EmitUint8(0x60);
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
+}
 
 void X86_64Assembler::fldl(const Address& src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -4236,6 +5187,14 @@ void X86_64Assembler::ud2() {
   EmitUint8(0x0B);
 }
 
+void X86_64Assembler::vzeroupper() {
+  DCHECK(CpuHasAVXFeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  EmitUint8(0xC5);
+  EmitUint8(0xF8);
+  EmitUint8(0x77);
+}
+
 void X86_64Assembler::LoadDoubleConstant(XmmRegister dst, double value) {
   // TODO: Need to have a code constants table.
   int64_t constant = bit_cast<int64_t, double>(value);
@@ -4515,6 +5474,10 @@ void X86_64Assembler::EmitRex64(CpuRegister dst, XmmRegister src) {
   EmitOptionalRex(false, true, dst.NeedsRex(), false, src.NeedsRex());
 }
 
+void X86_64Assembler::EmitRex64(XmmRegister dst, XmmRegister src) {
+  EmitOptionalRex(false, true, dst.NeedsRex(), false, src.NeedsRex());
+}
+
 void X86_64Assembler::EmitRex64(CpuRegister dst, const Operand& operand) {
   uint8_t rex = 0x48 | operand.rex();  // REX.W000
   if (dst.NeedsRex()) {
@@ -4723,7 +5686,9 @@ uint8_t X86_64Assembler::EmitVexPrefixByteTwo(bool W,
     vex_prefix |= SET_VEX_W;
   }
   // Bits[6:3] - 'vvvv' the source or dest register specifier
-  if (operand.IsXmmRegister()) {
+  if (operand.IsNoRegister()) {
+    vex_prefix |= 0x78;
+  } else if (operand.IsXmmRegister()) {
     XmmRegister vvvv = operand.AsXmmRegister();
     int inverted_reg = 15 - static_cast<int>(vvvv.AsFloatRegister());
     uint8_t reg = static_cast<uint8_t>(inverted_reg);
@@ -4745,25 +5710,8 @@ uint8_t X86_64Assembler::EmitVexPrefixByteTwo(bool W,
 uint8_t X86_64Assembler::EmitVexPrefixByteTwo(bool W,
                                               int SET_VEX_L,
                                               int SET_VEX_PP) {
-  // Vex Byte 2,
-  uint8_t vex_prefix = VEX_INIT;
-
-  /** Bit[7] This bits needs to be set to '1' with default value.
-  When using C4H form of VEX prefix, REX.W value is ignored */
-  if (W) {
-    vex_prefix |= SET_VEX_W;
-  }
-  /** Bits[6:3] - 'vvvv' the source or dest register specifier */
-  vex_prefix |= (0x0F << 3);
-  /** Bit[2] - "L" If VEX.L = 1 indicates 256-bit vector operation,
-  VEX.L = 0 indicates 128 bit vector operation */
-  vex_prefix |= SET_VEX_L;
-
-  // Bits[1:0] -  "pp"
-  if (SET_VEX_PP != SET_VEX_PP_NONE) {
-    vex_prefix |= SET_VEX_PP;
-  }
-  return vex_prefix;
+  X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+  return EmitVexPrefixByteTwo(W, vvvv_reg, SET_VEX_L, SET_VEX_PP);
 }
 
 void X86_64Assembler::EmitVecArithAndLogicalOperation(XmmRegister dst,
@@ -4775,26 +5723,199 @@ void X86_64Assembler::EmitVecArithAndLogicalOperation(XmmRegister dst,
   if (is_commutative && src2.NeedsRex() && !src1.NeedsRex()) {
     return EmitVecArithAndLogicalOperation(dst, src2, src1, opcode, vex_pp, is_commutative);
   }
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t vex_len = GetEncodedVexLen(dst);
   X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
   bool is_twobyte_form = !src2.NeedsRex();
   uint8_t byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
   uint8_t byte_one, byte_two;
   if (is_twobyte_form) {
-    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, vex_pp);
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, vex_len, vex_pp);
   } else {
     byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), /*X=*/ false, src2.NeedsRex(), SET_VEX_M_0F);
-    byte_two = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, vex_pp);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, vex_len, vex_pp);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+  EmitUint8(opcode);
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
+}
+
+void X86_64Assembler::EmitVecMoveCpuFpu(XmmRegister fp_reg,
+                                        CpuRegister cpu_reg,
+                                        bool is64bit,
+                                        uint8_t opcode) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = !is64bit && !cpu_reg.NeedsRex();
+
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(fp_reg.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+  } else {
+    byte_one = EmitVexPrefixByteOne(fp_reg.NeedsRex(),
+                                    /*X=*/false,
+                                    cpu_reg.NeedsRex(),
+                                    SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/is64bit, SET_VEX_L_128, SET_VEX_PP_66);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+  // Instruction Opcode
+  EmitUint8(opcode);
+
+  // Instruction Operands
+  EmitOperand(fp_reg.LowBits(), Operand(cpu_reg));
+}
+
+void X86_64Assembler::EmitVecMergeFPRegs(XmmRegister dst,
+                                         XmmRegister src1,
+                                         XmmRegister src2,
+                                         uint8_t vex_pp) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = !dst.NeedsRex() || !src2.NeedsRex();
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+  bool is_store = src2.NeedsRex() && !dst.NeedsRex();
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(
+        is_store ? src2.NeedsRex() : dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, vex_pp);
+  } else {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                    /*X=*/false,
+                                    src2.NeedsRex(),
+                                    SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, SET_VEX_L_128, vex_pp);
   }
   EmitUint8(byte_zero);
   EmitUint8(byte_one);
   if (!is_twobyte_form) {
     EmitUint8(byte_two);
   }
+
+  // Instruction Opcode
+  if (is_store) {
+    // Special opcode only when src2 needs rex
+    EmitUint8(0x11);
+    EmitXmmRegisterOperand(src2.LowBits(), dst);
+  } else {
+    EmitUint8(0x10);
+    EmitXmmRegisterOperand(dst.LowBits(), src2);
+  }
+}
+
+
+void X86_64Assembler::EmitVecMinMaxOperation(XmmRegister dst,
+                                             XmmRegister src1,
+                                             XmmRegister src2,
+                                             uint8_t vex_pp,
+                                             bool is_vex_3byte,
+                                             uint8_t opcode,
+                                             bool is_commutative) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = !is_vex_3byte;
+  uint8_t vex_len = GetEncodedVexLen(dst);
+
+  if (is_twobyte_form) {
+    if (is_commutative && src2.NeedsRex() && !src1.NeedsRex()) {
+      return EmitVecMinMaxOperation(dst, src2, src1, vex_pp, is_vex_3byte, opcode, is_commutative);
+    }
+    is_twobyte_form = !src2.NeedsRex();
+  }
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, vex_len, vex_pp);
+  } else {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                    /*X=*/false,
+                                    src2.NeedsRex(),
+                                    is_vex_3byte ? SET_VEX_M_0F_38 : SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, vex_len, vex_pp);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+  // Instruction Opcode
   EmitUint8(opcode);
+
+  // Instruction Operands
   EmitXmmRegisterOperand(dst.LowBits(), src2);
 }
 
+void X86_64Assembler::EmitVecBroadcastInstruction(XmmRegister dst,
+                                                  XmmRegister src,
+                                                  uint8_t opcode) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t byte_one = 0x00, byte_zero = 0x00, byte_two = 0x00;
+  uint8_t vex_len = GetEncodedVexLen(dst);
+  byte_zero = EmitVexPrefixByteZero(false /*is_twobyte_form */);
+  byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                  /*X=*/false,
+                                  src.NeedsRex(),
+                                  SET_VEX_M_0F_38);
+  byte_two = EmitVexPrefixByteTwo(/*W=*/false, vex_len, SET_VEX_PP_66);
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  EmitUint8(byte_two);
+  EmitUint8(opcode);
+  EmitXmmRegisterOperand(dst.LowBits(), src);
+}
+
+void X86_64Assembler::EmitVecShiftOperation(XmmRegister dst,
+                                            XmmRegister src,
+                                            const Immediate& shift_count,
+                                            uint8_t opcode,
+                                            uint8_t operand_byte) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = !src.NeedsRex();
+  uint8_t vex_len = GetEncodedVexLen(dst);
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(dst.AsFloatRegister());
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, vex_len, SET_VEX_PP_66);
+  } else {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                    /*X=*/false,
+                                    src.NeedsRex(),
+                                    SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, vex_len, SET_VEX_PP_66);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+  // Instruction Opcode
+  EmitUint8(opcode);
+
+  // Instruction Operands
+  EmitXmmRegisterOperand(operand_byte, src);
+  EmitUint8(shift_count.value());
+}
+
 }  // namespace x86_64
 }  // namespace art
diff --git a/compiler/utils/x86_64/assembler_x86_64.h b/compiler/utils/x86_64/assembler_x86_64.h
index 2c3b3c44ee..3a44d6389a 100644
--- a/compiler/utils/x86_64/assembler_x86_64.h
+++ b/compiler/utils/x86_64/assembler_x86_64.h
@@ -494,6 +494,9 @@ class X86_64Assembler final : public Assembler {
   void movss(XmmRegister dst, const Address& src);
   void movss(const Address& dst, XmmRegister src);
   void movss(XmmRegister dst, XmmRegister src);
+  void vmovss(XmmRegister dst, const Address& src);
+  void vmovss(const Address& dst, XmmRegister src);
+  void vmovss(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void movsxd(CpuRegister dst, CpuRegister src);
   void movsxd(CpuRegister dst, const Address& src);
@@ -503,6 +506,11 @@ class X86_64Assembler final : public Assembler {
   void movd(XmmRegister dst, CpuRegister src);
   void movd(CpuRegister dst, XmmRegister src);
 
+  void vmovq(XmmRegister dst, CpuRegister src);
+  void vmovq(CpuRegister dst, XmmRegister src);
+  void vmovd(XmmRegister dst, CpuRegister src);
+  void vmovd(CpuRegister dst, XmmRegister src);
+
   void addss(XmmRegister dst, XmmRegister src);
   void addss(XmmRegister dst, const Address& src);
   void subss(XmmRegister dst, XmmRegister src);
@@ -517,15 +525,10 @@ class X86_64Assembler final : public Assembler {
   void mulps(XmmRegister dst, XmmRegister src);
   void divps(XmmRegister dst, XmmRegister src);
 
-  void vmulps(XmmRegister dst, XmmRegister src1, XmmRegister src2);
-  void vmulpd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
-  void vdivps(XmmRegister dst, XmmRegister src1, XmmRegister src2);
-  void vdivpd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
-
   void vaddps(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
   void vsubps(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
-  void vsubpd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
-  void vaddpd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
+  void vmulps(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vdivps(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void vfmadd213ss(XmmRegister accumulator, XmmRegister left, XmmRegister right);
   void vfmadd213sd(XmmRegister accumulator, XmmRegister left, XmmRegister right);
@@ -545,6 +548,9 @@ class X86_64Assembler final : public Assembler {
   void movsd(XmmRegister dst, const Address& src);
   void movsd(const Address& dst, XmmRegister src);
   void movsd(XmmRegister dst, XmmRegister src);
+  void vmovsd(XmmRegister dst, const Address& src);
+  void vmovsd(const Address& dst, XmmRegister src);
+  void vmovsd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void addsd(XmmRegister dst, XmmRegister src);
   void addsd(XmmRegister dst, const Address& src);
@@ -560,6 +566,11 @@ class X86_64Assembler final : public Assembler {
   void mulpd(XmmRegister dst, XmmRegister src);
   void divpd(XmmRegister dst, XmmRegister src);
 
+  void vaddpd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
+  void vsubpd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
+  void vmulpd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vdivpd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+
   void movdqa(XmmRegister dst, XmmRegister src);     // move
   void movdqa(XmmRegister dst, const Address& src);  // load aligned
   void movdqu(XmmRegister dst, const Address& src);  // load unaligned
@@ -576,23 +587,23 @@ class X86_64Assembler final : public Assembler {
   void psubb(XmmRegister dst, XmmRegister src);
 
   void vpaddb(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
-  void vpaddw(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
+  void vpsubb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void paddw(XmmRegister dst, XmmRegister src);
   void psubw(XmmRegister dst, XmmRegister src);
   void pmullw(XmmRegister dst, XmmRegister src);
-  void vpmullw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
-  void vpsubb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpaddw(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
   void vpsubw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
-  void vpsubd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpmullw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void paddd(XmmRegister dst, XmmRegister src);
   void psubd(XmmRegister dst, XmmRegister src);
   void pmulld(XmmRegister dst, XmmRegister src);
-  void vpmulld(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void vpaddd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpsubd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpmulld(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void paddq(XmmRegister dst, XmmRegister src);
   void psubq(XmmRegister dst, XmmRegister src);
@@ -609,6 +620,15 @@ class X86_64Assembler final : public Assembler {
   void psubusw(XmmRegister dst, XmmRegister src);
   void psubsw(XmmRegister dst, XmmRegister src);
 
+  void vpaddusb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpaddsb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpaddusw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpaddsw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpsubusb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpsubsb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpsubusw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpsubsw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+
   void cvtsi2ss(XmmRegister dst, CpuRegister src);  // Note: this is the r/m32 version.
   void cvtsi2ss(XmmRegister dst, CpuRegister src, bool is64bit);
   void cvtsi2ss(XmmRegister dst, const Address& src, bool is64bit);
@@ -630,6 +650,7 @@ class X86_64Assembler final : public Assembler {
   void cvttsd2si(CpuRegister dst, XmmRegister src, bool is64bit);
 
   void cvtdq2ps(XmmRegister dst, XmmRegister src);
+  void vcvtdq2ps(XmmRegister dst, XmmRegister src);
   void cvtdq2pd(XmmRegister dst, XmmRegister src);
 
   void comiss(XmmRegister a, XmmRegister b);
@@ -681,11 +702,15 @@ class X86_64Assembler final : public Assembler {
 
   void pavgb(XmmRegister dst, XmmRegister src);  // no addr variant (for now)
   void pavgw(XmmRegister dst, XmmRegister src);
+  void vpavgb(XmmRegister dst, XmmRegister src1, XmmRegister src2);  // no addr variant (for now)
+  void vpavgw(XmmRegister dst, XmmRegister src, XmmRegister src2);
+
   void psadbw(XmmRegister dst, XmmRegister src);
   void pmaddwd(XmmRegister dst, XmmRegister src);
   void vpmaddwd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
   void phaddw(XmmRegister dst, XmmRegister src);
   void phaddd(XmmRegister dst, XmmRegister src);
+  void vphaddd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
   void haddps(XmmRegister dst, XmmRegister src);
   void haddpd(XmmRegister dst, XmmRegister src);
   void phsubw(XmmRegister dst, XmmRegister src);
@@ -712,24 +737,59 @@ class X86_64Assembler final : public Assembler {
   void minpd(XmmRegister dst, XmmRegister src);
   void maxpd(XmmRegister dst, XmmRegister src);
 
+  void vpminsb(XmmRegister dst, XmmRegister src1, XmmRegister src2);  // no addr variant (for now)
+  void vpmaxsb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpminsw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpmaxsw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpminsd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpmaxsd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+
+  void vpminub(XmmRegister dst, XmmRegister src1, XmmRegister src2);  // no addr variant (for now)
+  void vpmaxub(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpminuw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpmaxuw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpminud(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpmaxud(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+
+  void vminps(XmmRegister dst, XmmRegister src1, XmmRegister src2);  // no addr variant (for now)
+  void vmaxps(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vminpd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vmaxpd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+
   void pcmpeqb(XmmRegister dst, XmmRegister src);
   void pcmpeqw(XmmRegister dst, XmmRegister src);
   void pcmpeqd(XmmRegister dst, XmmRegister src);
   void pcmpeqq(XmmRegister dst, XmmRegister src);
+  void vpcmpeqb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void pcmpgtb(XmmRegister dst, XmmRegister src);
   void pcmpgtw(XmmRegister dst, XmmRegister src);
   void pcmpgtd(XmmRegister dst, XmmRegister src);
   void pcmpgtq(XmmRegister dst, XmmRegister src);  // SSE4.2
+  void vpcmpgtq(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void shufpd(XmmRegister dst, XmmRegister src, const Immediate& imm);
   void shufps(XmmRegister dst, XmmRegister src, const Immediate& imm);
   void pshufd(XmmRegister dst, XmmRegister src, const Immediate& imm);
+  void vbroadcastss(XmmRegister dst, XmmRegister src);
+  void vbroadcastsd(XmmRegister dst, XmmRegister src);
+  void vpbroadcastb(XmmRegister dst, XmmRegister src);
+  void vpbroadcastw(XmmRegister dst, XmmRegister src);
+  void vpbroadcastd(XmmRegister dst, XmmRegister src);
+  void vpbroadcastq(XmmRegister dst, XmmRegister src);
+
+  void pabsb(XmmRegister dst, XmmRegister src);
+  void pabsw(XmmRegister dst, XmmRegister src);
+  void pabsd(XmmRegister dst, XmmRegister src);
+  void vpabsb(XmmRegister dst, XmmRegister src);
+  void vpabsw(XmmRegister dst, XmmRegister src);
+  void vpabsd(XmmRegister dst, XmmRegister src);
 
   void punpcklbw(XmmRegister dst, XmmRegister src);
   void punpcklwd(XmmRegister dst, XmmRegister src);
   void punpckldq(XmmRegister dst, XmmRegister src);
   void punpcklqdq(XmmRegister dst, XmmRegister src);
+  void vpunpcklbw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void punpckhbw(XmmRegister dst, XmmRegister src);
   void punpckhwd(XmmRegister dst, XmmRegister src);
@@ -739,15 +799,25 @@ class X86_64Assembler final : public Assembler {
   void psllw(XmmRegister reg, const Immediate& shift_count);
   void pslld(XmmRegister reg, const Immediate& shift_count);
   void psllq(XmmRegister reg, const Immediate& shift_count);
+  void vpsllw(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
+  void vpslld(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
+  void vpsllq(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
 
   void psraw(XmmRegister reg, const Immediate& shift_count);
   void psrad(XmmRegister reg, const Immediate& shift_count);
   // no psraq
+  void vpsraw(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
+  void vpsrad(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
 
   void psrlw(XmmRegister reg, const Immediate& shift_count);
   void psrld(XmmRegister reg, const Immediate& shift_count);
   void psrlq(XmmRegister reg, const Immediate& shift_count);
   void psrldq(XmmRegister reg, const Immediate& shift_count);
+  void vpsrlw(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
+  void vpsrld(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
+  void vpsrlq(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
+
+  void vpermpd(XmmRegister dst, XmmRegister src, const Immediate& indices);
 
   void flds(const Address& src);
   void fstps(const Address& dst);
@@ -989,6 +1059,8 @@ class X86_64Assembler final : public Assembler {
 
   void ud2();
 
+  void vzeroupper();
+
   //
   // Macros for High-level operations.
   //
@@ -1110,7 +1182,8 @@ class X86_64Assembler final : public Assembler {
     }
   }
 
-  bool CpuHasAVXorAVX2FeatureFlag();
+  bool CpuHasAVXFeatureFlag();
+  bool CpuHasAVX2FeatureFlag();
 
  private:
   void EmitUint8(uint8_t value);
@@ -1156,6 +1229,7 @@ class X86_64Assembler final : public Assembler {
   void EmitRex64(XmmRegister dst, const Operand& operand);
   void EmitRex64(XmmRegister dst, CpuRegister src);
   void EmitRex64(CpuRegister dst, XmmRegister src);
+  void EmitRex64(XmmRegister dst, XmmRegister src);
 
   // Emit a REX prefix to normalize byte registers plus necessary register bit encodings.
   // `normalize_both` parameter controls if the REX prefix is checked only for the `src` register
@@ -1192,6 +1266,25 @@ class X86_64Assembler final : public Assembler {
                                        int vex_pp,
                                        bool is_commutative = false);
 
+  void EmitVecMoveCpuFpu(XmmRegister fp_reg, CpuRegister cpu_reg, bool is64bit, uint8_t opcode);
+  void EmitVecMergeFPRegs(XmmRegister dst, XmmRegister src1, XmmRegister src2, uint8_t vex_pp);
+
+  void EmitVecMinMaxOperation(XmmRegister dst,
+                              XmmRegister src1,
+                              XmmRegister src2,
+                              uint8_t vex_pp,
+                              bool is_vex_3byte,
+                              uint8_t opcode,
+                              bool is_commutative = false);
+
+  void EmitVecShiftOperation(XmmRegister dst,
+                             XmmRegister src,
+                             const Immediate& shift_count,
+                             uint8_t opcode,
+                             uint8_t operand_byte);
+
+  void EmitVecBroadcastInstruction(XmmRegister dst, XmmRegister src, uint8_t opcode);
+
   // Helper function to emit a shorter variant of XCHG if at least one operand is RAX/EAX/AX.
   bool try_xchg_rax(CpuRegister dst,
                     CpuRegister src,
diff --git a/compiler/utils/x86_64/assembler_x86_64_test.cc b/compiler/utils/x86_64/assembler_x86_64_test.cc
index 2d72bf6239..aa0d344d87 100644
--- a/compiler/utils/x86_64/assembler_x86_64_test.cc
+++ b/compiler/utils/x86_64/assembler_x86_64_test.cc
@@ -17,13 +17,15 @@
 #include "assembler_x86_64.h"
 
 #include <inttypes.h>
+
 #include <map>
 #include <random>
 
 #include "base/bit_utils.h"
 #include "base/macros.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "base/stl_util.h"
+#include "disassembler_x86.h"
 #include "jni_macro_assembler_x86_64.h"
 #include "utils/assembler_test.h"
 #include "utils/jni_macro_assembler_test.h"
@@ -31,7 +33,7 @@
 namespace art HIDDEN {
 
 TEST(AssemblerX86_64, CreateBuffer) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaAllocator allocator(&pool);
   AssemblerBuffer buffer(&allocator);
   AssemblerBuffer::EnsureCapacity ensured(&buffer);
@@ -136,13 +138,15 @@ class AssemblerX86_64Test : public AssemblerTest<x86_64::X86_64Assembler,
                                                  x86_64::Address,
                                                  x86_64::CpuRegister,
                                                  x86_64::XmmRegister,
-                                                 x86_64::Immediate> {
+                                                 x86_64::Immediate,
+                                                 x86_64::XmmRegister> {
  public:
   using Base = AssemblerTest<x86_64::X86_64Assembler,
                              x86_64::Address,
                              x86_64::CpuRegister,
                              x86_64::XmmRegister,
-                             x86_64::Immediate>;
+                             x86_64::Immediate,
+                             x86_64::XmmRegister>;
 
  protected:
   AssemblerX86_64Test() : Base() {
@@ -153,6 +157,36 @@ class AssemblerX86_64Test : public AssemblerTest<x86_64::X86_64Assembler,
     return InstructionSet::kX86_64;
   }
 
+  static void VerifyDisassemblerDriver(const std::vector<uint8_t>& art_code,
+                                       const std::string& ref_assembly_text,
+                                       [[maybe_unused]] const std::string& test_name) {
+    ASSERT_NE(ref_assembly_text.length(), 0U) << "Empty assembly";
+    std::string art_disassembly;
+    MemoryRegion code_mem(const_cast<uint8_t*>(&art_code[0]), art_code.size());
+    std::unique_ptr<x86::DisassemblerX86> disasm(static_cast<art::x86::DisassemblerX86*>(
+        Disassembler::Create(
+            InstructionSet::kX86_64,
+            new DisassemblerOptions(/* absolute_addresses= */ false,
+                                    code_mem.begin(),
+                                    code_mem.end(),
+                                    /* can_read_literals = */ true,
+                                    &Thread::DumpThreadOffset<PointerSize::k64>))));
+    size_t length = 0;
+    std::stringstream sstream;
+    for (const uint8_t* cur = code_mem.begin(); cur < code_mem.end(); cur += length) {
+      length = disasm->Dump(sstream, cur);
+      // ART dumps disassembly in this format
+      // Address: Hexbytes \t%-7s<prefix> opcode ....
+      // Extract just the disassembly and compress spaces
+      std::string disassembly = sstream.str();
+      disassembly = disassembly.substr(disassembly.find('\t') + 1);
+      disassembly = disassembly.substr(disassembly.find_first_not_of(" "));
+      art_disassembly += disassembly;
+      sstream.str("");
+    }
+    ASSERT_EQ(art_disassembly, ref_assembly_text) << "Disassembler check failed.";
+  }
+
   void SetUpHelpers() override {
     if (addresses_singleton_.size() == 0) {
       // One addressing mode to test the repeat drivers.
@@ -341,10 +375,34 @@ class AssemblerX86_64AVXTest : public AssemblerX86_64Test {
  public:
   AssemblerX86_64AVXTest()
       : instruction_set_features_(X86_64InstructionSetFeatures::FromVariant("kabylake", nullptr)) {}
+
  protected:
   x86_64::X86_64Assembler* CreateAssembler(ArenaAllocator* allocator) override {
     return new (allocator) x86_64::X86_64Assembler(allocator, instruction_set_features_.get());
   }
+
+  ArrayRef<const x86_64::XmmRegister> GetVectorRegisters() override {
+    static constexpr x86_64::XmmRegister kVectorRegisters[] = {
+        x86_64::XmmRegister(x86_64::XMM0, 32),
+        x86_64::XmmRegister(x86_64::XMM1, 32),
+        x86_64::XmmRegister(x86_64::XMM2, 32),
+        x86_64::XmmRegister(x86_64::XMM3, 32),
+        x86_64::XmmRegister(x86_64::XMM4, 32),
+        x86_64::XmmRegister(x86_64::XMM5, 32),
+        x86_64::XmmRegister(x86_64::XMM6, 32),
+        x86_64::XmmRegister(x86_64::XMM7, 32),
+        x86_64::XmmRegister(x86_64::XMM8, 32),
+        x86_64::XmmRegister(x86_64::XMM9, 32),
+        x86_64::XmmRegister(x86_64::XMM10, 32),
+        x86_64::XmmRegister(x86_64::XMM11, 32),
+        x86_64::XmmRegister(x86_64::XMM12, 32),
+        x86_64::XmmRegister(x86_64::XMM13, 32),
+        x86_64::XmmRegister(x86_64::XMM14, 32),
+        x86_64::XmmRegister(x86_64::XMM15, 32),
+    };
+    return ArrayRef<const x86_64::XmmRegister>(kVectorRegisters);
+  }
+
  private:
   std::unique_ptr<const X86_64InstructionSetFeatures> instruction_set_features_;
 };
@@ -1266,11 +1324,11 @@ TEST_F(AssemblerX86_64Test, Movaps) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovaps) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::vmovaps, "vmovaps %{reg2}, %{reg1}"), "vmovaps");
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vmovaps, "vmovaps %{reg2}, %{reg1}"), "vmovaps");
 }
 
 TEST_F(AssemblerX86_64AVXTest, Movaps) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::movaps, "vmovaps %{reg2}, %{reg1}"), "avx_movaps");
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::movaps, "vmovaps %{reg2}, %{reg1}"), "avx_movaps");
 }
 
 TEST_F(AssemblerX86_64Test, MovapsStore) {
@@ -1278,11 +1336,11 @@ TEST_F(AssemblerX86_64Test, MovapsStore) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapsStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovaps, "vmovaps %{reg}, {mem}"), "vmovaps_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::vmovaps, "vmovaps %{reg}, {mem}"), "vmovaps_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovapsStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movaps, "vmovaps %{reg}, {mem}"), "avx_movaps_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::movaps, "vmovaps %{reg}, {mem}"), "avx_movaps_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovapsLoad) {
@@ -1290,11 +1348,11 @@ TEST_F(AssemblerX86_64Test, MovapsLoad) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapsLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovaps, "vmovaps {mem}, %{reg}"), "vmovaps_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::vmovaps, "vmovaps {mem}, %{reg}"), "vmovaps_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovapsLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movaps, "vmovaps {mem}, %{reg}"), "avx_movaps_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::movaps, "vmovaps {mem}, %{reg}"), "avx_movaps_l");
 }
 
 TEST_F(AssemblerX86_64Test, MovupsStore) {
@@ -1302,11 +1360,11 @@ TEST_F(AssemblerX86_64Test, MovupsStore) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovupsStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovups, "vmovups %{reg}, {mem}"), "vmovups_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::vmovups, "vmovups %{reg}, {mem}"), "vmovups_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovupsStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movups, "vmovups %{reg}, {mem}"), "avx_movups_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::movups, "vmovups %{reg}, {mem}"), "avx_movups_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovupsLoad) {
@@ -1314,27 +1372,63 @@ TEST_F(AssemblerX86_64Test, MovupsLoad) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovupsLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovups, "vmovups {mem}, %{reg}"), "vmovups_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::vmovups, "vmovups {mem}, %{reg}"), "vmovups_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovupsLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movups, "vmovups {mem}, %{reg}"), "avx_movups_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::movups, "vmovups {mem}, %{reg}"), "avx_movups_l");
 }
 
 TEST_F(AssemblerX86_64Test, Movss) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::movss, "movss %{reg2}, %{reg1}"), "movss");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VMovss) {
+  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vmovss, "vmovss %{reg3}, %{reg2}, %{reg1}"),
+            "vmovss");
+}
+// Cannot verify auto forwarding as assembly supports only xmm regs inspite of passing vreg
+// TEST_F(AssemblerX86_64AVXTest, Movss) {
+//   DriverStr(RepeatVV(&x86_64::X86_64Assembler::movss, "vmovss %{reg2}, %{reg1}, %{reg1}"),
+//             "avx_movss");
+// }
+
+TEST_F(AssemblerX86_64Test, MovssLoad) {
+  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movss, "movss {mem}, %{reg}"), "movss_l");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VMovssLoad) {
+  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovss, "vmovss {mem}, %{reg}"), "vmovss_l");
+}
+
+// Cannot verify auto forwarding as assembly supports only xmm regs inspite of passing vreg
+// TEST_F(AssemblerX86_64AVXTest, MovssLoad) {
+//   DriverStr(RepeatVA(&x86_64::X86_64Assembler::movss, "vmovss {mem}, %{reg}"), "avx_movss_l");
+// }
+
+TEST_F(AssemblerX86_64Test, MovssStore) {
+  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movss, "movss %{reg}, {mem}"), "movss_s");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VMovssStore) {
+  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovss, "vmovss %{reg}, {mem}"), "vmovss_s");
+}
+
+// Cannot verify auto forwarding as assembly supports only xmm regs inspite of passing vreg
+// TEST_F(AssemblerX86_64AVXTest, MovssStore) {
+//   DriverStr(RepeatAF(&x86_64::X86_64Assembler::movss, "vmovss %{reg}, {mem}"), "avx_movss_s");
+// }
+
 TEST_F(AssemblerX86_64Test, Movapd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::movapd, "movapd %{reg2}, %{reg1}"), "movapd");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapd) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::vmovapd, "vmovapd %{reg2}, %{reg1}"), "vmovapd");
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vmovapd, "vmovapd %{reg2}, %{reg1}"), "vmovapd");
 }
 
 TEST_F(AssemblerX86_64AVXTest, Movapd) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::movapd, "vmovapd %{reg2}, %{reg1}"), "avx_movapd");
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::movapd, "vmovapd %{reg2}, %{reg1}"), "avx_movapd");
 }
 
 TEST_F(AssemblerX86_64Test, MovapdStore) {
@@ -1342,11 +1436,11 @@ TEST_F(AssemblerX86_64Test, MovapdStore) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapdStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovapd, "vmovapd %{reg}, {mem}"), "vmovapd_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::vmovapd, "vmovapd %{reg}, {mem}"), "vmovapd_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovapdStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movapd, "vmovapd %{reg}, {mem}"), "avx_movapd_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::movapd, "vmovapd %{reg}, {mem}"), "avx_movapd_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovapdLoad) {
@@ -1354,11 +1448,11 @@ TEST_F(AssemblerX86_64Test, MovapdLoad) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapdLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovapd, "vmovapd {mem}, %{reg}"), "vmovapd_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::vmovapd, "vmovapd {mem}, %{reg}"), "vmovapd_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovapdLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movapd, "vmovapd {mem}, %{reg}"), "avx_movapd_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::movapd, "vmovapd {mem}, %{reg}"), "avx_movapd_l");
 }
 
 TEST_F(AssemblerX86_64Test, MovupdStore) {
@@ -1366,11 +1460,11 @@ TEST_F(AssemblerX86_64Test, MovupdStore) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovupdStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovupd, "vmovupd %{reg}, {mem}"), "vmovupd_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::vmovupd, "vmovupd %{reg}, {mem}"), "vmovupd_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovupdStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movupd, "vmovupd %{reg}, {mem}"), "avx_movupd_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::movupd, "vmovupd %{reg}, {mem}"), "avx_movupd_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovupdLoad) {
@@ -1378,27 +1472,64 @@ TEST_F(AssemblerX86_64Test, MovupdLoad) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovupdLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovupd, "vmovupd {mem}, %{reg}"), "vmovupd_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::vmovupd, "vmovupd {mem}, %{reg}"), "vmovupd_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovupdLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movupd, "vmovupd {mem}, %{reg}"), "avx_movupd_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::movupd, "vmovupd {mem}, %{reg}"), "avx_movupd_l");
 }
 
 TEST_F(AssemblerX86_64Test, Movsd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::movsd, "movsd %{reg2}, %{reg1}"), "movsd");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VMovsd) {
+  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vmovsd, "vmovsd %{reg3}, %{reg2}, %{reg1}"),
+            "vmovsd");
+}
+
+// Cannot verify auto forwarding as assembly supports only xmm regs inspite of passing vreg
+// TEST_F(AssemblerX86_64AVXTest, Movsd) {
+//   DriverStr(RepeatFF(&x86_64::X86_64Assembler::movsd, "vmovsd %{reg2}, %{reg1}, %{reg1}"),
+//             "avx_movsd");
+// }
+
+TEST_F(AssemblerX86_64Test, MovsdLoad) {
+  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movsd, "movsd {mem}, %{reg}"), "movsd_l");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VMovsdLoad) {
+  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovsd, "vmovsd {mem}, %{reg}"), "vmovsd_l");
+}
+
+// Cannot verify auto forwarding as assembly supports only xmm regs inspite of passing vreg
+// TEST_F(AssemblerX86_64AVXTest, MovsdLoad) {
+//   DriverStr(RepeatFA(&x86_64::X86_64Assembler::movsd, "vmovsd {mem}, %{reg}"), "avx_movsd_l");
+// }
+
+TEST_F(AssemblerX86_64Test, MovsdStore) {
+  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movsd, "movsd %{reg}, {mem}"), "movsd_s");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VMovsdStore) {
+  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovsd, "vmovsd %{reg}, {mem}"), "vmovsd_s");
+}
+
+// Cannot verify auto forwarding as assembly supports only xmm regs inspite of passing vreg
+// TEST_F(AssemblerX86_64AVXTest, MovsdStore) {
+//   DriverStr(RepeatAF(&x86_64::X86_64Assembler::movsd, "vmovsd %{reg2}, {mem}"), "avx_movsd_s");
+// }
+
 TEST_F(AssemblerX86_64Test, Movdqa) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::movdqa, "movdqa %{reg2}, %{reg1}"), "movdqa");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovdqa) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::vmovdqa, "vmovdqa %{reg2}, %{reg1}"), "vmovdqa");
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vmovdqa, "vmovdqa %{reg2}, %{reg1}"), "vmovdqa");
 }
 
 TEST_F(AssemblerX86_64AVXTest, Movdqa) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::movdqa, "vmovdqa %{reg2}, %{reg1}"), "avx_movdqa");
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::movdqa, "vmovdqa %{reg2}, %{reg1}"), "avx_movdqa");
 }
 
 TEST_F(AssemblerX86_64Test, MovdqaStore) {
@@ -1406,11 +1537,11 @@ TEST_F(AssemblerX86_64Test, MovdqaStore) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovdqaStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovdqa, "vmovdqa %{reg}, {mem}"), "vmovdqa_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::vmovdqa, "vmovdqa %{reg}, {mem}"), "vmovdqa_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovdqaStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movdqa, "vmovdqa %{reg}, {mem}"), "avx_movdqa_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::movdqa, "vmovdqa %{reg}, {mem}"), "avx_movdqa_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovdqaLoad) {
@@ -1418,11 +1549,11 @@ TEST_F(AssemblerX86_64Test, MovdqaLoad) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovdqaLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovdqa, "vmovdqa {mem}, %{reg}"), "vmovdqa_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::vmovdqa, "vmovdqa {mem}, %{reg}"), "vmovdqa_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovdqaLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movdqa, "vmovdqa {mem}, %{reg}"), "avx_movdqa_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::movdqa, "vmovdqa {mem}, %{reg}"), "avx_movdqa_l");
 }
 
 TEST_F(AssemblerX86_64Test, MovdquStore) {
@@ -1430,11 +1561,11 @@ TEST_F(AssemblerX86_64Test, MovdquStore) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovdquStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovdqu, "vmovdqu %{reg}, {mem}"), "vmovdqu_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::vmovdqu, "vmovdqu %{reg}, {mem}"), "vmovdqu_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovdquStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movdqu, "vmovdqu %{reg}, {mem}"), "avx_movdqu_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::movdqu, "vmovdqu %{reg}, {mem}"), "avx_movdqu_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovdquLoad) {
@@ -1442,29 +1573,61 @@ TEST_F(AssemblerX86_64Test, MovdquLoad) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovdquLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovdqu, "vmovdqu {mem}, %{reg}"), "vmovdqu_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::vmovdqu, "vmovdqu {mem}, %{reg}"), "vmovdqu_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovdquLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movdqu, "vmovdqu {mem}, %{reg}"), "avx_movdqu_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::movdqu, "vmovdqu {mem}, %{reg}"), "avx_movdqu_l");
 }
 
 TEST_F(AssemblerX86_64Test, Movq1) {
   DriverStr(RepeatFR(&x86_64::X86_64Assembler::movq, "movq %{reg2}, %{reg1}"), "movq.1");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VMovq1) {
+  DriverStr(RepeatFR(&x86_64::X86_64Assembler::vmovq, "vmovq %{reg2}, %{reg1}"), "vmovq.1");
+}
+// Cannot verify auto forwarding as assembly supports only xmm regs inspite of passing vreg
+// TEST_F(AssemblerX86_64AVXTest, Movq1) {
+//   DriverStr(RepeatVR(&x86_64::X86_64Assembler::movq, "vmovq %{reg2}, %{reg1}"), "avx_movq.1");
+// }
+
 TEST_F(AssemblerX86_64Test, Movq2) {
   DriverStr(RepeatRF(&x86_64::X86_64Assembler::movq, "movq %{reg2}, %{reg1}"), "movq.2");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VMovq2) {
+  DriverStr(RepeatRF(&x86_64::X86_64Assembler::vmovq, "vmovq %{reg2}, %{reg1}"), "vmovq.2");
+}
+// Cannot verify auto forwarding as assembly supports only xmm regs inspite of passing vreg
+// TEST_F(AssemblerX86_64AVXTest, Movq2) {
+//   DriverStr(RepeatRV(&x86_64::X86_64Assembler::movq, "vmovq %{reg2}, %{reg1}"), "avx_movq.2");
+// }
+
 TEST_F(AssemblerX86_64Test, Movd1) {
   DriverStr(RepeatFr(&x86_64::X86_64Assembler::movd, "movd %{reg2}, %{reg1}"), "movd.1");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VMovd1) {
+  DriverStr(RepeatFr(&x86_64::X86_64Assembler::vmovd, "vmovd %{reg2}, %{reg1}"), "vmovd.1");
+}
+// Cannot verify auto forwarding as assembly supports only xmm regs inspite of passing vreg
+// TEST_F(AssemblerX86_64AVXTest, Movd1) {
+//   DriverStr(RepeatVr(&x86_64::X86_64Assembler::movd, "vmovd %{reg2}, %{reg1}"), "avx_movd.1");
+// }
+
 TEST_F(AssemblerX86_64Test, Movd2) {
   DriverStr(RepeatrF(&x86_64::X86_64Assembler::movd, "movd %{reg2}, %{reg1}"), "movd.2");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VMovd2) {
+  DriverStr(RepeatrF(&x86_64::X86_64Assembler::vmovd, "vmovd %{reg2}, %{reg1}"), "vmovd.2");
+}
+// Cannot verify auto forwarding as assembly supports only xmm regs inspite of passing vreg
+// TEST_F(AssemblerX86_64AVXTest, Movd2) {
+//   DriverStr(RepeatrV(&x86_64::X86_64Assembler::movd, "vmovd %{reg2}, %{reg1}"), "avx_movd.2");
+// }
+
 TEST_F(AssemblerX86_64Test, Addss) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::addss, "addss %{reg2}, %{reg1}"), "addss");
 }
@@ -1478,8 +1641,13 @@ TEST_F(AssemblerX86_64Test, Addps) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VAddps) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vaddps, "vaddps %{reg3}, %{reg2}, %{reg1}"), "vaddps");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vaddps, "vaddps %{reg3}, %{reg2}, %{reg1}"),
+            "vaddps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Addps) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::addps, "vaddps %{reg2}, %{reg1}, %{reg1}"),
+            "avx_addps");
 }
 
 TEST_F(AssemblerX86_64Test, Addpd) {
@@ -1487,8 +1655,13 @@ TEST_F(AssemblerX86_64Test, Addpd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VAddpd) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vaddpd, "vaddpd %{reg3}, %{reg2}, %{reg1}"), "vaddpd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vaddpd, "vaddpd %{reg3}, %{reg2}, %{reg1}"),
+            "vaddpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Addpd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::addpd, "vaddpd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_addpd");
 }
 
 TEST_F(AssemblerX86_64Test, Subss) {
@@ -1504,8 +1677,13 @@ TEST_F(AssemblerX86_64Test, Subps) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VSubps) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vsubps, "vsubps %{reg3},%{reg2}, %{reg1}"), "vsubps");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vsubps, "vsubps %{reg3}, %{reg2}, %{reg1}"),
+            "vsubps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Subps) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::subps, "vsubps %{reg2}, %{reg1}, %{reg1}"),
+            "avx_subps");
 }
 
 TEST_F(AssemblerX86_64Test, Subpd) {
@@ -1513,8 +1691,13 @@ TEST_F(AssemblerX86_64Test, Subpd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VSubpd) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vsubpd, "vsubpd %{reg3}, %{reg2}, %{reg1}"), "vsubpd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vsubpd, "vsubpd %{reg3}, %{reg2}, %{reg1}"),
+            "vsubpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Subpd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::subpd, "vsubpd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_subpd");
 }
 
 TEST_F(AssemblerX86_64Test, Mulss) {
@@ -1530,8 +1713,13 @@ TEST_F(AssemblerX86_64Test, Mulps) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMulps) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vmulps, "vmulps %{reg3}, %{reg2}, %{reg1}"), "vmulps");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vmulps, "vmulps %{reg3}, %{reg2}, %{reg1}"),
+            "vmulps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Mulps) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::mulps, "vmulps %{reg2}, %{reg1}, %{reg1}"),
+            "avx_vmulps");
 }
 
 TEST_F(AssemblerX86_64Test, Mulpd) {
@@ -1539,8 +1727,13 @@ TEST_F(AssemblerX86_64Test, Mulpd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMulpd) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vmulpd, "vmulpd %{reg3}, %{reg2}, %{reg1}"), "vmulpd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vmulpd, "vmulpd %{reg3}, %{reg2}, %{reg1}"),
+            "vmulpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Mulpd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::mulpd, "vmulpd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_mulpd");
 }
 
 TEST_F(AssemblerX86_64Test, Divss) {
@@ -1556,8 +1749,13 @@ TEST_F(AssemblerX86_64Test, Divps) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VDivps) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vdivps, "vdivps %{reg3}, %{reg2}, %{reg1}"), "vdivps");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vdivps, "vdivps %{reg3}, %{reg2}, %{reg1}"),
+            "vdivps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Divps) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::divps, "vdivps %{reg2}, %{reg1}, %{reg1}"),
+            "avx_divps");
 }
 
 TEST_F(AssemblerX86_64Test, Divpd) {
@@ -1565,8 +1763,13 @@ TEST_F(AssemblerX86_64Test, Divpd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VDivpd) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vdivpd, "vdivpd %{reg3}, %{reg2}, %{reg1}"), "vdivpd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vdivpd, "vdivpd %{reg3}, %{reg2}, %{reg1}"),
+            "vdivpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Divpd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::divpd, "vdivpd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_divpd");
 }
 
 TEST_F(AssemblerX86_64Test, Paddb) {
@@ -1574,8 +1777,13 @@ TEST_F(AssemblerX86_64Test, Paddb) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPaddb) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpaddb, "vpaddb %{reg3}, %{reg2}, %{reg1}"), "vpaddb");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddb, "vpaddb %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Paddb) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::paddb, "vpaddb %{reg2}, %{reg1}, %{reg1}"),
+            "avx_paddb");
 }
 
 TEST_F(AssemblerX86_64Test, Psubb) {
@@ -1583,35 +1791,55 @@ TEST_F(AssemblerX86_64Test, Psubb) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPsubb) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpsubb, "vpsubb %{reg3},%{reg2}, %{reg1}"), "vpsubb");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubb, "vpsubb %{reg3},%{reg2}, %{reg1}"),
+            "vpsubb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psubb) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::psubb, "vpsubb %{reg2},%{reg1}, %{reg1}"),
+            "avx_psubb");
 }
 
 TEST_F(AssemblerX86_64Test, Paddw) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::paddw, "paddw %{reg2}, %{reg1}"), "paddw");
 }
 
-TEST_F(AssemblerX86_64AVXTest, VPsubw) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpsubw, "vpsubw %{reg3}, %{reg2}, %{reg1}"), "vpsubw");
+TEST_F(AssemblerX86_64AVXTest, VPaddw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddw, "vpaddw %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddw");
 }
 
-TEST_F(AssemblerX86_64AVXTest, VPaddw) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpaddw, "vpaddw %{reg3}, %{reg2}, %{reg1}"), "vpaddw");
+TEST_F(AssemblerX86_64AVXTest, Paddw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::paddw, "vpaddw %{reg2}, %{reg1}, %{reg1}"),
+            "avx_paddw");
 }
 
 TEST_F(AssemblerX86_64Test, Psubw) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::psubw, "psubw %{reg2}, %{reg1}"), "psubw");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPsubw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubw, "vpsubw %{reg3}, %{reg2}, %{reg1}"),
+            "vpsubw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psubw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::psubw, "vpsubw %{reg2}, %{reg1}, %{reg1}"),
+            "avx_psubw");
+}
+
 TEST_F(AssemblerX86_64Test, Pmullw) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pmullw, "pmullw %{reg2}, %{reg1}"), "pmullw");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPmullw) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpmullw, "vpmullw %{reg3}, %{reg2}, %{reg1}"), "vpmullw");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmullw, "vpmullw %{reg3}, %{reg2}, %{reg1}"),
+            "vpmullw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pmullw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pmullw, "vpmullw %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pmullw");
 }
 
 TEST_F(AssemblerX86_64Test, Paddd) {
@@ -1619,8 +1847,13 @@ TEST_F(AssemblerX86_64Test, Paddd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPaddd) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpaddd, "vpaddd %{reg3}, %{reg2}, %{reg1}"), "vpaddd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddd, "vpaddd %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Paddd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::paddd, "vpaddd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_paddd");
 }
 
 TEST_F(AssemblerX86_64Test, Psubd) {
@@ -1628,8 +1861,13 @@ TEST_F(AssemblerX86_64Test, Psubd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPsubd) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpsubd, "vpsubd %{reg3}, %{reg2}, %{reg1}"), "vpsubd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubd, "vpsubd %{reg3}, %{reg2}, %{reg1}"),
+            "vpsubd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psubd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::psubd, "vpsubd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_psubd");
 }
 
 TEST_F(AssemblerX86_64Test, Pmulld) {
@@ -1637,8 +1875,13 @@ TEST_F(AssemblerX86_64Test, Pmulld) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPmulld) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpmulld, "vpmulld %{reg3}, %{reg2}, %{reg1}"), "vpmulld");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmulld, "vpmulld %{reg3}, %{reg2}, %{reg1}"),
+            "vpmulld");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pmulld) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pmulld, "vpmulld %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pmulld");
 }
 
 TEST_F(AssemblerX86_64Test, Paddq) {
@@ -1646,8 +1889,13 @@ TEST_F(AssemblerX86_64Test, Paddq) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPaddq) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpaddq, "vpaddq %{reg3}, %{reg2}, %{reg1}"), "vpaddq");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddq, "vpaddq %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddq");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Paddq) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::paddq, "vpaddq %{reg2}, %{reg1}, %{reg1}"),
+            "avx_paddq");
 }
 
 TEST_F(AssemblerX86_64Test, Psubq) {
@@ -1655,42 +1903,127 @@ TEST_F(AssemblerX86_64Test, Psubq) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPsubq) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpsubq, "vpsubq %{reg3}, %{reg2}, %{reg1}"), "vpsubq");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubq, "vpsubq %{reg3}, %{reg2}, %{reg1}"),
+            "vpsubq");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psubq) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::psubq, "vpsubq %{reg2}, %{reg1}, %{reg1}"),
+            "avx_psubq");
 }
 
 TEST_F(AssemblerX86_64Test, Paddusb) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::paddusb, "paddusb %{reg2}, %{reg1}"), "paddusb");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPaddusb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddusb, "vpaddusb %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddusb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Paddusb) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::paddusb, "vpaddusb %{reg2}, %{reg1}, %{reg1}"),
+            "avx_paddusb");
+}
+
 TEST_F(AssemblerX86_64Test, Paddsb) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::paddsb, "paddsb %{reg2}, %{reg1}"), "paddsb");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPaddsb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddsb, "vpaddsb %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Paddsb) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::paddsb, "vpaddsb %{reg2}, %{reg1}, %{reg1}"),
+            "avx_paddsb");
+}
+
 TEST_F(AssemblerX86_64Test, Paddusw) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::paddusw, "paddusw %{reg2}, %{reg1}"), "paddusw");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPaddusw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddusw, "vpaddusw %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddusw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Paddusw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::paddusw, "vpaddusw %{reg2}, %{reg1}, %{reg1}"),
+            "avx_paddusw");
+}
+
 TEST_F(AssemblerX86_64Test, Paddsw) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::paddsw, "paddsw %{reg2}, %{reg1}"), "paddsw");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPaddsw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddsw, "vpaddsw %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Paddsw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::paddsw, "vpaddsw %{reg2}, %{reg1}, %{reg1}"),
+            "avx_paddsw");
+}
+
 TEST_F(AssemblerX86_64Test, Psubusb) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::psubusb, "psubusb %{reg2}, %{reg1}"), "psubusb");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPsubusb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubusb, "vpsubusb %{reg3}, %{reg2}, %{reg1}"),
+            "vpsubusb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psubusb) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::psubusb, "vpsubusb %{reg2}, %{reg1}, %{reg1}"),
+            "avx_psubusb");
+}
+
 TEST_F(AssemblerX86_64Test, Psubsb) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::psubsb, "psubsb %{reg2}, %{reg1}"), "psubsb");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPsubsb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubsb, "vpsubsb %{reg3}, %{reg2}, %{reg1}"),
+            "vpsubsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psubsb) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::psubsb, "vpsubsb %{reg2}, %{reg1}, %{reg1}"),
+            "avx_psubsb");
+}
+
 TEST_F(AssemblerX86_64Test, Psubusw) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::psubusw, "psubusw %{reg2}, %{reg1}"), "psubusw");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPsubusw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubusw, "vpsubusw %{reg3}, %{reg2}, %{reg1}"),
+            "vpsubusw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psubusw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::psubusw, "vpsubusw %{reg2}, %{reg1}, %{reg1}"),
+            "avx_psubusw");
+}
+
 TEST_F(AssemblerX86_64Test, Psubsw) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::psubsw, "psubsw %{reg2}, %{reg1}"), "psubsw");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPsubsw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubsw, "vpsubsw %{reg3}, %{reg2}, %{reg1}"),
+            "vpsubsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psubsw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::psubsw, "vpsubsw %{reg2}, %{reg1}, %{reg1}"),
+            "avx_psubsw");
+}
+
 TEST_F(AssemblerX86_64Test, Cvtsi2ss) {
   DriverStr(RepeatFr(&x86_64::X86_64Assembler::cvtsi2ss, "cvtsi2ss %{reg2}, %{reg1}"), "cvtsi2ss");
 }
@@ -1729,6 +2062,16 @@ TEST_F(AssemblerX86_64Test, Cvtdq2ps) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::cvtdq2ps, "cvtdq2ps %{reg2}, %{reg1}"), "cvtdq2ps");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VCvtdq2ps) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vcvtdq2ps, "vcvtdq2ps %{reg2}, %{reg1}"),
+            "vcvtdq2ps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Cvtdq2ps) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::cvtdq2ps, "vcvtdq2ps %{reg2}, %{reg1}"),
+            "avx_cvtdq2ps");
+}
+
 TEST_F(AssemblerX86_64Test, Cvtdq2pd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::cvtdq2pd, "cvtdq2pd %{reg2}, %{reg1}"), "cvtdq2pd");
 }
@@ -1771,54 +2114,82 @@ TEST_F(AssemblerX86_64Test, Xorps) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::xorps, "xorps %{reg2}, %{reg1}"), "xorps");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VXorps) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vxorps, "vxorps %{reg3}, %{reg2}, %{reg1}"),
+            "vxorps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Xorps) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::xorps, "vxorps %{reg2}, %{reg1}, %{reg1}"),
+            "avx_xorps");
+}
+
 TEST_F(AssemblerX86_64Test, Xorpd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::xorpd, "xorpd %{reg2}, %{reg1}"), "xorpd");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VXorpd) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vxorpd, "vxorpd %{reg3}, %{reg2}, %{reg1}"),
+            "vxorpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Xorpd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::xorpd, "vxorpd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_xorpd");
+}
+
 TEST_F(AssemblerX86_64Test, Pxor) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pxor, "pxor %{reg2}, %{reg1}"), "pxor");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPXor) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vpxor,
-                      "vpxor %{reg3}, %{reg2}, %{reg1}"), "vpxor");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpxor, "vpxor %{reg3}, %{reg2}, %{reg1}"), "vpxor");
 }
 
-TEST_F(AssemblerX86_64AVXTest, VXorps) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vxorps,
-                      "vxorps %{reg3}, %{reg2}, %{reg1}"), "vxorps");
-}
-
-TEST_F(AssemblerX86_64AVXTest, VXorpd) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vxorpd,
-                      "vxorpd %{reg3}, %{reg2}, %{reg1}"), "vxorpd");
+TEST_F(AssemblerX86_64AVXTest, PXor) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pxor, "vpxor %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pxor");
 }
 
 TEST_F(AssemblerX86_64Test, Andps) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::andps, "andps %{reg2}, %{reg1}"), "andps");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VAndps) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vandps, "vandps %{reg3}, %{reg2}, %{reg1}"),
+            "vandps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Andps) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::andps, "vandps %{reg2}, %{reg1}, %{reg1}"),
+            "avx_andps");
+}
+
 TEST_F(AssemblerX86_64Test, Andpd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::andpd, "andpd %{reg2}, %{reg1}"), "andpd");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VAndpd) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vandpd, "vandpd %{reg3}, %{reg2}, %{reg1}"),
+            "vandpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Andpd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::andpd, "vandpd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_andpd");
+}
+
 TEST_F(AssemblerX86_64Test, Pand) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pand, "pand %{reg2}, %{reg1}"), "pand");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPAnd) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vpand,
-                      "vpand %{reg3}, %{reg2}, %{reg1}"), "vpand");
-}
-
-TEST_F(AssemblerX86_64AVXTest, VAndps) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vandps,
-                      "vandps %{reg3}, %{reg2}, %{reg1}"), "vandps");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpand, "vpand %{reg3}, %{reg2}, %{reg1}"), "vpand");
 }
 
-TEST_F(AssemblerX86_64AVXTest, VAndpd) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vandpd,
-                      "vandpd %{reg3}, %{reg2}, %{reg1}"), "vandpd");
+TEST_F(AssemblerX86_64AVXTest, PAnd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pand, "vpand %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pand");
 }
 
 TEST_F(AssemblerX86_64Test, Andn) {
@@ -1828,64 +2199,110 @@ TEST_F(AssemblerX86_64Test, andnpd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::andnpd, "andnpd %{reg2}, %{reg1}"), "andnpd");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VAndnpd) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vandnpd, "vandnpd %{reg3}, %{reg2}, %{reg1}"),
+            "vandnpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Andnpd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::andnpd, "vandnpd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_andnpd");
+}
+
 TEST_F(AssemblerX86_64Test, andnps) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::andnps, "andnps %{reg2}, %{reg1}"), "andnps");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VAndnps) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vandnps, "vandnps %{reg3}, %{reg2}, %{reg1}"),
+            "vandnps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Andnps) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::andnps, "vandnps %{reg2}, %{reg1}, %{reg1}"),
+            "avx_andnps");
+}
+
 TEST_F(AssemblerX86_64Test, Pandn) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pandn, "pandn %{reg2}, %{reg1}"), "pandn");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPAndn) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vpandn,
-                      "vpandn %{reg3}, %{reg2}, %{reg1}"), "vpandn");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpandn, "vpandn %{reg3}, %{reg2}, %{reg1}"),
+            "vpandn");
 }
 
-TEST_F(AssemblerX86_64AVXTest, VAndnps) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vandnps,
-                      "vandnps %{reg3}, %{reg2}, %{reg1}"), "vandnps");
-}
-
-TEST_F(AssemblerX86_64AVXTest, VAndnpd) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vandnpd,
-                      "vandnpd %{reg3}, %{reg2}, %{reg1}"), "vandnpd");
+TEST_F(AssemblerX86_64AVXTest, PAndn) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pandn, "vpandn %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pandn");
 }
 
 TEST_F(AssemblerX86_64Test, Orps) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::orps, "orps %{reg2}, %{reg1}"), "orps");
 }
 
+TEST_F(AssemblerX86_64AVXTest, Vorps) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vorps, "vorps %{reg3}, %{reg2}, %{reg1}"), "vorps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, orps) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::orps, "vorps %{reg2}, %{reg1}, %{reg1}"),
+            "avx_orps");
+}
+
 TEST_F(AssemblerX86_64Test, Orpd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::orpd, "orpd %{reg2}, %{reg1}"), "orpd");
 }
 
+TEST_F(AssemblerX86_64AVXTest, Vorpd) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vorpd, "vorpd %{reg3}, %{reg2}, %{reg1}"), "vorpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, orpd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::orpd, "vorpd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_orpd");
+}
+
 TEST_F(AssemblerX86_64Test, Por) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::por, "por %{reg2}, %{reg1}"), "por");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPor) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vpor,
-                      "vpor %{reg3}, %{reg2}, %{reg1}"), "vpor");
-}
-
-TEST_F(AssemblerX86_64AVXTest, Vorps) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vorps,
-                      "vorps %{reg3}, %{reg2}, %{reg1}"), "vorps");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpor, "vpor %{reg3}, %{reg2}, %{reg1}"), "vpor");
 }
 
-TEST_F(AssemblerX86_64AVXTest, Vorpd) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vorpd,
-                      "vorpd %{reg3}, %{reg2}, %{reg1}"), "vorpd");
+TEST_F(AssemblerX86_64AVXTest, Por) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::por, "vpor %{reg2}, %{reg1}, %{reg1}"), "avx_por");
 }
 
 TEST_F(AssemblerX86_64Test, Pavgb) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pavgb, "pavgb %{reg2}, %{reg1}"), "pavgb");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPavgb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpavgb, "vpavgb %{reg3}, %{reg2}, %{reg1}"),
+            "vpavgb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pavgb) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pavgb, "vpavgb %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pavgb");
+}
+
 TEST_F(AssemblerX86_64Test, Pavgw) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pavgw, "pavgw %{reg2}, %{reg1}"), "pavgw");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPavgw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpavgw, "vpavgw %{reg3}, %{reg2}, %{reg1}"),
+            "vpavgw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pavgw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pavgw, "vpavgw %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pavgw");
+}
+
 TEST_F(AssemblerX86_64Test, Psadbw) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::psadbw, "psadbw %{reg2}, %{reg1}"), "psadbw");
 }
@@ -1895,8 +2312,13 @@ TEST_F(AssemblerX86_64Test, Pmaddwd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPmaddwd) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vpmaddwd,
-                      "vpmaddwd %{reg3}, %{reg2}, %{reg1}"), "vpmaddwd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmaddwd, "vpmaddwd %{reg3}, %{reg2}, %{reg1}"),
+            "vpmaddwd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pmaddwd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pmaddwd, "vpmaddwd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pmaddwd");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VFmadd213ss) {
@@ -1917,6 +2339,16 @@ TEST_F(AssemblerX86_64Test, Phaddd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::phaddd, "phaddd %{reg2}, %{reg1}"), "phaddd");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPhaddd) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vphaddd, "vphaddd %{reg3}, %{reg2}, %{reg1}"),
+            "vphaddd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Phaddd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::phaddd, "vphaddd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_phaddd");
+}
+
 TEST_F(AssemblerX86_64Test, Haddps) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::haddps, "haddps %{reg2}, %{reg1}"), "haddps");
 }
@@ -1945,70 +2377,240 @@ TEST_F(AssemblerX86_64Test, Pminsb) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pminsb, "pminsb %{reg2}, %{reg1}"), "pminsb");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPminsb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpminsb, "vpminsb %{reg3}, %{reg2}, %{reg1}"),
+            "vpminsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pminsb) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pminsb, "vpminsb %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pminsb");
+}
+
 TEST_F(AssemblerX86_64Test, Pmaxsb) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pmaxsb, "pmaxsb %{reg2}, %{reg1}"), "pmaxsb");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPmaxsb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmaxsb, "vpmaxsb %{reg3}, %{reg2}, %{reg1}"),
+            "vpmaxsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pmaxsb) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pmaxsb, "vpmaxsb %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pmaxsb");
+}
+
 TEST_F(AssemblerX86_64Test, Pminsw) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pminsw, "pminsw %{reg2}, %{reg1}"), "pminsw");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPminsw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpminsw, "vpminsw %{reg3}, %{reg2}, %{reg1}"),
+            "vpminsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pminsw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pminsw, "vpminsw %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pminsw");
+}
+
 TEST_F(AssemblerX86_64Test, Pmaxsw) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pmaxsw, "pmaxsw %{reg2}, %{reg1}"), "pmaxsw");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPmaxsw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmaxsw, "vpmaxsw %{reg3}, %{reg2}, %{reg1}"),
+            "vpmaxsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pmaxsw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pmaxsw, "vpmaxsw %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pmaxsw");
+}
+
 TEST_F(AssemblerX86_64Test, Pminsd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pminsd, "pminsd %{reg2}, %{reg1}"), "pminsd");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPminsd) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpminsd, "vpminsd %{reg3}, %{reg2}, %{reg1}"),
+            "vpminsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pminsd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pminsd, "vpminsd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pminsd");
+}
+
 TEST_F(AssemblerX86_64Test, Pmaxsd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pmaxsd, "pmaxsd %{reg2}, %{reg1}"), "pmaxsd");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPmaxsd) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmaxsd, "vpmaxsd %{reg3}, %{reg2}, %{reg1}"),
+            "vpmaxsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pmaxsd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pmaxsd, "vpmaxsd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pmaxsd");
+}
+
 TEST_F(AssemblerX86_64Test, Pminub) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pminub, "pminub %{reg2}, %{reg1}"), "pminub");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPminub) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpminub, "vpminub %{reg3}, %{reg2}, %{reg1}"),
+            "vpminub");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pminub) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pminub, "vpminub %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pminub");
+}
+
 TEST_F(AssemblerX86_64Test, Pmaxub) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pmaxub, "pmaxub %{reg2}, %{reg1}"), "pmaxub");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPmaxub) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmaxub, "vpmaxub %{reg3}, %{reg2}, %{reg1}"),
+            "vpmaxub");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pmaxub) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pmaxub, "vpmaxub %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pmaxub");
+}
+
 TEST_F(AssemblerX86_64Test, Pminuw) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pminuw, "pminuw %{reg2}, %{reg1}"), "pminuw");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPminuw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpminuw, "vpminuw %{reg3}, %{reg2}, %{reg1}"),
+            "vpminuw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pminuw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pminuw, "vpminuw %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pminuw");
+}
+
 TEST_F(AssemblerX86_64Test, Pmaxuw) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pmaxuw, "pmaxuw %{reg2}, %{reg1}"), "pmaxuw");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPmaxuw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmaxuw, "vpmaxuw %{reg3}, %{reg2}, %{reg1}"),
+            "vpmaxuw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pmaxuw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pmaxuw, "vpmaxuw %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pmaxuw");
+}
+
 TEST_F(AssemblerX86_64Test, Pminud) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pminud, "pminud %{reg2}, %{reg1}"), "pminud");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPminud) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpminud, "vpminud %{reg3}, %{reg2}, %{reg1}"),
+            "vpminud");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pminud) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pminud, "vpminud %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pminud");
+}
+
 TEST_F(AssemblerX86_64Test, Pmaxud) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pmaxud, "pmaxud %{reg2}, %{reg1}"), "pmaxud");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPmaxud) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmaxud, "vpmaxud %{reg3}, %{reg2}, %{reg1}"),
+            "vpmaxud");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pmaxud) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pmaxud, "vpmaxud %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pmaxud");
+}
+
 TEST_F(AssemblerX86_64Test, Minps) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::minps, "minps %{reg2}, %{reg1}"), "minps");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VMinps) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vminps, "vminps %{reg3}, %{reg2}, %{reg1}"),
+            "vminps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Minps) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::minps, "vminps %{reg2}, %{reg1}, %{reg1}"),
+            "avx_minps");
+}
+
 TEST_F(AssemblerX86_64Test, Maxps) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::maxps, "maxps %{reg2}, %{reg1}"), "maxps");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VMaxps) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vmaxps, "vmaxps %{reg3}, %{reg2}, %{reg1}"),
+            "vmaxps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Maxps) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::maxps, "vmaxps %{reg2}, %{reg1}, %{reg1}"),
+            "avx_maxps");
+}
+
 TEST_F(AssemblerX86_64Test, Minpd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::minpd, "minpd %{reg2}, %{reg1}"), "minpd");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VMinpd) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vminpd, "vminpd %{reg3}, %{reg2}, %{reg1}"),
+            "vminpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Minpd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::minpd, "vminpd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_minpd");
+}
+
 TEST_F(AssemblerX86_64Test, Maxpd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::maxpd, "maxpd %{reg2}, %{reg1}"), "maxpd");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VMaxpd) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vmaxpd, "vmaxpd %{reg3}, %{reg2}, %{reg1}"),
+            "vmaxpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Maxpd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::maxpd, "vmaxpd %{reg2}, %{reg1}, %{reg1}"),
+            "avx_maxpd");
+}
+
 TEST_F(AssemblerX86_64Test, PCmpeqb) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pcmpeqb, "pcmpeqb %{reg2}, %{reg1}"), "pcmpeqb");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPcmpeqb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpcmpeqb, "vpcmpeqb %{reg3}, %{reg2}, %{reg1}"),
+            "vpcmpeqb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pcmpeqb) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pcmpeqb, "vpcmpeqb %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pcmpeqb");
+}
+
 TEST_F(AssemblerX86_64Test, PCmpeqw) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pcmpeqw, "pcmpeqw %{reg2}, %{reg1}"), "pcmpeqw");
 }
@@ -2037,6 +2639,16 @@ TEST_F(AssemblerX86_64Test, PCmpgtq) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::pcmpgtq, "pcmpgtq %{reg2}, %{reg1}"), "pcmpgtq");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPcmpgtq) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpcmpgtq, "vpcmpgtq %{reg3}, %{reg2}, %{reg1}"),
+            "vpcmpgtq");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pcmpgtq) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pcmpgtq, "vpcmpgtq %{reg2}, %{reg1}, %{reg1}"),
+            "avx_pcmpgtq");
+}
+
 TEST_F(AssemblerX86_64Test, Shufps) {
   DriverStr(RepeatFFI(&x86_64::X86_64Assembler::shufps, /*imm_bytes*/ 1U,
                       "shufps ${imm}, %{reg2}, %{reg1}"), "shufps");
@@ -2057,6 +2669,16 @@ TEST_F(AssemblerX86_64Test, Punpcklbw) {
                      "punpcklbw %{reg2}, %{reg1}"), "punpcklbw");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPunpcklbw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpunpcklbw,
+                     "vpunpcklbw %{reg3}, %{reg2}, %{reg1}"), "vpunpcklbw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Punpcklbw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::punpcklbw,
+                     "vpunpcklbw %{reg2}, %{reg1}, %{reg1}"), "avx_punpcklbw");
+}
+
 TEST_F(AssemblerX86_64Test, Punpcklwd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::punpcklwd,
                      "punpcklwd %{reg2}, %{reg1}"), "punpcklwd");
@@ -2096,34 +2718,114 @@ TEST_F(AssemblerX86_64Test, Psllw) {
   DriverStr(RepeatFI(&x86_64::X86_64Assembler::psllw, 4u, "psllw ${imm}, %{reg}"), "psllwi");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPsllw) {
+  DriverStr(RepeatVVIb(&x86_64::X86_64Assembler::vpsllw, 4U, "vpsllw ${imm}, %{reg2}, %{reg1}"),
+            "vpsllw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psllw) {
+  DriverStr(RepeatVIb(&x86_64::X86_64Assembler::psllw, 4U, "vpsllw ${imm}, %{reg}, %{reg}"),
+            "avx_psllw");
+}
+
 TEST_F(AssemblerX86_64Test, Pslld) {
   DriverStr(RepeatFI(&x86_64::X86_64Assembler::pslld, 5u, "pslld ${imm}, %{reg}"), "pslldi");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPslld) {
+  DriverStr(RepeatVVIb(&x86_64::X86_64Assembler::vpslld, 5U, "vpslld ${imm}, %{reg2}, %{reg1}"),
+            "vpslld");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pslld) {
+  DriverStr(RepeatVIb(&x86_64::X86_64Assembler::pslld, 5U, "vpslld ${imm}, %{reg}, %{reg}"),
+            "avx_pslld");
+}
+
 TEST_F(AssemblerX86_64Test, Psllq) {
   DriverStr(RepeatFI(&x86_64::X86_64Assembler::psllq, 6u, "psllq ${imm}, %{reg}"), "psllqi");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPsllq) {
+  DriverStr(RepeatVVIb(&x86_64::X86_64Assembler::vpsllq, 6U, "vpsllq ${imm}, %{reg2}, %{reg1}"),
+            "vpsllq");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psllq) {
+  DriverStr(RepeatVIb(&x86_64::X86_64Assembler::psllq, 6U, "vpsllq ${imm}, %{reg}, %{reg}"),
+            "avx_psllq");
+}
+
 TEST_F(AssemblerX86_64Test, Psraw) {
   DriverStr(RepeatFI(&x86_64::X86_64Assembler::psraw, 4u, "psraw ${imm}, %{reg}"), "psrawi");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPsraw) {
+  DriverStr(RepeatVVIb(&x86_64::X86_64Assembler::vpsraw, 4U, "vpsraw ${imm}, %{reg2}, %{reg1}"),
+            "vpsraw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psraw) {
+  DriverStr(RepeatVIb(&x86_64::X86_64Assembler::psraw, 4U, "vpsraw ${imm}, %{reg}, %{reg}"),
+            "avx_psraw");
+}
+
 TEST_F(AssemblerX86_64Test, Psrad) {
   DriverStr(RepeatFI(&x86_64::X86_64Assembler::psrad, 5u, "psrad ${imm}, %{reg}"), "psradi");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPsrad) {
+  DriverStr(RepeatVVIb(&x86_64::X86_64Assembler::vpsrad, 5U, "vpsrad ${imm}, %{reg2}, %{reg1}"),
+            "vpsrad");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psrad) {
+  DriverStr(RepeatVIb(&x86_64::X86_64Assembler::psrad, 5U, "vpsrad ${imm}, %{reg}, %{reg}"),
+            "avx_psrad");
+}
+
 TEST_F(AssemblerX86_64Test, Psrlw) {
   DriverStr(RepeatFI(&x86_64::X86_64Assembler::psrlw, 4u, "psrlw ${imm}, %{reg}"), "psrlwi");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPsrlw) {
+  DriverStr(RepeatVVIb(&x86_64::X86_64Assembler::vpsrlw, 4U, "vpsrlw ${imm}, %{reg2}, %{reg1}"),
+            "vpsrlw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psrlw) {
+  DriverStr(RepeatVIb(&x86_64::X86_64Assembler::psrlw, 4U, "vpsrlw ${imm}, %{reg}, %{reg}"),
+            "avx_psrlw");
+}
+
 TEST_F(AssemblerX86_64Test, Psrld) {
   DriverStr(RepeatFI(&x86_64::X86_64Assembler::psrld, 5u, "psrld ${imm}, %{reg}"), "psrldi");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPsrld) {
+  DriverStr(RepeatVVIb(&x86_64::X86_64Assembler::vpsrld, 5U, "vpsrld ${imm}, %{reg2}, %{reg1}"),
+            "vpsrld");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psrld) {
+  DriverStr(RepeatVIb(&x86_64::X86_64Assembler::psrld, 5U, "vpsrld ${imm}, %{reg}, %{reg}"),
+            "avx_psrld");
+}
+
 TEST_F(AssemblerX86_64Test, Psrlq) {
   DriverStr(RepeatFI(&x86_64::X86_64Assembler::psrlq, 6u, "psrlq ${imm}, %{reg}"), "psrlqi");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPsrlq) {
+  DriverStr(RepeatVVIb(&x86_64::X86_64Assembler::vpsrlq, 6U, "vpsrlq ${imm}, %{reg2}, %{reg1}"),
+            "vpsrlq");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Psrlq) {
+  DriverStr(RepeatVIb(&x86_64::X86_64Assembler::psrlq, 6U, "vpsrlq ${imm}, %{reg}, %{reg}"),
+            "avx_psrlq");
+}
+
 TEST_F(AssemblerX86_64Test, Psrldq) {
   GetAssembler()->psrldq(x86_64::XmmRegister(x86_64::XMM0),  x86_64::Immediate(1));
   GetAssembler()->psrldq(x86_64::XmmRegister(x86_64::XMM15), x86_64::Immediate(2));
@@ -2131,6 +2833,522 @@ TEST_F(AssemblerX86_64Test, Psrldq) {
             "psrldq $2, %xmm15\n", "psrldqi");
 }
 
+TEST_F(AssemblerX86_64AVXTest, VPermpd) {
+  DriverStr(RepeatVVIb(&x86_64::X86_64Assembler::vpermpd, 5U, "vpermpd ${imm}, %{reg2}, %{reg1}"),
+            "vpermpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VBroadcastss) {
+  DriverStr(RepeatVF(&x86_64::X86_64Assembler::vbroadcastss, "vbroadcastss %{reg2}, %{reg1}"),
+            "vbroadcastss");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VBroadcastsd) {
+  DriverStr(RepeatVF(&x86_64::X86_64Assembler::vbroadcastsd, "vbroadcastsd %{reg2}, %{reg1}"),
+            "vbroadcastsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPbroadcastb) {
+  DriverStr(RepeatVF(&x86_64::X86_64Assembler::vpbroadcastb, "vpbroadcastb %{reg2}, %{reg1}"),
+            "vpbroadcastb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPbroadcastw) {
+  DriverStr(RepeatVF(&x86_64::X86_64Assembler::vpbroadcastw, "vpbroadcastw %{reg2}, %{reg1}"),
+            "vpbroadcastw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPbroadcastd) {
+  DriverStr(RepeatVF(&x86_64::X86_64Assembler::vpbroadcastd, "vpbroadcastd %{reg2}, %{reg1}"),
+            "vpbroadcastd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPbroadcastq) {
+  DriverStr(RepeatVF(&x86_64::X86_64Assembler::vpbroadcastq, "vpbroadcastq %{reg2}, %{reg1}"),
+            "vpbroadcastq");
+}
+
+TEST_F(AssemblerX86_64Test, Pabsb) {
+  DriverStr(RepeatFF(&x86_64::X86_64Assembler::pabsb, "pabsb %{reg2}, %{reg1}"), "pabsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPabsb) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vpabsb, "vpabsb %{reg2}, %{reg1}"), "vpabsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pabsb) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pabsb, "vpabsb %{reg2}, %{reg1}"), "avx_pabsb");
+}
+
+TEST_F(AssemblerX86_64Test, Pabsw) {
+  DriverStr(RepeatFF(&x86_64::X86_64Assembler::pabsw, "pabsw %{reg2}, %{reg1}"), "pabsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPabsw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vpabsw, "vpabsw %{reg2}, %{reg1}"), "vpabsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pabsw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pabsw, "vpabsw %{reg2}, %{reg1}"), "avx_pabsw");
+}
+
+TEST_F(AssemblerX86_64Test, Pabsd) {
+  DriverStr(RepeatFF(&x86_64::X86_64Assembler::pabsd, "pabsd %{reg2}, %{reg1}"), "pabsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPabsd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vpabsd, "vpabsd %{reg2}, %{reg1}"), "vpabsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pabsd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::pabsd, "vpabsd %{reg2}, %{reg1}"), "avx_pabsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VZeroupper) {
+  GetAssembler()->vzeroupper();
+  const char* expected = "vzeroupper\n";
+  DriverStr(expected, "vzeroupper");
+}
+
+// Disassembler tests
+TEST_F(AssemblerX86_64Test, DisassMovaps) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFF(&x86_64::X86_64Assembler::movaps, "movaps {reg1}, {reg2}"),
+                  "disass-movaps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovaps) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::vmovaps, "vmovaps {reg1}, {reg2}"),
+                  "disass-vmovaps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassMovaps) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::movaps, "vmovaps {reg1}, {reg2}"),
+                  "disass-avx_movaps");
+}
+
+// TODO: Disassembler tests cannot handle memory right now because of difference in
+//       the format between ART disassembly and the combination generator
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovapsStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::vmovaps,
+//                   "vmovaps {mem}, {reg}"), "disass-vmovaps_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovapsStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::movaps,
+//                   "vmovaps {mem}, {reg}"), "disass-avx_movaps_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovapsLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::vmovaps,
+//                   "vmovaps {reg}, {mem}"), "disass-vmovaps_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovapsLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::movaps,
+//                   "vmovaps {reg}, {mem}"), "disass-avx_movaps_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovupsStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::vmovups,
+//                   "vmovups {mem}, {reg}"), "disass-vmovups_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovupsStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::movups,
+//                   "vmovups {mem}, {reg}"), "disass-avx_movups_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovupsLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::vmovups,
+//                   "vmovups {reg}, {mem}"), "disass-vmovups_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovupsLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::movups,
+//                   "vmovups {reg}, {mem}"), "disass-avx_movups_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovssLoad){
+//    CustomDriverStr(VerifyDisassemblerDriver, RepeatFA(&x86_64::X86_64Assembler::vmovss,
+//                    "vmovss {reg}, {mem}"), "disass-vmovss_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovssStore){
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAF(&x86_64::X86_64Assembler::vmovss,
+//                   "vmovss {mem}, {reg}"), "disass-vmovss_s");
+// }
+
+TEST_F(AssemblerX86_64Test, DisassMovss) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFF(&x86_64::X86_64Assembler::movss, "movss {reg1}, {reg2}"),
+                  "disass-movss");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovss) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFFF(&x86_64::X86_64Assembler::vmovss, "vmovss {reg1}, {reg2}, {reg3}"),
+                  "disass-vmovss");
+}
+
+TEST_F(AssemblerX86_64Test, DisassMovapd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFF(&x86_64::X86_64Assembler::movapd, "movapd {reg1}, {reg2}"),
+                  "disass-movapd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovapd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::vmovapd, "vmovapd {reg1}, {reg2}"),
+                  "disass-vmovapd");
+}
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovapdStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::vmovapd,
+//                   "vmovapd {mem}, {reg}"), "disass-vmovapd_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovapdStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::movapd,
+//                   "vmovapd {mem}, {reg}"), "disass-avx_movapd_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovapdLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::vmovapd,
+//                   "vmovapd {reg}, {mem}"), "disass-vmovapd_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovapdLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::movapd,
+//                   "vmovapd {reg}, {mem}"), "disass-avx_movapd_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovupdStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::vmovupd,
+//                   "vmovupd {mem}, {reg}"), "disass-vmovupd_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovupdStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::movupd,
+//                   "vmovupd {mem}, {reg}"), "disass-avx_movupd_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovupdLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::vmovupd,
+//                   "vmovupd {reg}, {mem}"), "disass-vmovupd_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovupdLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::movupd,
+//                   "vmovupd {reg}, {mem}"), "disass-avx_movupd_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovsdLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatFA(&x86_64::X86_64Assembler::vmovsd,
+//                   "vmovsd {reg}, {mem}"), "disass-vmovsd_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovsdStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAF(&x86_64::X86_64Assembler::vmovsd,
+//                   "vmovsd {mem}, {reg}"), "disass-vmovsd_s");
+// }
+
+TEST_F(AssemblerX86_64Test, DisassMovsd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFF(&x86_64::X86_64Assembler::movsd, "movsd {reg1}, {reg2}"),
+                  "disass-movsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovsd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFFF(&x86_64::X86_64Assembler::vmovsd, "vmovsd {reg1}, {reg2}, {reg3}"),
+                  "disass-vmovsd");
+}
+
+TEST_F(AssemblerX86_64Test, DisassMovdqa) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFF(&x86_64::X86_64Assembler::movdqa, "movdqa {reg1}, {reg2}"),
+                  "disass-movdqa");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovdqa) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::vmovdqa, "vmovdqa {reg1}, {reg2}"),
+                  "disass-vmovdqa");
+}
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovdqaStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::vmovdqa,
+//                   "vmovdqa {mem}, {reg}"), "disass-vmovdqa_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovdqaStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::movdqa,
+//                   "vmovdqa {mem}, {reg}"), "disass-avx_movdqa_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovdqaLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::vmovdqa,
+//                   "vmovdqa {reg}, {mem}"), "disass-vmovdqa_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovdqaLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::movdqa,
+//                   "vmovdqa {reg}, {mem}"), "disass-avx_movdqa_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovdquStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::vmovdqu,
+//                   "vmovdqu {mem}, {reg}"), "disass-vmovdqu_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovdquStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::movdqu,
+//                   "vmovdqu {mem}, {reg}"), "disass-avx_movdqu_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovdquLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::vmovdqu,
+//                   "vmovdqu {reg}, {mem}"), "disass-vmovdqu_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovdquLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::movdqu,
+//                   "vmovdqu {reg}, {mem}"), "disass-avx_movdqu_l");
+// }
+
+TEST_F(AssemblerX86_64Test, DisassMovq1) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFR(&x86_64::X86_64Assembler::movq, "movq {reg1}, {reg2}"),
+                  "disass-movq.1");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovq1) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFR(&x86_64::X86_64Assembler::vmovq, "vmovq {reg1}, {reg2}"),
+                  "disass-vmovq.1");
+}
+
+TEST_F(AssemblerX86_64Test, DisassMovq2) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatRF(&x86_64::X86_64Assembler::movq, "movq {reg1}, {reg2}"),
+                  "disass-movq.2");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovq2) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatRF(&x86_64::X86_64Assembler::vmovq, "vmovq {reg1}, {reg2}"),
+                  "disass-vmovq.2");
+}
+
+TEST_F(AssemblerX86_64Test, DisassMovd1) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFr(&x86_64::X86_64Assembler::movd, "movd {reg1}, {reg2}"),
+                  "disass-movd.1");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovd1) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFr(&x86_64::X86_64Assembler::vmovd, "vmovd {reg1}, {reg2}"),
+                  "disass-vmovd.1");
+}
+
+TEST_F(AssemblerX86_64Test, DisassMovd2) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatrF(&x86_64::X86_64Assembler::movd, "movd {reg1}, {reg2}"),
+                  "disass-movd.2");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovd2) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatrF(&x86_64::X86_64Assembler::vmovd, "vmovd {reg1}, {reg2}"),
+                  "disass-vmovd.2");
+}
+
+// TODO: Disassembler tests cannot handle commutative instructions like vaddps
+//       Hence no disassembler tests for add, mul, avg, xor, or, and etc.
+//       Also, some integral min/max operations are not treated as commutative because
+//       it does not affect the encoding length and therefore they can be tested here.
+
+#define DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(inst)                                      \
+  TEST_F(AssemblerX86_64Test, Disass##inst) {                                              \
+    CustomDriverStr(VerifyDisassemblerDriver,                                              \
+                    RepeatFF(&x86_64::X86_64Assembler::inst, #inst " {reg1}, {reg2}"),     \
+                    "disass-" #inst);                                                      \
+  }                                                                                        \
+  TEST_F(AssemblerX86_64AVXTest, DisassV##inst) {                                          \
+    CustomDriverStr(                                                                       \
+        VerifyDisassemblerDriver,                                                          \
+        RepeatVVV(&x86_64::X86_64Assembler::v##inst, "v" #inst " {reg1}, {reg2}, {reg3}"), \
+        "disass-v" #inst);                                                                 \
+  }
+
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(subps);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(subpd);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(divps);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(divpd);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(psubb);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(psubw);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(psubd);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pmulld);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(psubq);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(psubusb);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(psubsb);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(psubusw);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(psubsw);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(andnpd);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(andnps);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pandn);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(phaddd);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pminsb);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pmaxsb);
+// Cannot test commutative operations
+// DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pminsw);
+// DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pmaxsw);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pminsd);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pmaxsd);
+// Cannot test commutative operations
+// DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pminub);
+// DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pmaxub);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pminuw);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pmaxuw);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pminud);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pmaxud);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(minps);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(maxps);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(minpd);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(maxpd);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(pcmpgtq);
+DISASSEMBLER_TEST_INSTR_WITH_3_VEC_REGS(punpcklbw);
+
+TEST_F(AssemblerX86_64Test, DisassCvtdq2ps) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFF(&x86_64::X86_64Assembler::cvtdq2ps, "cvtdq2ps {reg1}, {reg2}"),
+                  "disass-cvtdq2ps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVCvtdq2ps) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::vcvtdq2ps, "vcvtdq2ps {reg1}, {reg2}"),
+                  "disass-vcvtdq2ps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVFmadd213ss) {
+  CustomDriverStr(
+      VerifyDisassemblerDriver,
+      RepeatFFF(&x86_64::X86_64Assembler::vfmadd213ss, "vfmadd213ss {reg1}, {reg2}, {reg3}"),
+      "disass-vfmadd213ss");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVFmadd213sd) {
+  CustomDriverStr(
+      VerifyDisassemblerDriver,
+      RepeatFFF(&x86_64::X86_64Assembler::vfmadd213sd, "vfmadd213sd {reg1}, {reg2}, {reg3}"),
+      "disass-vfmadd213sd");
+}
+
+#define DISASSEMBLER_TEST_SHIFT_INSTR(inst, bits)                                                \
+  TEST_F(AssemblerX86_64Test, Disass##inst) {                                                    \
+    CustomDriverStr(VerifyDisassemblerDriver,                                                    \
+                    RepeatFI(&x86_64::X86_64Assembler::inst, bits, #inst " {reg}, {imm}"),       \
+                    "disass-" #inst);                                                            \
+  }                                                                                              \
+  TEST_F(AssemblerX86_64AVXTest, DisassV##inst) {                                                \
+    CustomDriverStr(                                                                             \
+        VerifyDisassemblerDriver,                                                                \
+        RepeatVVIb(&x86_64::X86_64Assembler::v##inst, bits, "v" #inst " {reg1}, {reg2}, {imm}"), \
+        "disass-v" #inst);                                                                       \
+  }
+
+DISASSEMBLER_TEST_SHIFT_INSTR(psllw, 4U);
+DISASSEMBLER_TEST_SHIFT_INSTR(pslld, 5U);
+DISASSEMBLER_TEST_SHIFT_INSTR(psllq, 6U);
+DISASSEMBLER_TEST_SHIFT_INSTR(psraw, 4U);
+DISASSEMBLER_TEST_SHIFT_INSTR(psrad, 5U);
+DISASSEMBLER_TEST_SHIFT_INSTR(psrlw, 4U);
+DISASSEMBLER_TEST_SHIFT_INSTR(psrld, 5U);
+DISASSEMBLER_TEST_SHIFT_INSTR(psrlq, 6U);
+
+TEST_F(AssemblerX86_64AVXTest, DisassVBroadcastss) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVF(&x86_64::X86_64Assembler::vbroadcastss, "vbroadcastss {reg1}, {reg2}"),
+                  "disass-vbroadcastss");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVBroadcastsd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVF(&x86_64::X86_64Assembler::vbroadcastsd, "vbroadcastsd {reg1}, {reg2}"),
+                  "disass-vbroadcastsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPbroadcastb) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVF(&x86_64::X86_64Assembler::vpbroadcastb, "vpbroadcastb {reg1}, {reg2}"),
+                  "disass-vpbroadcastb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPbroadcastw) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVF(&x86_64::X86_64Assembler::vpbroadcastw, "vpbroadcastw {reg1}, {reg2}"),
+                  "disass-vpbroadcastw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPbroadcastd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVF(&x86_64::X86_64Assembler::vpbroadcastd, "vpbroadcastd {reg1}, {reg2}"),
+                  "disass-vpbroadcastd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPbroadcastq) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVF(&x86_64::X86_64Assembler::vpbroadcastq, "vpbroadcastq {reg1}, {reg2}"),
+                  "disass-vpbroadcastq");
+}
+
+TEST_F(AssemblerX86_64Test, DisassPabsb) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFF(&x86_64::X86_64Assembler::pabsb, "pabsb {reg1}, {reg2}"),
+                  "disass-pabsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPabsb) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::vpabsb, "vpabsb {reg1}, {reg2}"),
+                  "disass-vpabsb");
+}
+
+TEST_F(AssemblerX86_64Test, DisassPabsw) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFF(&x86_64::X86_64Assembler::pabsw, "pabsw {reg1}, {reg2}"),
+                  "disass-pabsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPabsw) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::vpabsw, "vpabsw {reg1}, {reg2}"),
+                  "disass-vpabsw");
+}
+
+TEST_F(AssemblerX86_64Test, DisassPabsd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFF(&x86_64::X86_64Assembler::pabsd, "pabsd {reg1}, {reg2}"),
+                  "disass-pabsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPabsd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::vpabsd, "vpabsd {reg1}, {reg2}"),
+                  "disass-vpabsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVZeroupper) {
+  GetAssembler()->vzeroupper();
+  const char* expected = "vzeroupper\n";
+  CustomDriverStr(VerifyDisassemblerDriver, expected, "disass-vzeroupper");
+}
+
 std::string x87_fn([[maybe_unused]] AssemblerX86_64Test::Base* assembler_test,
                    x86_64::X86_64Assembler* assembler) {
   std::ostringstream str;
diff --git a/compiler/utils/x86_64/constants_x86_64.h b/compiler/utils/x86_64/constants_x86_64.h
index 52ac987766..2b883c38ae 100644
--- a/compiler/utils/x86_64/constants_x86_64.h
+++ b/compiler/utils/x86_64/constants_x86_64.h
@@ -51,8 +51,12 @@ std::ostream& operator<<(std::ostream& os, const CpuRegister& reg);
 
 class XmmRegister {
  public:
-  explicit constexpr XmmRegister(FloatRegister r) : reg_(r) {}
-  explicit constexpr XmmRegister(int r) : reg_(FloatRegister(r)) {}
+  constexpr XmmRegister(FloatRegister r, size_t vector_length)
+      : reg_(r), vector_length_(vector_length) {}
+  constexpr XmmRegister(int r, size_t vector_length)
+      : XmmRegister(FloatRegister(r), vector_length) {}
+  explicit constexpr XmmRegister(FloatRegister r) : XmmRegister(r, 0) {}
+  explicit constexpr XmmRegister(int r) : XmmRegister(FloatRegister(r), 0) {}
   constexpr FloatRegister AsFloatRegister() const {
     return reg_;
   }
@@ -65,8 +69,18 @@ class XmmRegister {
   bool operator==(const XmmRegister& other) const {
     return reg_ == other.reg_;
   }
+  size_t GetVecLen() const { return vector_length_; }
+  bool IsYMM() const {
+    return vector_length_ == 32;  // 256-bit
+  }
+
  private:
   const FloatRegister reg_;
+  // TODO: Since the valid values for vector_length is a limited set 16/32,
+  //       consider using an enum. This enum may have to be arch agnostic
+  //       and visible to Locations, as the register object will eventually be
+  //       created there.
+  const size_t vector_length_;
 };
 std::ostream& operator<<(std::ostream& os, const XmmRegister& reg);
 
diff --git a/dalvikvm/Android.bp b/dalvikvm/Android.bp
index 026689ed6c..b2ba6f41e7 100644
--- a/dalvikvm/Android.bp
+++ b/dalvikvm/Android.bp
@@ -40,9 +40,6 @@ art_cc_binary {
     ],
     target: {
         android: {
-            header_libs: [
-                "libnativeloader-headers",
-            ],
             shared_libs: [
                 "libsigchain",
                 "libdl_android",
diff --git a/dalvikvm/dalvikvm.cc b/dalvikvm/dalvikvm.cc
index ebdcef1775..6e3e65df16 100644
--- a/dalvikvm/dalvikvm.cc
+++ b/dalvikvm/dalvikvm.cc
@@ -30,7 +30,7 @@
 #include "nativehelper/toStringArray.h"
 
 #ifdef ART_TARGET_ANDROID
-#include "nativeloader/dlext_namespaces.h"
+#include <bionic/dlext_namespaces.h>
 #endif
 
 namespace art {
diff --git a/dex2oat/Android.bp b/dex2oat/Android.bp
index f97aef2deb..8f77b7ccca 100644
--- a/dex2oat/Android.bp
+++ b/dex2oat/Android.bp
@@ -102,12 +102,13 @@ gensrcs {
 
 cc_defaults {
     name: "libart-dex2oat_static_base_defaults",
+    defaults: [
+        "art_liblog_static_defaults",
+        "art_libz_static_defaults",
+    ],
     whole_static_libs: [
         "libbase",
-        "libcrypto_static", // Not FIPS tested - for SHA-1 checksumming of build ID only.
-        "liblog",
         "liblz4",
-        "libz",
     ],
 }
 
@@ -140,6 +141,18 @@ cc_defaults {
     ],
 }
 
+cc_defaults {
+    name: "libart-dex2oat-for-test_static_defaults",
+    defaults: [
+        "libart-dex2oat_static_base_defaults",
+        "libart-for-test_static_defaults",
+        "libprofile_static_defaults",
+    ],
+    whole_static_libs: [
+        "libart-dex2oat",
+    ],
+}
+
 // Collect all the static defaults and build a host-only static library, which
 // is then used for the (mostly) static host dex2oat binary.
 art_cc_library_static {
@@ -187,6 +200,18 @@ cc_defaults {
     ],
 }
 
+cc_defaults {
+    name: "libartd-dex2oat-for-test_static_defaults",
+    defaults: [
+        "libart-dex2oat_static_base_defaults",
+        "libartd-for-test_static_defaults",
+        "libprofiled_static_defaults",
+    ],
+    whole_static_libs: [
+        "libartd-dex2oat",
+    ],
+}
+
 art_cc_library_static {
     name: "libdex2oatd_static",
     device_supported: false,
@@ -279,6 +304,9 @@ art_cc_binary {
             // it'd get overridden by the load hook even when it uses
             // PrependProperties.
             compile_multilib: "64",
+            // libz is a libdex2oat_static transitive dependency from art_libz_static_defaults,
+            // duplicated here because we cannot import *_static_defaults only for host.
+            shared_libs: ["libz"],
         },
     },
 
@@ -323,12 +351,13 @@ art_cc_binary {
             ],
         },
         host: {
-            // Comments for host in dex2oat apply here too.
+            // Comments for host in the dex2oat binary apply here too.
             static_libs: [
                 "libdex2oatd_static",
             ],
             stl: "c++_static",
             compile_multilib: "64",
+            shared_libs: ["libz"],
         },
     },
 
@@ -345,6 +374,11 @@ cc_defaults {
         "dex2oat-defaults",
     ],
     target: {
+        host: {
+            // libz is normally linked dynamically on host in
+            // art_libz_static_defaults, so need to bring it in statically here.
+            static_libs: ["libz"],
+        },
         darwin: {
             enabled: false,
         },
@@ -379,35 +413,6 @@ art_cc_binary {
     ],
 }
 
-art_cc_library_static {
-    name: "libart-dex2oat-gtest",
-    defaults: ["libart-gtest-defaults"],
-    srcs: [
-        "common_compiler_driver_test.cc",
-        "common_transaction_test.cc",
-    ],
-    static_libs: [
-        "libart-dex2oat",
-        "libart-gtest",
-    ],
-}
-
-art_cc_library_static {
-    name: "libartd-dex2oat-gtest",
-    defaults: [
-        "art_debug_defaults",
-        "libart-gtest-defaults",
-    ],
-    srcs: [
-        "common_compiler_driver_test.cc",
-        "common_transaction_test.cc",
-    ],
-    static_libs: [
-        "libartd-dex2oat",
-        "libartd-gtest",
-    ],
-}
-
 art_cc_defaults {
     name: "art_dex2oat_tests_defaults",
     defaults: [
@@ -452,6 +457,8 @@ art_cc_defaults {
         "linker/arm64/relative_patcher_arm64_test.cc",
     ],
     srcs: [
+        "common_compiler_driver_test.cc",
+        "common_transaction_test.cc",
         "dex2oat_test.cc",
         "dex2oat_vdex_test.cc",
         "dex2oat_image_test.cc",
@@ -514,7 +521,6 @@ art_cc_defaults {
     static_libs: [
         "libelf",
         "libgmock",
-        "liblz4", // libart(d)-dex2oat dependency; must be repeated here since it's a static lib.
     ],
 }
 
@@ -525,11 +531,7 @@ art_cc_test {
     defaults: [
         "art_gtest_defaults",
         "art_dex2oat_tests_defaults",
-    ],
-    static_libs: [
-        "libartd-dex2oat",
-        "libartd-dex2oat-gtest",
-        "libvixld",
+        "libartd-dex2oat-for-test_static_defaults",
     ],
 }
 
@@ -539,13 +541,9 @@ art_cc_test {
     defaults: [
         "art_standalone_gtest_defaults",
         "art_dex2oat_tests_defaults",
+        "libart-dex2oat-for-test_static_defaults",
     ],
     data: [":generate-boot-image"],
-    static_libs: [
-        "libart-dex2oat",
-        "libart-dex2oat-gtest",
-        "libvixl",
-    ],
     test_config: "art_standalone_dex2oat_tests.xml",
 }
 
diff --git a/dex2oat/aot_class_linker.cc b/dex2oat/aot_class_linker.cc
index 6efddb2f42..b6eca10eb0 100644
--- a/dex2oat/aot_class_linker.cc
+++ b/dex2oat/aot_class_linker.cc
@@ -220,10 +220,12 @@ bool AotClassLinker::CanReferenceInBootImageExtensionOrAppImage(
     PointerSize pointer_size = Runtime::Current()->GetClassLinker()->GetImagePointerSize();
     ObjPtr<mirror::Class> k = klass;
     while (!heap->ObjectIsInBootImageSpace(k)) {
-      for (auto& m : k->GetVirtualMethods(pointer_size)) {
-        ObjPtr<mirror::Class> declaring_class = m.GetDeclaringClass();
-        CHECK(heap->ObjectIsInBootImageSpace(declaring_class) ||
-              can_reference_dex_cache(declaring_class->GetDexCache()));
+      for (auto& m : k->GetMethods(pointer_size)) {
+        if (m.IsVirtual()) {
+          ObjPtr<mirror::Class> declaring_class = m.GetDeclaringClass();
+          CHECK(heap->ObjectIsInBootImageSpace(declaring_class) ||
+                can_reference_dex_cache(declaring_class->GetDexCache()));
+        }
       }
       k = k->GetSuperClass();
     }
diff --git a/dex2oat/dex2oat.cc b/dex2oat/dex2oat.cc
index 5586fafb47..cd91f5a0e6 100644
--- a/dex2oat/dex2oat.cc
+++ b/dex2oat/dex2oat.cc
@@ -974,6 +974,10 @@ class Dex2Oat final {
     key_value_store_->Put(OatHeader::kCompilerFilter,
                           CompilerFilter::NameOfFilter(compiler_options_->GetCompilerFilter()));
     key_value_store_->Put(OatHeader::kConcurrentCopying, compiler_options_->EmitReadBarrier());
+    if (compiler_options_->GetAssumeValueOptions().HasValidSdkInt()) {
+      key_value_store_->Put(OatHeader::kAssumeValueSdkIntKey,
+                            std::to_string(compiler_options_->GetAssumeValueOptions().SdkInt()));
+    }
     if (invocation_file_.get() != -1) {
       std::ostringstream oss;
       for (int i = 0; i < argc; ++i) {
diff --git a/dex2oat/dex2oat_test.cc b/dex2oat/dex2oat_test.cc
index 31891fd2b5..c11e596ff4 100644
--- a/dex2oat/dex2oat_test.cc
+++ b/dex2oat/dex2oat_test.cc
@@ -35,6 +35,7 @@
 #include "base/mutex-inl.h"
 #include "base/utils.h"
 #include "base/zip_archive.h"
+#include "com_android_art_flags.h"
 #include "common_runtime_test.h"
 #include "dex/art_dex_file_loader.h"
 #include "dex/base64_test_util.h"
@@ -2093,4 +2094,36 @@ TEST_F(Dex2oatTest, LoadOutOfDateOatFile) {
   }
 }
 
+TEST_F(Dex2oatTest, AssumedValuesPropagateToOatHeader) {
+  std::string dex_location = GetScratchDir() + "/AssumedValuesPropagateToOatHeader.jar";
+  std::string odex_location = GetOdexDir() + "/AssumedValuesPropagateToOatHeader.odex";
+  Copy(GetDexSrc1(), dex_location);
+
+  std::vector<std::string> extra_args{
+      "--assume-value=Landroid/os/Build$VERSION;->SDK_INT:77",
+  };
+  ASSERT_TRUE(GenerateOdexForTest(dex_location,
+                                  odex_location,
+                                  CompilerFilter::kVerify,
+                                  extra_args,
+                                  /*expect_status=*/Status::kSuccess));
+
+  std::string error_msg;
+  std::unique_ptr<OatFile> odex_file(OatFile::Open(/*zip_fd=*/-1,
+                                                   odex_location,
+                                                   odex_location,
+                                                   /*executable=*/false,
+                                                   /*low_4gb=*/false,
+                                                   dex_location,
+                                                   &error_msg));
+  ASSERT_TRUE(odex_file != nullptr);
+  // The assumed SDK_INT arg should only propagate when the feature flag is enabled.
+  if (com::android::art::flags::compile_sdk_int_constant()) {
+    ASSERT_TRUE(odex_file->GetOatHeader().HasAssumeValueSdkInt());
+    EXPECT_EQ(odex_file->GetOatHeader().GetAssumeValueSdkInt(), 77);
+  } else {
+    ASSERT_FALSE(odex_file->GetOatHeader().HasAssumeValueSdkInt());
+  }
+}
+
 }  // namespace art
diff --git a/dex2oat/driver/compiler_driver.cc b/dex2oat/driver/compiler_driver.cc
index 7f4ec13f1b..4de8cd06f9 100644
--- a/dex2oat/driver/compiler_driver.cc
+++ b/dex2oat/driver/compiler_driver.cc
@@ -16,11 +16,8 @@
 
 #include "compiler_driver.h"
 
-#include <unistd.h>
-
-#ifndef __APPLE__
 #include <malloc.h>  // For mallinfo
-#endif
+#include <unistd.h>
 
 #include <string_view>
 #include <vector>
@@ -952,7 +949,7 @@ class ResolveCatchBlockExceptionsClassVisitor : public ClassVisitor {
     }
     // Filter out classes without methods.
     // These include primitive types and array types which have no dex file.
-    if (c->GetMethodsPtr() == nullptr) {
+    if (c->GetMethodsPtr()->size() == 0u) {
       return true;
     }
     auto it = dex_file_records_.find(&c->GetDexFile());
@@ -1315,7 +1312,7 @@ static void MaybeAddToImageClasses(Thread* self,
       DCHECK(interface != nullptr);
       MaybeAddToImageClasses(self, interface, image_classes);
     }
-    for (auto& m : klass->GetVirtualMethods(pointer_size)) {
+    for (auto& m : klass->GetCopiedMethods(pointer_size)) {
       MaybeAddToImageClasses(self, m.GetDeclaringClass(), image_classes);
     }
     if (klass->IsArrayClass()) {
@@ -2522,12 +2519,13 @@ class InitializeClassVisitor : public CompilationVisitor {
       for (int32_t i = 0; i < klass->GetIfTableCount(); ++i) {
         super_klass.Assign(klass->GetIfTable()->GetInterface(i));
         if (klass->GetClassLoader() != super_klass->GetClassLoader()) {
-          uint32_t num_methods = super_klass->NumVirtualMethods();
-          for (uint32_t j = 0; j < num_methods; ++j) {
+          for (ArtMethod& super_m : super_klass->GetMethods(pointer_size)) {
+            if (!super_m.IsVirtual()) {
+              continue;
+            }
             ArtMethod* m = klass->GetIfTable()->GetMethodArray(i)->GetElementPtrSize<ArtMethod*>(
-                j, pointer_size);
-            ArtMethod* super_m = super_klass->GetVirtualMethod(j, pointer_size);
-            if (!ResolveTypesOfMethods(self, m) || !ResolveTypesOfMethods(self, super_m)) {
+                super_m.GetMethodIndex(), pointer_size);
+            if (!ResolveTypesOfMethods(self, m) || !ResolveTypesOfMethods(self, &super_m)) {
               return false;
             }
           }
@@ -2873,13 +2871,11 @@ std::string CompilerDriver::GetMemoryUsageString(bool extended) const {
   const size_t java_alloc = heap->GetBytesAllocated();
   oss << "arena alloc=" << PrettySize(max_arena_alloc_) << " (" << max_arena_alloc_ << "B)";
   oss << " java alloc=" << PrettySize(java_alloc) << " (" << java_alloc << "B)";
-#if defined(__BIONIC__) || defined(__GLIBC__) || defined(ANDROID_HOST_MUSL)
   const struct mallinfo info = mallinfo();
   const size_t allocated_space = static_cast<size_t>(info.uordblks);
   const size_t free_space = static_cast<size_t>(info.fordblks);
   oss << " native alloc=" << PrettySize(allocated_space) << " (" << allocated_space << "B)"
       << " free=" << PrettySize(free_space) << " (" << free_space << "B)";
-#endif
   compiled_method_storage_.DumpMemoryUsage(oss, extended);
   return oss.str();
 }
diff --git a/dex2oat/driver/compiler_driver_test.cc b/dex2oat/driver/compiler_driver_test.cc
index 5dc6589d53..6f3da8f804 100644
--- a/dex2oat/driver/compiler_driver_test.cc
+++ b/dex2oat/driver/compiler_driver_test.cc
@@ -247,15 +247,17 @@ class CompilerDriverProfileTest : public CompilerDriverTest {
 
     const auto pointer_size = class_linker->GetImagePointerSize();
     size_t number_of_compiled_methods = 0;
-    for (auto& m : klass->GetVirtualMethods(pointer_size)) {
-      std::string name = m.PrettyMethod(true);
-      const void* code = m.GetEntryPointFromQuickCompiledCodePtrSize(pointer_size);
-      ASSERT_NE(code, nullptr);
-      if (expected_methods.find(name) != expected_methods.end()) {
-        number_of_compiled_methods++;
-        EXPECT_FALSE(class_linker->IsQuickToInterpreterBridge(code));
-      } else {
-        EXPECT_TRUE(class_linker->IsQuickToInterpreterBridge(code));
+    for (auto& m : klass->GetMethods(pointer_size)) {
+      if (m.IsVirtual()) {
+        std::string name = m.PrettyMethod(true);
+        const void* code = m.GetEntryPointFromQuickCompiledCodePtrSize(pointer_size);
+        ASSERT_NE(code, nullptr);
+        if (expected_methods.find(name) != expected_methods.end()) {
+          number_of_compiled_methods++;
+          EXPECT_FALSE(class_linker->IsQuickToInterpreterBridge(code));
+        } else {
+          EXPECT_TRUE(class_linker->IsQuickToInterpreterBridge(code));
+        }
       }
     }
     EXPECT_EQ(expected_methods.size(), number_of_compiled_methods);
diff --git a/dex2oat/linker/arm/relative_patcher_thumb2.cc b/dex2oat/linker/arm/relative_patcher_thumb2.cc
index 45a4e8b061..4244bf55f0 100644
--- a/dex2oat/linker/arm/relative_patcher_thumb2.cc
+++ b/dex2oat/linker/arm/relative_patcher_thumb2.cc
@@ -21,7 +21,6 @@
 #include "arch/arm/asm_support_arm.h"
 #include "art_method.h"
 #include "base/bit_utils.h"
-#include "base/malloc_arena_pool.h"
 #include "driver/compiled_method.h"
 #include "entrypoints/quick/quick_entrypoints_enum.h"
 #include "linker/linker_patch.h"
diff --git a/dex2oat/linker/arm64/relative_patcher_arm64.cc b/dex2oat/linker/arm64/relative_patcher_arm64.cc
index 79fcd18b00..738c45d69e 100644
--- a/dex2oat/linker/arm64/relative_patcher_arm64.cc
+++ b/dex2oat/linker/arm64/relative_patcher_arm64.cc
@@ -20,7 +20,6 @@
 #include "arch/arm64/instruction_set_features_arm64.h"
 #include "art_method.h"
 #include "base/bit_utils.h"
-#include "base/malloc_arena_pool.h"
 #include "driver/compiled_method-inl.h"
 #include "driver/compiler_driver.h"
 #include "entrypoints/quick/quick_entrypoints_enum.h"
diff --git a/dex2oat/linker/code_info_table_deduper_test.cc b/dex2oat/linker/code_info_table_deduper_test.cc
index 54b7dd5940..771231a8db 100644
--- a/dex2oat/linker/code_info_table_deduper_test.cc
+++ b/dex2oat/linker/code_info_table_deduper_test.cc
@@ -19,7 +19,7 @@
 #include "code_info_table_deduper.h"
 
 #include "arch/instruction_set.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "base/scoped_arena_allocator.h"
 #include "base/scoped_arena_containers.h"
 #include "optimizing/stack_map_stream.h"
@@ -31,7 +31,7 @@ TEST(StackMapTest, TestDedupeBitTables) {
   constexpr static uint32_t kPcAlign = GetInstructionSetInstructionAlignment(kRuntimeISA);
   using Kind = DexRegisterLocation::Kind;
 
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
   StackMapStream stream(&allocator, kRuntimeISA);
diff --git a/dex2oat/linker/image_writer.cc b/dex2oat/linker/image_writer.cc
index 04bf4833cd..a96636f875 100644
--- a/dex2oat/linker/image_writer.cc
+++ b/dex2oat/linker/image_writer.cc
@@ -1406,11 +1406,9 @@ void ImageWriter::RecordNativeRelocations(ObjPtr<mirror::Class> klass, size_t oa
   }
   ImageInfo& image_info = GetImageInfo(oat_index);
   LengthPrefixedArray<ArtField>* fields = klass->GetFieldsPtr();
-  // Total array length including header.
-  if (fields != nullptr) {
+  if (!NativeRelocationAssigned(fields) && !IsInBootImage(fields)) {
     // Forward the entire array at once.
     size_t offset = image_info.GetBinSlotSize(Bin::kArtField);
-    DCHECK(!IsInBootImage(fields));
     bool inserted =
         native_object_relocations_.insert(std::make_pair(
             fields,
@@ -1418,14 +1416,16 @@ void ImageWriter::RecordNativeRelocations(ObjPtr<mirror::Class> klass, size_t oa
                 oat_index, offset, NativeObjectRelocationType::kArtFieldArray
             })).second;
     CHECK(inserted) << "Field array " << fields << " already forwarded";
+    // Total array length including header.
     const size_t size = LengthPrefixedArray<ArtField>::ComputeSize(fields->size());
     offset += size;
     image_info.IncrementBinSlotSize(Bin::kArtField, size);
     DCHECK_EQ(offset, image_info.GetBinSlotSize(Bin::kArtField));
   }
-  // Visit and assign offsets for methods.
-  size_t num_methods = klass->NumMethods();
-  if (num_methods != 0) {
+  LengthPrefixedArray<ArtMethod>* array = klass->GetMethodsPtr();
+  if (!NativeRelocationAssigned(array) && !IsInBootImage(array)) {
+    // Visit and assign offsets for methods.
+    size_t num_methods = klass->NumMethods();
     bool any_dirty = false;
     for (auto& m : klass->GetMethods(target_ptr_size_)) {
       if (WillMethodBeDirty(&m)) {
@@ -1443,7 +1443,6 @@ void ImageWriter::RecordNativeRelocations(ObjPtr<mirror::Class> klass, size_t oa
     const size_t header_size = LengthPrefixedArray<ArtMethod>::ComputeSize(0,
                                                                            method_size,
                                                                            method_alignment);
-    LengthPrefixedArray<ArtMethod>* array = klass->GetMethodsPtr();
     size_t offset = image_info.GetBinSlotSize(bin_type);
     DCHECK(!IsInBootImage(array));
     bool inserted =
diff --git a/dex2oat/linker/oat_writer.cc b/dex2oat/linker/oat_writer.cc
index 6bd9e57725..b1c8bb1ef7 100644
--- a/dex2oat/linker/oat_writer.cc
+++ b/dex2oat/linker/oat_writer.cc
@@ -47,9 +47,11 @@
 #include "dex/dex_file_loader.h"
 #include "dex/dex_file_types.h"
 #include "dex/dex_file_verifier.h"
+#include "dex/method_reference.h"
 #include "dex/proto_reference.h"
 #include "dex/standard_dex_file.h"
 #include "dex/type_lookup_table.h"
+#include "dex/type_reference.h"
 #include "dex/verification_results.h"
 #include "driver/compiled_method-inl.h"
 #include "driver/compiler_driver-inl.h"
@@ -290,6 +292,62 @@ class OatWriter::OatClass {
   DISALLOW_COPY_AND_ASSIGN(OatClass);
 };
 
+// CompiledMethod + metadata required to do ordered method layout.
+//
+// See also OrderedMethodVisitor.
+struct OatWriter::OrderedMethodData {
+  uint32_t hotness_bits;
+  OatClass* oat_class;
+  CompiledMethod* compiled_method;
+  MethodReference method_reference;
+  size_t method_offsets_index;
+
+  size_t class_def_index;
+  uint32_t access_flags;
+  const dex::CodeItem* code_item;
+
+  // A value of -1 denotes missing debug info
+  static constexpr size_t kDebugInfoIdxInvalid = static_cast<size_t>(-1);
+  // Index into writer_->method_info_
+  size_t debug_info_idx;
+
+  bool HasDebugInfo() const {
+    return debug_info_idx != kDebugInfoIdxInvalid;
+  }
+
+  // Bin each method according to the profile flags.
+  //
+  // Groups by e.g.
+  //  -- startup and hot and poststartup
+  //  -- startup and hot
+  //  -- startup and post-startup
+  //  -- startup
+  //  -- hot and post-startup
+  //  -- hot
+  //  -- post-startup
+  //  -- not hot at all
+  //
+  // (See MethodHotness enum definition for up-to-date binning order.)
+  bool operator<(const OrderedMethodData& other) const {
+    if (kOatWriterForceOatCodeLayout) {
+      // Development flag: Override default behavior by sorting by name.
+
+      std::string name = method_reference.PrettyMethod();
+      std::string other_name = other.method_reference.PrettyMethod();
+      return name < other_name;
+    }
+
+    // Use the profile's method hotness to determine sort order, with startup
+    // methods appearing first.
+    if (hotness_bits > other.hotness_bits) {
+      return true;
+    }
+
+    // Default: retain the original order.
+    return false;
+  }
+};
+
 class OatWriter::OatDexFile {
  public:
   explicit OatDexFile(std::unique_ptr<const DexFile> dex_file);
@@ -405,8 +463,10 @@ OatWriter::OatWriter(const CompilerOptions& compiler_options,
       bss_string_entry_references_(),
       bss_method_type_entry_references_(),
       app_image_rel_ro_method_entries_(),
+      app_image_rel_ro_method_entries_sorted_(),
       bss_method_entries_(),
       app_image_rel_ro_type_entries_(),
+      app_image_rel_ro_type_entries_sorted_(),
       bss_type_entries_(),
       bss_public_type_entries_(),
       bss_package_type_entries_(),
@@ -778,49 +838,64 @@ void OatWriter::InitBssAndRelRoData() {
       }
       DCHECK_IMPLIES(!compiled_method->GetPatches().empty(), HasCompiledCode(compiled_method));
       for (const LinkerPatch& patch : compiled_method->GetPatches()) {
+        BssMap<TypeReference>* bss_type_entries = nullptr;
+        bool add_type_entry = false;
+        SafeMap<const DexFile*, BitVector>* bss_references = nullptr;
+        DexFileReference bss_ref(nullptr, dex::kDexNoIndex);
+        size_t number_of_indexes = 0;
+        bool add_bss_reference = false;
         if (patch.GetType() == LinkerPatch::Type::kBootImageRelRo) {
           boot_image_rel_ro_entries_.Overwrite(patch.BootImageOffset(), /* placeholder */ 0u);
         } else if (patch.GetType() == LinkerPatch::Type::kMethodAppImageRelRo) {
           MethodReference target_method = patch.TargetMethod();
-          app_image_rel_ro_method_entries_.Overwrite(target_method, /* placeholder */ 0u);
+          app_image_rel_ro_method_entries_.insert(
+              std::make_pair(target_method, /* placeholder */ 0u));
         } else if (patch.GetType() == LinkerPatch::Type::kMethodBssEntry) {
           MethodReference target_method = patch.TargetMethod();
-          AddBssReference(target_method,
-                          target_method.dex_file->NumMethodIds(),
-                          &bss_method_entry_references_);
-          bss_method_entries_.Overwrite(target_method, /* placeholder */ 0u);
+          bss_method_entries_.insert(std::make_pair(target_method, /* placeholder */ 0u));
+          bss_ref = target_method;
+          number_of_indexes = target_method.dex_file->NumMethodIds();
+          bss_references = &bss_method_entry_references_;
+          add_bss_reference = true;
         } else if (patch.GetType() == LinkerPatch::Type::kTypeAppImageRelRo) {
-          app_image_rel_ro_type_entries_.Overwrite(patch.TargetType(), /* placeholder */ 0u);
+          app_image_rel_ro_type_entries_.insert(
+              std::make_pair(patch.TargetType(), /* placeholder */ 0u));
         } else if (patch.GetType() == LinkerPatch::Type::kTypeBssEntry) {
-          TypeReference target_type = patch.TargetType();
-          AddBssReference(target_type,
-                          target_type.dex_file->NumTypeIds(),
-                          &bss_type_entry_references_);
-          bss_type_entries_.Overwrite(target_type, /* placeholder */ 0u);
+          bss_type_entries = &bss_type_entries_;
+          bss_references = &bss_type_entry_references_;
+          add_type_entry = true;
         } else if (patch.GetType() == LinkerPatch::Type::kPublicTypeBssEntry) {
-          TypeReference target_type = patch.TargetType();
-          AddBssReference(target_type,
-                          target_type.dex_file->NumTypeIds(),
-                          &bss_public_type_entry_references_);
-          bss_public_type_entries_.Overwrite(target_type, /* placeholder */ 0u);
+          bss_type_entries = &bss_public_type_entries_;
+          bss_references = &bss_public_type_entry_references_;
+          add_type_entry = true;
         } else if (patch.GetType() == LinkerPatch::Type::kPackageTypeBssEntry) {
-          TypeReference target_type = patch.TargetType();
-          AddBssReference(target_type,
-                          target_type.dex_file->NumTypeIds(),
-                          &bss_package_type_entry_references_);
-          bss_package_type_entries_.Overwrite(target_type, /* placeholder */ 0u);
+          bss_type_entries = &bss_package_type_entries_;
+          bss_references = &bss_package_type_entry_references_;
+          add_type_entry = true;
         } else if (patch.GetType() == LinkerPatch::Type::kStringBssEntry) {
           StringReference target_string = patch.TargetString();
-          AddBssReference(target_string,
-                          target_string.dex_file->NumStringIds(),
-                          &bss_string_entry_references_);
-          bss_string_entries_.Overwrite(target_string, /* placeholder */ 0u);
+          bss_string_entries_.insert(std::make_pair(target_string, /* placeholder */ 0u));
+          bss_ref = target_string;
+          number_of_indexes = target_string.dex_file->NumStringIds();
+          bss_references = &bss_string_entry_references_;
+          add_bss_reference = true;
         } else if (patch.GetType() == LinkerPatch::Type::kMethodTypeBssEntry) {
           ProtoReference target_proto = patch.TargetProto();
-          AddBssReference(target_proto,
-                          target_proto.dex_file->NumProtoIds(),
-                          &bss_method_type_entry_references_);
-          bss_method_type_entries_.Overwrite(target_proto, /* placeholder */ 0u);
+          bss_method_type_entries_.insert(std::make_pair(target_proto, /* placeholder */ 0u));
+          bss_ref = target_proto;
+          number_of_indexes = target_proto.dex_file->NumProtoIds();
+          bss_references = &bss_method_type_entry_references_;
+          add_bss_reference = true;
+        }
+        if (add_type_entry) {
+          TypeReference target_type = patch.TargetType();
+          bss_type_entries->insert(std::make_pair(target_type, /* placeholder */ 0u));
+          bss_ref = target_type;
+          number_of_indexes = target_type.dex_file->NumTypeIds();
+          add_bss_reference = true;
+        }
+        if (add_bss_reference) {
+          AddBssReference(bss_ref, number_of_indexes, bss_references);
         }
       }
     }
@@ -927,68 +1002,12 @@ class OatWriter::InitOatClassesMethodVisitor : public DexMethodVisitor {
   size_t compiled_methods_with_code_;
 };
 
-// CompiledMethod + metadata required to do ordered method layout.
-//
-// See also OrderedMethodVisitor.
-struct OatWriter::OrderedMethodData {
-  uint32_t hotness_bits;
-  OatClass* oat_class;
-  CompiledMethod* compiled_method;
-  MethodReference method_reference;
-  size_t method_offsets_index;
-
-  size_t class_def_index;
-  uint32_t access_flags;
-  const dex::CodeItem* code_item;
-
-  // A value of -1 denotes missing debug info
-  static constexpr size_t kDebugInfoIdxInvalid = static_cast<size_t>(-1);
-  // Index into writer_->method_info_
-  size_t debug_info_idx;
-
-  bool HasDebugInfo() const {
-    return debug_info_idx != kDebugInfoIdxInvalid;
-  }
-
-  // Bin each method according to the profile flags.
-  //
-  // Groups by e.g.
-  //  -- startup and hot and poststartup
-  //  -- startup and hot
-  //  -- startup and post-startup
-  //  -- startup
-  //  -- hot and post-startup
-  //  -- hot
-  //  -- post-startup
-  //  -- not hot at all
-  //
-  // (See MethodHotness enum definition for up-to-date binning order.)
-  bool operator<(const OrderedMethodData& other) const {
-    if (kOatWriterForceOatCodeLayout) {
-      // Development flag: Override default behavior by sorting by name.
-
-      std::string name = method_reference.PrettyMethod();
-      std::string other_name = other.method_reference.PrettyMethod();
-      return name < other_name;
-    }
-
-    // Use the profile's method hotness to determine sort order, with startup
-    // methods appearing first.
-    if (hotness_bits > other.hotness_bits) {
-      return true;
-    }
-
-    // Default: retain the original order.
-    return false;
-  }
-};
-
 // Given a queue of CompiledMethod in some total order,
 // visit each one in that order.
 class OatWriter::OrderedMethodVisitor {
  public:
-  explicit OrderedMethodVisitor(OrderedMethodList ordered_methods)
-      : ordered_methods_(std::move(ordered_methods)) {
+  explicit OrderedMethodVisitor(ArrayRef<const OrderedMethodData> ordered_methods)
+      : ordered_methods_(ordered_methods) {
   }
 
   virtual ~OrderedMethodVisitor() {}
@@ -1024,14 +1043,10 @@ class OatWriter::OrderedMethodVisitor {
   // Return false to indicate the overall `Visit` has failed.
   virtual bool VisitComplete() = 0;
 
-  OrderedMethodList ReleaseOrderedMethods() {
-    return std::move(ordered_methods_);
-  }
-
  private:
   // List of compiled methods, sorted by the order defined in OrderedMethodData.
   // Methods can be inserted more than once in case of duplicated methods.
-  OrderedMethodList ordered_methods_;
+  ArrayRef<const OrderedMethodData> ordered_methods_;
 };
 
 // Visit every compiled method in order to determine its order within the OAT file.
@@ -1137,7 +1152,7 @@ class OatWriter::LayoutCodeMethodVisitor final : public OatDexMethodVisitor {
     return true;
   }
 
-  OrderedMethodList ReleaseOrderedMethods() {
+  std::vector<OrderedMethodData> ReleaseOrderedMethods() {
     if (kOatWriterForceOatCodeLayout || writer_->profile_compilation_info_ != nullptr) {
       // Sort by the method ordering criteria (in OrderedMethodData).
       // Since most methods will have the same ordering criteria,
@@ -1162,7 +1177,7 @@ class OatWriter::LayoutCodeMethodVisitor final : public OatDexMethodVisitor {
 
   // List of compiled methods, later to be sorted by order defined in OrderedMethodData.
   // Methods can be inserted more than once in case of duplicated methods.
-  OrderedMethodList ordered_methods_;
+  std::vector<OrderedMethodData> ordered_methods_;
 };
 
 // Given a method order, reserve the offsets for each CompiledMethod in the OAT file.
@@ -1170,11 +1185,11 @@ class OatWriter::LayoutReserveOffsetCodeMethodVisitor : public OrderedMethodVisi
  public:
   LayoutReserveOffsetCodeMethodVisitor(OatWriter* writer,
                                        size_t offset,
-                                       OrderedMethodList ordered_methods)
+                                       ArrayRef<const OrderedMethodData> ordered_methods)
       : LayoutReserveOffsetCodeMethodVisitor(writer,
                                              offset,
                                              writer->GetCompilerOptions(),
-                                             std::move(ordered_methods)) {
+                                             ordered_methods) {
   }
 
   bool VisitComplete() override {
@@ -1306,8 +1321,8 @@ class OatWriter::LayoutReserveOffsetCodeMethodVisitor : public OrderedMethodVisi
   LayoutReserveOffsetCodeMethodVisitor(OatWriter* writer,
                                        size_t offset,
                                        const CompilerOptions& compiler_options,
-                                       OrderedMethodList ordered_methods)
-      : OrderedMethodVisitor(std::move(ordered_methods)),
+                                       ArrayRef<const OrderedMethodData> ordered_methods)
+      : OrderedMethodVisitor(ordered_methods),
         writer_(writer),
         offset_(offset),
         relative_patcher_(writer->relative_patcher_),
@@ -1569,8 +1584,8 @@ class OatWriter::WriteCodeMethodVisitor : public OrderedMethodVisitor {
                          OutputStream* out,
                          const size_t file_offset,
                          size_t relative_offset,
-                         OrderedMethodList ordered_methods)
-      : OrderedMethodVisitor(std::move(ordered_methods)),
+                         ArrayRef<const OrderedMethodData> ordered_methods)
+      : OrderedMethodVisitor(ordered_methods),
         writer_(writer),
         offset_(relative_offset),
         dex_file_(nullptr),
@@ -1599,13 +1614,9 @@ class OatWriter::WriteCodeMethodVisitor : public OrderedMethodVisitor {
 
     // Ordered method visiting is only for compiled methods.
     DCHECK(writer_->MayHaveCompiledMethods());
-
-    if (writer_->GetCompilerOptions().IsAotCompilationEnabled()) {
-      // Only need to set the dex cache if we have compilation. Other modes might have unloaded it.
-      if (dex_cache_ == nullptr || dex_cache_->GetDexFile() != dex_file) {
-        dex_cache_ = class_linker_->FindDexCache(Thread::Current(), *dex_file);
-        DCHECK(dex_cache_ != nullptr);
-      }
+    if (dex_cache_ == nullptr || dex_cache_->GetDexFile() != dex_file) {
+      dex_cache_ = class_linker_->FindDexCache(Thread::Current(), *dex_file);
+      DCHECK(dex_cache_ != nullptr);
     }
   }
 
@@ -1701,7 +1712,8 @@ class OatWriter::WriteCodeMethodVisitor : public OrderedMethodVisitor {
             }
             case LinkerPatch::Type::kMethodBssEntry: {
               uint32_t target_offset =
-                  writer_->bss_start_ + writer_->bss_method_entries_.Get(patch.TargetMethod());
+                  writer_->bss_start_ +
+                  writer_->bss_method_entries_.find(patch.TargetMethod())->second;
               writer_->relative_patcher_->PatchPcRelativeReference(&patched_code_,
                                                                    patch,
                                                                    offset_ + literal_offset,
@@ -1727,7 +1739,8 @@ class OatWriter::WriteCodeMethodVisitor : public OrderedMethodVisitor {
             }
             case LinkerPatch::Type::kStringBssEntry: {
               uint32_t target_offset =
-                  writer_->bss_start_ + writer_->bss_string_entries_.Get(patch.TargetString());
+                  writer_->bss_start_ +
+                  writer_->bss_string_entries_.find(patch.TargetString())->second;
               writer_->relative_patcher_->PatchPcRelativeReference(&patched_code_,
                                                                    patch,
                                                                    offset_ + literal_offset,
@@ -1737,7 +1750,7 @@ class OatWriter::WriteCodeMethodVisitor : public OrderedMethodVisitor {
             case LinkerPatch::Type::kMethodAppImageRelRo: {
               uint32_t target_offset =
                   writer_->data_img_rel_ro_start_ +
-                  writer_->app_image_rel_ro_method_entries_.Get(patch.TargetMethod());
+                  writer_->app_image_rel_ro_method_entries_.find(patch.TargetMethod())->second;
               writer_->relative_patcher_->PatchPcRelativeReference(&patched_code_,
                                                                    patch,
                                                                    offset_ + literal_offset,
@@ -1746,7 +1759,8 @@ class OatWriter::WriteCodeMethodVisitor : public OrderedMethodVisitor {
             }
             case LinkerPatch::Type::kMethodTypeBssEntry: {
               uint32_t target_offset =
-                  writer_->bss_start_ + writer_->bss_method_type_entries_.Get(patch.TargetProto());
+                  writer_->bss_start_ +
+                  writer_->bss_method_type_entries_.find(patch.TargetProto())->second;
               writer_->relative_patcher_->PatchPcRelativeReference(&patched_code_,
                                                                    patch,
                                                                    offset_ + literal_offset,
@@ -1764,7 +1778,7 @@ class OatWriter::WriteCodeMethodVisitor : public OrderedMethodVisitor {
             case LinkerPatch::Type::kTypeAppImageRelRo: {
               uint32_t target_offset =
                   writer_->data_img_rel_ro_start_ +
-                  writer_->app_image_rel_ro_type_entries_.Get(patch.TargetType());
+                  writer_->app_image_rel_ro_type_entries_.find(patch.TargetType())->second;
               writer_->relative_patcher_->PatchPcRelativeReference(&patched_code_,
                                                                    patch,
                                                                    offset_ + literal_offset,
@@ -1773,7 +1787,7 @@ class OatWriter::WriteCodeMethodVisitor : public OrderedMethodVisitor {
             }
             case LinkerPatch::Type::kTypeBssEntry: {
               uint32_t target_offset =
-                  writer_->bss_start_ + writer_->bss_type_entries_.Get(patch.TargetType());
+                  writer_->bss_start_ + writer_->bss_type_entries_.find(patch.TargetType())->second;
               writer_->relative_patcher_->PatchPcRelativeReference(&patched_code_,
                                                                    patch,
                                                                    offset_ + literal_offset,
@@ -1782,7 +1796,8 @@ class OatWriter::WriteCodeMethodVisitor : public OrderedMethodVisitor {
             }
             case LinkerPatch::Type::kPublicTypeBssEntry: {
               uint32_t target_offset =
-                  writer_->bss_start_ + writer_->bss_public_type_entries_.Get(patch.TargetType());
+                  writer_->bss_start_ +
+                  writer_->bss_public_type_entries_.find(patch.TargetType())->second;
               writer_->relative_patcher_->PatchPcRelativeReference(&patched_code_,
                                                                    patch,
                                                                    offset_ + literal_offset,
@@ -1791,7 +1806,8 @@ class OatWriter::WriteCodeMethodVisitor : public OrderedMethodVisitor {
             }
             case LinkerPatch::Type::kPackageTypeBssEntry: {
               uint32_t target_offset =
-                  writer_->bss_start_ + writer_->bss_package_type_entries_.Get(patch.TargetType());
+                  writer_->bss_start_ +
+                  writer_->bss_package_type_entries_.find(patch.TargetType())->second;
               writer_->relative_patcher_->PatchPcRelativeReference(&patched_code_,
                                                                    patch,
                                                                    offset_ + literal_offset,
@@ -2111,15 +2127,16 @@ static size_t CalculateIndexBssMappingSize(size_t number_of_indexes,
   return IndexBssMapping::ComputeSize(number_of_entries);
 }
 
-static size_t CalculateIndexBssMappingSize(
-    const DexFile* dex_file,
-    const BitVector& type_indexes,
-    const SafeMap<TypeReference, size_t, TypeReferenceValueComparator>& bss_entries) {
+static size_t CalculateIndexBssMappingSize(const DexFile* dex_file,
+                                           const BitVector& type_indexes,
+                                           const OatWriter::BssMap<TypeReference>& bss_entries) {
   return CalculateIndexBssMappingSize(
       dex_file->NumTypeIds(),
       sizeof(GcRoot<mirror::Class>),
       type_indexes,
-      [=](uint32_t index) { return bss_entries.Get({dex_file, dex::TypeIndex(index)}); });
+      [dex_file, &bss_entries](uint32_t index) {
+        return bss_entries.find(TypeReference(dex_file, dex::TypeIndex(index)))->second;
+      });
 }
 
 size_t OatWriter::InitIndexBssMappings(size_t offset) {
@@ -2232,12 +2249,13 @@ size_t OatWriter::InitIndexBssMappingsHelper(size_t offset,
     const BitVector& method_indexes = method_it->second;
     ++number_of_method_dex_files;
     method_bss_mapping_offset = offset;
-    offset += CalculateIndexBssMappingSize(dex_file->NumMethodIds(),
-                                           static_cast<size_t>(pointer_size),
-                                           method_indexes,
-                                           [this, dex_file](uint32_t index) {
-                                             return bss_method_entries_.Get({dex_file, index});
-                                           });
+    offset += CalculateIndexBssMappingSize(
+        dex_file->NumMethodIds(),
+        static_cast<size_t>(pointer_size),
+        method_indexes,
+        [this, dex_file](uint32_t index) {
+          return bss_method_entries_.find(MethodReference(dex_file, index))->second;
+        });
   }
 
   auto type_it = bss_type_entry_references_.find(dex_file);
@@ -2274,7 +2292,8 @@ size_t OatWriter::InitIndexBssMappingsHelper(size_t offset,
         sizeof(GcRoot<mirror::String>),
         string_indexes,
         [this, dex_file](uint32_t index) {
-          return bss_string_entries_.Get({dex_file, dex::StringIndex(index)});
+          return bss_string_entries_.find(StringReference(dex_file, dex::StringIndex(index)))
+              ->second;
         });
   }
 
@@ -2288,7 +2307,8 @@ size_t OatWriter::InitIndexBssMappingsHelper(size_t offset,
         sizeof(GcRoot<mirror::MethodType>),
         proto_indexes,
         [this, dex_file](uint32_t index) {
-          return bss_method_type_entries_.Get({dex_file, dex::ProtoIndex(index)});
+          return bss_method_type_entries_.find(ProtoReference(dex_file, dex::ProtoIndex(index)))
+              ->second;
         });
   }
 
@@ -2392,24 +2412,22 @@ size_t OatWriter::InitOatCodeDexFiles(size_t offset) {
     success = VisitDexMethods(&layout_code_visitor);
     DCHECK(success);
 
+    // Save the method order because the WriteCodeMethodVisitor will need this
+    // order again.
+    DCHECK(ordered_methods_.empty());
+    ordered_methods_ = layout_code_visitor.ReleaseOrderedMethods();
+
     LayoutReserveOffsetCodeMethodVisitor layout_reserve_code_visitor(
         this,
         offset,
-        layout_code_visitor.ReleaseOrderedMethods());
+        ArrayRef<const OrderedMethodData>(ordered_methods_));
     success = layout_reserve_code_visitor.Visit();
     DCHECK(success);
     offset = layout_reserve_code_visitor.GetOffset();
 
-    // Save the method order because the WriteCodeMethodVisitor will need this
-    // order again.
-    DCHECK(ordered_methods_ == nullptr);
-    ordered_methods_.reset(
-        new OrderedMethodList(
-            layout_reserve_code_visitor.ReleaseOrderedMethods()));
-
     if (kOatWriterDebugOatCodeLayout) {
       LOG(INFO) << "IniatOatCodeDexFiles: method order: ";
-      for (const OrderedMethodData& ordered_method : *ordered_methods_) {
+      for (const OrderedMethodData& ordered_method : ordered_methods_) {
         std::string pretty_name = ordered_method.method_reference.PrettyMethod();
         LOG(INFO) << pretty_name
                   << "@ offset "
@@ -2433,6 +2451,54 @@ size_t OatWriter::InitOatCodeDexFiles(size_t offset) {
   return offset;
 }
 
+// Returns a vector of iterators of `map` using `Comp` to sort.
+template <typename T, typename Compare>
+std::vector<typename OatWriter::BssMap<T>::iterator> VectorizeAndSort(OatWriter::BssMap<T>& map,
+                                                                      Compare comp) {
+  using iterator = OatWriter::BssMap<T>::iterator;
+  std::vector<iterator> vec;
+  vec.reserve(map.size());
+  for (auto it = map.begin(); it != map.end(); ++it) {
+    vec.push_back(it);
+  }
+  std::sort(vec.begin(), vec.end(), [&comp](iterator lhs, iterator rhs) {
+    return comp(lhs->first, rhs->first);
+  });
+  return vec;
+}
+
+// Helper to initialize layout offsets, considering duplicate values. Note that `offset` is passed
+// by reference as we want to update the value as we iterate.
+template <typename T, typename Compare>
+std::vector<typename OatWriter::BssMap<T>::iterator> InitLayoutOffset(OatWriter::BssMap<T>& map,
+                                                                      size_t entry_size,
+                                                                      Compare comp,
+                                                                      size_t& offset) {
+  std::vector<typename OatWriter::BssMap<T>::iterator> vec = VectorizeAndSort(map, comp);
+  for (size_t i = 0; i < vec.size(); ++i) {
+    DCHECK_EQ(vec[i]->second, 0u);
+    if (i != 0 && !comp(vec[i - 1]->first, vec[i]->first)) {
+      // Copy the offset from the duplicate entry.
+      vec[i]->second = vec[i - 1]->second;
+    } else {
+      vec[i]->second = offset;
+      offset += entry_size;
+    }
+  }
+  return vec;
+}
+
+template <typename T, typename Compare>
+void OatWriter::InitBssLayoutOffset(BssMap<T>& map, size_t entry_size, Compare comp) {
+  InitLayoutOffset(map, entry_size, comp, bss_size_);
+}
+
+template <typename T, typename Compare>
+std::vector<typename OatWriter::BssMap<T>::iterator> OatWriter::InitDataImgRelRoLayoutOffset(
+    BssMap<T>& map, size_t entry_size, Compare comp) {
+  return InitLayoutOffset(map, entry_size, comp, data_img_rel_ro_size_);
+}
+
 size_t OatWriter::InitDataImgRelRoLayout(size_t offset) {
   DCHECK_EQ(data_img_rel_ro_size_, 0u);
   if (boot_image_rel_ro_entries_.empty() &&
@@ -2452,17 +2518,13 @@ size_t OatWriter::InitDataImgRelRoLayout(size_t offset) {
 
   data_img_rel_ro_app_image_offset_ = data_img_rel_ro_size_;
 
-  for (auto& entry : app_image_rel_ro_method_entries_) {
-    size_t& entry_offset = entry.second;
-    entry_offset = data_img_rel_ro_size_;
-    data_img_rel_ro_size_ += sizeof(uint32_t);
-  }
+  DCHECK(app_image_rel_ro_method_entries_sorted_.empty());
+  app_image_rel_ro_method_entries_sorted_ = InitDataImgRelRoLayoutOffset(
+      app_image_rel_ro_method_entries_, sizeof(uint32_t), MethodReferenceValueComparator());
 
-  for (auto& entry : app_image_rel_ro_type_entries_) {
-    size_t& entry_offset = entry.second;
-    entry_offset = data_img_rel_ro_size_;
-    data_img_rel_ro_size_ += sizeof(uint32_t);
-  }
+  DCHECK(app_image_rel_ro_type_entries_sorted_.empty());
+  app_image_rel_ro_type_entries_sorted_ = InitDataImgRelRoLayoutOffset(
+      app_image_rel_ro_type_entries_, sizeof(uint32_t), TypeReferenceValueComparator());
 
   offset = data_img_rel_ro_start_ + data_img_rel_ro_size_;
   return offset;
@@ -2480,48 +2542,26 @@ void OatWriter::InitBssLayout(InstructionSet instruction_set) {
     return;
   }
 
-  PointerSize pointer_size = GetInstructionSetPointerSize(instruction_set);
+  // Prepare offsets.
   bss_methods_offset_ = bss_size_;
 
-  // Prepare offsets for .bss ArtMethod entries.
-  for (auto& entry : bss_method_entries_) {
-    DCHECK_EQ(entry.second, 0u);
-    entry.second = bss_size_;
-    bss_size_ += static_cast<size_t>(pointer_size);
-  }
+  PointerSize pointer_size = GetInstructionSetPointerSize(instruction_set);
+  InitBssLayoutOffset(
+      bss_method_entries_, static_cast<size_t>(pointer_size), MethodReferenceValueComparator());
 
   bss_roots_offset_ = bss_size_;
 
-  // Prepare offsets for .bss Class entries.
-  for (auto& entry : bss_type_entries_) {
-    DCHECK_EQ(entry.second, 0u);
-    entry.second = bss_size_;
-    bss_size_ += sizeof(GcRoot<mirror::Class>);
-  }
-  // Prepare offsets for .bss public Class entries.
-  for (auto& entry : bss_public_type_entries_) {
-    DCHECK_EQ(entry.second, 0u);
-    entry.second = bss_size_;
-    bss_size_ += sizeof(GcRoot<mirror::Class>);
-  }
-  // Prepare offsets for .bss package Class entries.
-  for (auto& entry : bss_package_type_entries_) {
-    DCHECK_EQ(entry.second, 0u);
-    entry.second = bss_size_;
-    bss_size_ += sizeof(GcRoot<mirror::Class>);
-  }
-  // Prepare offsets for .bss String entries.
-  for (auto& entry : bss_string_entries_) {
-    DCHECK_EQ(entry.second, 0u);
-    entry.second = bss_size_;
-    bss_size_ += sizeof(GcRoot<mirror::String>);
-  }
-  // Prepare offsets for .bss MethodType entries.
-  for (auto& entry : bss_method_type_entries_) {
-    DCHECK_EQ(entry.second, 0u);
-    entry.second = bss_size_;
-    bss_size_ += sizeof(GcRoot<mirror::MethodType>);
-  }
+  InitBssLayoutOffset(
+      bss_type_entries_, sizeof(GcRoot<mirror::Class>), TypeReferenceValueComparator());
+  InitBssLayoutOffset(
+      bss_public_type_entries_, sizeof(GcRoot<mirror::Class>), TypeReferenceValueComparator());
+  InitBssLayoutOffset(
+      bss_package_type_entries_, sizeof(GcRoot<mirror::Class>), TypeReferenceValueComparator());
+  InitBssLayoutOffset(
+      bss_string_entries_, sizeof(GcRoot<mirror::String>), StringReferenceValueComparator());
+  InitBssLayoutOffset(bss_method_type_entries_,
+                      sizeof(GcRoot<mirror::MethodType>),
+                      ProtoReferenceValueComparator());
 }
 
 bool OatWriter::WriteRodata(OutputStream* out) {
@@ -2928,17 +2968,18 @@ size_t WriteIndexBssMapping(OutputStream* out,
   return mappings_size;
 }
 
-size_t WriteIndexBssMapping(
-    OutputStream* out,
-    const DexFile* dex_file,
-    const BitVector& type_indexes,
-    const SafeMap<TypeReference, size_t, TypeReferenceValueComparator>& bss_entries) {
+static size_t WriteIndexBssMapping(OutputStream* out,
+                                   const DexFile* dex_file,
+                                   const BitVector& type_indexes,
+                                   const OatWriter::BssMap<TypeReference>& bss_entries) {
   return WriteIndexBssMapping(
       out,
       dex_file->NumTypeIds(),
       sizeof(GcRoot<mirror::Class>),
       type_indexes,
-      [=](uint32_t index) { return bss_entries.Get({dex_file, dex::TypeIndex(index)}); });
+      [dex_file, &bss_entries](uint32_t index) {
+        return bss_entries.find(TypeReference(dex_file, dex::TypeIndex(index)))->second;
+      });
 }
 
 size_t OatWriter::WriteIndexBssMappingsHelper(OutputStream* out,
@@ -2957,14 +2998,14 @@ size_t OatWriter::WriteIndexBssMappingsHelper(OutputStream* out,
     const BitVector& method_indexes = method_it->second;
     DCHECK_EQ(relative_offset, method_bss_mapping_offset);
     DCHECK_OFFSET();
-    size_t method_mappings_size =
-        WriteIndexBssMapping(out,
-                             dex_file->NumMethodIds(),
-                             static_cast<size_t>(pointer_size),
-                             method_indexes,
-                             [this, dex_file](uint32_t index) {
-                               return bss_method_entries_.Get({dex_file, index});
-                             });
+    size_t method_mappings_size = WriteIndexBssMapping(
+        out,
+        dex_file->NumMethodIds(),
+        static_cast<size_t>(pointer_size),
+        method_indexes,
+        [this, dex_file](uint32_t index) {
+          return bss_method_entries_.find(MethodReference(dex_file, index))->second;
+        });
     if (method_mappings_size == 0u) {
       return 0u;
     }
@@ -3027,14 +3068,15 @@ size_t OatWriter::WriteIndexBssMappingsHelper(OutputStream* out,
     const BitVector& string_indexes = string_it->second;
     DCHECK_EQ(relative_offset, string_bss_mapping_offset);
     DCHECK_OFFSET();
-    size_t string_mappings_size =
-        WriteIndexBssMapping(out,
-                             dex_file->NumStringIds(),
-                             sizeof(GcRoot<mirror::String>),
-                             string_indexes,
-                             [this, dex_file](uint32_t index) {
-                               return bss_string_entries_.Get({dex_file, dex::StringIndex(index)});
-                             });
+    size_t string_mappings_size = WriteIndexBssMapping(
+        out,
+        dex_file->NumStringIds(),
+        sizeof(GcRoot<mirror::String>),
+        string_indexes,
+        [this, dex_file](uint32_t index) {
+          return bss_string_entries_.find(StringReference(dex_file, dex::StringIndex(index)))
+              ->second;
+        });
     if (string_mappings_size == 0u) {
       return 0u;
     }
@@ -3049,15 +3091,15 @@ size_t OatWriter::WriteIndexBssMappingsHelper(OutputStream* out,
     const BitVector& method_type_indexes = method_type_it->second;
     DCHECK_EQ(relative_offset, method_type_bss_mapping_offset);
     DCHECK_OFFSET();
-    size_t method_type_mappings_size =
-        WriteIndexBssMapping(out,
-                             dex_file->NumProtoIds(),
-                             sizeof(GcRoot<mirror::MethodType>),
-                             method_type_indexes,
-                             [this, dex_file](uint32_t index) {
-                               return bss_method_type_entries_
-                                   .Get({dex_file, dex::ProtoIndex(index)});
-                             });
+    size_t method_type_mappings_size = WriteIndexBssMapping(
+        out,
+        dex_file->NumProtoIds(),
+        sizeof(GcRoot<mirror::MethodType>),
+        method_type_indexes,
+        [this, dex_file](uint32_t index) {
+          return bss_method_type_entries_.find(ProtoReference(dex_file, dex::ProtoIndex(index)))
+              ->second;
+        });
     if (method_type_mappings_size == 0u) {
       return 0u;
     }
@@ -3233,14 +3275,11 @@ size_t OatWriter::WriteCodeDexFiles(OutputStream* out,
     return relative_offset;
   }
   ScopedObjectAccess soa(Thread::Current());
-  DCHECK(ordered_methods_ != nullptr);
-  std::unique_ptr<OrderedMethodList> ordered_methods_ptr =
-      std::move(ordered_methods_);
   WriteCodeMethodVisitor visitor(this,
                                  out,
                                  file_offset,
                                  relative_offset,
-                                 std::move(*ordered_methods_ptr));
+                                 ArrayRef<const OrderedMethodData>(ordered_methods_));
   if (UNLIKELY(!visitor.Visit())) {
     return 0;
   }
@@ -3270,6 +3309,12 @@ size_t OatWriter::WriteDataImgRelRo(OutputStream* out,
     uint32_t boot_image_offset = entry.first;
     data.push_back(boot_image_offset);
   }
+
+  // Both the sorted and unsorted variants contain duplicates. We skip the duplicates in the loops
+  // below, and we update `size` in the process.
+  DCHECK_EQ(app_image_rel_ro_method_entries_.size(),
+            app_image_rel_ro_method_entries_sorted_.size());
+  DCHECK_EQ(app_image_rel_ro_type_entries_.size(), app_image_rel_ro_type_entries_sorted_.size());
   if (!app_image_rel_ro_method_entries_.empty() || !app_image_rel_ro_type_entries_.empty()) {
     DCHECK(GetCompilerOptions().IsAppImage());
     ClassLinker* class_linker = Runtime::Current()->GetClassLinker();
@@ -3284,8 +3329,16 @@ size_t OatWriter::WriteDataImgRelRo(OutputStream* out,
         last_dex_file = dex_file;
       }
     };
-    for (const auto& entry : app_image_rel_ro_method_entries_) {
-      MethodReference target_method = entry.first;
+
+    for (size_t i : Range(app_image_rel_ro_method_entries_sorted_.size())) {
+      if (i != 0 && app_image_rel_ro_method_entries_sorted_[i]->second ==
+                        app_image_rel_ro_method_entries_sorted_[i - 1]->second) {
+        // Skip duplicates. We can do it cheaply by comparing the offsets, without the need to
+        // compare the MethodReferences themselves.
+        --size;
+        continue;
+      }
+      MethodReference target_method = app_image_rel_ro_method_entries_sorted_[i]->first;
       update_for_dex_file(target_method.dex_file);
       ArtMethod* method =
           class_linker->LookupResolvedMethod(target_method.index, dex_cache, class_loader);
@@ -3293,8 +3346,15 @@ size_t OatWriter::WriteDataImgRelRo(OutputStream* out,
       uint32_t app_image_offset = image_writer_->GetGlobalImageOffset(method);
       data.push_back(app_image_offset);
     }
-    for (const auto& entry : app_image_rel_ro_type_entries_) {
-      TypeReference target_type = entry.first;
+    for (size_t i : Range(app_image_rel_ro_type_entries_sorted_.size())) {
+      if (i != 0 && app_image_rel_ro_type_entries_sorted_[i]->second ==
+                        app_image_rel_ro_type_entries_sorted_[i - 1]->second) {
+        // Skip duplicates. We can do it cheaply by comparing the offsets, without the need to
+        // compare the TypeReferences themselves.
+        --size;
+        continue;
+      }
+      TypeReference target_type = app_image_rel_ro_type_entries_sorted_[i]->first;
       update_for_dex_file(target_type.dex_file);
       ObjPtr<mirror::Class> type =
           class_linker->LookupResolvedType(target_type.TypeIndex(), dex_cache, class_loader);
diff --git a/dex2oat/linker/oat_writer.h b/dex2oat/linker/oat_writer.h
index 4f50b1a39c..d43916853c 100644
--- a/dex2oat/linker/oat_writer.h
+++ b/dex2oat/linker/oat_writer.h
@@ -18,20 +18,21 @@
 #define ART_DEX2OAT_LINKER_OAT_WRITER_H_
 
 #include <stdint.h>
+
 #include <cstddef>
-#include <list>
 #include <memory>
+#include <unordered_map>
 #include <vector>
 
 #include "base/array_ref.h"
 #include "base/dchecked_vector.h"
-#include "base/os.h"
 #include "base/mem_map.h"
+#include "base/os.h"
 #include "base/safe_map.h"
 #include "debug/debug_info.h"
 #include "dex/method_reference.h"
-#include "dex/string_reference.h"
 #include "dex/proto_reference.h"
+#include "dex/string_reference.h"
 #include "dex/type_reference.h"
 #include "linker/relative_patcher.h"  // For RelativePatcherTargetProvider.
 #include "mirror/class.h"
@@ -270,6 +271,9 @@ class OatWriter {
     return compiler_options_;
   }
 
+  template <typename KeyType>
+  using BssMap = std::unordered_map<KeyType, size_t>;
+
  private:
   struct BssMappingInfo;
   class ChecksumUpdatingOutputStream;
@@ -393,6 +397,14 @@ class OatWriter {
     return RoundUp(GetFileOffset(offset_from_oat_data), alignment) - oat_data_offset_;
   }
 
+  template <typename T, typename Compare>
+  void InitBssLayoutOffset(BssMap<T>& map, size_t entry_size, Compare comp);
+
+  template <typename T, typename Compare>
+  std::vector<typename BssMap<T>::iterator> InitDataImgRelRoLayoutOffset(BssMap<T>& map,
+                                                                         size_t entry_size,
+                                                                         Compare comp);
+
   enum class WriteState {
     kAddingDexFileSources,
     kStartRoData,
@@ -499,42 +511,48 @@ class OatWriter {
   // for the target method in the dex file with the "method reference value comparator" for
   // deduplication. The value is the target offset for patching, starting at
   // `data_img_rel_ro_start_`.
-  SafeMap<MethodReference, size_t, MethodReferenceValueComparator> app_image_rel_ro_method_entries_;
+  BssMap<MethodReference> app_image_rel_ro_method_entries_;
+  // Vector containing iterators to `app_image_rel_ro_method_entries_`, sorted using
+  // MethodReferenceValueComparator.
+  std::vector<BssMap<MethodReference>::iterator> app_image_rel_ro_method_entries_sorted_;
 
   // Map for allocating ArtMethod entries in .bss. Indexed by MethodReference for the target
   // method in the dex file with the "method reference value comparator" for deduplication.
   // The value is the target offset for patching, starting at `bss_start_ + bss_methods_offset_`.
-  SafeMap<MethodReference, size_t, MethodReferenceValueComparator> bss_method_entries_;
+  BssMap<MethodReference> bss_method_entries_;
 
   // Map for allocating app image Class entries in .data.img.rel.ro. Indexed by TypeReference for
   // the source type in the dex file with the "type value comparator" for deduplication. The value
   // is the target offset for patching, starting at `data_img_rel_ro_start_`.
-  SafeMap<TypeReference, size_t, TypeReferenceValueComparator> app_image_rel_ro_type_entries_;
+  BssMap<TypeReference> app_image_rel_ro_type_entries_;
+  // Vector containing iterators to `app_image_rel_ro_type_entries_sorted_`, sorted using
+  // TypeReferenceValueComparator.
+  std::vector<BssMap<TypeReference>::iterator> app_image_rel_ro_type_entries_sorted_;
 
   // Map for allocating Class entries in .bss. Indexed by TypeReference for the source
   // type in the dex file with the "type value comparator" for deduplication. The value
   // is the target offset for patching, starting at `bss_start_ + bss_roots_offset_`.
-  SafeMap<TypeReference, size_t, TypeReferenceValueComparator> bss_type_entries_;
+  BssMap<TypeReference> bss_type_entries_;
 
   // Map for allocating public Class entries in .bss. Indexed by TypeReference for the source
   // type in the dex file with the "type value comparator" for deduplication. The value
   // is the target offset for patching, starting at `bss_start_ + bss_roots_offset_`.
-  SafeMap<TypeReference, size_t, TypeReferenceValueComparator> bss_public_type_entries_;
+  BssMap<TypeReference> bss_public_type_entries_;
 
   // Map for allocating package Class entries in .bss. Indexed by TypeReference for the source
   // type in the dex file with the "type value comparator" for deduplication. The value
   // is the target offset for patching, starting at `bss_start_ + bss_roots_offset_`.
-  SafeMap<TypeReference, size_t, TypeReferenceValueComparator> bss_package_type_entries_;
+  BssMap<TypeReference> bss_package_type_entries_;
 
   // Map for allocating String entries in .bss. Indexed by StringReference for the source
   // string in the dex file with the "string value comparator" for deduplication. The value
   // is the target offset for patching, starting at `bss_start_ + bss_roots_offset_`.
-  SafeMap<StringReference, size_t, StringReferenceValueComparator> bss_string_entries_;
+  BssMap<StringReference> bss_string_entries_;
 
   // Map for allocating MethodType entries in .bss. Indexed by ProtoReference for the source
   // proto in the dex file with the "proto value comparator" for deduplication. The value
   // is the target offset for patching, starting at `bss_start_ + bss_roots_offset_`.
-  SafeMap<ProtoReference, size_t, ProtoReferenceValueComparator> bss_method_type_entries_;
+  BssMap<ProtoReference> bss_method_type_entries_;
 
   // Offset of the oat data from the start of the mmapped region of the elf file.
   size_t oat_data_offset_;
@@ -630,12 +648,10 @@ class OatWriter {
   // Profile info used to generate new layout of files.
   ProfileCompilationInfo* profile_compilation_info_;
 
-  using OrderedMethodList = std::vector<OrderedMethodData>;
-
   // List of compiled methods, sorted by the order defined in OrderedMethodData.
   // Methods can be inserted more than once in case of duplicated methods.
   // This pointer is only non-null after InitOatCodeDexFiles succeeds.
-  std::unique_ptr<OrderedMethodList> ordered_methods_;
+  std::vector<OrderedMethodData> ordered_methods_;
 
   DISALLOW_COPY_AND_ASSIGN(OatWriter);
 };
diff --git a/dex2oat/linker/oat_writer_test.cc b/dex2oat/linker/oat_writer_test.cc
index 538c1abc9a..c9ca655462 100644
--- a/dex2oat/linker/oat_writer_test.cc
+++ b/dex2oat/linker/oat_writer_test.cc
@@ -480,19 +480,23 @@ TEST_F(OatTest, WriteRead) {
              oat_class.GetType()) << descriptor;
 
     size_t method_index = 0;
-    for (auto& m : klass->GetDirectMethods(pointer_size)) {
-      CheckMethod(&m, oat_class.GetOatMethod(method_index), dex_file);
-      ++method_index;
-    }
     size_t visited_virtuals = 0;
+    for (auto& m : klass->GetMethods(pointer_size)) {
+      if (!m.IsVirtual()) {
+        CheckMethod(&m, oat_class.GetOatMethod(method_index), dex_file);
+        ++method_index;
+      }
+    }
     // TODO We should also check copied methods in this test.
-    for (auto& m : klass->GetDeclaredVirtualMethods(pointer_size)) {
-      if (!klass->IsInterface()) {
-        EXPECT_FALSE(m.IsCopied());
+    for (auto& m : klass->GetDeclaredMethods(pointer_size)) {
+      if (m.IsVirtual()) {
+        if (!klass->IsInterface()) {
+          EXPECT_FALSE(m.IsCopied());
+        }
+        CheckMethod(&m, oat_class.GetOatMethod(method_index), dex_file);
+        ++method_index;
+        ++visited_virtuals;
       }
-      CheckMethod(&m, oat_class.GetOatMethod(method_index), dex_file);
-      ++method_index;
-      ++visited_virtuals;
     }
     EXPECT_EQ(visited_virtuals, num_virtual_methods);
   }
diff --git a/dexdump/dexdump.cc b/dexdump/dexdump.cc
index dd90d90cf2..8fd6ca2f34 100644
--- a/dexdump/dexdump.cc
+++ b/dexdump/dexdump.cc
@@ -745,11 +745,11 @@ static void dumpAnnotationSetItem(const DexFile* pDexFile, const dex::Annotation
       continue;
     }
     fputs("  ", gOutFile);
-    switch (annotation->visibility_) {
-      case DexFile::kDexVisibilityBuild:   fputs("VISIBILITY_BUILD ",   gOutFile); break;
-      case DexFile::kDexVisibilityRuntime: fputs("VISIBILITY_RUNTIME ", gOutFile); break;
-      case DexFile::kDexVisibilitySystem:  fputs("VISIBILITY_SYSTEM ",  gOutFile); break;
-      default:                             fputs("VISIBILITY_UNKNOWN ", gOutFile); break;
+    switch (static_cast<DexFile::DexVisibility>(annotation->visibility_)) {
+      case DexFile::DexVisibility::kBuild:   fputs("VISIBILITY_BUILD ",   gOutFile); break;
+      case DexFile::DexVisibility::kRuntime: fputs("VISIBILITY_RUNTIME ", gOutFile); break;
+      case DexFile::DexVisibility::kSystem:  fputs("VISIBILITY_SYSTEM ",  gOutFile); break;
+      default:                               fputs("VISIBILITY_UNKNOWN ", gOutFile); break;
     }  // switch
     // Decode raw bytes in annotation.
     const u1* rData = annotation->annotation_;
diff --git a/dexopt_chroot_setup/dexopt_chroot_setup.cc b/dexopt_chroot_setup/dexopt_chroot_setup.cc
index 2e3c9f46d4..4bed83147d 100644
--- a/dexopt_chroot_setup/dexopt_chroot_setup.cc
+++ b/dexopt_chroot_setup/dexopt_chroot_setup.cc
@@ -16,7 +16,6 @@
 
 #include "dexopt_chroot_setup.h"
 
-#include <linux/mount.h>
 #include <sched.h>
 #include <sys/mount.h>
 #include <sys/stat.h>
diff --git a/disassembler/disassembler_x86.cc b/disassembler/disassembler_x86.cc
index 98201f9a27..4122a0777c 100644
--- a/disassembler/disassembler_x86.cc
+++ b/disassembler/disassembler_x86.cc
@@ -23,6 +23,7 @@
 
 #include "android-base/logging.h"
 #include "android-base/stringprintf.h"
+#include "base/casts.h"
 
 #define TWO_BYTE_VEX    0xC5
 #define THREE_BYTE_VEX  0xC4
@@ -97,6 +98,8 @@ static void DumpAnyReg(std::ostream& os, uint8_t rex, size_t reg,
     DumpReg0(os, rex, reg, byte_operand, size_override);
   } else if (reg_file == SSE) {
     os << "xmm" << reg;
+  } else if (reg_file == AVX) {
+    os << "ymm" << reg;
   } else {
     os << "mm" << reg;
   }
@@ -255,7 +258,7 @@ std::string DisassemblerX86::DumpAddress(uint8_t mod, uint8_t rm, uint8_t rex64,
 }
 
 size_t DisassemblerX86::DumpNops(std::ostream& os, const uint8_t* instr) {
-static constexpr uint8_t kNops[][10] = {
+  static constexpr uint8_t kNops[][10] = {
       { },
       { 0x90 },
       { 0x66, 0x90 },
@@ -269,12 +272,17 @@ static constexpr uint8_t kNops[][10] = {
       { 0x66, 0x2e, 0x0f, 0x1f, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00 }
   };
 
+  size_t available = dchecked_integral_cast<size_t>(GetDisassemblerOptions()->end_address_ - instr);
+  DCHECK_NE(available, 0u);
   for (size_t i = 1; i < arraysize(kNops); ++i) {
     if (memcmp(instr, kNops[i], i) == 0) {
       os << FormatInstructionPointer(instr)
          << StringPrintf(": %22s    \t       nop \n", DumpCodeHex(instr, instr + i).c_str());
       return i;
     }
+    if (i == available) {
+      break;  // Not enough data to compare longer sequences.
+    }
   }
 
   return 0;
@@ -286,6 +294,8 @@ size_t DisassemblerX86::DumpInstruction(std::ostream& os, const uint8_t* instr)
     return nop_size;
   }
 
+  const InstructionContext ctxt(this, instr);
+  instr = ctxt.shadow_instr_;
   const uint8_t* begin_instr = instr;
   bool have_prefixes = true;
   uint8_t prefix[4] = {0, 0, 0, 0};
@@ -352,7 +362,10 @@ size_t DisassemblerX86::DumpInstruction(std::ostream& os, const uint8_t* instr)
   bool no_ops = false;
   RegFile src_reg_file = GPR;
   RegFile dst_reg_file = GPR;
-
+  bool needs_vex = false;  // To detect VEX only instructions
+  bool has_3_operands = false;
+  bool width_qualified = false;
+  bool vex_operand_is_dest = false;
 
   switch (*instr) {
 #define DISASSEMBLER_ENTRY(opname, \
@@ -455,9 +468,11 @@ DISASSEMBLER_ENTRY(cmp,
       case 0x10: case 0x11:
         if (prefix[0] == 0xF2) {
           opcode1 = "movsd";
+          has_3_operands = ctxt.hasVex() && ((instr[1] & 0xC0) == 0xC0);
           prefix[0] = 0;  // clear prefix now it's served its purpose as part of the opcode
         } else if (prefix[0] == 0xF3) {
           opcode1 = "movss";
+          has_3_operands = ctxt.hasVex() && ((instr[1] & 0xC0) == 0xC0);
           prefix[0] = 0;  // clear prefix now it's served its purpose as part of the opcode
         } else if (prefix[2] == 0x66) {
           opcode1 = "movupd";
@@ -578,6 +593,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x01:
               opcode1 = "phaddw";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex();
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -585,10 +601,38 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x02:
               opcode1 = "phaddd";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex();
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
               break;
+            case 0x18:
+              opcode1 = "broadcastss";
+              needs_vex = has_modrm = load = true;
+              dst_reg_file = AVX;
+              src_reg_file = SSE;
+              break;
+            case 0x19:
+              opcode1 = "broadcastsd";
+              needs_vex = has_modrm = load = true;
+              dst_reg_file = AVX;
+              src_reg_file = SSE;
+              break;
+            case 0x1C:
+              opcode1 = "pabsb";
+              width_qualified = has_modrm = load = true;
+              src_reg_file = dst_reg_file = SSE;
+              break;
+            case 0x1D:
+              opcode1 = "pabsw";
+              width_qualified = has_modrm = load = true;
+              src_reg_file = dst_reg_file = SSE;
+              break;
+            case 0x1E:
+              opcode1 = "pabsd";
+              width_qualified = has_modrm = load = true;
+              src_reg_file = dst_reg_file = SSE;
+              break;
             case 0x29:
               opcode1 = "pcmpeqq";
               prefix[2] = 0;
@@ -599,13 +643,14 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x37:
               opcode1 = "pcmpgtq";
               prefix[2] = 0;
-              has_modrm = true;
-              load = true;
+              has_3_operands = ctxt.hasVex();
+              width_qualified = has_modrm = load = true;
               src_reg_file = dst_reg_file = SSE;
               break;
             case 0x38:
               opcode1 = "pminsb";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex();
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -613,6 +658,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x39:
               opcode1 = "pminsd";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex();
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -620,6 +666,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x3A:
               opcode1 = "pminuw";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex();
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -627,6 +674,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x3B:
               opcode1 = "pminud";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex();
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -634,6 +682,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x3C:
               opcode1 = "pmaxsb";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex();
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -641,6 +690,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x3D:
               opcode1 = "pmaxsd";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex();
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -648,6 +698,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x3E:
               opcode1 = "pmaxuw";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex();
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -655,6 +706,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x3F:
               opcode1 = "pmaxud";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex();
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -662,10 +714,40 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x40:
               opcode1 = "pmulld";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex();
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
               break;
+            case 0x58:
+              opcode1 = "pbroadcastd";
+              needs_vex = has_modrm = load = true;
+              dst_reg_file = AVX;
+              src_reg_file = SSE;
+              break;
+            case 0x59:
+              opcode1 = "pbroadcastq";
+              needs_vex = has_modrm = load = true;
+              dst_reg_file = AVX;
+              src_reg_file = SSE;
+              break;
+            case 0x78:
+              opcode1 = "pbroadcastb";
+              needs_vex = has_modrm = load = true;
+              dst_reg_file = AVX;
+              src_reg_file = SSE;
+              break;
+            case 0x79:
+              opcode1 = "pbroadcastw";
+              needs_vex = has_modrm = load = true;
+              dst_reg_file = AVX;
+              src_reg_file = SSE;
+              break;
+            case 0xA9:
+              opcode1 = ((rex & REX_W) != 0) ? "fmadd213sd" : "fmadd213ss";
+              width_qualified = has_3_operands = needs_vex = has_modrm = load = true;
+              dst_reg_file = src_reg_file = SSE;
+              break;
             default:
               opcode_tmp = StringPrintf("unknown opcode '0F 38 %02X'", *instr);
               opcode1 = opcode_tmp.c_str();
@@ -679,6 +761,12 @@ DISASSEMBLER_ENTRY(cmp,
         instr++;
         if (prefix[2] == 0x66) {
           switch (*instr) {
+            case 0x01:
+              opcode1 = "permpd";
+              width_qualified = needs_vex = has_modrm = load = true;
+              dst_reg_file = src_reg_file = AVX;
+              immediate_bytes = 1;
+              break;
             case 0x0A:
               opcode1 = "roundss";
               prefix[2] = 0;
@@ -756,6 +844,7 @@ DISASSEMBLER_ENTRY(cmp,
           case 0x5F: opcode1 = "max"; break;
           default: LOG(FATAL) << "Unreachable"; UNREACHABLE();
         }
+        has_3_operands = ctxt.hasVex();
         if (prefix[2] == 0x66) {
           opcode2 = "pd";
           prefix[2] = 0;  // clear prefix now it's served its purpose as part of the opcode
@@ -814,6 +903,7 @@ DISASSEMBLER_ENTRY(cmp,
         } else {
           src_reg_file = dst_reg_file = MMX;
         }
+        has_3_operands = ctxt.hasVex();
         switch (*instr) {
           case 0x60: opcode1 = "punpcklbw"; break;
           case 0x61: opcode1 = "punpcklwd"; break;
@@ -852,7 +942,8 @@ DISASSEMBLER_ENTRY(cmp,
         } else {
           dst_reg_file = MMX;
         }
-        opcode1 = "movd";
+        opcode1 = ((rex & REX_W) != 0) ? "movq" : "movd";
+        width_qualified = true;
         load = true;
         has_modrm = true;
         break;
@@ -903,6 +994,7 @@ DISASSEMBLER_ENTRY(cmp,
             "unknown-71", "unknown-71", "psrlw", "unknown-71",
             "psraw",      "unknown-71", "psllw", "unknown-71"};
         modrm_opcodes = x71_opcodes;
+        has_3_operands = vex_operand_is_dest = ctxt.hasVex();
         reg_is_opcode = true;
         has_modrm = true;
         store = true;
@@ -919,6 +1011,7 @@ DISASSEMBLER_ENTRY(cmp,
             "unknown-72", "unknown-72", "psrld", "unknown-72",
             "psrad",      "unknown-72", "pslld", "unknown-72"};
         modrm_opcodes = x72_opcodes;
+        has_3_operands = vex_operand_is_dest = ctxt.hasVex();
         reg_is_opcode = true;
         has_modrm = true;
         store = true;
@@ -935,6 +1028,7 @@ DISASSEMBLER_ENTRY(cmp,
             "unknown-73", "unknown-73", "psrlq", "psrldq",
             "unknown-73", "unknown-73", "psllq", "unknown-73"};
         modrm_opcodes = x73_opcodes;
+        has_3_operands = vex_operand_is_dest = ctxt.hasVex();
         reg_is_opcode = true;
         has_modrm = true;
         store = true;
@@ -954,10 +1048,15 @@ DISASSEMBLER_ENTRY(cmp,
           case 0x75: opcode1 = "pcmpeqw"; break;
           case 0x76: opcode1 = "pcmpeqd"; break;
         }
+        has_3_operands = ctxt.hasVex();
         prefix[2] = 0;
         has_modrm = true;
         load = true;
         break;
+      case 0x77:
+        needs_vex = true;
+        opcode1 = "zeroupper";
+        break;
       case 0x7C:
         if (prefix[0] == 0xF2) {
           opcode1 = "haddps";
@@ -981,7 +1080,8 @@ DISASSEMBLER_ENTRY(cmp,
         } else {
           src_reg_file = MMX;
         }
-        opcode1 = "movd";
+        opcode1 = ((rex & REX_W) != 0) ? "movq" : "movd";
+        width_qualified = true;
         has_modrm = true;
         store = true;
         break;
@@ -1181,6 +1281,7 @@ DISASSEMBLER_ENTRY(cmp,
           src_reg_file = dst_reg_file = MMX;
         }
         opcode1 = "paddq";
+        has_3_operands = ctxt.hasVex();
         prefix[2] = 0;
         has_modrm = true;
         load = true;
@@ -1193,6 +1294,7 @@ DISASSEMBLER_ENTRY(cmp,
           src_reg_file = dst_reg_file = MMX;
         }
         opcode1 = "pand";
+        has_3_operands = ctxt.hasVex();
         prefix[2] = 0;
         has_modrm = true;
         load = true;
@@ -1201,6 +1303,7 @@ DISASSEMBLER_ENTRY(cmp,
         if (prefix[2] == 0x66) {
           opcode1 = "pmullw";
           prefix[2] = 0;
+          has_3_operands = ctxt.hasVex();
           has_modrm = true;
           load = true;
           src_reg_file = dst_reg_file = SSE;
@@ -1215,6 +1318,7 @@ DISASSEMBLER_ENTRY(cmp,
       case 0xDC:
       case 0xDD:
       case 0xDE:
+      case 0xDF:
       case 0xE0:
       case 0xE3:
       case 0xE8:
@@ -1229,6 +1333,7 @@ DISASSEMBLER_ENTRY(cmp,
         } else {
           src_reg_file = dst_reg_file = MMX;
         }
+        has_3_operands = ctxt.hasVex();
         switch (*instr) {
           case 0xD8: opcode1 = "psubusb"; break;
           case 0xD9: opcode1 = "psubusw"; break;
@@ -1236,6 +1341,7 @@ DISASSEMBLER_ENTRY(cmp,
           case 0xDC: opcode1 = "paddusb"; break;
           case 0xDD: opcode1 = "paddusw"; break;
           case 0xDE: opcode1 = "pmaxub"; break;
+          case 0xDF: opcode1 = "pandn"; break;
           case 0xE0: opcode1 = "pavgb"; break;
           case 0xE3: opcode1 = "pavgw"; break;
           case 0xE8: opcode1 = "psubsb"; break;
@@ -1258,6 +1364,7 @@ DISASSEMBLER_ENTRY(cmp,
         }
         opcode1 = "por";
         prefix[2] = 0;
+        has_3_operands = ctxt.hasVex();
         has_modrm = true;
         load = true;
         break;
@@ -1269,11 +1376,13 @@ DISASSEMBLER_ENTRY(cmp,
           src_reg_file = dst_reg_file = MMX;
         }
         opcode1 = "pxor";
+        has_3_operands = ctxt.hasVex();
         prefix[2] = 0;
         has_modrm = true;
         load = true;
         break;
       case 0xF4:
+      case 0xF5:
       case 0xF6:
       case 0xF8:
       case 0xF9:
@@ -1290,6 +1399,7 @@ DISASSEMBLER_ENTRY(cmp,
         }
         switch (*instr) {
           case 0xF4: opcode1 = "pmuludq"; break;
+          case 0xF5: opcode1 = "pmaddwd"; break;
           case 0xF6: opcode1 = "psadbw"; break;
           case 0xF8: opcode1 = "psubb"; break;
           case 0xF9: opcode1 = "psubw"; break;
@@ -1299,6 +1409,7 @@ DISASSEMBLER_ENTRY(cmp,
           case 0xFD: opcode1 = "paddw"; break;
           case 0xFE: opcode1 = "paddd"; break;
         }
+        has_3_operands = ctxt.hasVex();
         prefix[2] = 0;
         has_modrm = true;
         load = true;
@@ -1496,6 +1607,25 @@ DISASSEMBLER_ENTRY(cmp,
     opcode1 = opcode_tmp.c_str();
     break;
   }
+
+  if (needs_vex && !ctxt.hasVex()) {
+    opcode_tmp = StringPrintf("unknown opcode '%02X', may be missing VEX prefix", *instr);
+    opcode1 = opcode_tmp.c_str();
+  }
+
+  // If it needs vex, then we know the exact register types before hand
+  if (!needs_vex && ctxt.hasVex() && ctxt.vex_.vector_length_ == 1) {
+    if (src_reg_file == MMX) {
+      src_reg_file = SSE;
+    } else if (src_reg_file == SSE) {
+      src_reg_file = AVX;
+    }
+    if (dst_reg_file == MMX) {
+      dst_reg_file = SSE;
+    } else if (dst_reg_file == SSE) {
+      dst_reg_file = AVX;
+    }
+  }
   std::ostringstream args;
   // We force the REX prefix to be available for 64-bit target
   // in order to dump addr (base/index) registers correctly.
@@ -1522,13 +1652,17 @@ DISASSEMBLER_ENTRY(cmp,
       opcode3 = modrm_opcodes[reg_or_opcode];
     }
 
-    // Add opcode suffixes to indicate size.
-    if (byte_operand) {
-      opcode4 = "b";
-    } else if ((rex & REX_W) != 0) {
-      opcode4 = "q";
-    } else if (prefix[2] == 0x66) {
-      opcode4 = "w";
+    // Not applicable for vex only opcodes and opcodes with
+    // fully qualified widths, ex: vpbroadcastb, vpabsb
+    if (!needs_vex && !width_qualified) {
+      // Add opcode suffixes to indicate size.
+      if (byte_operand) {
+        opcode4 = "b";
+      } else if ((rex & REX_W) != 0) {
+        opcode4 = "q";
+      } else if (prefix[2] == 0x66) {
+        opcode4 = "w";
+      }
     }
 
     if (load) {
@@ -1536,13 +1670,27 @@ DISASSEMBLER_ENTRY(cmp,
         DumpReg(args, rex, reg_or_opcode, byte_operand, prefix[2], dst_reg_file);
         args << ", ";
       }
+      if (has_3_operands) {
+        DumpAnyReg(args, 0, ctxt.vex_.operand_, false, 0, dst_reg_file);
+        args << ", ";
+      }
       DumpSegmentOverride(args, prefix[1]);
 
       args << address;
     } else {
       DCHECK(store);
       DumpSegmentOverride(args, prefix[1]);
-      args << address;
+      if (vex_operand_is_dest) {
+        DCHECK(has_3_operands);
+        DumpAnyReg(args, 0, ctxt.vex_.operand_, false, 0, dst_reg_file);
+        args << ", " << address;
+      } else {
+        args << address;
+        if (has_3_operands) {
+          args << ", ";
+          DumpAnyReg(args, 0, ctxt.vex_.operand_, false, 0, dst_reg_file);
+        }
+      }
       if (!reg_is_opcode) {
         args << ", ";
         DumpReg(args, rex, reg_or_opcode, byte_operand, prefix[2], src_reg_file);
@@ -1608,12 +1756,151 @@ DISASSEMBLER_ENTRY(cmp,
     case 0: prefix_str = ""; break;
     default: LOG(FATAL) << "Unreachable"; UNREACHABLE();
   }
-  os << FormatInstructionPointer(begin_instr)
-     << StringPrintf(": %22s    \t%-7s%s%s%s%s%s ", DumpCodeHex(begin_instr, instr).c_str(),
-                     prefix_str, opcode0, opcode1, opcode2, opcode3, opcode4)
+  opcode0 = ctxt.hasVex() ? "v" : opcode0;
+  // We may be decoding from a shadow buffer, make adjustments to read from orig buffer
+  size_t actual_bytes_read =
+      (instr - begin_instr - (ctxt.vex_.shadow_prefix_length_ - ctxt.vex_.prefix_length_));
+  const uint8_t* orig_instr_end = ctxt.orig_instr_ + actual_bytes_read;
+  os << FormatInstructionPointer(ctxt.orig_instr_)
+     << StringPrintf(": %22s    \t%-7s%s%s%s%s%s",
+                     DumpCodeHex(ctxt.orig_instr_, orig_instr_end).c_str(),
+                     prefix_str,
+                     opcode0,
+                     opcode1,
+                     opcode2,
+                     opcode3,
+                     opcode4)
+     << (args.view().empty() ? "" : " ")
      << args.str() << '\n';
-    return instr - begin_instr;
+  return actual_bytes_read;
 }  // NOLINT(readability/fn_size)
 
+DisassemblerX86::InstructionContext::InstructionContext(DisassemblerX86* disass,
+                                                        const uint8_t* instr) {
+  disassembler_ = disass;
+  orig_instr_ = instr;
+  shadow_instr_ = instr;
+  // The existing disassembler understands REX prefix
+  // In order to reuse all existing code, interpret the VEX Prefix here,
+  // and generate byte sequence as if it were emitted with a REX prefix.
+  // Later when we enable EVEX ISA we should use similar strategy.
+  if (vex_.ConvertToRex(instr,
+                        disassembler_->GetDisassemblerOptions()->end_address_,
+                        /*out*/ &shadow_instr_buffer_[0])) {
+    shadow_instr_ = shadow_instr_buffer_;
+    has_vex_ = true;
+  }
+}
+
+bool DisassemblerX86::InstructionContext::VexPrefix::ConvertToRex(const uint8_t* instr,
+                                                                  const uint8_t* instr_end,
+                                                                  /*out*/ uint8_t* decode_buffer) {
+  if (*instr != TWO_BYTE_VEX && *instr != THREE_BYTE_VEX) {
+    prefix_length_ = 0;
+    shadow_prefix_length_ = 0;
+    vector_length_ = 0;
+    operand_ = 0;
+    return false;
+  }
+  DCHECK_GT(instr_end - instr, 2);
+  memset(decode_buffer, 0, kMaxInstructionLength + 1);
+  uint8_t byte_zero = instr[0], byte_one = instr[1], byte_two = 0;
+  uint8_t pp = 0, rex = 0, mm1 = 0, mm2 = 0;
+  instr += 2;
+
+  if (byte_zero == TWO_BYTE_VEX) {
+    prefix_length_ = 2;
+    // Emit REX prefix if required
+    // Extract REX.R [7] bit
+    // Note that in VEX prefix R is stored as 1's complement
+    if (((byte_one >> 7) & 1) == 0) {
+      rex = 0x44;  // REX.0R00
+    }
+    // VEX_M is assumed as SET_VEX_M_0F for 2-byte VEX
+    mm1 = 0x0F;
+    // Extract the VEX_PP [1:0] bits as prefix
+    pp = (byte_one & 0x03);
+    // Extract VEX_L [2] bit
+    vector_length_ = (byte_one >> 2) & 0x01;
+    // Extract operand [6:3] bits is operand stored as 1's complement
+    operand_ = (~(byte_one >> 3) & 0x0F);
+  } else {
+    prefix_length_ = 3;
+    byte_two = *instr;
+    instr++;
+    // Emit REX prefix if required
+    // Extract REX.RXB [7:5] bits
+    // Note that in VEX prefix R is stored as 1's complement
+    if ((byte_one & 0xE0) != 0xE0) {
+      rex = 0x40;
+      rex |= (~(byte_one >> 5) & 0x07);
+    }
+    // Extract the VEX_M [4:0] bits
+    switch (byte_one & 0x0F) {
+      case 0x01:  // SET_VEX_M_0F
+        mm1 = 0x0F;
+        break;
+      case 0x02:  // SET_VEX_M_0F_38
+        mm1 = 0x0F;
+        mm2 = 0x38;
+        break;
+      case 0x03:  // SET_VEX_M_0F_3A
+        mm1 = 0x0F;
+        mm2 = 0x3A;
+        break;
+      default:
+        break;
+    }
+    // Extract the REX.W [7] bit
+    if ((byte_two & 0x80) != 0) {
+      rex |= 0x48;
+    }
+    // Extract the VEX_PP [1:0] bits as prefix
+    pp = (byte_two & 0x03);
+    // Extract VEX_L [2] bit
+    vector_length_ = (byte_two >> 2) & 0x01;
+    // Extract operand [6:3] bits is operand stored as 1's complement
+    operand_ = (~(byte_two >> 3) & 0x0F);
+  }
+
+  // Decode the pp prefix
+  switch (pp) {
+    case 0x01:  // SET_VEX_PP_66
+      pp = 0x66;
+      break;
+    case 0x02:  // SET_VEX_PP_F3
+      pp = 0xF3;
+      break;
+    case 0x03:  // SET_VEX_PP_F2
+      pp = 0xF2;
+      break;
+    default:  // SET_VEX_PP_NONE
+      pp = 0;
+      break;
+  }
+
+  // Resultant byte sequence to pass on
+  // pp rex mm1 mm2 instr
+  int idx = 0;
+  if (pp != 0) {
+    decode_buffer[idx++] = pp;
+  }
+  if (rex != 0) {
+    decode_buffer[idx++] = rex;
+  }
+  if (mm1 != 0) {
+    decode_buffer[idx++] = mm1;
+  }
+  if (mm2 != 0) {
+    decode_buffer[idx++] = mm2;
+  }
+  shadow_prefix_length_ = idx;
+  // Fill the remaining buffer
+  for (; idx < kMaxInstructionLength && instr < instr_end; ++instr) {
+    decode_buffer[idx++] = *instr;
+  }
+  return true;
+}
+
 }  // namespace x86
 }  // namespace art
diff --git a/disassembler/disassembler_x86.h b/disassembler/disassembler_x86.h
index a329280b70..a6d4d3c7bd 100644
--- a/disassembler/disassembler_x86.h
+++ b/disassembler/disassembler_x86.h
@@ -22,7 +22,9 @@
 namespace art {
 namespace x86 {
 
-enum RegFile { GPR, MMX, SSE };
+static constexpr uint8_t kMaxInstructionLength = 15;
+
+enum RegFile { GPR, MMX, SSE, AVX };
 
 class DisassemblerX86 final : public Disassembler {
  public:
@@ -33,6 +35,30 @@ class DisassemblerX86 final : public Disassembler {
   void Dump(std::ostream& os, const uint8_t* begin, const uint8_t* end) override;
 
  private:
+  struct InstructionContext {
+    DisassemblerX86* disassembler_;
+    bool has_vex_;
+    const uint8_t* orig_instr_;
+    const uint8_t* shadow_instr_;
+    uint8_t shadow_instr_buffer_[kMaxInstructionLength + 1];
+
+    bool hasVex() const { return has_vex_; }
+
+    struct VexPrefix {
+      uint8_t prefix_length_;
+      uint8_t shadow_prefix_length_;
+      uint8_t vector_length_;
+      uint8_t operand_;
+
+      bool ConvertToRex(const uint8_t* instr,
+                        const uint8_t* instr_end,
+                        /*out*/ uint8_t* decodeBuffer);
+    };
+    VexPrefix vex_;
+
+    InstructionContext(DisassemblerX86* disass, const uint8_t* instr);
+  };
+
   size_t DumpNops(std::ostream& os, const uint8_t* instr);
   size_t DumpInstruction(std::ostream& os, const uint8_t* instr);
 
diff --git a/libartbase/Android.bp b/libartbase/Android.bp
index 24603c1220..d78c2a833e 100644
--- a/libartbase/Android.bp
+++ b/libartbase/Android.bp
@@ -40,7 +40,7 @@ cc_defaults {
         "base/flags.cc",
         "base/hex_dump.cc",
         "base/logging.cc",
-        "base/malloc_arena_pool.cc",
+        "base/calloc_arena_pool.cc",
         "base/membarrier.cc",
         "base/memfd.cc",
         "base/memory_region.cc",
@@ -142,9 +142,15 @@ cc_defaults {
             // libdexfile, and dexdump.
             static_libs: [
                 "art-aconfig-flags-lib",
+                "art-aconfig-native-flags-lib",
+                "libaconfig_storage_read_api_cc",
+                "libcore-aconfig-flags-native-lib",
             ],
             export_static_lib_headers: [
                 "art-aconfig-flags-lib",
+                "art-aconfig-native-flags-lib",
+                "libaconfig_storage_read_api_cc",
+                "libcore-aconfig-flags-native-lib",
             ],
         },
         linux: {
@@ -154,9 +160,15 @@ cc_defaults {
             // libdexfile, and dexdump.
             static_libs: [
                 "art-aconfig-flags-lib",
+                "art-aconfig-native-flags-lib",
+                "libaconfig_storage_read_api_cc",
+                "libcore-aconfig-flags-native-lib",
             ],
             export_static_lib_headers: [
                 "art-aconfig-flags-lib",
+                "art-aconfig-native-flags-lib",
+                "libaconfig_storage_read_api_cc",
+                "libcore-aconfig-flags-native-lib",
             ],
         },
     },
@@ -167,11 +179,13 @@ cc_defaults {
 
 cc_defaults {
     name: "libartbase_static_base_defaults",
+    defaults: [
+        "art_liblog_static_defaults",
+        "art_libz_static_defaults",
+    ],
     whole_static_libs: [
         "libbase",
         "libartpalette",
-        "liblog",
-        "libz",
         "libziparchive",
     ],
     target: {
@@ -185,6 +199,25 @@ cc_defaults {
                 "libcap",
             ],
         },
+        darwin: {
+            whole_static_libs: [
+                "art-aconfig-flags-lib",
+                "art-aconfig-native-flags-lib",
+                "libaconfig_storage_read_api_cc",
+            ],
+        },
+        linux: {
+            // TODO(b/350967139): Move art-aconfig-flags-lib to the top level
+            // when aconfig supports windows. Until then it's harder to use
+            // flags in code that needs to build for them, e.g. libartbase,
+            // libdexfile, and dexdump.
+            whole_static_libs: [
+                "art-aconfig-flags-lib",
+                "art-aconfig-native-flags-lib",
+                "libaconfig_storage_read_api_cc",
+            ],
+        },
+
     },
 }
 
@@ -271,8 +304,9 @@ art_cc_library {
     },
 }
 
-art_cc_defaults {
+cc_defaults {
     name: "libartbase-art-gtest-defaults",
+    defaults: ["libart-gtest-defaults"],
     srcs: [
         "base/common_art_test.cc",
     ],
@@ -281,37 +315,46 @@ art_cc_defaults {
         // Required for "base/mutex.h" in common_art_test.cc
         "libart_headers",
     ],
-    static: {
-        whole_static_libs: [
-            "libcap",
-        ],
-    },
-    shared: {
-        static_libs: [
-            "libcap",
-        ],
-    },
+    shared_libs: [ // Since we're building static libs we're only using the headers from this.
+        "libdexfile",
+    ],
 }
 
-art_cc_library_static {
+cc_library_static {
     name: "libartbase-art-gtest",
     defaults: [
-        "libart-gtest-defaults",
         "libartbase-art-gtest-defaults",
+    ],
+}
+
+cc_defaults {
+    name: "libartbase-art-gtest_static_defaults",
+    defaults: [
         "libartbase_static_defaults",
         "libdexfile_static_defaults",
     ],
+    whole_static_libs: [
+        "libartbase-art-gtest",
+    ],
 }
 
-art_cc_library_static {
+cc_library_static {
     name: "libartbased-art-gtest",
     defaults: [
         "art_debug_defaults",
-        "libart-gtest-defaults",
         "libartbase-art-gtest-defaults",
+    ],
+}
+
+cc_defaults {
+    name: "libartbased-art-gtest_static_defaults",
+    defaults: [
         "libartbased_static_defaults",
         "libdexfiled_static_defaults",
     ],
+    whole_static_libs: [
+        "libartbase-art-gtest",
+    ],
 }
 
 art_cc_library_static {
diff --git a/libartbase/base/aconfig_flags_test.cc b/libartbase/base/aconfig_flags_test.cc
index 8d873255bf..b024459100 100644
--- a/libartbase/base/aconfig_flags_test.cc
+++ b/libartbase/base/aconfig_flags_test.cc
@@ -14,7 +14,9 @@
  * limitations under the License.
  */
 
+#include "base/flags.h"
 #include "com_android_art_flags.h"
+#include "com_android_libcore.h"
 #include "gtest/gtest.h"
 
 namespace art {
@@ -23,4 +25,12 @@ static_assert(COM_ANDROID_ART_FLAGS_TEST == true);
 
 TEST(AconfigFlagsTest, TestFlag) { EXPECT_TRUE(com::android::art::flags::test()); }
 
+TEST(AconfigFlagsTest, TestLibcoreVApisFlag) { EXPECT_TRUE(com::android::libcore::v_apis()); }
+
+TEST(AconfigFlagsTest, TestRwFlag) {
+  // EXPECT_TRUE when this flag is fully ramped.
+  // EXPECT_TRUE(is_test_rw_flag_enabled());
+  is_test_rw_flag_enabled();
+}
+
 }  // namespace art
diff --git a/libartbase/base/allocator.cc b/libartbase/base/allocator.cc
index ef073a1fa2..509b1b0114 100644
--- a/libartbase/base/allocator.cc
+++ b/libartbase/base/allocator.cc
@@ -42,7 +42,7 @@ class CallocAllocator final : public Allocator {
   DISALLOW_COPY_AND_ASSIGN(CallocAllocator);
 };
 
-CallocAllocator g_malloc_allocator;
+CallocAllocator g_calloc_allocator;
 
 class NoopAllocator final : public Allocator {
  public:
@@ -65,7 +65,7 @@ class NoopAllocator final : public Allocator {
 NoopAllocator g_noop_allocator;
 
 Allocator* Allocator::GetCallocAllocator() {
-  return &g_malloc_allocator;
+  return &g_calloc_allocator;
 }
 
 Allocator* Allocator::GetNoopAllocator() {
diff --git a/libartbase/base/arena_allocator.h b/libartbase/base/arena_allocator.h
index e47319c53e..51bc3df6ce 100644
--- a/libartbase/base/arena_allocator.h
+++ b/libartbase/base/arena_allocator.h
@@ -213,7 +213,7 @@ class Arena {
   uint8_t* memory_;
   size_t size_;
   Arena* next_;
-  friend class MallocArenaPool;
+  friend class CallocArenaPool;
   friend class MemMapArenaPool;
   friend class ArenaAllocator;
   friend class ArenaStack;
diff --git a/libartbase/base/arena_allocator_test.cc b/libartbase/base/arena_allocator_test.cc
index d956b4ab5e..f6efd8cc61 100644
--- a/libartbase/base/arena_allocator_test.cc
+++ b/libartbase/base/arena_allocator_test.cc
@@ -18,7 +18,7 @@
 #include "arena_bit_vector.h"
 #include "base/common_art_test.h"
 #include "gtest/gtest.h"
-#include "malloc_arena_pool.h"
+#include "calloc_arena_pool.h"
 #include "memory_tool.h"
 
 namespace art {
@@ -35,7 +35,7 @@ class ArenaAllocatorTest : public ::testing::Test {
 };
 
 TEST_F(ArenaAllocatorTest, Test) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaAllocator allocator(&pool);
   ArenaBitVector bv(&allocator, 10, true);
   bv.SetBit(5);
@@ -46,7 +46,7 @@ TEST_F(ArenaAllocatorTest, Test) {
 
 TEST_F(ArenaAllocatorTest, MakeDefined) {
   // Regression test to make sure we mark the allocated area defined.
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   static constexpr size_t kSmallArraySize = 10;
   static constexpr size_t kLargeArraySize = 50;
   uint32_t* small_array;
@@ -73,7 +73,7 @@ TEST_F(ArenaAllocatorTest, LargeAllocations) {
   }
 
   {
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
     // Note: Leaving some space for memory tool red zones.
     void* alloc1 = allocator.Alloc(arena_allocator::kArenaDefaultSize * 5 / 8);
@@ -82,7 +82,7 @@ TEST_F(ArenaAllocatorTest, LargeAllocations) {
     ASSERT_EQ(1u, NumberOfArenas(&allocator));
   }
   {
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
     void* alloc1 = allocator.Alloc(arena_allocator::kArenaDefaultSize * 13 / 16);
     void* alloc2 = allocator.Alloc(arena_allocator::kArenaDefaultSize * 11 / 16);
@@ -94,7 +94,7 @@ TEST_F(ArenaAllocatorTest, LargeAllocations) {
     ASSERT_EQ(3u, NumberOfArenas(&allocator));
   }
   {
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
     void* alloc1 = allocator.Alloc(arena_allocator::kArenaDefaultSize * 13 / 16);
     void* alloc2 = allocator.Alloc(arena_allocator::kArenaDefaultSize * 9 / 16);
@@ -107,7 +107,7 @@ TEST_F(ArenaAllocatorTest, LargeAllocations) {
     ASSERT_EQ(2u, NumberOfArenas(&allocator));
   }
   {
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
     void* alloc1 = allocator.Alloc(arena_allocator::kArenaDefaultSize * 9 / 16);
     void* alloc2 = allocator.Alloc(arena_allocator::kArenaDefaultSize * 13 / 16);
@@ -120,7 +120,7 @@ TEST_F(ArenaAllocatorTest, LargeAllocations) {
     ASSERT_EQ(2u, NumberOfArenas(&allocator));
   }
   {
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
     // Note: Leaving some space for memory tool red zones.
     for (size_t i = 0; i != 15; ++i) {
@@ -135,7 +135,7 @@ TEST_F(ArenaAllocatorTest, LargeAllocations) {
 }
 
 TEST_F(ArenaAllocatorTest, AllocAlignment) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaAllocator allocator(&pool);
   for (size_t iterations = 0; iterations <= 10; ++iterations) {
     for (size_t size = 1; size <= ArenaAllocator::kAlignment + 1; ++size) {
@@ -152,7 +152,7 @@ TEST_F(ArenaAllocatorTest, ReallocReuse) {
 
   {
     // Case 1: small aligned allocation, aligned extend inside arena.
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
 
     const size_t original_size = ArenaAllocator::kAlignment * 2;
@@ -165,7 +165,7 @@ TEST_F(ArenaAllocatorTest, ReallocReuse) {
 
   {
     // Case 2: small aligned allocation, non-aligned extend inside arena.
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
 
     const size_t original_size = ArenaAllocator::kAlignment * 2;
@@ -178,7 +178,7 @@ TEST_F(ArenaAllocatorTest, ReallocReuse) {
 
   {
     // Case 3: small non-aligned allocation, aligned extend inside arena.
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
 
     const size_t original_size = ArenaAllocator::kAlignment * 2 + (ArenaAllocator::kAlignment / 2);
@@ -191,7 +191,7 @@ TEST_F(ArenaAllocatorTest, ReallocReuse) {
 
   {
     // Case 4: small non-aligned allocation, aligned non-extend inside arena.
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
 
     const size_t original_size = ArenaAllocator::kAlignment * 2 + (ArenaAllocator::kAlignment / 2);
@@ -207,7 +207,7 @@ TEST_F(ArenaAllocatorTest, ReallocReuse) {
 
   {
     // Case 5: large allocation, aligned extend into next arena.
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
 
     const size_t original_size = arena_allocator::kArenaDefaultSize -
@@ -221,7 +221,7 @@ TEST_F(ArenaAllocatorTest, ReallocReuse) {
 
   {
     // Case 6: large allocation, non-aligned extend into next arena.
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
 
     const size_t original_size = arena_allocator::kArenaDefaultSize -
@@ -240,7 +240,7 @@ TEST_F(ArenaAllocatorTest, ReallocReuse) {
 TEST_F(ArenaAllocatorTest, ReallocAlignment) {
   {
     // Case 1: small aligned allocation, aligned extend inside arena.
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
 
     const size_t original_size = ArenaAllocator::kAlignment * 2;
@@ -257,7 +257,7 @@ TEST_F(ArenaAllocatorTest, ReallocAlignment) {
 
   {
     // Case 2: small aligned allocation, non-aligned extend inside arena.
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
 
     const size_t original_size = ArenaAllocator::kAlignment * 2;
@@ -274,7 +274,7 @@ TEST_F(ArenaAllocatorTest, ReallocAlignment) {
 
   {
     // Case 3: small non-aligned allocation, aligned extend inside arena.
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
 
     const size_t original_size = ArenaAllocator::kAlignment * 2 + (ArenaAllocator::kAlignment / 2);
@@ -291,7 +291,7 @@ TEST_F(ArenaAllocatorTest, ReallocAlignment) {
 
   {
     // Case 4: small non-aligned allocation, aligned non-extend inside arena.
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
 
     const size_t original_size = ArenaAllocator::kAlignment * 2 + (ArenaAllocator::kAlignment / 2);
@@ -311,7 +311,7 @@ TEST_F(ArenaAllocatorTest, ReallocAlignment) {
 
   {
     // Case 5: large allocation, aligned extend into next arena.
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
 
     const size_t original_size = arena_allocator::kArenaDefaultSize -
@@ -329,7 +329,7 @@ TEST_F(ArenaAllocatorTest, ReallocAlignment) {
 
   {
     // Case 6: large allocation, non-aligned extend into next arena.
-    MallocArenaPool pool;
+    CallocArenaPool pool;
     ArenaAllocator allocator(&pool);
 
     const size_t original_size = arena_allocator::kArenaDefaultSize -
diff --git a/libartbase/base/arena_bit_vector.h b/libartbase/base/arena_bit_vector.h
index 757f481b24..33670432a7 100644
--- a/libartbase/base/arena_bit_vector.h
+++ b/libartbase/base/arena_bit_vector.h
@@ -21,7 +21,7 @@
 
 #include "arena_object.h"
 #include "base/arena_allocator.h"
-#include "bit_vector.h"
+#include "bit_vector-inl.h"
 
 namespace art {
 
diff --git a/libartbase/base/bit_field.h b/libartbase/base/bit_field.h
index 101fbd1c19..8ab4adfad9 100644
--- a/libartbase/base/bit_field.h
+++ b/libartbase/base/bit_field.h
@@ -23,8 +23,6 @@
 
 namespace art {
 
-static constexpr uintptr_t kUintPtrTOne = 1U;
-
 // BitField is a template for encoding and decoding a bit field inside
 // an unsigned machine word.
 template<typename T, size_t kPosition, size_t kSize>
@@ -41,18 +39,20 @@ class BitField {
 
   // Tells whether the provided value fits into the bit field.
   static constexpr bool IsValid(T value) {
-    return (static_cast<uintptr_t>(value) & ~((kUintPtrTOne << size) - 1)) == 0;
+    return (static_cast<uintptr_t>(value) & ~Mask()) == 0;
   }
 
   // Returns a uword mask of the bit field.
   static constexpr uintptr_t Mask() {
-    return (kUintPtrTOne << size) - 1;
+    return (size == sizeof(uintptr_t) * kBitsPerByte)
+        ? static_cast<uintptr_t>(-1)
+        : (static_cast<uintptr_t>(1u) << size) - 1u;
   }
 
   // Returns a uword mask of the bit field which can be applied directly to
   // the raw unshifted bits.
   static constexpr uintptr_t MaskInPlace() {
-    return ((kUintPtrTOne << size) - 1) << position;
+    return Mask() << position;
   }
 
   // Returns the shift count needed to right-shift the bit field to
@@ -74,7 +74,7 @@ class BitField {
 
   // Extracts the bit field from the value.
   static constexpr T Decode(uintptr_t value) {
-    return static_cast<T>((value >> position) & ((kUintPtrTOne << size) - 1));
+    return static_cast<T>((value >> position) & Mask());
   }
 
   // Returns a uword with the bit field value encoded based on the
diff --git a/libartbase/base/bit_table.h b/libartbase/base/bit_table.h
index d6d03f3d6f..833408778d 100644
--- a/libartbase/base/bit_table.h
+++ b/libartbase/base/bit_table.h
@@ -291,6 +291,7 @@ class BitTableBuilderBase {
   explicit BitTableBuilderBase(ScopedArenaAllocator* allocator)
       : rows_(allocator->Adapter(kArenaAllocBitTableBuilder)),
         dedup_(8, allocator->Adapter(kArenaAllocBitTableBuilder)) {
+    rows_.reserve(8);
   }
 
   Entry& operator[](size_t row) { return rows_[row]; }
@@ -387,7 +388,7 @@ class BitTableBuilderBase {
   }
 
  protected:
-  ScopedArenaDeque<Entry> rows_;
+  ScopedArenaVector<Entry> rows_;
   ScopedArenaUnorderedMultimap<uint32_t, uint32_t> dedup_;  // Hash -> row index.
 };
 
@@ -404,6 +405,7 @@ class BitmapTableBuilder {
       : allocator_(allocator),
         rows_(allocator->Adapter(kArenaAllocBitTableBuilder)),
         dedup_(8, allocator_->Adapter(kArenaAllocBitTableBuilder)) {
+    rows_.reserve(8);
   }
 
   MemoryRegion operator[](size_t row) { return rows_[row]; }
@@ -423,7 +425,7 @@ class BitmapTableBuilder {
     // Check if we have already added identical bitmap.
     auto range = dedup_.equal_range(hash);
     for (auto it = range.first; it != range.second; ++it) {
-      if (MemoryRegion::ContentEquals()(region, rows_[it->second])) {
+      if (region == rows_[it->second]) {
         return it->second;
       }
     }
@@ -479,7 +481,7 @@ class BitmapTableBuilder {
 
  private:
   ScopedArenaAllocator* const allocator_;
-  ScopedArenaDeque<MemoryRegion> rows_;
+  ScopedArenaVector<MemoryRegion> rows_;
   ScopedArenaUnorderedMultimap<uint32_t, uint32_t> dedup_;  // Hash -> row index.
   size_t max_num_bits_ = 0u;
 };
diff --git a/libartbase/base/bit_table_test.cc b/libartbase/base/bit_table_test.cc
index 692861a4a9..ada8984d2b 100644
--- a/libartbase/base/bit_table_test.cc
+++ b/libartbase/base/bit_table_test.cc
@@ -22,12 +22,12 @@
 
 #include "base/arena_allocator.h"
 #include "base/bit_utils.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 
 namespace art {
 
 TEST(BitTableTest, TestEmptyTable) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
 
@@ -43,7 +43,7 @@ TEST(BitTableTest, TestEmptyTable) {
 }
 
 TEST(BitTableTest, TestSingleColumnTable) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
 
@@ -69,7 +69,7 @@ TEST(BitTableTest, TestSingleColumnTable) {
 }
 
 TEST(BitTableTest, TestUnalignedTable) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
 
@@ -89,7 +89,7 @@ TEST(BitTableTest, TestUnalignedTable) {
 }
 
 TEST(BitTableTest, TestBigTable) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
 
@@ -120,7 +120,7 @@ TEST(BitTableTest, TestBigTable) {
 }
 
 TEST(BitTableTest, TestDedup) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
 
@@ -135,7 +135,7 @@ TEST(BitTableTest, TestDedup) {
 }
 
 TEST(BitTableTest, TestBitmapTable) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
 
@@ -165,7 +165,7 @@ TEST(BitTableTest, TestBitmapTable) {
 }
 
 TEST(BitTableTest, TestCollisions) {
-  MallocArenaPool pool;
+  CallocArenaPool pool;
   ArenaStack arena_stack(&pool);
   ScopedArenaAllocator allocator(&arena_stack);
   FNVHash<MemoryRegion> hasher;
diff --git a/libartbase/base/malloc_arena_pool.cc b/libartbase/base/calloc_arena_pool.cc
similarity index 88%
rename from libartbase/base/malloc_arena_pool.cc
rename to libartbase/base/calloc_arena_pool.cc
index a3a53f7b5a..85ebc0399f 100644
--- a/libartbase/base/malloc_arena_pool.cc
+++ b/libartbase/base/calloc_arena_pool.cc
@@ -14,8 +14,7 @@
  * limitations under the License.
  */
 
-#include "malloc_arena_pool.h"
-
+#include "calloc_arena_pool.h"
 
 #include <algorithm>
 #include <cstddef>
@@ -28,10 +27,10 @@
 
 namespace art {
 
-class MallocArena final : public Arena {
+class CallocArena final : public Arena {
  public:
-  explicit MallocArena(size_t size = arena_allocator::kArenaDefaultSize);
-  virtual ~MallocArena();
+  explicit CallocArena(size_t size = arena_allocator::kArenaDefaultSize);
+  virtual ~CallocArena();
  private:
   static constexpr size_t RequiredOverallocation() {
     return (alignof(std::max_align_t) < ArenaAllocator::kArenaAlignment)
@@ -42,7 +41,7 @@ class MallocArena final : public Arena {
   uint8_t* unaligned_memory_;
 };
 
-MallocArena::MallocArena(size_t size) {
+CallocArena::CallocArena(size_t size) {
   // We need to guarantee kArenaAlignment aligned allocation for the new arena.
   // TODO: Use std::aligned_alloc() when it becomes available with C++17.
   constexpr size_t overallocation = RequiredOverallocation();
@@ -64,7 +63,7 @@ MallocArena::MallocArena(size_t size) {
   size_ = size;
 }
 
-MallocArena::~MallocArena() {
+CallocArena::~CallocArena() {
   constexpr size_t overallocation = RequiredOverallocation();
   if (overallocation != 0u && kRunningOnMemoryTool) {
     size_t head = memory_ - unaligned_memory_;
@@ -82,14 +81,14 @@ void Arena::Reset() {
   }
 }
 
-MallocArenaPool::MallocArenaPool() : free_arenas_(nullptr) {
+CallocArenaPool::CallocArenaPool() : free_arenas_(nullptr) {
 }
 
-MallocArenaPool::~MallocArenaPool() {
+CallocArenaPool::~CallocArenaPool() {
   ReclaimMemory();
 }
 
-void MallocArenaPool::ReclaimMemory() {
+void CallocArenaPool::ReclaimMemory() {
   while (free_arenas_ != nullptr) {
     Arena* arena = free_arenas_;
     free_arenas_ = free_arenas_->next_;
@@ -97,12 +96,12 @@ void MallocArenaPool::ReclaimMemory() {
   }
 }
 
-void MallocArenaPool::LockReclaimMemory() {
+void CallocArenaPool::LockReclaimMemory() {
   std::lock_guard<std::mutex> lock(lock_);
   ReclaimMemory();
 }
 
-Arena* MallocArenaPool::AllocArena(size_t size) {
+Arena* CallocArenaPool::AllocArena(size_t size) {
   Arena* ret = nullptr;
   {
     std::lock_guard<std::mutex> lock(lock_);
@@ -128,17 +127,17 @@ Arena* MallocArenaPool::AllocArena(size_t size) {
     }
   }
   if (ret == nullptr) {
-    ret = new MallocArena(size);
+    ret = new CallocArena(size);
   }
   ret->Reset();
   return ret;
 }
 
-void MallocArenaPool::TrimMaps() {
+void CallocArenaPool::TrimMaps() {
   // Nop, because there is no way to do madvise here.
 }
 
-size_t MallocArenaPool::GetBytesAllocated() const {
+size_t CallocArenaPool::GetBytesAllocated() const {
   size_t total = 0;
   std::lock_guard<std::mutex> lock(lock_);
   for (Arena* arena = free_arenas_; arena != nullptr; arena = arena->next_) {
@@ -147,7 +146,7 @@ size_t MallocArenaPool::GetBytesAllocated() const {
   return total;
 }
 
-void MallocArenaPool::FreeArenaChain(Arena* first) {
+void CallocArenaPool::FreeArenaChain(Arena* first) {
   if (kRunningOnMemoryTool) {
     for (Arena* arena = first; arena != nullptr; arena = arena->next_) {
       MEMORY_TOOL_MAKE_UNDEFINED(arena->memory_, arena->bytes_allocated_);
diff --git a/libartbase/base/malloc_arena_pool.h b/libartbase/base/calloc_arena_pool.h
similarity index 79%
rename from libartbase/base/malloc_arena_pool.h
rename to libartbase/base/calloc_arena_pool.h
index 9216c033c3..247e4f7b25 100644
--- a/libartbase/base/malloc_arena_pool.h
+++ b/libartbase/base/calloc_arena_pool.h
@@ -14,8 +14,8 @@
  * limitations under the License.
  */
 
-#ifndef ART_LIBARTBASE_BASE_MALLOC_ARENA_POOL_H_
-#define ART_LIBARTBASE_BASE_MALLOC_ARENA_POOL_H_
+#ifndef ART_LIBARTBASE_BASE_CALLOC_ARENA_POOL_H_
+#define ART_LIBARTBASE_BASE_CALLOC_ARENA_POOL_H_
 
 #include <mutex>
 
@@ -23,10 +23,10 @@
 
 namespace art {
 
-class MallocArenaPool final : public ArenaPool {
+class CallocArenaPool final : public ArenaPool {
  public:
-  MallocArenaPool();
-  ~MallocArenaPool();
+  CallocArenaPool();
+  ~CallocArenaPool();
   Arena* AllocArena(size_t size) override;
   void FreeArenaChain(Arena* first) override;
   size_t GetBytesAllocated() const override;
@@ -40,9 +40,9 @@ class MallocArenaPool final : public ArenaPool {
   // Use a std::mutex here as Arenas are at the bottom of the lock hierarchy when malloc is used.
   mutable std::mutex lock_;
 
-  DISALLOW_COPY_AND_ASSIGN(MallocArenaPool);
+  DISALLOW_COPY_AND_ASSIGN(CallocArenaPool);
 };
 
 }  // namespace art
 
-#endif  // ART_LIBARTBASE_BASE_MALLOC_ARENA_POOL_H_
+#endif  // ART_LIBARTBASE_BASE_CALLOC_ARENA_POOL_H_
diff --git a/libartbase/base/common_art_test.h b/libartbase/base/common_art_test.h
index b27199c386..e7c294fd1c 100644
--- a/libartbase/base/common_art_test.h
+++ b/libartbase/base/common_art_test.h
@@ -35,7 +35,6 @@
 #include "base/testing.h"
 #include "base/unix_file/fd_file.h"
 #include "dex/art_dex_file_loader.h"
-#include "dex/compact_dex_file.h"
 #include "gtest/gtest.h"
 
 namespace art {
diff --git a/libartbase/base/file_utils.cc b/libartbase/base/file_utils.cc
index 4253fa1ce7..31b7b268f1 100644
--- a/libartbase/base/file_utils.cc
+++ b/libartbase/base/file_utils.cc
@@ -62,10 +62,6 @@
 #include "AvailabilityMacros.h"  // For MAC_OS_X_VERSION_MAX_ALLOWED
 #endif
 
-#if defined(__linux__)
-#include <linux/unistd.h>
-#endif
-
 #ifdef ART_TARGET_ANDROID
 #include "android-modules-utils/sdk_level.h"
 #endif
@@ -92,33 +88,6 @@ static constexpr const char* kApexDefaultPath = "/apex/";
 static constexpr const char* kArtApexDataEnvVar = "ART_APEX_DATA";
 static constexpr const char* kBootImageStem = "boot";
 
-// Get the "root" directory containing the "lib" directory where this instance
-// of the libartbase library (which contains `GetRootContainingLibartbase`) is
-// located:
-// - on host this "root" is normally the Android Root (e.g. something like
-//   "$ANDROID_BUILD_TOP/out/host/linux-x86/");
-// - on target this "root" is normally the ART Root ("/apex/com.android.art").
-// Return the empty string if that directory cannot be found or if this code is
-// run on Windows or macOS.
-static std::string GetRootContainingLibartbase() {
-#if !defined(_WIN32) && !defined(__APPLE__)
-  // Check where libartbase is from, and derive from there.
-  Dl_info info;
-  if (dladdr(reinterpret_cast<const void*>(&GetRootContainingLibartbase), /* out */ &info) != 0) {
-    // Make a duplicate of the fname so dirname can modify it.
-    UniqueCPtr<char> fname(strdup(info.dli_fname));
-
-    char* dir1 = dirname(fname.get());  // This is the lib directory.
-    char* dir2 = dirname(dir1);         // This is the "root" directory.
-    if (OS::DirectoryExists(dir2)) {
-      std::string tmp = dir2;  // Make a copy here so that fname can be released.
-      return tmp;
-    }
-  }
-#endif
-  return "";
-}
-
 static const char* GetAndroidDirSafe(const char* env_var,
                                      const char* default_dir,
                                      bool must_exist,
@@ -154,7 +123,7 @@ static const char* GetAndroidDir(const char* env_var,
 
 std::string GetAndroidRootSafe(std::string* error_msg) {
 #ifdef _WIN32
-  UNUSED(kAndroidRootEnvVar, kAndroidRootDefaultPath, GetRootContainingLibartbase);
+  UNUSED(kAndroidRootEnvVar, kAndroidRootDefaultPath);
   *error_msg = "GetAndroidRootSafe unsupported for Windows.";
   return "";
 #else
@@ -162,16 +131,18 @@ std::string GetAndroidRootSafe(std::string* error_msg) {
   const char* dir = GetAndroidDirSafe(kAndroidRootEnvVar, kAndroidRootDefaultPath,
       /*must_exist=*/ true, &local_error_msg);
   if (dir == nullptr) {
-    // On host, libartbase is currently installed in "$ANDROID_ROOT/lib"
-    // (e.g. something like "$ANDROID_BUILD_TOP/out/host/linux-x86/lib". Use this
-    // information to infer the location of the Android Root (on host only).
-    //
-    // Note that this could change in the future, if we decided to install ART
-    // artifacts in a different location, e.g. within an "ART APEX" directory.
     if (!kIsTargetBuild) {
-      std::string root_containing_libartbase = GetRootContainingLibartbase();
-      if (!root_containing_libartbase.empty()) {
-        return root_containing_libartbase;
+      // On host we assume the gtest binaries are in subdirectories like
+      // .../host/testcases/art_runtime_tests/x86_64/art_runtime_tests/, so
+      // determine the root by going two levels up from that.
+      std::string argv;
+      if (android::base::ReadFileToString("/proc/self/cmdline", &argv)) {
+        // /proc/self/cmdline is the programs 'argv' with elements delimited by '\0'.
+        std::filesystem::path path(argv.substr(0, argv.find('\0')));
+        path = std::filesystem::absolute(path).parent_path();
+        if (path.has_relative_path()) {
+          return path.parent_path();
+        }
       }
     }
     *error_msg = std::move(local_error_msg);
@@ -210,7 +181,7 @@ std::string GetSystemExtRoot() {
 
 static std::string GetArtRootSafe(bool must_exist, /*out*/ std::string* error_msg) {
 #ifdef _WIN32
-  UNUSED(kAndroidArtRootEnvVar, kAndroidArtApexDefaultPath, GetRootContainingLibartbase);
+  UNUSED(kAndroidArtRootEnvVar, kAndroidArtApexDefaultPath);
   UNUSED(must_exist);
   *error_msg = "GetArtRootSafe unsupported for Windows.";
   return "";
@@ -226,30 +197,6 @@ static std::string GetArtRootSafe(bool must_exist, /*out*/ std::string* error_ms
     return android_art_root_from_env;
   }
 
-  // On target, libartbase is normally installed in
-  // "$ANDROID_ART_ROOT/lib(64)" (e.g. something like
-  // "/apex/com.android.art/lib(64)". Use this information to infer the
-  // location of the ART Root (on target only).
-  if (kIsTargetBuild) {
-    // *However*, a copy of libartbase may still be installed outside the
-    // ART Root on some occasions, as ART target gtests install their binaries
-    // and their dependencies under the Android Root, i.e. "/system" (see
-    // b/129534335). For that reason, we cannot reliably use
-    // `GetRootContainingLibartbase` to find the ART Root. (Note that this is
-    // not really a problem in practice, as Android Q devices define
-    // ANDROID_ART_ROOT in their default environment, and will instead use
-    // the logic above anyway.)
-    //
-    // TODO(b/129534335): Re-enable this logic when the only instance of
-    // libartbase on target is the one from the ART APEX.
-    if ((false)) {
-      std::string root_containing_libartbase = GetRootContainingLibartbase();
-      if (!root_containing_libartbase.empty()) {
-        return root_containing_libartbase;
-      }
-    }
-  }
-
   // Try the default path.
   if (must_exist && !OS::DirectoryExists(kAndroidArtApexDefaultPath)) {
     *error_msg =
diff --git a/libartbase/base/file_utils_test.cc b/libartbase/base/file_utils_test.cc
index ff833cf770..9f60382d00 100644
--- a/libartbase/base/file_utils_test.cc
+++ b/libartbase/base/file_utils_test.cc
@@ -141,31 +141,6 @@ TEST_F(FileUtilsTest, GetArtRootSafe) {
   ASSERT_EQ(0, setenv("ANDROID_ART_ROOT", "/this/is/obviously/bogus", /* overwrite */ 1));
   EXPECT_EQ(GetArtRootSafe(&error_msg), "");
 
-  // Inferring the ART root from the location of libartbase only works on target.
-  if (kIsTargetBuild) {
-    // Disabled for now, as we cannot reliably use `GetRootContainingLibartbase`
-    // to find the ART root on target yet (see comment in `GetArtRootSafe`).
-    //
-    // TODO(b/129534335): Re-enable this part of the test on target when the
-    // only instance of libartbase is the one from the ART APEX.
-    if ((false)) {
-      // Unset ANDROID_ART_ROOT and see that it still returns something (as
-      // libartbase code is running).
-      ASSERT_EQ(0, unsetenv("ANDROID_ART_ROOT"));
-      std::string android_art_root3 = GetArtRootSafe(&error_msg);
-      // This should be the same as the other root (modulo realpath), otherwise
-      // the test setup is broken. On non-bionic. On bionic we can be running
-      // with a different libartbase that lives outside of ANDROID_ART_ROOT.
-      UniqueCPtr<char> real_root3(realpath(android_art_root3.c_str(), nullptr));
-#if !defined(__BIONIC__) || defined(__ANDROID__)
-      UniqueCPtr<char> real_root(realpath(android_art_root.c_str(), nullptr));
-      EXPECT_STREQ(real_root.get(), real_root3.get()) << error_msg;
-#else
-      EXPECT_STRNE(real_root3.get(), "") << error_msg;
-#endif
-    }
-  }
-
   // Reset ANDROID_ART_ROOT, as other things may depend on it.
   ASSERT_EQ(0, setenv("ANDROID_ART_ROOT", android_art_root_env.c_str(), /* overwrite */ 1));
 }
diff --git a/libartbase/base/flags.cc b/libartbase/base/flags.cc
index 6cf9ef1c99..6c76044f36 100644
--- a/libartbase/base/flags.cc
+++ b/libartbase/base/flags.cc
@@ -21,8 +21,11 @@
 #include "android-base/parsebool.h"
 #include "android-base/parseint.h"
 #include "android-base/properties.h"
-
 #include "base/utils.h"
+// TODO(b/350967139): Remove #ifndef when aconfig supports windows.
+#ifndef _WIN32
+#include "com_android_art_native_flags.h"
+#endif
 
 #pragma clang diagnostic push
 #pragma clang diagnostic error "-Wconversion"
@@ -176,6 +179,10 @@ template class Flag<bool>;
 template class Flag<int>;
 template class Flag<std::string>;
 
+// TODO(b/350967139): Remove #ifndef when aconfig supports windows.
+#ifndef _WIN32
+bool is_test_rw_flag_enabled() { return com_android_art_native_flags_test_rw(); }
+#endif
 }  // namespace art
 
 #pragma clang diagnostic pop  // -Wconversion
diff --git a/libartbase/base/flags.h b/libartbase/base/flags.h
index cec6c584ff..cbd21c9bde 100644
--- a/libartbase/base/flags.h
+++ b/libartbase/base/flags.h
@@ -319,6 +319,8 @@ struct Flags {
 // This is the actual instance of all the flags.
 extern Flags gFlags;
 
+EXPORT bool is_test_rw_flag_enabled();
+
 }  // namespace art
 
 #pragma clang diagnostic pop  // -Wconversion
diff --git a/libartbase/base/iteration_range.h b/libartbase/base/iteration_range.h
index 0685d591ee..91aecf3981 100644
--- a/libartbase/base/iteration_range.h
+++ b/libartbase/base/iteration_range.h
@@ -50,11 +50,10 @@ inline IterationRange<Iter> MakeIterationRange(const Iter& begin_it, const Iter&
   return IterationRange<Iter>(begin_it, end_it);
 }
 
-template <typename List>
-inline auto MakeIterationRange(List& list) -> IterationRange<decltype(list.begin())> {
-  static_assert(std::is_same_v<decltype(list.begin()), decltype(list.end())>,
-                "Different iterator types");
-  return MakeIterationRange(list.begin(), list.end());
+template <typename Container>
+inline auto MakeIterationRange(Container&& c) -> IterationRange<decltype(c.begin())> {
+  static_assert(std::is_same_v<decltype(c.begin()), decltype(c.end())>, "Different iterator types");
+  return MakeIterationRange(c.begin(), c.end());
 }
 
 template <typename Iter>
diff --git a/libartbase/base/macros.h b/libartbase/base/macros.h
index 3b8b8ff89e..189161b9d1 100644
--- a/libartbase/base/macros.h
+++ b/libartbase/base/macros.h
@@ -22,6 +22,7 @@
 
 #include "android-base/format.h"
 #include "android-base/macros.h"
+#include "android-base/stringify.h"
 #include "android-base/thread_annotations.h"
 
 // Declare a friend relationship in a class with a test. Used rather that FRIEND_TEST to avoid
@@ -63,10 +64,6 @@ template<typename T> ART_FRIEND_TEST(test_set_name, individual_test)
 #define ALIGNED(x) __attribute__ ((__aligned__(x)))
 #define PACKED(x) __attribute__ ((__aligned__(x), __packed__))
 
-// Stringify the argument.
-#define QUOTE(x) #x
-#define STRINGIFY(x) QUOTE(x)
-
 // Append tokens after evaluating.
 #define APPEND_TOKENS_AFTER_EVAL_2(a, b) a ## b
 #define APPEND_TOKENS_AFTER_EVAL(a, b) APPEND_TOKENS_AFTER_EVAL_2(a, b)
diff --git a/libartbase/base/mem_map_test.cc b/libartbase/base/mem_map_test.cc
index 76a50a59e4..1afc930bb4 100644
--- a/libartbase/base/mem_map_test.cc
+++ b/libartbase/base/mem_map_test.cc
@@ -20,6 +20,7 @@
 #include <random>
 
 #include "bit_utils.h"
+#include "casts.h"
 #include "common_art_test.h"
 #include "logging.h"
 #include "memory_tool.h"
diff --git a/libartbase/base/memfd.h b/libartbase/base/memfd.h
index b288f7bc37..b304005cab 100644
--- a/libartbase/base/memfd.h
+++ b/libartbase/base/memfd.h
@@ -20,47 +20,9 @@
 #include <fcntl.h>
 #include <unistd.h>
 
-#if defined(__BIONIC__)
-#include <linux/memfd.h>  // To access memfd flags.
-#else
-
-// If memfd flags don't exist in the current toolchain, define them ourselves.
-#ifndef F_ADD_SEALS
-# define F_ADD_SEALS          (1033)
-#endif
-
-#ifndef F_GET_SEALS
-# define F_GET_SEALS          (1034)
-#endif
-
-#ifndef F_SEAL_SEAL
-# define F_SEAL_SEAL          0x0001
-#endif
-
-#ifndef F_SEAL_SHRINK
-# define F_SEAL_SHRINK        0x0002
-#endif
-
-#ifndef F_SEAL_GROW
-# define F_SEAL_GROW          0x0004
-#endif
-
-#ifndef F_SEAL_WRITE
-# define F_SEAL_WRITE         0x0008
-#endif
-
-#ifndef F_SEAL_FUTURE_WRITE
-# define F_SEAL_FUTURE_WRITE  0x0010
-#endif
-
-#ifndef MFD_CLOEXEC
-# define MFD_CLOEXEC    0x0001U
-#endif
-
-#ifndef MFD_ALLOW_SEALING
-# define MFD_ALLOW_SEALING    0x0002U
-#endif
-
+// This header is also used on Windows.
+#if __has_include(<sys/mman.h>)
+#include <sys/mman.h>
 #endif
 
 namespace art {
diff --git a/libartbase/base/memory_region.h b/libartbase/base/memory_region.h
index 6265762ffb..02e699af0c 100644
--- a/libartbase/base/memory_region.h
+++ b/libartbase/base/memory_region.h
@@ -36,11 +36,9 @@ namespace art {
 // of the region.
 class MemoryRegion final : public ValueObject {
  public:
-  struct ContentEquals {
-    constexpr bool operator()(const MemoryRegion& lhs, const MemoryRegion& rhs) const {
-      return lhs.size() == rhs.size() && memcmp(lhs.begin(), rhs.begin(), lhs.size()) == 0;
-    }
-  };
+  constexpr bool operator==(const MemoryRegion& other) const {
+    return size() == other.size() && memcmp(begin(), other.begin(), size()) == 0;
+  }
 
   MemoryRegion() : pointer_(nullptr), size_(0) {}
   MemoryRegion(void* pointer_in, uintptr_t size_in) : pointer_(pointer_in), size_(size_in) {}
diff --git a/libartbase/base/sdk_version.h b/libartbase/base/sdk_version.h
index b955ab0322..7760cf75d1 100644
--- a/libartbase/base/sdk_version.h
+++ b/libartbase/base/sdk_version.h
@@ -39,6 +39,8 @@ enum class SdkVersion : uint32_t {
   kS_V2  = 32u,
   kT     = 33u,
   kU     = 34u,
+  kV     = 35u,
+  kB     = 36u,
   kMax   = std::numeric_limits<uint32_t>::max(),
 };
 
diff --git a/libartbase/base/stl_util.h b/libartbase/base/stl_util.h
index a6ab16c0d4..4af7b0a36d 100644
--- a/libartbase/base/stl_util.h
+++ b/libartbase/base/stl_util.h
@@ -146,7 +146,7 @@ struct FNVHash {
 
 // Returns a copy of the passed vector that doesn't memory-own its entries.
 template <typename T>
-static inline std::vector<T*> MakeNonOwningPointerVector(const std::vector<std::unique_ptr<T>>& src) {
+inline std::vector<T*> MakeNonOwningPointerVector(const std::vector<std::unique_ptr<T>>& src) {
   std::vector<T*> result;
   result.reserve(src.size());
   for (const std::unique_ptr<T>& t : src) {
@@ -224,26 +224,26 @@ class CountIter {
 };
 
 // Make an iteration range that returns a pair of the element and the index of the element.
-template <typename Iter>
-static inline IterationRange<ZipLeftIter<Iter, CountIter>> ZipCount(IterationRange<Iter> iter) {
-  return IterationRange(ZipLeftIter(iter.begin(), CountIter(0)),
-                        ZipLeftIter(iter.end(), CountIter(-1)));
+template <typename Container>
+inline auto ZipCount(Container&& c) -> IterationRange<ZipLeftIter<decltype(c.begin()), CountIter>>{
+  static_assert(std::is_same_v<decltype(c.begin()), decltype(c.end())>, "Different iterator types");
+  return IterationRange(ZipLeftIter(c.begin(), CountIter(0)), ZipLeftIter(c.end(), CountIter(-1)));
 }
 
 // Make an iteration range that returns a pair of the outputs of two iterators. Stops when the first
 // (left) one is exhausted. The left iterator must be at least as long as the right one.
 template <typename IterLeft, typename IterRight>
-static inline IterationRange<ZipLeftIter<IterLeft, IterRight>> ZipLeft(
+inline IterationRange<ZipLeftIter<IterLeft, IterRight>> ZipLeft(
     IterationRange<IterLeft> iter_left, IterationRange<IterRight> iter_right) {
   return IterationRange(ZipLeftIter(iter_left.begin(), iter_right.begin()),
                         ZipLeftIter(iter_left.end(), iter_right.end()));
 }
 
-static inline IterationRange<CountIter> Range(size_t start, size_t end) {
+inline IterationRange<CountIter> Range(size_t start, size_t end) {
   return IterationRange(CountIter(start), CountIter(end));
 }
 
-static inline IterationRange<CountIter> Range(size_t end) {
+inline IterationRange<CountIter> Range(size_t end) {
   return Range(0, end);
 }
 
@@ -294,7 +294,7 @@ struct FilterIterator {
 };
 
 template <typename BaseRange, typename FilterT>
-static inline auto Filter(BaseRange&& range, FilterT cond) {
+inline auto Filter(BaseRange&& range, FilterT cond) {
   auto end = range.end();
   auto start = std::find_if(range.begin(), end, cond);
   return MakeIterationRange(FilterIterator(start, cond, std::make_optional(end)),
@@ -314,7 +314,7 @@ template <typename InnerIter>
 using FilterNull = FilterIterator<InnerIter, NonNullFilter<typename InnerIter::value_type>>;
 
 template <typename InnerIter>
-static inline IterationRange<FilterNull<InnerIter>> FilterOutNull(IterationRange<InnerIter> inner) {
+inline IterationRange<FilterNull<InnerIter>> FilterOutNull(IterationRange<InnerIter> inner) {
   return Filter(inner, NonNullFilter<typename InnerIter::value_type>());
 }
 
diff --git a/libartbase/base/utils.cc b/libartbase/base/utils.cc
index 3f057604c1..3b9c8bb5ee 100644
--- a/libartbase/base/utils.cc
+++ b/libartbase/base/utils.cc
@@ -45,7 +45,6 @@
 #endif
 
 #if defined(__linux__)
-#include <linux/unistd.h>
 // NOLINTNEXTLINE - inclusion of syscall is dependent on arch
 #include <sys/syscall.h>
 #endif
diff --git a/libartpalette/apex/palette.cc b/libartpalette/apex/palette.cc
index 3efee45173..0f33096717 100644
--- a/libartpalette/apex/palette.cc
+++ b/libartpalette/apex/palette.cc
@@ -254,11 +254,17 @@ palette_status_t PaletteSetTaskProfiles(int32_t tid,
   return m(tid, profiles, profiles_len);
 }
 
-// Methods in version 4 API, corresponding to SDK level 36.
+// Introduced in version 4 API, corresponding to SDK level 36.
 palette_status_t PaletteDebugStoreGetString(char* result, size_t max_size) {
   PaletteDebugStoreGetStringMethod m =
       PaletteLoader::Instance().GetPaletteDebugStoreGetStringMethod();
   return m(result, max_size);
 }
 
+// Introduced in version 5 API, corresponding to SDK level 36.1.
+palette_status_t PaletteMapPriority(int32_t managed_priority, int* result) {
+  PaletteMapPriorityMethod m = PaletteLoader::Instance().GetPaletteMapPriorityMethod();
+  return m(managed_priority, result);
+}
+
 }  // extern "C"
diff --git a/libartpalette/apex/palette_test.cc b/libartpalette/apex/palette_test.cc
index a72e523365..648a914155 100644
--- a/libartpalette/apex/palette_test.cc
+++ b/libartpalette/apex/palette_test.cc
@@ -25,6 +25,7 @@
 
 #include "base/testing.h"
 #include "gtest/gtest.h"
+#include "system/palette_system.h"
 
 #ifdef ART_TARGET_ANDROID
 #include "android-modules-utils/sdk_level.h"
@@ -43,6 +44,7 @@ pid_t GetTid() {
 }
 
 #ifdef ART_TARGET_ANDROID
+
 bool PaletteSetTaskProfilesIsSupported(palette_status_t res) {
   if (android::modules::sdklevel::IsAtLeastU()) {
     return true;
@@ -51,10 +53,21 @@ bool PaletteSetTaskProfilesIsSupported(palette_status_t res) {
       << "Device API level: " << android_get_device_api_level();
   return false;
 }
-bool PaletteDebugStoreIsSupported() {
-  // TODO(b/345433959): Switch to android::modules::sdklevel::IsAtLeastW
-  return android_get_device_api_level() >= 36;
+bool PaletteDebugStoreIsSupported() { return android::modules::sdklevel::IsAtLeastB(); }
+
+bool PaletteMapPriorityIsSupported() {
+  // TODO: Switch to android::modules::sdklevel::IsAtLeastC
+  int dummy_result;
+  return android_get_device_api_level() >= 37 ||
+         PaletteMapPriority(6, &dummy_result) != PALETTE_STATUS_NOT_SUPPORTED;
+}
+
+#else  // !ART_TARGET_ANDROID
+
+bool PaletteMapPriorityIsSupported() {
+  return true;  // Safe?
 }
+
 #endif
 
 }  // namespace
@@ -136,16 +149,29 @@ TEST_F(PaletteClientTest, JniInvocation) {
 #endif
 }
 
-TEST_F(PaletteClientTest, SetTaskProfiles) {
+// Run with the expectation to be both root and non-root, skipping the
+// inapplicable one. Useful when expected results depend on rootness, to make it
+// clear in test results which variant was executed.
+class PaletteClientRootParamTest : public ::testing::TestWithParam<bool> {
+ protected:
+  bool TestAsRoot() { return GetParam(); }
+  bool RunningAsRoot() { return getuid() == 0; }
+  bool ShouldSkip() { return TestAsRoot() != RunningAsRoot(); }
+};
+
+TEST_P(PaletteClientRootParamTest, SetTaskProfiles) {
 #ifndef ART_TARGET_ANDROID
   GTEST_SKIP() << "SetTaskProfiles is only supported on Android";
 #else
+  if (ShouldSkip()) {
+    GTEST_SKIP() << (RunningAsRoot() ? "Running as root" : "Not running as root");
+  }
   const char* profiles[] = {"ProcessCapacityHigh", "TimerSlackNormal"};
   palette_status_t res = PaletteSetTaskProfiles(GetTid(), &profiles[0], 2);
   if (PaletteSetTaskProfilesIsSupported(res)) {
     // SetTaskProfiles will only work fully if we run as root. Otherwise it'll
     // return false which is mapped to PALETTE_STATUS_FAILED_CHECK_LOG.
-    if (getuid() == 0) {
+    if (TestAsRoot()) {
       EXPECT_EQ(PALETTE_STATUS_OK, res);
     } else {
       EXPECT_EQ(PALETTE_STATUS_FAILED_CHECK_LOG, res);
@@ -154,16 +180,19 @@ TEST_F(PaletteClientTest, SetTaskProfiles) {
 #endif
 }
 
-TEST_F(PaletteClientTest, SetTaskProfilesCpp) {
+TEST_P(PaletteClientRootParamTest, SetTaskProfilesCpp) {
 #ifndef ART_TARGET_ANDROID
   GTEST_SKIP() << "SetTaskProfiles is only supported on Android";
 #else
+  if (ShouldSkip()) {
+    GTEST_SKIP() << (RunningAsRoot() ? "Running as root" : "Not running as root");
+  }
   std::vector<std::string> profiles = {"ProcessCapacityHigh", "TimerSlackNormal"};
   palette_status_t res = PaletteSetTaskProfiles(GetTid(), profiles);
   if (PaletteSetTaskProfilesIsSupported(res)) {
     // SetTaskProfiles will only work fully if we run as root. Otherwise it'll
     // return false which is mapped to PALETTE_STATUS_FAILED_CHECK_LOG.
-    if (getuid() == 0) {
+    if (TestAsRoot()) {
       EXPECT_EQ(PALETTE_STATUS_OK, res);
     } else {
       EXPECT_EQ(PALETTE_STATUS_FAILED_CHECK_LOG, res);
@@ -172,6 +201,14 @@ TEST_F(PaletteClientTest, SetTaskProfilesCpp) {
 #endif
 }
 
+INSTANTIATE_TEST_SUITE_P(
+    Rootness,
+    PaletteClientRootParamTest,
+    ::testing::Bool(),
+    [](const ::testing::TestParamInfo<PaletteClientRootParamTest::ParamType>& info) {
+      return info.param ? "Root" : "NonRoot";
+    });
+
 TEST_F(PaletteClientTest, DebugStore) {
 #ifndef ART_TARGET_ANDROID
   GTEST_SKIP() << "DebugStore is only supported on Android";
@@ -193,3 +230,33 @@ TEST_F(PaletteClientTest, DebugStore) {
   EXPECT_EQ(strncmp(result.data() + len - strlen(end), end, strlen(end)), 0);
 #endif
 }
+
+TEST_F(PaletteClientTest, MapPriority) {
+  // Make sure the we are on a correct API level.
+  if (!PaletteMapPriorityIsSupported()) {
+    GTEST_SKIP() << "GetPriorityMapping is only supported on API 36.1+";
+  }
+  int result;
+  int last_result = 100;  // > any plausible niceness value.
+  palette_status_t pstatus;
+  for (int32_t i = art::palette::kMinManagedThreadPriority;
+       i <= art::palette::kMaxManagedThreadPriority;
+       ++i) {
+    pstatus = PaletteMapPriority(i, &result);
+    EXPECT_EQ(PALETTE_STATUS_OK, pstatus);
+    if (i == art::palette::kMinManagedThreadPriority) {
+      EXPECT_GT(result, 0);
+    }
+    if (i == art::palette::kMaxManagedThreadPriority) {
+      EXPECT_LT(result, 0);
+    }
+    EXPECT_LT(result, last_result);
+    last_result = result;
+  }
+
+  // This time with invalid Java priorities.
+  pstatus = PaletteMapPriority(art::palette::kMinManagedThreadPriority - 1, &result);
+  EXPECT_EQ(PALETTE_STATUS_INVALID_ARGUMENT, pstatus);
+  pstatus = PaletteMapPriority(art::palette::kMaxManagedThreadPriority + 1, &result);
+  EXPECT_EQ(PALETTE_STATUS_INVALID_ARGUMENT, pstatus);
+}
diff --git a/libartpalette/include/palette/palette_method_list.h b/libartpalette/include/palette/palette_method_list.h
index b2f0476892..3bf16ee3f8 100644
--- a/libartpalette/include/palette/palette_method_list.h
+++ b/libartpalette/include/palette/palette_method_list.h
@@ -77,8 +77,7 @@
   /*         internal unstable API. */                                                        \
   M(PaletteSetTaskProfiles, int32_t tid, const char* const profiles[], size_t profiles_len)   \
                                                                                               \
-  /* Methods in version 4 API, corresponding to SDK level 36. */                              \
-                                                                                              \
+  /* Introduced in version 4 API, corresponding to SDK level 36. */                           \
   /* Retrieves the debug store as a string. */                                                \
   /* */                                                                                       \
   /* This function retrieves debug information stored in a predefined debug store. */         \
@@ -92,6 +91,16 @@
   /*                 up to max_size characters. */                                            \
   /* @return PALETTE_STATUS_OK if the call succeeded. */                                      \
   /*          PALETTE_STATUS_INVALID_ARGUMENT if the pointer is a nullptr or max_size is 0 */ \
-  M(PaletteDebugStoreGetString, char* result, size_t max_size)
+  M(PaletteDebugStoreGetString, char* result, size_t max_size)                                \
+                                                                                              \
+  /* Introduced in version 5 API, corresponding to SDK level 36.1. */                         \
+  /* Retrieve the nice value (as used by Posix setpriority()) corresponding to a managed */   \
+  /* (Java) thread priority. */                                                               \
+  /* */                                                                                       \
+  /* @param managed_priority  The Java priority for which we want the niceness value. */      \
+  /* @param result  A pointer to an int in which to store the niceness result. */             \
+  /* @return PALETTE_STATUS_OK if the call succeeded. */                                      \
+  /*         PALETTE_STATUS_INVALID_ARGUMENT if the argument is not a valid Java priority. */ \
+  M(PaletteMapPriority, int32_t managed_priority, /*out*/ int* result)
 
 #endif  // ART_LIBARTPALETTE_INCLUDE_PALETTE_PALETTE_METHOD_LIST_H_
diff --git a/libartpalette/libartpalette.map b/libartpalette/libartpalette.map
index e3c36f5277..c48945002b 100644
--- a/libartpalette/libartpalette.map
+++ b/libartpalette/libartpalette.map
@@ -56,4 +56,11 @@ LIBARTPALETTE_4 { # introduced=36
   global:
     # --- VERSION 04 API ---
     PaletteDebugStoreGetString; # apex
-} LIBARTPALETTE_3;
\ No newline at end of file
+} LIBARTPALETTE_3;
+
+LIBARTPALETTE_5 { # introduced=37
+# Also available in 36.1
+  global:
+    # --- VERSION 05 API ---
+    PaletteMapPriority; # apex
+} LIBARTPALETTE_4;
diff --git a/libartpalette/system/palette_fake.cc b/libartpalette/system/palette_fake.cc
index 444ea0941c..b33cc14369 100644
--- a/libartpalette/system/palette_fake.cc
+++ b/libartpalette/system/palette_fake.cc
@@ -14,6 +14,10 @@
  * limitations under the License.
  */
 
+// This is essentially, but not quite, a copy of system/libartpalette/palette_fake.cc.
+// THEY SHOULD BE UPDATED AT THE SAME TIME.
+// TODO(b/265435354): Reconstruct if / why this is necessary.
+
 #include <android-base/logging.h>
 #include <stdbool.h>
 
@@ -24,6 +28,8 @@
 #include "palette_system.h"
 
 // Cached thread priority for testing. No thread priorities are ever affected.
+// Assumes thread priority is adjusted only through this interface, which is incorrect for
+// production code, but valid for relevant tests.
 static std::mutex g_tid_priority_map_mutex;
 static std::map<int32_t, int32_t> g_tid_priority_map;
 
@@ -50,6 +56,18 @@ palette_status_t PaletteSchedGetPriority(int32_t tid,
   return PALETTE_STATUS_OK;
 }
 
+// Introduced in version 5 API, corresponding to SDK level 36.1.
+palette_status_t PaletteMapPriority(int32_t managed_priority, /*out*/ int* result) {
+  if (managed_priority < art::palette::kMinManagedThreadPriority ||
+      managed_priority > art::palette::kMaxManagedThreadPriority) {
+    return PALETTE_STATUS_INVALID_ARGUMENT;
+  }
+  // Some test code assumes these are monotically decreasing, so we can reconstruct priority
+  // from niceness.
+  *result = 10 - 2 * managed_priority;
+  return PALETTE_STATUS_OK;
+}
+
 palette_status_t PaletteWriteCrashThreadStacks(/*in*/ const char* stacks, size_t stacks_len) {
   LOG(INFO) << std::string_view(stacks, stacks_len);
   return PALETTE_STATUS_OK;
@@ -148,7 +166,7 @@ palette_status_t PaletteSetTaskProfiles([[maybe_unused]] int32_t tid,
   return PALETTE_STATUS_OK;
 }
 
-// Methods in version 4 API, corresponding to SDK level 36.
+// Introduced in version 4 API, corresponding to SDK level 36.
 palette_status_t PaletteDebugStoreGetString([[maybe_unused]] char* result,
                                             [[maybe_unused]] size_t max_size) {
   result[0] = '\0';
diff --git a/libartservice/service/Android.bp b/libartservice/service/Android.bp
index e1a16197b0..655e63ee4c 100644
--- a/libartservice/service/Android.bp
+++ b/libartservice/service/Android.bp
@@ -270,17 +270,13 @@ android_test {
     ],
 
     jni_libs: [
-        "libartservice",
         // The two libraries below are required by ExtendedMockito.
         "libdexmakerjvmtiagent",
         "libstaticjvmtiagent",
     ],
     compile_multilib: "first",
 
-    // TODO: This module should move to sdk_version: "system_server_current" when possible,
-    //   as this will restrict the APIs available to just that expected system API. For now,
-    //   a compileOnly / runtimeOnly split for dependencies doesn't exist in the build system
-    //   and so it's not possible to enforce.
+    sdk_version: "system_server_current",
     min_sdk_version: "31",
 
     test_suites: ["general-tests"],
diff --git a/libartservice/service/README.md b/libartservice/service/README.md
index b7363ee184..1fff08ca8e 100644
--- a/libartservice/service/README.md
+++ b/libartservice/service/README.md
@@ -6,39 +6,78 @@ ART Service manages dexopt artifacts of apps. With ART Service, you can dexopt
 apps, query their dexopt status (the compiler filter, the compilation reason,
 whether the dexopt artifacts are up-to-date, etc.), and delete dexopt artifacts.
 
-Note: ART Service is introduced in Android U. Prior to ART Service, dexopt
+Note: ART Service is introduced in Android 14. Prior to ART Service, dexopt
 artifacts were managed by Package Manager with a legacy implementation. The
-legacy implementation will be removed in Android V. This doc only describes
-ART Service, not the legacy implementation.
+legacy implementation will be removed in future releases. This doc only
+describes ART Service, not the legacy implementation.
+
+## Dexopt
+
+*Dexopt* stands for the procedure of ahead-of-time (AOT) DEX optimization.
+Specifically, at the time of writing, it includes
+
+-   *Compilation*: Compiles the DEX code into native code.
+-   *Verification*: Verifies the DEX code against Java verification rules and
+    persists verification metadata.
+-   *Class resolution & initialization*: Resolves classes in the DEX code into
+    ART's internal data structures and persists a memory dump of the data
+    structures. For classes that can be statically initialized (i.e., the static
+    initializer can be executed at dexopt time), also initializes the classes
+    and saves the result.
+-   *Extraction*: If the DEX code is compressed, decompresses it and persists
+    the uncompressed copy.
+
+## Glossary
+
+-   *Dexopt*: The procedure of ahead-of-time (AOT) DEX optimization, as
+    described above.
+-   *dex2oat*: The tool to perform dexopt.
+-   *Dexpreopt*: Dexopt performed at build time on build host (when the Android
+    system image is being built). This is not in the scope of ART Service, as
+    ART Service runs on device.
+-   *DEX files*: The files that contain DEX code, including APKs, JARs, and
+    plain DEX files. Strictly speaking, an APK/JAR file is not a DEX file. It is
+    a ZIP file that contain one or more plain DEX files. However, it is called a
+    *DEX file* conventionally.
 
 ## Concepts
 
-### Primary dex vs. secondary dex
+### Compiler filters
 
-ART Service dexopts both primary dex files and secondary dex files of an app.
+One core ART option to configure is the compiler filter. The compiler filter
+drives how ART dexopts DEX code and is an option passed to the `dex2oat` tool.
+At the time of writing, there are three officially supported filters:
 
-A primary dex file refers to the base APK or a split APK of an app. It's
-installed by Package Manager or shipped as a part of the system image, and it's
-loaded by Framework on app startup.
+-   `verify`: Performs only *verification* and *extraction* (no *compilation* or
+    *class resolution & initialization*).
+-   `speed`: Performs *verification* and *extraction*, and performs
+    *compilation* for all methods (no *class resolution & initialization*).
+-   `speed-profile`: Performs *verification* and *extraction*, performs
+    *compilation* for methods listed in the profile, and performs *class
+    resolution & initialization* for classes listed in the profile.
 
-A secondary dex file refers to an APK or JAR file that an app adds to its own
-data directory and loads dynamically.
+### Dexopt dependencies
 
-Note: Strictly speaking, an APK/JAR file is not a DEX file. It is a ZIP file
-that contain one or more DEX files. However, it is called a *dex file*
-conventionally.
+To dexopt a DEX file, dex2oat needs to know all its dependencies. This includes
+bootclasspath jars, boot images, and class loader context (CLC, consisting of
+shared libraries and other splits of the same app).
 
-### Compiler filters
+When the runtime loads dexopt artifacts at execution time, it performs a check
+on the dexopt-time dependencies against the actual runtime dependencies. The
+dexopt-time dependencies and the runtime dependencies must exactly match.
 
-See
-[Compilation options](https://source.android.com/docs/core/runtime/configure#compilation_options).
+In the case of a dependency mismatch, the runtime is unable to use the result of
+*compilation* and *class resolution & initialization*. I.e., the result of
+*verification* and *extraction* can still be used, and dexopt artifacts are used
+as if the DEX file in the `verify` state, regardless of the actual compiler
+filter (see [The `vdex` reason](#the-reason)).
 
 ### Priority classes
 
 A priority class indicates the priority of an operation. The value affects the
-resource usage and the process priority. A higher value may result in faster
-execution but may consume more resources and compete for resources with other
-processes.
+resource usage (e.g., CPU cores) and the process priority. A higher value may
+result in faster execution but may consume more resources and compete for
+resources with other processes.
 
 Options are (from the highest to the lowest):
 
@@ -46,8 +85,8 @@ Options are (from the highest to the lowest):
 -   `PRIORITY_INTERACTIVE_FAST`: Indicates that a human is waiting on the result
     and the operation is more latency sensitive than usual. It's typically used
     when the user is entirely blocked, such as for restoring from cloud backup.
--   `PRIORITY_INTERACTIVE`: Indicates that a human is waiting on the result.
-    (E.g., for app install)
+-   `PRIORITY_INTERACTIVE`: Indicates that a human is waiting on the result
+    (e.g., for app install).
 -   `PRIORITY_BACKGROUND`: Indicates that the operation runs in background.
 
 ### Compilation reasons
@@ -70,19 +109,57 @@ oatdump --header-only --oat-file=<odex-filename> | grep 'compilation-reason ='
 ```
 
 It can be either a predefined value in
-`art/libartservice/service/java/com/android/server/art/ReasonMapping.java`
-or a custom string. If the value is a custom string, the priority class and the
+`art/libartservice/service/java/com/android/server/art/ReasonMapping.java` or a
+custom string. If the value is a custom string, the priority class and the
 compiler filter must be explicitly set.
 
 Each predefined value corresponds to one of the
 [dexopt scenarios](#dexopt-scenarios).
 
+#### The `vdex` reason
+
+The `vdex` reason is a special *compilation reason* indicating that a dex file
+is dexopted but there is a dependency mismatch. You may see it in the dexopt
+state dump (`pm art dump [<package-name>]` or `dumpsys package dexopt`), as
+`[reason=vdex]`). It is not an actual compilation reason passed to `dex2oat` or
+stored in the OAT header.
+
+It typically occurs when:
+
+-   An app is a user-installed app, and the app store delivered **uncompressed
+    DEX code** and **verification metadata** on installation, and
+    -   the app store didn't deliver a profile (for Play Store, this happens
+        during the first few hours after the app is published by an app
+        developer), or
+    -   dexopt on install was explicitly skipped by the app store, by setting
+        the install scenario to `INSTALL_SCENARIO_FAST`, which is translated to
+        the `install-fast` compilation reason, whose default behavior is to skip
+        dexopt, or
+    -   dexopt on install was skipped because the app store installed the app
+        through [incremental install](http://go/incremental-in-android).
+-   An app is a pre-installed app, and dexpreopt was performed with the wrong
+    dependencies. This is not unusual because dexpreopt has many known issues.
+-   The device just installed a Mainline update, which updated the dependencies,
+    and
+    -   (platform is Android 14 or below) Pre-reboot Dexopt didn't run because
+        it's not supported, or
+    -   (platform is Android 15 or above) Pre-reboot Dexopt didn't complete
+        before the reboot because there wasn't enough idle and charging time for
+        it to complete.
+-   The device just installed an OTA update, which updated the dependencies, and
+    -   (platform before update was Android 14 or below) *otapreopt* ran but
+        failed (this is not unusual because *otapreopt* has many known issues),
+        or
+    -   (platform before update was Android 16 or above) Pre-reboot Dexopt
+        didn't complete before the reboot because there wasn't enough idle and
+        charging time for it to complete.
+
 #### The `-dm` suffix
 
-Sometimes, you may see the `-dm` suffix in an OAT file, such as `install-dm`.
-However, the `-dm` suffix is **not** a part of the compilation reason. It's
-appended to the compilation reason to indicate that a DM (`.dm`) file is passed
-to `dex2oat` during dexopt for **app install**.
+Sometimes, you may see the `-dm` suffix in the compilation reason stored in an
+OAT header, such as `install-dm`. However, the `-dm` suffix is **not** a part of
+the compilation reason. It's appended to the compilation reason to indicate that
+a DM (`.dm`) file is passed to `dex2oat` during dexopt for **app install**.
 
 Note: ART Service also passes the DM file to `dex2oat` in other scenarios, such
 as background dexopt, but for compatibility reasons, the `-dm` suffix is not
@@ -93,6 +170,17 @@ Note: The `-dm` suffix does **not** imply anything in the DM file being used by
 the DM file is empty or if `dex2oat` leaves all contents of the DM file unused.
 That would only happen if there's a bug, like the wrong DM file being passed.
 
+### Primary dex vs. secondary dex
+
+ART Service dexopts both primary dex files and secondary dex files of an app.
+
+A primary dex file refers to the base APK or a split APK of an app. It's
+installed by Package Manager or shipped as a part of the system image, and it's
+loaded by Framework on app startup.
+
+A secondary dex file refers to an APK or JAR file that an app adds to its own
+data directory and loads dynamically.
+
 ## Dexopt scenarios
 
 At a high level, ART Service dexopts apps in the following scenarios:
@@ -102,10 +190,12 @@ At a high level, ART Service dexopts apps in the following scenarios:
     `boot-after-ota`)
 -   the device is on the first boot after a mainline update (Compilation reason:
     `boot-after-mainline-update`)
--   an app is being installed (Compilation reason: `install` / `install-fast`
-    / etc.)
+-   an app is being installed (Compilation reason: `install` / `install-bulk` /
+    etc.)
 -   the device is idle and charging (Compilation reason: `bg-dexopt` /
     `inactive`)
+-   the device has a pending OTA / Mainline update and is idle and charging
+    (Compilation reason: `ab-ota`)
 -   requested through commandline (Compilation reason: `cmdline`)
 
 Warning: The execution or scheduling of dexopt operations by ART Service is
@@ -118,18 +208,33 @@ options, can be customized by partners through system properties, APIs, etc.
 ### On the very first boot / the first boot after an OTA update
 
 On the very first boot / the first boot after an OTA update, ART Service only
-dexopts primary dex files of all apps with the "verify" compiler filter.
+dexopts primary dex files of all apps [[1]](#1) with the `verify` compiler
+filter.
 
-If `pm.dexopt.downgrade_after_inactive_days` is set, during the first boot after
-an OTA update, ART Service only dexopts apps used within the last given number of
-days.
-
-Note: It doesn't dexopt secondary dex files or use the "speed-profile" filter
+Note: It doesn't dexopt secondary dex files or use the `speed-profile` filter
 because doing so may block the boot for too long.
 
-In practice, ART Service does nothing for most of the apps. Because the default
-compiler filter is "verify", which tolerates dependency mismatches, apps with
-usable VDEX files generally don't need to be re-dexopted. This includes:
+However, this doesn't mean all apps are in the `verify` state on the first boot
+/ after OTA. In fact:
+
+-   On the first boot,
+    -   if an app has a profile on the Android source tree (see
+        [doc](http://go/art-app-profiles#using-the-profile-on-an-android-source-tree)),
+        the app is in `speed-profile`, dexopted by dexpreopt at build time;
+    -   if an app is added to `PRODUCT_DEXPREOPT_SPEED_APPS`, the app is in
+        `speed`, dexopted by dexpreopt at build time;
+    -   otherwise, the app is in `verify`.
+-   On boot after OTA,
+    -   if
+        [Pre-reboot Dexopt](#when-the-device-has-a-pending-ota-mainline-update-pre_reboot-dexopt)
+        (formerly *otapreopt*) ran correctly and completed before the reboot,
+        apps are in `speed-profile`;
+    -   otherwise, apps are in `verify`.
+
+In practice, ART Service **does nothing** for most of the apps in this scenario.
+Because the default compiler filter is `verify`, which tolerates dependency
+mismatches, apps with usable VDEX files generally don't need to be re-dexopted.
+This includes:
 
 -   apps on the **system partitions** that have artifacts generated by
     dexpreopt, even if the dependencies (class loader contexts) are not properly
@@ -141,57 +246,144 @@ usable VDEX files generally don't need to be re-dexopted. This includes:
 
 In other words, in this scenario, ART Service mostly only dexopts:
 
-- apps in APEXes, because they are not supported by dexpreopt
-- apps on the system partitions with dexpreopt disabled
-- apps forced to have "speed-profile" or "speed" compiler filters (the system UI
-  and the launcher) but dexpreopted with wrong dependencies
+-   apps in APEXes, because they are not supported by dexpreopt
+-   apps on the system partitions with dexpreopt disabled
+-   apps forced to have `speed-profile` or `speed` compiler filters (the system
+    UI and the launcher) but dexpreopted with wrong dependencies
 
 ### On the first boot after a mainline update
 
 On the first boot after a mainline update, ART Service dexopts the primary dex
 files of the system UI and the launcher. It uses the compiler filter specified
-by `dalvik.vm.systemuicompilerfilter` for the system UI, and uses the
-"speed-profile" compiler filter for the launcher.
+by `dalvik.vm.systemuicompilerfilter` [[2]](#2) for the system UI, and uses the
+`speed-profile` compiler filter for the launcher.
 
 Note: It only dexopts those two apps because they are important to user
 experience.
 
-Note: ART Service cannot use the "speed-profile" compiler filter for the system
-UI because the system UI is dexpreopted using the "speed" compiler filter and
-therefore it's never JITed and as a result there is no profile collected on the
-device to use, though this may change in the future. For now, we strongly
-recommend to set `dalvik.vm.systemuicompilerfilter` to "speed".
+However, this doesn't mean all other apps are in the `verify` state on the first
+boot after a mainline update. In fact,
+
+-   if Pre-reboot Dexopt ran correctly and completed before the reboot, apps are
+    in `speed-profile`;
+-   otherwise, apps are in `verify`.
 
 ### During app installation
 
-During app installation, ART Service dexopts the primary dex files of the app.
-If the app is installed along with a DM file that contains a profile (known as a
-*cloud profile*), it uses the "speed-profile" compiler filter. Otherwise, it
-uses the "verify" compiler filter.
+During app installation, ART Service dexopts the primary dex files [[3]](#3) of
+the app. If the app is installed along with a DM file that contains a profile
+(known as a *cloud profile*), it uses the `speed-profile` compiler filter.
+Otherwise, it uses the `verify` compiler filter.
 
-Note: If the APK is uncompressed and aligned, and it is installed along with a
-DM file that only contains a VDEX file (but not a profile), no dexopt will be
-performed because the compiler filter will be "verify" and the VDEX file is
-satisfactory.
+This procedure is by default performed, unless
 
-Note: There is no secondary dex file present during installation.
+-   explicitly skipped by the app store, by setting the install scenario to
+    `INSTALL_SCENARIO_FAST`, which is translated to the `install-fast`
+    compilation reason, whose default behavior is to skip dexopt, or
+-   skipped because the app store installed the app through
+    [incremental install](http://go/incremental-in-android).
 
-### When the device is idle and charging
+To skip *extraction*, an app store can deliver uncompressed DEX. To skip
+*verification*, an app store can perform Cloud Verification on the server side
+and deliver verification metadata.
 
-ART Service has a job called *background dexopt job* managed by Job Scheduler.
-It is triggered daily when the device is idle and charging. During the job
-execution, it dexopts primary dex files and secondary dex files of all apps with
-the "speed-profile" compiler filter.
+Note: This means, if an app store delivers **uncompressed DEX code**,
+**verification metadata**, and no profile on installation, then the app is
+already in the `verify` state and the target compiler filter is `verify`, so no
+dexopt will be performed.
+
+### When the device is idle and charging (background dexopt)
 
-If `pm.dexopt.downgrade_after_inactive_days` is set, ART Service only dexopts
-apps used within the last given number of days, and it downgrades other apps
-(with the compilation reason `inactive`).
+ART Service has a job called *background dexopt job*, managed by Job Scheduler.
+It is triggered daily when the device is idle and charging. During the job
+execution, it dexopts primary dex files and secondary dex files of all apps
+[[1]](#1) with the `speed-profile` compiler filter.
 
 The job is cancellable. When the device is no longer idle or charging, Job
 Scheduler cancels the job.
 
+It means serves two purposes:
+
+-   After sufficient information is collected in the local profile for an app
+    based on the user's use pattern, the background dexopt job re-dexopts the
+    app with the local profile combined with the cloud profile, to generate
+    dexopt artifacts tailored for the user.
+-   After an OTA / Mainline update, if an app was not re-dexopted by Pre-reboot
+    Dexopt, the background dexopt job re-dexopts it.
+
+### When the device has a pending OTA / Mainline update (Pre-reboot Dexopt)
+
+An OTA / Mainline update almost always deliver changes to apps' dependencies, so
+we cannot use the same dexopt artifacts after an update, or there will be a
+dependency mismatch. Therefore, we need a procedure to re-dexopt apps using the
+dependencies contained in the update. *Pre-reboot Dexopt* is for this purpose.
+
+When an OTA / Mainline update is downloaded and pending, *Pre-reboot Dexopt*
+dexopts primary dex files and secondary dex files of all apps [[1]](#1) for the
+update with the `speed-profile` compiler filter, before the reboot.
+
+Pre-reboot Dexopt was introduced in Android 15. Prior to that, there was
+*otapreopt*, performing a similar operation but having the following
+limitations:
+
+-   It only supports OTA updates, not Mainline updates.
+-   It has many known issues. For example, if the platform before the OTA update
+    is Android 14 (excluding Android 14 QPR1 and above), it cannot invoke
+    `dex2oat` due to a known issue.
+-   Its efficiency is not ideal. If the platform before the OTA update is
+    Android 12~14 (excluding Android 14 QPR1 and above), it typically takes
+    around an hour, whereas Pre-reboot Dexopt typically takes around 13 minutes.
+
+#### Asynchronous Pre-reboot Dexopt
+
+Asynchronous Pre-reboot Dexopt does not block the update installation. It is run
+by a job called *Pre-reboot Dexopt job*, managed by Job Scheduler, triggered
+once when the device is idle and charging. The user can reboot the device to
+apply the update at any time, even if Pre-reboot Dexopt has not completed yet.
+
+After applying the update, apps that were not dexopted by Pre-reboot Dexopt are
+in the `verify` state. They can still run normally, being interpreted or JITed,
+and will be dexopted by background dexopt, when the device is idle and charging.
+
+This is opposed to synchronous Pre-reboot Dexopt, which blocks the update
+installation. It is guaranteed to complete, but the drawback is that the user
+cannot reboot the device to apply the update until it completes. For users who
+are eager to try a new version of Android or urgently needs to apply a security
+fix, this is frustrating.
+
+There is no perfect solution, only trade-offs. We made Pre-reboot Dexopt
+asynchronous because we believe this delivers a better user experience.
+
+-   For Mainline updates, Pre-reboot Dexopt has already been asynchronous since
+    introduced.
+-   For OTA updates, Pre-reboot Dexopt was synchronous in Android 15, and is
+    asynchronous since Android 16.
+
 ### When requested through commandline
 
 ART Service can be invoked by commands (`pm compile`, `pm bg-dexopt-job`, and
 `pm art dexopt-packages`). Run `pm help` to see the usages and the differences
 between them.
+
+## Notes
+
+### [1]
+
+If `pm.dexopt.downgrade_after_inactive_days` is set (typically on low-end
+devices), ART Service only dexopts apps used within the last given number of
+days. In addition, during background dexopt, if the device's remaining disk
+space is low, ART Service downgrades other apps (with the compilation reason
+`inactive`, whose default compiler filter is `verify`).
+
+### [2]
+
+Typically, `dalvik.vm.systemuicompilerfilter` is set to `speed`. This is because
+the System UI package is typically added to `PRODUCT_DEXPREOPT_SPEED_APPS`,
+dexpreopted using the `speed` compiler filter, and therefore it's never JITed
+and as a result there is no profile collected on the device to use. This may
+change in the future, but for now, we strongly recommend to set
+`dalvik.vm.systemuicompilerfilter` to `speed`.
+
+### [3]
+
+There is no secondary dex file present during installation.
diff --git a/libartservice/service/java/com/android/server/art/ArtJni.java b/libartservice/service/java/com/android/server/art/ArtJni.java
index 62b53dd6c8..17d0f53ad1 100644
--- a/libartservice/service/java/com/android/server/art/ArtJni.java
+++ b/libartservice/service/java/com/android/server/art/ArtJni.java
@@ -32,40 +32,59 @@ import java.io.IOException;
  *
  * The wrappers are added for two reasons:
  * - They make the methods mockable, since Mockito cannot mock JNI methods.
- * - They delegate calls to artd if the code is running for Pre-reboot Dexopt, to avoid loading
- *   libartservice.so.
+ * - They delegate calls to artd if the code is running for Pre-reboot Dexopt,
+ *  to avoid loading libartservice.so.
  *
  * @hide
  */
 @RequiresApi(Build.VERSION_CODES.UPSIDE_DOWN_CAKE)
 public class ArtJni {
-    static {
-        // During Pre-reboot Dexopt, the code is loaded by a separate class loader from the chroot
-        // dir, where the new ART apex is mounted. In this case, loading libartservice.so is tricky.
-        // The library depends on libc++.so, libbase.so, etc. Although the classloader allows
-        // specifying a library search path, it doesnt allow specifying how to search for
-        // dependencies. Because the classloading takes place in system server, the old linkerconfig
-        // takes effect rather than the new one, and the old linkerconfig doesnt specify how to
-        // search for dependencies for the new libartservice.so. This leads to an undesired
-        // behavior: the dependencies are resolved to those on the old platform.
-        //
-        // Also, we can't statically link libartservice.so against all dependencies because it not
-        // only bloats libartservice.so by a lot, but also prevents us from accessing the global
-        // runtime instance when the code is running in the normal situation.
-        //
-        // Therefore, for Pre-reboot Dexopt, we just avoid loading libartservice.so, and delegate
-        // calls to artd instead.
-        if (!GlobalInjector.getInstance().isPreReboot()) {
+
+    private static volatile boolean sLoaded = false;
+
+    private ArtJni() {}
+
+    /**
+     * Loads the library lazily.
+     *
+     * This method is synchronized to avoid loading the library multiple times.
+     * This is mainly to be able to mock the JNI methods in tests without
+     *  actually loading the libartservice.so library.
+     */
+    private static void loadLibrary() {
+        if (sLoaded) {
+            return;
+        }
+        Utils.check(!GlobalInjector.getInstance().isPreReboot());
+        synchronized (ArtJni.class) {
+            if (sLoaded) {
+                return;
+            }
+
+            // During Pre-reboot Dexopt, the code is loaded by a separate class loader from the chroot
+            // dir, where the new ART apex is mounted. In this case, loading libartservice.so is tricky.
+            // The library depends on libc++.so, libbase.so, etc. Although the classloader allows
+            // specifying a library search path, it doesnt allow specifying how to search for
+            // dependencies. Because the classloading takes place in system server, the old linkerconfig
+            // takes effect rather than the new one, and the old linkerconfig doesnt specify how to
+            // search for dependencies for the new libartservice.so. This leads to an undesired
+            // behavior: the dependencies are resolved to those on the old platform.
+            //
+            // Also, we can't statically link libartservice.so against all dependencies because it not
+            // only bloats libartservice.so by a lot, but also prevents us from accessing the global
+            // runtime instance when the code is running in the normal situation.
+            //
+            // Therefore, for Pre-reboot Dexopt, we just avoid loading libartservice.so, and delegate
+            // calls to artd instead.
             if (VMRuntime.getRuntime().vmLibrary().equals("libartd.so")) {
                 System.loadLibrary("artserviced");
             } else {
                 System.loadLibrary("artservice");
             }
+            sLoaded = true;
         }
     }
 
-    private ArtJni() {}
-
     /**
      * Returns an error message if the given dex path is invalid, or null if the validation passes.
      */
@@ -79,6 +98,7 @@ public class ArtJni {
                 return null;
             }
         }
+        loadLibrary();
         return validateDexPathNative(dexPath);
     }
 
@@ -98,6 +118,7 @@ public class ArtJni {
                 return null;
             }
         }
+        loadLibrary();
         return validateClassLoaderContextNative(dexPath, classLoaderContext);
     }
 
@@ -111,6 +132,7 @@ public class ArtJni {
             // needs access to the global runtime instance.
             throw new UnsupportedOperationException();
         }
+        loadLibrary();
         return getGarbageCollectorNative();
     }
 
@@ -125,6 +147,7 @@ public class ArtJni {
             // We don't need this for Pre-reboot Dexopt.
             throw new UnsupportedOperationException();
         }
+        loadLibrary();
         setPropertyNative(key, value);
         // Return a placeholder value to make this method easier to mock. There is no good way to
         // mock a method that is both void and static, due to the poor design of Mockito API.
@@ -155,6 +178,7 @@ public class ArtJni {
             // We don't need this for Pre-reboot Dexopt.
             throw new UnsupportedOperationException();
         }
+        loadLibrary();
         ensureNoProcessInDirNative(dir, timeoutMs);
         // Return a placeholder value to make this method easier to mock. There is no good way to
         // mock a method that is both void and static, due to the poor design of Mockito API.
diff --git a/libartservice/service/java/com/android/server/art/ArtManagerLocal.java b/libartservice/service/java/com/android/server/art/ArtManagerLocal.java
index 993cb2c558..3f4b99d637 100644
--- a/libartservice/service/java/com/android/server/art/ArtManagerLocal.java
+++ b/libartservice/service/java/com/android/server/art/ArtManagerLocal.java
@@ -393,17 +393,20 @@ public final class ArtManagerLocal {
     }
 
     /**
-     * Resets the dexopt state of the package as if the package is newly installed without cloud
-     * dexopt artifacts (SDM files).
+     * Resets the dexopt state of the package as if the package is newly installed, but without any
+     * compilation. Clears current profiles and reference profiles. External profiles (e.g., cloud
+     * profiles and embedded profiles) are kept for future dexopt, but not used this time.
      *
-     * More specifically,
-     * - It clears current profiles, reference profiles, and all dexopt artifacts (including cloud
-     *   dexopt artifacts).
-     * - If there is an external profile (e.g., a cloud profile), the reference profile will be
-     *   re-created from that profile, and dexopt artifacts will be regenerated for that profile.
+     * For primary dex files, all dexopt optimizations except compilation (such as verification) are
+     * still performed, meaning dexopt artifacts are regenerated if necessary. In the current
+     * version, this is equivalent to dexopt with the compiler filter set to "verify"
      *
-     * For secondary dex files, it clears all profiles and dexopt artifacts without regeneration
-     * because secondary dex files are supposed to be unknown at install time.
+     * For secondary dex files, no dexopt optimizations are performed, meaning dexopt artifacts are
+     * deleted without regeneration, because secondary dex files are supposed to be unknown at
+     * install time.
+     *
+     * One of the use cases of this method is for app developers to get a baseline for measuring
+     * performance improvements from compilation.
      *
      * @hide
      */
@@ -414,9 +417,8 @@ public final class ArtManagerLocal {
         // We must delete the artifacts for primary dex files beforehand rather than relying on
         // `dexoptPackage` to replace them because:
         // - If dexopt is not needed after the deletion, then we shouldn't run dexopt at all. For
-        //   example, when we have a DM file that contains a VDEX file but doesn't contain a cloud
-        //   profile, this happens. Note that this is more about correctness rather than
-        //   performance.
+        //   example, when we have a DM file that contains a VDEX file, this happens. Note that this
+        //   is more about correctness rather than performance.
         // - We don't want the existing artifacts to affect dexopt. For example, the existing VDEX
         //   file should not be an input VDEX.
         //
@@ -427,8 +429,10 @@ public final class ArtManagerLocal {
         clearAppProfiles(snapshot, packageName);
 
         // Re-generate artifacts for primary dex files if needed.
-        return dexoptPackage(snapshot, packageName,
-                new DexoptParams.Builder(ReasonMapping.REASON_INSTALL).build(), cancellationSignal);
+        var dexoptParams = new DexoptParams.Builder(ReasonMapping.REASON_INSTALL)
+                                   .setCompilerFilter("verify")
+                                   .build();
+        return dexoptPackage(snapshot, packageName, dexoptParams, cancellationSignal);
     }
 
     /**
diff --git a/libartservice/service/java/com/android/server/art/ArtShellCommand.java b/libartservice/service/java/com/android/server/art/ArtShellCommand.java
index a3b2fba42a..1d977e5951 100644
--- a/libartservice/service/java/com/android/server/art/ArtShellCommand.java
+++ b/libartservice/service/java/com/android/server/art/ArtShellCommand.java
@@ -967,17 +967,19 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
         pw.println("    -f Force dexopt, also when the compiler filter being applied is not");
         pw.println("       better than that of the current dexopt artifacts for a package.");
         pw.println("    --reset Reset the dexopt state of the package as if the package is newly");
-        pw.println("       installed without cloud dexopt artifacts (SDM files).");
-        pw.println("       More specifically,");
-        pw.println("       - It clears current profiles, reference profiles, and all dexopt");
-        pw.println("         artifacts (including cloud dexopt artifacts).");
-        pw.println("       - If there is an external profile (e.g., a cloud profile), the");
-        pw.println("         reference profile will be re-created from that profile, and dexopt");
-        pw.println("         artifacts will be regenerated for that profile.");
-        pw.println("       For secondary dex files, it clears all profiles and dexopt artifacts");
-        pw.println("       without regeneration because secondary dex files are supposed to be");
-        pw.println("       unknown at install time.");
+        pw.println("       installed, but without any compilation. Clears current profiles and");
+        pw.println("       reference profiles. External profiles (e.g., cloud profiles and");
+        pw.println("       embedded profiles) are kept for future dexopt, but not used this time.");
+        pw.println("       For primary dex files, all dexopt optimizations except compilation");
+        pw.println("       (such as verification) are still performed, meaning dexopt artifacts");
+        pw.println("       are regenerated if necessary. In the current version, this is");
+        pw.println("       equivalent to dexopt with the compiler filter set to 'verify'.");
+        pw.println("       For secondary dex files, no dexopt optimizations are performed,");
+        pw.println("       meaning dexopt artifacts are deleted without regeneration, because");
+        pw.println("       secondary dex files are supposed to be unknown at install time.");
         pw.println("       When this flag is set, all the other flags are ignored.");
+        pw.println("       One of the use cases of this command is for app developers to get a");
+        pw.println("       baseline for measuring performance improvements from compilation.");
         pw.println("    -v Verbose mode. This mode prints detailed results.");
         pw.println("    --force-merge-profile Force merge profiles even if the difference between");
         pw.println("       before and after the merge is not significant.");
diff --git a/libartservice/service/java/com/android/server/art/Dex2OatStatsReporter.java b/libartservice/service/java/com/android/server/art/Dex2OatStatsReporter.java
index 2e45c01590..ec63da8c17 100644
--- a/libartservice/service/java/com/android/server/art/Dex2OatStatsReporter.java
+++ b/libartservice/service/java/com/android/server/art/Dex2OatStatsReporter.java
@@ -186,9 +186,8 @@ public class Dex2OatStatsReporter {
 
     public record Dex2OatResult(int status, int exitCode, int signal) {
         public static Dex2OatResult notRun() {
-            return new Dex2OatResult(
-                    ArtStatsLog.ART_DEX2_OAT_REPORTED__RESULT_STATUS__EXEC_RESULT_STATUS_NOT_RUN,
-                    -1 /* exitCode */, 0 /* signal */);
+            return baseResultWithStatus(
+                    ArtStatsLog.ART_DEX2_OAT_REPORTED__RESULT_STATUS__EXEC_RESULT_STATUS_NOT_RUN);
         }
 
         public static Dex2OatResult exited(int exitCode) {
@@ -198,9 +197,17 @@ public class Dex2OatStatsReporter {
         }
 
         public static Dex2OatResult cancelled() {
-            return new Dex2OatResult(
-                    ArtStatsLog.ART_DEX2_OAT_REPORTED__RESULT_STATUS__EXEC_RESULT_STATUS_CANCELLED,
-                    -1 /* exitCode */, 0 /* signal */);
+            return baseResultWithStatus(
+                    ArtStatsLog.ART_DEX2_OAT_REPORTED__RESULT_STATUS__EXEC_RESULT_STATUS_CANCELLED);
+        }
+
+        public static Dex2OatResult failedToStart() {
+            return baseResultWithStatus(ArtStatsLog
+                            .ART_DEX2_OAT_REPORTED__RESULT_STATUS__EXEC_RESULT_STATUS_START_FAILED);
+        }
+
+        private static Dex2OatResult baseResultWithStatus(int status) {
+            return new Dex2OatResult(status, -1 /* exitCode */, 0 /* signal */);
         }
     }
 }
diff --git a/libartservice/service/java/com/android/server/art/Dexopter.java b/libartservice/service/java/com/android/server/art/Dexopter.java
index 1c6abe6c41..2c46598052 100644
--- a/libartservice/service/java/com/android/server/art/Dexopter.java
+++ b/libartservice/service/java/com/android/server/art/Dexopter.java
@@ -303,10 +303,12 @@ public abstract class Dexopter<DexInfoType extends DetailedDexInfo> {
                         Pattern pattern = Pattern.compile(
                                 "\\[status=(-?\\d+),exit_code=(-?\\d+),signal=(-?\\d+)]");
                         Matcher matcher = pattern.matcher(Objects.requireNonNull(e.getMessage()));
-                        if (matcher.matches()) {
+                        if (matcher.find()) {
                             dex2OatResult = new Dex2OatResult(Integer.parseInt(matcher.group(1)),
                                     Integer.parseInt(matcher.group(2)),
                                     Integer.parseInt(matcher.group(3)));
+                        } else {
+                            dex2OatResult = Dex2OatResult.failedToStart();
                         }
                     } finally {
                         if (!externalProfileErrors.isEmpty()) {
diff --git a/libartservice/service/javatests/com/android/server/art/ArtManagerLocalTest.java b/libartservice/service/javatests/com/android/server/art/ArtManagerLocalTest.java
index 754208a05f..1c8cf5066f 100644
--- a/libartservice/service/javatests/com/android/server/art/ArtManagerLocalTest.java
+++ b/libartservice/service/javatests/com/android/server/art/ArtManagerLocalTest.java
@@ -559,8 +559,11 @@ public class ArtManagerLocalTest {
         var result = DexoptResult.create();
         var cancellationSignal = new CancellationSignal();
 
-        when(mDexoptHelper.dexopt(
-                     any(), deepEq(List.of(PKG_NAME_1)), any(), same(cancellationSignal), any()))
+        var dexoptParams = new DexoptParams.Builder(ReasonMapping.REASON_INSTALL)
+                                   .setCompilerFilter("verify")
+                                   .build();
+        when(mDexoptHelper.dexopt(any(), deepEq(List.of(PKG_NAME_1)), deepEq(dexoptParams),
+                     same(cancellationSignal), any()))
                 .thenReturn(result);
 
         assertThat(mArtManagerLocal.resetDexoptStatus(mSnapshot, PKG_NAME_1, cancellationSignal))
diff --git a/libartservice/service/javatests/com/android/server/art/PrimaryDexopterReporterTest.java b/libartservice/service/javatests/com/android/server/art/PrimaryDexopterReporterTest.java
new file mode 100644
index 0000000000..f594ee408b
--- /dev/null
+++ b/libartservice/service/javatests/com/android/server/art/PrimaryDexopterReporterTest.java
@@ -0,0 +1,236 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.android.server.art;
+
+import static com.android.dx.mockito.inline.extended.ExtendedMockito.verify;
+
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.anyInt;
+import static org.mockito.ArgumentMatchers.anyList;
+import static org.mockito.ArgumentMatchers.anyString;
+import static org.mockito.ArgumentMatchers.eq;
+import static org.mockito.Mockito.lenient;
+import static org.mockito.Mockito.times;
+
+import android.os.ServiceSpecificException;
+
+import com.android.server.art.model.ArtFlags;
+import com.android.server.art.model.DetailedDexInfo;
+import com.android.server.art.model.DexMetadata;
+import com.android.server.art.model.DexoptParams;
+import com.android.server.art.testing.TestingUtils;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import java.nio.file.NoSuchFileException;
+
+public final class PrimaryDexopterReporterTest extends PrimaryDexopterTestBase {
+    private final String COMPILER_FILTER_VERIFY = "verify";
+    private final String COMPILER_REASON_INSTALL = "install";
+    private final long DEX2OAT_ARTIFACTS_SIZE = 567L;
+    private final long DEX2OAT_COMPILATION_TIME = 234L;
+    private final DexoptParams DEXOPT_PARAMS_SKIP =
+            new DexoptParams.Builder(COMPILER_REASON_INSTALL)
+                    .setCompilerFilter(DexoptParams.COMPILER_FILTER_NOOP)
+                    .setFlags(ArtFlags.FLAG_FOR_PRIMARY_DEX)
+                    .build();
+    private final DexoptParams DEXOPT_PARAMS_VERIFY =
+            new DexoptParams.Builder(COMPILER_REASON_INSTALL)
+                    .setCompilerFilter(COMPILER_FILTER_VERIFY)
+                    .setFlags(ArtFlags.FLAG_FOR_PRIMARY_DEX)
+                    .build();
+
+    private PrimaryDexopter mPrimaryDexopter;
+
+    @Before
+    public void setUp() throws Exception {
+        super.setUp();
+
+        lenient().when(mInjector.getReporterExecutor()).thenReturn(Runnable::run);
+
+        // By default, none of the profiles are usable.
+        lenient().when(mArtd.isProfileUsable(any(), anyString())).thenReturn(false);
+        lenient()
+                .when(mArtd.copyAndRewriteProfile(any(), any(), anyString()))
+                .thenReturn(TestingUtils.createCopyAndRewriteProfileNoProfile());
+        lenient()
+                .when(mArtd.copyAndRewriteEmbeddedProfile(any(), anyString()))
+                .thenReturn(TestingUtils.createCopyAndRewriteProfileNoProfile());
+
+        // By default, no DM file exists.
+        lenient()
+                .when(mDexMetadataHelperInjector.openZipFile(anyString()))
+                .thenThrow(NoSuchFileException.class);
+
+        // Dexopt is by default needed and successful.
+        lenient()
+                .when(mArtd.getDexoptNeeded(
+                        anyString(), anyString(), anyString(), anyString(), anyInt()))
+                .thenReturn(dexoptIsNeeded());
+        mockArtdDexoptResultSuccess(createArtdDexoptResult(false /* cancelled */,
+                DEX2OAT_COMPILATION_TIME, DEX2OAT_COMPILATION_TIME + 10, DEX2OAT_ARTIFACTS_SIZE,
+                DEX2OAT_ARTIFACTS_SIZE - 10));
+
+        mockPrimaryDexopter(DEXOPT_PARAMS_VERIFY);
+    }
+
+    @Test
+    public void testDex2OatResult_Success() throws Exception {
+        mPrimaryDexopter.dexopt();
+
+        Dex2OatStatsReporter.Dex2OatResult expectedResult =
+                Dex2OatStatsReporter.Dex2OatResult.exited(0);
+        verify(()
+                        -> Dex2OatStatsReporter.report(eq(UID), eq(COMPILER_FILTER_VERIFY),
+                                eq(COMPILER_REASON_INSTALL), eq(DexMetadata.TYPE_NONE),
+                                any(DetailedDexInfo.class), eq("arm64"), eq(expectedResult),
+                                eq(DEX2OAT_ARTIFACTS_SIZE), eq(DEX2OAT_COMPILATION_TIME)),
+                times(2));
+        verify(()
+                        -> Dex2OatStatsReporter.report(eq(UID), eq(COMPILER_FILTER_VERIFY),
+                                eq(COMPILER_REASON_INSTALL), eq(DexMetadata.TYPE_NONE),
+                                any(DetailedDexInfo.class), eq("arm"), eq(expectedResult),
+                                eq(DEX2OAT_ARTIFACTS_SIZE), eq(DEX2OAT_COMPILATION_TIME)),
+                times(2));
+    }
+
+    @Test
+    public void testDex2OatResult_ExitedWithNonZeroCode() throws Exception {
+        int status = 1, exitCode = 2, signal = 0;
+        mockArtdDexoptResultFailure("dex2oat exited with non-zero code", status, exitCode, signal);
+
+        mPrimaryDexopter.dexopt();
+
+        Dex2OatStatsReporter.Dex2OatResult expectedResult =
+                new Dex2OatStatsReporter.Dex2OatResult(status, exitCode, signal);
+        verify(()
+                        -> Dex2OatStatsReporter.report(eq(mPkgState.getAppId()),
+                                eq(COMPILER_FILTER_VERIFY), eq(COMPILER_REASON_INSTALL),
+                                eq(DexMetadata.TYPE_NONE), any(DetailedDexInfo.class), eq("arm64"),
+                                eq(expectedResult), eq(0L), eq(0L)),
+                times(2));
+
+        verify(()
+                        -> Dex2OatStatsReporter.report(eq(mPkgState.getAppId()),
+                                eq(COMPILER_FILTER_VERIFY), eq(COMPILER_REASON_INSTALL),
+                                eq(DexMetadata.TYPE_NONE), any(DetailedDexInfo.class), eq("arm"),
+                                eq(expectedResult), eq(0L), eq(0L)),
+                times(2));
+    }
+
+    @Test
+    public void testDex2OatResult_Signaled() throws Exception {
+        int status = 2, exitCode = -1, signal = 4;
+        mockArtdDexoptResultFailure("dex2oat signaled", status, exitCode, signal);
+
+        mPrimaryDexopter.dexopt();
+
+        Dex2OatStatsReporter.Dex2OatResult expectedResult =
+                new Dex2OatStatsReporter.Dex2OatResult(status, exitCode, signal);
+        verify(()
+                        -> Dex2OatStatsReporter.report(eq(mPkgState.getAppId()),
+                                eq(COMPILER_FILTER_VERIFY), eq(COMPILER_REASON_INSTALL),
+                                eq(DexMetadata.TYPE_NONE), any(DetailedDexInfo.class), eq("arm64"),
+                                eq(expectedResult), eq(0L), eq(0L)),
+                times(2));
+
+        verify(()
+                        -> Dex2OatStatsReporter.report(eq(mPkgState.getAppId()),
+                                eq(COMPILER_FILTER_VERIFY), eq(COMPILER_REASON_INSTALL),
+                                eq(DexMetadata.TYPE_NONE), any(DetailedDexInfo.class), eq("arm"),
+                                eq(expectedResult), eq(0L), eq(0L)),
+                times(2));
+    }
+
+    @Test
+    public void testDex2OatResult_StartFailed() throws Exception {
+        int status = 4, exitCode = -1, signal = 0;
+        lenient()
+                .when(mArtd.dexopt(any(), anyString(), anyString(), anyString(), anyString(), any(),
+                        any(), any(), anyInt(), any(), any()))
+                .thenThrow(
+                        new ServiceSpecificException(-1, "Not your typical dex2oat error message"));
+
+        mPrimaryDexopter.dexopt();
+
+        Dex2OatStatsReporter.Dex2OatResult failedToStart =
+                new Dex2OatStatsReporter.Dex2OatResult(status, exitCode, signal);
+        verify(()
+                        -> Dex2OatStatsReporter.report(eq(mPkgState.getAppId()),
+                                eq(COMPILER_FILTER_VERIFY), eq(COMPILER_REASON_INSTALL),
+                                eq(DexMetadata.TYPE_NONE), any(DetailedDexInfo.class), eq("arm64"),
+                                eq(failedToStart), eq(0L), eq(0L)),
+                times(2));
+
+        verify(()
+                        -> Dex2OatStatsReporter.report(eq(mPkgState.getAppId()),
+                                eq(COMPILER_FILTER_VERIFY), eq(COMPILER_REASON_INSTALL),
+                                eq(DexMetadata.TYPE_NONE), any(DetailedDexInfo.class), eq("arm"),
+                                eq(failedToStart), eq(0L), eq(0L)),
+                times(2));
+    }
+
+    @Test
+    public void testDex2OatResult_NotRun() throws Exception {
+        mockPrimaryDexopter(DEXOPT_PARAMS_SKIP);
+
+        mPrimaryDexopter.dexopt();
+
+        verify(()
+                        -> Dex2OatStatsReporter.reportSkipped(eq(UID), eq(COMPILER_REASON_INSTALL),
+                                eq(DexMetadata.TYPE_NONE), any(DetailedDexInfo.class), anyList()),
+                times(2));
+    }
+
+    @Test
+    public void testDex2OatResult_Cancelled() throws Exception {
+        mockArtdDexoptResultSuccess(createArtdDexoptResult(true /* cancelled */));
+
+        mPrimaryDexopter.dexopt();
+
+        verify(()
+                        -> Dex2OatStatsReporter.report(eq(UID), eq(COMPILER_FILTER_VERIFY),
+                                eq(COMPILER_REASON_INSTALL), eq(DexMetadata.TYPE_NONE),
+                                any(DetailedDexInfo.class), eq("arm64"),
+                                eq(Dex2OatStatsReporter.Dex2OatResult.cancelled()), eq(0L),
+                                eq(0L)));
+    }
+
+    private void mockPrimaryDexopter(DexoptParams params) {
+        mPrimaryDexopter =
+                new PrimaryDexopter(mInjector, mPkgState, mPkg, params, mCancellationSignal);
+    }
+
+    private void mockArtdDexoptResultSuccess(ArtdDexoptResult result) throws Exception {
+        lenient()
+                .when(mArtd.dexopt(any(), anyString(), anyString(), anyString(), anyString(), any(),
+                        any(), any(), anyInt(), any(), any()))
+                .thenReturn(result);
+    }
+
+    private void mockArtdDexoptResultFailure(String message, int status, int exitCode, int signal)
+            throws Exception {
+        lenient()
+                .when(mArtd.dexopt(any(), anyString(), anyString(), anyString(), anyString(), any(),
+                        any(), any(), anyInt(), any(), any()))
+                .thenThrow(new ServiceSpecificException(-1,
+                        String.format(
+                                "Failed to run dex2oat: %s [status=%d,exit_code=%d,signal=%d]",
+                                message, status, exitCode, signal)));
+    }
+}
diff --git a/libartservice/service/javatests/com/android/server/art/PrimaryDexopterTestBase.java b/libartservice/service/javatests/com/android/server/art/PrimaryDexopterTestBase.java
index d94190ded5..26c6ce1daf 100644
--- a/libartservice/service/javatests/com/android/server/art/PrimaryDexopterTestBase.java
+++ b/libartservice/service/javatests/com/android/server/art/PrimaryDexopterTestBase.java
@@ -57,8 +57,8 @@ public class PrimaryDexopterTestBase {
     protected static final long APP_VERSION_CODE = 1536036288l;
 
     @Rule
-    public StaticMockitoRule mockitoRule = new StaticMockitoRule(
-            SystemProperties.class, Constants.class, PackageStateModulesUtils.class);
+    public StaticMockitoRule mockitoRule = new StaticMockitoRule(SystemProperties.class,
+            Constants.class, PackageStateModulesUtils.class, Dex2OatStatsReporter.class);
 
     @Mock protected PrimaryDexopter.Injector mInjector;
     @Mock protected IArtd mArtd;
diff --git a/libartservice/service/javatests/com/android/server/art/SecondaryDexopterTest.java b/libartservice/service/javatests/com/android/server/art/SecondaryDexopterTest.java
index 074edc0629..6ca8698c54 100644
--- a/libartservice/service/javatests/com/android/server/art/SecondaryDexopterTest.java
+++ b/libartservice/service/javatests/com/android/server/art/SecondaryDexopterTest.java
@@ -48,10 +48,8 @@ import com.android.server.art.model.DexoptParams;
 import com.android.server.art.model.DexoptResult;
 import com.android.server.art.testing.StaticMockitoRule;
 import com.android.server.art.testing.TestingUtils;
-import com.android.server.pm.PackageSetting;
 import com.android.server.pm.pkg.AndroidPackage;
 import com.android.server.pm.pkg.PackageState;
-import com.android.server.pm.pkg.PackageStateUnserialized;
 
 import org.junit.Before;
 import org.junit.Rule;
diff --git a/libdexfile/Android.bp b/libdexfile/Android.bp
index 69d51b6c01..5784cfdca7 100644
--- a/libdexfile/Android.bp
+++ b/libdexfile/Android.bp
@@ -33,8 +33,6 @@ cc_defaults {
     ],
     srcs: [
         "dex/art_dex_file_loader.cc",
-        "dex/compact_dex_file.cc",
-        "dex/compact_offset_table.cc",
         "dex/descriptors_names.cc",
         "dex/dex_file.cc",
         "dex/dex_file_exception_helpers.cc",
@@ -124,10 +122,12 @@ cc_defaults {
 
 cc_defaults {
     name: "libdexfile_static_base_defaults",
+    defaults: [
+        "art_liblog_static_defaults",
+        "art_libz_static_defaults",
+    ],
     whole_static_libs: [
         "libbase",
-        "liblog",
-        "libz",
         "libziparchive",
     ],
 }
@@ -173,6 +173,7 @@ art_cc_library {
         // Allow libdexfile_support users to list this as a runtime_libs
         // dependency - see comment for libdexfile_support. It shouldn't be used
         // for any other purpose.
+        "//cts/tests/tests/simpleperf",
         "//external/perfetto",
         "//frameworks/base/services/core/jni",
         "//system/core/debuggerd",
@@ -291,8 +292,6 @@ art_cc_defaults {
         "dex/art_dex_file_loader_test.cc",
         "dex/class_accessor_test.cc",
         "dex/code_item_accessors_test.cc",
-        "dex/compact_dex_file_test.cc",
-        "dex/compact_offset_table_test.cc",
         "dex/descriptors_names_test.cc",
         "dex/dex_file_loader_test.cc",
         "dex/dex_file_verifier_test.cc",
@@ -542,28 +541,19 @@ art_cc_test {
     ],
 }
 
-// For use by external packages allowed to link in static libdexfile_support.
-// This is not allowed in any module that may end up in an APEX or platform
-// image, so visibility is restrictive.
-//
-// TODO(b/169885605): This library brings with it all the exported headers from
-// libdexfile_support_static_defaults into the prebuilt SDK created by
-// art-module-sdk, many of which are transitive dependencies outside ART. Those
-// may conflict with other versions that the caller is using in their build. One
-// way to deal with that is to provide minimal headers without any transitive
-// dependencies on other headers.
+// For use by external packages allowed to link libdexfile statically. Use with
+// libdexfile_static_transitive_defaults. This is not allowed in any module that
+// may end up in an APEX or platform image, so visibility is restrictive.
 cc_library_static {
     name: "libdexfile_static",
     host_supported: true,
     visibility: [
-        // Required for simpleperf, libsimpleperf_record, and libsimpleperf_report
-        // in the NDK.
+        // Required for simpleperf host binaries and tests.
         "//system/extras/simpleperf",
-        "//cts/tests/tests/simpleperf",
     ],
     // Using libdexfile_support_static_defaults will link in external libs like
-    // libbase and libz statically as well, which are likely to cause duplicate
-    // copies in the depending module.
+    // libbase statically as well, which are likely to cause duplicate copies in
+    // the depending module.
     // TODO(b/169885605): Avoid exposing symbols from those libs.
     defaults: [
         "art_defaults",
@@ -571,9 +561,34 @@ cc_library_static {
     ],
 }
 
+cc_defaults {
+    name: "libdexfile_static_transitive_defaults",
+    defaults_visibility: [
+        "//system/extras/simpleperf",
+    ],
+    // Take the relevant bits from art_liblog_static_defaults and
+    // art_libz_static_defaults.
+    target: {
+        android: {
+            shared_libs: [
+                "liblog",
+                "libz",
+            ],
+        },
+        not_windows: {
+            shared_libs: [
+                "libz",
+            ],
+        },
+    },
+}
+
 art_cc_test {
     name: "art_libdexfile_static_tests",
-    defaults: ["art_test_defaults"],
+    defaults: [
+        "art_test_defaults",
+        "libdexfile_static_transitive_defaults",
+    ],
     test_suites: ["general-tests"],
     srcs: [
         "external/dex_file_supp_test.cc",
@@ -583,14 +598,6 @@ art_cc_test {
     ],
     enabled: false,
     target: {
-        android: {
-            // Build static test binary on device, to make sure libdexfile_static can be used in
-            // static simpleperf binary in ndk.
-            static_executable: true,
-            static_libs: [
-                "libc",
-            ],
-        },
         linux: {
             enabled: true,
         },
diff --git a/libdexfile/dex/art_dex_file_loader.cc b/libdexfile/dex/art_dex_file_loader.cc
index 056d2fb261..4f726350fc 100644
--- a/libdexfile/dex/art_dex_file_loader.cc
+++ b/libdexfile/dex/art_dex_file_loader.cc
@@ -30,7 +30,6 @@
 #include "base/systrace.h"
 #include "base/unix_file/fd_file.h"
 #include "base/zip_archive.h"
-#include "dex/compact_dex_file.h"
 #include "dex/dex_file.h"
 #include "dex/dex_file_verifier.h"
 #include "dex/standard_dex_file.h"
diff --git a/libdexfile/dex/code_item_accessors-inl.h b/libdexfile/dex/code_item_accessors-inl.h
index 8fa6ea3fd8..76af61ab73 100644
--- a/libdexfile/dex/code_item_accessors-inl.h
+++ b/libdexfile/dex/code_item_accessors-inl.h
@@ -20,7 +20,6 @@
 #include "code_item_accessors.h"
 
 #include "base/iteration_range.h"
-#include "compact_dex_file.h"
 #include "dex_file-inl.h"
 #include "dex_instruction_iterator.h"
 #include "standard_dex_file.h"
@@ -34,19 +33,6 @@ inline void CodeItemInstructionAccessor::Init(uint32_t insns_size_in_code_units,
   insns_ = insns;
 }
 
-template <>
-inline void CodeItemInstructionAccessor::Init<CompactDexFile::CodeItem>(
-    const CompactDexFile::CodeItem& code_item) {
-  uint32_t insns_size_in_code_units;
-  code_item.DecodeFields</*kDecodeOnlyInstructionCount*/ true>(
-      &insns_size_in_code_units,
-      /*registers_size*/ nullptr,
-      /*ins_size*/ nullptr,
-      /*outs_size*/ nullptr,
-      /*tries_size*/ nullptr);
-  Init(insns_size_in_code_units, code_item.insns_);
-}
-
 template <>
 inline void CodeItemInstructionAccessor::Init<StandardDexFile::CodeItem>(
     const StandardDexFile::CodeItem& code_item) {
@@ -57,12 +43,7 @@ inline void CodeItemInstructionAccessor::Init(const DexFile& dex_file,
                                               const dex::CodeItem* code_item) {
   if (code_item != nullptr) {
     DCHECK(dex_file.IsInDataSection(code_item));
-    if (dex_file.IsCompactDexFile()) {
-      Init(down_cast<const CompactDexFile::CodeItem&>(*code_item));
-    } else {
-      DCHECK(dex_file.IsStandardDexFile());
-      Init(down_cast<const StandardDexFile::CodeItem&>(*code_item));
-    }
+    Init(down_cast<const StandardDexFile::CodeItem&>(*code_item));
   }
 }
 
@@ -88,18 +69,6 @@ inline IterationRange<DexInstructionIterator> CodeItemInstructionAccessor::Instr
       DexInstructionIterator(insns_, insns_size_in_code_units_) };
 }
 
-template <>
-inline void CodeItemDataAccessor::Init<CompactDexFile::CodeItem>(
-    const CompactDexFile::CodeItem& code_item) {
-  uint32_t insns_size_in_code_units;
-  code_item.DecodeFields</*kDecodeOnlyInstructionCount*/ false>(&insns_size_in_code_units,
-                                                                &registers_size_,
-                                                                &ins_size_,
-                                                                &outs_size_,
-                                                                &tries_size_);
-  CodeItemInstructionAccessor::Init(insns_size_in_code_units, code_item.insns_);
-}
-
 template <>
 inline void CodeItemDataAccessor::Init<StandardDexFile::CodeItem>(
     const StandardDexFile::CodeItem& code_item) {
@@ -110,15 +79,10 @@ inline void CodeItemDataAccessor::Init<StandardDexFile::CodeItem>(
   tries_size_ = code_item.tries_size_;
 }
 
-inline void CodeItemDataAccessor::Init(const DexFile& dex_file,
+inline void CodeItemDataAccessor::Init([[maybe_unused]] const DexFile& dex_file,
                                        const dex::CodeItem* code_item) {
   if (code_item != nullptr) {
-    if (dex_file.IsCompactDexFile()) {
-      Init(down_cast<const CompactDexFile::CodeItem&>(*code_item));
-    } else {
-      DCHECK(dex_file.IsStandardDexFile());
-      Init(down_cast<const StandardDexFile::CodeItem&>(*code_item));
-    }
+    Init(down_cast<const StandardDexFile::CodeItem&>(*code_item));
   }
 }
 
@@ -167,15 +131,6 @@ inline const void* CodeItemDataAccessor::CodeItemDataEnd() const {
   return reinterpret_cast<const void*>(handler_data);
 }
 
-template <>
-inline void CodeItemDebugInfoAccessor::Init<CompactDexFile::CodeItem>(
-    const CompactDexFile::CodeItem& code_item,
-    uint32_t dex_method_index) {
-  debug_info_offset_ = down_cast<const CompactDexFile*>(dex_file_)->GetDebugInfoOffset(
-      dex_method_index);
-  CodeItemDataAccessor::Init(code_item);
-}
-
 template <>
 inline void CodeItemDebugInfoAccessor::Init<StandardDexFile::CodeItem>(
     const StandardDexFile::CodeItem& code_item, [[maybe_unused]] uint32_t dex_method_index) {
@@ -190,12 +145,7 @@ inline void CodeItemDebugInfoAccessor::Init(const DexFile& dex_file,
   if (code_item == nullptr) {
     return;
   }
-  if (dex_file.IsCompactDexFile()) {
-    Init(down_cast<const CompactDexFile::CodeItem&>(*code_item), dex_method_index);
-  } else {
-    DCHECK(dex_file.IsStandardDexFile());
-    Init(down_cast<const StandardDexFile::CodeItem&>(*code_item), dex_method_index);
-  }
+  Init(down_cast<const StandardDexFile::CodeItem&>(*code_item), dex_method_index);
 }
 
 template<typename NewLocalVisitor>
diff --git a/libdexfile/dex/code_item_accessors_test.cc b/libdexfile/dex/code_item_accessors_test.cc
index bd1ddc958d..ada1449515 100644
--- a/libdexfile/dex/code_item_accessors_test.cc
+++ b/libdexfile/dex/code_item_accessors_test.cc
@@ -28,23 +28,13 @@ namespace art {
 
 class CodeItemAccessorsTest : public ::testing::Test {};
 
-std::unique_ptr<const DexFile> CreateFakeDex(bool compact_dex, std::vector<uint8_t>* data) {
+std::unique_ptr<const DexFile> CreateFakeDex(std::vector<uint8_t>* data) {
   data->resize(MemMap::GetPageSize());
-  if (compact_dex) {
-    CompactDexFile::Header* header =
-        const_cast<CompactDexFile::Header*>(CompactDexFile::Header::At(data->data()));
-    CompactDexFile::WriteMagic(header->magic_.data());
-    CompactDexFile::WriteCurrentVersion(header->magic_.data());
-    header->data_off_ = 0;
-    header->data_size_ = data->size();
-    header->file_size_ = data->size();
-  } else {
-    auto* header = reinterpret_cast<DexFile::Header*>(data->data());
-    StandardDexFile::WriteMagic(data->data());
-    StandardDexFile::WriteCurrentVersion(data->data());
-    header->header_size_ = sizeof(*header);
-    header->file_size_ = data->size();
-  }
+  auto* header = reinterpret_cast<DexFile::Header*>(data->data());
+  StandardDexFile::WriteMagic(data->data());
+  StandardDexFile::WriteCurrentVersion(data->data());
+  header->header_size_ = sizeof(*header);
+  header->file_size_ = data->size();
   DexFileLoader dex_file_loader(data->data(), data->size(), "location");
   std::string error_msg;
   std::unique_ptr<const DexFile> dex(dex_file_loader.Open(/*location_checksum=*/123,
@@ -58,8 +48,7 @@ std::unique_ptr<const DexFile> CreateFakeDex(bool compact_dex, std::vector<uint8
 
 TEST(CodeItemAccessorsTest, TestDexInstructionsAccessor) {
   std::vector<uint8_t> standard_dex_data;
-  std::unique_ptr<const DexFile> standard_dex(CreateFakeDex(/*compact_dex=*/false,
-                                                            &standard_dex_data));
+  std::unique_ptr<const DexFile> standard_dex(CreateFakeDex(&standard_dex_data));
   ASSERT_TRUE(standard_dex != nullptr);
   static constexpr uint16_t kRegisterSize = 2;
   static constexpr uint16_t kInsSize = 1;
diff --git a/libdexfile/dex/compact_dex_file.cc b/libdexfile/dex/compact_dex_file.cc
deleted file mode 100644
index 9bc38f3d3f..0000000000
--- a/libdexfile/dex/compact_dex_file.cc
+++ /dev/null
@@ -1,99 +0,0 @@
-/*
- * Copyright (C) 2017 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "compact_dex_file.h"
-
-#include <memory>
-
-#include "base/leb128.h"
-#include "code_item_accessors-inl.h"
-#include "dex_file-inl.h"
-
-namespace art {
-
-void CompactDexFile::WriteMagic(uint8_t* magic) {
-  std::copy_n(kDexMagic, kDexMagicSize, magic);
-}
-
-void CompactDexFile::WriteCurrentVersion(uint8_t* magic) {
-  std::copy_n(kDexMagicVersion, kDexVersionLen, magic + kDexMagicSize);
-}
-
-bool CompactDexFile::IsMagicValid(const uint8_t* magic) {
-  return (memcmp(magic, kDexMagic, sizeof(kDexMagic)) == 0);
-}
-
-bool CompactDexFile::IsVersionValid(const uint8_t* magic) {
-  const uint8_t* version = &magic[sizeof(kDexMagic)];
-  return memcmp(version, kDexMagicVersion, kDexVersionLen) == 0;
-}
-
-bool CompactDexFile::IsMagicValid() const {
-  return IsMagicValid(header_->magic_);
-}
-
-bool CompactDexFile::IsVersionValid() const { return IsVersionValid(header_->magic_.data()); }
-
-bool CompactDexFile::SupportsDefaultMethods() const {
-  return (GetHeader().GetFeatureFlags() &
-      static_cast<uint32_t>(FeatureFlags::kDefaultMethods)) != 0;
-}
-
-uint32_t CompactDexFile::GetCodeItemSize(const dex::CodeItem& item) const {
-  DCHECK(IsInDataSection(&item));
-  return reinterpret_cast<uintptr_t>(CodeItemDataAccessor(*this, &item).CodeItemDataEnd()) -
-      reinterpret_cast<uintptr_t>(&item);
-}
-
-
-uint32_t CompactDexFile::CalculateChecksum(const uint8_t* base_begin,
-                                           size_t base_size,
-                                           const uint8_t* data_begin,
-                                           size_t data_size) {
-  Header temp_header(*Header::At(base_begin));
-  // Zero out fields that are not included in the sum.
-  temp_header.checksum_ = 0u;
-  temp_header.data_off_ = 0u;
-  temp_header.data_size_ = 0u;
-  uint32_t checksum = ChecksumMemoryRange(reinterpret_cast<const uint8_t*>(&temp_header),
-                                          sizeof(temp_header));
-  // Exclude the header since we already computed it's checksum.
-  checksum = (checksum * 31) ^ ChecksumMemoryRange(base_begin + sizeof(temp_header),
-                                                   base_size - sizeof(temp_header));
-  checksum = (checksum * 31) ^ ChecksumMemoryRange(data_begin, data_size);
-  return checksum;
-}
-
-uint32_t CompactDexFile::CalculateChecksum() const {
-  return CalculateChecksum(Begin(), Size(), DataBegin(), DataSize());
-}
-
-CompactDexFile::CompactDexFile(const uint8_t* base,
-                               const std::string& location,
-                               uint32_t location_checksum,
-                               const OatDexFile* oat_dex_file,
-                               std::shared_ptr<DexFileContainer> container)
-    : DexFile(base,
-              location,
-              location_checksum,
-              oat_dex_file,
-              std::move(container),
-              /*is_compact_dex=*/true),
-      debug_info_offsets_(DataBegin() + GetHeader().debug_info_offsets_pos_,
-                          GetHeader().debug_info_base_,
-                          GetHeader().debug_info_offsets_table_offset_) {}
-
-}  // namespace art
diff --git a/libdexfile/dex/compact_dex_file.h b/libdexfile/dex/compact_dex_file.h
deleted file mode 100644
index 468ff49ecd..0000000000
--- a/libdexfile/dex/compact_dex_file.h
+++ /dev/null
@@ -1,324 +0,0 @@
-/*
- * Copyright (C) 2017 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef ART_LIBDEXFILE_DEX_COMPACT_DEX_FILE_H_
-#define ART_LIBDEXFILE_DEX_COMPACT_DEX_FILE_H_
-
-#include <memory>
-
-#include "base/casts.h"
-#include "dex/compact_offset_table.h"
-#include "dex_file.h"
-
-namespace art {
-
-// CompactDex is a currently ART internal dex file format that aims to reduce storage/RAM usage.
-class CompactDexFile : public DexFile {
- public:
-  static constexpr uint8_t kDexMagic[kDexMagicSize] = { 'c', 'd', 'e', 'x' };
-  // Last change: remove code item deduping.
-  static constexpr uint8_t kDexMagicVersion[] = {'0', '0', '1', '\0'};
-
-  enum class FeatureFlags : uint32_t {
-    kDefaultMethods = 0x1,
-  };
-
-  class Header : public DexFile::Header {
-   public:
-    static const Header* At(const void* at) {
-      return reinterpret_cast<const Header*>(at);
-    }
-
-    uint32_t GetFeatureFlags() const {
-      return feature_flags_;
-    }
-
-    uint32_t GetDataOffset() const {
-      return data_off_;
-    }
-
-    uint32_t GetDataSize() const {
-      return data_size_;
-    }
-
-    // Range of the shared data section owned by the dex file. Owned in this context refers to data
-    // for this DEX that was not deduplicated to another DEX.
-    uint32_t OwnedDataBegin() const {
-      return owned_data_begin_;
-    }
-
-    uint32_t OwnedDataEnd() const {
-      return owned_data_end_;
-    }
-
-   private:
-    uint32_t feature_flags_ = 0u;
-
-    // Position in the compact dex file for the debug info table data starts.
-    uint32_t debug_info_offsets_pos_ = 0u;
-
-    // Offset into the debug info table data where the lookup table is.
-    uint32_t debug_info_offsets_table_offset_ = 0u;
-
-    // Base offset of where debug info starts in the dex file.
-    uint32_t debug_info_base_ = 0u;
-
-    // Range of the shared data section owned by the dex file.
-    uint32_t owned_data_begin_ = 0u;
-    uint32_t owned_data_end_ = 0u;
-
-    friend class CompactDexFile;
-    friend class CompactDexWriter;
-  };
-
-  // Like the standard code item except without a debug info offset. Each code item may have a
-  // preheader to encode large methods. In 99% of cases, the preheader is not used. This enables
-  // smaller size with a good fast path case in the accessors.
-  struct CodeItem : public dex::CodeItem {
-    static constexpr size_t kAlignment = sizeof(uint16_t);
-    // Max preheader size in uint16_ts.
-    static constexpr size_t kMaxPreHeaderSize = 6;
-
-    static constexpr size_t FieldsOffset() {
-      return OFFSETOF_MEMBER(CodeItem, fields_);
-    }
-
-    static constexpr size_t InsnsCountAndFlagsOffset() {
-      return OFFSETOF_MEMBER(CodeItem, insns_count_and_flags_);
-    }
-
-    static constexpr size_t InsnsOffset() {
-      return OFFSETOF_MEMBER(CodeItem, insns_);
-    }
-
-    static constexpr size_t kRegistersSizeShift = 12;
-    static constexpr size_t kInsSizeShift = 8;
-    static constexpr size_t kOutsSizeShift = 4;
-    static constexpr size_t kTriesSizeSizeShift = 0;
-    static constexpr uint16_t kBitPreHeaderRegistersSize = 0;
-    static constexpr uint16_t kBitPreHeaderInsSize = 1;
-    static constexpr uint16_t kBitPreHeaderOutsSize = 2;
-    static constexpr uint16_t kBitPreHeaderTriesSize = 3;
-    static constexpr uint16_t kBitPreHeaderInsnsSize = 4;
-    static constexpr uint16_t kFlagPreHeaderRegistersSize = 0x1 << kBitPreHeaderRegistersSize;
-    static constexpr uint16_t kFlagPreHeaderInsSize = 0x1 << kBitPreHeaderInsSize;
-    static constexpr uint16_t kFlagPreHeaderOutsSize = 0x1 << kBitPreHeaderOutsSize;
-    static constexpr uint16_t kFlagPreHeaderTriesSize = 0x1 << kBitPreHeaderTriesSize;
-    static constexpr uint16_t kFlagPreHeaderInsnsSize = 0x1 << kBitPreHeaderInsnsSize;
-    static constexpr size_t kInsnsSizeShift = 5;
-    static constexpr size_t kInsnsSizeBits = sizeof(uint16_t) * kBitsPerByte -  kInsnsSizeShift;
-
-   private:
-    CodeItem() = default;
-
-    // Combined preheader flags for fast testing if we need to go slow path.
-    static constexpr uint16_t kFlagPreHeaderCombined =
-        kFlagPreHeaderRegistersSize |
-        kFlagPreHeaderInsSize |
-        kFlagPreHeaderOutsSize |
-        kFlagPreHeaderTriesSize |
-        kFlagPreHeaderInsnsSize;
-
-    // Create a code item and associated preheader if required based on field values.
-    // Returns the start of the preheader. The preheader buffer must be at least as large as
-    // kMaxPreHeaderSize;
-    uint16_t* Create(uint16_t registers_size,
-                     uint16_t ins_size,
-                     uint16_t outs_size,
-                     uint16_t tries_size,
-                     uint32_t insns_size_in_code_units,
-                     uint16_t* out_preheader) {
-      // Dex verification ensures that registers size > ins_size, so we can subtract the registers
-      // size accordingly to reduce how often we need to use the preheader.
-      DCHECK_GE(registers_size, ins_size);
-      registers_size -= ins_size;
-      fields_ = (registers_size & 0xF) << kRegistersSizeShift;
-      fields_ |= (ins_size & 0xF) << kInsSizeShift;
-      fields_ |= (outs_size & 0xF) << kOutsSizeShift;
-      fields_ |= (tries_size & 0xF) << kTriesSizeSizeShift;
-      registers_size &= ~0xF;
-      ins_size &= ~0xF;
-      outs_size &= ~0xF;
-      tries_size &= ~0xF;
-      insns_count_and_flags_ = 0;
-      const size_t masked_count = insns_size_in_code_units & ((1 << kInsnsSizeBits) - 1);
-      insns_count_and_flags_ |= masked_count << kInsnsSizeShift;
-      insns_size_in_code_units -= masked_count;
-
-      // Since the preheader case is rare (1% of code items), use a suboptimally large but fast
-      // decoding format.
-      if (insns_size_in_code_units != 0) {
-        insns_count_and_flags_ |= kFlagPreHeaderInsnsSize;
-        --out_preheader;
-        *out_preheader = static_cast<uint16_t>(insns_size_in_code_units);
-        --out_preheader;
-        *out_preheader = static_cast<uint16_t>(insns_size_in_code_units >> 16);
-      }
-      auto preheader_encode = [&](uint16_t size, uint16_t flag) {
-        if (size != 0) {
-          insns_count_and_flags_ |= flag;
-          --out_preheader;
-          *out_preheader = size;
-        }
-      };
-      preheader_encode(registers_size, kFlagPreHeaderRegistersSize);
-      preheader_encode(ins_size, kFlagPreHeaderInsSize);
-      preheader_encode(outs_size, kFlagPreHeaderOutsSize);
-      preheader_encode(tries_size, kFlagPreHeaderTriesSize);
-      return out_preheader;
-    }
-
-    ALWAYS_INLINE bool HasPreHeader(uint16_t flag) const {
-      return (insns_count_and_flags_ & flag) != 0;
-    }
-
-    // Return true if the code item has any preheaders.
-    ALWAYS_INLINE static bool HasAnyPreHeader(uint16_t insns_count_and_flags) {
-      return (insns_count_and_flags & kFlagPreHeaderCombined) != 0;
-    }
-
-    ALWAYS_INLINE uint16_t* GetPreHeader() {
-      return reinterpret_cast<uint16_t*>(this);
-    }
-
-    ALWAYS_INLINE const uint16_t* GetPreHeader() const {
-      return reinterpret_cast<const uint16_t*>(this);
-    }
-
-    // Decode fields and read the preheader if necessary. If kDecodeOnlyInstructionCount is
-    // specified then only the instruction count is decoded.
-    template <bool kDecodeOnlyInstructionCount>
-    ALWAYS_INLINE void DecodeFields(uint32_t* insns_count,
-                                    uint16_t* registers_size,
-                                    uint16_t* ins_size,
-                                    uint16_t* outs_size,
-                                    uint16_t* tries_size) const {
-      *insns_count = insns_count_and_flags_ >> kInsnsSizeShift;
-      if (!kDecodeOnlyInstructionCount) {
-        const uint16_t fields = fields_;
-        *registers_size = (fields >> kRegistersSizeShift) & 0xF;
-        *ins_size = (fields >> kInsSizeShift) & 0xF;
-        *outs_size = (fields >> kOutsSizeShift) & 0xF;
-        *tries_size = (fields >> kTriesSizeSizeShift) & 0xF;
-      }
-      if (UNLIKELY(HasAnyPreHeader(insns_count_and_flags_))) {
-        const uint16_t* preheader = GetPreHeader();
-        if (HasPreHeader(kFlagPreHeaderInsnsSize)) {
-          --preheader;
-          *insns_count += static_cast<uint32_t>(*preheader);
-          --preheader;
-          *insns_count += static_cast<uint32_t>(*preheader) << 16;
-        }
-        if (!kDecodeOnlyInstructionCount) {
-          if (HasPreHeader(kFlagPreHeaderRegistersSize)) {
-            --preheader;
-            *registers_size += preheader[0];
-          }
-          if (HasPreHeader(kFlagPreHeaderInsSize)) {
-            --preheader;
-            *ins_size += preheader[0];
-          }
-          if (HasPreHeader(kFlagPreHeaderOutsSize)) {
-            --preheader;
-            *outs_size += preheader[0];
-          }
-          if (HasPreHeader(kFlagPreHeaderTriesSize)) {
-            --preheader;
-            *tries_size += preheader[0];
-          }
-        }
-      }
-      if (!kDecodeOnlyInstructionCount) {
-        *registers_size += *ins_size;
-      }
-    }
-
-    // Packed code item data, 4 bits each: [registers_size, ins_size, outs_size, tries_size]
-    uint16_t fields_;
-
-    // 5 bits for if either of the fields required preheader extension, 11 bits for the number of
-    // instruction code units.
-    uint16_t insns_count_and_flags_;
-
-    uint16_t insns_[1];                  // actual array of bytecode.
-
-    ART_FRIEND_TEST(CodeItemAccessorsTest, TestDexInstructionsAccessor);
-    ART_FRIEND_TEST(CompactDexFileTest, CodeItemFields);
-    friend class CodeItemDataAccessor;
-    friend class CodeItemDebugInfoAccessor;
-    friend class CodeItemInstructionAccessor;
-    friend class CompactDexFile;
-    friend class CompactDexWriter;
-    DISALLOW_COPY_AND_ASSIGN(CodeItem);
-  };
-
-  // Write the compact dex specific magic.
-  static void WriteMagic(uint8_t* magic);
-
-  // Write the current version, note that the input is the address of the magic.
-  static void WriteCurrentVersion(uint8_t* magic);
-
-  // Returns true if the byte string points to the magic value.
-  static bool IsMagicValid(const uint8_t* magic);
-  static bool IsMagicValid(DexFile::Magic magic) { return IsMagicValid(magic.data()); }
-  bool IsMagicValid() const override;
-
-  // Returns true if the byte string after the magic is the correct value.
-  static bool IsVersionValid(const uint8_t* magic);
-  bool IsVersionValid() const override;
-
-  // TODO This is completely a guess. We really need to do better. b/72402467
-  // We ask for 64 megabytes which should be big enough for any realistic dex file.
-  size_t GetDequickenedSize() const override {
-    return 64 * MB;
-  }
-
-  const Header& GetHeader() const {
-    return down_cast<const Header&>(DexFile::GetHeader());
-  }
-
-  bool SupportsDefaultMethods() const override;
-
-  uint32_t GetCodeItemSize(const dex::CodeItem& item) const override;
-
-  uint32_t GetDebugInfoOffset(uint32_t dex_method_index) const {
-    return debug_info_offsets_.GetOffset(dex_method_index);
-  }
-
-  static uint32_t CalculateChecksum(const uint8_t* base_begin,
-                                    size_t base_size,
-                                    const uint8_t* data_begin,
-                                    size_t data_size);
-  uint32_t CalculateChecksum() const override;
-
- private:
-  CompactDexFile(const uint8_t* base,
-                 const std::string& location,
-                 uint32_t location_checksum,
-                 const OatDexFile* oat_dex_file,
-                 // Shared since several dex files may be stored in the same logical container.
-                 std::shared_ptr<DexFileContainer> container);
-
-  CompactOffsetTable::Accessor debug_info_offsets_;
-
-  friend class DexFile;
-  friend class DexFileLoader;
-  DISALLOW_COPY_AND_ASSIGN(CompactDexFile);
-};
-
-}  // namespace art
-
-#endif  // ART_LIBDEXFILE_DEX_COMPACT_DEX_FILE_H_
diff --git a/libdexfile/dex/compact_dex_file_test.cc b/libdexfile/dex/compact_dex_file_test.cc
deleted file mode 100644
index 345e66b1d5..0000000000
--- a/libdexfile/dex/compact_dex_file_test.cc
+++ /dev/null
@@ -1,101 +0,0 @@
-/*
- * Copyright (C) 2017 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-#include "compact_dex_file.h"
-#include "dex_file_loader.h"
-#include "gtest/gtest.h"
-
-namespace art {
-
-TEST(CompactDexFileTest, MagicAndVersion) {
-  // Test permutations of valid/invalid headers.
-  for (size_t i = 0; i < 2; ++i) {
-    for (size_t j = 0; j < 2; ++j) {
-      static const size_t len = CompactDexFile::kDexVersionLen + CompactDexFile::kDexMagicSize;
-      uint8_t header[len] = {};
-      std::fill_n(header, len, 0x99);
-      const bool valid_magic = (i & 1) == 0;
-      const bool valid_version = (j & 1) == 0;
-      if (valid_magic) {
-        CompactDexFile::WriteMagic(header);
-      }
-      if (valid_version) {
-        CompactDexFile::WriteCurrentVersion(header);
-      }
-      EXPECT_EQ(valid_magic, CompactDexFile::IsMagicValid(header));
-      EXPECT_EQ(valid_version, CompactDexFile::IsVersionValid(header));
-      EXPECT_FALSE(DexFileLoader::IsMagicValid(header));
-      EXPECT_FALSE(DexFileLoader::IsVersionAndMagicValid(header));
-    }
-  }
-}
-
-TEST(CompactDexFileTest, CodeItemFields) {
-  auto test_and_write = [&] (uint16_t registers_size,
-                             uint16_t ins_size,
-                             uint16_t outs_size,
-                             uint16_t tries_size,
-                             uint32_t insns_size_in_code_units) {
-    ASSERT_GE(registers_size, ins_size);
-    uint16_t buffer[sizeof(CompactDexFile::CodeItem) +
-                        CompactDexFile::CodeItem::kMaxPreHeaderSize] = {};
-    CompactDexFile::CodeItem* code_item = reinterpret_cast<CompactDexFile::CodeItem*>(
-        &buffer[CompactDexFile::CodeItem::kMaxPreHeaderSize]);
-    const uint16_t* preheader_ptr = code_item->Create(registers_size,
-                                                      ins_size,
-                                                      outs_size,
-                                                      tries_size,
-                                                      insns_size_in_code_units,
-                                                      code_item->GetPreHeader());
-    ASSERT_GT(preheader_ptr, buffer);
-
-    uint16_t out_registers_size;
-    uint16_t out_ins_size;
-    uint16_t out_outs_size;
-    uint16_t out_tries_size;
-    uint32_t out_insns_size_in_code_units;
-    code_item->DecodeFields</*kDecodeOnlyInstructionCount=*/false>(&out_insns_size_in_code_units,
-                                                                   &out_registers_size,
-                                                                   &out_ins_size,
-                                                                   &out_outs_size,
-                                                                   &out_tries_size);
-    ASSERT_EQ(registers_size, out_registers_size);
-    ASSERT_EQ(ins_size, out_ins_size);
-    ASSERT_EQ(outs_size, out_outs_size);
-    ASSERT_EQ(tries_size, out_tries_size);
-    ASSERT_EQ(insns_size_in_code_units, out_insns_size_in_code_units);
-
-    ++out_insns_size_in_code_units;  // Force value to change.
-    code_item->DecodeFields</*kDecodeOnlyInstructionCount=*/true>(&out_insns_size_in_code_units,
-                                                                  /*registers_size=*/ nullptr,
-                                                                  /*ins_size=*/ nullptr,
-                                                                  /*outs_size=*/ nullptr,
-                                                                  /*tries_size=*/ nullptr);
-    ASSERT_EQ(insns_size_in_code_units, out_insns_size_in_code_units);
-  };
-  static constexpr uint32_t kMax32 = std::numeric_limits<uint32_t>::max();
-  static constexpr uint16_t kMax16 = std::numeric_limits<uint16_t>::max();
-  test_and_write(0, 0, 0, 0, 0);
-  test_and_write(kMax16, kMax16, kMax16, kMax16, kMax32);
-  test_and_write(kMax16 - 1, kMax16 - 2, kMax16 - 3, kMax16 - 4, kMax32 - 5);
-  test_and_write(kMax16 - 4, kMax16 - 5, kMax16 - 3, kMax16 - 2, kMax32 - 1);
-  test_and_write(5, 4, 3, 2, 1);
-  test_and_write(5, 0, 3, 2, 1);
-  test_and_write(kMax16, 0, kMax16 / 2, 1234, kMax32 / 4);
-}
-
-}  // namespace art
diff --git a/libdexfile/dex/compact_offset_table.cc b/libdexfile/dex/compact_offset_table.cc
deleted file mode 100644
index deec124193..0000000000
--- a/libdexfile/dex/compact_offset_table.cc
+++ /dev/null
@@ -1,131 +0,0 @@
-/*
- * Copyright (C) 2018 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "compact_offset_table.h"
-
-#include "compact_dex_utils.h"
-#include "base/leb128.h"
-
-namespace art {
-
-CompactOffsetTable::Accessor::Accessor(const uint8_t* data_begin,
-                                       uint32_t minimum_offset,
-                                       uint32_t table_offset)
-    : table_(reinterpret_cast<const uint32_t*>(data_begin + table_offset)),
-      minimum_offset_(minimum_offset),
-      data_begin_(data_begin) {}
-
-CompactOffsetTable::Accessor::Accessor(const uint8_t* data_begin)
-    : Accessor(data_begin + 2 * sizeof(uint32_t),
-               reinterpret_cast<const uint32_t*>(data_begin)[0],
-               reinterpret_cast<const uint32_t*>(data_begin)[1]) {}
-
-uint32_t CompactOffsetTable::Accessor::GetOffset(uint32_t index) const {
-  const uint32_t offset = table_[index / kElementsPerIndex];
-  const size_t bit_index = index % kElementsPerIndex;
-
-  const uint8_t* block = data_begin_ + offset;
-  uint16_t bit_mask = *block;
-  ++block;
-  bit_mask = (bit_mask << kBitsPerByte) | *block;
-  ++block;
-  if ((bit_mask & (1 << bit_index)) == 0) {
-    // Bit is not set means the offset is 0.
-    return 0u;
-  }
-  // Trim off the bits above the index we want and count how many bits are set. This is how many
-  // lebs we need to decode.
-  size_t count = POPCOUNT(static_cast<uintptr_t>(bit_mask) << (kBitsPerIntPtrT - 1 - bit_index));
-  DCHECK_GT(count, 0u);
-  uint32_t current_offset = minimum_offset_;
-  do {
-    current_offset += DecodeUnsignedLeb128(&block);
-    --count;
-  } while (count > 0);
-  return current_offset;
-}
-
-void CompactOffsetTable::Build(const std::vector<uint32_t>& offsets,
-                               std::vector<uint8_t>* out_data) {
-  static constexpr size_t kNumOffsets = 2;
-  uint32_t out_offsets[kNumOffsets] = {};
-  CompactOffsetTable::Build(offsets, out_data, &out_offsets[0], &out_offsets[1]);
-  // Write the offsets at the start of the debug info.
-  out_data->insert(out_data->begin(),
-                   reinterpret_cast<const uint8_t*>(&out_offsets[0]),
-                   reinterpret_cast<const uint8_t*>(&out_offsets[kNumOffsets]));
-}
-
-void CompactOffsetTable::Build(const std::vector<uint32_t>& offsets,
-                               std::vector<uint8_t>* out_data,
-                               uint32_t* out_min_offset,
-                               uint32_t* out_table_offset) {
-  DCHECK(out_data != nullptr);
-  DCHECK(out_data->empty());
-  // Calculate the base offset and return it.
-  *out_min_offset = std::numeric_limits<uint32_t>::max();
-  for (const uint32_t offset : offsets) {
-    if (offset != 0u) {
-      *out_min_offset = std::min(*out_min_offset, offset);
-    }
-  }
-  // Write the leb blocks and store the important offsets (each kElementsPerIndex elements).
-  size_t block_start = 0;
-
-  std::vector<uint32_t> offset_table;
-
-  // Write data first then the table.
-  while (block_start < offsets.size()) {
-    // Write the offset of the block for each block.
-    offset_table.push_back(out_data->size());
-
-    // Block size of up to kElementsPerIndex
-    const size_t block_size = std::min(offsets.size() - block_start, kElementsPerIndex);
-
-    // Calculate bit mask since need to write that first.
-    uint16_t bit_mask = 0u;
-    for (size_t i = 0; i < block_size; ++i) {
-      if (offsets[block_start + i] != 0u) {
-        bit_mask |= 1 << i;
-      }
-    }
-    // Write bit mask.
-    out_data->push_back(static_cast<uint8_t>(bit_mask >> kBitsPerByte));
-    out_data->push_back(static_cast<uint8_t>(bit_mask));
-
-    // Write offsets relative to the previous offset.
-    uint32_t prev_offset = *out_min_offset;
-    for (size_t i = 0; i < block_size; ++i) {
-      const uint32_t offset = offsets[block_start + i];
-      if (offset != 0u) {
-        uint32_t delta = offset - prev_offset;
-        EncodeUnsignedLeb128(out_data, delta);
-        prev_offset = offset;
-      }
-    }
-
-    block_start += block_size;
-  }
-
-  // Write the offset table.
-  AlignmentPadVector(out_data, alignof(uint32_t));
-  *out_table_offset = out_data->size();
-  out_data->insert(out_data->end(),
-                   reinterpret_cast<const uint8_t*>(&offset_table[0]),
-                   reinterpret_cast<const uint8_t*>(&offset_table[0] + offset_table.size()));
-}
-
-}  // namespace art
diff --git a/libdexfile/dex/compact_offset_table.h b/libdexfile/dex/compact_offset_table.h
deleted file mode 100644
index ec759e200d..0000000000
--- a/libdexfile/dex/compact_offset_table.h
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Copyright (C) 2018 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef ART_LIBDEXFILE_DEX_COMPACT_OFFSET_TABLE_H_
-#define ART_LIBDEXFILE_DEX_COMPACT_OFFSET_TABLE_H_
-
-#include <cstdint>
-#include <vector>
-
-namespace art {
-
-// Compact offset table that aims to minimize size while still providing reasonable speed (10-20ns
-// average time per lookup on host).
-class CompactOffsetTable {
- public:
-  // This value is coupled with the leb chunk bitmask. That logic must also be adjusted when the
-  // integer is modified.
-  static constexpr size_t kElementsPerIndex = 16;
-
-  // Leb block format:
-  // [uint16_t] 16 bit mask for what indexes actually have a non zero offset for the chunk.
-  // [lebs] Up to 16 lebs encoded using leb128, one leb bit. The leb specifies how the offset
-  // changes compared to the previous index.
-
-  class Accessor {
-   public:
-    // Read the minimum and table offsets from the data pointer.
-    explicit Accessor(const uint8_t* data_begin);
-
-    Accessor(const uint8_t* data_begin, uint32_t minimum_offset, uint32_t table_offset);
-
-    // Return the offset for the index.
-    uint32_t GetOffset(uint32_t index) const;
-
-   private:
-    const uint32_t* const table_;
-    const uint32_t minimum_offset_;
-    const uint8_t* const data_begin_;
-  };
-
-  // Version that also serializes the min offset and table offset.
-  static void Build(const std::vector<uint32_t>& offsets, std::vector<uint8_t>* out_data);
-
-  // Returned offsets are all relative to out_min_offset.
-  static void Build(const std::vector<uint32_t>& offsets,
-                    std::vector<uint8_t>* out_data,
-                    uint32_t* out_min_offset,
-                    uint32_t* out_table_offset);
-
-  // 32 bit aligned for the offset table.
-  static constexpr size_t kAlignment = sizeof(uint32_t);
-};
-
-}  // namespace art
-
-#endif  // ART_LIBDEXFILE_DEX_COMPACT_OFFSET_TABLE_H_
diff --git a/libdexfile/dex/compact_offset_table_test.cc b/libdexfile/dex/compact_offset_table_test.cc
deleted file mode 100644
index 724978dc9e..0000000000
--- a/libdexfile/dex/compact_offset_table_test.cc
+++ /dev/null
@@ -1,89 +0,0 @@
-/*
- * Copyright (C) 2018 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include <vector>
-
-#include <android-base/logging.h>
-#include "dex/compact_offset_table.h"
-#include "gtest/gtest.h"
-
-namespace art {
-
-TEST(CompactOffsetTableTest, TestBuildAndAccess) {
-  const size_t kDebugInfoMinOffset = 1234567;
-  std::vector<uint32_t> offsets = {
-      0, 17, 2, 3, 11, 0, 0, 0, 0, 1, 0, 1552, 100, 122, 44, 1234567, 0, 0,
-      std::numeric_limits<uint32_t>::max() - kDebugInfoMinOffset, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
-      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12,
-  };
-  // Add some large offset since the debug info section will never be that close to the beginning
-  // of the file.
-  for (uint32_t& offset : offsets) {
-    if (offset != 0u) {
-      offset += kDebugInfoMinOffset;
-    }
-  }
-
-  std::vector<uint8_t> data;
-  uint32_t min_offset = 0;
-  uint32_t table_offset = 0;
-  CompactOffsetTable::Build(offsets, /*out*/ &data, /*out*/ &min_offset, /*out*/ &table_offset);
-  EXPECT_GE(min_offset, kDebugInfoMinOffset);
-  EXPECT_LT(table_offset, data.size());
-  ASSERT_GT(data.size(), 0u);
-  const size_t before_size = offsets.size() * sizeof(offsets.front());
-  EXPECT_LT(data.size(), before_size);
-
-  // Note that the accessor requires the data to be aligned. Use memmap to accomplish this.
-  std::string error_msg;
-  // Leave some extra room since we don't copy the table at the start (for testing).
-  constexpr size_t kExtraOffset = 4 * 128;
-  std::vector<uint8_t> fake_dex(data.size() + kExtraOffset);
-  std::copy(data.begin(), data.end(), fake_dex.data() + kExtraOffset);
-
-  CompactOffsetTable::Accessor accessor(fake_dex.data() + kExtraOffset, min_offset, table_offset);
-  for (size_t i = 0; i < offsets.size(); ++i) {
-    EXPECT_EQ(offsets[i], accessor.GetOffset(i));
-  }
-
-  // Sort to produce a try and produce a smaller table. This happens because the leb diff is smaller
-  // for sorted increasing order.
-  std::sort(offsets.begin(), offsets.end());
-  std::vector<uint8_t> sorted_data;
-  CompactOffsetTable::Build(offsets,
-                            /*out*/ &sorted_data,
-                            /*out*/ &min_offset,
-                            /*out*/ &table_offset);
-  EXPECT_LT(sorted_data.size(), data.size());
-  {
-    android::base::ScopedLogSeverity sls(android::base::LogSeverity::INFO);
-    LOG(INFO) << "raw size " << before_size
-              << " table size " << data.size()
-              << " sorted table size " << sorted_data.size();
-  }
-
-  // Test constructor and accessor that serialize/read offsets.
-  {
-    std::vector<uint8_t> data2;
-    CompactOffsetTable::Build(offsets, /*out*/ &data2);
-    CompactOffsetTable::Accessor accessor2(&data2[0]);
-    for (size_t i = 0; i < offsets.size(); ++i) {
-      EXPECT_EQ(offsets[i], accessor2.GetOffset(i));
-    }
-  }
-}
-
-}  // namespace art
diff --git a/libdexfile/dex/dex_file-inl.h b/libdexfile/dex/dex_file-inl.h
index a6ea97d705..80524c4b48 100644
--- a/libdexfile/dex/dex_file-inl.h
+++ b/libdexfile/dex/dex_file-inl.h
@@ -24,7 +24,6 @@
 #include "base/leb128.h"
 #include "base/utils.h"
 #include "class_iterator.h"
-#include "compact_dex_file.h"
 #include "dex_instruction_iterator.h"
 #include "invoke_type.h"
 #include "signature.h"
@@ -547,13 +546,7 @@ bool DexFile::DecodeDebugPositionInfo(const uint8_t* stream,
   }
 }
 
-inline const CompactDexFile* DexFile::AsCompactDexFile() const {
-  DCHECK(IsCompactDexFile());
-  return down_cast<const CompactDexFile*>(this);
-}
-
 inline const StandardDexFile* DexFile::AsStandardDexFile() const {
-  DCHECK(IsStandardDexFile());
   return down_cast<const StandardDexFile*>(this);
 }
 
diff --git a/libdexfile/dex/dex_file.cc b/libdexfile/dex/dex_file.cc
index f30d39f30a..c24a492944 100644
--- a/libdexfile/dex/dex_file.cc
+++ b/libdexfile/dex/dex_file.cc
@@ -34,7 +34,6 @@
 #include "base/pointer_size.h"
 #include "base/stl_util.h"
 #include "class_accessor-inl.h"
-#include "compact_dex_file.h"
 #include "descriptors_names.h"
 #include "dex_file-inl.h"
 #include "standard_dex_file.h"
@@ -103,9 +102,6 @@ uint32_t DexFile::Header::GetExpectedHeaderSize() const {
 }
 
 bool DexFile::Header::HasDexContainer() const {
-  if (CompactDexFile::IsMagicValid(magic_.data())) {
-    return false;
-  }
   DCHECK_EQ(header_size_, GetExpectedHeaderSize());
   return header_size_ >= sizeof(HeaderV41);
 }
@@ -140,12 +136,6 @@ ALWAYS_INLINE const T* DexFile::GetSection(const uint32_t* offset, DexFileContai
   if (size < sizeof(Header)) {
     return nullptr;  // Invalid dex file.
   }
-  // Compact dex is inconsistent: section offsets are relative to the
-  // header as opposed to the data section like all other its offsets.
-  if (CompactDexFile::IsMagicValid(begin_)) {
-    const uint8_t* data = reinterpret_cast<const uint8_t*>(header_);
-    return reinterpret_cast<const T*>(data + *offset);
-  }
   return reinterpret_cast<const T*>(data_.data() + *offset);
 }
 
@@ -153,8 +143,7 @@ DexFile::DexFile(const uint8_t* base,
                  const std::string& location,
                  uint32_t location_checksum,
                  const OatDexFile* oat_dex_file,
-                 std::shared_ptr<DexFileContainer> container,
-                 bool is_compact_dex)
+                 std::shared_ptr<DexFileContainer> container)
     : begin_(base),
       data_(GetDataRange(base, container.get())),
       location_(location),
@@ -173,7 +162,6 @@ DexFile::DexFile(const uint8_t* base,
       hiddenapi_class_data_(nullptr),
       oat_dex_file_(oat_dex_file),
       container_(std::move(container)),
-      is_compact_dex_(is_compact_dex),
       hiddenapi_domain_(hiddenapi::Domain::kApplication) {
   CHECK(begin_ != nullptr) << GetLocation();
   // Check base (=header) alignment.
@@ -207,15 +195,13 @@ bool DexFile::Init(std::string* error_msg) {
   if (!CheckMagicAndVersion(error_msg)) {
     return false;
   }
-  if (!IsCompactDexFile()) {
-    uint32_t expected_header_size = header_->GetExpectedHeaderSize();
-    if (header_->header_size_ != expected_header_size) {
-      *error_msg = StringPrintf("Unable to open '%s' : Header size is %u but %u was expected",
-                                location_.c_str(),
-                                header_->header_size_,
-                                expected_header_size);
-      return false;
-    }
+  uint32_t expected_header_size = header_->GetExpectedHeaderSize();
+  if (header_->header_size_ != expected_header_size) {
+    *error_msg = StringPrintf("Unable to open '%s' : Header size is %u but %u was expected",
+                              location_.c_str(),
+                              header_->header_size_,
+                              expected_header_size);
+    return false;
   }
   if (container_size < header_->file_size_) {
     *error_msg = StringPrintf("Unable to open '%s' : File size is %zu but the header expects %u",
@@ -267,16 +253,6 @@ ArrayRef<const uint8_t> DexFile::GetDataRange(const uint8_t* data, DexFileContai
     } else {
       size = header->file_size_;
     }
-  } else if (size >= sizeof(CompactDexFile::Header) && CompactDexFile::IsMagicValid(data)) {
-    auto header = reinterpret_cast<const CompactDexFile::Header*>(data);
-    // TODO: Remove. This is a hack. See comment of the Data method.
-    ArrayRef<const uint8_t> separate_data = container->Data();
-    if (separate_data.size() > 0) {
-      return separate_data;
-    }
-    // Shared compact dex data is located at the end after all dex files.
-    data += std::min<size_t>(header->data_off_, size);
-    size = header->data_size_;
   }
   // The returned range is guaranteed to be in bounds of the container memory.
   return {data, std::min<size_t>(size, container->End() - data)};
diff --git a/libdexfile/dex/dex_file.h b/libdexfile/dex/dex_file.h
index 7cf0b7f62c..20dbf5742c 100644
--- a/libdexfile/dex/dex_file.h
+++ b/libdexfile/dex/dex_file.h
@@ -40,7 +40,6 @@ namespace art {
 
 class ClassDataItemIterator;
 class ClassIterator;
-class CompactDexFile;
 class DexInstructionIterator;
 enum InvokeType : uint32_t;
 template <typename Iter> class IterationRange;
@@ -105,11 +104,9 @@ class MemoryDexFileContainer : public DexFileContainer {
   DISALLOW_COPY_AND_ASSIGN(MemoryDexFileContainer);
 };
 
-// Dex file is the API that exposes native dex files (ordinary dex files) and CompactDex.
-// Originally, the dex file format used by ART was mostly the same as APKs. The only change was
-// quickened opcodes and layout optimizations.
-// Since ART needs to support both native dex files and CompactDex files, the DexFile interface
-// provides an abstraction to facilitate this.
+// Dex file is the API that exposes native dex files (ordinary dex files).
+// The dex file format used by ART is mostly the same as APKs, but this
+// abstraction is present to allow ART internal dex files.
 class DexFile {
  public:
   // Number of bytes in the dex file magic.
@@ -231,11 +228,13 @@ class DexFile {
   };
 
   // Annotation constants.
-  enum {
-    kDexVisibilityBuild         = 0x00,     /* annotation visibility */
-    kDexVisibilityRuntime       = 0x01,
-    kDexVisibilitySystem        = 0x02,
+  enum class DexVisibility : uint8_t {
+    kBuild         = 0x00,
+    kRuntime       = 0x01,
+    kSystem        = 0x02,
+  };
 
+  enum {
     kDexAnnotationByte          = 0x00,
     kDexAnnotationShort         = 0x02,
     kDexAnnotationChar          = 0x03,
@@ -876,15 +875,7 @@ class DexFile {
     return result;
   }
 
-  // Not virtual for performance reasons.
-  ALWAYS_INLINE bool IsCompactDexFile() const {
-    return is_compact_dex_;
-  }
-  ALWAYS_INLINE bool IsStandardDexFile() const {
-    return !is_compact_dex_;
-  }
   ALWAYS_INLINE const StandardDexFile* AsStandardDexFile() const;
-  ALWAYS_INLINE const CompactDexFile* AsCompactDexFile() const;
 
   hiddenapi::Domain GetHiddenapiDomain() const { return hiddenapi_domain_; }
   void SetHiddenapiDomain(hiddenapi::Domain value) const { hiddenapi_domain_ = value; }
@@ -923,8 +914,7 @@ class DexFile {
           uint32_t location_checksum,
           const OatDexFile* oat_dex_file,
           // Shared since several dex files may be stored in the same logical container.
-          std::shared_ptr<DexFileContainer> container,
-          bool is_compact_dex);
+          std::shared_ptr<DexFileContainer> container);
 
   template <typename T>
   const T* GetSection(const uint32_t* offset, DexFileContainer* container);
@@ -1014,9 +1004,6 @@ class DexFile {
   // Manages the underlying memory allocation.
   std::shared_ptr<DexFileContainer> container_;
 
-  // If the dex file is a compact dex file. If false then the dex file is a standard dex file.
-  const bool is_compact_dex_;
-
   // The domain this dex file belongs to for hidden API access checks.
   // It is decleared `mutable` because the domain is assigned after the DexFile
   // has been created and can be changed later by the runtime.
diff --git a/libdexfile/dex/dex_file_loader.cc b/libdexfile/dex/dex_file_loader.cc
index df9c9c11cb..96dd2632f2 100644
--- a/libdexfile/dex/dex_file_loader.cc
+++ b/libdexfile/dex/dex_file_loader.cc
@@ -23,6 +23,7 @@
 
 #include "android-base/stringprintf.h"
 #include "base/bit_utils.h"
+#include "base/casts.h"
 #include "base/file_magic.h"
 #include "base/mem_map.h"
 #include "base/os.h"
@@ -30,7 +31,6 @@
 #include "base/systrace.h"
 #include "base/unix_file/fd_file.h"
 #include "base/zip_archive.h"
-#include "compact_dex_file.h"
 #include "dex_file.h"
 #include "dex_file_verifier.h"
 #include "standard_dex_file.h"
@@ -575,10 +575,6 @@ bool DexFileLoader::OpenFromZipEntry(const ZipArchive& zip_archive,
     if (dex_file == nullptr) {
       return false;
     }
-    if (dex_file->IsCompactDexFile()) {
-      *error_msg = StringPrintf("Can not open compact dex file from zip '%s'", location.c_str());
-      return false;
-    }
     CHECK(dex_file->IsReadOnly()) << multidex_location;
     dex_files->push_back(std::move(dex_file));
     size_t file_size = dex_files->back()->GetHeader().file_size_;
diff --git a/libdexfile/dex/dex_file_verifier.cc b/libdexfile/dex/dex_file_verifier.cc
index 3f2fd627db..ad5b40a74c 100644
--- a/libdexfile/dex/dex_file_verifier.cc
+++ b/libdexfile/dex/dex_file_verifier.cc
@@ -138,7 +138,6 @@ class DexFileVerifier {
                       std::numeric_limits<size_t>::max(),
                       std::numeric_limits<size_t>::max(),
                       std::numeric_limits<size_t>::max()} {
-    CHECK(!dex_file->IsCompactDexFile()) << "Not supported";
   }
 
   bool Verify();
@@ -1727,7 +1726,6 @@ bool DexFileVerifier::CheckIntraClassDataItem() {
 bool DexFileVerifier::CheckIntraCodeItem() {
   const dex::CodeItem* code_item = reinterpret_cast<const dex::CodeItem*>(ptr_);
 
-  DCHECK(dex_file_->IsStandardDexFile());
   if (!CheckListSize(code_item, 1, sizeof(StandardDexFile::CodeItem), "code")) {
     return false;
   }
@@ -2062,10 +2060,10 @@ bool DexFileVerifier::CheckIntraAnnotationItem() {
 
   // Check visibility
   uint8_t visibility = *(ptr_++);
-  switch (visibility) {
-    case DexFile::kDexVisibilityBuild:
-    case DexFile::kDexVisibilityRuntime:
-    case DexFile::kDexVisibilitySystem:
+  switch (static_cast<DexFile::DexVisibility>(visibility)) {
+    case DexFile::DexVisibility::kBuild:
+    case DexFile::DexVisibility::kRuntime:
+    case DexFile::DexVisibility::kSystem:
       break;
     default:
       ErrorStringPrintf("Bad annotation visibility: %x", visibility);
@@ -3212,12 +3210,14 @@ bool DexFileVerifier::CheckInterClassDataItem() {
   uint32_t defining_class = FindFirstClassDataDefiner(accessor);
   DCHECK(IsUint<16>(defining_class) || defining_class == kDexNoIndex) << defining_class;
   if (defining_class == kDexNoIndex) {
-    return true;  // Empty definitions are OK (but useless) and could be shared by multiple classes.
+    // Empty definitions are OK and could be shared by multiple classes.
+    ptr_ = accessor.ptr_pos_;  // Move over the empty `class_data_item`.
+    return true;
   }
   if (!defined_classes_[defining_class]) {
-      // Should really have a class definition for this class data item.
-      ErrorStringPrintf("Could not find declaring class for non-empty class data item.");
-      return false;
+    // Should really have a class definition for this class data item.
+    ErrorStringPrintf("Could not find declaring class for non-empty class data item.");
+    return false;
   }
   const dex::TypeIndex class_type_index(defining_class);
   const dex::ClassDef& class_def = dex_file_->GetClassDef(defined_class_indexes_[defining_class]);
@@ -3488,14 +3488,12 @@ bool DexFileVerifier::CheckInterSection() {
 
   const dex::MapList* map = OffsetToPtr<dex::MapList>(header_->map_off_);
   const dex::MapItem* item = map->list_;
-  uint32_t count = map->size_;
 
   // Cross check the items listed in the map.
-  for (; count != 0u; --count) {
+  for (uint32_t count = map->size_; count != 0u; --count) {
     uint32_t section_offset = item->offset_;
     uint32_t section_count = item->size_;
     DexFile::MapItemType type = static_cast<DexFile::MapItemType>(item->type_);
-    bool found = false;
 
     if (type == DexFile::kDexTypeClassDataItem) {
       FindStringRangesForMethodNames();
@@ -3510,7 +3508,6 @@ bool DexFileVerifier::CheckInterSection() {
       case DexFile::kDexTypeDebugInfoItem:
       case DexFile::kDexTypeAnnotationItem:
       case DexFile::kDexTypeEncodedArrayItem:
-        found = true;
         break;
       case DexFile::kDexTypeStringIdItem:
       case DexFile::kDexTypeTypeIdItem:
@@ -3528,14 +3525,11 @@ bool DexFileVerifier::CheckInterSection() {
         if (!CheckInterSectionIterate(section_offset, section_count, type)) {
           return false;
         }
-        found = true;
         break;
       }
-    }
-
-    if (!found) {
-      ErrorStringPrintf("Unknown map item type %x", item->type_);
-      return false;
+      default:
+        ErrorStringPrintf("Unknown map item type %x", item->type_);
+        return false;
     }
 
     item++;
diff --git a/libdexfile/dex/method_reference.h b/libdexfile/dex/method_reference.h
index 209ab7a994..f9283ea219 100644
--- a/libdexfile/dex/method_reference.h
+++ b/libdexfile/dex/method_reference.h
@@ -75,4 +75,10 @@ struct MethodReferenceValueComparator {
 
 }  // namespace art
 
+namespace std {
+// Defer to DexFileReference's hash.
+template <>
+struct hash<art::MethodReference> : std::hash<art::DexFileReference> {};
+}  // namespace std
+
 #endif  // ART_LIBDEXFILE_DEX_METHOD_REFERENCE_H_
diff --git a/libdexfile/dex/proto_reference.h b/libdexfile/dex/proto_reference.h
index a12091cc46..12067d0b5c 100644
--- a/libdexfile/dex/proto_reference.h
+++ b/libdexfile/dex/proto_reference.h
@@ -88,4 +88,10 @@ struct ProtoReferenceValueComparator {
 
 }  // namespace art
 
+namespace std {
+// Defer to DexFileReference's hash.
+template <>
+struct hash<art::ProtoReference> : std::hash<art::DexFileReference> {};
+}  // namespace std
+
 #endif  // ART_LIBDEXFILE_DEX_PROTO_REFERENCE_H_
diff --git a/libdexfile/dex/standard_dex_file.h b/libdexfile/dex/standard_dex_file.h
index 0b12c186f6..d3e4bae474 100644
--- a/libdexfile/dex/standard_dex_file.h
+++ b/libdexfile/dex/standard_dex_file.h
@@ -125,8 +125,7 @@ class StandardDexFile : public DexFile {
                 location,
                 location_checksum,
                 oat_dex_file,
-                std::move(container),
-                /*is_compact_dex*/ false) {}
+                std::move(container)) {}
 
   friend class DexFileLoader;
   friend class DexFileVerifierTest;
diff --git a/libdexfile/dex/string_reference.h b/libdexfile/dex/string_reference.h
index 92095f4d2f..8b0dd9144e 100644
--- a/libdexfile/dex/string_reference.h
+++ b/libdexfile/dex/string_reference.h
@@ -68,4 +68,10 @@ struct StringReferenceValueComparator {
 
 }  // namespace art
 
+namespace std {
+// Defer to DexFileReference's hash.
+template <>
+struct hash<art::StringReference> : std::hash<art::DexFileReference> {};
+}  // namespace std
+
 #endif  // ART_LIBDEXFILE_DEX_STRING_REFERENCE_H_
diff --git a/libdexfile/dex/type_reference.h b/libdexfile/dex/type_reference.h
index 3207e32510..2ca662439f 100644
--- a/libdexfile/dex/type_reference.h
+++ b/libdexfile/dex/type_reference.h
@@ -52,4 +52,10 @@ struct TypeReferenceValueComparator {
 
 }  // namespace art
 
+namespace std {
+// Defer to DexFileReference's hash.
+template <>
+struct hash<art::TypeReference> : std::hash<art::DexFileReference> {};
+}  // namespace std
+
 #endif  // ART_LIBDEXFILE_DEX_TYPE_REFERENCE_H_
diff --git a/libdexfile/external/dex_file_ext.cc b/libdexfile/external/dex_file_ext.cc
index 3a511ea1c9..ee06a002af 100644
--- a/libdexfile/external/dex_file_ext.cc
+++ b/libdexfile/external/dex_file_ext.cc
@@ -164,19 +164,7 @@ ADexFile_Error ADexFile_create(const void* _Nonnull address,
 
   uint32_t dex_size = header->file_size_;  // Size of "one dex file" excluding any shared data.
   uint32_t full_size = dex_size;           // Includes referenced shared data past the end of dex.
-  if (art::CompactDexFile::IsMagicValid(header->magic_)) {
-    // Compact dex files store the data section separately so that it can be shared.
-    // Therefore we need to extend the read memory range to include it.
-    // TODO: This might be wasteful as we might read data in between as well.
-    //       In practice, this should be fine, as such sharing only happens on disk.
-    uint32_t computed_file_size;
-    if (__builtin_add_overflow(header->data_off_, header->data_size_, &computed_file_size)) {
-      return ADEXFILE_ERROR_INVALID_HEADER;
-    }
-    if (computed_file_size > full_size) {
-      full_size = computed_file_size;
-    }
-  } else if (art::StandardDexFile::IsMagicValid(header->magic_)) {
+  if (art::StandardDexFile::IsMagicValid(header->magic_)) {
     full_size = header->ContainerSize() - header->HeaderOffset();
   } else {
     return ADEXFILE_ERROR_INVALID_HEADER;
@@ -199,7 +187,7 @@ ADexFile_Error ADexFile_create(const void* _Nonnull address,
                                                                 /*verify_checksum=*/false,
                                                                 &error_msg);
   if (dex_file == nullptr) {
-    LOG(ERROR) << "Can not open dex file " << loc_str << ": " << error_msg;
+    LOG(ERROR) << error_msg;
     return ADEXFILE_ERROR_INVALID_DEX;
   }
 
@@ -220,18 +208,6 @@ size_t ADexFile_findMethodAtOffset(ADexFile* self,
     return 0;  // The DEX offset is not within the bytecode of this dex file.
   }
 
-  if (dex_file->IsCompactDexFile()) {
-    // The data section of compact dex files might be shared.
-    // Check the subrange unique to this compact dex.
-    const art::CompactDexFile::Header& cdex_header =
-        dex_file->AsCompactDexFile()->GetHeader();
-    uint32_t begin = cdex_header.data_off_ + cdex_header.OwnedDataBegin();
-    uint32_t end = cdex_header.data_off_ + cdex_header.OwnedDataEnd();
-    if (dex_offset < begin || dex_offset >= end) {
-      return 0;  // The DEX offset is not within the bytecode of this dex file.
-    }
-  }
-
   ADexFile_Method info;
   if (!self->FindMethod(dex_offset, &info)) {
     return 0;
diff --git a/libelffile/dwarf/headers.h b/libelffile/dwarf/headers.h
index 5e1b39b677..24408a286c 100644
--- a/libelffile/dwarf/headers.h
+++ b/libelffile/dwarf/headers.h
@@ -80,7 +80,6 @@ void WriteFDE(bool is64bit,
     writer.PushUint32(code_address);
     writer.PushUint32(code_size);
   }
-  writer.PushUleb128(0);  // Augmentation data size.
   writer.PushData(opcodes.data(), opcodes.size());
   writer.Pad(is64bit ? 8 : 4);
   writer.UpdateUint32(fde_header_start, writer.data()->size() - fde_header_start - 4);
diff --git a/libnativebridge/Android.bp b/libnativebridge/Android.bp
index fc83a4e00b..c8ef4532f6 100644
--- a/libnativebridge/Android.bp
+++ b/libnativebridge/Android.bp
@@ -62,9 +62,6 @@ art_cc_library {
 
     target: {
         android: {
-            header_libs: [
-                "libnativeloader-headers", // For dlext_namespaces.h
-            ],
             shared_libs: ["libdl_android"],
         },
     },
diff --git a/libnativebridge/native_bridge.cc b/libnativebridge/native_bridge.cc
index af85d4ee35..6af4d51d61 100644
--- a/libnativebridge/native_bridge.cc
+++ b/libnativebridge/native_bridge.cc
@@ -22,26 +22,20 @@
 #include <errno.h>
 #include <fcntl.h>
 #include <stdio.h>
-#include <sys/mount.h>
 #include <sys/stat.h>
 #include <unistd.h>
 
 #include <cstring>
 
-#include <android-base/macros.h>
-#include <log/log.h>
+#include "android-base/macros.h"
+#include "log/log.h"
 
 #ifdef ART_TARGET_ANDROID
-#include "nativeloader/dlext_namespaces.h"
+#include <bionic/dlext_namespaces.h>
 #endif
 
 namespace android {
 
-#ifdef __APPLE__
-template <typename T>
-void UNUSED(const T&) {}
-#endif
-
 extern "C" {
 
 void* OpenSystemLibrary(const char* path, int flags) {
@@ -112,7 +106,7 @@ static const char* GetNativeBridgeStateString(NativeBridgeState state) {
 }
 
 // Current state of the native bridge.
-static NativeBridgeState state = NativeBridgeState::kNotSetup;
+static NativeBridgeState g_state = NativeBridgeState::kNotSetup;
 
 // The version of NativeBridge implementation.
 // Different Nativebridge interface needs the service of different version of
@@ -138,18 +132,18 @@ enum NativeBridgeImplementationVersion {
 };
 
 // Whether we had an error at some point.
-static bool had_error = false;
+static bool g_had_error = false;
 
 // Handle of the loaded library.
-static void* native_bridge_handle = nullptr;
+static void* g_native_bridge_handle = nullptr;
 // Pointer to the callbacks. Available as soon as LoadNativeBridge succeeds, but only initialized
 // later.
-static const NativeBridgeCallbacks* callbacks = nullptr;
+static const NativeBridgeCallbacks* g_callbacks = nullptr;
 // Callbacks provided by the environment to the bridge. Passed to LoadNativeBridge.
-static const NativeBridgeRuntimeCallbacks* runtime_callbacks = nullptr;
+static const NativeBridgeRuntimeCallbacks* g_runtime_callbacks = nullptr;
 
 // The app's code cache directory.
-static char* app_code_cache_dir = nullptr;
+static char* g_app_code_cache_dir = nullptr;
 
 // Code cache directory (relative to the application private directory)
 // Ideally we'd like to call into framework to retrieve this name. However that's considered an
@@ -169,9 +163,9 @@ static bool CharacterAllowed(char c, bool first) {
 }
 
 static void ReleaseAppCodeCacheDir() {
-  if (app_code_cache_dir != nullptr) {
-    delete[] app_code_cache_dir;
-    app_code_cache_dir = nullptr;
+  if (g_app_code_cache_dir != nullptr) {
+    delete[] g_app_code_cache_dir;
+    g_app_code_cache_dir = nullptr;
   }
 }
 
@@ -212,21 +206,21 @@ bool NativeBridgeNameAcceptable(const char* nb_library_filename) {
 static bool isCompatibleWith(const uint32_t version) {
   // Libnativebridge is now designed to be forward-compatible. So only "0" is an unsupported
   // version.
-  if (callbacks == nullptr || callbacks->version == 0 || version == 0) {
+  if (g_callbacks == nullptr || g_callbacks->version == 0 || version == 0) {
     return false;
   }
 
   // If this is a v2+ bridge, it may not be forwards- or backwards-compatible. Check.
-  if (callbacks->version >= SIGNAL_VERSION) {
-    return callbacks->isCompatibleWith(version);
+  if (g_callbacks->version >= SIGNAL_VERSION) {
+    return g_callbacks->isCompatibleWith(version);
   }
 
   return true;
 }
 
 static void CloseNativeBridge(bool with_error) {
-  state = NativeBridgeState::kClosed;
-  had_error |= with_error;
+  g_state = NativeBridgeState::kClosed;
+  g_had_error |= with_error;
   ReleaseAppCodeCacheDir();
 }
 
@@ -235,14 +229,14 @@ bool LoadNativeBridge(const char* nb_library_filename,
   // We expect only one place that calls LoadNativeBridge: Runtime::Init. At that point we are not
   // multi-threaded, so we do not need locking here.
 
-  if (state != NativeBridgeState::kNotSetup) {
+  if (g_state != NativeBridgeState::kNotSetup) {
     // Setup has been called before. Ignore this call.
     if (nb_library_filename != nullptr) {  // Avoids some log-spam for dalvikvm.
       ALOGW("Called LoadNativeBridge for an already set up native bridge. State is %s.",
-            GetNativeBridgeStateString(state));
+            GetNativeBridgeStateString(g_state));
     }
     // Note: counts as an error, even though the bridge may be functional.
-    had_error = true;
+    g_had_error = true;
     return false;
   }
 
@@ -260,16 +254,18 @@ bool LoadNativeBridge(const char* nb_library_filename,
       void* handle = OpenSystemLibrary(nb_library_filename, RTLD_LAZY);
 
       if (handle != nullptr) {
-        callbacks = reinterpret_cast<NativeBridgeCallbacks*>(dlsym(handle,
-                                                                   kNativeBridgeInterfaceSymbol));
-        if (callbacks != nullptr) {
+        g_callbacks =
+            reinterpret_cast<NativeBridgeCallbacks*>(dlsym(handle, kNativeBridgeInterfaceSymbol));
+        if (g_callbacks != nullptr) {
           if (isCompatibleWith(NAMESPACE_VERSION)) {
             // Store the handle for later.
-            native_bridge_handle = handle;
+            g_native_bridge_handle = handle;
           } else {
             ALOGW("Unsupported native bridge API in %s (is version %d not compatible with %d)",
-                  nb_library_filename, callbacks->version, NAMESPACE_VERSION);
-            callbacks = nullptr;
+                  nb_library_filename,
+                  g_callbacks->version,
+                  NAMESPACE_VERSION);
+            g_callbacks = nullptr;
             dlclose(handle);
           }
         } else {
@@ -283,14 +279,14 @@ bool LoadNativeBridge(const char* nb_library_filename,
 
       // Two failure conditions: could not find library (dlopen failed), or could not find native
       // bridge interface (dlsym failed). Both are an error and close the native bridge.
-      if (callbacks == nullptr) {
+      if (g_callbacks == nullptr) {
         CloseNativeBridge(true);
       } else {
-        runtime_callbacks = runtime_cbs;
-        state = NativeBridgeState::kOpened;
+        g_runtime_callbacks = runtime_cbs;
+        g_state = NativeBridgeState::kOpened;
       }
     }
-    return state == NativeBridgeState::kOpened;
+    return g_state == NativeBridgeState::kOpened;
   }
 }
 
@@ -302,67 +298,9 @@ bool NeedsNativeBridge(const char* instruction_set) {
   return strncmp(instruction_set, ABI_STRING, strlen(ABI_STRING) + 1) != 0;
 }
 
-#ifndef __APPLE__
-static bool MountCpuinfo(const char* cpuinfo_path) {
-  // If the file does not exist, the mount command will fail,
-  // so we save the extra file existence check.
-  if (TEMP_FAILURE_RETRY(mount(cpuinfo_path,        // Source.
-                               "/proc/cpuinfo",     // Target.
-                               nullptr,             // FS type.
-                               MS_BIND,             // Mount flags: bind mount.
-                               nullptr)) == -1) {   // "Data."
-    ALOGW("Failed to bind-mount %s as /proc/cpuinfo: %s", cpuinfo_path, strerror(errno));
-    return false;
-  }
-  return true;
-}
-#endif
-
-static void MountCpuinfoForInstructionSet(const char* instruction_set) {
-  if (instruction_set == nullptr) {
-    return;
-  }
-
-  size_t isa_len = strlen(instruction_set);
-  if (isa_len > 10) {
-    // 10 is a loose upper bound on the currently known instruction sets (a tight bound is 7 for
-    // x86_64 [including the trailing \0]). This is so we don't have to change here if there will
-    // be another instruction set in the future.
-    ALOGW("Instruction set %s is malformed, must be less than or equal to 10 characters.",
-          instruction_set);
-    return;
-  }
-
-#if defined(__APPLE__)
-  ALOGW("Mac OS does not support bind-mounting. Host simulation of native bridge impossible.");
-
-#elif !defined(__ANDROID__)
-  // To be able to test on the host, we hardwire a relative path.
-  MountCpuinfo("./cpuinfo");
-
-#else  // __ANDROID__
-  char cpuinfo_path[1024];
-
-  // Bind-mount /system/etc/cpuinfo.<isa>.txt to /proc/cpuinfo.
-  snprintf(cpuinfo_path, sizeof(cpuinfo_path), "/system/etc/cpuinfo.%s.txt", instruction_set);
-  if (MountCpuinfo(cpuinfo_path)) {
-    return;
-  }
-
-  // Bind-mount /system/lib{,64}/<isa>/cpuinfo to /proc/cpuinfo.
-  // TODO(b/179753190): remove when all implementations migrate to system/etc!
-#ifdef __LP64__
-  snprintf(cpuinfo_path, sizeof(cpuinfo_path), "/system/lib64/%s/cpuinfo", instruction_set);
-#else
-  snprintf(cpuinfo_path, sizeof(cpuinfo_path), "/system/lib/%s/cpuinfo", instruction_set);
-#endif  // __LP64__
-  MountCpuinfo(cpuinfo_path);
-
-#endif
-}
-
-bool PreInitializeNativeBridge(const char* app_data_dir_in, const char* instruction_set) {
-  if (state != NativeBridgeState::kOpened) {
+bool PreInitializeNativeBridge(const char* app_data_dir_in,
+                               [[maybe_unused]] const char* instruction_set) {
+  if (g_state != NativeBridgeState::kOpened) {
     ALOGE("Invalid state: native bridge is expected to be opened.");
     CloseNativeBridge(true);
     return false;
@@ -372,25 +310,21 @@ bool PreInitializeNativeBridge(const char* app_data_dir_in, const char* instruct
     // Create the path to the application code cache directory.
     // The memory will be release after Initialization or when the native bridge is closed.
     const size_t len = strlen(app_data_dir_in) + strlen(kCodeCacheDir) + 2;  // '\0' + '/'
-    app_code_cache_dir = new char[len];
-    snprintf(app_code_cache_dir, len, "%s/%s", app_data_dir_in, kCodeCacheDir);
+    g_app_code_cache_dir = new char[len];
+    snprintf(g_app_code_cache_dir, len, "%s/%s", app_data_dir_in, kCodeCacheDir);
   } else {
     ALOGW("Application private directory isn't available.");
-    app_code_cache_dir = nullptr;
+    g_app_code_cache_dir = nullptr;
   }
 
-  // Mount cpuinfo that corresponds to the instruction set.
-  // Failure is not fatal.
-  MountCpuinfoForInstructionSet(instruction_set);
-
-  state = NativeBridgeState::kPreInitialized;
+  g_state = NativeBridgeState::kPreInitialized;
   return true;
 }
 
 void PreZygoteForkNativeBridge() {
   if (NativeBridgeInitialized()) {
     if (isCompatibleWith(PRE_ZYGOTE_FORK_VERSION)) {
-      return callbacks->preZygoteFork();
+      return g_callbacks->preZygoteFork();
     } else {
       ALOGE("not compatible with version %d, preZygoteFork() isn't invoked",
             PRE_ZYGOTE_FORK_VERSION);
@@ -398,26 +332,6 @@ void PreZygoteForkNativeBridge() {
   }
 }
 
-static void SetCpuAbi(JNIEnv* env, jclass build_class, const char* field, const char* value) {
-  if (value != nullptr) {
-    jfieldID field_id = env->GetStaticFieldID(build_class, field, "Ljava/lang/String;");
-    if (field_id == nullptr) {
-      env->ExceptionClear();
-      ALOGW("Could not find %s field.", field);
-      return;
-    }
-
-    jstring str = env->NewStringUTF(value);
-    if (str == nullptr) {
-      env->ExceptionClear();
-      ALOGW("Could not create string %s.", value);
-      return;
-    }
-
-    env->SetStaticObjectField(build_class, field_id, str);
-  }
-}
-
 // Set up the environment for the bridged app.
 static void SetupEnvironment(const NativeBridgeCallbacks* cbs, JNIEnv* env, const char* isa) {
   // Need a JNIEnv* to do anything.
@@ -441,20 +355,6 @@ static void SetupEnvironment(const NativeBridgeCallbacks* cbs, JNIEnv* env, cons
     return;
   }
 
-  // Reset CPU_ABI & CPU_ABI2 to values required by the apps running with native bridge.
-  if (env_values->cpu_abi != nullptr || env_values->cpu_abi2 != nullptr ||
-      env_values->abi_count >= 0) {
-    jclass bclass_id = env->FindClass("android/os/Build");
-    if (bclass_id != nullptr) {
-      SetCpuAbi(env, bclass_id, "CPU_ABI", env_values->cpu_abi);
-      SetCpuAbi(env, bclass_id, "CPU_ABI2", env_values->cpu_abi2);
-    } else {
-      // For example in a host test environment.
-      env->ExceptionClear();
-      ALOGW("Could not find Build class.");
-    }
-  }
-
   if (env_values->os_arch != nullptr) {
     jclass sclass_id = env->FindClass("java/lang/System");
     if (sclass_id != nullptr) {
@@ -482,58 +382,56 @@ bool InitializeNativeBridge(JNIEnv* env, const char* instruction_set) {
   // We expect only one place that calls InitializeNativeBridge: Runtime::DidForkFromZygote. At that
   // point we are not multi-threaded, so we do not need locking here.
 
-  if (state == NativeBridgeState::kPreInitialized) {
-    if (app_code_cache_dir != nullptr) {
-      // Check for code cache: if it doesn't exist try to create it.
-      struct stat st;
-      if (stat(app_code_cache_dir, &st) == -1) {
-        if (errno == ENOENT) {
-          if (mkdir(app_code_cache_dir, S_IRWXU | S_IRWXG | S_IXOTH) == -1) {
-            ALOGW("Cannot create code cache directory %s: %s.",
-                  app_code_cache_dir, strerror(errno));
-            ReleaseAppCodeCacheDir();
-          }
-        } else {
-          ALOGW("Cannot stat code cache directory %s: %s.",
-                app_code_cache_dir, strerror(errno));
+  if (g_state != NativeBridgeState::kPreInitialized) {
+    CloseNativeBridge(true);
+    return false;
+  }
+
+  if (g_app_code_cache_dir != nullptr) {
+    // Check for code cache: if it doesn't exist try to create it.
+    struct stat st;
+    if (stat(g_app_code_cache_dir, &st) == -1) {
+      if (errno == ENOENT) {
+        if (mkdir(g_app_code_cache_dir, S_IRWXU | S_IRWXG | S_IXOTH) == -1) {
+          ALOGW(
+              "Cannot create code cache directory %s: %s.", g_app_code_cache_dir, strerror(errno));
           ReleaseAppCodeCacheDir();
         }
-      } else if (!S_ISDIR(st.st_mode)) {
-        ALOGW("Code cache is not a directory %s.", app_code_cache_dir);
+      } else {
+        ALOGW("Cannot stat code cache directory %s: %s.", g_app_code_cache_dir, strerror(errno));
         ReleaseAppCodeCacheDir();
       }
+    } else if (!S_ISDIR(st.st_mode)) {
+      ALOGW("Code cache is not a directory %s.", g_app_code_cache_dir);
+      ReleaseAppCodeCacheDir();
     }
+  }
 
-    // If we're still PreInitialized (didn't fail the code cache checks) try to initialize.
-    if (state == NativeBridgeState::kPreInitialized) {
-      if (callbacks->initialize(runtime_callbacks, app_code_cache_dir, instruction_set)) {
-        SetupEnvironment(callbacks, env, instruction_set);
-        state = NativeBridgeState::kInitialized;
-        // We no longer need the code cache path, release the memory.
-        ReleaseAppCodeCacheDir();
-      } else {
-        // Unload the library.
-        dlclose(native_bridge_handle);
-        CloseNativeBridge(true);
-      }
-    }
+  if (g_callbacks->initialize(g_runtime_callbacks, g_app_code_cache_dir, instruction_set)) {
+    // TODO(b/419835068): SetupEnvironment is likely not needed anymore and can be removed.
+    SetupEnvironment(g_callbacks, env, instruction_set);
+    g_state = NativeBridgeState::kInitialized;
+    // We no longer need the code cache path, release the memory.
+    ReleaseAppCodeCacheDir();
   } else {
+    // Unload the library.
+    dlclose(g_native_bridge_handle);
     CloseNativeBridge(true);
   }
 
-  return state == NativeBridgeState::kInitialized;
+  return g_state == NativeBridgeState::kInitialized;
 }
 
 void UnloadNativeBridge() {
   // We expect only one place that calls UnloadNativeBridge: Runtime::DidForkFromZygote. At that
   // point we are not multi-threaded, so we do not need locking here.
 
-  switch (state) {
+  switch (g_state) {
     case NativeBridgeState::kOpened:
     case NativeBridgeState::kPreInitialized:
     case NativeBridgeState::kInitialized:
       // Unload.
-      dlclose(native_bridge_handle);
+      dlclose(g_native_bridge_handle);
       CloseNativeBridge(false);
       break;
 
@@ -549,24 +447,24 @@ void UnloadNativeBridge() {
 }
 
 bool NativeBridgeError() {
-  return had_error;
+  return g_had_error;
 }
 
 bool NativeBridgeAvailable() {
-  return state == NativeBridgeState::kOpened
-      || state == NativeBridgeState::kPreInitialized
-      || state == NativeBridgeState::kInitialized;
+  return g_state == NativeBridgeState::kOpened ||
+         g_state == NativeBridgeState::kPreInitialized ||
+         g_state == NativeBridgeState::kInitialized;
 }
 
 bool NativeBridgeInitialized() {
   // Calls of this are supposed to happen in a state where the native bridge is stable, i.e., after
   // Runtime::DidForkFromZygote. In that case we do not need a lock.
-  return state == NativeBridgeState::kInitialized;
+  return g_state == NativeBridgeState::kInitialized;
 }
 
 void* NativeBridgeLoadLibrary(const char* libpath, int flag) {
   if (NativeBridgeInitialized()) {
-    return callbacks->loadLibrary(libpath, flag);
+    return g_callbacks->loadLibrary(libpath, flag);
   }
   return nullptr;
 }
@@ -584,15 +482,15 @@ void* NativeBridgeGetTrampoline2(
 
   // For version 1 isCompatibleWith is always true, even though the extensions
   // are not supported, so we need to handle it separately.
-  if (callbacks != nullptr && callbacks->version == DEFAULT_VERSION) {
-    return callbacks->getTrampoline(handle, name, shorty, len);
+  if (g_callbacks != nullptr && g_callbacks->version == DEFAULT_VERSION) {
+    return g_callbacks->getTrampoline(handle, name, shorty, len);
   }
 
   if (isCompatibleWith(CRITICAL_NATIVE_SUPPORT_VERSION)) {
-    return callbacks->getTrampolineWithJNICallType(handle, name, shorty, len, jni_call_type);
+    return g_callbacks->getTrampolineWithJNICallType(handle, name, shorty, len, jni_call_type);
   }
 
-  return callbacks->getTrampoline(handle, name, shorty, len);
+  return g_callbacks->getTrampoline(handle, name, shorty, len);
 }
 
 void* NativeBridgeGetTrampolineForFunctionPointer(const void* method,
@@ -604,7 +502,7 @@ void* NativeBridgeGetTrampolineForFunctionPointer(const void* method,
   }
 
   if (isCompatibleWith(CRITICAL_NATIVE_SUPPORT_VERSION)) {
-    return callbacks->getTrampolineForFunctionPointer(method, shorty, len, jni_call_type);
+    return g_callbacks->getTrampolineForFunctionPointer(method, shorty, len, jni_call_type);
   } else {
     ALOGE("not compatible with version %d, getTrampolineFnPtrWithJNICallType() isn't invoked",
           CRITICAL_NATIVE_SUPPORT_VERSION);
@@ -614,14 +512,14 @@ void* NativeBridgeGetTrampolineForFunctionPointer(const void* method,
 
 bool NativeBridgeIsSupported(const char* libpath) {
   if (NativeBridgeInitialized()) {
-    return callbacks->isSupported(libpath);
+    return g_callbacks->isSupported(libpath);
   }
   return false;
 }
 
 uint32_t NativeBridgeGetVersion() {
   if (NativeBridgeAvailable()) {
-    return callbacks->version;
+    return g_callbacks->version;
   }
   return 0;
 }
@@ -629,7 +527,7 @@ uint32_t NativeBridgeGetVersion() {
 NativeBridgeSignalHandlerFn NativeBridgeGetSignalHandler(int signal) {
   if (NativeBridgeInitialized()) {
     if (isCompatibleWith(SIGNAL_VERSION)) {
-      return callbacks->getSignalHandler(signal);
+      return g_callbacks->getSignalHandler(signal);
     } else {
       ALOGE("not compatible with version %d, cannot get signal handler", SIGNAL_VERSION);
     }
@@ -640,7 +538,7 @@ NativeBridgeSignalHandlerFn NativeBridgeGetSignalHandler(int signal) {
 int NativeBridgeUnloadLibrary(void* handle) {
   if (NativeBridgeInitialized()) {
     if (isCompatibleWith(NAMESPACE_VERSION)) {
-      return callbacks->unloadLibrary(handle);
+      return g_callbacks->unloadLibrary(handle);
     } else {
       ALOGE("not compatible with version %d, cannot unload library", NAMESPACE_VERSION);
     }
@@ -651,7 +549,7 @@ int NativeBridgeUnloadLibrary(void* handle) {
 const char* NativeBridgeGetError() {
   if (NativeBridgeInitialized()) {
     if (isCompatibleWith(NAMESPACE_VERSION)) {
-      return callbacks->getError();
+      return g_callbacks->getError();
     } else {
       return "native bridge implementation is not compatible with version 3, cannot get message";
     }
@@ -662,7 +560,7 @@ const char* NativeBridgeGetError() {
 bool NativeBridgeIsPathSupported(const char* path) {
   if (NativeBridgeInitialized()) {
     if (isCompatibleWith(NAMESPACE_VERSION)) {
-      return callbacks->isPathSupported(path);
+      return g_callbacks->isPathSupported(path);
     } else {
       ALOGE("not compatible with version %d, cannot check via library path", NAMESPACE_VERSION);
     }
@@ -678,12 +576,12 @@ native_bridge_namespace_t* NativeBridgeCreateNamespace(const char* name,
                                                        native_bridge_namespace_t* parent_ns) {
   if (NativeBridgeInitialized()) {
     if (isCompatibleWith(NAMESPACE_VERSION)) {
-      return callbacks->createNamespace(name,
-                                        ld_library_path,
-                                        default_library_path,
-                                        type,
-                                        permitted_when_isolated_path,
-                                        parent_ns);
+      return g_callbacks->createNamespace(name,
+                                          ld_library_path,
+                                          default_library_path,
+                                          type,
+                                          permitted_when_isolated_path,
+                                          parent_ns);
     } else {
       ALOGE("not compatible with version %d, cannot create namespace %s", NAMESPACE_VERSION, name);
     }
@@ -696,7 +594,7 @@ bool NativeBridgeLinkNamespaces(native_bridge_namespace_t* from, native_bridge_n
                                 const char* shared_libs_sonames) {
   if (NativeBridgeInitialized()) {
     if (isCompatibleWith(NAMESPACE_VERSION)) {
-      return callbacks->linkNamespaces(from, to, shared_libs_sonames);
+      return g_callbacks->linkNamespaces(from, to, shared_libs_sonames);
     } else {
       ALOGE("not compatible with version %d, cannot init namespace", NAMESPACE_VERSION);
     }
@@ -711,13 +609,13 @@ native_bridge_namespace_t* NativeBridgeGetExportedNamespace(const char* name) {
   }
 
   if (isCompatibleWith(RUNTIME_NAMESPACE_VERSION)) {
-    return callbacks->getExportedNamespace(name);
+    return g_callbacks->getExportedNamespace(name);
   }
 
-  // sphal is vendor namespace name -> use v4 callback in the case NB callbacks
+  // sphal is vendor namespace name -> use v4 callback in the case NB g_callbacks
   // are not compatible with v5
   if (isCompatibleWith(VENDOR_NAMESPACE_VERSION) && name != nullptr && strcmp("sphal", name) == 0) {
-    return callbacks->getVendorNamespace();
+    return g_callbacks->getVendorNamespace();
   }
 
   return nullptr;
@@ -726,7 +624,7 @@ native_bridge_namespace_t* NativeBridgeGetExportedNamespace(const char* name) {
 void* NativeBridgeLoadLibraryExt(const char* libpath, int flag, native_bridge_namespace_t* ns) {
   if (NativeBridgeInitialized()) {
     if (isCompatibleWith(NAMESPACE_VERSION)) {
-      return callbacks->loadLibraryExt(libpath, flag, ns);
+      return g_callbacks->loadLibraryExt(libpath, flag, ns);
     } else {
       ALOGE("not compatible with version %d, cannot load library in namespace", NAMESPACE_VERSION);
     }
@@ -737,7 +635,7 @@ void* NativeBridgeLoadLibraryExt(const char* libpath, int flag, native_bridge_na
 bool NativeBridgeIsNativeBridgeFunctionPointer(const void* method) {
   if (NativeBridgeInitialized()) {
     if (isCompatibleWith(IDENTIFY_NATIVELY_BRIDGED_FUNCTION_POINTERS_VERSION)) {
-      return callbacks->isNativeBridgeFunctionPointer(method);
+      return g_callbacks->isNativeBridgeFunctionPointer(method);
     } else {
       ALOGW("not compatible with version %d, unable to call isNativeBridgeFunctionPointer",
             IDENTIFY_NATIVELY_BRIDGED_FUNCTION_POINTERS_VERSION);
diff --git a/libnativebridge/tests/Android.bp b/libnativebridge/tests/Android.bp
index add18d12e3..1b4c8568f6 100644
--- a/libnativebridge/tests/Android.bp
+++ b/libnativebridge/tests/Android.bp
@@ -133,7 +133,6 @@ cc_test {
         "NativeBridge2Signal_test.cpp",
         "NativeBridgeVersion_test.cpp",
         "NeedsNativeBridge_test.cpp",
-        "PreInitializeNativeBridge_test.cpp",
         "PreInitializeNativeBridgeFail2_test.cpp",
         "ReSetupNativeBridge_test.cpp",
         "UnavailableNativeBridge_test.cpp",
@@ -190,6 +189,7 @@ cc_test {
 
     test_suites: [
         "general-tests",
+        "mts-art",
     ],
 }
 
diff --git a/libnativebridge/tests/PreInitializeNativeBridge_test.cpp b/libnativebridge/tests/PreInitializeNativeBridge_test.cpp
deleted file mode 100644
index 98ef30f11b..0000000000
--- a/libnativebridge/tests/PreInitializeNativeBridge_test.cpp
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Copyright (C) 2014 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include <dlfcn.h>
-#include <errno.h>
-#include <fcntl.h>
-#include <stdio.h>
-#include <sys/mount.h>
-#include <sys/stat.h>
-
-#include <cstdio>
-#include <cstring>
-
-#include <android/log.h>
-
-#include "NativeBridgeTest.h"
-
-namespace android {
-
-TEST_F(NativeBridgeTest, PreInitializeNativeBridge) {
-    ASSERT_TRUE(LoadNativeBridge(kNativeBridgeLibrary, nullptr));
-#if !defined(__APPLE__)         // Mac OS does not support bind-mount.
-#if !defined(__ANDROID__)       // Cannot write into the hard-wired location.
-    static constexpr const char* kTestData = "PreInitializeNativeBridge test.";
-
-    // Try to create our mount namespace.
-    if (unshare(CLONE_NEWNS) != -1) {
-        // Create a placeholder file.
-        FILE* cpuinfo = fopen("./cpuinfo", "we");
-        ASSERT_NE(nullptr, cpuinfo) << strerror(errno);
-        fprintf(cpuinfo, kTestData);
-        fclose(cpuinfo);
-
-        ASSERT_TRUE(PreInitializeNativeBridge("does not matter 1", "short 2"));
-
-        // Read /proc/cpuinfo
-        FILE* proc_cpuinfo = fopen("/proc/cpuinfo", "re");
-        ASSERT_NE(nullptr, proc_cpuinfo) << strerror(errno);
-        char buf[1024];
-        EXPECT_NE(nullptr, fgets(buf, sizeof(buf), proc_cpuinfo)) << "Error reading.";
-        fclose(proc_cpuinfo);
-
-        EXPECT_EQ(0, strcmp(buf, kTestData));
-
-        // Delete the file.
-        ASSERT_EQ(0, unlink("./cpuinfo")) << "Error unlinking temporary file.";
-        // Ending the test will tear down the mount namespace.
-    } else {
-        GTEST_LOG_(WARNING) << "Could not create mount namespace. Are you running this as root?";
-    }
-#endif
-#endif
-}
-
-}  // namespace android
diff --git a/libnativeloader/include/nativeloader/dlext_namespaces.h b/libnativeloader/include/nativeloader/dlext_namespaces.h
deleted file mode 100644
index 79537ef15f..0000000000
--- a/libnativeloader/include/nativeloader/dlext_namespaces.h
+++ /dev/null
@@ -1,114 +0,0 @@
-/*
- * Copyright (C) 2016 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef ART_LIBNATIVELOADER_INCLUDE_NATIVELOADER_DLEXT_NAMESPACES_H_
-#define ART_LIBNATIVELOADER_INCLUDE_NATIVELOADER_DLEXT_NAMESPACES_H_
-
-#include <android/dlext.h>
-#include <stdbool.h>
-
-__BEGIN_DECLS
-
-enum {
-  /* A regular namespace is the namespace with a custom search path that does
-   * not impose any restrictions on the location of native libraries.
-   */
-  ANDROID_NAMESPACE_TYPE_REGULAR = 0,
-
-  /* An isolated namespace requires all the libraries to be on the search path
-   * or under permitted_when_isolated_path. The search path is the union of
-   * ld_library_path and default_library_path.
-   */
-  ANDROID_NAMESPACE_TYPE_ISOLATED = 1,
-
-  /* The shared namespace clones the list of libraries of the caller namespace upon creation
-   * which means that they are shared between namespaces - the caller namespace and the new one
-   * will use the same copy of a library if it was loaded prior to android_create_namespace call.
-   *
-   * Note that libraries loaded after the namespace is created will not be shared.
-   *
-   * Shared namespaces can be isolated or regular. Note that they do not inherit the search path nor
-   * permitted_path from the caller's namespace.
-   */
-  ANDROID_NAMESPACE_TYPE_SHARED = 2,
-
-  /* This flag instructs linker to enable exempt-list workaround for the namespace.
-   * See http://b/26394120 for details.
-   */
-  ANDROID_NAMESPACE_TYPE_EXEMPT_LIST_ENABLED = 0x08000000,
-
-  /* This flag instructs linker to use this namespace as the anonymous
-   * namespace. The anonymous namespace is used in the case when linker cannot
-   * identify the caller of dlopen/dlsym. This happens for the code not loaded
-   * by dynamic linker; for example calls from the mono-compiled code. There can
-   * be only one anonymous namespace in a process. If there already is an
-   * anonymous namespace in the process, using this flag when creating a new
-   * namespace causes an error.
-   */
-  ANDROID_NAMESPACE_TYPE_ALSO_USED_AS_ANONYMOUS = 0x10000000,
-
-  ANDROID_NAMESPACE_TYPE_SHARED_ISOLATED =
-      ANDROID_NAMESPACE_TYPE_SHARED | ANDROID_NAMESPACE_TYPE_ISOLATED,
-};
-
-/*
- * Creates new linker namespace.
- * ld_library_path and default_library_path represent the search path
- * for the libraries in the namespace.
- *
- * The libraries in the namespace are searched by folowing order:
- * 1. ld_library_path (Think of this as namespace-local LD_LIBRARY_PATH)
- * 2. In directories specified by DT_RUNPATH of the "needed by" binary.
- * 3. deault_library_path (This of this as namespace-local default library path)
- *
- * When type is ANDROID_NAMESPACE_TYPE_ISOLATED the resulting namespace requires all of
- * the libraries to be on the search path or under the permitted_when_isolated_path;
- * the search_path is ld_library_path:default_library_path. Note that the
- * permitted_when_isolated_path path is not part of the search_path and
- * does not affect the search order. It is a way to allow loading libraries from specific
- * locations when using absolute path.
- * If a library or any of its dependencies are outside of the permitted_when_isolated_path
- * and search_path, and it is not part of the public namespace dlopen will fail.
- */
-extern struct android_namespace_t* android_create_namespace(
-    const char* name, const char* ld_library_path, const char* default_library_path, uint64_t type,
-    const char* permitted_when_isolated_path, struct android_namespace_t* parent);
-
-/*
- * Creates a link between namespaces. Every link has list of sonames of
- * shared libraries. These are the libraries which are accessible from
- * namespace 'from' but loaded within namespace 'to' context.
- * When to namespace is nullptr this function establishes a link between
- * 'from' namespace and the default namespace.
- *
- * The lookup order of the libraries in namespaces with links is following:
- * 1. Look inside current namespace using 'this' namespace search path.
- * 2. Look in linked namespaces
- * 2.1. Perform soname check - if library soname is not in the list of shared
- *      libraries sonames skip this link, otherwise
- * 2.2. Search library using linked namespace search path. Note that this
- *      step will not go deeper into linked namespaces for this library but
- *      will do so for DT_NEEDED libraries.
- */
-extern bool android_link_namespaces(struct android_namespace_t* from,
-                                    struct android_namespace_t* to,
-                                    const char* shared_libs_sonames);
-
-extern struct android_namespace_t* android_get_exported_namespace(const char* name);
-
-__END_DECLS
-
-#endif  // ART_LIBNATIVELOADER_INCLUDE_NATIVELOADER_DLEXT_NAMESPACES_H_
diff --git a/libnativeloader/library_namespaces.cpp b/libnativeloader/library_namespaces.cpp
index b27b268d16..4ec00852de 100644
--- a/libnativeloader/library_namespaces.cpp
+++ b/libnativeloader/library_namespaces.cpp
@@ -37,8 +37,8 @@
 #include "android-base/result.h"
 #include "android-base/stringprintf.h"
 #include "android-base/strings.h"
+#include "bionic/dlext_namespaces.h"
 #include "nativehelper/scoped_utf_chars.h"
-#include "nativeloader/dlext_namespaces.h"
 #include "public_libraries.h"
 #include "utils.h"
 
diff --git a/libnativeloader/native_loader.cpp b/libnativeloader/native_loader.cpp
index 964279446f..5a99436bf6 100644
--- a/libnativeloader/native_loader.cpp
+++ b/libnativeloader/native_loader.cpp
@@ -42,9 +42,9 @@
 #ifdef ART_TARGET_ANDROID
 #include "android-modules-utils/sdk_level.h"
 #include "android/api-level.h"
+#include <bionic/dlext_namespaces.h>
 #include "library_namespaces.h"
 #include "log/log.h"
-#include "nativeloader/dlext_namespaces.h"
 #endif
 
 namespace android {
diff --git a/libnativeloader/native_loader_namespace.cpp b/libnativeloader/native_loader_namespace.cpp
index 98236064d0..e19789f961 100644
--- a/libnativeloader/native_loader_namespace.cpp
+++ b/libnativeloader/native_loader_namespace.cpp
@@ -28,7 +28,7 @@
 #include <log/log.h>
 #include <nativebridge/native_bridge.h>
 
-#include "nativeloader/dlext_namespaces.h"
+#include <bionic/dlext_namespaces.h>
 
 using android::base::Error;
 
diff --git a/libnativeloader/native_loader_test.cpp b/libnativeloader/native_loader_test.cpp
index 90810647c3..6abfaf7ecb 100644
--- a/libnativeloader/native_loader_test.cpp
+++ b/libnativeloader/native_loader_test.cpp
@@ -27,13 +27,13 @@
 #include "android-base/result.h"
 #include "android-base/stringprintf.h"
 #include "android-base/strings.h"
+#include "bionic/dlext_namespaces.h"
 #include "dlfcn.h"
 #include "gmock/gmock.h"
 #include "gtest/gtest.h"
 #include "jni.h"
 #include "native_loader_namespace.h"
 #include "nativehelper/scoped_utf_chars.h"
-#include "nativeloader/dlext_namespaces.h"
 #include "nativeloader/native_loader.h"
 #include "public_libraries.h"
 
diff --git a/libprofile/Android.bp b/libprofile/Android.bp
index 91c5cbc9fd..9120e578f6 100644
--- a/libprofile/Android.bp
+++ b/libprofile/Android.bp
@@ -76,9 +76,11 @@ cc_defaults {
 
 cc_defaults {
     name: "libprofile_static_base_defaults",
+    defaults: [
+        "art_libz_static_defaults",
+    ],
     whole_static_libs: [
         "libbase",
-        "libz",
         "libziparchive",
     ],
 }
diff --git a/libprofile/profile/profile_compilation_info.cc b/libprofile/profile/profile_compilation_info.cc
index 160e27e15e..1e6f643396 100644
--- a/libprofile/profile/profile_compilation_info.cc
+++ b/libprofile/profile/profile_compilation_info.cc
@@ -47,7 +47,7 @@
 #include "base/file_utils.h"
 #include "base/globals.h"
 #include "base/logging.h"  // For VLOG.
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "base/os.h"
 #include "base/safe_map.h"
 #include "base/scoped_flock.h"
diff --git a/libprofile/profile/profile_compilation_info.h b/libprofile/profile/profile_compilation_info.h
index 5df8c1b575..39a5a2872c 100644
--- a/libprofile/profile/profile_compilation_info.h
+++ b/libprofile/profile/profile_compilation_info.h
@@ -30,7 +30,7 @@
 #include "base/bit_memory_region.h"
 #include "base/hash_map.h"
 #include "base/hash_set.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "base/mem_map.h"
 #include "base/safe_map.h"
 #include "dex/dex_file-inl.h"
@@ -1074,7 +1074,7 @@ class ProfileCompilationInfo {
   friend class ProfileAssistantTest;
   friend class Dex2oatLayoutTest;
 
-  MallocArenaPool default_arena_pool_;
+  CallocArenaPool default_arena_pool_;
   ArenaAllocator allocator_;
 
   // Vector containing the actual profile info.
diff --git a/libprofile/profile/profile_compilation_info_test.cc b/libprofile/profile/profile_compilation_info_test.cc
index f2df896c70..ca52ee481c 100644
--- a/libprofile/profile/profile_compilation_info_test.cc
+++ b/libprofile/profile/profile_compilation_info_test.cc
@@ -21,7 +21,6 @@
 #include "base/arena_allocator.h"
 #include "base/common_art_test.h"
 #include "base/unix_file/fd_file.h"
-#include "dex/compact_dex_file.h"
 #include "dex/dex_file.h"
 #include "dex/dex_file_loader.h"
 #include "dex/method_reference.h"
@@ -233,7 +232,7 @@ class ProfileCompilationInfoTest : public CommonArtTest, public ProfileTestHelpe
   static constexpr int kProfileMagicSize = 4;
   static constexpr int kProfileVersionSize = 4;
 
-  MallocArenaPool pool_;
+  CallocArenaPool pool_;
   std::unique_ptr<ArenaAllocator> allocator_;
 
   const DexFile* dex1;
diff --git a/oatdump/Android.bp b/oatdump/Android.bp
index 066ad1b17b..01ec25db9f 100644
--- a/oatdump/Android.bp
+++ b/oatdump/Android.bp
@@ -75,6 +75,9 @@ art_cc_binary {
             // Make the host binary static, except for system libraries.
             static_libs: ["liboatdump_static"],
             stl: "c++_static",
+            // libz is a libdex2oat_static transitive dependency from art_libz_static_defaults,
+            // duplicated here because we cannot import *_static_defaults only for host.
+            shared_libs: ["libz"],
         },
     },
 
@@ -126,9 +129,10 @@ art_cc_binary {
             ],
         },
         host: {
-            // Make the host binary static, except for system libraries.
+            // Comments for host in the oatdump binary apply here too.
             static_libs: ["liboatdumpd_static"],
             stl: "c++_static",
+            shared_libs: ["libz"],
         },
     },
 
@@ -145,6 +149,11 @@ cc_defaults {
         "oatdump-defaults",
     ],
     target: {
+        host: {
+            // libz is normally linked dynamically on host in
+            // art_libz_static_defaults, so need to bring it in statically here.
+            static_libs: ["libz"],
+        },
         darwin: {
             enabled: false,
         },
diff --git a/oatdump/oatdump.cc b/oatdump/oatdump.cc
index f7320765a7..ae2d42e84f 100644
--- a/oatdump/oatdump.cc
+++ b/oatdump/oatdump.cc
@@ -637,42 +637,22 @@ class OatDumper {
 
     if (options_.export_dex_location_) {
       std::string error_msg;
-      std::string vdex_filename = GetVdexFilename(oat_file_.GetLocation());
-      if (!OS::FileExists(vdex_filename.c_str())) {
-        os << "File " << vdex_filename.c_str() << " does not exist\n";
-        return false;
-      }
-
-      DexFileUniqV vdex_dex_files;
-      std::unique_ptr<const VdexFile> vdex_file = OpenVdex(vdex_filename,
-                                                           &vdex_dex_files,
-                                                           &error_msg);
-      if (vdex_file.get() == nullptr) {
-        os << "Failed to open vdex file: " << error_msg << "\n";
-        return false;
-      }
-      if (oat_dex_files_.size() != vdex_dex_files.size()) {
-        os << "Dex files number in Vdex file does not match Dex files number in Oat file: "
-           << vdex_dex_files.size() << " vs " << oat_dex_files_.size() << '\n';
-        return false;
-      }
-
-      size_t i = 0;
-      for (const auto& vdex_dex_file : vdex_dex_files) {
-        const OatDexFile* oat_dex_file = oat_dex_files_[i];
+      for (const OatDexFile* oat_dex_file : oat_dex_files_) {
         CHECK(oat_dex_file != nullptr);
-        CHECK(vdex_dex_file != nullptr);
-
-        if (!vdex_dex_file->IsDexContainerFirstEntry()) {
+        const DexFile* dex_file = OpenDexFile(oat_dex_file, &error_msg);
+        if (dex_file == nullptr) {
+          os << "Failed to open dex file '" << oat_dex_file->GetDexFileLocation()
+             << "': " << error_msg;
+          return false;
+        }
+        if (!dex_file->IsDexContainerFirstEntry()) {
           // All the data was already exported together with the primary dex file.
           continue;
         }
-
-        if (!ExportDexFile(os, *oat_dex_file, vdex_dex_file.get(), /*used_dexlayout=*/ false)) {
+        if (!ExportDexFile(os, dex_file)) {
           success = false;
           break;
         }
-        i++;
       }
     }
 
@@ -790,53 +770,6 @@ class OatDumper {
     return nullptr;
   }
 
-  // Returns nullptr and updates error_msg if the Vdex file cannot be opened, otherwise all Dex
-  // files are stored in dex_files.
-  std::unique_ptr<const VdexFile> OpenVdex(const std::string& vdex_filename,
-                                           /* out */ DexFileUniqV* dex_files,
-                                           /* out */ std::string* error_msg) {
-    std::unique_ptr<const File> file(OS::OpenFileForReading(vdex_filename.c_str()));
-    if (file == nullptr) {
-      *error_msg = "Could not open file " + vdex_filename + " for reading.";
-      return nullptr;
-    }
-
-    int64_t vdex_length = file->GetLength();
-    if (vdex_length == -1) {
-      *error_msg = "Could not read the length of file " + vdex_filename;
-      return nullptr;
-    }
-
-    MemMap mmap = MemMap::MapFile(
-        file->GetLength(),
-        PROT_READ | PROT_WRITE,
-        MAP_PRIVATE,
-        file->Fd(),
-        /* start offset= */ 0,
-        /* low_4gb= */ false,
-        vdex_filename.c_str(),
-        error_msg);
-    if (!mmap.IsValid()) {
-      *error_msg = "Failed to mmap file " + vdex_filename + ": " + *error_msg;
-      return nullptr;
-    }
-
-    std::unique_ptr<VdexFile> vdex_file(new VdexFile(std::move(mmap)));
-    if (!vdex_file->IsValid()) {
-      *error_msg = "Vdex file is not valid";
-      return nullptr;
-    }
-
-    DexFileUniqV tmp_dex_files;
-    if (!vdex_file->OpenAllDexFiles(&tmp_dex_files, error_msg)) {
-      *error_msg = "Failed to open Dex files from Vdex: " + *error_msg;
-      return nullptr;
-    }
-
-    *dex_files = std::move(tmp_dex_files);
-    return vdex_file;
-  }
-
   bool AddStatsObject(const void* address) {
     return seen_stats_objects_.insert(address).second;  // Inserted new entry.
   }
@@ -962,20 +895,11 @@ class OatDumper {
     return success;
   }
 
-  // Backwards compatible Dex file export. If dex_file is nullptr (valid Vdex file not present) the
-  // Dex resource is extracted from the oat_dex_file and its checksum is repaired since it's not
-  // unquickened. Otherwise the dex_file has been fully unquickened and is expected to verify the
-  // original checksum.
-  bool ExportDexFile(std::ostream& os,
-                     const OatDexFile& oat_dex_file,
-                     const DexFile* dex_file,
-                     bool used_dexlayout) {
+  // Dex file export.
+  bool ExportDexFile(std::ostream& os, const DexFile* dex_file) {
     std::string error_msg;
-    std::string dex_file_location = oat_dex_file.GetDexFileLocation();
-
-    // If dex_file (from unquicken or dexlayout) is not available, the output DexFile size is the
-    // same as the one extracted from the Oat container (pre-oreo)
-    size_t fsize = dex_file == nullptr ? oat_dex_file.FileSize() : dex_file->Size();
+    std::string dex_file_location = dex_file->GetLocation();
+    size_t fsize = dex_file->GetHeader().ContainerSize();
 
     // Some quick checks just in case
     if (fsize == 0 || fsize < sizeof(DexFile::Header)) {
@@ -983,33 +907,15 @@ class OatDumper {
       return false;
     }
 
-    if (dex_file == nullptr) {
-      // Exported bytecode is quickened (dex-to-dex transformations present)
-      dex_file = OpenDexFile(&oat_dex_file, &error_msg);
-      if (dex_file == nullptr) {
-        os << "Failed to open dex file '" << dex_file_location << "': " << error_msg;
-        return false;
-      }
-
-      // Recompute checksum
-      reinterpret_cast<DexFile::Header*>(const_cast<uint8_t*>(dex_file->Begin()))->checksum_ =
-          dex_file->CalculateChecksum();
-    } else {
-      // If dexlayout was used to convert CompactDex back to StandardDex, checksum will be updated
-      // due to `update_checksum_` option, otherwise we expect a reproducible checksum.
-      if (!used_dexlayout) {
-        // Vdex unquicken output should match original input bytecode
-        uint32_t orig_checksum =
-            reinterpret_cast<DexFile::Header*>(const_cast<uint8_t*>(dex_file->Begin()))->checksum_;
-        if (orig_checksum != dex_file->CalculateChecksum()) {
-          os << "Unexpected checksum from unquicken dex file '" << dex_file_location << "'\n";
-          return false;
-        }
-      }
-      // Extend the data range to export all the dex files in the container.
-      CHECK(dex_file->IsDexContainerFirstEntry()) << dex_file_location;
-      fsize = dex_file->GetHeader().ContainerSize();
+    // We expect a reproducible checksum.
+    uint32_t orig_checksum =
+        reinterpret_cast<DexFile::Header*>(const_cast<uint8_t*>(dex_file->Begin()))->checksum_;
+    if (orig_checksum != dex_file->CalculateChecksum()) {
+      os << "Unexpected checksum from dex file '" << dex_file_location << "'\n";
+      return false;
     }
+    // Extend the data range to export all the dex files in the container.
+    CHECK(dex_file->IsDexContainerFirstEntry()) << dex_file_location;
 
     // Verify output directory exists
     if (!OS::DirectoryExists(options_.export_dex_location_)) {
@@ -1384,7 +1290,7 @@ class OatDumper {
                     const CodeInfo& code_info,
                     const OatFile::OatMethod& oat_method) {
     code_info.Dump(vios,
-                   oat_method.GetCodeOffset(),
+                   AdjustOffset(oat_method.GetCodeOffset()),
                    options_.dump_code_info_stack_maps_,
                    instruction_set_);
   }
@@ -1558,7 +1464,8 @@ class OatDumper {
         if (it != stack_maps.end()) {
           ScopedIndentation indent1(vios);
           for (StackMap stack_map : it->second) {
-            stack_map.Dump(vios, code_info, oat_method.GetCodeOffset(), instruction_set_);
+            stack_map.Dump(
+                vios, code_info, AdjustOffset(oat_method.GetCodeOffset()), instruction_set_);
           }
           stack_maps.erase(it);
         }
@@ -3001,7 +2908,10 @@ class IMTDumper {
       std::string iface_name;
       std::cerr << "  " << iface->GetDescriptor(&iface_name) << std::endl;
 
-      for (ArtMethod& iface_method : iface->GetVirtualMethods(pointer_size)) {
+      for (ArtMethod& iface_method : iface->GetMethods(pointer_size)) {
+        if (!iface_method.IsVirtual()) {
+          continue;
+        }
         uint32_t class_hash, name_hash, signature_hash;
         ImTable::GetImtHashComponents(*iface_method.GetDexFile(),
                                       iface_method.GetDexMethodIndex(),
diff --git a/openjdkjvmti/ti_allocator.cc b/openjdkjvmti/ti_allocator.cc
index 79d2af9d5d..06aac4b5f4 100644
--- a/openjdkjvmti/ti_allocator.cc
+++ b/openjdkjvmti/ti_allocator.cc
@@ -29,6 +29,8 @@
  * questions.
  */
 
+#define __BIONIC_DISABLE_MALLOC_USABLE_SIZE_FORTIFY_WARNINGS
+
 #include "ti_allocator.h"
 
 #if defined(__APPLE__)
diff --git a/openjdkjvmti/ti_class_definition.cc b/openjdkjvmti/ti_class_definition.cc
index 72fe7c7193..29a94f18b4 100644
--- a/openjdkjvmti/ti_class_definition.cc
+++ b/openjdkjvmti/ti_class_definition.cc
@@ -171,41 +171,10 @@ jvmtiError ArtClassDefinition::InitFirstLoad(const char* descriptor,
 }
 
 jvmtiError ArtClassDefinition::Init(const art::DexFile& dex_file) {
-  if (dex_file.IsCompactDexFile()) {
-    std::string error_msg;
-    std::vector<std::unique_ptr<const art::DexFile>> dex_files;
-    art::ArtDexFileLoader dex_file_loader(dex_file.GetLocation());
-    if (!dex_file_loader.Open(/* verify= */ false,
-                              /* verify_checksum= */ false,
-                              &error_msg,
-                              &dex_files)) {
-      return ERR(INTERNAL);
-    }
-    const std::vector<const art::OatDexFile*>& oat_dex_files =
-        dex_file.GetOatDexFile()->GetOatFile()->GetOatDexFiles();
-    const art::DexFile* original_dex_file = nullptr;
-    for (uint32_t i = 0; i < oat_dex_files.size(); ++i) {
-      if (dex_file.GetOatDexFile() == oat_dex_files[i]) {
-        original_dex_file = dex_files[i].get();
-        break;
-      }
-    }
-    // Keep the dex_data alive.
-    dex_data_memory_.resize(original_dex_file->SizeIncludingSharedData());
-    memcpy(dex_data_memory_.data(), original_dex_file->Begin(), dex_data_memory_.size());
-    dex_data_ = art::ArrayRef<const unsigned char>(dex_data_memory_);
-
-    // In case dex_data gets re-used for redefinition, keep the dex file live
-    // with current_dex_memory.
-    current_dex_memory_.resize(dex_data_.size());
-    memcpy(current_dex_memory_.data(), dex_data_.data(), current_dex_memory_.size());
-    current_dex_file_ = art::ArrayRef<const unsigned char>(current_dex_memory_);
-  } else {
-    // Dex file will always stay live, use it directly.
-    dex_data_ =
-        art::ArrayRef<const unsigned char>(dex_file.Begin(), dex_file.SizeIncludingSharedData());
-    current_dex_file_ = dex_data_;
-  }
+  // Dex file will always stay live, use it directly.
+  dex_data_ =
+      art::ArrayRef<const unsigned char>(dex_file.Begin(), dex_file.SizeIncludingSharedData());
+  current_dex_file_ = dex_data_;
   return OK;
 }
 
diff --git a/openjdkjvmti/ti_redefine.cc b/openjdkjvmti/ti_redefine.cc
index 8e7885a726..a82890c514 100644
--- a/openjdkjvmti/ti_redefine.cc
+++ b/openjdkjvmti/ti_redefine.cc
@@ -2850,12 +2850,13 @@ void Redefiner::ClassRedefinition::UpdateClassStructurally(const RedefinitionDat
           replacement_classes_iter.begin(),
           replacement_classes_iter.end(),
           [&](art::ObjPtr<art::mirror::Class> cand) REQUIRES(art::Locks::mutator_lock_) {
-            auto direct_methods = cand->GetDirectMethods(art::kRuntimePointerSize);
-            return std::find_if(direct_methods.begin(),
-                                direct_methods.end(),
-                                [&](art::ArtMethod& m) REQUIRES(art::Locks::mutator_lock_) {
-                                  return UNLIKELY(m.HasSameNameAndSignature(field_or_method));
-                                }) != direct_methods.end();
+            auto methods = cand->GetMethods(art::kRuntimePointerSize);
+            return std::any_of(methods.begin(),
+                               methods.end(),
+                               [&](art::ArtMethod& m) REQUIRES(art::Locks::mutator_lock_) {
+                                 return !m.IsVirtual() &&
+                                        UNLIKELY(m.HasSameNameAndSignature(field_or_method));
+                               });
           });
     } else {
       auto pred = [&](art::ArtField& f) REQUIRES(art::Locks::mutator_lock_) {
diff --git a/openjdkjvmti/ti_thread.cc b/openjdkjvmti/ti_thread.cc
index f89a9d9a14..e8fc7cde77 100644
--- a/openjdkjvmti/ti_thread.cc
+++ b/openjdkjvmti/ti_thread.cc
@@ -340,9 +340,9 @@ jvmtiError ThreadUtil::GetThreadInfo(jvmtiEnv* env, jthread thread, jvmtiThreadI
 
     // Priority.
     {
-      art::ArtField* f = art::WellKnownClasses::java_lang_Thread_priority;
+      art::ArtField* f = art::WellKnownClasses::java_lang_Thread_niceness;
       CHECK(f != nullptr);
-      info_ptr->priority = static_cast<jint>(f->GetInt(peer));
+      info_ptr->priority = static_cast<jint>(art::Thread::NicenessToPriority(f->GetInt(peer)));
     }
 
     // Daemon.
diff --git a/perfetto_hprof/perfetto_hprof.cc b/perfetto_hprof/perfetto_hprof.cc
index 0602437383..211aeec7de 100644
--- a/perfetto_hprof/perfetto_hprof.cc
+++ b/perfetto_hprof/perfetto_hprof.cc
@@ -91,7 +91,7 @@ static art::ConditionVariable& GetStateCV() {
 static int requested_tracing_session_id = 0;
 static State g_state = State::kUninitialized;
 static bool g_oome_triggered = false;
-static uint32_t g_oome_sessions_pending = 0;
+static uint32_t g_oome_sessions_started = 0;
 
 // Pipe to signal from the signal handler into a worker thread that handles the
 // dump requests.
@@ -187,10 +187,11 @@ class JavaHprofDataSource : public perfetto::DataSource<JavaHprofDataSource> {
   constexpr static perfetto::BufferExhaustedPolicy kBufferExhaustedPolicy =
     perfetto::BufferExhaustedPolicy::kStall;
 
-  explicit JavaHprofDataSource(bool is_oome_heap) : is_oome_heap_(is_oome_heap) {}
+  explicit JavaHprofDataSource(uint32_t oome_sessions_pending)
+      : oome_sessions_pending_(oome_sessions_pending) {}
 
   void OnSetup(const SetupArgs& args) override {
-    if (!is_oome_heap_) {
+    if (!IsOome()) {
       uint64_t normalized_tracing_session_id =
         args.config->tracing_session_id() % std::numeric_limits<int32_t>::max();
       if (requested_tracing_session_id < 0) {
@@ -214,8 +215,7 @@ class JavaHprofDataSource : public perfetto::DataSource<JavaHprofDataSource> {
     }
     // This tracing session ID matches the requesting tracing session ID, so we know heapprofd
     // has verified it targets this process.
-    enabled_ =
-        !is_oome_heap_ || (IsOomeHeapDumpAllowed(*args.config) && IsOomeDumpEnabled(*cfg.get()));
+    enabled_ = !IsOome() || (IsOomeHeapDumpAllowed(*args.config) && IsOomeDumpEnabled(*cfg.get()));
   }
 
   bool dump_smaps() { return dump_smaps_; }
@@ -225,17 +225,17 @@ class JavaHprofDataSource : public perfetto::DataSource<JavaHprofDataSource> {
 
   void OnStart(const StartArgs&) override {
     art::MutexLock lk(art_thread(), GetStateMutex());
-    // In case there are multiple tracing sessions waiting for an OOME error,
-    // there will be a data source instance for each of them. Before the
-    // transition to kStart and signaling the dumping thread, we need to make
-    // sure all the data sources are ready.
-    if (is_oome_heap_ && g_oome_sessions_pending > 0) {
-      --g_oome_sessions_pending;
+    if (IsOome()) {
+      // In case there are multiple tracing sessions waiting for an OOME error,
+      // there will be a data source instance for each of them. Before the
+      // transition to kStart and signaling the dumping thread, we need to make
+      // sure all the data sources are ready.
+      ++g_oome_sessions_started;
     }
     if (g_state == State::kWaitForStart) {
-      // WriteHeapPackets is responsible for checking whether the DataSource is\
+      // WriteHeapPackets is responsible for checking whether the DataSource is
       // actually enabled.
-      if (!is_oome_heap_ || g_oome_sessions_pending == 0) {
+      if (!IsOome() || g_oome_sessions_started == oome_sessions_pending_) {
         g_state = State::kStart;
         GetStateCV().Broadcast(art_thread());
       }
@@ -279,6 +279,8 @@ class JavaHprofDataSource : public perfetto::DataSource<JavaHprofDataSource> {
   }
 
  private:
+  bool IsOome() const { return oome_sessions_pending_ > 0; }
+
   static bool IsOomeDumpEnabled(const perfetto::protos::pbzero::JavaHprofConfig::Decoder& cfg) {
     std::string cmdline;
     if (!android::base::ReadFileToString("/proc/self/cmdline", &cmdline)) {
@@ -295,7 +297,7 @@ class JavaHprofDataSource : public perfetto::DataSource<JavaHprofDataSource> {
     return false;
   }
 
-  bool is_oome_heap_ = false;
+  uint32_t oome_sessions_pending_ = 0;
   bool enabled_ = false;
   bool dump_smaps_ = false;
   std::vector<std::string> ignored_types_;
@@ -306,7 +308,7 @@ class JavaHprofDataSource : public perfetto::DataSource<JavaHprofDataSource> {
   std::function<void()> async_stop_;
 };
 
-void SetupDataSource(const std::string& ds_name, bool is_oome_heap) {
+void SetupDataSource(const std::string& ds_name, uint32_t oome_sessions_pending) {
   perfetto::TracingInitArgs args;
   args.backends = perfetto::BackendType::kSystemBackend;
   perfetto::Tracing::Initialize(args);
@@ -314,7 +316,7 @@ void SetupDataSource(const std::string& ds_name, bool is_oome_heap) {
   perfetto::DataSourceDescriptor dsd;
   dsd.set_name(ds_name);
   dsd.set_will_notify_on_stop(true);
-  JavaHprofDataSource::Register(dsd, is_oome_heap);
+  JavaHprofDataSource::Register(dsd, oome_sessions_pending);
 }
 
 // Waits for the data source OnStart
@@ -501,6 +503,7 @@ perfetto::protos::pbzero::HeapGraphRoot::Type ToProtoType(art::RootType art_type
 
 perfetto::protos::pbzero::HeapGraphType::Kind ProtoClassKind(uint32_t class_flags) {
   using perfetto::protos::pbzero::HeapGraphType;
+  class_flags &= ~art::mirror::kClassFlagStaticRefInfo;
   switch (class_flags) {
     case art::mirror::kClassFlagNormal:
     case art::mirror::kClassFlagRecord:
@@ -591,7 +594,7 @@ std::vector<std::pair<std::string, art::mirror::Object*>> GetReferences(art::mir
   std::vector<std::pair<std::string, art::mirror::Object*>> referred_objects;
   ReferredObjectsFinder objf(&referred_objects, emit_field_ids);
 
-  uint32_t klass_flags = klass->GetClassFlags();
+  uint32_t klass_flags = klass->GetClassFlags() & ~art::mirror::kClassFlagStaticRefInfo;
   if (klass_flags != art::mirror::kClassFlagNormal &&
       klass_flags != art::mirror::kClassFlagSoftReference &&
       klass_flags != art::mirror::kClassFlagWeakReference &&
@@ -824,7 +827,7 @@ class HeapGraphDumper {
                       art::mirror::Class* klass,
                       perfetto::protos::pbzero::HeapGraphObject* object_proto)
       REQUIRES_SHARED(art::Locks::mutator_lock_) {
-    const uint32_t klass_flags = klass->GetClassFlags();
+    const uint32_t klass_flags = klass->GetClassFlags() & ~art::mirror::kClassFlagStaticRefInfo;
     const bool emit_field_ids = klass_flags != art::mirror::kClassFlagObjectArray &&
                                 klass_flags != art::mirror::kClassFlagNormal &&
                                 klass_flags != art::mirror::kClassFlagSoftReference &&
@@ -1118,7 +1121,7 @@ void DumpPerfetto(art::Thread* self) {
       // Make sure that this is the first thing we do after forking, so if anything
       // below hangs, the fork will go away from the watchdog.
       ArmWatchdogOrDie();
-      SetupDataSource("android.java_hprof", false);
+      SetupDataSource("android.java_hprof", /* oome_sessions_pending= */ 0);
       WaitForDataSource(self);
       WriteHeapPackets(dumped_pid, timestamp);
       LOG(INFO) << "finished dumping heap for " << dumped_pid;
@@ -1146,7 +1149,6 @@ void DumpPerfettoOutOfMemory() REQUIRES_SHARED(art::Locks::mutator_lock_) {
       return;
     }
     g_oome_triggered = true;
-    g_oome_sessions_pending = session_cnt;
   }
 
   art::ScopedThreadSuspension sts(self, art::ThreadState::kSuspended);
@@ -1172,21 +1174,25 @@ void DumpPerfettoOutOfMemory() REQUIRES_SHARED(art::Locks::mutator_lock_) {
       BusyWaitpid(child, kWatchdogTimeoutSec * 1000);
     },
     // child process
-    [self](pid_t dumped_pid, uint64_t timestamp) {
+    [self, session_cnt](pid_t dumped_pid, uint64_t timestamp) {
       ArmWatchdogOrDie();
       art::SetThreadName("perfetto_oome_hprof");
       art::ScopedTrace trace("perfetto_hprof oome");
-      SetupDataSource("android.java_hprof.oom", true);
-      perfetto::Tracing::ActivateTriggers({"com.android.telemetry.art-outofmemory"}, 500);
-
-      // A pre-armed tracing session might not exist, so we should wait for a
-      // limited amount of time before we decide to let the execution continue.
-      if (!TimedWaitForDataSource(self, 1000)) {
-        LOG(INFO) << "OOME hprof timeout (state " << g_state << ")";
-        return;
+      SetupDataSource("android.java_hprof.oom", session_cnt);
+      perfetto::Tracing::ActivateTriggers({"com.android.telemetry.art-outofmemory"}, 1000);
+
+      // We know that there are > 0 tracing sessions waiting for the oome data source.
+      // However they could be configured to filter the trigger (e.g. based on the producer regex)
+      // We should wait for a limited amount of time and still flush to established sessions.
+      TimedWaitForDataSource(self, 1000);
+      if (g_oome_sessions_started > 0) {
+        WriteHeapPackets(dumped_pid, timestamp);
+        LOG(INFO) << "OOME hprof complete for " << dumped_pid << ", written to "
+                  << g_oome_sessions_started << "/" << session_cnt << " sessions";
+      } else {
+        LOG(INFO) << "OOME hprof ds setup timeout for " << dumped_pid << "(g_state: "
+          << g_state << ", g_oome_sessions_started: " << g_oome_sessions_started << ")";
       }
-      WriteHeapPackets(dumped_pid, timestamp);
-      LOG(INFO) << "OOME hprof complete for " << dumped_pid;
     });
 }
 
diff --git a/profman/Android.bp b/profman/Android.bp
index b3dcb6d490..77184b93f4 100644
--- a/profman/Android.bp
+++ b/profman/Android.bp
@@ -106,7 +106,9 @@ art_cc_binary {
             // avoids having to bundle host dynamic libs in prebuilts.
             static_libs: ["libprofman_static"],
             stl: "c++_static",
-
+            // libz is a libprofman_static transitive dependency from art_libz_static_defaults,
+            // duplicated here because we cannot import *_static_defaults only for host.
+            shared_libs: ["libz"],
         },
     },
     apex_available: [
@@ -131,11 +133,10 @@ art_cc_binary {
             ],
         },
         host: {
-            // Make the host binary static, except for system libraries. This
-            // avoids having to bundle host dynamic libs in prebuilts.
+            // Comments for host in the profman binary apply here too.
             static_libs: ["libprofmand_static"],
             stl: "c++_static",
-
+            shared_libs: ["libz"],
         },
     },
     apex_available: [
diff --git a/profman/profile_assistant_test.cc b/profman/profile_assistant_test.cc
index 15fe49db81..8ed95895fb 100644
--- a/profman/profile_assistant_test.cc
+++ b/profman/profile_assistant_test.cc
@@ -296,8 +296,8 @@ class ProfileAssistantTest : public CommonRuntimeTest, public ProfileTestHelper
     ClassLinker* class_linker = Runtime::Current()->GetClassLinker();
     const auto pointer_size = class_linker->GetImagePointerSize();
     ArtMethod* method = nullptr;
-    for (auto& m : klass->GetVirtualMethods(pointer_size)) {
-      if (name == m.GetName()) {
+    for (auto& m : klass->GetMethods(pointer_size)) {
+      if (m.IsVirtual() && name == m.GetName()) {
         EXPECT_TRUE(method == nullptr);
         method = &m;
       }
diff --git a/runtime/Android.bp b/runtime/Android.bp
index c0a65f27a7..edd63e56f7 100644
--- a/runtime/Android.bp
+++ b/runtime/Android.bp
@@ -109,9 +109,6 @@ cc_defaults {
     host_supported: true,
     target: {
         android: {
-            header_libs: [
-                "libnativeloader-headers", // For dlext_namespaces.h
-            ],
             shared_libs: [
                 "libdl_android",
                 "libstatspull", // for pulled atoms
@@ -656,25 +653,25 @@ cc_defaults {
 // where all non-NDK dependencies are static as well.
 cc_defaults {
     name: "art_libunwindstack_static_defaults",
+    defaults: [
+        "art_liblog_static_defaults",
+    ],
     whole_static_libs: [
         "libunwindstack",
         "libbase",
         "liblzma",
     ],
-    shared_libs: [
-        "liblog",
-    ],
 }
 
 cc_defaults {
     name: "libart_static_base_defaults",
     defaults: [
+        "art_liblog_static_defaults",
         "art_libunwindstack_static_defaults",
     ],
     whole_static_libs: [
         "libartpalette",
         "libbase",
-        "liblog",
         "liblz4",
         "liblzma", // libelffile dependency; must be repeated here since it's a static lib.
         "libnativebridge",
@@ -745,8 +742,7 @@ cc_defaults {
 }
 
 // libart-runtime_static_defaults for standalone gtests.
-// Doesn't link libsigchain_fake/libnativeloader (see art_gtest_common_defaults
-// in test/Android.bp for explanation).
+// Doesn't link libsigchain_fake because tests get libsigchain from art_gtest_common_defaults.
 // Uses libart-runtime-for-test instead of libart-runtime.
 cc_defaults {
     name: "libart-runtime-for-test_static_defaults",
@@ -763,9 +759,17 @@ cc_defaults {
     ],
 }
 
+// libart_static_defaults for standalone gtests.
+cc_defaults {
+    name: "libart-for-test_static_defaults",
+    defaults: [
+        "libart-runtime-for-test_static_defaults",
+        "libart-compiler-for-test_static_defaults",
+    ],
+}
+
 // libartd-runtime_static_defaults for gtests.
-// Doesn't link libsigchain_fake/libnativeloader (see art_gtest_common_defaults
-// in test/Android.bp for explanation).
+// Doesn't link libsigchain_fake because tests get libsigchain from art_gtest_common_defaults.
 // Note that `libartd-runtime-for-test` is not required here, because `libartd-runtime`
 // doesn't use LTO.
 cc_defaults {
@@ -783,6 +787,17 @@ cc_defaults {
     ],
 }
 
+// libartd_static_defaults for gtests.
+cc_defaults {
+    name: "libartd-for-test_static_defaults",
+    defaults: [
+        "libartd-runtime-for-test_static_defaults",
+        // Note that something like `libartd-compiler-for-test_static_defaults`
+        // is not required here, because `libartd-compiler` doesn't use LTO.
+        "libartd-compiler_static_defaults",
+    ],
+}
+
 gensrcs {
     name: "art_operator_srcs",
     cmd: "$(location generate_operator_out) art/runtime $(in) > $(out)",
@@ -952,8 +967,9 @@ art_cc_library {
     ],
 }
 
-art_cc_defaults {
+cc_defaults {
     name: "libart-runtime-gtest-defaults",
+    defaults: ["libart-gtest-defaults"],
     target: {
         host: {
             cflags: [
@@ -969,30 +985,58 @@ art_cc_defaults {
         "common_runtime_test.cc",
         "dexopt_test.cc",
     ],
-    static_libs: [
+    shared_libs: [ // Since we're building static libs we're only using the headers from these.
+        "libdexfile",
         "libprocinfo",
+        "libprofile",
+        "libziparchive",
     ],
     header_libs: [
+        "libart_headers",
         "libnativehelper_header_only",
     ],
 }
 
-art_cc_library_static {
+cc_library_static {
     name: "libart-runtime-gtest",
     defaults: [
         "libart-runtime-gtest-defaults",
-        "libart-gtest-defaults",
+    ],
+}
+
+cc_defaults {
+    name: "libart-runtime-gtest_static_defaults",
+    defaults: [
         "libart-runtime-for-test_static_defaults",
+        "libdexfile_static_defaults",
+        "libprofile_static_defaults",
+    ],
+    whole_static_libs: [
+        "libart-runtime-gtest",
+        "libprocinfo",
+        "libziparchive",
     ],
 }
 
-art_cc_library_static {
+cc_library_static {
     name: "libartd-runtime-gtest",
     defaults: [
         "art_debug_defaults",
         "libart-runtime-gtest-defaults",
-        "libart-gtest-defaults",
+    ],
+}
+
+cc_defaults {
+    name: "libartd-runtime-gtest_static_defaults",
+    defaults: [
         "libartd-runtime-for-test_static_defaults",
+        "libdexfiled_static_defaults",
+        "libprofiled_static_defaults",
+    ],
+    whole_static_libs: [
+        "libartd-runtime-gtest",
+        "libprocinfo",
+        "libziparchive",
     ],
 }
 
diff --git a/runtime/arch/arm/quick_entrypoints_arm.S b/runtime/arch/arm/quick_entrypoints_arm.S
index 150780b921..58c7a4fde9 100644
--- a/runtime/arch/arm/quick_entrypoints_arm.S
+++ b/runtime/arch/arm/quick_entrypoints_arm.S
@@ -554,6 +554,11 @@ END art_quick_osr_stub
      * The r12 (IP) shall be clobbered rather than retrieved from gprs_.
      */
 ARM_ENTRY art_quick_do_long_jump
+    // We don't use the LR, but it is needed for unwinding.
+    str lr, [sp, #-8]!
+    .cfi_adjust_cfa_offset 8
+    .cfi_rel_offset lr, 0
+
     // Reserve space for the gprs + fprs;
     INCREASE_FRAME ARM_LONG_JUMP_CONTEXT_SIZE
 
@@ -572,6 +577,7 @@ ARM_ENTRY art_quick_do_long_jump
     ldr  sp, [sp, #52]    @ Load SP from gprs_ 52 = 4 * 13.
                           @ Do not access gprs_ from now, they are below SP.
     .cfi_def_cfa_offset 0
+    .cfi_register lr, r12
     REFRESH_MARKING_REGISTER
     bx   r12              @ Do long jump.
 END art_quick_do_long_jump
diff --git a/runtime/arch/arm64/fault_handler_arm64.cc b/runtime/arch/arm64/fault_handler_arm64.cc
index a09872a4bb..681a616f36 100644
--- a/runtime/arch/arm64/fault_handler_arm64.cc
+++ b/runtime/arch/arm64/fault_handler_arm64.cc
@@ -93,13 +93,14 @@ bool NullPointerHandler::Action([[maybe_unused]] int sig, siginfo_t* info, void*
   return true;
 }
 
+static constexpr uint32_t kSuspendCheckRegister = 21;
+
 // A suspend check is done using the following instruction:
 //      0x...: f94002b5  ldr x21, [x21, #0]
 // To check for a suspend check, we examine the instruction that caused the fault (at PC).
 bool SuspensionHandler::Action([[maybe_unused]] int sig,
                                [[maybe_unused]] siginfo_t* info,
                                void* context) {
-  constexpr uint32_t kSuspendCheckRegister = 21;
   constexpr uint32_t checkinst =
       0xf9400000 | (kSuspendCheckRegister << 5) | (kSuspendCheckRegister << 0);
 
@@ -129,6 +130,22 @@ bool SuspensionHandler::Action([[maybe_unused]] int sig,
   return true;
 }
 
+void FaultManager::SuspendFaster(siginfo_t* info, void* context) {
+  // We need to trigger a suspend check. Since we are in an asynchronous signal handler, we
+  // cannot do so directly. But we can make sure the next suspend check does so, rather than
+  // waiting for the one after that, by clearing kSuspendCheckRegister. This is likely to be
+  // beneficial immediately after a flip, when we are likely to see several signals in a row.
+
+  if (!IsInGeneratedCode(info, context)) {
+    // Suspend trigger register is reserved for the purpose only in ART-generated code.
+    return;
+  }
+  ucontext_t* uc = reinterpret_cast<ucontext_t*>(context);
+  mcontext_t* mc = reinterpret_cast<mcontext_t*>(&uc->uc_mcontext);
+  mc->regs[kSuspendCheckRegister] = reinterpret_cast<uintptr_t>(static_cast<void*>(nullptr));
+  VLOG(signals) << "Accelerating suspension";
+}
+
 bool StackOverflowHandler::Action([[maybe_unused]] int sig,
                                   [[maybe_unused]] siginfo_t* info,
                                   void* context) {
diff --git a/runtime/arch/arm64/quick_entrypoints_arm64.S b/runtime/arch/arm64/quick_entrypoints_arm64.S
index 98b7e969fc..88f8eff66c 100644
--- a/runtime/arch/arm64/quick_entrypoints_arm64.S
+++ b/runtime/arch/arm64/quick_entrypoints_arm64.S
@@ -795,6 +795,9 @@ END art_quick_osr_stub
      */
 
 ENTRY art_quick_do_long_jump
+    // We don't use the LR, but it is needed for unwinding.
+    SAVE_REG_INCREASE_FRAME xLR, 16
+
     // Reserve space for the gprs + fprs;
     INCREASE_FRAME ARM64_LONG_JUMP_CONTEXT_SIZE
 
@@ -847,6 +850,7 @@ ENTRY art_quick_do_long_jump
     // Set SP. Do not access fprs_ and gprs_ from now, they are below SP.
     mov sp, xIP0
     .cfi_def_cfa_offset 0
+    .cfi_register xLR, xIP1
 
     REFRESH_MARKING_REGISTER
     REFRESH_SUSPEND_CHECK_REGISTER
diff --git a/runtime/arch/riscv64/quick_entrypoints_riscv64.S b/runtime/arch/riscv64/quick_entrypoints_riscv64.S
index e97c786b91..fc5ab6d0e5 100644
--- a/runtime/arch/riscv64/quick_entrypoints_riscv64.S
+++ b/runtime/arch/riscv64/quick_entrypoints_riscv64.S
@@ -453,6 +453,10 @@ END art_quick_method_exit_hook
 // On entry a0 is the long jump context. This is expected to be returned from a previous entrypoint
 // call which threw an exception or deoptimized.
 ENTRY art_quick_do_long_jump
+    // We don't use the RA, but it is needed for unwinding.
+    INCREASE_FRAME 16
+    SAVE_GPR ra, 0*8
+
     // Reserve space for the gprs + fprs;
     INCREASE_FRAME RISCV64_LONG_JUMP_CONTEXT_SIZE
 
@@ -540,6 +544,7 @@ ENTRY art_quick_do_long_jump
     // Set sp. Do not access fprs_ and gprs_ from now, they are below sp.
     mv sp, t0
     .cfi_def_cfa_offset 0
+    .cfi_register ra, t1
 
     jr  t1
 END art_quick_do_long_jump
diff --git a/runtime/arch/stub_test.cc b/runtime/arch/stub_test.cc
index 1dd8c83499..69f65c0455 100644
--- a/runtime/arch/stub_test.cc
+++ b/runtime/arch/stub_test.cc
@@ -1560,7 +1560,7 @@ static void TestFields(Thread* self, StubTest* test, Primitive::Type test_type)
   Handle<mirror::Object> obj(hs.NewHandle(soa.Decode<mirror::Object>(o)));
   Handle<mirror::Class> c(hs.NewHandle(obj->GetClass()));
   // Need a method as a referrer
-  ArtMethod* m = c->GetDirectMethod(0, kRuntimePointerSize);
+  ArtMethod* m = &c->GetMethods(kRuntimePointerSize)[0];
 
   // Play with it...
 
diff --git a/runtime/arch/x86/fault_handler_x86.cc b/runtime/arch/x86/fault_handler_x86.cc
index a392b54839..0a415d5e06 100644
--- a/runtime/arch/x86/fault_handler_x86.cc
+++ b/runtime/arch/x86/fault_handler_x86.cc
@@ -368,7 +368,7 @@ bool SuspensionHandler::Action(int, siginfo_t*, void* context) {
     return false;
   }
 
-  // The first instruction can a little bit up the stream due to load hoisting
+  // The first instruction can be a little bit up the stream due to load hoisting
   // in the compiler.
   uint8_t* limit = pc - 100;   // Compiler will hoist to a max of 20 instructions.
   uint8_t* ptr = pc - sizeof(checkinst1);
diff --git a/runtime/arch/x86/instruction_set_features_x86.h b/runtime/arch/x86/instruction_set_features_x86.h
index e6fbc33fdb..668c4ed741 100644
--- a/runtime/arch/x86/instruction_set_features_x86.h
+++ b/runtime/arch/x86/instruction_set_features_x86.h
@@ -31,7 +31,7 @@
 #define SET_VEX_M_0F_3A 0x03
 #define SET_VEX_W       0x80
 #define SET_VEX_L_128   0x00
-#define SET_VEL_L_256   0x04
+#define SET_VEX_L_256   0x04
 #define SET_VEX_PP_NONE 0x00
 #define SET_VEX_PP_66   0x01
 #define SET_VEX_PP_F3   0x02
diff --git a/runtime/art_method-inl.h b/runtime/art_method-inl.h
index 476c894cdb..d9068feb3c 100644
--- a/runtime/art_method-inl.h
+++ b/runtime/art_method-inl.h
@@ -777,6 +777,7 @@ inline uint16_t ArtMethod::GetCounter() {
 }
 
 inline uint32_t ArtMethod::GetImtIndex() {
+  DCHECK(GetDeclaringClass()->IsInterface());
   if (LIKELY(IsAbstract())) {
     return imt_index_;
   } else {
diff --git a/runtime/art_method.cc b/runtime/art_method.cc
index 856a3d6d44..e662e78782 100644
--- a/runtime/art_method.cc
+++ b/runtime/art_method.cc
@@ -87,7 +87,7 @@ ArtMethod* ArtMethod::GetNonObsoleteMethod() {
   }
   DCHECK_EQ(kRuntimePointerSize, Runtime::Current()->GetClassLinker()->GetImagePointerSize());
   if (IsDirect()) {
-    return &GetDeclaringClass()->GetDirectMethodsSlice(kRuntimePointerSize)[GetMethodIndex()];
+    return &GetDeclaringClass()->GetMethodsSlice(kRuntimePointerSize)[GetMethodIndex()];
   } else {
     return GetDeclaringClass()->GetVTableEntry(GetMethodIndex(), kRuntimePointerSize);
   }
@@ -279,7 +279,10 @@ ArtMethod* ArtMethod::FindOverriddenMethod(PointerSize pointer_size) {
       ObjPtr<mirror::IfTable> iftable = GetDeclaringClass()->GetIfTable();
       for (size_t i = 0; i < iftable->Count() && result == nullptr; i++) {
         ObjPtr<mirror::Class> interface = iftable->GetInterface(i);
-        for (ArtMethod& interface_method : interface->GetVirtualMethods(pointer_size)) {
+        for (ArtMethod& interface_method : interface->GetMethods(pointer_size)) {
+          if (!interface_method.IsVirtual()) {
+            continue;
+          }
           if (HasSameNameAndSignature(interface_method.GetInterfaceMethodIfProxy(pointer_size))) {
             result = &interface_method;
             break;
@@ -533,13 +536,15 @@ static const OatFile::OatMethod FindOatMethodFor(ArtMethod* method,
     // Compute the oat_method_index by search for its position in the declared virtual methods.
     oat_method_index = declaring_class->NumDirectMethods();
     bool found_virtual = false;
-    for (ArtMethod& art_method : declaring_class->GetVirtualMethods(pointer_size)) {
-      // Check method index instead of identity in case of duplicate method definitions.
-      if (method->GetDexMethodIndex() == art_method.GetDexMethodIndex()) {
-        found_virtual = true;
-        break;
+    for (ArtMethod& art_method : declaring_class->GetMethods(pointer_size)) {
+      if (art_method.IsVirtual()) {
+        // Check method index instead of identity in case of duplicate method definitions.
+        if (method->GetDexMethodIndex() == art_method.GetDexMethodIndex()) {
+          found_virtual = true;
+          break;
+        }
+        oat_method_index++;
       }
-      oat_method_index++;
     }
     CHECK(found_virtual) << "Didn't find oat method index for virtual method: "
                          << method->PrettyMethod();
@@ -633,7 +638,6 @@ const OatQuickMethodHeader* ArtMethod::GetOatQuickMethodHeader(uintptr_t pc) {
       return method_header;
     } else {
       if (kIsDebugBuild && code_cache->ContainsPc(reinterpret_cast<const void*>(pc))) {
-        code_cache->DumpAllCompiledMethods(LOG_STREAM(FATAL_WITHOUT_ABORT));
         LOG(FATAL)
             << PrettyMethod()
             << ", pc=" << std::hex << pc
diff --git a/runtime/art_method.h b/runtime/art_method.h
index c941696c29..222074ed85 100644
--- a/runtime/art_method.h
+++ b/runtime/art_method.h
@@ -200,6 +200,14 @@ class EXPORT ArtMethod final {
   static bool IsConstructor(uint32_t access_flags) {
     return (access_flags & kAccConstructor) != 0;
   }
+  // Returns true if the method is an instance constructor according to access flags.
+  bool IsInstanceConstructor() const {
+    return IsInstanceConstructor(GetAccessFlags());
+  }
+
+  static bool IsInstanceConstructor(uint32_t access_flags) {
+    return IsConstructor(access_flags) && !IsStatic(access_flags);
+  }
 
   // Returns true if the method is a class initializer according to access flags.
   bool IsClassInitializer() const {
@@ -220,6 +228,11 @@ class EXPORT ArtMethod final {
     return (access_flags & direct) != 0;
   }
 
+  bool IsVirtual() const {
+    uint32_t access_flags = GetAccessFlags();
+    return !IsDirect(access_flags) && !IsStatic(access_flags);
+  }
+
   // Returns true if the method is declared synchronized.
   bool IsSynchronized() const {
     return IsSynchronized(GetAccessFlags());
diff --git a/runtime/cha.cc b/runtime/cha.cc
index da355b7e22..34048fea71 100644
--- a/runtime/cha.cc
+++ b/runtime/cha.cc
@@ -163,16 +163,13 @@ void ClassHierarchyAnalysis::ResetSingleImplementationInHierarchy(ObjPtr<mirror:
   for (size_t i = 0; i < ifcount; ++i) {
     ObjPtr<mirror::Class> interface =
         iftable->GetInterface<kDefaultVerifyFlags, kWithoutReadBarrier>(i);
-    for (size_t j = 0,
-         count = iftable->GetMethodArrayCount<kDefaultVerifyFlags, kWithoutReadBarrier>(i);
-         j < count;
-         ++j) {
-      ArtMethod* method = interface->GetVirtualMethod(j, pointer_size);
-      if (method->HasSingleImplementation() &&
-          alloc->ContainsUnsafe(method->GetSingleImplementation(pointer_size)) &&
-          !method->IsDefault()) {
+    for (ArtMethod& interface_method : interface->GetDeclaredMethods(pointer_size)) {
+      if (interface_method.IsVirtual() &&
+          interface_method.HasSingleImplementation() &&
+          alloc->ContainsUnsafe(interface_method.GetSingleImplementation(pointer_size)) &&
+          !interface_method.IsDefault()) {
         // Do like there was no single implementation defined previously for this method.
-        method->SetSingleImplementation(nullptr, pointer_size);
+        interface_method.SetSingleImplementation(nullptr, pointer_size);
       }
     }
   }
@@ -545,9 +542,11 @@ void ClassHierarchyAnalysis::InitSingleImplementationFlag(Handle<mirror::Class>
 void ClassHierarchyAnalysis::UpdateAfterLoadingOf(Handle<mirror::Class> klass) {
   PointerSize image_pointer_size = Runtime::Current()->GetClassLinker()->GetImagePointerSize();
   if (klass->IsInterface()) {
-    for (ArtMethod& method : klass->GetDeclaredVirtualMethods(image_pointer_size)) {
-      DCHECK(method.IsAbstract() || method.IsDefault());
-      InitSingleImplementationFlag(klass, &method, image_pointer_size);
+    for (ArtMethod& method : klass->GetDeclaredMethods(image_pointer_size)) {
+      if (method.IsVirtual()) {
+        DCHECK(method.IsAbstract() || method.IsDefault());
+        InitSingleImplementationFlag(klass, &method, image_pointer_size);
+      }
     }
     return;
   }
@@ -596,14 +595,16 @@ void ClassHierarchyAnalysis::UpdateAfterLoadingOf(Handle<mirror::Class> klass) {
     const size_t ifcount = klass->GetIfTableCount();
     for (size_t i = 0; i < ifcount; ++i) {
       ObjPtr<mirror::Class> interface = iftable->GetInterface(i);
-      for (size_t j = 0, count = iftable->GetMethodArrayCount(i); j < count; ++j) {
-        ArtMethod* interface_method = interface->GetVirtualMethod(j, image_pointer_size);
+      for (ArtMethod& interface_method : interface->GetDeclaredMethods(image_pointer_size)) {
+        if (!interface_method.IsVirtual()) {
+          continue;
+        }
         ObjPtr<mirror::PointerArray> method_array = iftable->GetMethodArray(i);
-        ArtMethod* implementation_method =
-            method_array->GetElementPtrSize<ArtMethod*>(j, image_pointer_size);
+        ArtMethod* implementation_method = method_array->GetElementPtrSize<ArtMethod*>(
+            interface_method.GetMethodIndex(), image_pointer_size);
         DCHECK(implementation_method != nullptr) << klass->PrettyClass();
         CheckInterfaceMethodSingleImplementationInfo(klass,
-                                                     interface_method,
+                                                     &interface_method,
                                                      implementation_method,
                                                      invalidated_single_impl_methods,
                                                      image_pointer_size);
diff --git a/runtime/class_linker.cc b/runtime/class_linker.cc
index b14eaae51b..ee45879a43 100644
--- a/runtime/class_linker.cc
+++ b/runtime/class_linker.cc
@@ -706,7 +706,8 @@ bool ClassLinker::InitWithoutImage(std::vector<std::unique_ptr<const DexFile>> b
   Handle<mirror::Class> java_lang_Class(hs.NewHandle(ObjPtr<mirror::Class>::DownCast(
       heap->AllocNonMovableObject(self, nullptr, class_class_size, VoidFunctor()))));
   CHECK(java_lang_Class != nullptr);
-  java_lang_Class->SetClassFlags(mirror::kClassFlagClass);
+  java_lang_Class->AddRemoveClassFlags(mirror::kClassFlagClass |
+                                       mirror::kClassFlagHasEmbeddedVTable);
   java_lang_Class->SetClass(java_lang_Class.Get());
   if (kUseBakerReadBarrier) {
     java_lang_Class->AssertReadBarrierState();
@@ -755,6 +756,26 @@ bool ClassLinker::InitWithoutImage(std::vector<std::unique_ptr<const DexFile>> b
                  mirror::ObjectArray<mirror::Object>::ClassSize(image_pointer_size_))));
   object_array_class->SetComponentType(java_lang_Object.Get());
 
+  // Set the empty field array to the object array class, to be used by other
+  // classes which don't have fields.
+  LinearAlloc* linear_alloc = runtime->GetLinearAlloc();
+  auto* empty_field_array = linear_alloc->Alloc(
+      self,
+      LengthPrefixedArray<ArtField>::ComputeSize(0),
+      LinearAllocKind::kNoGCRoots);
+  object_array_class->SetFieldsPtr(
+      reinterpret_cast<LengthPrefixedArray<ArtField>*>(empty_field_array));
+  size_t method_alignment = ArtMethod::Alignment(image_pointer_size_);
+  size_t method_size = ArtMethod::Size(image_pointer_size_);
+  auto* empty_method_array = linear_alloc->Alloc(
+      self,
+      LengthPrefixedArray<ArtMethod>::ComputeSize(0, method_size, method_alignment),
+      LinearAllocKind::kNoGCRoots);
+  object_array_class->SetMethodsPtr(
+      reinterpret_cast<LengthPrefixedArray<ArtMethod>*>(empty_method_array),
+      /* num_direct= */ 0u,
+      /* num_virtual= */ 0u);
+
   // Setup java.lang.String.
   //
   // We make this class non-movable for the unlikely case where it were to be
@@ -842,7 +863,6 @@ bool ClassLinker::InitWithoutImage(std::vector<std::unique_ptr<const DexFile>> b
   object_array_string->SetComponentType(java_lang_String.Get());
   SetClassRoot(ClassRoot::kJavaLangStringArrayClass, object_array_string.Get());
 
-  LinearAlloc* linear_alloc = runtime->GetLinearAlloc();
   // Create runtime resolution and imt conflict methods.
   runtime->SetResolutionMethod(runtime->CreateResolutionMethod());
   runtime->SetImtConflictMethod(runtime->CreateImtConflictMethod(linear_alloc));
@@ -1038,17 +1058,13 @@ bool ClassLinker::InitWithoutImage(std::vector<std::unique_ptr<const DexFile>> b
   CHECK_EQ(java_lang_ref_Reference->GetClassSize(),
            mirror::Reference::ClassSize(image_pointer_size_));
   class_root = FindSystemClass(self, "Ljava/lang/ref/FinalizerReference;");
-  CHECK_EQ(class_root->GetClassFlags(), mirror::kClassFlagNormal);
-  class_root->SetClassFlags(class_root->GetClassFlags() | mirror::kClassFlagFinalizerReference);
+  class_root->AddRemoveClassFlags(mirror::kClassFlagFinalizerReference, mirror::kClassFlagNormal);
   class_root = FindSystemClass(self, "Ljava/lang/ref/PhantomReference;");
-  CHECK_EQ(class_root->GetClassFlags(), mirror::kClassFlagNormal);
-  class_root->SetClassFlags(class_root->GetClassFlags() | mirror::kClassFlagPhantomReference);
+  class_root->AddRemoveClassFlags(mirror::kClassFlagPhantomReference, mirror::kClassFlagNormal);
   class_root = FindSystemClass(self, "Ljava/lang/ref/SoftReference;");
-  CHECK_EQ(class_root->GetClassFlags(), mirror::kClassFlagNormal);
-  class_root->SetClassFlags(class_root->GetClassFlags() | mirror::kClassFlagSoftReference);
+  class_root->AddRemoveClassFlags(mirror::kClassFlagSoftReference, mirror::kClassFlagNormal);
   class_root = FindSystemClass(self, "Ljava/lang/ref/WeakReference;");
-  CHECK_EQ(class_root->GetClassFlags(), mirror::kClassFlagNormal);
-  class_root->SetClassFlags(class_root->GetClassFlags() | mirror::kClassFlagWeakReference);
+  class_root->AddRemoveClassFlags(mirror::kClassFlagWeakReference, mirror::kClassFlagNormal);
 
   // Setup the ClassLoader, verifying the object_size_.
   class_root = FindSystemClass(self, "Ljava/lang/ClassLoader;");
@@ -1286,10 +1302,11 @@ static void InitializeObjectVirtualMethodHashes(ObjPtr<mirror::Class> java_lang_
                                                 PointerSize pointer_size,
                                                 /*out*/ ArrayRef<uint32_t> virtual_method_hashes)
     REQUIRES_SHARED(Locks::mutator_lock_) {
-  ArraySlice<ArtMethod> virtual_methods = java_lang_Object->GetVirtualMethods(pointer_size);
-  DCHECK_EQ(virtual_method_hashes.size(), virtual_methods.size());
-  for (size_t i = 0; i != virtual_method_hashes.size(); ++i) {
-    virtual_method_hashes[i] = ComputeMethodHash(&virtual_methods[i]);
+  ArraySlice<ArtMethod> methods = java_lang_Object->GetMethods(pointer_size);
+  for (size_t i = 0, j = 0; i != methods.size(); ++i) {
+    if (methods[i].IsVirtual()) {
+      virtual_method_hashes[j++] = ComputeMethodHash(&methods[i]);
+    }
   }
 }
 
@@ -1434,7 +1451,8 @@ bool ClassLinker::InitFromBootImage(std::string* error_msg) {
   class_roots_ = GcRoot<mirror::ObjectArray<mirror::Class>>(
       ObjPtr<mirror::ObjectArray<mirror::Class>>::DownCast(
           image_header.GetImageRoot(ImageHeader::kClassRoots)));
-  DCHECK_EQ(GetClassRoot<mirror::Class>(this)->GetClassFlags(), mirror::kClassFlagClass);
+  DCHECK_EQ(GetClassRoot<mirror::Class>(this)->GetClassFlags() & ~mirror::kClassFlagStaticRefInfo,
+            mirror::kClassFlagClass);
 
   DCHECK_EQ(GetClassRoot<mirror::Object>(this)->GetObjectSize(), sizeof(mirror::Object));
   ObjPtr<mirror::ObjectArray<mirror::Object>> boot_image_live_objects =
@@ -2337,10 +2355,6 @@ bool ClassLinker::AddImageSpace(gc::space::ImageSpace* space,
       }, space->Begin(), image_pointer_size_);
     }
 
-    for (auto dex_cache : dex_caches.Iterate<mirror::DexCache>()) {
-      CHECK(!dex_cache->GetDexFile()->IsCompactDexFile());
-    }
-
     ScopedTrace trace("AppImage:UpdateCodeItemAndNterp");
     bool can_use_nterp = interpreter::CanRuntimeUseNterp();
     uint16_t hotness_threshold = runtime->GetJITOptions()->GetWarmupThreshold();
@@ -2868,13 +2882,12 @@ void ClassLinker::FinishArrayClassSetup(ObjPtr<mirror::Class> array_class) {
   class_flags |= component_type->IsPrimitive()
                      ? (mirror::kClassFlagNoReferenceFields | mirror::kClassFlagPrimitiveArray)
                      : mirror::kClassFlagObjectArray;
-  array_class->SetClassFlags(class_flags);
+  array_class->AddRemoveClassFlags(class_flags);
   array_class->SetClassLoader(component_type->GetClassLoader());
   array_class->SetStatusForPrimitiveOrArray(ClassStatus::kLoaded);
   array_class->PopulateEmbeddedVTable(image_pointer_size_);
   ImTable* object_imt = java_lang_Object->GetImt(image_pointer_size_);
   array_class->SetImt(object_imt, image_pointer_size_);
-  DCHECK_EQ(array_class->NumMethods(), 0u);
 
   // don't need to set new_class->SetObjectSize(..)
   // because Object::SizeOf delegates to Array::SizeOf
@@ -2905,6 +2918,11 @@ void ClassLinker::FinishArrayClassSetup(ObjPtr<mirror::Class> array_class) {
   // Array classes are fully initialized either during single threaded startup,
   // or from a pre-fence visitor, so visibly initialized.
   array_class->SetStatusForPrimitiveOrArray(ClassStatus::kVisiblyInitialized);
+
+  // Arrays have no field and no method.
+  array_class->SetFieldsPtrUnchecked(GetEmptyFieldArray());
+  array_class->SetMethodsPtrUnchecked(GetEmptyMethodArray(), 0, 0);
+  DCHECK_EQ(array_class->NumMethods(), 0u);
 }
 
 void ClassLinker::FinishCoreArrayClassSetup(ClassRoot array_root) {
@@ -3784,13 +3802,17 @@ void ClassLinker::FixupStaticTrampolines(Thread* self, ObjPtr<mirror::Class> kla
     return;
   }
   PointerSize pointer_size = image_pointer_size_;
-  if (std::any_of(klass->GetDirectMethods(pointer_size).begin(),
-                  klass->GetDirectMethods(pointer_size).end(),
-                  [](const ArtMethod& m) { return m.IsCriticalNative(); })) {
+  if (std::any_of(klass->GetMethods(pointer_size).begin(),
+                  klass->GetMethods(pointer_size).end(),
+                  [](const ArtMethod& m) {
+                    DCHECK_IMPLIES(m.IsCriticalNative(), m.IsStatic());
+                    return m.IsCriticalNative();
+                  })) {
     // Store registered @CriticalNative methods, if any, to JNI entrypoints.
-    // Direct methods are a contiguous chunk of memory, so use the ordering of the map.
-    ArtMethod* first_method = klass->GetDirectMethod(0u, pointer_size);
-    ArtMethod* last_method = klass->GetDirectMethod(num_direct_methods - 1u, pointer_size);
+    // Methods are a contiguous chunk of memory, so use the ordering of the map.
+    ArraySlice<ArtMethod> methods = klass->GetMethods(pointer_size);
+    ArtMethod* first_method = &methods[0];
+    ArtMethod* last_method = &methods[methods.size() - 1u];
     MutexLock lock(self, critical_native_code_with_clinit_check_lock_);
     auto lb = critical_native_code_with_clinit_check_.lower_bound(first_method);
     while (lb != critical_native_code_with_clinit_check_.end() && lb->first <= last_method) {
@@ -3807,18 +3829,17 @@ void ClassLinker::FixupStaticTrampolines(Thread* self, ObjPtr<mirror::Class> kla
 
   instrumentation::Instrumentation* instrumentation = runtime->GetInstrumentation();
   bool enable_boot_jni_stub = !runtime->IsJavaDebuggable();
-  for (size_t method_index = 0; method_index < num_direct_methods; ++method_index) {
-    ArtMethod* method = klass->GetDirectMethod(method_index, pointer_size);
-    if (method->NeedsClinitCheckBeforeCall()) {
-      const void* quick_code = instrumentation->GetCodeForInvoke(method);
-      if (method->IsNative() && IsQuickGenericJniStub(quick_code) && enable_boot_jni_stub) {
-        const void* boot_jni_stub = FindBootJniStub(method);
+  for (ArtMethod& method : klass->GetMethods(pointer_size)) {
+    if (method.NeedsClinitCheckBeforeCall()) {
+      const void* quick_code = instrumentation->GetCodeForInvoke(&method);
+      if (method.IsNative() && IsQuickGenericJniStub(quick_code) && enable_boot_jni_stub) {
+        const void* boot_jni_stub = FindBootJniStub(&method);
         if (boot_jni_stub != nullptr) {
           // Use boot JNI stub if found.
           quick_code = boot_jni_stub;
         }
       }
-      instrumentation->UpdateMethodsCode(method, quick_code);
+      instrumentation->UpdateMethodsCode(&method, quick_code);
     }
   }
   // Ignore virtual methods on the iterator.
@@ -3919,7 +3940,7 @@ LengthPrefixedArray<ArtField>* ClassLinker::AllocArtFieldArray(Thread* self,
                                                                LinearAlloc* allocator,
                                                                size_t length) {
   if (length == 0) {
-    return nullptr;
+    return GetEmptyFieldArray();
   }
   // If the ArtField alignment changes, review all uses of LengthPrefixedArray<ArtField>.
   static_assert(alignof(ArtField) == 4, "ArtField alignment is expected to be 4.");
@@ -3935,7 +3956,7 @@ LengthPrefixedArray<ArtMethod>* ClassLinker::AllocArtMethodArray(Thread* self,
                                                                  LinearAlloc* allocator,
                                                                  size_t length) {
   if (length == 0) {
-    return nullptr;
+    return GetEmptyMethodArray();
   }
   const size_t method_alignment = ArtMethod::Alignment(image_pointer_size_);
   const size_t method_size = ArtMethod::Size(image_pointer_size_);
@@ -4315,6 +4336,13 @@ void ClassLinker::LoadClassHelper::Load(const ClassAccessor& accessor,
               return lhs.dex_field_index < rhs.dex_field_index;
             });
 
+  // Sort the methods by dex methods index to facilitate fast lookups.
+  std::sort(methods.begin(),
+            methods.end(),
+            [](ArtMethodData& lhs, ArtMethodData& rhs) {
+              return lhs.dex_method_index < rhs.dex_method_index;
+            });
+
   fields_ = fields;
   methods_ = methods;
   num_direct_methods_ = accessor.NumDirectMethods();
@@ -4403,11 +4431,12 @@ void ClassLinker::LoadClass(Thread* self,
                             const DexFile& dex_file,
                             const dex::ClassDef& dex_class_def,
                             Handle<mirror::Class> klass) {
-  CHECK(!dex_file.IsCompactDexFile());
   ClassAccessor accessor(dex_file,
                          dex_class_def,
                          /* parse_hiddenapi_class_data= */ klass->IsBootStrapClassLoaded());
   if (!accessor.HasClassData()) {
+    klass->SetFieldsPtrUnchecked(GetEmptyFieldArray());
+    klass->SetMethodsPtrUnchecked(GetEmptyMethodArray(), 0, 0);
     return;
   }
   Runtime* const runtime = Runtime::Current();
@@ -4805,7 +4834,6 @@ void ClassLinker::CreatePrimitiveClass(Thread* self,
   primitive_class->SetAccessFlagsDuringLinking(kAccPublic | kAccFinal | kAccAbstract);
   primitive_class->SetPrimitiveType(type);
   primitive_class->SetIfTable(GetClassRoot<mirror::Object>(this)->GetIfTable());
-  DCHECK_EQ(primitive_class->NumMethods(), 0u);
   // Primitive classes are initialized during single threaded startup, so visibly initialized.
   primitive_class->SetStatusForPrimitiveOrArray(ClassStatus::kVisiblyInitialized);
   std::string_view descriptor(Primitive::Descriptor(type));
@@ -4814,12 +4842,27 @@ void ClassLinker::CreatePrimitiveClass(Thread* self,
                                                ComputeModifiedUtf8Hash(descriptor));
   CHECK(existing == nullptr) << "InitPrimitiveClass(" << type << ") failed";
   SetClassRoot(primitive_root, primitive_class);
+  primitive_class->SetFieldsPtrUnchecked(GetEmptyFieldArray());
+  primitive_class->SetMethodsPtrUnchecked(GetEmptyMethodArray(), 0, 0);
+  DCHECK_EQ(primitive_class->NumMethods(), 0u);
 }
 
 inline ObjPtr<mirror::IfTable> ClassLinker::GetArrayIfTable() {
   return GetClassRoot<mirror::ObjectArray<mirror::Object>>(this)->GetIfTable();
 }
 
+inline LengthPrefixedArray<ArtField>* ClassLinker::GetEmptyFieldArray() {
+  // No need for a read barrier, as the array is a constant.
+  return GetClassRoot<mirror::ObjectArray<mirror::Object>, kWithoutReadBarrier>(this)
+      ->GetFieldsPtrUnchecked();
+}
+
+inline LengthPrefixedArray<ArtMethod>* ClassLinker::GetEmptyMethodArray() {
+  // No need for a read barrier, as the array is a constant.
+  return GetClassRoot<mirror::ObjectArray<mirror::Object>, kWithoutReadBarrier>(this)
+      ->GetMethodsPtr();
+}
+
 // Create an array class (i.e. the class object for the array, not the
 // array itself).  "descriptor" looks like "[C" or "[[[[B" or
 // "[Ljava/lang/String;".
@@ -5010,12 +5053,6 @@ ObjPtr<mirror::Class> ClassLinker::InsertClass(std::string_view descriptor,
     class_table->InsertWithHash(klass, hash);
     WriteBarrierOnClassLoaderLocked(class_loader, klass);
   }
-  if (kIsDebugBuild) {
-    // Test that copied methods correctly can find their holder.
-    for (ArtMethod& method : klass->GetCopiedMethods(image_pointer_size_)) {
-      CHECK_EQ(GetHoldingClassOfCopiedMethod(&method), klass);
-    }
-  }
   return nullptr;
 }
 
@@ -5596,16 +5633,17 @@ ObjPtr<mirror::Class> ClassLinker::CreateProxyClass(ScopedObjectAccessAlreadyRun
   temp_klass->SetMethodsPtr(proxy_class_methods, num_direct_methods, num_virtual_methods);
 
   // Create the single direct method.
-  CreateProxyConstructor(temp_klass, temp_klass->GetDirectMethodUnchecked(0, image_pointer_size_));
+  CreateProxyConstructor(temp_klass, &temp_klass->GetMethods(image_pointer_size_)[0]);
 
   // Create virtual method using specified prototypes.
-  // TODO These should really use the iterators.
-  for (size_t i = 0; i < num_virtual_methods; ++i) {
-    auto* virtual_method = temp_klass->GetVirtualMethodUnchecked(i, image_pointer_size_);
-    auto* prototype = proxied_methods[i];
-    CreateProxyMethod(temp_klass, prototype, virtual_method);
-    DCHECK(virtual_method->GetDeclaringClass() != nullptr);
-    DCHECK(prototype->GetDeclaringClass() != nullptr);
+  size_t index = 0;
+  for (ArtMethod& m : temp_klass->GetMethods(image_pointer_size_)) {
+    if (m.IsVirtual()) {
+      ArtMethod* prototype = proxied_methods[index++];
+      CreateProxyMethod(temp_klass, prototype, &m);
+      DCHECK(m.GetDeclaringClass() != nullptr);
+      DCHECK(prototype->GetDeclaringClass() != nullptr);
+    }
   }
 
   // The super class is java.lang.reflect.Proxy
@@ -5668,11 +5706,13 @@ ObjPtr<mirror::Class> ClassLinker::CreateProxyClass(ScopedObjectAccessAlreadyRun
 
   // Consistency checks.
   if (kIsDebugBuild) {
-    CheckProxyConstructor(klass->GetDirectMethod(0, image_pointer_size_));
+    CheckProxyConstructor(&klass->GetMethods(image_pointer_size_)[0]);
 
-    for (size_t i = 0; i < num_virtual_methods; ++i) {
-      auto* virtual_method = klass->GetVirtualMethodUnchecked(i, image_pointer_size_);
-      CheckProxyMethod(virtual_method, proxied_methods[i]);
+    index = 0;
+    for (ArtMethod& m : klass->GetMethods(image_pointer_size_)) {
+      if (m.IsVirtual()) {
+        CheckProxyMethod(&m, proxied_methods[index++]);
+      }
     }
 
     StackHandleScope<1> hs2(self);
@@ -6400,17 +6440,18 @@ bool ClassLinker::ValidateSuperClassDescriptors(Handle<mirror::Class> klass) {
   for (int32_t i = 0; i < klass->GetIfTableCount(); ++i) {
     super_klass.Assign(klass->GetIfTable()->GetInterface(i));
     if (klass->GetClassLoader() != super_klass->GetClassLoader()) {
-      uint32_t num_methods = super_klass->NumVirtualMethods();
-      for (uint32_t j = 0; j < num_methods; ++j) {
+      for (ArtMethod& super_m : super_klass->GetMethods(image_pointer_size_)) {
+        if (!super_m.IsVirtual()) {
+          continue;
+        }
         auto* m = klass->GetIfTable()->GetMethodArray(i)->GetElementPtrSize<ArtMethod*>(
-            j, image_pointer_size_);
-        auto* super_m = super_klass->GetVirtualMethod(j, image_pointer_size_);
-        if (m != super_m) {
+            super_m.GetMethodIndex(), image_pointer_size_);
+        if (m != &super_m) {
           if (UNLIKELY(!HasSameSignatureWithDifferentClassLoaders(self,
                                                                   klass,
                                                                   super_klass,
                                                                   m,
-                                                                  super_m))) {
+                                                                  &super_m))) {
             self->AssertPendingException();
             return false;
           }
@@ -6479,8 +6520,7 @@ void ClassLinker::FixupTemporaryDeclaringClass(ObjPtr<mirror::Class> temp_class,
     }
   }
 
-  DCHECK_EQ(temp_class->NumDirectMethods(), 0u);
-  DCHECK_EQ(temp_class->NumVirtualMethods(), 0u);
+  DCHECK_EQ(temp_class->NumMethods(), 0u);
   for (auto& method : new_class->GetMethods(image_pointer_size_)) {
     if (method.GetDeclaringClass() == temp_class) {
       method.SetDeclaringClass(new_class);
@@ -6615,12 +6655,11 @@ bool ClassLinker::LinkClass(Thread* self,
     StackHandleScope<1> hs(self);
     Handle<mirror::Class> h_new_class =
         hs.NewHandle(mirror::Class::CopyOf(klass, self, class_size, imt, image_pointer_size_));
-    // Set arrays to null since we don't want to have multiple classes with the same ArtField or
-    // ArtMethod array pointers. If this occurs, it causes bugs in remembered sets since the GC
-    // may not see any references to the target space and clean the card for a class if another
-    // class had the same array pointer.
-    klass->SetMethodsPtrUnchecked(nullptr, 0, 0);
-    klass->SetFieldsPtrUnchecked(nullptr);
+    // Set the fields array to the empty shared array, as we don't expect null
+    // for field array.
+    klass->SetFieldsPtrUnchecked(GetEmptyFieldArray());
+    // Set method array to the empty shared array.
+    klass->SetMethodsPtrUnchecked(GetEmptyMethodArray(), 0, 0);
     if (UNLIKELY(h_new_class == nullptr)) {
       self->AssertPendingOOMException();
       mirror::Class::SetStatus(klass, ClassStatus::kErrorUnresolved, self);
@@ -6785,7 +6824,7 @@ bool ClassLinker::LinkSuperClass(Handle<mirror::Class> klass) {
     klass->SetFinalizable();
   }
 
-  // Inherit class loader flag form super class.
+  // Inherit class loader flag from super class.
   if (super->IsClassLoaderClass()) {
     klass->SetClassLoaderClass();
   }
@@ -6794,7 +6833,7 @@ bool ClassLinker::LinkSuperClass(Handle<mirror::Class> klass) {
   uint32_t reference_flags = (super->GetClassFlags() & mirror::kClassFlagReference);
   if (reference_flags != 0) {
     CHECK_EQ(klass->GetClassFlags(), 0u);
-    klass->SetClassFlags(klass->GetClassFlags() | reference_flags);
+    klass->AddRemoveClassFlags(reference_flags);
   }
   // Disallow custom direct subclasses of java.lang.ref.Reference.
   if (init_done_ && super == GetClassRoot<mirror::Reference>(this)) {
@@ -6972,7 +7011,6 @@ void ClassLinker::FillIMTAndConflictTables(ObjPtr<mirror::Class> klass) {
                        conflict_method,
                        klass,
                        /*create_conflict_tables=*/true,
-                       /*ignore_copied_methods=*/false,
                        &new_conflict,
                        &imt_data[0]);
   }
@@ -7034,7 +7072,6 @@ void ClassLinker::FillIMTFromIfTable(ObjPtr<mirror::IfTable> if_table,
                                      ArtMethod* imt_conflict_method,
                                      ObjPtr<mirror::Class> klass,
                                      bool create_conflict_tables,
-                                     bool ignore_copied_methods,
                                      /*out*/bool* new_conflict,
                                      /*out*/ArtMethod** imt) {
   uint32_t conflict_counts[ImTable::kSize] = {};
@@ -7055,19 +7092,18 @@ void ClassLinker::FillIMTFromIfTable(ObjPtr<mirror::IfTable> if_table,
       continue;
     }
     ObjPtr<mirror::PointerArray> method_array = if_table->GetMethodArray(i);
-    for (size_t j = 0; j < method_array_count; ++j) {
-      ArtMethod* implementation_method =
-          method_array->GetElementPtrSize<ArtMethod*>(j, image_pointer_size_);
-      if (ignore_copied_methods && implementation_method->IsCopied()) {
+    for (ArtMethod& interface_method : interface->GetDeclaredMethods(image_pointer_size_)) {
+      if (!interface_method.IsVirtual()) {
         continue;
       }
+      ArtMethod* implementation_method = method_array->GetElementPtrSize<ArtMethod*>(
+          interface_method.GetMethodIndex(), image_pointer_size_);
       DCHECK(implementation_method != nullptr);
       // Miranda methods cannot be used to implement an interface method, but they are safe to put
       // in the IMT since their entrypoint is the interface trampoline. If we put any copied methods
       // or interface methods in the IMT here they will not create extra conflicts since we compare
       // names and signatures in SetIMTRef.
-      ArtMethod* interface_method = interface->GetVirtualMethod(j, image_pointer_size_);
-      const uint32_t imt_index = interface_method->GetImtIndex();
+      const uint32_t imt_index = interface_method.GetImtIndex();
 
       // There is only any conflicts if all of the interface methods for an IMT slot don't have
       // the same implementation method, keep track of this to avoid creating a conflict table in
@@ -7107,21 +7143,15 @@ void ClassLinker::FillIMTFromIfTable(ObjPtr<mirror::IfTable> if_table,
 
     for (size_t i = 0, length = if_table->Count(); i < length; ++i) {
       ObjPtr<mirror::Class> interface = if_table->GetInterface(i);
-      const size_t method_array_count = if_table->GetMethodArrayCount(i);
-      // Virtual methods can be larger than the if table methods if there are default methods.
-      if (method_array_count == 0) {
-        continue;
-      }
-      ObjPtr<mirror::PointerArray> method_array = if_table->GetMethodArray(i);
-      for (size_t j = 0; j < method_array_count; ++j) {
-        ArtMethod* implementation_method =
-            method_array->GetElementPtrSize<ArtMethod*>(j, image_pointer_size_);
-        if (ignore_copied_methods && implementation_method->IsCopied()) {
+      ObjPtr<mirror::PointerArray> method_array = if_table->GetMethodArrayOrNull(i);
+      for (ArtMethod& interface_method : interface->GetDeclaredMethods(image_pointer_size_)) {
+        if (!interface_method.IsVirtual()) {
           continue;
         }
+        ArtMethod* implementation_method = method_array->GetElementPtrSize<ArtMethod*>(
+            interface_method.GetMethodIndex(), image_pointer_size_);
         DCHECK(implementation_method != nullptr);
-        ArtMethod* interface_method = interface->GetVirtualMethod(j, image_pointer_size_);
-        const uint32_t imt_index = interface_method->GetImtIndex();
+        const uint32_t imt_index = interface_method.GetImtIndex();
         if (!imt[imt_index]->IsRuntimeMethod() ||
             imt[imt_index] == unimplemented_method ||
             imt[imt_index] == imt_conflict_method) {
@@ -7129,7 +7159,7 @@ void ClassLinker::FillIMTFromIfTable(ObjPtr<mirror::IfTable> if_table,
         }
         ImtConflictTable* table = imt[imt_index]->GetImtConflictTable(image_pointer_size_);
         const size_t num_entries = table->NumEntries(image_pointer_size_);
-        table->SetInterfaceMethod(num_entries, image_pointer_size_, interface_method);
+        table->SetInterfaceMethod(num_entries, image_pointer_size_, &interface_method);
         table->SetImplementationMethod(num_entries, image_pointer_size_, implementation_method);
       }
     }
@@ -7422,12 +7452,12 @@ void CheckClassOwnsVTableEntries(Thread* self,
                    << " has an unexpected method index for its spot in the vtable for class"
                    << klass->PrettyClass();
     }
-    ArraySlice<ArtMethod> virtuals = klass->GetVirtualMethodsSliceUnchecked(pointer_size);
+    ArraySlice<ArtMethod> methods = klass->GetMethodsSliceUnchecked(pointer_size);
     auto is_same_method = [m] (const ArtMethod& meth) {
       return &meth == m;
     };
     if (!((super_vtable_length > i && superclass->GetVTableEntry(i, pointer_size) == m) ||
-          std::find_if(virtuals.begin(), virtuals.end(), is_same_method) != virtuals.end())) {
+          std::find_if(methods.begin(), methods.end(), is_same_method) != methods.end())) {
       LOG(WARNING) << m->PrettyMethod() << " does not seem to be owned by current class "
                    << klass->PrettyClass() << " or any of its superclasses!";
     }
@@ -7702,11 +7732,12 @@ class ClassLinker::LinkMethodsHelper {
       REQUIRES_SHARED(Locks::mutator_lock_);
 
   void ClobberOldMethods(LengthPrefixedArray<ArtMethod>* old_methods,
-                         LengthPrefixedArray<ArtMethod>* methods) {
+                         LengthPrefixedArray<ArtMethod>* methods)
+      REQUIRES_SHARED(Locks::mutator_lock_) {
     if (kIsDebugBuild && old_methods != nullptr) {
       CHECK(methods != nullptr);
       // Put some random garbage in old methods to help find stale pointers.
-      if (methods != old_methods) {
+      if (methods != old_methods && old_methods != class_linker_->GetEmptyMethodArray()) {
         // Need to make sure the GC is not running since it could be scanning the methods we are
         // about to overwrite.
         ScopedThreadStateChange tsc(self_, ThreadState::kSuspended);
@@ -7873,9 +7904,11 @@ class ClassLinker::LinkMethodsHelper {
 
     // NO_THREAD_SAFETY_ANALYSIS: This is called from unannotated `HashSet<>` functions.
     size_t operator()(uint32_t index) const NO_THREAD_SAFETY_ANALYSIS {
-      DCHECK_LT(index, klass_->NumDeclaredVirtualMethods());
-      ArtMethod* method = klass_->GetVirtualMethodDuringLinking(index, kPointerSize);
-      return ComputeMethodHash(method->GetInterfaceMethodIfProxy(kPointerSize));
+      DCHECK_LT(index, klass_->NumMethods());
+      ArtMethod& method = klass_->GetMethods(kPointerSize)[index];
+      DCHECK(method.IsVirtual());
+      DCHECK(!method.IsCopied());
+      return ComputeMethodHash(method.GetInterfaceMethodIfProxy(kPointerSize));
     }
 
    private:
@@ -7890,15 +7923,17 @@ class ClassLinker::LinkMethodsHelper {
 
     // NO_THREAD_SAFETY_ANALYSIS: This is called from unannotated `HashSet<>` functions.
     bool operator()(uint32_t lhs_index, ArtMethod* rhs) const NO_THREAD_SAFETY_ANALYSIS {
-      DCHECK_LT(lhs_index, klass_->NumDeclaredVirtualMethods());
-      ArtMethod* lhs = klass_->GetVirtualMethodDuringLinking(lhs_index, kPointerSize);
-      return MethodSignatureEquals(lhs->GetInterfaceMethodIfProxy(kPointerSize), rhs);
+      DCHECK_LT(lhs_index, klass_->NumMethods());
+      ArtMethod& lhs = klass_->GetMethods(kPointerSize)[lhs_index];
+      DCHECK(lhs.IsVirtual());
+      DCHECK(!lhs.IsCopied());
+      return MethodSignatureEquals(lhs.GetInterfaceMethodIfProxy(kPointerSize), rhs);
     }
 
     // NO_THREAD_SAFETY_ANALYSIS: This is called from unannotated `HashSet<>` functions.
     bool operator()(uint32_t lhs_index, uint32_t rhs_index) const NO_THREAD_SAFETY_ANALYSIS {
-      DCHECK_LT(lhs_index, klass_->NumDeclaredVirtualMethods());
-      DCHECK_LT(rhs_index, klass_->NumDeclaredVirtualMethods());
+      DCHECK_LT(lhs_index, klass_->NumMethods());
+      DCHECK_LT(rhs_index, klass_->NumMethods());
       return lhs_index == rhs_index;
     }
 
@@ -8041,20 +8076,23 @@ class ClassLinker::LinkMethodsHelper {
             --i;
             ObjPtr<mirror::Class> iface = iftable->GetInterface(i);
             DCHECK(iface == super_iftable->GetInterface(i));
-            auto [found, index] =
-                MethodArrayContains(super_iftable->GetMethodArrayOrNull(i), super_method);
-            if (found) {
-              ArtMethod* interface_method = iface->GetVirtualMethod(index, kPointerSize);
-              auto slow_is_masked = [=]() REQUIRES_SHARED(Locks::mutator_lock_) {
-                // Note: The `iftable` has method arrays in range [super_ifcount, ifcount) filled
-                // with vtable indexes but the range [0, super_ifcount) is empty, so we need to
-                // use the `super_iftable` filled with implementation methods for that range.
-                return ContainsImplementingMethod(
-                           super_iftable, i + 1u, super_ifcount, iface, super_method) ||
-                       ContainsImplementingMethod(
-                           iftable, super_ifcount, ifcount, iface, vtable_index);
-              };
-              UpdateStateImpl(iface, interface_method, slow_is_masked);
+            ObjPtr<mirror::PointerArray> method_array = super_iftable->GetMethodArrayOrNull(i);
+            for (ArtMethod& method : iface->GetDeclaredMethods(kPointerSize)) {
+              if (method.IsVirtual() &&
+                  method_array->GetElementPtrSize<ArtMethod*, kPointerSize>(method.GetMethodIndex())
+                      == super_method) {
+                auto slow_is_masked = [=]() REQUIRES_SHARED(Locks::mutator_lock_) {
+                  // Note: The `iftable` has method arrays in range [super_ifcount, ifcount) filled
+                  // with vtable indexes but the range [0, super_ifcount) is empty, so we need to
+                  // use the `super_iftable` filled with implementation methods for that range.
+                  return ContainsImplementingMethod(
+                             super_iftable, i + 1u, super_ifcount, iface, super_method) ||
+                         ContainsImplementingMethod(
+                             iftable, super_ifcount, ifcount, iface, vtable_index);
+                };
+                UpdateStateImpl(iface, &method, slow_is_masked);
+                break;
+              }
             }
           }
           if (GetState() == State::kDefaultConflict) {
@@ -8153,13 +8191,15 @@ class ClassLinker::LinkMethodsHelper {
         REQUIRES_SHARED(Locks::mutator_lock_) {
       for (size_t i = begin; i != end; ++i) {
         ObjPtr<mirror::Class> current_iface = iftable->GetInterface(i);
-        for (ArtMethod& current_method : current_iface->GetDeclaredVirtualMethods(kPointerSize)) {
-          if (MethodSignatureEquals(&current_method, interface_method)) {
-            // Check if the i'th interface is a subtype of this one.
-            if (current_iface->Implements(iface)) {
-              return true;
+        for (ArtMethod& current_method : current_iface->GetDeclaredMethods(kPointerSize)) {
+          if (current_method.IsVirtual()) {
+            if (MethodSignatureEquals(&current_method, interface_method)) {
+              // Check if the i'th interface is a subtype of this one.
+              if (current_iface->Implements(iface)) {
+                return true;
+              }
+              break;
             }
-            break;
           }
         }
       }
@@ -8308,6 +8348,9 @@ void ClassLinker::LinkMethodsHelper<kPointerSize>::ReallocMethods(ObjPtr<mirror:
 
   // Attempt to realloc to save RAM if possible.
   LengthPrefixedArray<ArtMethod>* old_methods = klass->GetMethodsPtr();
+  if (old_methods == class_linker_->GetEmptyMethodArray()) {
+    old_methods = nullptr;
+  }
   // The Realloced virtual methods aren't visible from the class roots, so there is no issue
   // where GCs could attempt to mark stale pointers due to memcpy. And since we overwrite the
   // realloced memory with out->CopyFrom, we are guaranteed to have objects in the to space since
@@ -8385,7 +8428,7 @@ void ClassLinker::LinkMethodsHelper<kPointerSize>::ReallocMethods(ObjPtr<mirror:
   for (size_t i = 0; i != num_new_copied_methods; ++i) {
     const CopiedMethodRecord* record = sorted_records[i];
     ArtMethod* interface_method = record->GetMainMethod();
-    DCHECK(!interface_method->IsCopied());
+    DCHECK(!interface_method->IsCopied()) << interface_method->PrettyMethod();
     ArtMethod& new_method = methods->At(old_method_count + i, kMethodSize, kMethodAlignment);
     new_method.CopyFrom(interface_method, kPointerSize);
     new_method.SetMethodIndex(dchecked_integral_cast<uint16_t>(record->GetMethodIndex()));
@@ -8491,8 +8534,7 @@ bool ClassLinker::LinkMethodsHelper<kPointerSize>::FinalizeIfTable(
       // the implementation method is already in the superclass, only for new methods.
       // For simplicity, use the entire method array including direct methods.
       LengthPrefixedArray<ArtMethod>* const new_methods = klass->GetMethodsPtr();
-      if (new_methods != nullptr) {
-        DCHECK_NE(new_methods->size(), 0u);
+      if (new_methods->size() != 0u) {
         imt_methods_begin =
             reinterpret_cast<uintptr_t>(&new_methods->At(0, kMethodSize, kMethodAlignment));
         imt_methods_size = new_methods->size() * kMethodSize;
@@ -8500,10 +8542,10 @@ bool ClassLinker::LinkMethodsHelper<kPointerSize>::FinalizeIfTable(
     }
   }
 
-  auto update_imt = [=](ObjPtr<mirror::Class> iface, size_t j, ArtMethod* implementation)
+  auto update_imt = [=](size_t imt_index, ArtMethod* implementation)
       REQUIRES_SHARED(Locks::mutator_lock_) {
     // Place method in imt if entry is empty, place conflict otherwise.
-    ArtMethod** imt_ptr = &out_imt[iface->GetVirtualMethod(j, kPointerSize)->GetImtIndex()];
+    ArtMethod** imt_ptr = &out_imt[imt_index];
     class_linker->SetIMTRef(unimplemented_method,
                             imt_conflict_method,
                             implementation,
@@ -8519,19 +8561,24 @@ bool ClassLinker::LinkMethodsHelper<kPointerSize>::FinalizeIfTable(
     if (method_array == nullptr) {
       continue;
     }
-    size_t num_methods = method_array->GetLength();
     ObjPtr<mirror::Class> iface = iftable->GetInterface(i);
-    size_t j = 0;
     // First loop has method array shared with the super class.
+    size_t j = 0;
+    auto methods = iface->GetDeclaredMethods(kPointerSize);
+    size_t num_methods = methods.size();
     for (; j != num_methods; ++j) {
-      ArtMethod* super_implementation =
-          method_array->GetElementPtrSize<ArtMethod*, kPointerSize>(j);
+      ArtMethod& interface_method = methods[j];
+      if (!interface_method.IsVirtual()) {
+        continue;
+      }
+      ArtMethod* super_implementation = method_array->GetElementPtrSize<ArtMethod*, kPointerSize>(
+          interface_method.GetMethodIndex());
       size_t vtable_index = super_implementation->GetMethodIndex();
       ArtMethod* implementation =
           vtable->GetElementPtrSize<ArtMethod*, kPointerSize>(vtable_index);
       // Check if we need to update IMT with this method, see above.
       if (reinterpret_cast<uintptr_t>(implementation) - imt_methods_begin < imt_methods_size) {
-        update_imt(iface, j, implementation);
+        update_imt(interface_method.GetImtIndex(), implementation);
       }
       if (implementation != super_implementation) {
         // Copy-on-write and move to the next loop.
@@ -8549,27 +8596,33 @@ bool ClassLinker::LinkMethodsHelper<kPointerSize>::FinalizeIfTable(
           iftable.Assign(new_iftable);
         }
         method_array = ObjPtr<mirror::PointerArray>::DownCast(
-            mirror::Array::CopyOf(old_method_array, self, num_methods));
+            mirror::Array::CopyOf(old_method_array, self, old_method_array->GetLength()));
         if (method_array == nullptr) {
           return false;
         }
         iftable->SetMethodArray(i, method_array);
-        method_array->SetElementPtrSize(j, implementation, kPointerSize);
+        method_array->SetElementPtrSize(
+            interface_method.GetMethodIndex(), implementation, kPointerSize);
         ++j;
         break;
       }
     }
     // Second loop (if non-empty) has method array different from the superclass.
     for (; j != num_methods; ++j) {
-      ArtMethod* super_implementation =
-          method_array->GetElementPtrSize<ArtMethod*, kPointerSize>(j);
+      ArtMethod& interface_method = methods[j];
+      if (!interface_method.IsVirtual()) {
+        continue;
+      }
+      ArtMethod* super_implementation = method_array->GetElementPtrSize<ArtMethod*, kPointerSize>(
+          interface_method.GetMethodIndex());
       size_t vtable_index = super_implementation->GetMethodIndex();
       ArtMethod* implementation =
           vtable->GetElementPtrSize<ArtMethod*, kPointerSize>(vtable_index);
-      method_array->SetElementPtrSize(j, implementation, kPointerSize);
+      method_array->SetElementPtrSize(
+          interface_method.GetMethodIndex(), implementation, kPointerSize);
       // Check if we need to update IMT with this method, see above.
       if (reinterpret_cast<uintptr_t>(implementation) - imt_methods_begin < imt_methods_size) {
-        update_imt(iface, j, implementation);
+        update_imt(interface_method.GetImtIndex(), implementation);
       }
     }
   }
@@ -8583,13 +8636,18 @@ bool ClassLinker::LinkMethodsHelper<kPointerSize>::FinalizeIfTable(
     }
     size_t num_methods = method_array->GetLength();
     ObjPtr<mirror::Class> iface = iftable->GetInterface(i);
-    for (size_t j = 0; j != num_methods; ++j) {
-      size_t vtable_index = method_array->GetElementPtrSize<size_t, kPointerSize>(j);
+    for (ArtMethod& interface_method : iface->GetDeclaredMethods(kPointerSize)) {
+      if (!interface_method.IsVirtual()) {
+        continue;
+      }
+      size_t vtable_index =
+          method_array->GetElementPtrSize<size_t, kPointerSize>(interface_method.GetMethodIndex());
       ArtMethod* implementation =
           vtable->GetElementPtrSize<ArtMethod*, kPointerSize>(vtable_index);
-      method_array->SetElementPtrSize(j, implementation, kPointerSize);
+      method_array->SetElementPtrSize(
+          interface_method.GetMethodIndex(), implementation, kPointerSize);
       if (!is_klass_abstract) {
-        update_imt(iface, j, implementation);
+        update_imt(interface_method.GetImtIndex(), implementation);
       }
     }
   }
@@ -8687,7 +8745,7 @@ size_t ClassLinker::LinkMethodsHelper<kPointerSize>::AssignVTableIndexes(
   static constexpr size_t kMaxStackBuferSize = 256;
   const size_t declared_virtuals_buffer_size = num_virtual_methods * 3;
   const size_t super_vtable_buffer_size = super_vtable_length * 3;
-  const size_t bit_vector_size = BitVector::BitsToWords(num_virtual_methods);
+  const size_t bit_vector_size = BitVector::BitsToWords(klass->NumMethods());
   const size_t total_size =
       declared_virtuals_buffer_size + super_vtable_buffer_size + bit_vector_size;
 
@@ -8722,14 +8780,17 @@ size_t ClassLinker::LinkMethodsHelper<kPointerSize>::AssignVTableIndexes(
   // super vtable (which is only lazy populated in case of interface overriding,
   // see below). This makes sure that we pay the performance price only on that
   // class, and not on its subclasses (except in the case of interface overriding, see below).
-  for (size_t i = 0; i != num_virtual_methods; ++i) {
-    ArtMethod* virtual_method = klass->GetVirtualMethodDuringLinking(i, kPointerSize);
-    DCHECK(!virtual_method->IsStatic()) << virtual_method->PrettyMethod();
-    ArtMethod* signature_method = UNLIKELY(is_proxy_class)
-        ? virtual_method->GetInterfaceMethodForProxyUnchecked(kPointerSize)
-        : virtual_method;
-    size_t hash = ComputeMethodHash(signature_method);
-    declared_virtual_signatures.PutWithHash(i, hash);
+  size_t index = 0;
+  for (ArtMethod& method : klass->GetMethods(kPointerSize)) {
+    DCHECK(!method.IsCopied());
+    if (method.IsVirtual()) {
+      ArtMethod* signature_method = UNLIKELY(is_proxy_class)
+          ? method.GetInterfaceMethodForProxyUnchecked(kPointerSize)
+          : &method;
+      size_t hash = ComputeMethodHash(signature_method);
+      declared_virtual_signatures.PutWithHash(index, hash);
+    }
+    ++index;
   }
 
   // Loop through each super vtable method and see if they are overridden by a method we added to
@@ -8751,11 +8812,13 @@ size_t ClassLinker::LinkMethodsHelper<kPointerSize>::AssignVTableIndexes(
     if (it == declared_virtual_signatures.end()) {
       continue;
     }
-    ArtMethod* virtual_method = klass->GetVirtualMethodDuringLinking(*it, kPointerSize);
+    ArtMethod& virtual_method = klass->GetMethods(kPointerSize)[*it];
+    DCHECK(virtual_method.IsVirtual());
+    DCHECK(!virtual_method.IsCopied());
     if (super_method->IsFinal()) {
       sants.reset();
       ThrowLinkageError(klass, "Method %s overrides final method in class %s",
-                        virtual_method->PrettyMethod().c_str(),
+                        virtual_method.PrettyMethod().c_str(),
                         super_method->GetDeclaringClassDescriptor());
       return 0u;
     }
@@ -8769,23 +8832,25 @@ size_t ClassLinker::LinkMethodsHelper<kPointerSize>::AssignVTableIndexes(
         std::fill_n(same_signature_vtable_lists.data(), super_vtable_length, dex::kDexNoIndex);
         same_signature_vtable_lists_ = same_signature_vtable_lists;
       }
-      same_signature_vtable_lists[j] = virtual_method->GetMethodIndexDuringLinking();
+      same_signature_vtable_lists[j] = virtual_method.GetMethodIndexDuringLinking();
     } else {
       initialized_methods.SetBit(*it);
     }
 
     // We arbitrarily set to the largest index. This is also expected when
     // iterating over the `same_signature_vtable_lists_`.
-    virtual_method->SetMethodIndex(j);
+    virtual_method.SetMethodIndex(j);
   }
 
   // Add the non-overridden methods at the end.
-  for (size_t i = 0; i < num_virtual_methods; ++i) {
-    if (!initialized_methods.IsBitSet(i)) {
-      ArtMethod* local_method = klass->GetVirtualMethodDuringLinking(i, kPointerSize);
-      local_method->SetMethodIndex(vtable_length);
-      vtable_length++;
+  index = 0;
+  for (ArtMethod& m : klass->GetMethods(kPointerSize)) {
+    DCHECK(!m.IsCopied());
+    if (m.IsVirtual() && !initialized_methods.IsBitSet(index)) {
+      m.SetMethodIndex(vtable_length);
+      ++vtable_length;
     }
+    ++index;
   }
 
   // A lazily constructed super vtable set, which we only populate in the less
@@ -8816,17 +8881,20 @@ size_t ClassLinker::LinkMethodsHelper<kPointerSize>::AssignVTableIndexes(
     DCHECK_LT(i, ifcount);
     ObjPtr<mirror::Class> iface = iftable->GetInterface(i);
     ObjPtr<mirror::PointerArray> method_array = iftable->GetMethodArrayOrNull(i);
-    size_t num_methods = (method_array != nullptr) ? method_array->GetLength() : 0u;
-    for (size_t j = 0; j != num_methods; ++j) {
-      ArtMethod* interface_method = iface->GetVirtualMethod(j, kPointerSize);
-      size_t hash = ComputeMethodHash(interface_method);
+    for (ArtMethod& interface_method : iface->GetDeclaredMethods(kPointerSize)) {
+      if (!interface_method.IsVirtual()) {
+        continue;
+      }
+      size_t hash = ComputeMethodHash(&interface_method);
       ArtMethod* vtable_method = nullptr;
-      auto it1 = declared_virtual_signatures.FindWithHash(interface_method, hash);
+      auto it1 = declared_virtual_signatures.FindWithHash(&interface_method, hash);
       if (it1 != declared_virtual_signatures.end()) {
-        ArtMethod* found_method = klass->GetVirtualMethodDuringLinking(*it1, kPointerSize);
+        ArtMethod& found_method = klass->GetMethods(kPointerSize)[*it1];
+        DCHECK(found_method.IsVirtual());
+        DCHECK(!found_method.IsCopied());
         // For interface overriding, we only look at public methods.
-        if (found_method->IsPublic()) {
-          vtable_method = found_method;
+        if (found_method.IsPublic()) {
+          vtable_method = &found_method;
         }
       } else {
         // This situation should be rare (a superclass implements a method
@@ -8846,7 +8914,7 @@ size_t ClassLinker::LinkMethodsHelper<kPointerSize>::AssignVTableIndexes(
             DCHECK(inserted || super_vtable_accessor.GetVTableEntry(*it) == super_method);
           }
         }
-        auto it2 = super_vtable_signatures.FindWithHash(interface_method, hash);
+        auto it2 = super_vtable_signatures.FindWithHash(&interface_method, hash);
         if (it2 != super_vtable_signatures.end()) {
           vtable_method = super_vtable_accessor.GetVTableEntry(*it2);
         }
@@ -8856,13 +8924,14 @@ size_t ClassLinker::LinkMethodsHelper<kPointerSize>::AssignVTableIndexes(
       if (vtable_method != nullptr) {
         vtable_index = vtable_method->GetMethodIndexDuringLinking();
         if (!vtable_method->IsOverridableByDefaultMethod()) {
-          method_array->SetElementPtrSize(j, vtable_index, kPointerSize);
+          method_array->SetElementPtrSize(
+              interface_method.GetMethodIndex(), vtable_index, kPointerSize);
           continue;
         }
       }
 
       auto [it, inserted] = copied_method_records_.InsertWithHash(
-          CopiedMethodRecord(interface_method, vtable_index), hash);
+          CopiedMethodRecord(&interface_method, vtable_index), hash);
       if (vtable_method != nullptr) {
         DCHECK_EQ(vtable_index, it->GetMethodIndex());
       } else if (inserted) {
@@ -8872,12 +8941,13 @@ size_t ClassLinker::LinkMethodsHelper<kPointerSize>::AssignVTableIndexes(
       } else {
         vtable_index = it->GetMethodIndex();
       }
-      method_array->SetElementPtrSize(j, it->GetMethodIndex(), kPointerSize);
+      method_array->SetElementPtrSize(
+          interface_method.GetMethodIndex(), it->GetMethodIndex(), kPointerSize);
       if (inserted) {
-        it->SetState(interface_method->IsAbstract() ? CopiedMethodRecord::State::kAbstractSingle
-                                                    : CopiedMethodRecord::State::kDefaultSingle);
+        it->SetState(interface_method.IsAbstract() ? CopiedMethodRecord::State::kAbstractSingle
+                                                   : CopiedMethodRecord::State::kDefaultSingle);
       } else {
-        it->UpdateState(iface, interface_method, vtable_index, iftable, ifcount, i);
+        it->UpdateState(iface, &interface_method, vtable_index, iftable, ifcount, i);
       }
     }
   }
@@ -8937,11 +9007,14 @@ bool ClassLinker::LinkMethodsHelper<kPointerSize>::FindCopiedMethodsForInterface
       declared_virtuals_buffer_ptr,
       declared_virtuals_buffer_size,
       allocator_.Adapter());
-  for (size_t i = 0; i != num_virtual_methods; ++i) {
-    ArtMethod* virtual_method = klass->GetVirtualMethodDuringLinking(i, kPointerSize);
-    DCHECK(!virtual_method->IsStatic()) << virtual_method->PrettyMethod();
-    size_t hash = ComputeMethodHash(virtual_method);
-    declared_virtual_signatures.PutWithHash(i, hash);
+  size_t index = 0;
+  for (ArtMethod& method : klass->GetMethods(kPointerSize)) {
+    DCHECK(!method.IsCopied());
+    if (method.IsVirtual()) {
+      size_t hash = ComputeMethodHash(&method);
+      declared_virtual_signatures.PutWithHash(index, hash);
+    }
+    ++index;
   }
 
   // We do not create miranda methods for interface classes, so we do not need to track
@@ -8958,29 +9031,28 @@ bool ClassLinker::LinkMethodsHelper<kPointerSize>::FindCopiedMethodsForInterface
     if (!iface->HasDefaultMethods()) {
       continue;  // No default methods to process.
     }
-    size_t num_methods = iface->NumDeclaredVirtualMethods();
-    for (size_t j = 0; j != num_methods; ++j) {
-      ArtMethod* interface_method = iface->GetVirtualMethod(j, kPointerSize);
-      if (!interface_method->IsDefault()) {
+    for (ArtMethod& interface_method : iface->GetDeclaredMethods(kPointerSize)) {
+      if (!interface_method.IsDefault()) {
         continue;  // Do not process this non-default method.
       }
-      size_t hash = ComputeMethodHash(interface_method);
-      auto it1 = declared_virtual_signatures.FindWithHash(interface_method, hash);
+      DCHECK(interface_method.IsVirtual());
+      size_t hash = ComputeMethodHash(&interface_method);
+      auto it1 = declared_virtual_signatures.FindWithHash(&interface_method, hash);
       if (it1 != declared_virtual_signatures.end()) {
         // Virtual methods in interfaces are always public.
         // This is checked by the `DexFileVerifier`.
-        DCHECK(klass->GetVirtualMethodDuringLinking(*it1, kPointerSize)->IsPublic());
+        DCHECK(klass->GetDeclaredMethods(kPointerSize)[*it1].IsPublic());
         continue;  // This default method is masked by a method declared in this interface.
       }
 
-      CopiedMethodRecord new_record(interface_method, new_method_index);
+      CopiedMethodRecord new_record(&interface_method, new_method_index);
       auto it = copied_method_records_.FindWithHash(new_record, hash);
       if (it == copied_method_records_.end()) {
         // Pretend that there is another default method and try to update the state.
         // If the `interface_method` is not masked, the state shall change to
         // `kDefaultConflict`; if it is masked, the state remains `kDefault`.
         new_record.SetState(CopiedMethodRecord::State::kDefault);
-        new_record.UpdateStateForInterface(iface, interface_method, iftable, ifcount, i);
+        new_record.UpdateStateForInterface(iface, &interface_method, iftable, ifcount, i);
         if (new_record.GetState() == CopiedMethodRecord::State::kDefaultConflict) {
           // Insert the new record with the state `kDefault`.
           new_record.SetState(CopiedMethodRecord::State::kDefault);
@@ -8989,7 +9061,7 @@ bool ClassLinker::LinkMethodsHelper<kPointerSize>::FindCopiedMethodsForInterface
           ++new_method_index;
         }
       } else {
-        it->UpdateStateForInterface(iface, interface_method, iftable, ifcount, i);
+        it->UpdateStateForInterface(iface, &interface_method, iftable, ifcount, i);
       }
     }
   }
@@ -9028,10 +9100,15 @@ bool ClassLinker::LinkMethodsHelper<kPointerSize>::LinkMethods(
     }
     // Assign each method an interface table index and set the default flag.
     bool has_defaults = false;
-    for (size_t i = 0; i < num_virtual_methods; ++i) {
-      ArtMethod* m = klass->GetVirtualMethodDuringLinking(i, kPointerSize);
-      m->SetMethodIndex(i);
-      uint32_t access_flags = m->GetAccessFlags();
+    size_t index = 0;
+    for (ArtMethod& method : klass->GetMethods(kPointerSize)) {
+      DCHECK(!method.IsCopied());
+      if (!method.IsVirtual()) {
+        continue;
+      }
+      method.SetMethodIndex(index);
+      ++index;
+      uint32_t access_flags = method.GetAccessFlags();
       DCHECK(!ArtMethod::IsDefault(access_flags));
       DCHECK_EQ(!ArtMethod::IsAbstract(access_flags), ArtMethod::IsInvokable(access_flags));
       if (ArtMethod::IsInvokable(access_flags)) {
@@ -9042,24 +9119,24 @@ bool ClassLinker::LinkMethodsHelper<kPointerSize>::LinkMethods(
         // currently running CTS tests for default methods with dex file version 035 which
         // does not support default methods. So, we limit this to native methods. b/157718952
         if (ArtMethod::IsNative(access_flags)) {
-          DCHECK(!m->GetDexFile()->SupportsDefaultMethods());
+          DCHECK(!method.GetDexFile()->SupportsDefaultMethods());
           ThrowClassFormatError(klass.Get(),
                                 "Dex file does not support default method '%s'",
-                                m->PrettyMethod().c_str());
+                                method.PrettyMethod().c_str());
           return false;
         }
         if (!ArtMethod::IsPublic(access_flags)) {
           // The verifier should have caught the non-public method for dex version 37.
           // Just warn and skip it since this is from before default-methods so we don't
           // really need to care that it has code.
-          LOG(WARNING) << "Default interface method " << m->PrettyMethod() << " is not public! "
+          LOG(WARNING) << "Default interface method " << method.PrettyMethod() << " is not public! "
                        << "This will be a fatal error in subsequent versions of android. "
                        << "Continuing anyway.";
         }
         static_assert((kAccDefault & kAccIntrinsicBits) != 0);
-        DCHECK(!m->IsIntrinsic()) << "Adding kAccDefault to an intrinsic would be a mistake as it "
-                                  << "overlaps with kAccIntrinsicBits.";
-        m->SetAccessFlags(access_flags | kAccDefault);
+        DCHECK(!method.IsIntrinsic()) << "Adding kAccDefault to an intrinsic would be a "
+                                      << "mistake as it overlaps with kAccIntrinsicBits.";
+        method.SetAccessFlags(access_flags | kAccDefault);
         has_defaults = true;
       }
     }
@@ -9149,7 +9226,6 @@ bool ClassLinker::LinkMethodsHelper<kPointerSize>::LinkMethods(
                                             runtime_->GetImtConflictMethod(),
                                             klass.Get(),
                                             /*create_conflict_tables=*/false,
-                                            /*ignore_copied_methods=*/false,
                                             out_new_conflict,
                                             out_imt);
         }
@@ -9202,15 +9278,18 @@ bool ClassLinker::LinkMethodsHelper<kPointerSize>::LinkMethods(
 
     // Store new virtual methods in the new vtable.
     ArrayRef<uint32_t> same_signature_vtable_lists = same_signature_vtable_lists_;
-    for (ArtMethod& virtual_method : klass->GetVirtualMethodsSliceUnchecked(kPointerSize)) {
-      uint32_t vtable_index = virtual_method.GetMethodIndexDuringLinking();
-      vtable->SetElementPtrSize(vtable_index, &virtual_method, kPointerSize);
+    for (ArtMethod& method : klass->GetMethodsSliceUnchecked(kPointerSize)) {
+      if (!method.IsVirtual()) {
+        continue;
+      }
+      uint32_t vtable_index = method.GetMethodIndexDuringLinking();
+      vtable->SetElementPtrSize(vtable_index, &method, kPointerSize);
       if (UNLIKELY(vtable_index < same_signature_vtable_lists.size())) {
         // We may override more than one method according to JLS, see b/211854716.
         while (same_signature_vtable_lists[vtable_index] != dex::kDexNoIndex) {
           DCHECK_LT(same_signature_vtable_lists[vtable_index], vtable_index);
           vtable_index = same_signature_vtable_lists[vtable_index];
-          vtable->SetElementPtrSize(vtable_index, &virtual_method, kPointerSize);
+          vtable->SetElementPtrSize(vtable_index, &method, kPointerSize);
           if (kIsDebugBuild) {
             ArtMethod* current_method = super_class->GetVTableEntry(vtable_index, kPointerSize);
             DCHECK(klass->CanAccessMember(current_method->GetDeclaringClass(),
@@ -9265,11 +9344,15 @@ bool ClassLinker::LinkMethodsHelper<kPointerSize>::LinkJavaLangObjectMethods(
     self->AssertPendingOOMException();
     return false;
   }
-  for (size_t i = 0; i < mirror::Object::kVTableLength; ++i) {
-    ArtMethod* virtual_method = klass->GetVirtualMethodDuringLinking(i, kPointerSize);
-    vtable->SetElementPtrSize(i, virtual_method, kPointerSize);
-    virtual_method->SetMethodIndex(i);
+  size_t index = 0;
+  for (ArtMethod& m : klass->GetMethods(kPointerSize)) {
+    if (m.IsVirtual()) {
+      vtable->SetElementPtrSize(index, &m, kPointerSize);
+      m.SetMethodIndex(index);
+      ++index;
+    }
   }
+  DCHECK_EQ(index, mirror::Object::kVTableLength);
   klass->SetVTable(vtable);
   InitializeObjectVirtualMethodHashes(
       klass.Get(),
@@ -9721,7 +9804,7 @@ bool ClassLinker::LinkFieldsHelper::LinkFields(ClassLinker* class_linker,
       // super_class is null iff the class is java.lang.Object.
       if (super_class == nullptr ||
           (super_class->GetClassFlags() & mirror::kClassFlagNoReferenceFields) != 0) {
-        klass->SetClassFlags(klass->GetClassFlags() | mirror::kClassFlagNoReferenceFields);
+        klass->AddRemoveClassFlags(mirror::kClassFlagNoReferenceFields);
       }
     }
     if (kIsDebugBuild) {
@@ -9869,12 +9952,13 @@ class RecordAnnotationVisitor final : public annotations::AnnotationVisitor {
 
   bool IsRecordAnnotationFound() { return count_ != 0; }
 
-  annotations::VisitorStatus VisitAnnotation(const char* descriptor, uint8_t visibility) override {
+  annotations::VisitorStatus VisitAnnotation(const char* descriptor,
+                                             DexFile::DexVisibility visibility) override {
     if (has_error_) {
       return annotations::VisitorStatus::kVisitBreak;
     }
 
-    if (visibility != DexFile::kDexVisibilitySystem) {
+    if (visibility != DexFile::DexVisibility::kSystem) {
       return annotations::VisitorStatus::kVisitNext;
     }
 
@@ -11275,7 +11359,7 @@ class ClassLinker::FindVirtualMethodHolderVisitor : public ClassVisitor {
         pointer_size_(pointer_size) {}
 
   bool operator()(ObjPtr<mirror::Class> klass) REQUIRES_SHARED(Locks::mutator_lock_) override {
-    if (klass->GetVirtualMethodsSliceUnchecked(pointer_size_).Contains(method_)) {
+    if (klass->GetMethodsSliceUnchecked(pointer_size_).Contains(method_)) {
       holder_ = klass;
     }
     // Return false to stop searching if holder_ is not null.
diff --git a/runtime/class_linker.h b/runtime/class_linker.h
index 406172ca5d..93e2e5ba4c 100644
--- a/runtime/class_linker.h
+++ b/runtime/class_linker.h
@@ -1389,11 +1389,12 @@ class ClassLinker {
                           ArtMethod* imt_conflict_method,
                           ObjPtr<mirror::Class> klass,
                           bool create_conflict_tables,
-                          bool ignore_copied_methods,
                           /*out*/bool* new_conflict,
                           /*out*/ArtMethod** imt) REQUIRES_SHARED(Locks::mutator_lock_);
 
   ObjPtr<mirror::IfTable> GetArrayIfTable() REQUIRES_SHARED(Locks::mutator_lock_);
+  LengthPrefixedArray<ArtField>* GetEmptyFieldArray() REQUIRES_SHARED(Locks::mutator_lock_);
+  LengthPrefixedArray<ArtMethod>* GetEmptyMethodArray() REQUIRES_SHARED(Locks::mutator_lock_);
 
   bool OpenAndInitImageDexFiles(const gc::space::ImageSpace* space,
                                 Handle<mirror::ClassLoader> class_loader,
diff --git a/runtime/class_linker_test.cc b/runtime/class_linker_test.cc
index 1176bb133e..2233487853 100644
--- a/runtime/class_linker_test.cc
+++ b/runtime/class_linker_test.cc
@@ -279,11 +279,8 @@ class ClassLinkerTest : public CommonRuntimeTest {
                                                klass->GetDescriptor(&temp2)));
     if (klass->IsInterface()) {
       EXPECT_TRUE(klass->IsAbstract());
-      // Check that all methods are direct and either static (<clinit> or a regular static method),
-      // or private.
-      for (ArtMethod& m : klass->GetDirectMethods(kRuntimePointerSize)) {
-        EXPECT_TRUE(m.IsStatic() || m.IsPrivate());
-        EXPECT_TRUE(m.IsDirect());
+      for (ArtMethod& m : klass->GetMethods(kRuntimePointerSize)) {
+        EXPECT_TRUE(m.IsStatic() || m.IsPrivate() || m.IsVirtual());
       }
     } else {
       if (!klass->IsSynthetic()) {
@@ -318,15 +315,9 @@ class ClassLinkerTest : public CommonRuntimeTest {
     EXPECT_FALSE(klass->IsPrimitive());
     EXPECT_TRUE(klass->CanAccess(klass.Get()));
 
-    for (ArtMethod& method : klass->GetDirectMethods(kRuntimePointerSize)) {
+    for (ArtMethod& method : klass->GetDeclaredMethods(kRuntimePointerSize)) {
       AssertMethod(&method);
-      EXPECT_TRUE(method.IsDirect());
-      EXPECT_OBJ_PTR_EQ(klass.Get(), method.GetDeclaringClass());
-    }
-
-    for (ArtMethod& method : klass->GetDeclaredVirtualMethods(kRuntimePointerSize)) {
-      AssertMethod(&method);
-      EXPECT_FALSE(method.IsDirect());
+      EXPECT_NE(method.IsVirtual(), method.IsDirect());
       EXPECT_OBJ_PTR_EQ(klass.Get(), method.GetDeclaringClass());
     }
 
@@ -403,7 +394,7 @@ class ClassLinkerTest : public CommonRuntimeTest {
     StackHandleScope<1> hs(self);
     Handle<mirror::Class> klass(
         hs.NewHandle(class_linker_->FindSystemClass(self, descriptor.c_str())));
-    ASSERT_TRUE(klass != nullptr);
+    ASSERT_TRUE(klass != nullptr) << " Class: " << descriptor.c_str();
     std::string temp;
     EXPECT_STREQ(descriptor.c_str(), klass->GetDescriptor(&temp));
     EXPECT_OBJ_PTR_EQ(class_loader, klass->GetClassLoader());
@@ -588,7 +579,6 @@ struct ClassOffsets : public CheckOffsets<mirror::Class> {
     addOffset(OFFSETOF_MEMBER(mirror::Class, class_size_), "classSize");
     addOffset(OFFSETOF_MEMBER(mirror::Class, clinit_thread_id_), "clinitThreadId");
     addOffset(OFFSETOF_MEMBER(mirror::Class, component_type_), "componentType");
-    addOffset(OFFSETOF_MEMBER(mirror::Class, copied_methods_offset_), "copiedMethodsOffset");
     addOffset(OFFSETOF_MEMBER(mirror::Class, dex_cache_), "dexCache");
     addOffset(OFFSETOF_MEMBER(mirror::Class, dex_class_def_idx_), "dexClassDefIndex");
     addOffset(OFFSETOF_MEMBER(mirror::Class, dex_type_idx_), "dexTypeIndex");
@@ -609,7 +599,6 @@ struct ClassOffsets : public CheckOffsets<mirror::Class> {
               "referenceInstanceOffsets");
     addOffset(OFFSETOF_MEMBER(mirror::Class, status_), "status");
     addOffset(OFFSETOF_MEMBER(mirror::Class, super_class_), "superClass");
-    addOffset(OFFSETOF_MEMBER(mirror::Class, virtual_methods_offset_), "virtualMethodsOffset");
     addOffset(OFFSETOF_MEMBER(mirror::Class, vtable_), "vtable");
   }
 };
@@ -786,10 +775,12 @@ struct MethodHandleOffsets : public CheckOffsets<mirror::MethodHandle> {
 struct MethodHandleImplOffsets : public CheckOffsets<mirror::MethodHandleImpl> {
   MethodHandleImplOffsets() : CheckOffsets<mirror::MethodHandleImpl>(
       false, "Ljava/lang/invoke/MethodHandleImpl;") {
+    // Beware: changing offsets of field and targetClassOrMethodHandleInfo might lead to compat
+    // issues.
     addOffset(OFFSETOF_MEMBER(mirror::MethodHandleImpl, field_), "field");
-    addOffset(OFFSETOF_MEMBER(mirror::MethodHandleImpl, target_), "target");
     addOffset(OFFSETOF_MEMBER(mirror::MethodHandleImpl, target_class_or_info_),
               "targetClassOrMethodHandleInfo");
+    addOffset(OFFSETOF_MEMBER(mirror::MethodHandleImpl, target_method_entry_), "targetMethodEntry");
   }
 };
 
diff --git a/runtime/dex/dex_file_annotations.cc b/runtime/dex/dex_file_annotations.cc
index efcc4a0a38..27a587ad33 100644
--- a/runtime/dex/dex_file_annotations.cc
+++ b/runtime/dex/dex_file_annotations.cc
@@ -139,10 +139,10 @@ ObjPtr<mirror::Object> CreateAnnotationMember(const ClassData& klass,
                                               const uint8_t** annotation)
     REQUIRES_SHARED(Locks::mutator_lock_);
 
-bool IsVisibilityCompatible(uint32_t actual, uint32_t expected) {
-  if (expected == DexFile::kDexVisibilityRuntime) {
+bool IsVisibilityCompatible(DexFile::DexVisibility actual, DexFile::DexVisibility expected) {
+  if (expected == DexFile::DexVisibility::kRuntime) {
     if (IsSdkVersionSetAndAtMost(Runtime::Current()->GetTargetSdkVersion(), SdkVersion::kM)) {
-      return actual == DexFile::kDexVisibilityRuntime || actual == DexFile::kDexVisibilityBuild;
+      return actual == DexFile::DexVisibility::kRuntime || actual == DexFile::DexVisibility::kBuild;
     }
   }
   return actual == expected;
@@ -183,12 +183,13 @@ static const AnnotationSetItem* FindAnnotationSetForField(ArtField* field)
 const AnnotationItem* SearchAnnotationSet(const DexFile& dex_file,
                                           const AnnotationSetItem* annotation_set,
                                           const char* descriptor,
-                                          uint32_t visibility)
+                                          DexFile::DexVisibility visibility)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   const AnnotationItem* result = nullptr;
   for (uint32_t i = 0; i < annotation_set->size_; ++i) {
     const AnnotationItem* annotation_item = dex_file.GetAnnotationItem(annotation_set, i);
-    if (!IsVisibilityCompatible(annotation_item->visibility_, visibility)) {
+    if (!IsVisibilityCompatible(static_cast<DexFile::DexVisibility>(annotation_item->visibility_),
+                                visibility)) {
       continue;
     }
     const uint8_t* annotation = annotation_item->annotation_;
@@ -765,13 +766,14 @@ ObjPtr<mirror::Object> CreateAnnotationMember(const ClassData& klass,
 
 const AnnotationItem* GetAnnotationItemFromAnnotationSet(const ClassData& klass,
                                                          const AnnotationSetItem* annotation_set,
-                                                         uint32_t visibility,
+                                                         DexFile::DexVisibility visibility,
                                                          Handle<mirror::Class> annotation_class)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   const DexFile& dex_file = klass.GetDexFile();
   for (uint32_t i = 0; i < annotation_set->size_; ++i) {
     const AnnotationItem* annotation_item = dex_file.GetAnnotationItem(annotation_set, i);
-    if (!IsVisibilityCompatible(annotation_item->visibility_, visibility)) {
+    if (!IsVisibilityCompatible(static_cast<DexFile::DexVisibility>(annotation_item->visibility_),
+                                visibility)) {
       continue;
     }
     const uint8_t* annotation = annotation_item->annotation_;
@@ -801,7 +803,7 @@ const AnnotationItem* GetAnnotationItemFromAnnotationSet(const ClassData& klass,
 
 ObjPtr<mirror::Object> GetAnnotationObjectFromAnnotationSet(const ClassData& klass,
                                                             const AnnotationSetItem* annotation_set,
-                                                            uint32_t visibility,
+                                                            DexFile::DexVisibility visibility,
                                                             Handle<mirror::Class> annotation_class)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   const AnnotationItem* annotation_item = GetAnnotationItemFromAnnotationSet(
@@ -857,9 +859,8 @@ static inline ObjPtr<mirror::ObjectArray<T>> GetAnnotationArrayValue(
   if (annotation_set == nullptr) {
     return nullptr;
   }
-  const AnnotationItem* annotation_item =
-      SearchAnnotationSet(data.GetDexFile(), annotation_set, annotation_name,
-                          DexFile::kDexVisibilitySystem);
+  const AnnotationItem* annotation_item = SearchAnnotationSet(
+      data.GetDexFile(), annotation_set, annotation_name, DexFile::DexVisibility::kSystem);
   if (annotation_item == nullptr) {
     return nullptr;
   }
@@ -884,9 +885,8 @@ static ObjPtr<mirror::ObjectArray<mirror::String>> GetSignatureValue(
     REQUIRES_SHARED(Locks::mutator_lock_) {
   const DexFile& dex_file = klass.GetDexFile();
   StackHandleScope<1> hs(Thread::Current());
-  const AnnotationItem* annotation_item =
-      SearchAnnotationSet(dex_file, annotation_set, "Ldalvik/annotation/Signature;",
-                          DexFile::kDexVisibilitySystem);
+  const AnnotationItem* annotation_item = SearchAnnotationSet(
+      dex_file, annotation_set, "Ldalvik/annotation/Signature;", DexFile::DexVisibility::kSystem);
   if (annotation_item == nullptr) {
     return nullptr;
   }
@@ -906,9 +906,8 @@ ObjPtr<mirror::ObjectArray<mirror::Class>> GetThrowsValue(const ClassData& klass
                                                           const AnnotationSetItem* annotation_set)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   const DexFile& dex_file = klass.GetDexFile();
-  const AnnotationItem* annotation_item =
-      SearchAnnotationSet(dex_file, annotation_set, "Ldalvik/annotation/Throws;",
-                          DexFile::kDexVisibilitySystem);
+  const AnnotationItem* annotation_item = SearchAnnotationSet(
+      dex_file, annotation_set, "Ldalvik/annotation/Throws;", DexFile::DexVisibility::kSystem);
   if (annotation_item == nullptr) {
     return nullptr;
   }
@@ -928,7 +927,7 @@ ObjPtr<mirror::ObjectArray<mirror::Class>> GetThrowsValue(const ClassData& klass
 ObjPtr<mirror::ObjectArray<mirror::Object>> ProcessAnnotationSet(
     const ClassData& klass,
     const AnnotationSetItem* annotation_set,
-    uint32_t visibility)
+    DexFile::DexVisibility visibility)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   const DexFile& dex_file = klass.GetDexFile();
   Thread* self = Thread::Current();
@@ -951,7 +950,7 @@ ObjPtr<mirror::ObjectArray<mirror::Object>> ProcessAnnotationSet(
     const AnnotationItem* annotation_item = dex_file.GetAnnotationItem(annotation_set, i);
     // Note that we do not use IsVisibilityCompatible here because older code
     // was correct for this case.
-    if (annotation_item->visibility_ != visibility) {
+    if (static_cast<DexFile::DexVisibility>(annotation_item->visibility_) != visibility) {
       continue;
     }
     const uint8_t* annotation = annotation_item->annotation_;
@@ -1006,9 +1005,8 @@ ObjPtr<mirror::ObjectArray<mirror::Object>> ProcessAnnotationSetRefList(
   for (uint32_t index = 0; index < size; ++index) {
     const AnnotationSetRefItem* set_ref_item = &set_ref_list->list_[index];
     const AnnotationSetItem* set_item = dex_file.GetSetRefItemItem(set_ref_item);
-    ObjPtr<mirror::Object> annotation_set = ProcessAnnotationSet(klass,
-                                                                 set_item,
-                                                                 DexFile::kDexVisibilityRuntime);
+    ObjPtr<mirror::Object> annotation_set =
+        ProcessAnnotationSet(klass, set_item, DexFile::DexVisibility::kRuntime);
     if (annotation_set == nullptr) {
       return nullptr;
     }
@@ -1028,17 +1026,15 @@ ObjPtr<mirror::Object> GetAnnotationForField(ArtField* field,
   }
   StackHandleScope<1> hs(Thread::Current());
   const ClassData field_class(hs, field);
-  return GetAnnotationObjectFromAnnotationSet(field_class,
-                                              annotation_set,
-                                              DexFile::kDexVisibilityRuntime,
-                                              annotation_class);
+  return GetAnnotationObjectFromAnnotationSet(
+      field_class, annotation_set, DexFile::DexVisibility::kRuntime, annotation_class);
 }
 
 ObjPtr<mirror::ObjectArray<mirror::Object>> GetAnnotationsForField(ArtField* field) {
   const AnnotationSetItem* annotation_set = FindAnnotationSetForField(field);
   StackHandleScope<1> hs(Thread::Current());
   const ClassData field_class(hs, field);
-  return ProcessAnnotationSet(field_class, annotation_set, DexFile::kDexVisibilityRuntime);
+  return ProcessAnnotationSet(field_class, annotation_set, DexFile::DexVisibility::kRuntime);
 }
 
 ObjPtr<mirror::ObjectArray<mirror::String>> GetSignatureAnnotationForField(ArtField* field) {
@@ -1059,7 +1055,7 @@ bool IsFieldAnnotationPresent(ArtField* field, Handle<mirror::Class> annotation_
   StackHandleScope<1> hs(Thread::Current());
   const ClassData field_class(hs, field);
   const AnnotationItem* annotation_item = GetAnnotationItemFromAnnotationSet(
-      field_class, annotation_set, DexFile::kDexVisibilityRuntime, annotation_class);
+      field_class, annotation_set, DexFile::DexVisibility::kRuntime, annotation_class);
   return annotation_item != nullptr;
 }
 
@@ -1076,8 +1072,11 @@ ObjPtr<mirror::Object> GetAnnotationDefaultValue(ArtMethod* method) {
   if (annotation_set == nullptr) {
     return nullptr;
   }
-  const AnnotationItem* annotation_item = SearchAnnotationSet(*dex_file, annotation_set,
-      "Ldalvik/annotation/AnnotationDefault;", DexFile::kDexVisibilitySystem);
+  const AnnotationItem* annotation_item =
+      SearchAnnotationSet(*dex_file,
+                          annotation_set,
+                          "Ldalvik/annotation/AnnotationDefault;",
+                          DexFile::DexVisibility::kSystem);
   if (annotation_item == nullptr) {
     return nullptr;
   }
@@ -1113,15 +1112,13 @@ ObjPtr<mirror::Object> GetAnnotationForMethod(ArtMethod* method,
   if (annotation_set == nullptr) {
     return nullptr;
   }
-  return GetAnnotationObjectFromAnnotationSet(ClassData(method), annotation_set,
-                                              DexFile::kDexVisibilityRuntime, annotation_class);
+  return GetAnnotationObjectFromAnnotationSet(
+      ClassData(method), annotation_set, DexFile::DexVisibility::kRuntime, annotation_class);
 }
 
 ObjPtr<mirror::ObjectArray<mirror::Object>> GetAnnotationsForMethod(ArtMethod* method) {
   const AnnotationSetItem* annotation_set = FindAnnotationSetForMethod(method);
-  return ProcessAnnotationSet(ClassData(method),
-                              annotation_set,
-                              DexFile::kDexVisibilityRuntime);
+  return ProcessAnnotationSet(ClassData(method), annotation_set, DexFile::DexVisibility::kRuntime);
 }
 
 ObjPtr<mirror::ObjectArray<mirror::Class>> GetExceptionTypesForMethod(ArtMethod* method) {
@@ -1185,10 +1182,8 @@ ObjPtr<mirror::Object> GetAnnotationForMethodParameter(ArtMethod* method,
   if (annotation_set == nullptr) {
     return nullptr;
   }
-  return GetAnnotationObjectFromAnnotationSet(ClassData(method),
-                                              annotation_set,
-                                              DexFile::kDexVisibilityRuntime,
-                                              annotation_class);
+  return GetAnnotationObjectFromAnnotationSet(
+      ClassData(method), annotation_set, DexFile::DexVisibility::kRuntime, annotation_class);
 }
 
 bool GetParametersMetadataForMethod(
@@ -1206,7 +1201,7 @@ bool GetParametersMetadataForMethod(
       SearchAnnotationSet(*dex_file,
                           annotation_set,
                           "Ldalvik/annotation/MethodParameters;",
-                          DexFile::kDexVisibilitySystem);
+                          DexFile::DexVisibility::kSystem);
   if (annotation_item == nullptr) {
     return false;
   }
@@ -1256,9 +1251,10 @@ ObjPtr<mirror::ObjectArray<mirror::String>> GetSignatureAnnotationForMethod(ArtM
   return GetSignatureValue(ClassData(method), annotation_set);
 }
 
-bool IsMethodAnnotationPresent(ArtMethod* method,
-                               Handle<mirror::Class> annotation_class,
-                               uint32_t visibility /* = DexFile::kDexVisibilityRuntime */) {
+bool IsMethodAnnotationPresent(
+    ArtMethod* method,
+    Handle<mirror::Class> annotation_class,
+    DexFile::DexVisibility visibility /* = DexFile::DexVisibility::kRuntime */) {
   const AnnotationSetItem* annotation_set = FindAnnotationSetForMethod(method);
   if (annotation_set == nullptr) {
     return false;
@@ -1289,7 +1285,8 @@ static bool IsMethodBuildAnnotationPresent(const DexFile& dex_file,
                                            jclass annotation_class) {
   for (uint32_t i = 0; i < annotation_set.size_; ++i) {
     const AnnotationItem* annotation_item = dex_file.GetAnnotationItem(&annotation_set, i);
-    if (!IsVisibilityCompatible(annotation_item->visibility_, DexFile::kDexVisibilityBuild)) {
+    if (!IsVisibilityCompatible(static_cast<DexFile::DexVisibility>(annotation_item->visibility_),
+                                DexFile::DexVisibility::kBuild)) {
       continue;
     }
     const uint8_t* annotation = annotation_item->annotation_;
@@ -1390,8 +1387,11 @@ bool FieldIsReachabilitySensitive(const DexFile& dex_file,
   if (annotation_set == nullptr) {
     return false;
   }
-  const AnnotationItem* annotation_item = SearchAnnotationSet(dex_file, annotation_set,
-      "Ldalvik/annotation/optimization/ReachabilitySensitive;", DexFile::kDexVisibilityRuntime);
+  const AnnotationItem* annotation_item =
+      SearchAnnotationSet(dex_file,
+                          annotation_set,
+                          "Ldalvik/annotation/optimization/ReachabilitySensitive;",
+                          DexFile::DexVisibility::kRuntime);
   // TODO: We're missing the equivalent of DCheckNativeAnnotation (not a DCHECK). Does it matter?
   return annotation_item != nullptr;
 }
@@ -1405,8 +1405,11 @@ bool MethodIsReachabilitySensitive(const DexFile& dex_file,
   if (annotation_set == nullptr) {
     return false;
   }
-  const AnnotationItem* annotation_item = SearchAnnotationSet(dex_file, annotation_set,
-      "Ldalvik/annotation/optimization/ReachabilitySensitive;", DexFile::kDexVisibilityRuntime);
+  const AnnotationItem* annotation_item =
+      SearchAnnotationSet(dex_file,
+                          annotation_set,
+                          "Ldalvik/annotation/optimization/ReachabilitySensitive;",
+                          DexFile::DexVisibility::kRuntime);
   return annotation_item != nullptr;
 }
 
@@ -1523,8 +1526,11 @@ bool HasDeadReferenceSafeAnnotation(const DexFile& dex_file,
   if (annotation_set == nullptr) {
     return false;
   }
-  const AnnotationItem* annotation_item = SearchAnnotationSet(dex_file, annotation_set,
-      "Ldalvik/annotation/optimization/DeadReferenceSafe;", DexFile::kDexVisibilityRuntime);
+  const AnnotationItem* annotation_item =
+      SearchAnnotationSet(dex_file,
+                          annotation_set,
+                          "Ldalvik/annotation/optimization/DeadReferenceSafe;",
+                          DexFile::DexVisibility::kRuntime);
   return annotation_item != nullptr;
 }
 
@@ -1535,16 +1541,14 @@ ObjPtr<mirror::Object> GetAnnotationForClass(Handle<mirror::Class> klass,
   if (annotation_set == nullptr) {
     return nullptr;
   }
-  return GetAnnotationObjectFromAnnotationSet(data,
-                                              annotation_set,
-                                              DexFile::kDexVisibilityRuntime,
-                                              annotation_class);
+  return GetAnnotationObjectFromAnnotationSet(
+      data, annotation_set, DexFile::DexVisibility::kRuntime, annotation_class);
 }
 
 ObjPtr<mirror::ObjectArray<mirror::Object>> GetAnnotationsForClass(Handle<mirror::Class> klass) {
   ClassData data(klass);
   const AnnotationSetItem* annotation_set = FindAnnotationSetForClass(data);
-  return ProcessAnnotationSet(data, annotation_set, DexFile::kDexVisibilityRuntime);
+  return ProcessAnnotationSet(data, annotation_set, DexFile::DexVisibility::kRuntime);
 }
 
 ObjPtr<mirror::ObjectArray<mirror::Class>> GetDeclaredClasses(Handle<mirror::Class> klass) {
@@ -1559,9 +1563,10 @@ ObjPtr<mirror::Class> GetDeclaringClass(Handle<mirror::Class> klass) {
   if (annotation_set == nullptr) {
     return nullptr;
   }
-  const AnnotationItem* annotation_item =
-      SearchAnnotationSet(data.GetDexFile(), annotation_set, "Ldalvik/annotation/EnclosingClass;",
-                          DexFile::kDexVisibilitySystem);
+  const AnnotationItem* annotation_item = SearchAnnotationSet(data.GetDexFile(),
+                                                              annotation_set,
+                                                              "Ldalvik/annotation/EnclosingClass;",
+                                                              DexFile::DexVisibility::kSystem);
   if (annotation_item == nullptr) {
     return nullptr;
   }
@@ -1591,11 +1596,10 @@ ObjPtr<mirror::Class> GetEnclosingClass(Handle<mirror::Class> klass) {
   if (annotation_set == nullptr) {
     return nullptr;
   }
-  const AnnotationItem* annotation_item =
-      SearchAnnotationSet(data.GetDexFile(),
-                          annotation_set,
-                          "Ldalvik/annotation/EnclosingMethod;",
-                          DexFile::kDexVisibilitySystem);
+  const AnnotationItem* annotation_item = SearchAnnotationSet(data.GetDexFile(),
+                                                              annotation_set,
+                                                              "Ldalvik/annotation/EnclosingMethod;",
+                                                              DexFile::DexVisibility::kSystem);
   if (annotation_item == nullptr) {
     return nullptr;
   }
@@ -1632,11 +1636,10 @@ ObjPtr<mirror::Object> GetEnclosingMethod(Handle<mirror::Class> klass) {
   if (annotation_set == nullptr) {
     return nullptr;
   }
-  const AnnotationItem* annotation_item =
-      SearchAnnotationSet(data.GetDexFile(),
-                          annotation_set,
-                          "Ldalvik/annotation/EnclosingMethod;",
-                          DexFile::kDexVisibilitySystem);
+  const AnnotationItem* annotation_item = SearchAnnotationSet(data.GetDexFile(),
+                                                              annotation_set,
+                                                              "Ldalvik/annotation/EnclosingMethod;",
+                                                              DexFile::DexVisibility::kSystem);
   if (annotation_item == nullptr) {
     return nullptr;
   }
@@ -1650,11 +1653,10 @@ bool GetInnerClass(Handle<mirror::Class> klass, /*out*/ ObjPtr<mirror::String>*
   if (annotation_set == nullptr) {
     return false;
   }
-  const AnnotationItem* annotation_item = SearchAnnotationSet(
-      data.GetDexFile(),
-      annotation_set,
-      "Ldalvik/annotation/InnerClass;",
-      DexFile::kDexVisibilitySystem);
+  const AnnotationItem* annotation_item = SearchAnnotationSet(data.GetDexFile(),
+                                                              annotation_set,
+                                                              "Ldalvik/annotation/InnerClass;",
+                                                              DexFile::DexVisibility::kSystem);
   if (annotation_item == nullptr) {
     return false;
   }
@@ -1685,9 +1687,10 @@ bool GetInnerClassFlags(Handle<mirror::Class> klass, uint32_t* flags) {
   if (annotation_set == nullptr) {
     return false;
   }
-  const AnnotationItem* annotation_item =
-      SearchAnnotationSet(data.GetDexFile(), annotation_set, "Ldalvik/annotation/InnerClass;",
-                          DexFile::kDexVisibilitySystem);
+  const AnnotationItem* annotation_item = SearchAnnotationSet(data.GetDexFile(),
+                                                              annotation_set,
+                                                              "Ldalvik/annotation/InnerClass;",
+                                                              DexFile::DexVisibility::kSystem);
   if (annotation_item == nullptr) {
     return false;
   }
@@ -1736,11 +1739,11 @@ const char* GetSourceDebugExtension(Handle<mirror::Class> klass) {
     return nullptr;
   }
 
-  const AnnotationItem* annotation_item = SearchAnnotationSet(
-      data.GetDexFile(),
-      annotation_set,
-      "Ldalvik/annotation/SourceDebugExtension;",
-      DexFile::kDexVisibilitySystem);
+  const AnnotationItem* annotation_item =
+      SearchAnnotationSet(data.GetDexFile(),
+                          annotation_set,
+                          "Ldalvik/annotation/SourceDebugExtension;",
+                          DexFile::DexVisibility::kSystem);
   if (annotation_item == nullptr) {
     return nullptr;
   }
@@ -1771,9 +1774,10 @@ ObjPtr<mirror::Class> GetNestHost(Handle<mirror::Class> klass) {
   if (annotation_set == nullptr) {
     return nullptr;
   }
-  const AnnotationItem* annotation_item =
-      SearchAnnotationSet(data.GetDexFile(), annotation_set, "Ldalvik/annotation/NestHost;",
-                          DexFile::kDexVisibilitySystem);
+  const AnnotationItem* annotation_item = SearchAnnotationSet(data.GetDexFile(),
+                                                              annotation_set,
+                                                              "Ldalvik/annotation/NestHost;",
+                                                              DexFile::DexVisibility::kSystem);
   if (annotation_item == nullptr) {
     return nullptr;
   }
@@ -1815,7 +1819,7 @@ ObjPtr<mirror::Object> getRecordAnnotationElement(Handle<mirror::Class> klass,
     return nullptr;
   }
   const AnnotationItem* annotation_item = SearchAnnotationSet(
-      dex_file, annotation_set, "Ldalvik/annotation/Record;", DexFile::kDexVisibilitySystem);
+      dex_file, annotation_set, "Ldalvik/annotation/Record;", DexFile::DexVisibility::kSystem);
   if (annotation_item == nullptr) {
     return nullptr;
   }
@@ -1849,7 +1853,7 @@ bool IsClassAnnotationPresent(Handle<mirror::Class> klass, Handle<mirror::Class>
     return false;
   }
   const AnnotationItem* annotation_item = GetAnnotationItemFromAnnotationSet(
-      data, annotation_set, DexFile::kDexVisibilityRuntime, annotation_class);
+      data, annotation_set, DexFile::DexVisibility::kRuntime, annotation_class);
   return annotation_item != nullptr;
 }
 
@@ -2019,7 +2023,8 @@ void VisitClassAnnotations(Handle<mirror::Class> klass, AnnotationVisitor* visit
   const DexFile& dex_file = data.GetDexFile();
   for (uint32_t i = 0; i < annotation_set->size_; ++i) {
     const AnnotationItem* annotation_item = dex_file.GetAnnotationItem(annotation_set, i);
-    uint8_t visibility = annotation_item->visibility_;
+    DexFile::DexVisibility visibility =
+        static_cast<DexFile::DexVisibility>(annotation_item->visibility_);
     const uint8_t* annotation = annotation_item->annotation_;
     uint32_t type_index = DecodeUnsignedLeb128(&annotation);
     const char* annotation_descriptor = dex_file.GetTypeDescriptor(dex::TypeIndex(type_index));
diff --git a/runtime/dex/dex_file_annotations.h b/runtime/dex/dex_file_annotations.h
index 29267b89b8..0fb9cea3a7 100644
--- a/runtime/dex/dex_file_annotations.h
+++ b/runtime/dex/dex_file_annotations.h
@@ -76,7 +76,7 @@ EXPORT ObjPtr<mirror::ObjectArray<mirror::String>> GetSignatureAnnotationForMeth
 // side effect.
 bool IsMethodAnnotationPresent(ArtMethod* method,
                                Handle<mirror::Class> annotation_class,
-                               uint32_t visibility = DexFile::kDexVisibilityRuntime)
+                               DexFile::DexVisibility visibility = DexFile::DexVisibility::kRuntime)
     REQUIRES_SHARED(Locks::mutator_lock_);
 
 // Check whether a method from the `dex_file` with the given `method_index`
@@ -195,7 +195,8 @@ enum class VisitorStatus : uint8_t { kVisitBreak, kVisitNext, kVisitInner };
 class AnnotationVisitor {
  public:
   virtual ~AnnotationVisitor() {}
-  virtual VisitorStatus VisitAnnotation(const char* annotation_descriptor, uint8_t visibility) = 0;
+  virtual VisitorStatus VisitAnnotation(const char* annotation_descriptor,
+                                        DexFile::DexVisibility visibility) = 0;
   virtual VisitorStatus VisitAnnotationElement(const char* element_name,
                                                uint8_t type,
                                                const JValue& value) = 0;
diff --git a/runtime/entrypoints/entrypoint_utils.cc b/runtime/entrypoints/entrypoint_utils.cc
index 8e20974111..804194b519 100644
--- a/runtime/entrypoints/entrypoint_utils.cc
+++ b/runtime/entrypoints/entrypoint_utils.cc
@@ -145,16 +145,9 @@ JValue InvokeProxyInvocationHandler(ScopedObjectAccessAlreadyRunnable& soa,
         ObjPtr<mirror::Object> rcvr = soa.Decode<mirror::Object>(rcvr_jobj);
         ObjPtr<mirror::Class> proxy_class = rcvr->GetClass();
         ObjPtr<mirror::Method> interface_method = soa.Decode<mirror::Method>(interface_method_jobj);
-        ArtMethod* proxy_method = rcvr->GetClass()->FindVirtualMethodForInterface(
+        ArtMethod* proxy_method = proxy_class->FindVirtualMethodForInterface(
             interface_method->GetArtMethod(), kRuntimePointerSize);
-        auto virtual_methods = proxy_class->GetVirtualMethodsSlice(kRuntimePointerSize);
-        size_t num_virtuals = proxy_class->NumVirtualMethods();
-        size_t method_size = ArtMethod::Size(kRuntimePointerSize);
-        // Rely on the fact that the methods are contiguous to determine the index of the method in
-        // the slice.
-        int throws_index = (reinterpret_cast<uintptr_t>(proxy_method) -
-            reinterpret_cast<uintptr_t>(&virtual_methods[0])) / method_size;
-        CHECK_LT(throws_index, static_cast<int>(num_virtuals));
+        int throws_index = proxy_class->GetProxyThrowsIndex(proxy_method);
         ObjPtr<mirror::ObjectArray<mirror::Class>> declared_exceptions =
             proxy_class->GetProxyThrows()->Get(throws_index);
         ObjPtr<mirror::Class> exception_class = exception->GetClass();
diff --git a/runtime/entrypoints/quick/quick_trampoline_entrypoints.cc b/runtime/entrypoints/quick/quick_trampoline_entrypoints.cc
index f7f3673259..628172dd1e 100644
--- a/runtime/entrypoints/quick/quick_trampoline_entrypoints.cc
+++ b/runtime/entrypoints/quick/quick_trampoline_entrypoints.cc
@@ -793,6 +793,14 @@ extern "C" uint64_t artQuickToInterpreterBridge(ArtMethod* method, Thread* self,
     ShadowFrameAllocaUniquePtr shadow_frame_unique_ptr =
         CREATE_SHADOW_FRAME(num_regs, method, /* dex_pc= */ 0);
     ShadowFrame* shadow_frame = shadow_frame_unique_ptr.get();
+
+    // Restore the values of virtual registers if a virtual thread is unparking
+    if (kIsVirtualThreadEnabled && self->IsVirtualThreadUnparking()) {
+      interpreter::FillVirtualThreadFrame(self, shadow_frame);
+    }
+    // TODO: Consider skip the following operations, e.g. copying registers, if
+    //   a virtual thread is unparking.
+
     size_t first_arg_reg = accessor.RegistersSize() - accessor.InsSize();
     BuildQuickShadowFrameVisitor shadow_frame_builder(
         sp, method->IsStatic(), shorty, shadow_frame, first_arg_reg);
diff --git a/runtime/fault_handler.cc b/runtime/fault_handler.cc
index 881dbadd0c..9c66747114 100644
--- a/runtime/fault_handler.cc
+++ b/runtime/fault_handler.cc
@@ -16,6 +16,7 @@
 
 #include "fault_handler.h"
 
+#include <signal.h>
 #include <string.h>
 #include <sys/mman.h>
 #include <sys/ucontext.h>
@@ -52,6 +53,10 @@ extern "C" NO_INLINE __attribute__((visibility("default"))) void art_sigbus_faul
   // Set a breakpoint here to be informed when a SIGBUS is unhandled by ART.
   VLOG(signals) << "Caught unknown SIGBUS in ART fault handler - chaining to next handler.";
 }
+extern "C" NO_INLINE __attribute__((visibility("default"))) void art_sigsys_fault() {
+  // Set a breakpoint here to be informed when a SIGSYS is unhandled by ART.
+  VLOG(signals) << "Caught unknown SIGSYS in ART fault handler - chaining to next handler.";
+}
 
 // Signal handler called on SIGSEGV.
 static bool art_sigsegv_handler(int sig, siginfo_t* info, void* context) {
@@ -63,9 +68,15 @@ static bool art_sigbus_handler(int sig, siginfo_t* info, void* context) {
   return fault_manager.HandleSigbusFault(sig, info, context);
 }
 
+// Signal handler called on SIGSYS.
+static bool art_sigsys_handler(int sig, siginfo_t* info, void* context) {
+  return fault_manager.HandleSigsysFault(sig, info, context);
+}
+
 FaultManager::FaultManager()
     : generated_code_ranges_lock_("FaultHandler generated code ranges lock",
                                   LockLevel::kGenericBottomLock),
+      mark_compact_(nullptr),
       initialized_(false) {}
 
 FaultManager::~FaultManager() {
@@ -87,6 +98,13 @@ static const char* SignalCodeName(int sig, int code) {
       case BUS_OBJERR: return "BUS_OBJERR";
       default:         return "BUS_UNKNOWN";
     }
+  } else if (sig == SIGSYS) {
+    switch (code) {
+      case SYS_SECCOMP:
+        return "SYS_SECCOMP";
+      default:
+        return "SYS_UNKNOWN";
+    }
   } else {
     return "UNKNOWN";
   }
@@ -98,12 +116,18 @@ static std::ostream& PrintSignalInfo(std::ostream& os, siginfo_t* info) {
      << " (" << SignalCodeName(info->si_signo, info->si_code) << ")";
   if (info->si_signo == SIGSEGV || info->si_signo == SIGBUS) {
     os << "\n" << "  si_addr: " << info->si_addr;
+  } else if (info->si_signo == SIGSYS) {
+    os << "\n" << " si_syscall: " << info->si_syscall;
   }
   return os;
 }
 
 void FaultManager::Init(bool use_sig_chain) {
   CHECK(!initialized_);
+  if (gUseUserfaultfd) {
+    mark_compact_ = Runtime::Current()->GetHeap()->MarkCompactCollector();
+    CHECK_NE(mark_compact_, nullptr);
+  }
   if (use_sig_chain) {
     sigset_t mask;
     sigfillset(&mask);
@@ -123,6 +147,9 @@ void FaultManager::Init(bool use_sig_chain) {
     if (gUseUserfaultfd) {
       sa.sc_sigaction = art_sigbus_handler;
       AddSpecialSignalHandlerFn(SIGBUS, &sa);
+
+      sa.sc_sigaction = art_sigsys_handler;
+      AddSpecialSignalHandlerFn(SIGSYS, &sa);
     }
 
     // Notify the kernel that we intend to use a specific `membarrier()` command.
@@ -170,6 +197,7 @@ void FaultManager::Release() {
     RemoveSpecialSignalHandlerFn(SIGSEGV, art_sigsegv_handler);
     if (gUseUserfaultfd) {
       RemoveSpecialSignalHandlerFn(SIGBUS, art_sigbus_handler);
+      RemoveSpecialSignalHandlerFn(SIGSYS, art_sigsys_handler);
     }
     initialized_ = false;
   }
@@ -221,6 +249,36 @@ bool FaultManager::HandleFaultByOtherHandlers(int sig, siginfo_t* info, void* co
   return false;
 }
 
+bool FaultManager::HandleSigsysFault(int sig, siginfo_t* info, void* context) {
+  DCHECK_EQ(sig, SIGSYS);
+  if (VLOG_IS_ON(signals)) {
+    PrintSignalInfo(VLOG_STREAM(signals) << "Handling SIGSYS fault:\n", info);
+  }
+
+#ifdef TEST_NESTED_SIGNAL
+  // Simulate a crash in a handler.
+  raise(SIGSYS);
+#endif
+  if (mark_compact_->SigsysHandler(info, context)) {
+    return true;
+  }
+
+  // Set a breakpoint in this function to catch unhandled signals.
+  art_sigsys_fault();
+  return false;
+}
+
+static inline void MaybeSuspendFaster([[maybe_unused]] FaultManager* fm,
+                                      [[maybe_unused]] siginfo_t* info,
+                                      [[maybe_unused]] void* context) {
+#ifdef __aarch64__
+  Thread* self = Thread::Current();
+  if (self != nullptr && self->IsSuspendTriggerSet()) {
+    fm->SuspendFaster(info, context);
+  }
+#endif
+}
+
 bool FaultManager::HandleSigbusFault(int sig, siginfo_t* info, [[maybe_unused]] void* context) {
   DCHECK_EQ(sig, SIGBUS);
   if (VLOG_IS_ON(signals)) {
@@ -231,10 +289,10 @@ bool FaultManager::HandleSigbusFault(int sig, siginfo_t* info, [[maybe_unused]]
   // Simulate a crash in a handler.
   raise(SIGBUS);
 #endif
-  if (Runtime::Current()->GetHeap()->MarkCompactCollector()->SigbusHandler(info)) {
+  if (mark_compact_->SigbusHandler(info)) {
+    MaybeSuspendFaster(this, info, context);
     return true;
   }
-
   // Set a breakpoint in this function to catch unhandled signals.
   art_sigbus_fault();
   return false;
diff --git a/runtime/fault_handler.h b/runtime/fault_handler.h
index 4d9c3f5271..8cbcf5168a 100644
--- a/runtime/fault_handler.h
+++ b/runtime/fault_handler.h
@@ -34,6 +34,12 @@ namespace art HIDDEN {
 class ArtMethod;
 class FaultHandler;
 
+namespace gc {
+namespace collector {
+class MarkCompact;
+}  // namespace collector
+}  // namespace gc
+
 class FaultManager {
  public:
   FaultManager();
@@ -55,6 +61,9 @@ class FaultManager {
   // Try to handle a SIGBUS fault, returns true if successful.
   bool HandleSigbusFault(int sig, siginfo_t* info, void* context);
 
+  // Try to handle a SIGSYS fault, returns true if successful.
+  bool HandleSigsysFault(int sig, siginfo_t* info, void* context);
+
   // Added handlers are owned by the fault handler and will be freed on Shutdown().
   EXPORT void AddHandler(FaultHandler* handler, bool generated_code);
   EXPORT void RemoveHandler(FaultHandler* handler);
@@ -75,6 +84,14 @@ class FaultManager {
   // Called in the context of a signal handler.
   bool IsInGeneratedCode(siginfo_t* siginfo, void *context) NO_THREAD_SAFETY_ANALYSIS;
 
+#ifdef __aarch64__
+  // Update context to cause pending thread suspension request to be recognized
+  // more quickly upon return from signal handler, if that is possible.
+  void SuspendFaster(siginfo_t* info, void* context);
+#else
+  void SuspendFaster(siginfo_t*, void*) {}
+#endif
+
  private:
   struct GeneratedCodeRange {
     std::atomic<GeneratedCodeRange*> next;
@@ -104,6 +121,7 @@ class FaultManager {
 
   std::vector<FaultHandler*> generated_code_handlers_;
   std::vector<FaultHandler*> other_handlers_;
+  gc::collector::MarkCompact* mark_compact_;
   bool initialized_;
 
   // We keep a certain number of generated code ranges locally to avoid too many
diff --git a/runtime/gc/accounting/card_table.h b/runtime/gc/accounting/card_table.h
index 98ff107baf..16e8f89850 100644
--- a/runtime/gc/accounting/card_table.h
+++ b/runtime/gc/accounting/card_table.h
@@ -140,16 +140,16 @@ class CardTable {
               uint8_t* scan_end,
               const Visitor& visitor,
               const ModifyVisitor& mod_visitor,
-              const uint8_t minimum_age) REQUIRES(Locks::heap_bitmap_lock_)
-      REQUIRES_SHARED(Locks::mutator_lock_);
+              const uint8_t minimum_age)
+      REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_);
 
   template <bool kClearCard, typename Visitor>
   size_t Scan(SpaceBitmap<kObjectAlignment>* bitmap,
               uint8_t* scan_begin,
               uint8_t* scan_end,
               const Visitor& visitor,
-              const uint8_t minimum_age = kCardDirty) REQUIRES(Locks::heap_bitmap_lock_)
-      REQUIRES_SHARED(Locks::mutator_lock_) {
+              const uint8_t minimum_age = kCardDirty)
+      REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_) {
     return Scan<kClearCard>(bitmap, scan_begin, scan_end, visitor, VoidFunctor(), minimum_age);
   }
 
diff --git a/runtime/gc/collector/concurrent_copying.cc b/runtime/gc/collector/concurrent_copying.cc
index f666a8f6cc..59caaed43c 100644
--- a/runtime/gc/collector/concurrent_copying.cc
+++ b/runtime/gc/collector/concurrent_copying.cc
@@ -3736,8 +3736,7 @@ void ConcurrentCopying::FinishPhase() {
   }
 }
 
-bool ConcurrentCopying::IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* field,
-                                                    bool do_atomic_update) {
+bool ConcurrentCopying::IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* field) {
   mirror::Object* from_ref = field->AsMirrorPtr();
   if (from_ref == nullptr) {
     return true;
@@ -3747,17 +3746,15 @@ bool ConcurrentCopying::IsNullOrMarkedHeapReference(mirror::HeapReference<mirror
     return false;
   }
   if (from_ref != to_ref) {
-    if (do_atomic_update) {
-      do {
-        if (field->AsMirrorPtr() != from_ref) {
-          // Concurrently overwritten by a mutator.
-          break;
-        }
-      } while (!field->CasWeakRelaxed(from_ref, to_ref));
-      // See comment in MarkHeapReference() for memory ordering.
-    } else {
-      field->Assign(to_ref);
+    // We have to update it while it may be concurrently overwritten by the mutator.
+    // If the mutator overwrites it, we're done.
+    while (UNLIKELY(!field->CasWeakRelaxed(from_ref, to_ref))) {
+      if (field->AsMirrorPtr() != from_ref) {
+        // Concurrently overwritten by a mutator.
+        break;
+      }
     }
+    // See comment in MarkHeapReference() for memory ordering.
   }
   return true;
 }
diff --git a/runtime/gc/collector/concurrent_copying.h b/runtime/gc/collector/concurrent_copying.h
index 97d120e1a0..dad26b2e47 100644
--- a/runtime/gc/collector/concurrent_copying.h
+++ b/runtime/gc/collector/concurrent_copying.h
@@ -249,8 +249,7 @@ class ConcurrentCopying : public GarbageCollector {
       REQUIRES_SHARED(Locks::mutator_lock_);
   bool IsMarkedInNonMovingSpace(mirror::Object* from_ref)
       REQUIRES_SHARED(Locks::mutator_lock_);
-  bool IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* field,
-                                   bool do_atomic_update) override
+  bool IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* field) override
       REQUIRES_SHARED(Locks::mutator_lock_);
   void SweepSystemWeaks(Thread* self)
       REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(!Locks::heap_bitmap_lock_);
diff --git a/runtime/gc/collector/garbage_collector.cc b/runtime/gc/collector/garbage_collector.cc
index 14556c5a27..385309f844 100644
--- a/runtime/gc/collector/garbage_collector.cc
+++ b/runtime/gc/collector/garbage_collector.cc
@@ -87,7 +87,9 @@ void TraceGCMetric(const char* name, int64_t value) {
 }  // namespace
 
 Iteration::Iteration()
-    : duration_ns_(0), timings_("GC iteration timing logger", true, VLOG_IS_ON(heap)) {
+    : duration_ns_(0),
+      thread_cpu_time_ns_(0),
+      timings_("GC iteration timing logger", true, VLOG_IS_ON(heap)) {
   Reset(kGcCauseBackground, false);  // Reset to some place holder values.
 }
 
@@ -95,6 +97,7 @@ void Iteration::Reset(GcCause gc_cause, bool clear_soft_references) {
   timings_.Reset();
   pause_times_.clear();
   duration_ns_ = 0;
+  thread_cpu_time_ns_ = 0;
   app_slow_path_duration_ms_ = 0;
   bytes_scanned_ = 0;
   clear_soft_references_ = clear_soft_references;
@@ -106,7 +109,7 @@ void Iteration::Reset(GcCause gc_cause, bool clear_soft_references) {
 
 uint64_t Iteration::GetEstimatedThroughput() const {
   // Add 1ms to prevent possible division by 0.
-  return (static_cast<uint64_t>(freed_.bytes) * 1000) / (NsToMs(GetDurationNs()) + 1);
+  return (static_cast<uint64_t>(freed_.bytes) * 1000) / (NsToMs(GetThreadCpuTimeNs()) + 1);
 }
 
 GarbageCollector::GarbageCollector(Heap* heap, const std::string& name)
@@ -218,9 +221,11 @@ void GarbageCollector::Run(GcCause gc_cause, bool clear_soft_references) {
   freed_bytes_histogram_.AddValue(std::max<int64_t>(freed_bytes / KB, 0));
   uint64_t end_time = NanoTime();
   uint64_t thread_cpu_end_time = ThreadCpuNanoTime();
-  total_thread_cpu_time_ns_ += thread_cpu_end_time - thread_cpu_start_time;
+  uint64_t thread_cpu_time = thread_cpu_end_time - thread_cpu_start_time;
   uint64_t duration_ns = end_time - start_time;
+  total_thread_cpu_time_ns_ += thread_cpu_time;
   current_iteration->SetDurationNs(duration_ns);
+  current_iteration->SetThreadCpuTimeNs(thread_cpu_time);
   if (Locks::mutator_lock_->IsExclusiveHeld(self)) {
     // The entire GC was paused, clear the fake pauses which might be in the pause times and add
     // the whole GC duration.
@@ -394,7 +399,7 @@ void GarbageCollector::SweepArray(accounting::ObjectStack* allocations,
 
 uint64_t GarbageCollector::GetEstimatedMeanThroughput() const {
   // Add 1ms to prevent possible division by 0.
-  return (total_freed_bytes_ * 1000) / (NsToMs(GetCumulativeTimings().GetTotalNs()) + 1);
+  return (total_freed_bytes_ * 1000) / (NsToMs(GetTotalCpuTime()) + 1);
 }
 
 void GarbageCollector::ResetMeasurements() {
diff --git a/runtime/gc/collector/garbage_collector.h b/runtime/gc/collector/garbage_collector.h
index 1f697fd596..386ef6be6e 100644
--- a/runtime/gc/collector/garbage_collector.h
+++ b/runtime/gc/collector/garbage_collector.h
@@ -132,9 +132,8 @@ class GarbageCollector : public RootVisitor, public IsMarkedVisitor, public Mark
   virtual mirror::Object* IsMarked(mirror::Object* obj)
       REQUIRES_SHARED(Locks::mutator_lock_) = 0;
   // Returns true if the given heap reference is null or is already marked. If it's already marked,
-  // update the reference (uses a CAS if do_atomic_update is true). Otherwise, returns false.
-  virtual bool IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* obj,
-                                           bool do_atomic_update)
+  // update the reference, using a CAS if the GC requires it. Otherwise, returns false.
+  virtual bool IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* obj)
       REQUIRES_SHARED(Locks::mutator_lock_) = 0;
   // Used by reference processor.
   virtual void ProcessMarkStack() REQUIRES_SHARED(Locks::mutator_lock_) = 0;
diff --git a/runtime/gc/collector/iteration.h b/runtime/gc/collector/iteration.h
index 086ed541b0..82d637f363 100644
--- a/runtime/gc/collector/iteration.h
+++ b/runtime/gc/collector/iteration.h
@@ -46,6 +46,7 @@ class Iteration {
   uint64_t GetDurationNs() const {
     return duration_ns_;
   }
+  uint64_t GetThreadCpuTimeNs() const { return thread_cpu_time_ns_; }
   int64_t GetFreedBytes() const {
     return freed_.bytes;
   }
@@ -90,9 +91,12 @@ class Iteration {
     duration_ns_ = duration;
   }
 
+  void SetThreadCpuTimeNs(uint64_t time) { thread_cpu_time_ns_ = time; }
+
   GcCause gc_cause_;
   bool clear_soft_references_;
   uint64_t duration_ns_;
+  uint64_t thread_cpu_time_ns_;
   uint64_t app_slow_path_duration_ms_;
   uint64_t bytes_scanned_;
   TimingLogger timings_;
diff --git a/runtime/gc/collector/mark_compact-inl.h b/runtime/gc/collector/mark_compact-inl.h
index 70db85e657..c33afb913d 100644
--- a/runtime/gc/collector/mark_compact-inl.h
+++ b/runtime/gc/collector/mark_compact-inl.h
@@ -220,7 +220,6 @@ inline mirror::Object* MarkCompact::UpdateRef(mirror::Object* obj,
                  << " from_ref=" << from_ref
                  << " offset=" << offset
                  << " obj=" << obj
-                 << " obj-validity=" << IsValidObject(obj)
                  << " from-space=" << static_cast<void*>(from_space_begin_)
                  << " bitmap= " << moving_space_bitmap_->DumpMemAround(old_ref)
                  << " from_ref "
diff --git a/runtime/gc/collector/mark_compact.cc b/runtime/gc/collector/mark_compact.cc
index 82f5e1baf5..ddfe217a50 100644
--- a/runtime/gc/collector/mark_compact.cc
+++ b/runtime/gc/collector/mark_compact.cc
@@ -15,10 +15,6 @@
  */
 
 #include <fcntl.h>
-// Glibc v2.19 doesn't include these in fcntl.h so host builds will fail without.
-#if !defined(FALLOC_FL_PUNCH_HOLE) || !defined(FALLOC_FL_KEEP_SIZE)
-#include <linux/falloc.h>
-#endif
 #include <linux/userfaultfd.h>
 #include <poll.h>
 #include <sys/ioctl.h>
@@ -26,6 +22,7 @@
 #include <sys/resource.h>
 #include <sys/stat.h>
 #include <sys/syscall.h>
+#include <sys/utsname.h>
 #include <unistd.h>
 
 #include <fstream>
@@ -322,8 +319,12 @@ bool ShouldUseGenerationalGC() {
   // can pick a different values than zygote and will be able to execute.
   return GetBoolProperty("persist.device_config.runtime_native_boot.use_generational_gc", true);
 }
+static const bool gMoveIoctlRequested =
+    com::android::art::flags::use_uffd_move_ioctl() &&
+    GetBoolProperty("persist.device_config.runtime_native_boot.use_uffd_move_ioctl", true);
 #else
 bool ShouldUseGenerationalGC() { return true; }
+static const bool gMoveIoctlRequested = true;
 #endif
 
 namespace gc {
@@ -337,6 +338,8 @@ static constexpr bool kVerifyRootsMarked = kIsDebugBuild;
 static constexpr bool kVerifyNoMissingCardMarks = kIsDebugBuild;
 // Verify that all references in post-GC objects are valid.
 static constexpr bool kVerifyPostGCObjects = kIsDebugBuild;
+// Assert during marking that GC-roots are valid.
+static constexpr bool kVerifyGcRootDuringMarking = kIsDebugBuild;
 // Number of compaction buffers reserved for mutator threads in SIGBUS feature
 // case. It's extremely unlikely that we will ever have more than these number
 // of mutator threads trying to access the moving-space during one compaction
@@ -480,6 +483,7 @@ void YoungMarkCompact::RunPhases() {
 
 MarkCompact::MarkCompact(Heap* heap)
     : GarbageCollector(heap, "concurrent mark compact"),
+      overflow_arrays_(nullptr),
       gc_barrier_(0),
       lock_("mark compact lock", kGenericBottomLock),
       sigbus_in_progress_count_{kSigbusCounterCompactionDoneMask, kSigbusCounterCompactionDoneMask},
@@ -488,6 +492,7 @@ MarkCompact::MarkCompact(Heap* heap)
       post_compact_end_(nullptr),
       young_gen_(false),
       use_generational_(heap->GetUseGenerational()),
+      use_move_ioctl_(false),
       compacting_(false),
       moving_space_bitmap_(bump_pointer_space_->GetMarkBitmap()),
       moving_space_begin_(bump_pointer_space_->Begin()),
@@ -497,7 +502,8 @@ MarkCompact::MarkCompact(Heap* heap)
       uffd_(kFdUnused),
       marking_done_(false),
       uffd_initialized_(false),
-      clamp_info_map_status_(ClampInfoStatus::kClampInfoNotDone) {
+      clamp_info_map_status_(ClampInfoStatus::kClampInfoNotDone),
+      prev_moving_space_end_at_compaction_(moving_space_begin_) {
   if (kIsDebugBuild) {
     updated_roots_.reset(new std::unordered_set<void*>());
   }
@@ -558,12 +564,6 @@ MarkCompact::MarkCompact(Heap* heap)
   if (UNLIKELY(!compaction_buffers_map_.IsValid())) {
     LOG(FATAL) << "Failed to allocate concurrent mark-compact compaction buffers" << err_msg;
   }
-  // We also use the first page-sized buffer for the purpose of terminating concurrent compaction.
-  conc_compaction_termination_page_ = compaction_buffers_map_.Begin();
-  // Touch the page deliberately to avoid userfaults on it. We madvise it in
-  // CompactionPhase() before using it to terminate concurrent compaction.
-  ForceRead(conc_compaction_termination_page_);
-
   // In most of the cases, we don't expect more than one LinearAlloc space.
   linear_alloc_spaces_data_.reserve(1);
 
@@ -797,6 +797,8 @@ void MarkCompact::InitializePhase() {
   DCHECK_EQ(moving_space_begin_, bump_pointer_space_->Begin());
   from_space_slide_diff_ = from_space_begin_ - moving_space_begin_;
   moving_space_end_ = bump_pointer_space_->Limit();
+  last_reclaimable_page_.store(moving_space_end_, std::memory_order_relaxed);
+  cur_reclaimable_page_.store(moving_space_begin_, std::memory_order_relaxed);
   if (use_generational_ && !young_gen_) {
     class_after_obj_map_.clear();
   }
@@ -842,6 +844,25 @@ class MarkCompact::FlipCallback : public Closure {
   MarkCompact* const collector_;
 };
 
+// Traces the page faults incurred in the context of the GC thread. The 'Majflt' counter traces the
+// major faults i.e. all faults that had to bring a page into the memory from disk as well as
+// decompression from zram. The 'Minflt' counter traces all minor page faults(for eg. CoW and
+// anonymous page allocations). Since we only measure page faults hit by the GC thread, these
+// counters do not measure userfaults.
+void TraceFaults() {
+  if (!ATraceEnabled())
+    return;
+
+  struct rusage usage = {};
+
+  int ret = getrusage(RUSAGE_THREAD, &usage);
+  if (ret)
+    return;
+
+  ATraceIntegerValue("Minflt", usage.ru_minflt);
+  ATraceIntegerValue("Majflt", usage.ru_majflt);
+}
+
 void MarkCompact::RunPhases() {
   Thread* self = Thread::Current();
   thread_running_gc_ = self;
@@ -850,6 +871,7 @@ void MarkCompact::RunPhases() {
   InitializePhase();
   {
     ReaderMutexLock mu(self, *Locks::mutator_lock_);
+    TraceFaults();
     MarkingPhase();
   }
   {
@@ -860,6 +882,7 @@ void MarkCompact::RunPhases() {
       bump_pointer_space_->AssertAllThreadLocalBuffersAreRevoked();
     }
   }
+  TraceFaults();
   bool perform_compaction;
   {
     ReaderMutexLock mu(self, *Locks::mutator_lock_);
@@ -896,9 +919,10 @@ void MarkCompact::InitMovingSpaceFirstObjects(size_t vec_len, size_t to_space_pa
 
   // Find the first live word.
   size_t chunk_idx = to_space_page_idx * (gPageSize / kOffsetChunkSize);
-  DCHECK_LT(chunk_idx, vec_len);
+  CHECK_LT(chunk_idx, vec_len);
   // Find the first live word in the space
-  for (; chunk_info_vec_[chunk_idx] == 0; chunk_idx++) {
+  while (chunk_info_vec_[chunk_idx] == 0) {
+    chunk_idx++;
     if (chunk_idx >= vec_len) {
       // We don't have any live data on the moving-space.
       moving_first_objs_count_ = to_space_page_idx;
@@ -1028,6 +1052,93 @@ size_t MarkCompact::InitNonMovingFirstObjects(uintptr_t begin,
   return page_idx;
 }
 
+bool MarkCompact::MoveIoctlKernelCheck() {
+  DCHECK_GE(compaction_buffers_map_.Size(), 2 * gPageSize);
+  auto move_ioctl = [&](uint64_t additional_mode) {
+    uint8_t* buf = compaction_buffers_map_.Begin();
+    RegisterUffd(buf, gPageSize);
+    int ret = madvise(buf, gPageSize, MADV_DONTNEED);
+    CHECK(ret == 0) << "madvise failed: " << strerror(errno);
+    struct uffdio_move move_buf = {.dst = reinterpret_cast<uintptr_t>(buf),
+                                   .src = reinterpret_cast<uintptr_t>(buf) + gPageSize,
+                                   .len = gPageSize,
+                                   .mode = UFFDIO_MOVE_MODE_ALLOW_SRC_HOLES | additional_mode,
+                                   .move = 0};
+    // If the ioctl succeeds (indicated by 0 return value) then we know seccomp filter
+    // allows it and we can use MOVE. Otherwise, we fallback to using COPY ioctl.
+    bool success = (ioctl(uffd_, UFFDIO_MOVE, &move_buf) == 0);
+    if (success) {
+      DCHECK_EQ(move_buf.move, static_cast<ssize_t>(gPageSize));
+    }
+    UnregisterUffd(buf, gPageSize);
+    return success;
+  };
+
+  if ((gUffdFeatures & UFFD_FEATURE_MOVE) != 0 && gMoveIoctlRequested) {
+    // MOVE ioctl isn't available before 6.1 even on target devices.
+    DCHECK(IsKernelVersionAtLeast(6, 1));
+    static bool safe_to_use_move = [&]() {
+      // Handle the case of no lts in the release by initializing to 0.
+      int major, minor, lts = 0;
+      struct utsname uts;
+      int ret = uname(&uts);
+      DCHECK_EQ(ret, 0);
+      DCHECK_EQ(strcmp(uts.sysname, "Linux"), 0);
+      ret = sscanf(uts.release, "%d.%d.%d:", &major, &minor, &lts);
+      CHECK_GE(ret, 2);
+      CHECK_GE(major, 6);
+      if (kIsTargetAndroid) {
+        if (std::make_pair(major, minor) <= std::make_pair(6, 6)) {
+          // Special mode added in 6.1 and 6.6 kernels to confirm that MOVE
+          // ioctl bug-fixes are in the kernel. On these kernels on devices, the
+          // ioctl should succeed with this additional mode. If it fails then we
+          // don't use MOVE ioctl (See: https://r.android.com/3533441 and
+          // https://r.android.com/413428616).
+          size_t bit_shift;
+          switch (minor) {
+            case 1:
+              bit_shift = 62;
+              break;
+            case 6:
+              bit_shift = 63;
+              break;
+            default:
+              UNREACHABLE();
+          }
+          bool success = move_ioctl(1ull << bit_shift);
+          if (!success) {
+            // The ioctl should fail only because the kernel doesn't have the
+            // bug-fixes and therefore the additional mode is not recognized.
+            CHECK_EQ(errno, EINVAL);
+          }
+          return success;
+        }
+        return true;
+      } else {
+        return major > 6 || minor > 13 || (minor == 13 && lts > 7) || (minor == 12 && lts > 19);
+      }
+    }();
+
+    if (safe_to_use_move) {
+      if (Runtime::Current()->IsZygote()) {
+        // No need to check for zygote.
+        return true;
+      } else {
+        // Invoke the ioctl in the app to see if its seccomp filter allows
+        // MOVE ioctl or not. This will be done only once during the first
+        // GC after fork.
+        // TODO (b/398036867): remove this code once we are sure that app-compat
+        // issues are taken care of.
+        return move_ioctl(/*additional_mode=*/0);
+      }
+    } else {
+      return false;
+    }
+  } else {
+    return false;
+  }
+}
+
 // Generational CMC description
 // ============================
 //
@@ -1306,12 +1417,77 @@ bool MarkCompact::PrepareForCompaction() {
   // The chunk-info vector entries for the post marking-pause allocations will be
   // also updated in the pre-compaction pause.
 
-  if (!uffd_initialized_) {
-    CreateUserfaultfd(/*post_fork=*/false);
+  if (!uffd_initialized_ && CreateUserfaultfd(/*post_fork=*/false)) {
+    // Can we use MOVE ioctl from kernel bug-fixe and app seccomp pov.
+    use_move_ioctl_ = MoveIoctlKernelCheck();
+    if (!use_move_ioctl_) {
+      // TODO: add logic to also get reported on pitot as the below log
+      // message will get lost in the logcat.
+      LOG(WARNING) << "userfaultfd: MOVE ioctl seems unsupported: " << strerror(errno);
+    }
   }
   return true;
 }
 
+template <typename Visitor>
+class MarkCompact::VisitReferencesVisitor {
+ public:
+  explicit VisitReferencesVisitor(Visitor visitor) : visitor_(visitor) {}
+
+  ALWAYS_INLINE void operator()(mirror::Object* obj,
+                                MemberOffset offset,
+                                [[maybe_unused]] bool is_static) const
+      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
+    visitor_(obj->GetFieldObject<mirror::Object>(offset));
+  }
+
+  ALWAYS_INLINE void operator()([[maybe_unused]] ObjPtr<mirror::Class> klass,
+                                ObjPtr<mirror::Reference> ref) const
+      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
+    visitor_(ref.Ptr());
+  }
+
+  void VisitRootIfNonNull(mirror::CompressedReference<mirror::Object>* root) const
+      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
+    if (!root->IsNull()) {
+      VisitRoot(root);
+    }
+  }
+
+  void VisitRoot(mirror::CompressedReference<mirror::Object>* root) const
+      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
+    visitor_(root->AsMirrorPtr());
+  }
+
+ private:
+  Visitor visitor_;
+};
+
+void MarkCompact::VerifyNoMissingCardMarks() {
+  if (kVerifyNoMissingCardMarks) {
+    accounting::CardTable* card_table = heap_->GetCardTable();
+    for (const auto& space : heap_->GetContinuousSpaces()) {
+      auto obj_visitor = [&](mirror::Object* obj) {
+        VisitReferencesVisitor ref_visitor([&](mirror::Object* ref)
+            REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_) {
+          if (ref != nullptr && !IsMarked(ref)) {
+            CHECK(card_table->IsDirty(obj))
+                << "obj:" << obj << " (" << obj->PrettyTypeOf() << ") ref:" << ref
+                << " card:" << static_cast<int>(card_table->GetCard(obj))
+                << " space:" << space->GetName()
+                << " retention-policy:" << space->GetGcRetentionPolicy();
+          }
+        });
+        // We can't expect referent to hold the assertion.
+        obj->VisitReferences</*kVisitNativeRoots=*/true>(ref_visitor, VoidFunctor());
+      };
+      space->GetMarkBitmap()->VisitMarkedRange(reinterpret_cast<uintptr_t>(space->Begin()),
+                                               reinterpret_cast<uintptr_t>(space->End()),
+                                               obj_visitor);
+    }
+  }
+}
+
 class MarkCompact::VerifyRootMarkedVisitor : public SingleRootVisitor {
  public:
   explicit VerifyRootMarkedVisitor(MarkCompact* collector) : collector_(collector) { }
@@ -1348,6 +1524,7 @@ void MarkCompact::MarkingPause() {
   {
     // Handle the dirty objects as we are a concurrent GC
     WriterMutexLock mu(thread_running_gc_, *Locks::heap_bitmap_lock_);
+    VerifyNoMissingCardMarks();
     {
       MutexLock mu2(thread_running_gc_, *Locks::runtime_shutdown_lock_);
       MutexLock mu3(thread_running_gc_, *Locks::thread_list_lock_);
@@ -1540,6 +1717,11 @@ class MarkCompact::RefsUpdateVisitor {
         dirty_card_(dirty_card) {}
 
   bool ShouldDirtyCard() const { return dirty_card_; }
+  static consteval bool CheckBegin() { return kCheckBegin; }
+  static consteval bool CheckEnd() { return kCheckEnd; }
+  constexpr uint8_t* Begin() const { return begin_; }
+  constexpr uint8_t* End() const { return end_; }
+  constexpr uint8_t* Object() const { return reinterpret_cast<uint8_t*>(obj_); }
 
   void operator()([[maybe_unused]] mirror::Object* old,
                   MemberOffset offset,
@@ -1605,6 +1787,596 @@ class MarkCompact::RefsUpdateVisitor {
   mutable bool dirty_card_;
 };
 
+template <uint32_t kYieldMax = 5, uint64_t kSleepUs = 10>
+static void BackOff(uint32_t i) {
+  // TODO: Consider adding x86 PAUSE and/or ARM YIELD here.
+  if (i <= kYieldMax) {
+    sched_yield();
+  } else {
+    // nanosleep is not in the async-signal-safe list, but bionic implements it
+    // with a pure system call, so it should be fine.
+    NanoSleep(kSleepUs * 1000 * (i - kYieldMax));
+  }
+}
+
+template <bool kHandleZeroReads, VerifyObjectFlags kVerifyFlags>
+size_t MarkCompact::GetClassSize(mirror::Class* klass, mirror::Class* moved_klass) {
+  size_t size = klass->GetClassSize<kVerifyFlags>();
+  // Handle the case where the page containing the class size is already
+  // moved to to-space.
+  if (kHandleZeroReads && size == 0) {
+    DCHECK(use_move_ioctl_);
+    DCHECK(from_space_map_.HasAddress(klass));
+    DCHECK(HasAddress(moved_klass, moving_space_begin_, black_dense_end_));
+    size = moved_klass->GetClassSize<kVerifyFlags>();
+    DCHECK_NE(size, 0u);
+  }
+  return size;
+}
+
+void MarkCompact::MoveBlackDensePageForUpdate(uint8_t* page) {
+  DCHECK_ALIGNED_PARAM(page, gPageSize);
+  DCHECK(from_space_map_.HasAddress(page));
+  uint8_t* to_page = GetToSpaceAddr(page);
+  DCHECK_ALIGNED_PARAM(to_page, gPageSize);
+  DCHECK(HasAddress(to_page, moving_space_begin_, black_dense_end_));
+  size_t idx = DivideByPageSize(to_page - moving_space_begin_);
+  DCHECK_LT(idx, moving_first_objs_count_);
+  mirror::Object* first_obj = first_objs_moving_space_[idx].AsMirrorPtr();
+  // If we are claiming that the page has part of an object on it, then its
+  // first-obj should not be null.
+  DCHECK_NE(first_obj, nullptr);
+  bool success = DoPageCompactionWithStateChange<kUffdMode>(
+      idx,
+      to_page,
+      page,
+      /*map_immediately=*/true,
+      [&]() REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_) {
+        if (use_generational_) {
+          UpdateNonMovingPage</*kSetupForGenerational=*/true, /*kObjInBlackDense=*/true>(
+              first_obj, to_page, from_space_slide_diff_, moving_space_bitmap_);
+        } else {
+          UpdateNonMovingPage</*kSetupForGenerational=*/false, /*kObjInBlackDense=*/true>(
+              first_obj, to_page, from_space_slide_diff_, moving_space_bitmap_);
+        }
+      });
+  DCHECK_LE(moving_pages_status_[idx].load(std::memory_order_relaxed),
+            static_cast<uint8_t>(PageState::kProcessedAndMapped));
+  if (!success) {
+    uint32_t i = 0;
+    while (moving_pages_status_[idx].load(std::memory_order_acquire) !=
+           static_cast<uint8_t>(PageState::kProcessedAndMapped)) {
+      BackOff</*kYieldMax=*/2, /*kSleepUs=*/5>(i++);
+    }
+  }
+}
+
+/*
+ * Challenges in using MOVE ioctl for mapping black-dense pages
+ * ============================================================
+ *
+ * When using MOVE ioctl, the source (from-space) pages move to to-space, leaving
+ * behind unmapped page-table entries. The next read from the same source page
+ * leads to a page-fault, which is handled by mapping the shared zero-page by the
+ * kernel. If the field being read is expected to hold a genuine value of 0, then
+ * it gets tricky to tell whether the read value is a genuine 0 or from the shared
+ * zero-page because the page moved.
+ * Updating object arrays and static references of classes which are in the black-
+ * dense region need to deal with this situation as some of the fields involved in
+ * the update process may hold 0 as value. Furthermore, updating objects whose class
+ * is in black-dense region may also require dealing with the situation in certain
+ * cases. The case-by-case situations are explained in the corresponding functions
+ * below. But, the overall approach takes into consideration the following:
+ *
+ * 1. This is a very performance critical code and therefore whenever possible we
+ * try to avoid performing such zero-read confirmations. For example, object-arrays
+ * and classes which are entirely contained within the page being updated don't need
+ * it as all the required fields are available in that page and there is no possibility
+ * of reading from shared zero-page. Similarly, objects whose reference-fields bitmap
+ * fits within the 32-bit reference_instance_offsets_ field in the corresponding class
+ * object also don't need it.
+ *
+ * 2. There are some case-specific situations which make it easy to determine that
+ * either we read from shared zero-page, or vice versa.
+ *
+ * 3. If there is a field, say 'A', which is guaranteed to be non-zero, and it is on
+ * the same page on which we want to read another field, say 'B', which maybe 0, then
+ * we can confirm accuracy of the value of B using the value of A. If we read non-zero
+ * value from A *after* reading B, then we know we correctly read B. To avoid re-ordering,
+ * we use 'acquire' fence between the two reads. Also, to ensure that the compiler
+ * doesn't optimize away the read from A, we perform it atomically. If we read 0
+ * from A, then it must be from the shared zero-page. In that case, we read B from
+ * to-space, which is guaranteed to be a correct value.
+ *
+ * 4. If none of the above is applicable, then the last resort is to first move the
+ * page containing the field and then read from to-space. Fortunately, this is an
+ * extremely rare situation.
+ */
+
+static uint32_t ReadNonZeroFieldAfterAcquire(void* field_addr) {
+  // Acquire fence to ensure the following load doesn't get re-ordered
+  // with the load of the field which we want to check if it's from shared
+  // zero-page.
+  std::atomic_thread_fence(std::memory_order_acquire);
+  // atomic read to ensure compiler doesn't optimize away.
+  auto atomic_field = std::atomic_ref<uint32_t>(*static_cast<uint32_t*>(field_addr));
+  return atomic_field.load(std::memory_order_relaxed);
+}
+
+template <bool kHandleZeroReads, VerifyObjectFlags kVerifyFlags, typename Visitor>
+int32_t MarkCompact::UpdateObjArrayReferences(mirror::ObjectArray<mirror::Object>* arr,
+                                              Visitor& visitor,
+                                              MemberOffset begin,
+                                              MemberOffset end) {
+  int32_t length = arr->GetLength<kVerifyFlags>();
+
+  auto updater = [&]() REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_) {
+    begin = std::max(begin, mirror::ObjectArray<mirror::Object>::OffsetOfElement(0));
+    end = std::min(
+        end, mirror::ObjectArray<mirror::Object>::OffsetOfElement(static_cast<size_t>(length)));
+    for (MemberOffset offset = begin; offset < end; offset += kHeapReferenceSize) {
+      visitor(arr, offset, /*is_static=*/false, /*is_obj_array=*/true);
+    }
+  };
+
+  if (!kHandleZeroReads || length > 0) {
+    updater();
+    return length;
+  }
+
+  DCHECK(from_space_map_.HasAddress(arr));
+  DCHECK(HasAddress(GetToSpaceAddr(arr), moving_space_begin_, black_dense_end_));
+  DCHECK_ALIGNED_PARAM(visitor.Begin(), gPageSize);
+  uint8_t* raw_class_addr =
+      reinterpret_cast<uint8_t*>(arr) + mirror::Object::ClassOffset().Int32Value();
+  uint8_t* raw_len_addr =
+      reinterpret_cast<uint8_t*>(arr) + mirror::Array::LengthOffset().Int32Value();
+  uint8_t* length_page = AlignDown(raw_len_addr, gPageSize);
+  uint8_t* class_page = AlignDown(raw_class_addr, gPageSize);
+  if (length_page == visitor.Begin()) {
+    // If length is on the same page as the one we are updating. Then we correctly
+    // read 0 length. Nothing to do.
+  } else if (class_page == length_page) {
+    // If class-object and length are on the same page, which is quite likely,
+    // then reading class will confirm.
+    if (ReadNonZeroFieldAfterAcquire(raw_class_addr) == 0) {
+      auto* to_space_arr = GetToSpaceAddr(arr);
+      length = to_space_arr->GetLength<kVerifyFlags>();
+      updater();
+    } else {
+      // we read the right value. Nothing to update.
+    }
+  } else if (class_page == visitor.Begin()) {
+    // There is nothing to update. And in this case we couldn't have asked for
+    // object-size.
+  } else {
+    // The only case left is where the updating page is after the page containing
+    // length, or the length is in moving-side. In the former case it is certain
+    // that array is non-zero in length, so the fact that we read zero-length means
+    // it came from shared zero-page. Re-read from to-space. In the latter case we
+    // must have correctly read 0 length as the from-space page is guaranteed to
+    // stick around.
+    auto* to_arr = GetToSpaceAddr(arr);
+    raw_len_addr = reinterpret_cast<uint8_t*>(to_arr) + mirror::Array::LengthOffset().Int32Value();
+    if (raw_len_addr < black_dense_end_) {
+      length = to_arr->GetLength<kVerifyFlags>();
+      DCHECK_GT(length, 0);
+      updater();
+    }
+  }
+  return length;
+}
+
+template <bool kHandleZeroReads, VerifyObjectFlags kVerifyFlags, typename Visitor>
+void MarkCompact::UpdateStaticFieldsReferences(mirror::Class* klass, Visitor& visitor) {
+  // NOTE: Unlike Class::VisitStaticFieldsReferences, we are not checking if the
+  // class is resolved before visiting static references. That's because we may
+  // wrongly interpret 0 status (read from shared zero-page) as kNotReady and
+  // skip updating static-references. Therefore, we instead depend on class-flags
+  // indicating that we have at least one static reference. OTOH, it is safe to
+  // update static references in a class which is not fully resolved yet as the
+  // references will be null and hence updating them is a no-op.
+  mirror::Class* to_klass;
+  bool updating_page_black_dense;
+  if (kHandleZeroReads) {
+    DCHECK(from_space_map_.HasAddress(klass));
+    to_klass = GetToSpaceAddr(klass);
+    DCHECK(HasAddress(to_klass, moving_space_begin_, black_dense_end_));
+    if (Visitor::CheckEnd()) {
+      DCHECK_EQ(visitor.End() - visitor.Begin(), static_cast<ssize_t>(gPageSize));
+      uint8_t* to_addr = visitor.End() - from_space_slide_diff_;
+      updating_page_black_dense = to_addr <= black_dense_end_ && to_addr > moving_space_begin_;
+      if (updating_page_black_dense &&
+          reinterpret_cast<uint8_t*>(klass) + sizeof(mirror::Class) >= visitor.End()) {
+        // Static references are not in the page being updated.
+        return;
+      }
+    }
+  }
+  uint32_t class_flags = klass->GetClassFlags<kVerifyFlags>();
+  if (kHandleZeroReads && class_flags == 0) {
+    // Class-flags can be 0 only until it is resolved. But we may be reading from a shared
+    // zero-page also. So we need to figure out which case is this. Class-size and class-class
+    // are guaranteed to be non-zero right from the beginning. At least one of them must be
+    // on the same page as class-flags.
+    DCHECK_LT(
+        mirror::Class::ClassSizeOffset().SizeValue() - mirror::Object::ClassOffset().SizeValue(),
+        gPageSize);
+    uint8_t* klass_addr = reinterpret_cast<uint8_t*>(klass);
+    uint8_t* klass_flags_addr = klass_addr + mirror::Class::ClassFlagsOffset().Int32Value();
+    uint8_t* klass_size_addr = klass_addr + mirror::Class::ClassSizeOffset().Int32Value();
+    if (AlignDown(klass_flags_addr, gPageSize) == AlignDown(klass_size_addr, gPageSize)) {
+      if (ReadNonZeroFieldAfterAcquire(klass_size_addr) != 0) {
+        return;
+      }
+    } else {
+      uint8_t* klass_klass_addr = klass_addr + mirror::Object::ClassOffset().Int32Value();
+      DCHECK_EQ(AlignDown(klass_flags_addr, gPageSize), AlignDown(klass_klass_addr, gPageSize));
+      if (ReadNonZeroFieldAfterAcquire(klass_klass_addr) != 0) {
+        return;
+      }
+    }
+    // The page containing class-flags is definitely moved. Re-read from to-space.
+    class_flags = to_klass->GetClassFlags<kVerifyFlags>();
+  }
+
+  if ((class_flags & mirror::kClassFlagHasStaticRefs) == 0) {
+    return;
+  }
+
+  size_t num_reference_fields = klass->NumReferenceStaticFieldsUnchecked<kVerifyFlags>();
+  if (kHandleZeroReads && num_reference_fields == 0) {
+    num_reference_fields = to_klass->NumReferenceStaticFieldsUnchecked<kVerifyFlags>();
+  }
+  DCHECK_NE(num_reference_fields, 0u);
+
+  MemberOffset field_offset(0);
+  if ((class_flags & mirror::kClassFlagHasEmbeddedVTable) == 0) {
+    field_offset = MemberOffset(sizeof(mirror::Class));
+  } else {
+    // It is still possible that the stored vtable length is 0. So we cannot
+    // just check for non-zero.
+    uint32_t vtable_len = klass->GetEmbeddedVTableLength<kVerifyFlags>();
+    if (kHandleZeroReads && vtable_len == 0) {
+      uint8_t* raw_klass_addr = reinterpret_cast<uint8_t*>(klass);
+      uint8_t* raw_vtable_len_addr =
+          raw_klass_addr + mirror::Class::EmbeddedVTableLengthOffset().Int32Value();
+      uint8_t* raw_num_refs_addr =
+          raw_klass_addr + mirror::Class::NumReferenceStaticFieldsOffset().Int32Value();
+      uint8_t* vtable_len_page = AlignDown(raw_vtable_len_addr, gPageSize);
+      uint8_t* num_static_refs_page = AlignDown(raw_num_refs_addr, gPageSize);
+
+      if (visitor.Begin() == vtable_len_page) {
+        // We have read the length from the page being updated. So it must have been 0.
+      } else if (Visitor::CheckEnd() && updating_page_black_dense &&
+                 visitor.End() <= raw_vtable_len_addr) {
+        // The page being updated is before vtable-length, so we are not looking for
+        // updating static references, which come after vtable-length.
+        return;
+      } else if (vtable_len_page == num_static_refs_page) {
+        // We know that number of static refs is supposed to be non-zero. So if
+        // it's on vtable-length's page, then reading it confirms correctness of
+        // vtable length that we read.
+        if (ReadNonZeroFieldAfterAcquire(raw_num_refs_addr) == 0) {
+          vtable_len = to_klass->GetEmbeddedVTableLength<kVerifyFlags>();
+        } else {
+          // We read correct vtable length. Proceed with that.
+        }
+      } else {
+        // This is the rare case in which the page preceding the one being updated started after
+        // num_reference_static_fields_ and ended before the class-end and contains embedded
+        // vtable-length. It may also contain the static references on it. But it will not cause
+        // recursive page-moves as it contains vtable-lengh on it. Hence will not end up here again.
+        DCHECK(HasAddress(GetToSpaceAddr(vtable_len_page), moving_space_begin_, black_dense_end_));
+        MoveBlackDensePageForUpdate(vtable_len_page);
+        // Now we can read from the to-space.
+        vtable_len = to_klass->GetEmbeddedVTableLength<kVerifyFlags>();
+      }
+    }
+    field_offset = MemberOffset(mirror::Class::ComputeClassSize(/*has_embedded_vtable=*/true,
+                                                                vtable_len,
+                                                                /*num_8bit_static_fields=*/0,
+                                                                /*num_16bit_static_fields=*/0,
+                                                                /*num_32bit_static_fields=*/0,
+                                                                /*num_64bit_static_fields=*/0,
+                                                                /*num_ref_static_fields=*/0,
+                                                                /*num_ref_bitmap_entries=*/0,
+                                                                pointer_size_));
+  }
+
+  for (size_t i = 0u; i < num_reference_fields; ++i) {
+    visitor(klass, field_offset, /*is_static=*/true);
+    field_offset =
+        MemberOffset(field_offset.Uint32Value() + sizeof(mirror::HeapReference<mirror::Object>));
+  }
+}
+
+template <VerifyObjectFlags kVerifyFlags, typename Visitor>
+void MarkCompact::UpdateInstanceFieldsReferences(mirror::Object* obj,
+                                                 mirror::Class* to_klass,
+                                                 mirror::Class* klass,
+                                                 const Visitor& visitor) {
+  /* mirror::Class fields relevant in this function:
+   *    --------class begin--------
+   *    ...
+   *    uint32_t class_flags_;
+   *    uint32_t class_size_;
+   *    ...
+   *    uint32_t reference_instance_offsets_;
+   *    ...
+   *    uint32_t overflow_bitmap_[];
+   *    --------class end---------
+   */
+
+  // For classes in immune spaces there is no extra care required.
+  if (to_klass == klass || reinterpret_cast<uint8_t*>(to_klass) >= black_dense_end_) {
+    DCHECK_IMPLIES(
+        to_klass == klass,
+        immune_spaces_.ContainsObject(to_klass) || non_moving_space_->HasAddress(to_klass));
+    return obj->VisitInstanceFieldsReferences<kVerifyFlags, kWithoutReadBarrier>(klass, visitor);
+  }
+  DCHECK(from_space_map_.HasAddress(klass));
+  DCHECK(HasAddress(to_klass, moving_space_begin_, black_dense_end_));
+
+  auto visit_one_map_word = [&visitor, obj](uint32_t field_offset, uint32_t ref_offsets)
+                                REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_) {
+                                  while (ref_offsets != 0) {
+                                    if ((ref_offsets & 1) != 0) {
+                                      visitor(obj, MemberOffset(field_offset), /*is_static=*/false);
+                                    }
+                                    ref_offsets >>= 1;
+                                    field_offset += sizeof(mirror::HeapReference<mirror::Object>);
+                                  }
+                                };
+
+  uint32_t ref_bitmap = klass->GetReferenceInstanceOffsetsUnchecked<kVerifyFlags>();
+  bool ref_bitmap_page_moved = false;
+  if (ref_bitmap == 0) {
+    ref_bitmap = to_klass->GetReferenceInstanceOffsetsUnchecked<kVerifyFlags>();
+    ref_bitmap_page_moved = true;
+  }
+  DCHECK_NE(ref_bitmap, 0u);
+
+  if ((ref_bitmap & mirror::Class::kVisitReferencesSlowpathMask) == 0) {
+    visit_one_map_word(mirror::kObjectHeaderSize, ref_bitmap);
+  } else {
+    size_t class_size = GetClassSize</*kHandleZeroReads=*/true, kVerifyNone>(klass, to_klass);
+    uint8_t* raw_klass_addr = reinterpret_cast<uint8_t*>(klass);
+    uint8_t* raw_klass_end = raw_klass_addr + class_size;
+    // Optimize the case where the reference_instance_offsets_ and last overflow bitmap-word
+    // of the class are on the same page and that page has already moved.
+    if (ref_bitmap_page_moved) {
+      if (LIKELY(AlignDown(
+                     raw_klass_addr + mirror::Class::ReferenceInstanceOffsetsOffset().Int32Value(),
+                     gPageSize) == AlignDown(raw_klass_end - 1, gPageSize))) {
+        klass = to_klass;
+        raw_klass_addr = reinterpret_cast<uint8_t*>(klass);
+        raw_klass_end = raw_klass_addr + class_size;
+      }
+    }
+
+    uint32_t bitmap_num_words = ref_bitmap & ~mirror::Class::kVisitReferencesSlowpathMask;
+    uint32_t* overflow_bitmap =
+        reinterpret_cast<uint32_t*>(raw_klass_end - bitmap_num_words * sizeof(uint32_t));
+    for (uint32_t i = 0; i < bitmap_num_words; i++) {
+      ref_bitmap = overflow_bitmap[i];
+      if (LIKELY(ref_bitmap != 0)) {
+        visit_one_map_word(
+            mirror::kObjectHeaderSize + i * sizeof(mirror::HeapReference<mirror::Object>) * 32,
+            ref_bitmap);
+      } else if (klass != to_klass) {
+        // We cannot tolerate recursive page move operations. So avoid them.
+        uint32_t* bitmap_last_word = reinterpret_cast<uint32_t*>(raw_klass_end - sizeof(uint32_t));
+        uint32_t* ref_instance_offsets_addr = reinterpret_cast<uint32_t*>(
+            raw_klass_addr + mirror::Class::ReferenceInstanceOffsetsOffset().Int32Value());
+        uint8_t* curr_bitmap_word_page =
+            AlignDown(reinterpret_cast<uint8_t*>(overflow_bitmap + i), gPageSize);
+        if (LIKELY(curr_bitmap_word_page ==
+                   AlignDown(reinterpret_cast<uint8_t*>(bitmap_last_word), gPageSize))) {
+          // If the last word of the bitmap is on the same page as the current word, then
+          // we can confirm by reading the last-word as it is guaranteed to be non-zero.
+          if (ReadNonZeroFieldAfterAcquire(bitmap_last_word) == 0) {
+            // If the last-word is 0 then it's guaranteed that the page
+            // containing overflow-bitmap has moved. Continue from the to-space.
+            overflow_bitmap = GetToSpaceAddr(overflow_bitmap);
+            klass = to_klass;
+            // ref_bitmap_page_moved = true;
+            DCHECK_LT(i, bitmap_num_words);
+            i--;  // Revisit the word on next iteration.
+          } else {
+            // We correctly read the bitmap word to be 0.
+          }
+        } else if (curr_bitmap_word_page ==
+                   AlignDown(reinterpret_cast<uint8_t*>(ref_instance_offsets_addr), gPageSize)) {
+          // If the reference_instance_offsets_ field is on the same page as
+          // the current word, then we can confirm by reading that.
+          ref_bitmap = ReadNonZeroFieldAfterAcquire(ref_instance_offsets_addr);
+          if (ref_bitmap == 0) {
+            // Consume all overflow-ref-bitmap words from to-space on this page
+            // before going back to from-space.
+            uint32_t* to_overflow_bitmap = GetToSpaceAddr(overflow_bitmap + i);
+            uint32_t* to_overflow_bitmap_page_end = AlignUp(to_overflow_bitmap, gPageSize);
+            DCHECK_LT(to_overflow_bitmap, to_overflow_bitmap_page_end);
+            // We already ruled out that bitmap's last word is on the same page
+            // as the current word. Asserting the same on the to-space side.
+            DCHECK_LE(to_overflow_bitmap_page_end, GetToSpaceAddr(bitmap_last_word));
+            for (; to_overflow_bitmap < to_overflow_bitmap_page_end; to_overflow_bitmap++, i++) {
+              ref_bitmap = *to_overflow_bitmap;
+              if (ref_bitmap != 0) {
+                visit_one_map_word(mirror::kObjectHeaderSize +
+                                       i * sizeof(mirror::HeapReference<mirror::Object>) * 32,
+                                   ref_bitmap);
+              }
+            }
+            i--;  // reverse the extra increment from the above loop.
+            DCHECK_LT(i, bitmap_num_words);
+          } else {
+            // We read a genuine 0 bitmap at index i. So, continue the loop in from-space.
+            DCHECK_EQ(ref_bitmap & ~mirror::Class::kVisitReferencesSlowpathMask, bitmap_num_words);
+          }
+        } else if (curr_bitmap_word_page == AlignDown(visitor.Object(), gPageSize)) {
+          // This is the unlikely case where the class is on the same page in
+          // black-dense region where the object being updated is. In that case
+          // we couldn't have read from shared zero-page. So, continue the loop.
+          // NOTE: it's important to identify this case as otherwise the following
+          // would try to move the page and would get stuck in a deadlock.
+          DCHECK(from_space_map_.HasAddress(visitor.Object()));
+          DCHECK(from_space_map_.HasAddress(curr_bitmap_word_page));
+          DCHECK(
+              HasAddress(GetToSpaceAddr(visitor.Object()), moving_space_begin_, black_dense_end_));
+          size_t page_idx =
+              DivideByPageSize(GetToSpaceAddr(visitor.Object()) - moving_space_begin_);
+          uint8_t state = moving_pages_status_[page_idx].load(std::memory_order_relaxed);
+          DCHECK_LT(state, static_cast<uint8_t>(PageState::kProcessedAndMapping));
+          DCHECK_NE(state, static_cast<uint8_t>(PageState::kProcessed));
+          DCHECK_NE(state, static_cast<uint8_t>(PageState::kUnprocessed));
+        } else {
+          // This case is probably never going to happen, where there is at
+          // least one non-overlapping page between reference_instance_offsets_
+          // and last word of overflow-bitmap. Such page(s) can have static
+          // references on them, but that won't cause recursive page-move. Test
+          // coverage for this case is in 160-read-barrier-stress art-test.
+          //
+          // Move all but last page and consume bitmap words from them.
+          auto* bitmap_last_word_page =
+              AlignDown(reinterpret_cast<uint8_t*>(bitmap_last_word), gPageSize);
+          auto* to_cur_bitmap_word = GetToSpaceAddr(overflow_bitmap + i);
+          for (auto* page = AlignDown(reinterpret_cast<uint8_t*>(overflow_bitmap + i), gPageSize);
+               page < bitmap_last_word_page;) {
+            MoveBlackDensePageForUpdate(page);
+            page += gPageSize;
+            auto* to_page_end = reinterpret_cast<uint32_t*>(GetToSpaceAddr(page));
+            for (; to_cur_bitmap_word < to_page_end; to_cur_bitmap_word++, i++) {
+              ref_bitmap = *to_cur_bitmap_word;
+              if (ref_bitmap != 0) {
+                visit_one_map_word(mirror::kObjectHeaderSize +
+                                       i * sizeof(mirror::HeapReference<mirror::Object>) * 32,
+                                   ref_bitmap);
+              }
+            }
+          }
+          i--;  // reverse the extra increment from the above loop.
+          DCHECK_LT(i, bitmap_num_words);
+        }
+      }
+    }
+  }
+}
+
+template <bool kFetchObjSize,
+          bool kObjInBlackDense,
+          VerifyObjectFlags kVerifyFlags,
+          typename Visitor>
+size_t MarkCompact::UpdateRefsForCompaction(mirror::Object* obj,
+                                            const Visitor& visitor,
+                                            MemberOffset begin,
+                                            MemberOffset end) {
+  // We depend on kFetchObjSize to be true only for objects in compacting side
+  // of the moving space. We almost never require size of black-dense objects.
+  // The only exception is the object overlapping on the boundary of black-dense
+  // and moving regions. Arrays and strings require reading the length/count from
+  // within the object, which can be 0. OTOH, class-object's size is also stored
+  // in the object, but is guaranteed to be non-zero. All other cases require
+  // reading from the class, which is elsewhere.
+  static_assert(mirror::Object::ClassOffset().Int32Value() == 0);
+  constexpr VerifyObjectFlags kSizeOfFlags = RemoveThisFlags(kVerifyFlags);
+  constexpr bool kHandleZeroReads =
+      kObjInBlackDense && (Visitor::CheckBegin() || Visitor::CheckEnd());
+  mirror::Object* to_obj = nullptr;
+  if (kHandleZeroReads) {
+    to_obj = GetToSpaceAddr(obj);
+  }
+  mirror::Class* klass = obj->GetClass<kVerifyFlags, kWithoutReadBarrier>();
+  if (kHandleZeroReads && klass == nullptr) {
+    DCHECK(from_space_map_.HasAddress(obj));
+    // null class-object in an object not in black-dense region indicates memory
+    // corruption.
+    DCHECK(HasAddress(to_obj, moving_space_begin_, black_dense_end_));
+    klass = to_obj->GetClass<kVerifyFlags, kWithoutReadBarrier>();
+  }
+  DCHECK_NE(klass, nullptr);
+  mirror::Class* from_klass = static_cast<mirror::Class*>(GetFromAddrAllSpaces(klass));
+  uint32_t class_flags = from_klass->GetClassFlags<kVerifyNone>();
+  if (class_flags == 0) {
+    DCHECK(klass != from_klass && reinterpret_cast<uint8_t*>(klass) < black_dense_end_);
+    // The page containing class-flags has been moved to the to-space. Re-read from there.
+    class_flags = klass->GetClassFlags<kVerifyNone>();
+  }
+  DCHECK_NE(class_flags, 0u);
+
+  visitor(obj, mirror::Object::ClassOffset(), /*is_static=*/false);
+  if ((class_flags & mirror::kClassFlagNoReferenceFields) != 0) {
+    // An overlapping string/array can be actually zero-len only when its
+    // length/count field lands on the moving-region side, where the page is
+    // guaranteed to not get moved. In other words, if the length/count field is
+    // also in the black-dense region, then the fact that it is overlapping proves
+    // it is non-zero. Therefore, we leverage this and re-read length/count only
+    // if it lands in the black-dense region.
+    if ((class_flags & mirror::kClassFlagString) != 0) {
+      if (kFetchObjSize) {
+        int32_t str_count = static_cast<mirror::String*>(obj)->GetCount<kSizeOfFlags>();
+        if (kHandleZeroReads && str_count == 0) {
+          DCHECK_NE(to_obj, nullptr);
+          uint8_t* raw_count_addr =
+              reinterpret_cast<uint8_t*>(to_obj) + mirror::String::CountOffset().Int32Value();
+          if (HasAddress(raw_count_addr, moving_space_begin_, black_dense_end_)) {
+            str_count = static_cast<mirror::String*>(to_obj)->GetCount<kSizeOfFlags>();
+          }
+        }
+        return mirror::String::SizeOf(str_count);
+      } else {
+        return 0;
+      }
+    } else if ((class_flags & mirror::kClassFlagPrimitiveArray) != 0) {
+      if (kFetchObjSize) {
+        int32_t len = static_cast<mirror::Array*>(obj)->GetLength<kSizeOfFlags>();
+        if (kHandleZeroReads && len == 0) {
+          DCHECK_NE(to_obj, nullptr);
+          uint8_t* raw_len_addr =
+              reinterpret_cast<uint8_t*>(to_obj) + mirror::Array::LengthOffset().Int32Value();
+          if (HasAddress(raw_len_addr, moving_space_begin_, black_dense_end_)) {
+            len = static_cast<mirror::Array*>(to_obj)->GetLength<kSizeOfFlags>();
+          }
+        }
+        return mirror::Array::SizeOf(class_flags >> mirror::kArrayComponentSizeShiftShift, len);
+      } else {
+        return 0;
+      }
+    }
+  } else if ((class_flags & mirror::kClassFlagObjectArray) != 0) {
+    int32_t len = UpdateObjArrayReferences<kHandleZeroReads, kVerifyFlags>(
+        static_cast<mirror::ObjectArray<mirror::Object>*>(obj), visitor, begin, end);
+    return kFetchObjSize
+               ? mirror::Array::SizeOf(class_flags >> mirror::kArrayComponentSizeShiftShift, len)
+               : 0;
+  } else {
+    // We have to read reference-bitmap to visit references.
+    UpdateInstanceFieldsReferences<kVerifyFlags>(obj, klass, from_klass, visitor);
+    if ((class_flags & mirror::kClassFlagClass) != 0) {
+      mirror::Class* as_klass = static_cast<mirror::Class*>(obj);
+      // Check if class is resolved and in that case. Fetch obj, which is a
+      // class, class_flags for non-zero static-refs and non-zero vtable-len.
+      UpdateStaticFieldsReferences<kHandleZeroReads, kVerifyFlags>(as_klass, visitor);
+      return kFetchObjSize ? GetClassSize<kHandleZeroReads, kSizeOfFlags>(
+                                 as_klass, static_cast<mirror::Class*>(to_obj))
+                           : 0;
+    } else if ((class_flags & mirror::kClassFlagReference) != 0) {
+      visitor(obj, mirror::Reference::ReferentOffset(), /*is_static=*/false);
+    }
+  }
+  if (kFetchObjSize) {
+    size_t obj_size = from_klass->GetObjectSizeUnchecked<kSizeOfFlags>();
+    if (obj_size == 0) {
+      DCHECK(klass != from_klass && reinterpret_cast<uint8_t*>(klass) < black_dense_end_);
+      obj_size = klass->GetObjectSizeUnchecked<kSizeOfFlags>();
+    }
+    DCHECK_NE(obj_size, 0u);
+    return obj_size;
+  } else {
+    return 0;
+  }
+}
+
 inline void MarkCompact::SetBitForMidToOldPromotion(uint8_t* obj) {
   DCHECK(use_generational_);
   DCHECK_GE(obj, old_gen_end_);
@@ -1614,21 +2386,20 @@ inline void MarkCompact::SetBitForMidToOldPromotion(uint8_t* obj) {
   mid_to_old_promo_bit_vec_->SetBit((obj - old_gen_end_) / kObjectAlignment);
 }
 
-bool MarkCompact::IsValidObject(mirror::Object* obj) const {
-  mirror::Class* klass = obj->GetClass<kVerifyNone, kWithoutReadBarrier>();
-  if (!heap_->GetVerification()->IsValidHeapObjectAddress(klass)) {
-    return false;
-  }
-  return heap_->GetVerification()->IsValidClassUnchecked<kWithFromSpaceBarrier>(
-          obj->GetClass<kVerifyNone, kWithFromSpaceBarrier>());
-}
-
 template <typename Callback>
 void MarkCompact::VerifyObject(mirror::Object* ref, Callback& callback) const {
   if (kIsDebugBuild) {
-    mirror::Class* klass = ref->GetClass<kVerifyNone, kWithFromSpaceBarrier>();
     mirror::Class* pre_compact_klass = ref->GetClass<kVerifyNone, kWithoutReadBarrier>();
+    mirror::Class* klass = ref->GetClass<kVerifyNone, kWithFromSpaceBarrier>();
     mirror::Class* klass_klass = klass->GetClass<kVerifyNone, kWithFromSpaceBarrier>();
+    if (klass_klass == nullptr) {
+      // When using move ioctl, a class in black-dense region may have moved
+      // to-space and therefore re-read from to-space.
+      CHECK(use_move_ioctl_);
+      CHECK(HasAddress(pre_compact_klass, moving_space_begin_, black_dense_end_));
+      klass_klass = pre_compact_klass->GetClass<kVerifyNone, kWithFromSpaceBarrier>();
+    }
+    CHECK_NE(klass_klass, nullptr);
     mirror::Class* klass_klass_klass = klass_klass->GetClass<kVerifyNone, kWithFromSpaceBarrier>();
     if (HasAddress(pre_compact_klass) &&
         reinterpret_cast<uint8_t*>(pre_compact_klass) < black_allocations_begin_) {
@@ -1641,7 +2412,10 @@ void MarkCompact::VerifyObject(mirror::Object* ref, Callback& callback) const {
         CHECK(live_words_bitmap_->Test(pre_compact_klass));
       }
     }
-    if (!IsValidObject(ref)) {
+    if (!heap_->GetVerification()->IsValidHeapObjectAddress(pre_compact_klass) ||
+        !heap_->GetVerification()->IsValidHeapObjectAddress(klass_klass) ||
+        !heap_->GetVerification()->IsValidHeapObjectAddress(klass_klass_klass) ||
+        klass_klass != klass_klass_klass) {
       std::ostringstream oss;
       oss << "Invalid object: "
           << "ref=" << ref
@@ -1790,6 +2564,7 @@ void MarkCompact::CompactPage(mirror::Object* obj,
     bool should_dirty_card;
     mirror::Object* to_ref = reinterpret_cast<mirror::Object*>(start_addr - offset_within_obj);
     mirror::Object* from_obj = GetFromSpaceAddr(obj);
+    bool obj_in_black_dense = reinterpret_cast<uint8_t*>(obj) < black_dense_end_;
     mirror::Object* post_compact_obj = nullptr;
     if (kSetupForGenerational) {
       post_compact_obj = PostCompactAddress(obj, black_dense_end_, moving_space_end_);
@@ -1797,18 +2572,26 @@ void MarkCompact::CompactPage(mirror::Object* obj,
     if (stride_count > 1) {
       RefsUpdateVisitor</*kCheckBegin*/ true, /*kCheckEnd*/ false, kSetupForGenerational> visitor(
           this, to_ref, start_addr, nullptr, card_table, post_compact_obj);
-      obj_size =
-          from_obj->VisitRefsForCompaction</*kFetchObjSize*/ true, /*kVisitNativeRoots*/ false>(
-              visitor, MemberOffset(offset_within_obj), MemberOffset(-1));
+      obj_size = obj_in_black_dense
+                     ? UpdateRefsForCompaction</*kFetchObjSize=*/true, /*kObjInBlackDense=*/true>(
+                           from_obj, visitor, MemberOffset(offset_within_obj), MemberOffset(-1))
+                     : UpdateRefsForCompaction</*kFetchObjSize=*/true, /*kObjInBlackDense=*/false>(
+                           from_obj, visitor, MemberOffset(offset_within_obj), MemberOffset(-1));
       should_dirty_card = visitor.ShouldDirtyCard();
     } else {
       RefsUpdateVisitor</*kCheckBegin*/ true, /*kCheckEnd*/ true, kSetupForGenerational> visitor(
           this, to_ref, start_addr, start_addr + gPageSize, card_table, post_compact_obj);
-      obj_size =
-          from_obj->VisitRefsForCompaction</*kFetchObjSize*/ true, /*kVisitNativeRoots*/ false>(
-              visitor,
-              MemberOffset(offset_within_obj),
-              MemberOffset(offset_within_obj + gPageSize));
+      obj_size = obj_in_black_dense
+                     ? UpdateRefsForCompaction</*kFetchObjSize=*/true, /*kObjInBlackDense=*/true>(
+                           from_obj,
+                           visitor,
+                           MemberOffset(offset_within_obj),
+                           MemberOffset(offset_within_obj + gPageSize))
+                     : UpdateRefsForCompaction</*kFetchObjSize=*/true, /*kObjInBlackDense=*/false>(
+                           from_obj,
+                           visitor,
+                           MemberOffset(offset_within_obj),
+                           MemberOffset(offset_within_obj + gPageSize));
       should_dirty_card = visitor.ShouldDirtyCard();
     }
     if (kSetupForGenerational && should_dirty_card) {
@@ -1859,7 +2642,8 @@ void MarkCompact::CompactPage(mirror::Object* obj,
         nullptr,
         nullptr,
         dest_cards & (1 << (bytes_done >> accounting::CardTable::kCardShift)));
-    obj_size = ref->VisitRefsForCompaction(visitor, MemberOffset(0), MemberOffset(-1));
+    obj_size = UpdateRefsForCompaction</*kFetchObjSize=*/true, /*kObjInBlackDense=*/false>(
+        ref, visitor, MemberOffset(0), MemberOffset(-1));
     if (kSetupForGenerational) {
       SetBitForMidToOldPromotion(to_space_addr + bytes_done);
       if (visitor.ShouldDirtyCard()) {
@@ -1872,7 +2656,7 @@ void MarkCompact::CompactPage(mirror::Object* obj,
   // Last stride may have multiple objects in it and we don't know where the
   // last object which crosses the page boundary starts, therefore check
   // page-end in all of these objects. Also, we need to call
-  // VisitRefsForCompaction() with from-space object as we fetch object size,
+  // UpdateRefsForCompaction() with from-space object as we fetch object size,
   // which in case of klass requires 'class_size_'.
   uint8_t* from_addr = from_space_begin_ + last_stride_begin * kAlignment;
   bytes_to_visit = end_addr - addr;
@@ -1887,9 +2671,8 @@ void MarkCompact::CompactPage(mirror::Object* obj,
         nullptr,
         start_addr + gPageSize,
         dest_cards & (1 << (bytes_done >> accounting::CardTable::kCardShift)));
-    obj_size = obj->VisitRefsForCompaction(visitor,
-                                           MemberOffset(0),
-                                           MemberOffset(end_addr - (addr + bytes_done)));
+    obj_size = UpdateRefsForCompaction</*kFetchObjSize=*/true, /*kObjInBlackDense=*/false>(
+        obj, visitor, MemberOffset(0), MemberOffset(end_addr - (addr + bytes_done)));
     if (kSetupForGenerational) {
       SetBitForMidToOldPromotion(to_space_addr + bytes_done);
       if (visitor.ShouldDirtyCard()) {
@@ -1988,20 +2771,18 @@ void MarkCompact::SlideBlackPage(mirror::Object* first_obj,
                                                                            to_obj,
                                                                            dest,
                                                                            nullptr);
-        obj_size = from_obj->VisitRefsForCompaction<
-                /*kFetchObjSize*/true, /*kVisitNativeRoots*/false>(visitor,
-                                                                   MemberOffset(offset),
-                                                                   MemberOffset(-1));
+        obj_size = UpdateRefsForCompaction<
+            /*kFetchObjSize=*/true,
+            /*kObjInBlackDense=*/false>(from_obj, visitor, MemberOffset(offset), MemberOffset(-1));
       } else {
         RefsUpdateVisitor</*kCheckBegin*/true, /*kCheckEnd*/true> visitor(this,
                                                                           to_obj,
                                                                           dest,
                                                                           dest_page_end);
-        obj_size = from_obj->VisitRefsForCompaction<
-                /*kFetchObjSize*/true, /*kVisitNativeRoots*/false>(visitor,
-                                                                   MemberOffset(offset),
-                                                                   MemberOffset(offset
-                                                                                + gPageSize));
+        obj_size = UpdateRefsForCompaction<
+            /*kFetchObjSize=*/true,
+            /*kObjInBlackDense=*/false>(
+            from_obj, visitor, MemberOffset(offset), MemberOffset(offset + gPageSize));
         if (first_obj == next_page_first_obj) {
           // First object is the only object on this page. So there's nothing else left to do.
           return;
@@ -2031,7 +2812,8 @@ void MarkCompact::SlideBlackPage(mirror::Object* first_obj,
                                                                           dest_obj,
                                                                           nullptr,
                                                                           nullptr);
-      obj_size = dest_obj->VisitRefsForCompaction(visitor, MemberOffset(0), MemberOffset(-1));
+      obj_size = UpdateRefsForCompaction</*kFetchObjSize=*/true, /*kObjInBlackDense=*/false>(
+          dest_obj, visitor, MemberOffset(0), MemberOffset(-1));
       obj_size = RoundUp(obj_size, kAlignment);
       bytes_to_visit -= obj_size;
       dest += obj_size;
@@ -2045,9 +2827,8 @@ void MarkCompact::SlideBlackPage(mirror::Object* first_obj,
                                                                          nullptr,
                                                                          dest_page_end);
       mirror::Object* obj = GetFromSpaceAddr(next_page_first_obj);
-      obj->VisitRefsForCompaction</*kFetchObjSize*/false>(visitor,
-                                                          MemberOffset(0),
-                                                          MemberOffset(dest_page_end - dest));
+      UpdateRefsForCompaction</*kFetchObjSize=*/false, /*kObjInBlackDense=*/false>(
+          obj, visitor, MemberOffset(0), MemberOffset(dest_page_end - dest));
       return;
     }
   }
@@ -2082,18 +2863,17 @@ void MarkCompact::SlideBlackPage(mirror::Object* first_obj,
     std::memcpy(dest, src_addr, remaining_bytes);
     DCHECK_LT(reinterpret_cast<uintptr_t>(found_obj), page_end);
     moving_space_bitmap_->VisitMarkedRange(
-            reinterpret_cast<uintptr_t>(found_obj) + mirror::kObjectHeaderSize,
-            page_end,
-            [&found_obj, pre_compact_addr, dest, this, verify_obj_callback] (mirror::Object* obj)
-            REQUIRES_SHARED(Locks::mutator_lock_) {
+        reinterpret_cast<uintptr_t>(found_obj) + mirror::kObjectHeaderSize,
+        page_end,
+        [&found_obj, pre_compact_addr, dest, this, verify_obj_callback](mirror::Object* obj)
+            REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_) {
               ptrdiff_t diff = reinterpret_cast<uint8_t*>(found_obj) - pre_compact_addr;
               mirror::Object* ref = reinterpret_cast<mirror::Object*>(dest + diff);
               VerifyObject(ref, verify_obj_callback);
               RefsUpdateVisitor</*kCheckBegin*/false, /*kCheckEnd*/false>
                       visitor(this, ref, nullptr, nullptr);
-              ref->VisitRefsForCompaction</*kFetchObjSize*/false>(visitor,
-                                                                  MemberOffset(0),
-                                                                  MemberOffset(-1));
+              UpdateRefsForCompaction</*kFetchObjSize*/ false, /*kObjInBlackDense=*/false>(
+                  ref, visitor, MemberOffset(0), MemberOffset(-1));
               // Remember for next round.
               found_obj = obj;
             });
@@ -2110,20 +2890,11 @@ void MarkCompact::SlideBlackPage(mirror::Object* first_obj,
     // class, then we may access something (like static-fields' offsets) which
     // is on the next page. Therefore, use from-space's reference.
     mirror::Object* obj = GetFromSpaceAddr(found_obj);
-    obj->VisitRefsForCompaction</*kFetchObjSize*/ false>(
-        visitor, MemberOffset(0), MemberOffset(page_end - reinterpret_cast<uintptr_t>(found_obj)));
-  }
-}
-
-template <uint32_t kYieldMax = 5, uint64_t kSleepUs = 10>
-static void BackOff(uint32_t i) {
-  // TODO: Consider adding x86 PAUSE and/or ARM YIELD here.
-  if (i <= kYieldMax) {
-    sched_yield();
-  } else {
-    // nanosleep is not in the async-signal-safe list, but bionic implements it
-    // with a pure system call, so it should be fine.
-    NanoSleep(kSleepUs * 1000 * (i - kYieldMax));
+    UpdateRefsForCompaction</*kFetchObjSize=*/false, /*kObjInBlackDense=*/false>(
+        obj,
+        visitor,
+        MemberOffset(0),
+        MemberOffset(page_end - reinterpret_cast<uintptr_t>(found_obj)));
   }
 }
 
@@ -2179,6 +2950,81 @@ size_t MarkCompact::ZeropageIoctl(void* addr,
   }
 }
 
+size_t MarkCompact::MoveIoctl(void* dst, void* src, size_t len, bool tolerate_enoent) {
+  DCHECK_ALIGNED_PARAM(dst, gPageSize);
+  DCHECK_ALIGNED_PARAM(src, gPageSize);
+  DCHECK_ALIGNED_PARAM(len, gPageSize);
+  struct uffdio_move uffd_move{.dst = reinterpret_cast<uintptr_t>(dst),
+                               .src = reinterpret_cast<uintptr_t>(src),
+                               .len = len,
+                               .mode = 0,
+                               .move = 0};
+  int iters = 0;
+  while (ioctl(uffd_, UFFDIO_MOVE, &uffd_move) != 0) {
+    iters++;
+    if (errno == EEXIST) {
+      DCHECK_EQ(uffd_move.move, -EEXIST);
+      uffd_move.move = gPageSize;
+      break;
+    } else if (errno == ENOENT) {
+      if (uffd_move.move > 0) {
+        uffd_move.move += gPageSize;
+      } else {
+        DCHECK_EQ(uffd_move.move, -ENOENT);
+        uffd_move.move = gPageSize;
+      }
+      break;
+    } else if (errno == EAGAIN) {
+      DCHECK_LT(uffd_move.move, static_cast<ssize_t>(len));
+      DCHECK_NE(uffd_move.move, 0);
+      if (uffd_move.move < 0) {
+        uffd_move.move = 0;
+        if (iters == 10) {
+          LOG(FATAL) << __FUNCTION__ << ": repeated ioctls not working with EAGAIN"
+                     << " dst:" << dst << " src:" << src << " len:" << len
+                     << " tolerate_enoent:" << tolerate_enoent;
+        }
+      } else {
+        break;
+      }
+    } else if (errno == EINVAL && tolerate_enoent) {
+      // Unlike other ioctls, MOVE returns EINVAL when the memory range is found
+      // to be not registered with userfaultfd context associated with 'uffd_'
+      // file descriptor.
+      if (uffd_move.move < 0) {
+        uffd_move.move = 0;
+      }
+      break;
+    } else if (errno == EBUSY) {
+      // There are a couple of situations in which MOVE ioctl can return EBUSY.
+      // The most relevant to this GC is where the page is not exclusively
+      // mapped in the process. This can happen if the page is Copy-on-Write
+      // shared with another process (like zygote), or if the page is in the
+      // process of getting swapped out. Both these situations can be handled
+      // by writing to the page. However, the page is shared by multiple
+      // threads. So we use a CAS to make sure we don't corrupt the data on it.
+      // We rely on the fact that a compare-and-exchange instruction causes the
+      // page to be exclusively mapped, even when the store isn't performed.
+      auto atomic_src = std::atomic_ref<size_t>(*static_cast<size_t*>(src));
+      size_t expected = 0;
+      atomic_src.compare_exchange_strong(expected, 0, std::memory_order_relaxed);
+      uffd_move.move = 0;
+      if (iters == 10) {
+        LOG(FATAL) << __FUNCTION__ << ": repeated ioctls not working with EBUSY"
+                   << " dst:" << dst << " src:" << src << " len:" << len
+                   << " tolerate_enoent:" << tolerate_enoent;
+      }
+    } else {
+      CHECK_EQ(uffd_move.move, -errno);
+      LOG(FATAL) << "ioctl_userfaultfd: move failed: " << strerror(errno) << ". src:" << src
+                 << " dst:" << dst << " length:" << len;
+      UNREACHABLE();
+    }
+  }
+  DCHECK_ALIGNED_PARAM(uffd_move.move, gPageSize);
+  return uffd_move.move;
+}
+
 size_t MarkCompact::CopyIoctl(
     void* dst, void* buffer, size_t length, bool return_on_contention, bool tolerate_enoent) {
   int32_t backoff_count = -1;
@@ -2259,13 +3105,20 @@ bool MarkCompact::DoPageCompactionWithStateChange(size_t page_idx,
   if (kMode == kFallbackMode || moving_pages_status_[page_idx].compare_exchange_strong(
                                     expected_state, desired_state, std::memory_order_acquire)) {
     func();
-    if (kMode == kCopyMode) {
+    if (kMode == kUffdMode) {
       if (map_immediately) {
-        CopyIoctl(to_space_page,
-                  page,
-                  gPageSize,
-                  /*return_on_contention=*/false,
-                  /*tolerate_enoent=*/false);
+        if (use_move_ioctl_) {
+          MoveIoctl(to_space_page,
+                    page,
+                    gPageSize,
+                    /*tolerate_enoent=*/false);
+        } else {
+          CopyIoctl(to_space_page,
+                    page,
+                    gPageSize,
+                    /*return_on_contention=*/false,
+                    /*tolerate_enoent=*/false);
+        }
         // Store is sufficient as no other thread could modify the status at this
         // point. Relaxed order is sufficient as the ioctl will act as a fence.
         moving_pages_status_[page_idx].store(static_cast<uint8_t>(PageState::kProcessedAndMapped),
@@ -2383,10 +3236,11 @@ bool MarkCompact::FreeFromSpacePages(size_t cur_page_idx, int mode, size_t end_i
   DCHECK_ALIGNED_PARAM(last_reclaimed_page_, gPageSize);
   // Check if the 'class_after_obj_map_' map allows pages to be freed.
   for (; class_after_obj_iter_ != class_after_obj_map_.rend(); class_after_obj_iter_++) {
-    mirror::Object* klass = class_after_obj_iter_->first.AsMirrorPtr();
-    mirror::Class* from_klass = static_cast<mirror::Class*>(GetFromSpaceAddr(klass));
+    mirror::Class* klass = static_cast<mirror::Class*>(class_after_obj_iter_->first.AsMirrorPtr());
+    mirror::Class* from_klass = GetFromSpaceAddr(klass);
     // Check with class' end to ensure that, if required, the entire class survives.
-    uint8_t* klass_end = reinterpret_cast<uint8_t*>(klass) + from_klass->SizeOf<kVerifyNone>();
+    size_t class_size = GetClassSize</*kHandleZeroReads=*/true, kVerifyNone>(from_klass, klass);
+    uint8_t* klass_end = reinterpret_cast<uint8_t*>(klass) + class_size;
     DCHECK_LE(klass_end, last_reclaimed_page_);
     if (reinterpret_cast<uint8_t*>(klass_end) >= reclaim_begin) {
       // Found a class which is in the reclaim range.
@@ -2408,7 +3262,7 @@ bool MarkCompact::FreeFromSpacePages(size_t cur_page_idx, int mode, size_t end_i
   ssize_t size = last_reclaimed_page_ - reclaim_begin;
   if (size > kMinFromSpaceMadviseSize) {
     // Map all the pages in the range.
-    if (mode == kCopyMode && cur_page_idx < end_idx_for_mapping) {
+    if (mode == kUffdMode && cur_page_idx < end_idx_for_mapping) {
       if (MapMovingSpacePages(cur_page_idx,
                               end_idx_for_mapping,
                               /*from_ioctl=*/false,
@@ -2423,22 +3277,56 @@ bool MarkCompact::FreeFromSpacePages(size_t cur_page_idx, int mode, size_t end_i
     // If not all pages are mapped, then take it as a hint that mmap_lock is
     // contended and hence don't madvise as that also needs the same lock.
     if (all_mapped) {
-      // Retain a few pages for subsequent compactions.
-      const ssize_t gBufferPages = 4 * gPageSize;
-      DCHECK_LT(gBufferPages, kMinFromSpaceMadviseSize);
-      size -= gBufferPages;
-      uint8_t* addr = last_reclaimed_page_ - size;
-      CHECK_EQ(madvise(addr + from_space_slide_diff_, size, MADV_DONTNEED), 0)
-          << "madvise of from-space failed: " << strerror(errno);
-      last_reclaimed_page_ = addr;
-      cur_reclaimable_page_ = addr;
-    }
-  }
-  last_reclaimable_page_ = std::min(reclaim_begin, last_reclaimable_page_);
+      if (!use_move_ioctl_) {
+        // Retain a few pages for subsequent compactions.
+        const ssize_t gBufferPages = 4 * gPageSize;
+        DCHECK_LT(gBufferPages, kMinFromSpaceMadviseSize);
+        size -= gBufferPages;
+        uint8_t* addr = last_reclaimed_page_ - size;
+        int ret = madvise(addr + from_space_slide_diff_, size, MADV_DONTNEED);
+        CHECK(ret == 0) << "madvise of from-space failed: " << strerror(errno);
+        last_reclaimed_page_ = addr;
+        cur_reclaimable_page_ = addr;
+      } else {
+        last_reclaimed_page_ -= size;
+      }
+    }
+  }
+  // When using MOVE ioctl black-dense pages are moved without copying. So there
+  // will not be any actual pages in there that can be recycled/reclaimed.
+  if (reclaim_begin < last_reclaimable_page_.load(std::memory_order_relaxed) &&
+      (!use_move_ioctl_ || reclaim_begin >= black_dense_end_)) {
+    last_reclaimable_page_.store(reclaim_begin, std::memory_order_relaxed);
+  }
   last_checked_reclaim_page_idx_ = idx;
   return all_mapped;
 }
 
+uint8_t* MarkCompact::GetFreePagesForMapping(size_t size, bool atomic) {
+  DCHECK_ALIGNED_PARAM(size, gPageSize);
+  uint8_t* expected = cur_reclaimable_page_.load(std::memory_order_relaxed);
+  if (atomic) {
+    DCHECK(use_move_ioctl_);
+    while (expected - last_reclaimable_page_.load(std::memory_order_relaxed) >=
+           static_cast<ssize_t>(size)) {
+      uint8_t* desired = expected - size;
+      // 'expected' gets updated by compare_exchange_weak() on failure
+      if (cur_reclaimable_page_.compare_exchange_weak(
+              expected, desired, std::memory_order_relaxed)) {
+        return desired;
+      }
+    }
+  } else {
+    if (expected - last_reclaimable_page_.load(std::memory_order_relaxed) >=
+        static_cast<ssize_t>(size)) {
+      uint8_t* desired = expected - size;
+      cur_reclaimable_page_.store(desired, std::memory_order_relaxed);
+      return desired;
+    }
+  }
+  return nullptr;
+}
+
 template <int kMode>
 void MarkCompact::CompactMovingSpace(uint8_t* page) {
   // For every page we have a starting object, which may have started in some
@@ -2447,11 +3335,12 @@ void MarkCompact::CompactMovingSpace(uint8_t* page) {
   // Consult the live-words bitmap to copy all contiguously live words at a
   // time. These words may constitute multiple objects. To avoid the need for
   // consulting mark-bitmap to find where does the next live object start, we
-  // use the object-size returned by VisitRefsForCompaction.
+  // use the object-size returned by UpdateRefsForCompaction.
   //
   // We do the compaction in reverse direction so that the pages containing
   // TLAB and latest allocations are processed first.
   TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());
+  TraceFaults();
   size_t page_status_arr_len = moving_first_objs_count_ + black_page_count_;
   size_t idx = page_status_arr_len;
   size_t black_dense_end_idx = (black_dense_end_ - moving_space_begin_) / gPageSize;
@@ -2462,8 +3351,8 @@ void MarkCompact::CompactMovingSpace(uint8_t* page) {
 
   // These variables are maintained by FreeFromSpacePages().
   last_reclaimed_page_ = pre_compact_page;
-  last_reclaimable_page_ = last_reclaimed_page_;
-  cur_reclaimable_page_ = last_reclaimed_page_;
+  last_reclaimable_page_.store(last_reclaimed_page_, std::memory_order_relaxed);
+  cur_reclaimable_page_.store(last_reclaimed_page_, std::memory_order_relaxed);
   last_checked_reclaim_page_idx_ = idx;
   class_after_obj_iter_ = class_after_obj_map_.rbegin();
   // Allocated-black pages
@@ -2482,13 +3371,14 @@ void MarkCompact::CompactMovingSpace(uint8_t* page) {
                                              to_space_end,
                                              page,
                                              /*map_immediately=*/true,
-                                             [&]() REQUIRES_SHARED(Locks::mutator_lock_) {
+                                             [&]() REQUIRES_SHARED(Locks::mutator_lock_,
+                                                                   Locks::heap_bitmap_lock_) {
                                                SlideBlackPage(first_obj,
                                                               next_page_first_obj,
                                                               first_chunk_size,
                                                               pre_compact_page,
                                                               page,
-                                                              kMode == kCopyMode);
+                                                              kMode == kUffdMode);
                                              });
       // We are sliding here, so no point attempting to madvise for every
       // page. Wait for enough pages to be done.
@@ -2508,12 +3398,12 @@ void MarkCompact::CompactMovingSpace(uint8_t* page) {
     if (kMode == kFallbackMode) {
       page = to_space_end;
     } else {
-      DCHECK_EQ(kMode, kCopyMode);
-      if (cur_reclaimable_page_ > last_reclaimable_page_) {
-        cur_reclaimable_page_ -= gPageSize;
-        page = cur_reclaimable_page_ + from_space_slide_diff_;
-      } else {
+      DCHECK_EQ(kMode, kUffdMode);
+      page = GetFreePagesForMapping(gPageSize, use_move_ioctl_);
+      if (page == nullptr) {
         page = reserve_page;
+      } else {
+        page += from_space_slide_diff_;
       }
     }
     mirror::Object* first_obj = first_objs_moving_space_[idx].AsMirrorPtr();
@@ -2522,22 +3412,22 @@ void MarkCompact::CompactMovingSpace(uint8_t* page) {
         to_space_end,
         page,
         /*map_immediately=*/page == reserve_page,
-        [&]() REQUIRES_SHARED(Locks::mutator_lock_) {
+        [&]() REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_) {
           if (use_generational_ && to_space_end < mid_gen_end_) {
             CompactPage</*kSetupForGenerational=*/true>(first_obj,
                                                         pre_compact_offset_moving_space_[idx],
                                                         page,
                                                         to_space_end,
-                                                        kMode == kCopyMode);
+                                                        kMode == kUffdMode);
           } else {
             CompactPage</*kSetupForGenerational=*/false>(first_obj,
                                                          pre_compact_offset_moving_space_[idx],
                                                          page,
                                                          to_space_end,
-                                                         kMode == kCopyMode);
+                                                         kMode == kUffdMode);
           }
         });
-    if (kMode == kCopyMode && (!success || page == reserve_page) && end_idx_for_mapping - idx > 1) {
+    if (kMode == kUffdMode && (!success || page == reserve_page) && end_idx_for_mapping - idx > 1) {
       // map the pages in the following address as they can't be mapped with the
       // pages yet-to-be-compacted as their src-side pages won't be contiguous.
       MapMovingSpacePages(idx + 1,
@@ -2560,12 +3450,12 @@ void MarkCompact::CompactMovingSpace(uint8_t* page) {
           to_space_end,
           to_space_end + from_space_slide_diff_,
           /*map_immediately=*/false,
-          [&]() REQUIRES_SHARED(Locks::mutator_lock_) {
+          [&]() REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_) {
             if (use_generational_) {
-              UpdateNonMovingPage</*kSetupForGenerational=*/true>(
+              UpdateNonMovingPage</*kSetupForGenerational=*/true, /*kObjInBlackDense=*/true>(
                   first_obj, to_space_end, from_space_slide_diff_, moving_space_bitmap_);
             } else {
-              UpdateNonMovingPage</*kSetupForGenerational=*/false>(
+              UpdateNonMovingPage</*kSetupForGenerational=*/false, /*kObjInBlackDense=*/true>(
                   first_obj, to_space_end, from_space_slide_diff_, moving_space_bitmap_);
             }
             if (kMode == kFallbackMode) {
@@ -2586,7 +3476,7 @@ void MarkCompact::CompactMovingSpace(uint8_t* page) {
     }
   }
   // map one last time to finish anything left.
-  if (kMode == kCopyMode && end_idx_for_mapping > 0) {
+  if (kMode == kUffdMode && end_idx_for_mapping > 0) {
     MapMovingSpacePages(idx,
                         end_idx_for_mapping,
                         /*from_fault=*/false,
@@ -2594,6 +3484,7 @@ void MarkCompact::CompactMovingSpace(uint8_t* page) {
                         /*tolerate_enoent=*/false);
   }
   DCHECK_EQ(to_space_end, bump_pointer_space_->Begin());
+  TraceFaults();
 }
 
 size_t MarkCompact::MapMovingSpacePages(size_t start_idx,
@@ -2637,11 +3528,17 @@ size_t MarkCompact::MapMovingSpacePages(size_t start_idx,
       uint8_t* from_space_start = from_space_begin_ + from_space_offset;
       DCHECK_ALIGNED_PARAM(to_space_start, gPageSize);
       DCHECK_ALIGNED_PARAM(from_space_start, gPageSize);
-      size_t mapped_len = CopyIoctl(to_space_start,
-                                    from_space_start,
-                                    map_count * gPageSize,
-                                    return_on_contention,
-                                    tolerate_enoent);
+      size_t mapped_len;
+      if (use_move_ioctl_) {
+        mapped_len =
+            MoveIoctl(to_space_start, from_space_start, map_count * gPageSize, tolerate_enoent);
+      } else {
+        mapped_len = CopyIoctl(to_space_start,
+                               from_space_start,
+                               map_count * gPageSize,
+                               return_on_contention,
+                               tolerate_enoent);
+      }
       for (size_t l = 0; l < mapped_len; l += gPageSize, arr_idx++) {
         // Store is sufficient as anyone storing is doing it with the same value.
         moving_pages_status_[arr_idx].store(static_cast<uint8_t>(PageState::kProcessedAndMapped),
@@ -2671,7 +3568,7 @@ size_t MarkCompact::MapMovingSpacePages(size_t start_idx,
   return arr_len - start_idx;
 }
 
-template <bool kSetupForGenerational>
+template <bool kSetupForGenerational, bool kObjInBlackDense>
 void MarkCompact::UpdateNonMovingPage(mirror::Object* first,
                                       uint8_t* page,
                                       ptrdiff_t from_space_diff,
@@ -2685,11 +3582,12 @@ void MarkCompact::UpdateNonMovingPage(mirror::Object* first,
       std::max(reinterpret_cast<uint8_t*>(first) + mirror::kObjectHeaderSize, page);
   // For every object found in the page, visit the previous object. This ensures
   // that we can visit without checking page-end boundary.
-  // Call VisitRefsForCompaction with from-space read-barrier as the klass object and
+  // Call UpdateRefsForCompaction with from-space read-barrier as the klass object and
   // super-class loads require it.
   // TODO: Set kVisitNativeRoots to false once we implement concurrent
   // compaction
-  auto obj_visitor = [&](mirror::Object* next_obj) {
+  auto obj_visitor = [&](mirror::Object* next_obj) REQUIRES_SHARED(Locks::mutator_lock_,
+                                                                   Locks::heap_bitmap_lock_) {
     if (curr_obj != nullptr) {
       mirror::Object* from_obj =
           reinterpret_cast<mirror::Object*>(reinterpret_cast<uint8_t*>(curr_obj) + from_space_diff);
@@ -2700,14 +3598,14 @@ void MarkCompact::UpdateNonMovingPage(mirror::Object* first,
         MemberOffset begin_offset(page - reinterpret_cast<uint8_t*>(curr_obj));
         // Native roots shouldn't be visited as they are done when this
         // object's beginning was visited in the preceding page.
-        from_obj->VisitRefsForCompaction</*kFetchObjSize*/ false, /*kVisitNativeRoots*/ false>(
-            visitor, begin_offset, MemberOffset(-1));
+        UpdateRefsForCompaction</*kFetchObjSize=*/false, kObjInBlackDense>(
+            from_obj, visitor, begin_offset, MemberOffset(-1));
         should_dirty_card = visitor.ShouldDirtyCard();
       } else {
         RefsUpdateVisitor</*kCheckBegin*/ false, /*kCheckEnd*/ false, kSetupForGenerational>
             visitor(this, from_obj, from_page, from_page_end, card_table, curr_obj);
-        from_obj->VisitRefsForCompaction</*kFetchObjSize*/ false>(
-            visitor, MemberOffset(0), MemberOffset(-1));
+        UpdateRefsForCompaction</*kFetchObjSize=*/false, kObjInBlackDense>(
+            from_obj, visitor, MemberOffset(0), MemberOffset(-1));
         should_dirty_card = visitor.ShouldDirtyCard();
       }
       if (kSetupForGenerational && should_dirty_card) {
@@ -2726,10 +3624,6 @@ void MarkCompact::UpdateNonMovingPage(mirror::Object* first,
         card_table->CardFromAddr(first) == card_table->CardFromAddr(scan_begin)) {
       curr_obj = nullptr;
     }
-    // We cannot acquire heap-bitmap-lock here as this function is called from
-    // SIGBUS handler. But it's safe as the bitmap passed to Scan function
-    // can't get modified until this GC cycle is finished.
-    FakeMutexLock mu(*Locks::heap_bitmap_lock_);
     card_table->Scan</*kClearCard=*/false>(
         bitmap, scan_begin, page + gPageSize, obj_visitor, accounting::CardTable::kCardAged2);
   } else {
@@ -2746,14 +3640,14 @@ void MarkCompact::UpdateNonMovingPage(mirror::Object* first,
     if (reinterpret_cast<uint8_t*>(curr_obj) < page) {
       RefsUpdateVisitor</*kCheckBegin*/ true, /*kCheckEnd*/ true, kSetupForGenerational> visitor(
           this, from_obj, from_page, from_page_end, card_table, curr_obj);
-      from_obj->VisitRefsForCompaction</*kFetchObjSize*/ false, /*kVisitNativeRoots*/ false>(
-          visitor, MemberOffset(page - reinterpret_cast<uint8_t*>(curr_obj)), end_offset);
+      UpdateRefsForCompaction</*kFetchObjSize=*/false, kObjInBlackDense>(
+          from_obj, visitor, MemberOffset(page - reinterpret_cast<uint8_t*>(curr_obj)), end_offset);
       should_dirty_card = visitor.ShouldDirtyCard();
     } else {
       RefsUpdateVisitor</*kCheckBegin*/ false, /*kCheckEnd*/ true, kSetupForGenerational> visitor(
           this, from_obj, from_page, from_page_end, card_table, curr_obj);
-      from_obj->VisitRefsForCompaction</*kFetchObjSize*/ false>(
-          visitor, MemberOffset(0), end_offset);
+      UpdateRefsForCompaction</*kFetchObjSize=*/false, kObjInBlackDense>(
+          from_obj, visitor, MemberOffset(0), end_offset);
       should_dirty_card = visitor.ShouldDirtyCard();
     }
     if (kSetupForGenerational && should_dirty_card) {
@@ -2766,10 +3660,11 @@ void MarkCompact::UpdateNonMovingSpace() {
   TimingLogger::ScopedTiming t("(Paused)UpdateNonMovingSpace", GetTimings());
   // Iterating in reverse ensures that the class pointer in objects which span
   // across more than one page gets updated in the end. This is necessary for
-  // VisitRefsForCompaction() to work correctly.
-  // TODO: If and when we make non-moving space update concurrent, implement a
-  // mechanism to remember class pointers for such objects off-heap and pass it
-  // to VisitRefsForCompaction().
+  // UpdateRefsForCompaction() to work correctly.
+  // TODO: If and when we make non-moving space update concurrent, we can get
+  // rid of kObjInBlackDense template parameter as we will have to make changes
+  // to UpdateRefsForCompaction to detect non-moving-space objects anyways. And
+  // they will be almost exactly handled the way black-dense pages are handled.
   uint8_t* page = non_moving_space_->Begin() + non_moving_first_objs_count_ * gPageSize;
   for (ssize_t i = non_moving_first_objs_count_ - 1; i >= 0; i--) {
     mirror::Object* obj = first_objs_non_moving_space_[i].AsMirrorPtr();
@@ -2777,10 +3672,10 @@ void MarkCompact::UpdateNonMovingSpace() {
     // null means there are no objects on the page to update references.
     if (obj != nullptr) {
       if (use_generational_) {
-        UpdateNonMovingPage</*kSetupForGenerational=*/true>(
+        UpdateNonMovingPage</*kSetupForGenerational=*/true, /*kObjInBlackDense=*/false>(
             obj, page, /*from_space_diff=*/0, non_moving_space_bitmap_);
       } else {
-        UpdateNonMovingPage</*kSetupForGenerational=*/false>(
+        UpdateNonMovingPage</*kSetupForGenerational=*/false, /*kObjInBlackDense=*/false>(
             obj, page, /*from_space_diff=*/0, non_moving_space_bitmap_);
       }
     }
@@ -2922,6 +3817,7 @@ void MarkCompact::UpdateMovingSpaceBlackAllocations() {
   bump_pointer_space_->SetBlockSizes(thread_running_gc_,
                                      post_compact_end_ - begin,
                                      consumed_blocks_count);
+  prev_moving_space_end_at_compaction_ = static_cast<void*>(bump_pointer_space_->End());
   if (kIsDebugBuild) {
     size_t moving_space_size = bump_pointer_space_->Size();
     size_t los_size = 0;
@@ -2986,16 +3882,18 @@ class MarkCompact::ImmuneSpaceUpdateObjVisitor {
  public:
   explicit ImmuneSpaceUpdateObjVisitor(MarkCompact* collector) : collector_(collector) {}
 
-  void operator()(mirror::Object* obj) const ALWAYS_INLINE REQUIRES(Locks::mutator_lock_) {
+  void operator()(mirror::Object* obj) const ALWAYS_INLINE
+      REQUIRES(Locks::mutator_lock_, Locks::heap_bitmap_lock_) {
     RefsUpdateVisitor</*kCheckBegin*/false, /*kCheckEnd*/false> visitor(collector_,
                                                                         obj,
                                                                         /*begin_*/nullptr,
                                                                         /*end_*/nullptr);
-    obj->VisitRefsForCompaction</*kFetchObjSize*/ false>(
-        visitor, MemberOffset(0), MemberOffset(-1));
+    collector_->UpdateRefsForCompaction</*kFetchObjSize=*/false, /*kObjInBlackDense=*/false>(
+        obj, visitor, MemberOffset(0), MemberOffset(-1));
   }
 
-  static void Callback(mirror::Object* obj, void* arg) REQUIRES(Locks::mutator_lock_) {
+  static void Callback(mirror::Object* obj, void* arg)
+      REQUIRES(Locks::mutator_lock_, Locks::heap_bitmap_lock_) {
     reinterpret_cast<ImmuneSpaceUpdateObjVisitor*>(arg)->operator()(obj);
   }
 
@@ -3261,7 +4159,7 @@ void MarkCompact::CompactionPause() {
   {
     // TODO: Immune space updation has to happen either before or after
     // remapping pre-compact pages to from-space. And depending on when it's
-    // done, we have to invoke VisitRefsForCompaction() with or without
+    // done, we have to invoke UpdateRefsForCompaction() with or without
     // read-barrier.
     TimingLogger::ScopedTiming t2("(Paused)UpdateImmuneSpaces", GetTimings());
     accounting::CardTable* const card_table = heap_->GetCardTable();
@@ -3381,6 +4279,7 @@ void MarkCompact::CompactionPause() {
     KernelPreparation();
   }
 
+  ReaderMutexLock rmu(thread_running_gc_, *Locks::heap_bitmap_lock_);
   UpdateNonMovingSpace();
   // fallback mode
   if (uffd_ == kFallbackMode) {
@@ -3481,6 +4380,44 @@ void MarkCompact::KernelPreparation() {
   }
 }
 
+bool MarkCompact::SigsysHandler(siginfo_t* info, void* context) {
+// Arch-specific register access
+#if defined(__aarch64__)
+#define REG(ctxt, reg)  ((ctxt)->uc_mcontext.regs[(reg)])
+#define PARM2_REG(ctxt) REG(ctxt, 1)
+#define RET_REG(ctxt)   REG(ctxt, 0)
+#elif defined(__arm__)
+#define REG(ctxt, reg)  ((ctxt)->uc_mcontext.arm_r##reg)
+#define PARM2_REG(ctxt) REG(ctxt, 1)
+#define RET_REG(ctxt)   REG(ctxt, 0)
+#elif defined(__i386__)
+#define REG(ctxt, reg)  ((ctxt)->uc_mcontext.gregs[(reg)])
+#define PARM2_REG(ctxt) REG(ctxt, REG_ECX)
+#define RET_REG(ctxt)   REG(ctxt, REG_EAX)
+#elif defined(__x86_64__)
+#define REG(ctxt, reg)  ((ctxt)->uc_mcontext.gregs[(reg)])
+#define PARM2_REG(ctxt) REG(ctxt, REG_RSI)
+#define RET_REG(ctxt)   REG(ctxt, REG_RAX)
+#elif defined(__riscv)
+#define REG(ctxt, reg)  ((ctxt)->uc_mcontext.__gregs[(reg)])
+#define PARM2_REG(ctxt) REG(ctxt, 11)
+#define RET_REG(ctxt)   REG(ctxt, 10)
+#else
+#error "unsupported architecture"
+#endif
+  CHECK_EQ(info->si_signo, SIGSYS);
+  // Detect if the MOVE ioctl was prevented from execution by seccomp filter.
+  if (info->si_code == SYS_SECCOMP && info->si_syscall == __NR_ioctl) {
+    ucontext_t* uctxt = static_cast<ucontext_t*>(context);
+    // Second parameter of the ioctl has the command passed to the kernel.
+    if (static_cast<uint64_t>(PARM2_REG(uctxt)) == UFFDIO_MOVE) {
+      RET_REG(uctxt) = -EINVAL;
+      return true;
+    }
+  }
+  return false;
+}
+
 bool MarkCompact::SigbusHandler(siginfo_t* info) {
   class ScopedInProgressCount {
    public:
@@ -3576,6 +4513,19 @@ void MarkCompact::ConcurrentlyProcessMovingPage(uint8_t* fault_page,
   mirror::Object* first_obj = first_objs_moving_space_[page_idx].AsMirrorPtr();
   if (first_obj == nullptr) {
     DCHECK_GT(fault_page, post_compact_end_);
+    if (use_move_ioctl_) {
+      uint8_t* free_page = GetFreePagesForMapping(gPageSize, /*atomic=*/true);
+      if (free_page != nullptr) {
+        DCHECK_ALIGNED_PARAM(free_page, gPageSize);
+        free_page += from_space_slide_diff_;
+        std::memset(free_page, 0x0, gPageSize);
+        if (MoveIoctl(fault_page, free_page, gPageSize, tolerate_enoent) == gPageSize) {
+          moving_pages_status_[page_idx].store(static_cast<uint8_t>(PageState::kProcessedAndMapped),
+                                               std::memory_order_release);
+        }
+        return;
+      }
+    }
     // Install zero-page in the entire remaining tlab to avoid multiple ioctl invocations.
     uint8_t* end = AlignDown(self->GetTlabEnd(), gPageSize);
     if (fault_page < self->GetTlabStart() || fault_page >= end) {
@@ -3623,12 +4573,16 @@ void MarkCompact::ConcurrentlyProcessMovingPage(uint8_t* fault_page,
               raw_state,
               static_cast<uint8_t>(PageState::kMutatorProcessing),
               std::memory_order_acquire)) {
+        // We cannot acquire heap-bitmap-lock here as this function is called from
+        // SIGBUS handler. But it's safe as the GC thread is holding the lock for
+        // entire compaction phase ensuring that bitmap accessed don't get modified.
+        FakeMutexLock mu(*Locks::heap_bitmap_lock_);
         if (fault_page < black_dense_end_) {
           if (use_generational_) {
-            UpdateNonMovingPage</*kSetupForGenerational=*/true>(
+            UpdateNonMovingPage</*kSetupForGenerational=*/true, /*kObjInBlackDense=*/true>(
                 first_obj, fault_page, from_space_slide_diff_, moving_space_bitmap_);
           } else {
-            UpdateNonMovingPage</*kSetupForGenerational=*/false>(
+            UpdateNonMovingPage</*kSetupForGenerational=*/false, /*kObjInBlackDense=*/true>(
                 first_obj, fault_page, from_space_slide_diff_, moving_space_bitmap_);
           }
           buf = fault_page + from_space_slide_diff_;
@@ -3643,6 +4597,12 @@ void MarkCompact::ConcurrentlyProcessMovingPage(uint8_t* fault_page,
           }
 
           if (fault_page < post_compact_end_) {
+            if (use_move_ioctl_) {
+              uint8_t* free_page = GetFreePagesForMapping(gPageSize, /*atomic=*/true);
+              if (free_page != nullptr) {
+                buf = free_page + from_space_slide_diff_;
+              }
+            }
             // The page has to be compacted.
             if (use_generational_ && fault_page < mid_gen_end_) {
               CompactPage</*kSetupGenerational=*/true>(first_obj,
@@ -3683,7 +4643,11 @@ void MarkCompact::ConcurrentlyProcessMovingPage(uint8_t* fault_page,
         // to immediately map the page, so that info is not needed.
         moving_pages_status_[page_idx].store(static_cast<uint8_t>(PageState::kProcessedAndMapping),
                                              std::memory_order_release);
-        CopyIoctl(fault_page, buf, gPageSize, /*return_on_contention=*/false, tolerate_enoent);
+        if (use_move_ioctl_) {
+          MoveIoctl(fault_page, buf, gPageSize, tolerate_enoent);
+        } else {
+          CopyIoctl(fault_page, buf, gPageSize, /*return_on_contention=*/false, tolerate_enoent);
+        }
         // Store is sufficient as no other thread modifies the status at this stage.
         moving_pages_status_[page_idx].store(static_cast<uint8_t>(PageState::kProcessedAndMapped),
                                              std::memory_order_release);
@@ -4077,7 +5041,10 @@ void MarkCompact::CompactionPhase() {
     RecordFree(ObjectBytePair(freed_objects_, freed_bytes));
   }
 
-  CompactMovingSpace<kCopyMode>(compaction_buffers_map_.Begin());
+  {
+    ReaderMutexLock rmu(thread_running_gc_, *Locks::heap_bitmap_lock_);
+    CompactMovingSpace<kUffdMode>(compaction_buffers_map_.Begin());
+  }
 
   ProcessLinearAlloc();
 
@@ -4135,11 +5102,35 @@ void MarkCompact::CompactionPhase() {
 template <size_t kBufferSize>
 class MarkCompact::ThreadRootsVisitor : public RootVisitor {
  public:
+  using RefType = StackReference<mirror::Object>;
+
   explicit ThreadRootsVisitor(MarkCompact* mark_compact, Thread* const self)
-        : mark_compact_(mark_compact), self_(self) {}
+      : mark_compact_(mark_compact), self_(self) {
+    if (kVerifyGcRootDuringMarking) {
+      verification_ = mark_compact->GetHeap()->GetVerification();
+    }
+  }
 
   ~ThreadRootsVisitor() {
-    Flush();
+    if (overflow_arr_start_ != nullptr) {
+      // Pass on the thread-local overflow array to the gc-thread for processing
+      // after checkpoint.
+      CHECK_GT(top_, overflow_arr_start_);
+      auto pair = std::make_pair(overflow_arr_start_, top_ - overflow_arr_start_);
+      MutexLock mu(self_, mark_compact_->lock_);
+      if (mark_compact_->overflow_arrays_ == nullptr) {
+        mark_compact_->overflow_arrays_ = new std::vector<std::pair<RefType*, size_t>>(1, pair);
+      } else {
+        mark_compact_->overflow_arrays_->push_back(pair);
+      }
+    } else {
+      // Since we don't reset mark-stack between the two stack-scan checkpoints
+      // in marking phase, we need to clear the stale references that are left
+      // unused in the stack.
+      for (; top_ < end_; top_++) {
+        top_->Assign(nullptr);
+      }
+    }
   }
 
   void VisitRoots(mirror::Object*** roots,
@@ -4148,6 +5139,9 @@ class MarkCompact::ThreadRootsVisitor : public RootVisitor {
       REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(Locks::heap_bitmap_lock_) {
     for (size_t i = 0; i < count; i++) {
       mirror::Object* obj = *roots[i];
+      if (kVerifyGcRootDuringMarking) {
+        CHECK(verification_->IsValidObject(obj)) << obj;
+      }
       if (mark_compact_->MarkObjectNonNullNoPush</*kParallel*/true>(obj)) {
         Push(obj);
       }
@@ -4160,6 +5154,9 @@ class MarkCompact::ThreadRootsVisitor : public RootVisitor {
       REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(Locks::heap_bitmap_lock_) {
     for (size_t i = 0; i < count; i++) {
       mirror::Object* obj = roots[i]->AsMirrorPtr();
+      if (kVerifyGcRootDuringMarking) {
+        CHECK(verification_->IsValidObject(obj)) << obj;
+      }
       if (mark_compact_->MarkObjectNonNullNoPush</*kParallel*/true>(obj)) {
         Push(obj);
       }
@@ -4167,37 +5164,49 @@ class MarkCompact::ThreadRootsVisitor : public RootVisitor {
   }
 
  private:
-  void Flush() REQUIRES_SHARED(Locks::mutator_lock_)
-               REQUIRES(Locks::heap_bitmap_lock_) {
-    StackReference<mirror::Object>* start;
-    StackReference<mirror::Object>* end;
-    {
-      MutexLock mu(self_, mark_compact_->lock_);
-      // Loop here because even after expanding once it may not be sufficient to
-      // accommodate all references. It's almost impossible, but there is no harm
-      // in implementing it this way.
-      while (!mark_compact_->mark_stack_->BumpBack(idx_, &start, &end)) {
-        mark_compact_->ExpandMarkStack();
+  void FetchBuffer() REQUIRES_SHARED(Locks::mutator_lock_) {
+    size_t requested_size;
+    ptrdiff_t new_top_offset;
+    if (LIKELY(overflow_arr_start_ == nullptr)) {
+      // During stack scanning threads can only be calling AtomicBumpBack() on
+      // the mark-stack.
+      if (mark_compact_->mark_stack_->AtomicBumpBack(kBufferSize, &top_, &end_)) {
+        return;
       }
+      new_top_offset = 0;
+      requested_size = kBufferSize;
+    } else {
+      DCHECK_GT(end_, overflow_arr_start_);
+      new_top_offset = end_ - overflow_arr_start_;
+      requested_size = 2 * new_top_offset;
     }
-    while (idx_ > 0) {
-      *start++ = roots_[--idx_];
-    }
-    DCHECK_EQ(start, end);
+    // realloc() acts like malloc() when overflow_arr_start_ is null.
+    overflow_arr_start_ =
+        static_cast<RefType*>(realloc(overflow_arr_start_, requested_size * sizeof(RefType)));
+    top_ = overflow_arr_start_ + new_top_offset;
+    end_ = overflow_arr_start_ + requested_size;
   }
 
   void Push(mirror::Object* obj) REQUIRES_SHARED(Locks::mutator_lock_)
                                  REQUIRES(Locks::heap_bitmap_lock_) {
-    if (UNLIKELY(idx_ >= kBufferSize)) {
-      Flush();
+    if (UNLIKELY(top_ == end_)) {
+      FetchBuffer();
+      DCHECK_GE(end_ - top_, static_cast<ssize_t>(kBufferSize));
     }
-    roots_[idx_++].Assign(obj);
+    top_->Assign(obj);
+    top_++;
   }
 
-  StackReference<mirror::Object> roots_[kBufferSize];
-  size_t idx_ = 0;
+  // If mark-stack has slots available, [top_, end_) represents the slots
+  // acquired from the mark-stack for storing references. After mark-stack
+  // is full, [top_, end_) is the range in overflow array.
+  RefType* top_ = nullptr;
+  RefType* end_ = nullptr;
+  // Thread-local array of references to be used if and when mark-stack is full.
+  RefType* overflow_arr_start_ = nullptr;
   MarkCompact* const mark_compact_;
   Thread* const self_;
+  const Verification* verification_;
 };
 
 class MarkCompact::CheckpointMarkThreadRoots : public Closure {
@@ -4229,6 +5238,21 @@ class MarkCompact::CheckpointMarkThreadRoots : public Closure {
   MarkCompact* const mark_compact_;
 };
 
+inline void MarkCompact::ProcessMarkObject(mirror::Object* obj) {
+  DCHECK(obj != nullptr);
+  ScanObject</*kUpdateLiveWords=*/true>(obj);
+}
+
+void MarkCompact::ProcessMarkStackNonNull() {
+  TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());
+  while (!mark_stack_->IsEmpty()) {
+    mirror::Object* obj = mark_stack_->PopBack();
+    if (obj != nullptr) {
+      ProcessMarkObject(obj);
+    }
+  }
+}
+
 void MarkCompact::MarkRootsCheckpoint(Thread* self, Runtime* runtime) {
   // We revote TLABs later during paused round of marking.
   TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());
@@ -4239,20 +5263,39 @@ void MarkCompact::MarkRootsCheckpoint(Thread* self, Runtime* runtime) {
   // run through the barrier including self.
   size_t barrier_count = thread_list->RunCheckpoint(&check_point);
   // Release locks then wait for all mutator threads to pass the barrier.
-  // If there are no threads to wait which implys that all the checkpoint functions are finished,
+  // If there are no threads to wait which implies that all the checkpoint functions are finished,
   // then no need to release locks.
-  if (barrier_count == 0) {
-    return;
+  if (barrier_count > 0) {
+    Locks::heap_bitmap_lock_->ExclusiveUnlock(self);
+    Locks::mutator_lock_->SharedUnlock(self);
+    {
+      ScopedThreadStateChange tsc(self, ThreadState::kWaitingForCheckPointsToRun);
+      gc_barrier_.Increment(self, barrier_count);
+    }
+    Locks::mutator_lock_->SharedLock(self);
+    Locks::heap_bitmap_lock_->ExclusiveLock(self);
   }
-  Locks::heap_bitmap_lock_->ExclusiveUnlock(self);
-  Locks::mutator_lock_->SharedUnlock(self);
+  // We may have null in the mark-stack as some thread(s) may have not filled
+  // the buffer completely.
+  ProcessMarkStackNonNull();
+  std::vector<std::pair<StackReference<mirror::Object>*, size_t>>* vec = nullptr;
   {
-    ScopedThreadStateChange tsc(self, ThreadState::kWaitingForCheckPointsToRun);
-    gc_barrier_.Increment(self, barrier_count);
+    MutexLock mu(self, lock_);
+    if (overflow_arrays_ != nullptr) {
+      vec = overflow_arrays_;
+      overflow_arrays_ = nullptr;
+    }
+  }
+  if (vec != nullptr) {
+    for (auto [arr, size] : *vec) {
+      for (size_t i = 0; i < size; i++) {
+        ProcessMarkObject(arr[i].AsMirrorPtr());
+      }
+      free(arr);
+      ProcessMarkStack();
+    }
+    delete vec;
   }
-  Locks::mutator_lock_->SharedLock(self);
-  Locks::heap_bitmap_lock_->ExclusiveLock(self);
-  ProcessMarkStack();
 }
 
 void MarkCompact::MarkNonThreadRoots(Runtime* runtime) {
@@ -4443,6 +5486,7 @@ void MarkCompact::MarkingPhase() {
   WriterMutexLock mu(thread_running_gc_, *Locks::heap_bitmap_lock_);
   MaybeClampGcStructures();
   PrepareForMarking(/*pre_marking=*/true);
+  TraceFaults();
   MarkZygoteLargeObjects();
   MarkRoots(
         static_cast<VisitRootFlags>(kVisitRootFlagAllRoots | kVisitRootFlagStartLoggingNewRoots));
@@ -4585,6 +5629,8 @@ void MarkCompact::ScanObject(mirror::Object* obj) {
                                << " prev_post_compact_end: " << prev_post_compact_end_
                                << " prev_black_allocations_begin: " << prev_black_allocations_begin_
                                << " prev_black_dense_end: " << prev_black_dense_end_
+                               << " prev_moving_space_end_at_compaction: "
+                               << prev_moving_space_end_at_compaction_
                                << " prev_gc_young: " << prev_gc_young_
                                << " prev_gc_performed_compaction: "
                                << prev_gc_performed_compaction_;
@@ -4625,14 +5671,14 @@ void MarkCompact::ProcessMarkStack() {
   TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());
   // TODO: try prefetch like in CMS
   while (!mark_stack_->IsEmpty()) {
-    mirror::Object* obj = mark_stack_->PopBack();
-    DCHECK(obj != nullptr);
-    ScanObject</*kUpdateLiveWords*/ true>(obj);
+    ProcessMarkObject(mark_stack_->PopBack());
   }
 }
 
 void MarkCompact::ExpandMarkStack() {
   const size_t new_size = mark_stack_->Capacity() * 2;
+  // TODO: We could reduce the overhead here by making the Resize() of
+  // AtomicStack take care of transferring references.
   std::vector<StackReference<mirror::Object>> temp(mark_stack_->Begin(),
                                                    mark_stack_->End());
   mark_stack_->Resize(new_size);
@@ -4733,8 +5779,13 @@ void MarkCompact::VisitRoots(mirror::Object*** roots,
       UpdateRoot(roots[i], moving_space_begin, moving_space_end, info);
     }
   } else {
+    const Verification* verification = GetHeap()->GetVerification();
     for (size_t i = 0; i < count; ++i) {
-      MarkObjectNonNull(*roots[i]);
+      mirror::Object* obj = *roots[i];
+      if (kVerifyGcRootDuringMarking) {
+        CHECK(verification->IsValidObject(obj)) << obj << " info:" << info;
+      }
+      MarkObjectNonNull(obj);
     }
   }
 }
@@ -4750,8 +5801,13 @@ void MarkCompact::VisitRoots(mirror::CompressedReference<mirror::Object>** roots
       UpdateRoot(roots[i], moving_space_begin, moving_space_end, info);
     }
   } else {
+    const Verification* verification = GetHeap()->GetVerification();
     for (size_t i = 0; i < count; ++i) {
-      MarkObjectNonNull(roots[i]->AsMirrorPtr());
+      mirror::Object* obj = roots[i]->AsMirrorPtr();
+      if (kVerifyGcRootDuringMarking) {
+        CHECK(verification->IsValidObject(obj)) << obj << " info:" << info;
+      }
+      MarkObjectNonNull(obj);
     }
   }
 }
@@ -4801,8 +5857,7 @@ mirror::Object* MarkCompact::IsMarked(mirror::Object* obj) {
   return marking_done_ && IsOnAllocStack(obj) ? obj : nullptr;
 }
 
-bool MarkCompact::IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* obj,
-                                              [[maybe_unused]] bool do_atomic_update) {
+bool MarkCompact::IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* obj) {
   mirror::Object* ref = obj->AsMirrorPtr();
   if (ref == nullptr) {
     return true;
@@ -4818,41 +5873,7 @@ void MarkCompact::DelayReferenceReferent(ObjPtr<mirror::Class> klass,
   heap_->GetReferenceProcessor()->DelayReferenceReferent(klass, ref, this);
 }
 
-template <typename Visitor>
-class MarkCompact::VisitReferencesVisitor {
- public:
-  explicit VisitReferencesVisitor(Visitor visitor) : visitor_(visitor) {}
-
-  ALWAYS_INLINE void operator()(mirror::Object* obj,
-                                MemberOffset offset,
-                                [[maybe_unused]] bool is_static) const
-      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
-    visitor_(obj->GetFieldObject<mirror::Object>(offset));
-  }
-
-  ALWAYS_INLINE void operator()([[maybe_unused]] ObjPtr<mirror::Class> klass,
-                                ObjPtr<mirror::Reference> ref) const
-      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
-    visitor_(ref.Ptr());
-  }
-
-  void VisitRootIfNonNull(mirror::CompressedReference<mirror::Object>* root) const
-      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
-    if (!root->IsNull()) {
-      VisitRoot(root);
-    }
-  }
-
-  void VisitRoot(mirror::CompressedReference<mirror::Object>* root) const
-      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
-    visitor_(root->AsMirrorPtr());
-  }
-
- private:
-  Visitor visitor_;
-};
-
-void MarkCompact::VerifyNoMissingCardMarks() {
+void MarkCompact::VerifyNoMissingGenerationalCardMarks() {
   if (kVerifyNoMissingCardMarks) {
     accounting::CardTable* card_table = heap_->GetCardTable();
     auto obj_visitor = [&](mirror::Object* obj) REQUIRES_SHARED(Locks::mutator_lock_) {
@@ -5133,7 +6154,7 @@ void MarkCompact::FinishPhase(bool performed_compaction) {
   }
   CHECK(mark_stack_->IsEmpty());  // Ensure that the mark stack is empty.
   mark_stack_->Reset();
-  ZeroAndReleaseMemory(compaction_buffers_map_.Begin(), compaction_buffers_map_.Size());
+  compaction_buffers_map_.MadviseDontNeedAndZero();
   info_map_.MadviseDontNeedAndZero();
   live_words_bitmap_->ClearBitmap();
   DCHECK_EQ(thread_running_gc_, Thread::Current());
@@ -5198,7 +6219,7 @@ void MarkCompact::FinishPhase(bool performed_compaction) {
     // and card-dirtying by a mutator will spuriosely fail.
     ScopedPause pause(this);
     WriterMutexLock mu(thread_running_gc_, *Locks::heap_bitmap_lock_);
-    VerifyNoMissingCardMarks();
+    VerifyNoMissingGenerationalCardMarks();
   }
   if (kVerifyPostGCObjects && use_generational_) {
     ReaderMutexLock mu(thread_running_gc_, *Locks::mutator_lock_);
diff --git a/runtime/gc/collector/mark_compact.h b/runtime/gc/collector/mark_compact.h
index 41d2ab31c2..bc8d460fe0 100644
--- a/runtime/gc/collector/mark_compact.h
+++ b/runtime/gc/collector/mark_compact.h
@@ -89,8 +89,8 @@ class YoungMarkCompact final : public GarbageCollector {
                   [[maybe_unused]] const RootInfo& info) override {
     UNIMPLEMENTED(FATAL);
   }
-  bool IsNullOrMarkedHeapReference([[maybe_unused]] mirror::HeapReference<mirror::Object>* obj,
-                                   [[maybe_unused]] bool do_atomic_update) override {
+  bool IsNullOrMarkedHeapReference(
+      [[maybe_unused]] mirror::HeapReference<mirror::Object>* obj) override {
     UNIMPLEMENTED(FATAL);
     UNREACHABLE();
   }
@@ -117,7 +117,7 @@ class MarkCompact final : public GarbageCollector {
   using SigbusCounterType = uint32_t;
 
   static constexpr size_t kAlignment = kObjectAlignment;
-  static constexpr int kCopyMode = -1;
+  static constexpr int kUffdMode = -1;
   // Fake file descriptor for fall back mode (when uffd isn't available)
   static constexpr int kFallbackMode = -3;
   static constexpr int kFdUnused = -2;
@@ -141,6 +141,8 @@ class MarkCompact final : public GarbageCollector {
   // Called by SIGBUS handler. NO_THREAD_SAFETY_ANALYSIS for mutator-lock, which
   // is asserted in the function.
   bool SigbusHandler(siginfo_t* info) REQUIRES(!lock_) NO_THREAD_SAFETY_ANALYSIS;
+  // Called by SIGSYS handler to detect seccomp deny-listed syscalls.
+  bool SigsysHandler(siginfo_t* info, void* context);
 
   GcType GetGcType() const override { return kGcTypePartial; }
 
@@ -172,10 +174,8 @@ class MarkCompact final : public GarbageCollector {
       REQUIRES_SHARED(Locks::mutator_lock_)
       REQUIRES(Locks::heap_bitmap_lock_);
 
-  bool IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* obj,
-                                   bool do_atomic_update) override
-      REQUIRES_SHARED(Locks::mutator_lock_)
-      REQUIRES(Locks::heap_bitmap_lock_);
+  bool IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* obj) override
+      REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(Locks::heap_bitmap_lock_);
 
   void RevokeAllThreadLocalBuffers() override;
 
@@ -186,13 +186,11 @@ class MarkCompact final : public GarbageCollector {
   mirror::Object* IsMarked(mirror::Object* obj) override
       REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_);
 
-  mirror::Object* GetFromSpaceAddrFromBarrier(mirror::Object* old_ref) {
+  mirror::Object* GetFromSpaceAddrFromBarrier(mirror::Object* old_ref) const {
     CHECK(compacting_);
-    if (HasAddress(old_ref)) {
-      return GetFromSpaceAddr(old_ref);
-    }
-    return old_ref;
+    return GetFromAddrAllSpaces(old_ref);
   }
+
   // Called from Heap::PostForkChildAction() for non-zygote processes and from
   // PrepareForCompaction() for zygote processes. Returns true if uffd was
   // created or was already done.
@@ -302,21 +300,37 @@ class MarkCompact final : public GarbageCollector {
     }
   };
 
-  static bool HasAddress(mirror::Object* obj, uint8_t* begin, uint8_t* end) {
-    uint8_t* ptr = reinterpret_cast<uint8_t*>(obj);
-    return ptr >= begin && ptr < end;
+  static bool HasAddress(uint8_t* addr, uint8_t* begin, uint8_t* end) {
+    return addr >= begin && addr < end;
   }
 
-  bool HasAddress(mirror::Object* obj) const {
+  static bool HasAddress(void* obj, uint8_t* begin, uint8_t* end) {
+    return HasAddress(reinterpret_cast<uint8_t*>(obj), begin, end);
+  }
+
+  bool HasAddress(void* obj) const {
     return HasAddress(obj, moving_space_begin_, moving_space_end_);
   }
   // For a given object address in pre-compact space, return the corresponding
   // address in the from-space, where heap pages are relocated in the compaction
   // pause.
-  mirror::Object* GetFromSpaceAddr(mirror::Object* obj) const {
+  template <typename T>
+  T* GetFromSpaceAddr(T* obj) const {
     DCHECK(HasAddress(obj)) << " obj=" << obj;
-    return reinterpret_cast<mirror::Object*>(reinterpret_cast<uintptr_t>(obj)
-                                             + from_space_slide_diff_);
+    return reinterpret_cast<T*>(reinterpret_cast<uintptr_t>(obj) + from_space_slide_diff_);
+  }
+
+  mirror::Object* GetFromAddrAllSpaces(mirror::Object* old_ref) const {
+    if (HasAddress(old_ref)) {
+      return GetFromSpaceAddr(old_ref);
+    }
+    return old_ref;
+  }
+
+  template <typename T>
+  inline T* GetToSpaceAddr(T* addr) const {
+    DCHECK(from_space_map_.HasAddress(addr));
+    return reinterpret_cast<T*>(reinterpret_cast<uint8_t*>(addr) - from_space_slide_diff_);
   }
 
   inline bool IsOnAllocStack(mirror::Object* ref)
@@ -326,14 +340,11 @@ class MarkCompact final : public GarbageCollector {
   template <typename Callback>
   void VerifyObject(mirror::Object* ref, Callback& callback) const
       REQUIRES_SHARED(Locks::mutator_lock_);
-  // Check if the obj is within heap and has a klass which is likely to be valid
-  // mirror::Class.
-  bool IsValidObject(mirror::Object* obj) const REQUIRES_SHARED(Locks::mutator_lock_);
   void InitializePhase();
   void FinishPhase(bool performed_compaction)
       REQUIRES(!Locks::mutator_lock_, !Locks::heap_bitmap_lock_, !lock_);
   void MarkingPhase() REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(!Locks::heap_bitmap_lock_);
-  void CompactionPhase() REQUIRES_SHARED(Locks::mutator_lock_);
+  void CompactionPhase() REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(!Locks::heap_bitmap_lock_);
 
   void SweepSystemWeaks(Thread* self, Runtime* runtime, const bool paused)
       REQUIRES_SHARED(Locks::mutator_lock_)
@@ -397,7 +408,7 @@ class MarkCompact final : public GarbageCollector {
   // Updates GC-roots and protects heap so that during the concurrent
   // compaction phase we can receive faults and compact the corresponding pages
   // on the fly.
-  void CompactionPause() REQUIRES(Locks::mutator_lock_);
+  void CompactionPause() REQUIRES(Locks::mutator_lock_, !Locks::heap_bitmap_lock_);
   // Compute offsets (in chunk_info_vec_) and other data structures required
   // during concurrent compaction. Also determines a black-dense region at the
   // beginning of the moving space which is not compacted. Returns false if
@@ -415,11 +426,13 @@ class MarkCompact final : public GarbageCollector {
                    uint32_t offset,
                    uint8_t* addr,
                    uint8_t* to_space_addr,
-                   bool needs_memset_zero) REQUIRES_SHARED(Locks::mutator_lock_);
+                   bool needs_memset_zero)
+      REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_);
   // Compact the bump-pointer space. Pass page that should be used as buffer for
   // userfaultfd.
   template <int kMode>
-  void CompactMovingSpace(uint8_t* page) REQUIRES_SHARED(Locks::mutator_lock_);
+  void CompactMovingSpace(uint8_t* page)
+      REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_);
 
   // Compact the given page as per func and change its state. Also map/copy the
   // page, if required. Returns true if the page was compacted, else false.
@@ -429,18 +442,20 @@ class MarkCompact final : public GarbageCollector {
                                                      uint8_t* page,
                                                      bool map_immediately,
                                                      CompactionFn func)
-      REQUIRES_SHARED(Locks::mutator_lock_);
+      REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_);
 
   // Update all the objects in the given non-moving page. 'first' object
-  // could have started in some preceding page.
-  template <bool kSetupForGenerational>
+  // could have started in some preceding page. 'kObjInBlackDense' is true when
+  // called for black-dense/old-gen pages, and false when called for non-moving
+  // space pages.
+  template <bool kSetupForGenerational, bool kObjInBlackDense>
   void UpdateNonMovingPage(mirror::Object* first,
                            uint8_t* page,
                            ptrdiff_t from_space_diff,
                            accounting::ContinuousSpaceBitmap* bitmap)
-      REQUIRES_SHARED(Locks::mutator_lock_);
+      REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_);
   // Update all the references in the non-moving space.
-  void UpdateNonMovingSpace() REQUIRES_SHARED(Locks::mutator_lock_);
+  void UpdateNonMovingSpace() REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_);
 
   // For all the pages in non-moving space, find the first object that overlaps
   // with the pages' start address, and store in first_objs_non_moving_space_ array.
@@ -474,7 +489,8 @@ class MarkCompact final : public GarbageCollector {
                       uint32_t first_chunk_size,
                       uint8_t* const pre_compact_page,
                       uint8_t* dest,
-                      bool needs_memset_zero) REQUIRES_SHARED(Locks::mutator_lock_);
+                      bool needs_memset_zero)
+      REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_);
 
   // Perform reference-processing and the likes before sweeping the non-movable
   // spaces.
@@ -641,6 +657,8 @@ class MarkCompact final : public GarbageCollector {
   // returns. Returns number of bytes (multiple of page-size) mapped.
   size_t CopyIoctl(
       void* dst, void* buffer, size_t length, bool return_on_contention, bool tolerate_enoent);
+  // Move 'len/page-size' pages from 'src' to 'dst'.
+  size_t MoveIoctl(void* dst, void* src, size_t len, bool tolerate_enoent);
 
   // Called after updating linear-alloc page(s) to map the page. It first
   // updates the state of the pages to kProcessedAndMapping and after ioctl to
@@ -681,9 +699,16 @@ class MarkCompact final : public GarbageCollector {
   // Scan old-gen for young GCs by looking for cards that are at least 'aged' in
   // the card-table corresponding to moving and non-moving spaces.
   void ScanOldGenObjects() REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_);
+  // Return free pages from 'from-space' that can be used to copy objects into
+  // and then passed onto userfaultfd ioctls. Return nullptr if no page is
+  // available. Size must be a multiple of page-size.
+  uint8_t* GetFreePagesForMapping(size_t size, bool atomic);
 
   // Verify that cards corresponding to objects containing references to
   // young-gen are dirty.
+  void VerifyNoMissingGenerationalCardMarks()
+      REQUIRES(Locks::heap_bitmap_lock_, Locks::mutator_lock_);
+  // Verify that card corresponding to a marked object with unmarked reference is dirty.
   void VerifyNoMissingCardMarks() REQUIRES(Locks::heap_bitmap_lock_, Locks::mutator_lock_);
   // Verify that post-GC objects (all objects except the ones allocated after
   // marking pause) are valid with valid references in them. Bitmap corresponding
@@ -692,6 +717,77 @@ class MarkCompact final : public GarbageCollector {
   void VerifyPostGCObjects(bool performed_compaction, uint8_t* mark_bitmap_clear_end)
       REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_);
 
+  // Like ProcessMarkStack(), but ignores null entries.
+  void ProcessMarkStackNonNull() REQUIRES_SHARED(Locks::mutator_lock_)
+      REQUIRES(Locks::heap_bitmap_lock_);
+  // Process one object popped out of mark_stack. Expects obj to be non-null.
+  void ProcessMarkObject(mirror::Object* obj) REQUIRES_SHARED(Locks::mutator_lock_)
+      REQUIRES(Locks::heap_bitmap_lock_);
+  // Called to assess if it's safe to use MOVE ioctl, both from kernel bug-fixes
+  // as well as seccomp filter point of view.
+  bool MoveIoctlKernelCheck();
+
+  // Returns class-size of 'klass'. If kHandleZeroReads == true, then it checks
+  // for 0 class-size, which can happen if, during compaction, the page
+  // containing klass' size gets moved to the to-space and we are reading from a
+  // shared zero-page. In that case, we read class-size from 'moved_klass'.
+  template <bool kHandleZeroReads, VerifyObjectFlags kVerifyFlags>
+  size_t GetClassSize(mirror::Class* klass, mirror::Class* moved_klass)
+      REQUIRES_SHARED(Locks::mutator_lock_);
+  // Moves the from-space 'page' to to-space. If the move operation is already
+  // in-progress by another thread, then it waits until the page's status
+  // changes to 'mapped'.
+  void MoveBlackDensePageForUpdate(uint8_t* page)
+      REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_);
+
+  // Updates references in the given object-array in [begin, end) range by
+  // calling the visitor. Returns length of the array. If kHandleZeroReads ==
+  // true, then makes sure it is reading the correct length and not 0 from a
+  // shared zero-page because the containing page has been moved to to-space.
+  template <bool kHandleZeroReads, VerifyObjectFlags kVerifyFlags, typename Visitor>
+  int32_t UpdateObjArrayReferences(mirror::ObjectArray<mirror::Object>* arr,
+                                   Visitor& visitor,
+                                   MemberOffset begin,
+                                   MemberOffset end)
+      REQUIRES_SHARED(Locks::heap_bitmap_lock_, Locks::mutator_lock_);
+
+  // Updates static references in the given klass by calling visitor on each of
+  // them. If kHandleZeroReads == true, then makes sure that the static
+  // references' offset and count is not incorrectly read to be 0 from shared
+  // zero-page in from space.
+  template <bool kHandleZeroReads, VerifyObjectFlags kVerifyFlags, typename Visitor>
+  void UpdateStaticFieldsReferences(mirror::Class* klass, Visitor& visitor)
+      REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_);
+
+  // Updates obj's instance references by calling visitor on each of them. Makes
+  // sure that if the klass is in black-dense region then it's not incorrectly
+  // assuming instance-reference bitmap to be 0 due to shared zero-page.
+  template <VerifyObjectFlags kVerifyFlags, typename Visitor>
+  void UpdateInstanceFieldsReferences(mirror::Object* obj,
+                                      mirror::Class* to_klass,
+                                      mirror::Class* klass,
+                                      const Visitor& visitor)
+      REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_);
+
+  // Updates references in 'obj' by calling 'visitor' on each reference during
+  // compaction. Returns object-size, if kFetchObjSize == true. If
+  // kObjInBlackDense == true, then it takes extra precautions when reading
+  // fields from 'obj' in case a part of it is on a page which has already moved
+  // to to-space.
+  template <bool kFetchObjSize,
+            bool kObjInBlackDense,
+            VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags,
+            typename Visitor>
+  size_t UpdateRefsForCompaction(mirror::Object* obj,
+                                 const Visitor& visitor,
+                                 MemberOffset begin,
+                                 MemberOffset end)
+      REQUIRES_SHARED(Locks::heap_bitmap_lock_, Locks::mutator_lock_);
+
+  // Vector to hold thread-local overflow arrays (and the number of entries in
+  // there) of gc-roots found during mutator-stack scanning in marking phase.
+  std::vector<std::pair<StackReference<mirror::Object>*, size_t>>* overflow_arrays_
+      GUARDED_BY(lock_);
   // For checkpoints
   Barrier gc_barrier_;
   // Required only when mark-stack is accessed in shared mode, which happens
@@ -800,10 +896,12 @@ class MarkCompact final : public GarbageCollector {
   // All the pages in [last_reclaimable_page_, last_reclaimed_page_) in
   // from-space are available to store compacted contents for batching until the
   // next time madvise is called.
-  uint8_t* last_reclaimable_page_;
+  // Declared atomic as gc-thread may write to it while mutators are accessing
+  // it concurrently.
+  std::atomic<uint8_t*> last_reclaimable_page_;
   // [cur_reclaimable_page_, last_reclaimed_page_) have been used to store
   // compacted contents for batching.
-  uint8_t* cur_reclaimable_page_;
+  std::atomic<uint8_t*> cur_reclaimable_page_;
 
   // Mark bits for non-moving space
   accounting::ContinuousSpaceBitmap* non_moving_space_bitmap_;
@@ -827,10 +925,6 @@ class MarkCompact final : public GarbageCollector {
   // First object for every page. It could be greater than the page's start
   // address, or null if the page is empty.
   ObjReference* first_objs_non_moving_space_;
-
-  // Cache (from_space_begin_ - bump_pointer_space_->Begin()) so that we can
-  // compute from-space address of a given pre-comapct address efficiently.
-  ptrdiff_t from_space_slide_diff_;
   uint8_t* from_space_begin_;
 
   // The moving space markers are ordered as follows:
@@ -852,10 +946,14 @@ class MarkCompact final : public GarbageCollector {
   // Set to true when doing young gen collection.
   bool young_gen_;
   const bool use_generational_;
+  bool use_move_ioctl_;
   // True while compacting.
   bool compacting_;
   // Mark bits for main space
   accounting::ContinuousSpaceBitmap* const moving_space_bitmap_;
+  // Cache (from_space_begin_ - bump_pointer_space_->Begin()) so that we can
+  // compute from-space address of a given pre-comapct address efficiently.
+  ptrdiff_t from_space_slide_diff_;
   // Cached values of moving-space range to optimize checking if reference
   // belongs to moving-space or not. May get updated if and when heap is clamped.
   uint8_t* const moving_space_begin_;
@@ -910,7 +1008,6 @@ class MarkCompact final : public GarbageCollector {
   // END HOT FIELDS: accessed per reference update
   // END HOT FIELDS: accessed per object
 
-  uint8_t* conc_compaction_termination_page_;
   PointerSize pointer_size_;
   // Userfault file descriptor, accessed only by the GC itself.
   // kFallbackMode value indicates that we are in the fallback mode.
@@ -945,6 +1042,7 @@ class MarkCompact final : public GarbageCollector {
   void* prev_post_compact_end_;
   void* prev_black_dense_end_;
   void* prev_black_allocations_begin_;
+  void* prev_moving_space_end_at_compaction_;
   bool prev_gc_young_;
   bool prev_gc_performed_compaction_;
   // Timestamp when the read-barrier is enabled
diff --git a/runtime/gc/collector/mark_sweep-inl.h b/runtime/gc/collector/mark_sweep-inl.h
index 2eac037940..188604e3c3 100644
--- a/runtime/gc/collector/mark_sweep-inl.h
+++ b/runtime/gc/collector/mark_sweep-inl.h
@@ -40,7 +40,7 @@ inline void MarkSweep::ScanObjectVisit(mirror::Object* obj,
     uint32_t class_flags = klass->GetClassFlags();
     if ((class_flags & mirror::kClassFlagNoReferenceFields) != 0) {
       ++no_reference_class_count_;
-    } else if (class_flags == mirror::kClassFlagNormal || class_flags == mirror::kClassFlagRecord) {
+    } else if ((class_flags & (mirror::kClassFlagNormal | mirror::kClassFlagRecord)) != 0) {
       ++normal_count_;
     } else if (class_flags == mirror::kClassFlagObjectArray) {
       ++object_array_count_;
diff --git a/runtime/gc/collector/mark_sweep.cc b/runtime/gc/collector/mark_sweep.cc
index 918e97fb96..9fcd4600de 100644
--- a/runtime/gc/collector/mark_sweep.cc
+++ b/runtime/gc/collector/mark_sweep.cc
@@ -403,8 +403,7 @@ inline void MarkSweep::MarkObjectNonNullParallel(mirror::Object* obj) {
   }
 }
 
-bool MarkSweep::IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* ref,
-                                            [[maybe_unused]] bool do_atomic_update) {
+bool MarkSweep::IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* ref) {
   mirror::Object* obj = ref->AsMirrorPtr();
   if (obj == nullptr) {
     return true;
diff --git a/runtime/gc/collector/mark_sweep.h b/runtime/gc/collector/mark_sweep.h
index b022c7bcf0..367a1ffc2d 100644
--- a/runtime/gc/collector/mark_sweep.h
+++ b/runtime/gc/collector/mark_sweep.h
@@ -178,10 +178,8 @@ class MarkSweep : public GarbageCollector {
   void VerifyIsLive(const mirror::Object* obj)
       REQUIRES_SHARED(Locks::mutator_lock_, Locks::heap_bitmap_lock_);
 
-  bool IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* ref,
-                                   bool do_atomic_update) override
-      REQUIRES(Locks::heap_bitmap_lock_)
-      REQUIRES_SHARED(Locks::mutator_lock_);
+  bool IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* ref) override
+      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_);
 
   void VisitRoots(mirror::Object*** roots, size_t count, const RootInfo& info) override
       REQUIRES(Locks::heap_bitmap_lock_)
diff --git a/runtime/gc/collector/semi_space.cc b/runtime/gc/collector/semi_space.cc
index 9ec44fa765..e6bd38d426 100644
--- a/runtime/gc/collector/semi_space.cc
+++ b/runtime/gc/collector/semi_space.cc
@@ -609,9 +609,7 @@ mirror::Object* SemiSpace::IsMarked(mirror::Object* obj) {
   return mark_bitmap_->Test(obj) ? obj : nullptr;
 }
 
-bool SemiSpace::IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* object,
-                                            // SemiSpace does the GC in a pause. No CAS needed.
-                                            [[maybe_unused]] bool do_atomic_update) {
+bool SemiSpace::IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* object) {
   mirror::Object* obj = object->AsMirrorPtr();
   if (obj == nullptr) {
     return true;
@@ -621,8 +619,8 @@ bool SemiSpace::IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object
     return false;
   }
   if (new_obj != obj) {
-    // Write barrier is not necessary since it still points to the same object, just at a different
-    // address.
+    // SemiSpace does the GC in a pause. No CAS needed.  Write barrier is not necessary since it
+    // still points to the same object, just at a different address.
     object->Assign(new_obj);
   }
   return true;
diff --git a/runtime/gc/collector/semi_space.h b/runtime/gc/collector/semi_space.h
index e6d1266e55..c72ce98a28 100644
--- a/runtime/gc/collector/semi_space.h
+++ b/runtime/gc/collector/semi_space.h
@@ -160,10 +160,8 @@ class SemiSpace : public GarbageCollector {
       REQUIRES(Locks::mutator_lock_)
       REQUIRES_SHARED(Locks::heap_bitmap_lock_);
 
-  bool IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* object,
-                                   bool do_atomic_update) override
-      REQUIRES(Locks::mutator_lock_)
-      REQUIRES_SHARED(Locks::heap_bitmap_lock_);
+  bool IsNullOrMarkedHeapReference(mirror::HeapReference<mirror::Object>* object) override
+      REQUIRES(Locks::mutator_lock_) REQUIRES_SHARED(Locks::heap_bitmap_lock_);
 
   // Marks or unmarks a large object based on whether or not set is true. If set is true, then we
   // mark, otherwise we unmark.
diff --git a/runtime/gc/heap-inl.h b/runtime/gc/heap-inl.h
index b309dfad06..df58a59359 100644
--- a/runtime/gc/heap-inl.h
+++ b/runtime/gc/heap-inl.h
@@ -292,10 +292,7 @@ inline mirror::Object* Heap::AllocLargeObject(Thread* self,
   auto klass_wrapper = hs.NewHandleWrapper(klass);
   mirror::Object* obj = AllocObjectWithAllocator<kInstrumented, false, PreFenceVisitor>
                         (self, *klass, byte_count, kAllocatorTypeLOS, pre_fence_visitor);
-  // Java Heap Profiler check and sample allocation.
-  if (GetHeapSampler().IsEnabled()) {
-    JHPCheckNonTlabSampleAllocation(self, obj, byte_count);
-  }
+  ReportAllocationForJavaHeapProf(obj, byte_count);
   return obj;
 }
 
@@ -482,6 +479,12 @@ inline bool Heap::ShouldConcurrentGCForJava(size_t new_num_bytes_allocated) {
   return new_num_bytes_allocated >= concurrent_start_bytes_;
 }
 
+inline void Heap::ReportAllocationForJavaHeapProf(mirror::Object* obj, size_t alloc_size) {
+  if (heap_sampler_.IsEnabled()) {
+    heap_sampler_.ReportAllocation(obj, alloc_size);
+  }
+}
+
 }  // namespace gc
 }  // namespace art
 
diff --git a/runtime/gc/heap.cc b/runtime/gc/heap.cc
index dcb25a08d7..073aab3986 100644
--- a/runtime/gc/heap.cc
+++ b/runtime/gc/heap.cc
@@ -161,8 +161,10 @@ static constexpr bool kCompactZygote = kMovingCollector;
 // How many reserve entries are at the end of the allocation stack, these are only needed if the
 // allocation stack overflows.
 static constexpr size_t kAllocationStackReserveSize = 1024;
-// Default mark stack size in bytes.
-static const size_t kDefaultMarkStackSize = 64 * KB;
+// Default mark stack size in bytes. Use a smaller size for debug builds to
+// stress stack expansion logic in GC code.
+static const size_t kDefaultMarkStackSize =
+    kIsDebugBuild ? (kMaxPageSize / sizeof(StackReference<mirror::Object>)) : 64 * KB;
 // Define space name.
 static const char* kDlMallocSpaceName[2] = {"main dlmalloc space", "main dlmalloc space 1"};
 static const char* kRosAllocSpaceName[2] = {"main rosalloc space", "main rosalloc space 1"};
@@ -804,7 +806,7 @@ Heap::Heap(size_t initial_size,
     InitPerfettoJavaHeapProf();
   } else {
     // Disable the Java Heap Profiler.
-    GetHeapSampler().DisableHeapSampler();
+    heap_sampler_.DisableHeapSampler();
   }
 
   instrumentation::Instrumentation* const instrumentation = runtime->GetInstrumentation();
@@ -4306,9 +4308,7 @@ void Heap::RegisterNativeAllocation(JNIEnv* env, size_t bytes) {
     CheckGCForNative(Thread::ForEnv(env));
   }
   // Heap profiler treats this as a Java allocation with a null object.
-  if (GetHeapSampler().IsEnabled()) {
-    JHPCheckNonTlabSampleAllocation(Thread::Current(), nullptr, bytes);
-  }
+  ReportAllocationForJavaHeapProf(nullptr, bytes);
 }
 
 void Heap::RegisterNativeFree(JNIEnv*, size_t bytes) {
@@ -4458,39 +4458,6 @@ void Heap::InitPerfettoJavaHeapProf() {
   VLOG(heap) << "Java Heap Profiler Initialized";
 }
 
-void Heap::JHPCheckNonTlabSampleAllocation(Thread* self, mirror::Object* obj, size_t alloc_size) {
-  bool take_sample = false;
-  size_t bytes_until_sample = 0;
-  HeapSampler& prof_heap_sampler = GetHeapSampler();
-  // An allocation occurred, sample it, even if non-Tlab.
-  // In case take_sample is already set from the previous GetSampleOffset
-  // because we tried the Tlab allocation first, we will not use this value.
-  // A new value is generated below. Also bytes_until_sample will be updated.
-  // Note that we are not using the return value from the GetSampleOffset in
-  // the NonTlab case here.
-  prof_heap_sampler.GetSampleOffset(
-      alloc_size, self->GetTlabPosOffset(), &take_sample, &bytes_until_sample);
-  prof_heap_sampler.SetBytesUntilSample(bytes_until_sample);
-  if (take_sample) {
-    prof_heap_sampler.ReportSample(obj, alloc_size);
-  }
-  VLOG(heap) << "JHP:NonTlab Non-moving or Large Allocation or RegisterNativeAllocation";
-}
-
-size_t Heap::JHPCalculateNextTlabSize(Thread* self,
-                                      size_t jhp_def_tlab_size,
-                                      size_t alloc_size,
-                                      bool* take_sample,
-                                      size_t* bytes_until_sample) {
-  size_t next_sample_point = GetHeapSampler().GetSampleOffset(
-      alloc_size, self->GetTlabPosOffset(), take_sample, bytes_until_sample);
-  return std::min(next_sample_point, jhp_def_tlab_size);
-}
-
-void Heap::AdjustSampleOffset(size_t adjustment) {
-  GetHeapSampler().AdjustSampleOffset(adjustment);
-}
-
 void Heap::CheckGcStressMode(Thread* self, ObjPtr<mirror::Object>* obj) {
   DCHECK(gc_stress_mode_);
   auto* const runtime = Runtime::Current();
@@ -4581,19 +4548,14 @@ mirror::Object* Heap::AllocWithNewTLAB(Thread* self,
                                        size_t* usable_size,
                                        size_t* bytes_tl_bulk_allocated) {
   mirror::Object* ret = nullptr;
-  bool take_sample = false;
-  size_t bytes_until_sample = 0;
-  bool jhp_enabled = GetHeapSampler().IsEnabled();
+  size_t pre_tlab_size = self->TlabSize();
 
   if (kUsePartialTlabs && alloc_size <= self->TlabRemainingCapacity()) {
     DCHECK_GT(alloc_size, self->TlabSize());
     // There is enough space if we grow the TLAB. Lets do that. This increases the
     // TLAB bytes.
     const size_t min_expand_size = alloc_size - self->TlabSize();
-    size_t next_tlab_size =
-        jhp_enabled ? JHPCalculateNextTlabSize(
-                          self, kPartialTlabSize, alloc_size, &take_sample, &bytes_until_sample) :
-                      kPartialTlabSize;
+    size_t next_tlab_size = heap_sampler_.NextTlabSize(kPartialTlabSize);
     const size_t expand_bytes = std::max(
         min_expand_size,
         std::min(self->TlabRemainingCapacity() - self->TlabSize(), next_tlab_size));
@@ -4609,11 +4571,8 @@ mirror::Object* Heap::AllocWithNewTLAB(Thread* self,
     // TODO: for large allocations, which are rare, maybe we should allocate
     // that object and return. There is no need to revoke the current TLAB,
     // particularly if it's mostly unutilized.
-    size_t next_tlab_size = RoundDown(alloc_size + kDefaultTLABSize, gPageSize) - alloc_size;
-    if (jhp_enabled) {
-      next_tlab_size = JHPCalculateNextTlabSize(
-          self, next_tlab_size, alloc_size, &take_sample, &bytes_until_sample);
-    }
+    size_t next_tlab_size = heap_sampler_.NextTlabSize(
+        RoundDown(alloc_size + kDefaultTLABSize, gPageSize) - alloc_size);
     const size_t new_tlab_size = alloc_size + next_tlab_size;
     if (UNLIKELY(IsOutOfMemoryOnAllocation(allocator_type, new_tlab_size, grow))) {
       return nullptr;
@@ -4623,9 +4582,6 @@ mirror::Object* Heap::AllocWithNewTLAB(Thread* self,
     if (!bump_pointer_space_->AllocNewTlab(self, new_tlab_size, bytes_tl_bulk_allocated)) {
       return nullptr;
     }
-    if (jhp_enabled) {
-      VLOG(heap) << "JHP:kAllocatorTypeTLAB, New Tlab bytes allocated= " << new_tlab_size;
-    }
   } else {
     DCHECK(allocator_type == kAllocatorTypeRegionTLAB);
     DCHECK(region_space_ != nullptr);
@@ -4634,12 +4590,8 @@ mirror::Object* Heap::AllocWithNewTLAB(Thread* self,
       if (LIKELY(!IsOutOfMemoryOnAllocation(allocator_type,
                                             space::RegionSpace::kRegionSize,
                                             grow))) {
-        size_t next_pr_tlab_size =
-            kUsePartialTlabs ? kPartialTlabSize : gc::space::RegionSpace::kRegionSize;
-        if (jhp_enabled) {
-          next_pr_tlab_size = JHPCalculateNextTlabSize(
-              self, next_pr_tlab_size, alloc_size, &take_sample, &bytes_until_sample);
-        }
+        size_t next_pr_tlab_size = heap_sampler_.NextTlabSize(
+            kUsePartialTlabs ? kPartialTlabSize : gc::space::RegionSpace::kRegionSize);
         const size_t new_tlab_size = kUsePartialTlabs
             ? std::max(alloc_size, next_pr_tlab_size)
             : next_pr_tlab_size;
@@ -4650,9 +4602,7 @@ mirror::Object* Heap::AllocWithNewTLAB(Thread* self,
                                                       bytes_allocated,
                                                       usable_size,
                                                       bytes_tl_bulk_allocated);
-          if (jhp_enabled) {
-            JHPCheckNonTlabSampleAllocation(self, ret, alloc_size);
-          }
+          ReportAllocationForJavaHeapProf(ret, alloc_size);
           return ret;
         }
         // Fall-through to using the TLAB below.
@@ -4663,9 +4613,7 @@ mirror::Object* Heap::AllocWithNewTLAB(Thread* self,
                                                       bytes_allocated,
                                                       usable_size,
                                                       bytes_tl_bulk_allocated);
-          if (jhp_enabled) {
-            JHPCheckNonTlabSampleAllocation(self, ret, alloc_size);
-          }
+          ReportAllocationForJavaHeapProf(ret, alloc_size);
           return ret;
         }
         // Neither tlab or non-tlab works. Give up.
@@ -4678,9 +4626,7 @@ mirror::Object* Heap::AllocWithNewTLAB(Thread* self,
                                                     bytes_allocated,
                                                     usable_size,
                                                     bytes_tl_bulk_allocated);
-        if (jhp_enabled) {
-          JHPCheckNonTlabSampleAllocation(self, ret, alloc_size);
-        }
+        ReportAllocationForJavaHeapProf(ret, alloc_size);
         return ret;
       }
       return nullptr;
@@ -4695,13 +4641,9 @@ mirror::Object* Heap::AllocWithNewTLAB(Thread* self,
   // JavaHeapProfiler: Send the thread information about this allocation in case a sample is
   // requested.
   // This is the fallthrough from both the if and else if above cases => Cases that use TLAB.
-  if (jhp_enabled) {
-    if (take_sample) {
-      GetHeapSampler().ReportSample(ret, alloc_size);
-      // Update the bytes_until_sample now that the allocation is already done.
-      GetHeapSampler().SetBytesUntilSample(bytes_until_sample);
-    }
-    VLOG(heap) << "JHP:Fallthrough Tlab allocation";
+  if (heap_sampler_.IsEnabled()) {
+    size_t post_tlab_size = self->TlabSize();
+    heap_sampler_.ReportTlabAllocation(ret, alloc_size, pre_tlab_size, post_tlab_size);
   }
 
   return ret;
diff --git a/runtime/gc/heap.h b/runtime/gc/heap.h
index 7e30c3d14e..e56e9a8f24 100644
--- a/runtime/gc/heap.h
+++ b/runtime/gc/heap.h
@@ -280,10 +280,7 @@ class Heap {
                                                                   num_bytes,
                                                                   GetCurrentNonMovingAllocator(),
                                                                   pre_fence_visitor);
-    // Java Heap Profiler check and sample allocation.
-    if (GetHeapSampler().IsEnabled()) {
-      JHPCheckNonTlabSampleAllocation(self, obj, num_bytes);
-    }
+    ReportAllocationForJavaHeapProf(obj, num_bytes);
     return obj;
   }
 
@@ -912,26 +909,8 @@ class Heap {
   }
   uint64_t GetPreOomeGcCount() const;
 
-  // Perfetto Art Heap Profiler Support.
-  HeapSampler& GetHeapSampler() {
-    return heap_sampler_;
-  }
-
   void InitPerfettoJavaHeapProf();
-  // In NonTlab case: Check whether we should report a sample allocation and if so report it.
-  // Also update state (bytes_until_sample).
-  // By calling JHPCheckNonTlabSampleAllocation from different functions for Large allocations and
-  // non-moving allocations we are able to use the stack to identify these allocations separately.
-  EXPORT void JHPCheckNonTlabSampleAllocation(Thread* self, mirror::Object* ret, size_t alloc_size);
-  // In Tlab case: Calculate the next tlab size (location of next sample point) and whether
-  // a sample should be taken.
-  size_t JHPCalculateNextTlabSize(Thread* self,
-                                  size_t jhp_def_tlab_size,
-                                  size_t alloc_size,
-                                  bool* take_sample,
-                                  size_t* bytes_until_sample);
-  // Reduce the number of bytes to the next sample position by this adjustment.
-  void AdjustSampleOffset(size_t adjustment);
+  ALWAYS_INLINE void ReportAllocationForJavaHeapProf(mirror::Object* obj, size_t alloc_size);
 
   // Allocation tracking support
   // Callers to this function use double-checked locking to ensure safety on allocation_records_
diff --git a/runtime/gc/reference_processor.cc b/runtime/gc/reference_processor.cc
index 6420fbbff6..26aa4d776a 100644
--- a/runtime/gc/reference_processor.cc
+++ b/runtime/gc/reference_processor.cc
@@ -325,7 +325,7 @@ void ReferenceProcessor::DelayReferenceReferent(ObjPtr<mirror::Class> klass,
   mirror::HeapReference<mirror::Object>* referent = ref->GetReferentReferenceAddr();
   // do_atomic_update needs to be true because this happens outside of the reference processing
   // phase.
-  if (!collector->IsNullOrMarkedHeapReference(referent, /*do_atomic_update=*/true)) {
+  if (!collector->IsNullOrMarkedHeapReference(referent)) {
     if (UNLIKELY(collector->IsTransactionActive())) {
       // In transaction mode, keep the referent alive and avoid any reference processing to avoid the
       // issue of rolling back reference processing.  do_atomic_update needs to be true because this
@@ -405,11 +405,10 @@ SelfDeletingTask* ReferenceProcessor::CollectClearedReferences(Thread* self) {
 void ReferenceProcessor::ClearReferent(ObjPtr<mirror::Reference> ref) {
   Thread* self = Thread::Current();
   MutexLock mu(self, *Locks::reference_processor_lock_);
-  // Need to wait until reference processing is done since IsMarkedHeapReference does not have a
-  // CAS. If we do not wait, it can result in the GC un-clearing references due to race conditions.
-  // This also handles the race where the referent gets cleared after a null check but before
-  // IsMarkedHeapReference is called.
-  WaitUntilDoneProcessingReferences(self);
+  // If the collector requires the mutator to update references, the IsNullOrMarkedHeapReference
+  // now uses a CAS to perform the update, as with other reference forwarding. Since this also
+  // cannot introduce new strong references, we go ahead and do this even while processing
+  // references.
   if (Runtime::Current()->IsActiveTransaction()) {
     ref->ClearReferent<true>();
   } else {
diff --git a/runtime/gc/reference_queue.cc b/runtime/gc/reference_queue.cc
index 82fd89ecb1..f1ac031076 100644
--- a/runtime/gc/reference_queue.cc
+++ b/runtime/gc/reference_queue.cc
@@ -139,7 +139,7 @@ void ReferenceQueue::ClearWhiteReferences(ReferenceQueue* cleared_references,
     mirror::HeapReference<mirror::Object>* referent_addr = ref->GetReferentReferenceAddr();
     // do_atomic_update is false because this happens during the reference processing phase where
     // Reference.clear() would block.
-    if (!collector->IsNullOrMarkedHeapReference(referent_addr, /*do_atomic_update=*/false)) {
+    if (!collector->IsNullOrMarkedHeapReference(referent_addr)) {
       // Referent is white, clear it.
       if (Runtime::Current()->IsActiveTransaction()) {
         ref->ClearReferent<true>();
@@ -172,7 +172,7 @@ FinalizerStats ReferenceQueue::EnqueueFinalizerReferences(ReferenceQueue* cleare
     mirror::HeapReference<mirror::Object>* referent_addr = ref->GetReferentReferenceAddr();
     // do_atomic_update is false because this happens during the reference processing phase where
     // Reference.clear() would block.
-    if (!collector->IsNullOrMarkedHeapReference(referent_addr, /*do_atomic_update=*/false)) {
+    if (!collector->IsNullOrMarkedHeapReference(referent_addr)) {
       ObjPtr<mirror::Object> forward_address = collector->MarkObject(referent_addr->AsMirrorPtr());
       // Move the updated referent to the zombie field.
       if (Runtime::Current()->IsActiveTransaction()) {
diff --git a/runtime/hidden_api_test.cc b/runtime/hidden_api_test.cc
index 39cca7cce6..d127b55891 100644
--- a/runtime/hidden_api_test.cc
+++ b/runtime/hidden_api_test.cc
@@ -656,8 +656,9 @@ TEST_F(HiddenApiTest, CheckMemberSignatureForProxyClass) {
 
   // Find the "method" virtual method.
   ArtMethod* method = nullptr;
-  for (auto& m : proxyClass->GetDeclaredVirtualMethods(kRuntimePointerSize)) {
-    if (strcmp("method", m.GetInterfaceMethodIfProxy(kRuntimePointerSize)->GetName()) == 0) {
+  for (auto& m : proxyClass->GetDeclaredMethods(kRuntimePointerSize)) {
+    if (m.IsVirtual() &&
+        strcmp("method", m.GetInterfaceMethodIfProxy(kRuntimePointerSize)->GetName()) == 0) {
       method = &m;
       break;
     }
diff --git a/runtime/interpreter/interpreter.cc b/runtime/interpreter/interpreter.cc
index 2665d00c8b..5cce5a1c02 100644
--- a/runtime/interpreter/interpreter.cc
+++ b/runtime/interpreter/interpreter.cc
@@ -22,6 +22,7 @@
 #include "common_dex_operations.h"
 #include "common_throws.h"
 #include "dex/dex_file_types.h"
+#include "interpreter/shadow_frame.h"
 #include "interpreter_common.h"
 #include "interpreter_switch_impl.h"
 #include "jit/jit.h"
@@ -33,6 +34,7 @@
 #include "shadow_frame-inl.h"
 #include "stack.h"
 #include "thread-inl.h"
+#include "thread.h"
 #include "unstarted_runtime.h"
 
 namespace art HIDDEN {
@@ -264,7 +266,13 @@ static inline JValue Execute(
 
   if (LIKELY(!from_deoptimize)) {  // Entering the method, but not via deoptimization.
     if (kIsDebugBuild) {
-      CHECK_EQ(shadow_frame.GetDexPC(), 0u);
+      // TODO(b/346542404): Check this precondition prorperly, and shouldn't emit method enter event
+      // when unparking a virtual thread.
+      bool is_virtual = kIsVirtualThreadEnabled &&
+                        self->AreVirtualThreadFlagsEnabled(VirtualThreadFlag::kIsVirtual);
+      if (!is_virtual) {
+        CHECK_EQ(shadow_frame.GetDexPC(), 0u);
+      }
       self->AssertNoPendingException();
     }
     ArtMethod *method = shadow_frame.GetMethod();
@@ -385,6 +393,11 @@ void EnterInterpreterFromInvoke(Thread* self,
   ShadowFrameAllocaUniquePtr shadow_frame_unique_ptr =
       CREATE_SHADOW_FRAME(num_regs, method, /* dex pc */ 0);
   ShadowFrame* shadow_frame = shadow_frame_unique_ptr.get();
+  if (kIsVirtualThreadEnabled &&
+      self->AreVirtualThreadFlagsEnabled(VirtualThreadFlag::kIsVirtual |
+                                         VirtualThreadFlag::kUnparking)) {
+    interpreter::FillVirtualThreadFrame(self, shadow_frame);
+  }
 
   size_t cur_reg = num_regs - num_ins;
   if (!method->IsStatic()) {
diff --git a/runtime/interpreter/interpreter_common.cc b/runtime/interpreter/interpreter_common.cc
index 5deb0e1881..31b8986d98 100644
--- a/runtime/interpreter/interpreter_common.cc
+++ b/runtime/interpreter/interpreter_common.cc
@@ -17,15 +17,23 @@
 #include "interpreter_common.h"
 
 #include <cmath>
+#include <cstddef>
+#include <cstdint>
+#include <iostream>
+#include <memory>
+#include <ostream>
 
+#include "android-base/logging.h"
 #include "base/casts.h"
 #include "base/pointer_size.h"
 #include "class_linker.h"
 #include "class_root-inl.h"
+#include "com_android_art_flags.h"
 #include "debugger.h"
 #include "dex/dex_file_types.h"
 #include "entrypoints/runtime_asm_entrypoints.h"
 #include "handle.h"
+#include "interpreter/shadow_frame.h"
 #include "intrinsics_enum.h"
 #include "intrinsics_list.h"
 #include "jit/jit.h"
@@ -39,16 +47,22 @@
 #include "mirror/emulated_stack_frame.h"
 #include "mirror/method_handle_impl-inl.h"
 #include "mirror/method_type-inl.h"
+#include "mirror/object.h"
 #include "mirror/object_array-alloc-inl.h"
 #include "mirror/object_array-inl.h"
+#include "mirror/object_array.h"
 #include "mirror/var_handle.h"
+#include "obj_ptr.h"
 #include "reflection-inl.h"
 #include "reflection.h"
 #include "shadow_frame-inl.h"
 #include "stack.h"
 #include "thread-inl.h"
+#include "thread.h"
+#include "thread_state.h"
 #include "var_handles.h"
 #include "well_known_classes-inl.h"
+#include "well_known_classes.h"
 
 namespace art HIDDEN {
 namespace interpreter {
@@ -1234,6 +1248,12 @@ static inline bool DoCallCommon(ArtMethod* called_method,
   ShadowFrameAllocaUniquePtr shadow_frame_unique_ptr =
       CREATE_SHADOW_FRAME(num_regs, called_method, /* dex pc */ 0);
   ShadowFrame* new_shadow_frame = shadow_frame_unique_ptr.get();
+  // Restore the values of virtual registers if a virtual thread is unparking
+  if (kIsVirtualThreadEnabled && self->IsVirtualThreadUnparking()) {
+    FillVirtualThreadFrame(self, new_shadow_frame);
+  }
+  // TODO: Consider skip the following operations, e.g. copying registers, if
+  //   a virtual thread is unparking.
 
   // Initialize new shadow frame by copying the registers from the callee shadow frame.
   if (!shadow_frame.GetMethod()->SkipAccessChecks()) {
@@ -1353,6 +1373,103 @@ static inline bool DoCallCommon(ArtMethod* called_method,
   return !self->IsExceptionPending();
 }
 
+void FillVirtualThreadFrame(Thread* self, ShadowFrame* frame) {
+  ScopedAssertNoThreadSuspension ns("No thread suspension when filling virtual thread frame)");
+  ObjPtr<mirror::Object> jpeer = self->GetPeer();
+  ObjPtr<mirror::Object> v_context = WellKnownClasses::java_lang_Thread_target->GetObject(jpeer);
+  DCHECK(v_context->GetClass()->DescriptorEquals("Ldalvik/system/VirtualThreadContext;"))
+      << frame->GetMethod()->PrettyMethod();
+  ObjPtr<mirror::Object> parked_states =
+      WellKnownClasses::dalvik_system_VirtualThreadContext_parkedStates->GetObject(v_context);
+  DCHECK(!parked_states.IsNull()) << frame->GetMethod()->PrettyMethod();
+  ObjPtr<mirror::ObjectArray<mirror::Object>> frames =
+      WellKnownClasses::dalvik_system_VirtualThreadParkedStates_frames->GetObject(parked_states)
+          ->AsObjectArray<mirror::Object>();
+  DCHECK(!frames.IsNull()) << frame->GetMethod()->PrettyMethod();
+
+  // TODO(b/346542404): Cache the tail index in the VirtualThreadContext.
+  int32_t frames_size = frames->GetLength();
+  int32_t frame_index = -1;
+  bool is_last_frame_or_empty = true;
+  ObjPtr<mirror::Object> src_frame;
+  for (int32_t i = frames_size - 1; i >= 0; i--) {
+    ObjPtr<mirror::Object> obj = frames->Get(i);
+    if (!obj.IsNull()) {
+      if (frame_index != -1) {
+        is_last_frame_or_empty = false;
+        break;
+      }
+      frame_index = i;
+      src_frame = obj;
+    }
+  }
+
+  DCHECK(!src_frame.IsNull()) << frame->GetMethod()->PrettyMethod();
+  ObjPtr<mirror::ByteArray> frame_jbytes =
+      WellKnownClasses::dalvik_system_VirtualThreadFrame_frame->GetObject(src_frame)->AsByteArray();
+  DCHECK(!frame_jbytes.IsNull()) << frame->GetMethod()->PrettyMethod();
+  ObjPtr<mirror::Object> refs =
+      WellKnownClasses::dalvik_system_VirtualThreadFrame_refs->GetObject(src_frame);
+
+  size_t non_vref_size = frame_jbytes->GetLength();
+  DCHECK_EQ(non_vref_size, ShadowFrame::ComputeSizeWithoutReferences(frame->NumberOfVRegs()))
+      << frame->GetMethod()->PrettyMethod();
+
+  constexpr size_t vreg_offset = ShadowFrame::VRegsOffset();
+  static_assert(ShadowFrame::DexPCOffset() < vreg_offset,
+                "memcpy dex_pc will fail because frame_jbytes is a partial shadow frame.");
+  uint32_t dex_pc;
+  memcpy(&dex_pc, frame_jbytes->GetData() + ShadowFrame::DexPCOffset(), sizeof(uint32_t));
+
+  // Verify the frame on a debug build.
+  if (kIsDebugBuild) {
+    frame->CheckConsistentVRegs();
+    std::unique_ptr<uint8_t[]> frame_bytes(new uint8_t[non_vref_size]);
+    frame_jbytes->MemcpyTo(0, reinterpret_cast<int8_t*>(frame_bytes.get()), 0, non_vref_size);
+    ShadowFrame* sf = reinterpret_cast<ShadowFrame*>(frame_bytes.get());
+
+    CHECK_EQ(frame->NumberOfVRegs(), sf->NumberOfVRegs()) << frame->GetMethod()->PrettyMethod();
+    CHECK_EQ(frame->GetMethod(), sf->GetMethod()) << frame->GetMethod()->PrettyMethod();
+    CHECK_EQ(dex_pc, sf->GetDexPC()) << frame->GetMethod()->PrettyMethod();
+  }
+  frame->SetDexPC(dex_pc);
+
+  // Copy non-reference vregs.
+  frame_jbytes->MemcpyTo(vreg_offset,
+                         reinterpret_cast<int8_t*>(frame),
+                         vreg_offset,
+                         frame->NumberOfVRegs() * sizeof(uint32_t));
+
+  if (!refs.IsNull()) {
+    // Copy reference vregs.
+    ObjPtr<mirror::ObjectArray<mirror::Object>> objs = refs->AsObjectArray<mirror::Object>();
+    DCHECK_EQ(static_cast<uint32_t>(objs->GetLength()), frame->NumberOfVRegs())
+        << frame->GetMethod()->PrettyMethod();
+    for (uint32_t i = 0; i < frame->NumberOfVRegs(); i++) {
+      ObjPtr<mirror::Object> obj = objs->Get(i);
+      DCHECK(!obj.IsNull() || frame->GetVRegReference(i) == nullptr)
+          << frame->GetMethod()->PrettyMethod() << " vreg " << i
+          << " nullness mismatch: " << (obj.IsNull());
+
+      if (!obj.IsNull()) {
+        frame->SetVRegReference(i, obj);
+      }
+    }
+  }
+
+  // Remove the reference to the heap object and let the GC collect it.
+  frames->Set(frame_index, nullptr);
+  // If it's the last frame, the current dex instruction is an invoke-* instruction that
+  // calls from libcore to park the virtual thread. Let's move to the next instruction to unpark.
+  if (is_last_frame_or_empty) {
+    DCHECK(!Runtime::Current()->IsActiveTransaction()) << frame->GetMethod()->PrettyMethod();
+    WellKnownClasses::dalvik_system_VirtualThreadContext_parkedStates->SetObject<false>(v_context,
+                                                                                        nullptr);
+    self->SetVirtualThreadFlags(VirtualThreadFlag::kUnparking, false);
+    frame->AdvanceDexPc();
+  }
+}
+
 template<bool is_range>
 NO_STACK_PROTECTOR
 bool DoCall(ArtMethod* called_method,
diff --git a/runtime/interpreter/interpreter_common.h b/runtime/interpreter/interpreter_common.h
index ce6c412ba8..e069ff4aca 100644
--- a/runtime/interpreter/interpreter_common.h
+++ b/runtime/interpreter/interpreter_common.h
@@ -17,19 +17,15 @@
 #ifndef ART_RUNTIME_INTERPRETER_INTERPRETER_COMMON_H_
 #define ART_RUNTIME_INTERPRETER_INTERPRETER_COMMON_H_
 
-#include "android-base/macros.h"
-#include "instrumentation.h"
-#include "interpreter.h"
-
+#include <android-base/logging.h>
+#include <android-base/stringprintf.h>
 #include <math.h>
 
 #include <atomic>
 #include <iostream>
 #include <sstream>
 
-#include <android-base/logging.h>
-#include <android-base/stringprintf.h>
-
+#include "android-base/macros.h"
 #include "art_field-inl.h"
 #include "art_method-inl.h"
 #include "base/locks.h"
@@ -44,6 +40,9 @@
 #include "dex/dex_instruction-inl.h"
 #include "entrypoints/entrypoint_utils-inl.h"
 #include "handle_scope-inl.h"
+#include "instrumentation.h"
+#include "interpreter.h"
+#include "interpreter/shadow_frame.h"
 #include "interpreter_cache-inl.h"
 #include "interpreter_switch_impl.h"
 #include "intrinsics_list.h"
@@ -58,8 +57,8 @@
 #include "mirror/string-inl.h"
 #include "obj_ptr.h"
 #include "stack.h"
-#include "thread.h"
 #include "thread-inl.h"
+#include "thread.h"
 #include "unstarted_runtime.h"
 #include "verifier/method_verifier.h"
 
@@ -122,6 +121,10 @@ EXPORT bool DoCall(ArtMethod* called_method,
                    bool string_init,
                    JValue* result);
 
+// Fill the ShadowFrame from stored values in the parked virtual thread.
+EXPORT void FillVirtualThreadFrame(Thread* self, ShadowFrame* this_frame)
+    REQUIRES_SHARED(Locks::mutator_lock_);
+
 // Called by the switch interpreter to know if we can stay in it.
 bool ShouldStayInSwitchInterpreter(ArtMethod* method)
     REQUIRES_SHARED(Locks::mutator_lock_);
@@ -290,22 +293,27 @@ LIBART_PROTECTED
 extern "C" uint32_t NterpGetInstanceFieldOffset(Thread* self,
                                                 ArtMethod* caller,
                                                 const uint16_t* dex_pc_ptr,
-                                                size_t resolve_field_type);
+                                                uint32_t* registers);
 
 static inline void GetFieldInfo(Thread* self,
-                                ArtMethod* caller,
+                                ShadowFrame& shadow_frame,
                                 const uint16_t* dex_pc_ptr,
                                 bool is_static,
                                 bool resolve_field_type,
                                 ArtField** field,
                                 bool* is_volatile,
-                                MemberOffset* offset) {
+                                MemberOffset* offset)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
   size_t tls_value = 0u;
   if (!self->GetInterpreterCache()->Get(self, dex_pc_ptr, &tls_value)) {
     if (is_static) {
-      tls_value = NterpGetStaticField(self, caller, dex_pc_ptr, resolve_field_type);
+      tls_value = NterpGetStaticField(
+          self, shadow_frame.GetMethod(), dex_pc_ptr, resolve_field_type);
     } else {
-      tls_value = NterpGetInstanceFieldOffset(self, caller, dex_pc_ptr, resolve_field_type);
+      tls_value = NterpGetInstanceFieldOffset(self,
+                                              shadow_frame.GetMethod(),
+                                              dex_pc_ptr,
+                                              shadow_frame.GetVRegAddr(0));
     }
 
     if (self->IsExceptionPending()) {
@@ -506,7 +514,6 @@ EXPORT bool MoveToExceptionHandler(Thread* self,
                                    ShadowFrame& shadow_frame,
                                    bool skip_listeners,
                                    bool skip_throw_listener) REQUIRES_SHARED(Locks::mutator_lock_);
-
 NO_RETURN EXPORT void UnexpectedOpcode(const Instruction* inst, const ShadowFrame& shadow_frame)
     COLD_ATTR
     REQUIRES_SHARED(Locks::mutator_lock_);
diff --git a/runtime/interpreter/interpreter_switch_impl-inl.h b/runtime/interpreter/interpreter_switch_impl-inl.h
index 7915a10094..24d19fb817 100644
--- a/runtime/interpreter/interpreter_switch_impl-inl.h
+++ b/runtime/interpreter/interpreter_switch_impl-inl.h
@@ -17,19 +17,19 @@
 #ifndef ART_RUNTIME_INTERPRETER_INTERPRETER_SWITCH_IMPL_INL_H_
 #define ART_RUNTIME_INTERPRETER_INTERPRETER_SWITCH_IMPL_INL_H_
 
-#include "interpreter_switch_impl.h"
-
 #include "base/globals.h"
 #include "base/memory_tool.h"
 #include "base/pointer_size.h"
 #include "base/quasi_atomic.h"
+#include "com_android_art_flags.h"
 #include "common_throws.h"
 #include "dex/dex_file_types.h"
 #include "dex/dex_instruction_list.h"
 #include "experimental_flags.h"
 #include "handle_scope.h"
-#include "interpreter_common.h"
 #include "interpreter/shadow_frame.h"
+#include "interpreter_common.h"
+#include "interpreter_switch_impl.h"
 #include "jit/jit-inl.h"
 #include "jvalue-inl.h"
 #include "mirror/string-alloc-inl.h"
@@ -73,7 +73,7 @@ ALWAYS_INLINE bool DoFieldGet(Thread* self,
   MemberOffset offset(0u);
   bool is_volatile;
   GetFieldInfo(self,
-               shadow_frame.GetMethod(),
+               shadow_frame,
                reinterpret_cast<const uint16_t*>(inst),
                is_static,
                /*resolve_field_type=*/ false,
@@ -157,7 +157,7 @@ ALWAYS_INLINE bool DoFieldGet(Thread* self,
 // Returns true on success, otherwise throws an exception and returns false.
 template<FindFieldType find_type, Primitive::Type field_type, bool transaction_active>
 ALWAYS_INLINE bool DoFieldPut(Thread* self,
-                              const ShadowFrame& shadow_frame,
+                              ShadowFrame& shadow_frame,
                               const Instruction* inst,
                               uint16_t inst_data,
                               const instrumentation::Instrumentation* instrumentation)
@@ -172,7 +172,7 @@ ALWAYS_INLINE bool DoFieldPut(Thread* self,
   MemberOffset offset(0u);
   bool is_volatile;
   GetFieldInfo(self,
-               shadow_frame.GetMethod(),
+               shadow_frame,
                reinterpret_cast<const uint16_t*>(inst),
                is_static,
                resolve_field_type,
@@ -2087,6 +2087,14 @@ void ExecuteSwitchImplCpp(SwitchImplContext* ctx) {
       return;  // Return statement or debugger forced exit.
     }
     if (self->IsExceptionPending()) {
+      // VirtualThreadParkingError is a special exception thrown to unwind the stack
+      // when parking a virtual thread. It shouldn't be caught or handled by the java code.
+      if (kIsVirtualThreadEnabled && self->IsVirtualThreadParking()) {
+        DCHECK(self->GetException()->GetClass()->DescriptorEquals(
+            "Ldalvik/system/VirtualThreadParkingError;"));
+        return;
+      }
+
       if (!InstructionHandler<transaction_active, Instruction::kInvalidFormat>(
               ctx, instrumentation, self, shadow_frame, dex_pc, inst, inst_data, next, exit).
               HandlePendingException()) {
diff --git a/runtime/interpreter/lock_count_data.cc b/runtime/interpreter/lock_count_data.cc
index ad53d703b3..8699be827a 100644
--- a/runtime/interpreter/lock_count_data.cc
+++ b/runtime/interpreter/lock_count_data.cc
@@ -75,6 +75,8 @@ void MonitorExitHelper(Thread* self, mirror::Object* obj) NO_THREAD_SAFETY_ANALY
   obj->MonitorExit(self);
 }
 
+bool LockCountData::IsEmpty() const { return monitors_ == nullptr || monitors_->empty(); }
+
 bool LockCountData::CheckAllMonitorsReleasedOrThrow(Thread* self) {
   DCHECK(self != nullptr);
   if (monitors_ != nullptr) {
diff --git a/runtime/interpreter/lock_count_data.h b/runtime/interpreter/lock_count_data.h
index 6e6b6c986d..978f0cc7db 100644
--- a/runtime/interpreter/lock_count_data.h
+++ b/runtime/interpreter/lock_count_data.h
@@ -54,6 +54,8 @@ class LockCountData {
   // check shows that everything is OK wrt/ lock counting, false otherwise.
   EXPORT bool CheckAllMonitorsReleasedOrThrow(Thread* self) REQUIRES_SHARED(Locks::mutator_lock_);
 
+  EXPORT bool IsEmpty() const REQUIRES_SHARED(Locks::mutator_lock_);
+
   template <typename T, typename... Args>
   void VisitMonitors(T visitor, Args&&... args) REQUIRES_SHARED(Locks::mutator_lock_) {
     if (monitors_ != nullptr) {
diff --git a/runtime/interpreter/mterp/arm64ng/object.S b/runtime/interpreter/mterp/arm64ng/object.S
index ade39d5e89..941974e33c 100644
--- a/runtime/interpreter/mterp/arm64ng/object.S
+++ b/runtime/interpreter/mterp/arm64ng/object.S
@@ -189,10 +189,20 @@
    .endif
 
 %def op_iget_slow_path(volatile_load, maybe_extend, wide, is_object):
+   ldr     x0, [sp]
+   ldr     w1, [x0, #ART_METHOD_ACCESS_FLAGS_OFFSET]
+   tbnz    w1, #ART_METHOD_IS_OBSOLETE_FLAG_BIT, .L${opcode}_slow_path_continue
+   // We don't do a read barrier for faster execution.
+   // See comment in `NterpGetLocalInstanceFieldInternal`.
+   ldr     w0, [x0, ART_METHOD_DECLARING_CLASS_OFFSET]
+   mov     x1, xPC
+   bl      NterpGetLocalInstanceField
+   tbz     w0, #31, .L${opcode}_resume
+.L${opcode}_slow_path_continue:
    mov     x0, xSELF
    ldr     x1, [sp]
    mov     x2, xPC
-   mov     x3, #0
+   mov     x3, xFP
    EXPORT_PC
    bl      nterp_get_instance_field_offset
    // Zero extension (nterp_get_instance_field_offset returns uint32_t) of the return value is
@@ -267,14 +277,24 @@
    GOTO_OPCODE ip
 
 %def op_iput_slow_path(volatile_store, wide, is_object):
-   mov     x0, xSELF
-   ldr     x1, [sp]
-   mov     x2, xPC
+   ldr     x0, [sp]
+   ldr     w1, [x0, #ART_METHOD_ACCESS_FLAGS_OFFSET]
+   tbnz    w1, #ART_METHOD_IS_OBSOLETE_FLAG_BIT, .L${opcode}_slow_path_continue
+   // We don't do a read barrier for faster execution.
+   // See comment in `NterpGetLocalInstanceFieldInternal`.
+   ldr     w0, [x0, ART_METHOD_DECLARING_CLASS_OFFSET]
+   mov     x1, xPC
    .if $is_object
-   mov     x3, x26
+   bl      NterpGetLocalInstanceFieldForIPutObject
    .else
-   mov     x3, #0
+   bl      NterpGetLocalInstanceField
    .endif
+   tbz     w0, #31, .L${opcode}_resume
+.L${opcode}_slow_path_continue:
+   mov     x0, xSELF
+   ldr     x1, [sp]
+   mov     x2, xPC
+   mov     x3, xFP
    EXPORT_PC
    bl      nterp_get_instance_field_offset
    // Zero extension (nterp_get_instance_field_offset returns uint32_t) of the return value is
@@ -361,6 +381,16 @@
    .endif
 
 %def op_sget_slow_path(volatile_load, maybe_extend, wide, is_object):
+   ldr     x0, [sp]
+   ldr     w1, [x0, #ART_METHOD_ACCESS_FLAGS_OFFSET]
+   tbnz    w1, #ART_METHOD_IS_OBSOLETE_FLAG_BIT, .L${opcode}_slow_path_continue
+   // We don't do a read barrier for faster execution.
+   // See comment in `NterpGetLocalStaticFieldInternal`.
+   ldr     w0, [x0, ART_METHOD_DECLARING_CLASS_OFFSET]
+   mov     x1, xPC
+   bl      NterpGetLocalStaticField
+   cbnz    x0, .L${opcode}_resume
+.L${opcode}_slow_path_continue:
    mov     x0, xSELF
    ldr     x1, [sp]
    mov     x2, xPC
@@ -443,6 +473,20 @@
    b       .L${opcode}_resume_after_read_barrier
 
 %def op_sput_slow_path(volatile_store, wide, is_object):
+   ldr     x0, [sp]
+   ldr     w1, [x0, #ART_METHOD_ACCESS_FLAGS_OFFSET]
+   tbnz    w1, #ART_METHOD_IS_OBSOLETE_FLAG_BIT, .L${opcode}_slow_path_continue
+   // We don't do a read barrier for faster execution.
+   // See comment in `NterpGetLocalStaticFieldInternal`.
+   ldr     w0, [x0, ART_METHOD_DECLARING_CLASS_OFFSET]
+   mov     x1, xPC
+   .if $is_object
+   bl      NterpGetLocalStaticFieldForSPutObject
+   .else
+   bl      NterpGetLocalStaticField
+   .endif
+   cbnz    x0, .L${opcode}_resume
+.L${opcode}_slow_path_continue:
    mov     x0, xSELF
    ldr     x1, [sp]
    mov     x2, xPC
diff --git a/runtime/interpreter/mterp/armng/object.S b/runtime/interpreter/mterp/armng/object.S
index 9188592fed..39ec3612dc 100644
--- a/runtime/interpreter/mterp/armng/object.S
+++ b/runtime/interpreter/mterp/armng/object.S
@@ -195,10 +195,22 @@
    .endif
 
 %def op_iget_slow_path(load, wide, is_object):
+   ldr     r0, [sp]
+   ldr     r1, [r0, #ART_METHOD_ACCESS_FLAGS_OFFSET]
+   tst     r1, #ART_METHOD_IS_OBSOLETE_FLAG
+   bne     .L${opcode}_slow_path_continue
+   // We don't do a read barrier for faster execution.
+   // See comment in `NterpGetLocalStaticFieldInternal`.
+   ldr     r0, [r0, ART_METHOD_DECLARING_CLASS_OFFSET]
+   mov     r1, rPC
+   bl      NterpGetLocalInstanceField
+   cmp     r0, #0
+   bge     .L${opcode}_resume
+.L${opcode}_slow_path_continue:
    mov     r0, rSELF
    ldr     r1, [sp]
    mov     r2, rPC
-   mov     r3, #0
+   mov     r3, rFP
    EXPORT_PC
    bl      nterp_get_instance_field_offset
    cmp     r0, #0
@@ -278,14 +290,26 @@
    GOTO_OPCODE ip
 
 %def op_iput_slow_path(store, wide, is_object):
-   mov     r0, rSELF
-   ldr     r1, [sp]
-   mov     r2, rPC
+   ldr     r0, [sp]
+   ldr     r1, [r0, #ART_METHOD_ACCESS_FLAGS_OFFSET]
+   tst     r1, #ART_METHOD_IS_OBSOLETE_FLAG
+   bne     .L${opcode}_slow_path_continue
+   // We don't do a read barrier for faster execution.
+   // See comment in `NterpGetLocalStaticFieldInternal`.
+   ldr     r0, [r0, ART_METHOD_DECLARING_CLASS_OFFSET]
+   mov     r1, rPC
    .if $is_object
-   mov     r3, r4
+   bl      NterpGetLocalInstanceFieldForIPutObject
    .else
-   mov     r3, #0
+   bl      NterpGetLocalInstanceField
    .endif
+   cmp     r0, #0
+   bge     .L${opcode}_resume
+.L${opcode}_slow_path_continue:
+   mov     r0, rSELF
+   ldr     r1, [sp]
+   mov     r2, rPC
+   mov     r3, rFP
    EXPORT_PC
    bl      nterp_get_instance_field_offset
    .if $is_object
@@ -384,6 +408,18 @@
    .endif
 
 %def op_sget_slow_path(load="ldr", wide="0", is_object="0"):
+   ldr     r0, [sp]
+   ldr     r1, [r0, #ART_METHOD_ACCESS_FLAGS_OFFSET]
+   tst     r1, #ART_METHOD_IS_OBSOLETE_FLAG
+   bne     .L${opcode}_slow_path_continue
+   // We don't do a read barrier for faster execution.
+   // See comment in `NterpGetLocalStaticFieldInternal`.
+   ldr     r0, [r0, ART_METHOD_DECLARING_CLASS_OFFSET]
+   mov     r1, rPC
+   bl      NterpGetLocalStaticField
+   cmp     r0, #0
+   bne     .L${opcode}_resume
+.L${opcode}_slow_path_continue:
    mov     r0, rSELF
    ldr     r1, [sp]
    mov     r2, rPC
@@ -473,6 +509,22 @@
    b       .L${opcode}_resume_after_read_barrier
 
 %def op_sput_slow_path(store, wide, is_object):
+   ldr     r0, [sp]
+   ldr     r1, [r0, #ART_METHOD_ACCESS_FLAGS_OFFSET]
+   tst     r1, #ART_METHOD_IS_OBSOLETE_FLAG
+   bne     .L${opcode}_slow_path_continue
+   // We don't do a read barrier for faster execution.
+   // See comment in `NterpGetLocalStaticFieldInternal`.
+   ldr     r0, [r0, ART_METHOD_DECLARING_CLASS_OFFSET]
+   mov     r1, rPC
+   .if $is_object
+   bl      NterpGetLocalStaticFieldForSPutObject
+   .else
+   bl      NterpGetLocalStaticField
+   .endif
+   cmp     r0, #0
+   bne     .L${opcode}_resume
+.L${opcode}_slow_path_continue:
    mov     r0, rSELF
    ldr     r1, [sp]
    mov     r2, rPC
diff --git a/runtime/interpreter/mterp/nterp.cc b/runtime/interpreter/mterp/nterp.cc
index 95cfe7fb7d..544fd3f5d9 100644
--- a/runtime/interpreter/mterp/nterp.cc
+++ b/runtime/interpreter/mterp/nterp.cc
@@ -318,10 +318,72 @@ static constexpr std::array<uint8_t, 256u> GenerateOpcodeInvokeTypes() {
   return opcode_invoke_types;
 }
 
+ALWAYS_INLINE FLATTEN
+inline bool IsInvokeClassMismatch(ObjPtr<mirror::Class> klass, InvokeType type, ArtMethod* caller)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  if (type == kInterface && UNLIKELY(!klass->IsInterface())) {
+    return true;
+  }
+
+  if (type == kVirtual && UNLIKELY(klass->IsInterface())) {
+    return true;
+  }
+
+  if (type == kDirect &&
+      UNLIKELY(klass->IsInterface()) &&
+      !caller->GetDexFile()->SupportsDefaultMethods()) {
+    return true;
+  }
+  return false;
+}
+
+ALWAYS_INLINE FLATTEN
+static ArtMethod* FindMethodFast(ArtMethod* caller,
+                                 uint16_t method_index,
+                                 InvokeType type)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  if (caller->IsObsolete()) {
+    return nullptr;
+  }
+
+  ObjPtr<mirror::Class> cls = caller->GetDeclaringClass();
+  const dex::MethodId& method_id = cls->GetDexFile().GetMethodId(method_index);
+
+  // Check within the caller's declaring class.
+  if (cls->GetDexTypeIndex() == method_id.class_idx_) {
+    PointerSize pointer_size = Runtime::Current()->GetClassLinker()->GetImagePointerSize();
+    ArtMethod* method = nullptr;
+    if (pointer_size == PointerSize::k64) {
+      method = cls->FindDeclaredClassMethod</* kOnlyLookAtIndex= */ false, PointerSize::k64>(
+            method_index);
+    } else {
+      method = cls->FindDeclaredClassMethod</* kOnlyLookAtIndex= */ false, PointerSize::k32>(
+            method_index);
+    }
+    if (caller->SkipAccessChecks()) {
+      return method;
+    }
+    if (method != nullptr &&
+        !IsInvokeClassMismatch(method->GetDeclaringClass<kWithoutReadBarrier>(), type, caller) &&
+        !method->CheckIncompatibleClassChange(type)) {
+      return method;
+    }
+    return nullptr;
+  }
+
+  if (caller->SkipAccessChecks()) {
+    return caller->GetDexCache()->GetResolvedMethod(method_index);
+  }
+
+  return nullptr;
+}
+
 static constexpr std::array<uint8_t, 256u> kOpcodeInvokeTypes = GenerateOpcodeInvokeTypes();
 
 LIBART_PROTECTED FLATTEN
-extern "C" size_t NterpGetMethod(Thread* self, ArtMethod* caller, const uint16_t* dex_pc_ptr)
+extern "C" size_t NterpGetMethod(Thread* self,
+                                 ArtMethod* caller,
+                                 const uint16_t* dex_pc_ptr)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   UpdateHotness(caller);
   const Instruction* inst = Instruction::At(dex_pc_ptr);
@@ -336,13 +398,16 @@ extern "C" size_t NterpGetMethod(Thread* self, ArtMethod* caller, const uint16_t
   uint16_t method_index =
       (opcode >= Instruction::INVOKE_VIRTUAL_RANGE) ? inst->VRegB_3rc() : inst->VRegB_35c();
 
-  ClassLinker* const class_linker = Runtime::Current()->GetClassLinker();
-  ArtMethod* resolved_method = caller->SkipAccessChecks()
-      ? class_linker->ResolveMethodId(method_index, caller)
-      : class_linker->ResolveMethodWithChecks(method_index, caller, invoke_type);
+  ArtMethod* resolved_method = FindMethodFast(caller, method_index, invoke_type);
   if (resolved_method == nullptr) {
-    DCHECK(self->IsExceptionPending());
-    return 0;
+    ClassLinker* const class_linker = Runtime::Current()->GetClassLinker();
+    resolved_method = caller->SkipAccessChecks()
+        ? class_linker->ResolveMethodId(method_index, caller)
+        : class_linker->ResolveMethodWithChecks(method_index, caller, invoke_type);
+    if (resolved_method == nullptr) {
+      DCHECK(self->IsExceptionPending());
+      return 0;
+    }
   }
 
   if (invoke_type == kSuper) {
@@ -391,8 +456,33 @@ extern "C" size_t NterpGetMethod(Thread* self, ArtMethod* caller, const uint16_t
   }
 }
 
+template <bool kIsPut>
+ALWAYS_INLINE FLATTEN
+static bool CanAccessFastInternal(ArtField* field, ArtMethod* caller)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  if (caller->SkipAccessChecks() || field == nullptr) {
+    return true;
+  }
+  return field->IsPublic() &&
+      field->GetDeclaringClass<kWithoutReadBarrier>()->IsPublic() &&
+      !(kIsPut && field->IsFinal());
+}
+
+template <bool kIsStatic>
 ALWAYS_INLINE FLATTEN
-static ArtField* FindFieldFast(ArtMethod* caller, uint16_t field_index)
+static bool CanAccessFast(ArtField* field, ArtMethod* caller, const Instruction* inst)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  bool is_put = kIsStatic ? IsInstructionSPut(inst->Opcode()) : IsInstructionIPut(inst->Opcode());
+  return is_put ? CanAccessFastInternal</*kIsPut=*/true>(field, caller)
+                : CanAccessFastInternal</*kIsPut=*/false>(field, caller);
+}
+
+template <bool kStatic>
+ALWAYS_INLINE FLATTEN
+static ArtField* FindFieldFast(ArtMethod* caller,
+                               uint16_t field_index,
+                               uint32_t* registers,
+                               const Instruction* inst)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   if (caller->IsObsolete()) {
     return nullptr;
@@ -405,6 +495,25 @@ static ArtField* FindFieldFast(ArtMethod* caller, uint16_t field_index)
     return cls->FindDeclaredField(field_index);
   }
 
+  if (!kStatic) {
+    mirror::Object* obj = reinterpret_cast32<mirror::Object*>(registers[inst->VRegB_22c()]);
+    if (obj != nullptr) {
+      mirror::Class* obj_cls = obj->GetClass();
+      if (obj_cls->GetDexTypeIndex() == field_id.class_idx_ &&
+          obj_cls->GetDexCache() == cls->GetDexCache()) {
+        ArtField* resolved_field = obj_cls->FindDeclaredField(field_index);
+        if (CanAccessFast<kStatic>(resolved_field, caller, inst)) {
+          return resolved_field;
+        }
+      }
+    }
+  }
+
+  ArtField* field = caller->GetDexCache()->GetResolvedField(field_index);
+  if (CanAccessFast<kStatic>(field, caller, inst)) {
+    return field;
+  }
+
   return nullptr;
 }
 
@@ -435,7 +544,7 @@ extern "C" size_t NterpGetStaticField(Thread* self,
   uint16_t field_index = inst->VRegB_21c();
   Instruction::Code opcode = inst->Opcode();
 
-  ArtField* resolved_field = FindFieldFast(caller, field_index);
+  ArtField* resolved_field = FindFieldFast</*kStatic=*/true>(caller, field_index, nullptr, inst);
   if (resolved_field == nullptr || !resolved_field->IsStatic()) {
     resolved_field = FindFieldSlow(
         self, caller, field_index, /*is_static=*/ true, IsInstructionSPut(opcode));
@@ -488,17 +597,92 @@ extern "C" size_t NterpGetStaticField(Thread* self,
   return reinterpret_cast<size_t>(resolved_field);
 }
 
+// For faster execution, `cls` can be a from-space reference which is OK, as
+// we're only using native fields from that object, and checking for state
+// invariants that don't roll back (ie that the class is initialized).
+ALWAYS_INLINE FLATTEN
+static size_t NterpGetLocalStaticFieldInternal(mirror::Class* cls, const uint16_t* dex_pc_ptr)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  // We're checking if we're accessing a field of the currently executing class.
+  // We only need to check that the class is initialized, and don't need
+  // a synchrnonization barrier for this.
+  if (cls->GetStatus<kVerifyNone, /*kWithSynchronizationBarrier=*/ false>()
+          < ClassStatus::kInitialized) {
+    return 0u;
+  }
+  ArtField* resolved_field = cls->FindDeclaredField</*kOnlyLookAtIndex=*/true>(
+      Instruction::At(dex_pc_ptr)->VRegB_21c());
+  if (resolved_field != nullptr && resolved_field->IsStatic() && !resolved_field->IsVolatile()) {
+    return reinterpret_cast<size_t>(resolved_field);
+  }
+  return 0u;
+}
+
+// `cls` can be a from-space, see comment in `NterpGetLocalStaticFieldInternal`.
+FLATTEN
+extern "C" size_t NterpGetLocalStaticField(mirror::Class* cls, const uint16_t* dex_pc_ptr)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  ScopedAssertNoThreadSuspension sants("In nterp");
+  return NterpGetLocalStaticFieldInternal(cls, dex_pc_ptr);
+}
+
+// `cls` can be a from-space, see comment in `NterpGetLocalStaticFieldInternal`.
+FLATTEN
+extern "C" size_t NterpGetLocalStaticFieldForSPutObject(mirror::Class* cls,
+                                                        const uint16_t* dex_pc_ptr)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  ScopedAssertNoThreadSuspension sants("In nterp");
+  // For object store in methods that may have type check failures, we need to
+  // resolve the type of the field. In such a case, go to slow path.
+  if (cls->HasTypeChecksFailure()) {
+    return 0u;
+  }
+  return NterpGetLocalStaticFieldInternal(cls, dex_pc_ptr);
+}
+
+// For faster execution, `cls` can be a from-space reference which is OK, as
+// we're only using native fields from that object.
+ALWAYS_INLINE FLATTEN
+static size_t NterpGetLocalInstanceFieldInternal(mirror::Class* cls, const uint16_t* dex_pc_ptr)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  ScopedAssertNoThreadSuspension sants("In nterp");
+  const Instruction* inst = Instruction::At(dex_pc_ptr);
+  uint16_t field_index = inst->VRegC_22c();
+  ArtField* resolved_field = cls->FindDeclaredField</*kOnlyLookAtIndex=*/true>(field_index);
+  if (resolved_field != nullptr && !resolved_field->IsStatic() && !resolved_field->IsVolatile()) {
+    return resolved_field->GetOffset().Uint32Value();
+  }
+
+  return -1;
+}
+
+FLATTEN
+extern "C" size_t NterpGetLocalInstanceField(mirror::Class* cls, const uint16_t* dex_pc_ptr)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  return NterpGetLocalInstanceFieldInternal(cls, dex_pc_ptr);
+}
+
+FLATTEN
+extern "C" size_t NterpGetLocalInstanceFieldForIPutObject(mirror::Class* cls,
+                                                          const uint16_t* dex_pc_ptr)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  if (cls->HasTypeChecksFailure()) {
+    return -1;
+  }
+  return NterpGetLocalInstanceFieldInternal(cls, dex_pc_ptr);
+}
+
 LIBART_PROTECTED
 extern "C" uint32_t NterpGetInstanceFieldOffset(Thread* self,
                                                 ArtMethod* caller,
                                                 const uint16_t* dex_pc_ptr,
-                                                size_t resolve_field_type)  // Resolve if not zero
+                                                uint32_t* registers)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   const Instruction* inst = Instruction::At(dex_pc_ptr);
   uint16_t field_index = inst->VRegC_22c();
   Instruction::Code opcode = inst->Opcode();
 
-  ArtField* resolved_field = FindFieldFast(caller, field_index);
+  ArtField* resolved_field = FindFieldFast</*kStatic=*/false>(caller, field_index, registers, inst);
   if (resolved_field == nullptr || resolved_field->IsStatic()) {
     resolved_field = FindFieldSlow(
         self, caller, field_index, /*is_static=*/ false, IsInstructionIPut(opcode));
@@ -519,7 +703,7 @@ extern "C" uint32_t NterpGetInstanceFieldOffset(Thread* self,
       caller->GetDeclaringClass()->HasTypeChecksFailure() &&
       resolved_field->ResolveType() == nullptr) {
     DCHECK(self->IsExceptionPending());
-    if (resolve_field_type != 0u) {
+    if (registers[inst->VRegA_22c()] != 0u) {
       return 0;
     }
     self->ClearException();
diff --git a/runtime/interpreter/mterp/riscv64/object.S b/runtime/interpreter/mterp/riscv64/object.S
index 43bb31f1e9..920fa63bba 100644
--- a/runtime/interpreter/mterp/riscv64/object.S
+++ b/runtime/interpreter/mterp/riscv64/object.S
@@ -289,7 +289,7 @@
    mv a0, xSELF
    ld a1, (sp)  // a1 := caller ArtMethod*
    mv a2, xPC
-   mv a3, zero
+   mv a3, xFP
    EXPORT_PC
    call nterp_get_instance_field_offset  // result a0 := field_offset
 
@@ -366,7 +366,7 @@
    mv a0, xSELF
    ld a1, (sp)  // a1 := caller ArtMethod*
    mv a2, xPC
-   mv a3, zero
+   mv a3, xFP
    EXPORT_PC
    call nterp_get_instance_field_offset  // result a0 := field_offset
 
@@ -451,7 +451,7 @@
    mv a0, xSELF
    ld a1, (sp)  // a1 := caller ArtMethod*
    mv a2, xPC
-   mv a3, zero
+   mv a3, xFP
    EXPORT_PC
    call nterp_get_instance_field_offset  // result a0 := field_offset
 
@@ -534,7 +534,7 @@
    mv a0, xSELF
    ld a1, (sp)  // a1 := caller ArtMethod*
    mv a2, xPC
-   mv a3, $value
+   mv a3, xFP
    EXPORT_PC
    call nterp_get_instance_field_offset  // result a0 := field_offset
 
diff --git a/runtime/interpreter/mterp/x86_64ng/main.S b/runtime/interpreter/mterp/x86_64ng/main.S
index 80753c4fc2..77d71e568b 100644
--- a/runtime/interpreter/mterp/x86_64ng/main.S
+++ b/runtime/interpreter/mterp/x86_64ng/main.S
@@ -1453,6 +1453,8 @@ END_FUNCTION \name
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalStaticField", success_label="1b", fail_label="5f", is_static="1")
+5:
    EXPORT_PC
    movq rSELF:THREAD_SELF_OFFSET, %rdi
    movq 0(%rsp), %rsi
@@ -1486,6 +1488,8 @@ END_FUNCTION \name
    \store    \rINST_reg, (%rax,%rdx,1)
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalStaticField", success_label="1b", fail_label="7f", is_static="1")
+7:
    EXPORT_PC
    movq rSELF:THREAD_SELF_OFFSET, %rdi
    movq 0(%rsp), %rsi
@@ -1542,11 +1546,13 @@ END_FUNCTION \name
    OP_IPUT_INTERNAL \rINST_reg, \store, \wide
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalInstanceField", success_label="1b", fail_label="3f", is_static="0")
+3:
    EXPORT_PC
    movq rSELF:THREAD_SELF_OFFSET, %rdi
    movq 0(%rsp), %rsi
    movq rPC, %rdx
-   movq $$0, %rcx
+   movq rFP, %rcx
    call nterp_get_instance_field_offset
    testl %eax, %eax
    jns 1b
@@ -1576,11 +1582,13 @@ END_FUNCTION \name
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalInstanceField", success_label="1b", fail_label="3f", is_static="0")
+3:
    EXPORT_PC
    movq rSELF:THREAD_SELF_OFFSET, %rdi
    movq 0(%rsp), %rsi
    movq rPC, %rdx
-   movq $$0, %rcx
+   movq rFP, %rcx
    call nterp_get_instance_field_offset
    testl %eax, %eax
    jns 1b
@@ -1802,6 +1810,23 @@ OAT_ENTRY ExecuteNterpImpl
    jne ${miss_label}
    movq __SIZEOF_POINTER__+THREAD_INTERPRETER_CACHE_OFFSET(%rax, %rdx, 1), ${dest_reg}
 
+%def fetch_field_from_fast_path(get_local, success_label, fail_label, is_static):
+   movq 0(%rsp), %rdi
+   testl $$ART_METHOD_IS_OBSOLETE_FLAG, ART_METHOD_ACCESS_FLAGS_OFFSET(%rdi)
+   jne ${fail_label}
+   // We don't do a read barrier for faster execution.
+   // See comment in `NterpGetLocal{Static,Instance}FieldInternal`.
+   movl ART_METHOD_DECLARING_CLASS_OFFSET(%rdi), %edi
+   movq rPC, %rsi
+   call ${get_local}
+   .if ${is_static}
+   testq %rax, %rax
+   jne ${success_label}
+   .else
+   testl %eax, %eax
+   jns ${success_label}
+   .endif
+
 %def footer():
 /*
  * ===========================================================================
@@ -1919,13 +1944,13 @@ NterpNewArray:
    jmp 1b
 
 NterpPutObjectInstanceField:
+   // Fast-path which gets the field from thread-local cache.
+%  fetch_from_thread_cache("%rax", miss_label="2f")
+1:
    movl    rINST, %ebp                     # rbp <- BA
    andl    $$0xf, %ebp                     # rbp <- A
    GET_VREG %ecx, %rbp                     # ecx <- v[A]
    sarl    $$4, rINST
-   // Fast-path which gets the field from thread-local cache.
-%  fetch_from_thread_cache("%rax", miss_label="2f")
-1:
    GET_VREG rINST, rINSTq                  # vB (object we're operating on)
    testl   rINST, rINST                    # is object null?
    je      common_errNullObject
@@ -1939,16 +1964,20 @@ NterpPutObjectInstanceField:
 4:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalInstanceFieldForIPutObject", success_label="1b", fail_label="3f", is_static="0")
+3:
    EXPORT_PC
    movq rSELF:THREAD_SELF_OFFSET, %rdi
    movq 0(%rsp), %rsi
    movq rPC, %rdx
-   // %rcx is already set.
+   movq rFP, %rcx
    call nterp_get_instance_field_offset
-   // Reload the value as it may have moved.
-   GET_VREG %ecx, %rbp                     # ecx <- v[A]
    testl %eax, %eax
    jns 1b
+   movl    rINST, %ebp                     # rbp <- BA
+   andl    $$0xf, %ebp                     # rbp <- A
+   GET_VREG %ecx, %rbp                     # ecx <- v[A]
+   sarl    $$4, rINST
    GET_VREG rINST, rINSTq                  # vB (object we're operating on)
    testl   rINST, rINST                    # is object null?
    je      common_errNullObject
@@ -1982,11 +2011,13 @@ NterpGetObjectInstanceField:
    SET_VREG_OBJECT %eax, rINSTq            # fp[A] <- value
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalInstanceField", success_label="1b", fail_label="5f", is_static="0")
+5:
    EXPORT_PC
    movq rSELF:THREAD_SELF_OFFSET, %rdi
    movq 0(%rsp), %rsi
    movq rPC, %rdx
-   movq $$0, %rcx
+   movq rFP, %rcx
    call nterp_get_instance_field_offset
    testl %eax, %eax
    jns 1b
@@ -2020,6 +2051,8 @@ NterpPutObjectStaticField:
 4:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalStaticFieldForSPutObject", success_label="1b", fail_label="9f", is_static="1")
+9:
    EXPORT_PC
    movq rSELF:THREAD_SELF_OFFSET, %rdi
    movq 0(%rsp), %rsi
@@ -2070,6 +2103,8 @@ NterpGetObjectStaticField:
    SET_VREG_OBJECT %eax, rINSTq            # fp[A] <- value
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalStaticField", success_label="1b", fail_label="7f", is_static="1")
+7:
    EXPORT_PC
    movq rSELF:THREAD_SELF_OFFSET, %rdi
    movq 0(%rsp), %rsi
diff --git a/runtime/interpreter/mterp/x86ng/main.S b/runtime/interpreter/mterp/x86ng/main.S
index d2f4271f99..ea5e3c5733 100644
--- a/runtime/interpreter/mterp/x86ng/main.S
+++ b/runtime/interpreter/mterp/x86ng/main.S
@@ -1483,6 +1483,8 @@ END_FUNCTION \name
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalStaticField", success_label="1b", fail_label="9f", is_static="1")
+9:
    EXPORT_PC
    movl rSELF:THREAD_SELF_OFFSET, ARG0
    movl 0(%esp), ARG1
@@ -1535,6 +1537,8 @@ END_FUNCTION \name
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalStaticField", success_label="1b", fail_label="7f", is_static="1")
+7:
    EXPORT_PC
    movl rSELF:THREAD_SELF_OFFSET, ARG0
    movl 0(%esp), ARG1
@@ -1594,11 +1598,13 @@ END_FUNCTION \name
    OP_IPUT_INTERNAL \rINST_reg, \store, \wide, volatile=0
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalInstanceField", success_label="1b", fail_label="3f", is_static="0")
+3:
    EXPORT_PC
    movl rSELF:THREAD_SELF_OFFSET, ARG0
    movl 0(%esp), ARG1
    movl rPC, ARG2
-   movl $$0, ARG3
+   movl rFP, ARG3
    call nterp_get_instance_field_offset
    testl %eax, %eax
    jns 1b
@@ -1631,11 +1637,13 @@ END_FUNCTION \name
    .endif
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalInstanceField", success_label="1b", fail_label="3f", is_static="0")
+3:
    EXPORT_PC
    movl rSELF:THREAD_SELF_OFFSET, ARG0
    movl 0(%esp), ARG1
    movl rPC, ARG2
-   movl $$0, ARG3
+   movl rFP, ARG3
    call nterp_get_instance_field_offset
    testl %eax, %eax
    jns 1b
@@ -1873,6 +1881,27 @@ OAT_ENTRY ExecuteNterpImpl
    jne  ${miss_label}
    movl __SIZEOF_POINTER__+THREAD_INTERPRETER_CACHE_OFFSET(%eax, %ecx, 1), ${dest_reg}
 
+%def fetch_field_from_fast_path(get_local, success_label, fail_label, is_static):
+   movl 0(%esp), %eax
+   testl $$ART_METHOD_IS_OBSOLETE_FLAG, ART_METHOD_ACCESS_FLAGS_OFFSET(%eax)
+   jne ${fail_label}
+   // We don't do a read barrier for faster execution.
+   // See comment in `NterpGetLocalStaticFieldInternal`.
+   movl ART_METHOD_DECLARING_CLASS_OFFSET(%eax), %eax
+   subl MACRO_LITERAL(8), %esp
+   pushl rPC
+   pushl %eax
+   call ${get_local}
+   addl MACRO_LITERAL(16), %esp
+   RESTORE_IBASE
+   FETCH_INST_CLEAR_OPCODE
+   cmpl MACRO_LITERAL(0), %eax
+   .if ${is_static}
+   jne ${success_label}
+   .else
+   jge ${success_label}
+   .endif
+
 %def footer():
 /*
  * ===========================================================================
@@ -2013,14 +2042,13 @@ NterpPutObjectInstanceField:
 4:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalInstanceFieldForIPutObject", success_label="1b", fail_label="6f", is_static="0")
+6:
    EXPORT_PC
-   // Fetch the value, needed by nterp_get_instance_field_offset.
-   movl    rINST, %ecx                     # ecx <- BA
-   andl    $$0xf, %ecx                     # ecx <- A
-   GET_VREG ARG3, %ecx                     # ecx <- v[A]
    movl rSELF:THREAD_SELF_OFFSET, ARG0
    movl 0(%esp), ARG1
    movl rPC, ARG2
+   movl rFP, ARG3
    call nterp_get_instance_field_offset
    testl %eax, %eax
    jns 1b
@@ -2062,11 +2090,13 @@ NterpGetObjectInstanceField:
    SET_VREG_OBJECT %eax, rINST             # fp[A] <- value
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalInstanceField", success_label="1b", fail_label="5f", is_static="0")
+5:
    EXPORT_PC
    movl rSELF:THREAD_SELF_OFFSET, ARG0
    movl 0(%esp), ARG1
    movl rPC, ARG2
-   movl $$0, ARG3
+   movl rFP, ARG3
    call nterp_get_instance_field_offset
    testl %eax, %eax
    jns 1b
@@ -2081,10 +2111,10 @@ NterpGetObjectInstanceField:
    jmp 4b
 
 NterpPutObjectStaticField:
-   GET_VREG rINST, rINST
    // Fast-path which gets the field from thread-local cache.
 %  fetch_from_thread_cache("%eax", miss_label="2f")
 1:
+   GET_VREG rINST, rINST
    movl ART_FIELD_OFFSET_OFFSET(%eax), %ecx
    movl ART_FIELD_DECLARING_CLASS_OFFSET(%eax), %eax
    cmpl $$0, rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
@@ -2100,14 +2130,14 @@ NterpPutObjectStaticField:
 4:
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalStaticFieldForSPutObject", success_label="1b", fail_label="9f", is_static="1")
+9:
    EXPORT_PC
    movl rSELF:THREAD_SELF_OFFSET, ARG0
    movl 0(%esp), ARG1
    movl rPC, ARG2
-   movl rINST, ARG3
+   GET_VREG ARG3, rINST
    call nterp_get_static_field
-   // Reload the value as it may have moved.
-   GET_VREG rINST, rINST
    testl MACRO_LITERAL(1), %eax
    je 1b
    CLEAR_VOLATILE_MARKER %eax
@@ -2116,6 +2146,7 @@ NterpPutObjectStaticField:
    cmpl $$0, rSELF:THREAD_READ_BARRIER_MARK_REG00_OFFSET
    jne 7f
 6:
+   GET_VREG rINST, rINST
    POISON_HEAP_REF ebx  // `rINST` is `%ebx` but we need to pass `ebx`.
    movl rINST, (%eax, %ecx, 1)
    testl rINST, rINST
@@ -2150,6 +2181,8 @@ NterpGetObjectStaticField:
    SET_VREG_OBJECT %eax, rINST             # fp[A] <- value
    ADVANCE_PC_FETCH_AND_GOTO_NEXT 2
 2:
+%  fetch_field_from_fast_path("NterpGetLocalStaticField", success_label="1b", fail_label="9f", is_static="1")
+9:
    EXPORT_PC
    movl rSELF:THREAD_SELF_OFFSET, ARG0
    movl 0(%esp), ARG1
diff --git a/runtime/interpreter/shadow_frame.cc b/runtime/interpreter/shadow_frame.cc
index 5ed4224900..79a35086a1 100644
--- a/runtime/interpreter/shadow_frame.cc
+++ b/runtime/interpreter/shadow_frame.cc
@@ -16,7 +16,9 @@
 
 #include "shadow_frame.h"
 
+#include "android-base/macros.h"
 #include "art_method-inl.h"
+#include "shadow_frame-inl.h"
 
 namespace art HIDDEN {
 
@@ -43,4 +45,11 @@ mirror::Object* ShadowFrame::GetThisObject(uint16_t num_ins) const {
   }
 }
 
+void ShadowFrame::AdvanceDexPc() {
+  CodeItemDataAccessor accessor(this->GetMethod()->DexInstructionData());
+  const Instruction* instr = &accessor.InstructionAt(dex_pc_);
+  uint32_t new_dex_pc = dex_pc_ + instr->SizeInCodeUnits();
+  SetDexPC(new_dex_pc);
+}
+
 }  // namespace art
diff --git a/runtime/interpreter/shadow_frame.h b/runtime/interpreter/shadow_frame.h
index 0c542f0173..bf298ff91e 100644
--- a/runtime/interpreter/shadow_frame.h
+++ b/runtime/interpreter/shadow_frame.h
@@ -17,12 +17,15 @@
 #ifndef ART_RUNTIME_INTERPRETER_SHADOW_FRAME_H_
 #define ART_RUNTIME_INTERPRETER_SHADOW_FRAME_H_
 
+#include <cstddef>
 #include <cstdint>
 #include <cstring>
 #include <string>
 
 #include "base/locks.h"
 #include "base/macros.h"
+#include "handle.h"
+#include "handle_scope.h"
 #include "lock_count_data.h"
 #include "read_barrier.h"
 #include "stack_reference.h"
@@ -78,6 +81,11 @@ class ShadowFrame {
            (sizeof(StackReference<mirror::Object>) * num_vregs);
   }
 
+  // Compute size of interpreter ShadowFrame in bytes without a reference array.
+  static size_t ComputeSizeWithoutReferences(uint32_t num_vregs) {
+    return sizeof(ShadowFrame) + (sizeof(uint32_t) * num_vregs);
+  }
+
   // Create ShadowFrame in heap for deoptimization.
   static ShadowFrame* CreateDeoptimizedFrame(uint32_t num_vregs,
                                              ArtMethod* method,
@@ -104,6 +112,8 @@ class ShadowFrame {
 
   ~ShadowFrame() {}
 
+  void AdvanceDexPc() REQUIRES_SHARED(Locks::mutator_lock_);
+
   uint32_t NumberOfVRegs() const {
     return number_of_vregs_;
   }
diff --git a/runtime/javaheapprof/javaheapsampler.cc b/runtime/javaheapprof/javaheapsampler.cc
index 959934bedf..2969bb320e 100644
--- a/runtime/javaheapprof/javaheapsampler.cc
+++ b/runtime/javaheapprof/javaheapsampler.cc
@@ -25,121 +25,60 @@
 
 namespace art HIDDEN {
 
-size_t HeapSampler::NextGeoDistRandSample() {
-  // Make sure that rng_ and geo_dist are thread safe by acquiring a lock to access.
-  art::MutexLock mu(art::Thread::Current(), geo_dist_rng_lock_);
-  size_t nsample = geo_dist_(rng_);
-  if (nsample == 0) {
-    // Geometric distribution results in +ve values but could have zero.
-    // In the zero case, return 1.
-    nsample = 1;
+size_t HeapSampler::NextRandomTlabSize(size_t target) {
+  // Scale the target tlab size by sampling interval, so the cost of
+  // profiling can be reduced by increasing the sampling interval.
+  // The factor 1/8 is somewhat arbitrarily chosen to avoid interfering with
+  // Perfetto's subsequent sampling of the allocations when we call
+  // AHeapProfile_reportAllocation.
+  target = std::min(target, GetSamplingInterval() / 8);
+  if (target == 0) {
+    target = 1;
   }
-  return nsample;
-}
 
-size_t HeapSampler::PickAndAdjustNextSample(size_t sample_adjust_bytes) {
-  size_t bytes_until_sample;
-  if (GetSamplingInterval() == 1) {
-    bytes_until_sample = 1;
-    return bytes_until_sample;
-  }
-  bytes_until_sample = NextGeoDistRandSample();
-  VLOG(heap) << "JHP:PickAndAdjustNextSample, sample_adjust_bytes: "
-             << sample_adjust_bytes
-             << " bytes_until_sample: " << bytes_until_sample;
-  // Adjust the sample bytes
-  if (sample_adjust_bytes > 0 && bytes_until_sample > sample_adjust_bytes) {
-    bytes_until_sample -= sample_adjust_bytes;
-    VLOG(heap) << "JHP:PickAndAdjustNextSample, final bytes_until_sample: "
-               << bytes_until_sample;
-  }
-  return bytes_until_sample;
+  // Draw randomly from exponential distribution with the goal of having
+  // equal probability of sampling any given byte. This is the same logic
+  // used in AHeapProfile_reportAllocation for sampling.
+  double rate = 1.0 / static_cast<double>(target);
+  std::exponential_distribution<double> dist(rate);
+  art::MutexLock mu(art::Thread::Current(), rng_lock_);
+  int64_t next = static_cast<int64_t>(dist(rng_));
+  return next + 1;
 }
 
-// Report to Perfetto an allocation sample.
-// Samples can only be reported after the allocation is done.
-// Also bytes_until_sample can only be updated after the allocation and reporting is done.
-// Thus next bytes_until_sample is previously calculated (before allocation) to be able to
-// get the next tlab_size, but only saved/updated here.
-void HeapSampler::ReportSample(art::mirror::Object* obj, size_t allocation_size) {
-  VLOG(heap) << "JHP:***Report Perfetto Allocation: alloc_size: " << allocation_size;
-  uint64_t perf_alloc_id = reinterpret_cast<uint64_t>(obj);
-  VLOG(heap) << "JHP:***Report Perfetto Allocation: obj: " << perf_alloc_id;
+void HeapSampler::ReportAllocation([[maybe_unused]] art::mirror::Object* obj,
+                                   [[maybe_unused]] size_t allocation_size) {
 #ifdef ART_TARGET_ANDROID
-  AHeapProfile_reportSample(perfetto_heap_id_, perf_alloc_id, allocation_size);
+  uint64_t perf_alloc_id = reinterpret_cast<uint64_t>(obj);
+  AHeapProfile_reportAllocation(perfetto_heap_id_, perf_alloc_id, allocation_size);
 #endif
 }
 
-// Check whether we should take a sample or not at this allocation and calculate the sample
-// offset to use in the expand Tlab calculation. Thus the offset from current pos to the next
-// sample.
-// tlab_used = pos - start
-size_t HeapSampler::GetSampleOffset(size_t alloc_size,
-                                    size_t tlab_used,
-                                    bool* take_sample,
-                                    size_t* temp_bytes_until_sample) {
-  size_t exhausted_size = alloc_size + tlab_used;
-  VLOG(heap) << "JHP:GetSampleOffset: exhausted_size = " << exhausted_size;
-  // Note bytes_until_sample is used as an offset from the start point
-  size_t bytes_until_sample = *GetBytesUntilSample();
-  ssize_t diff = bytes_until_sample - exhausted_size;
-  VLOG(heap) << "JHP:GetSampleOffset: diff = " << diff << " bytes_until_sample = "
-             << bytes_until_sample;
-  if (diff <= 0) {
-    *take_sample = true;
-    // Compute a new bytes_until_sample
-    size_t sample_adj_bytes = -diff;
-    size_t next_bytes_until_sample = PickAndAdjustNextSample(sample_adj_bytes);
-    VLOG(heap) << "JHP:GetSampleOffset: Take sample, next_bytes_until_sample = "
-               << next_bytes_until_sample;
-    next_bytes_until_sample += tlab_used;
-    VLOG(heap) << "JHP:GetSampleOffset:Next sample offset = "
-               << (next_bytes_until_sample - tlab_used);
-    // This function is called before the actual allocation happens so we cannot update
-    // the bytes_until_sample till after the allocation happens, save it to temp which
-    // will be saved after the allocation by the calling function.
-    *temp_bytes_until_sample = next_bytes_until_sample;
-    return (next_bytes_until_sample - tlab_used);
-    // original bytes_until_sample, not offseted
-  } else {
-    *take_sample = false;
-    // The following 2 lines are used in the NonTlab case but have no effect on the
-    // Tlab case, because we will only use the temp_bytes_until_sample if the
-    // take_sample was true (after returning from this function in Tlab case in the
-    // SetBytesUntilSample).
-    size_t next_bytes_until_sample = bytes_until_sample - alloc_size;
-    *temp_bytes_until_sample = next_bytes_until_sample;
-    VLOG(heap) << "JHP:GetSampleOffset: No sample, next_bytes_until_sample= "
-               << next_bytes_until_sample << " alloc= " << alloc_size;
-    return diff;
+void HeapSampler::ReportTlabAllocation(art::mirror::Object* obj,
+                                       size_t allocation_size,
+                                       size_t pre_tlab_size,
+                                       size_t post_tlab_size) {
+  if (pre_tlab_size > tlab_unsampled_bytes_) {
+    // In theory pre_tlab_size shouldn't exceed tlab_unsampled_bytes_. In
+    // practice our tlab_unsampled_bytes_ could be out of date if profiling
+    // was disabled/enabled since we last set it. Ignore the previously
+    // allocated bytes in that case.
+    pre_tlab_size = 0;
+    tlab_unsampled_bytes_ = 0;
   }
+  size_t adjusted_size = allocation_size + tlab_unsampled_bytes_ - pre_tlab_size;
+  tlab_unsampled_bytes_ = post_tlab_size;
+  ReportAllocation(obj, adjusted_size);
 }
 
-// We are tracking the location of samples from the start location of the Tlab
-// We need to adjust how to calculate the sample position in cases where ResetTlab.
-// Adjustment is the new reference position adjustment, usually the new pos-start.
-void HeapSampler::AdjustSampleOffset(size_t adjustment) {
-  size_t* bytes_until_sample = GetBytesUntilSample();
-  size_t cur_bytes_until_sample = *bytes_until_sample;
-  if (cur_bytes_until_sample < adjustment) {
-    VLOG(heap) << "JHP:AdjustSampleOffset:No Adjustment";
-    return;
-  }
-  size_t next_bytes_until_sample = cur_bytes_until_sample - adjustment;
-  *bytes_until_sample = next_bytes_until_sample;
-  VLOG(heap) << "JHP:AdjustSampleOffset: adjustment = " << adjustment
-             << " next_bytes_until_sample = " << next_bytes_until_sample;
-}
-
-int HeapSampler::GetSamplingInterval() {
+size_t HeapSampler::GetSamplingInterval() {
   return p_sampling_interval_.load(std::memory_order_acquire);
 }
 
-void HeapSampler::SetSamplingInterval(int sampling_interval) {
-  // Make sure that rng_ and geo_dist are thread safe by acquiring a lock to access.
-  art::MutexLock mu(art::Thread::Current(), geo_dist_rng_lock_);
+void HeapSampler::SetSamplingInterval(size_t sampling_interval) {
   p_sampling_interval_.store(sampling_interval, std::memory_order_release);
-  geo_dist_.param(std::geometric_distribution<size_t>::param_type(1.0/p_sampling_interval_));
 }
 
+thread_local size_t HeapSampler::tlab_unsampled_bytes_ = 0;
+
 }  // namespace art
diff --git a/runtime/javaheapprof/javaheapsampler.h b/runtime/javaheapprof/javaheapsampler.h
index b7742c0f5e..0c3f491b3c 100644
--- a/runtime/javaheapprof/javaheapsampler.h
+++ b/runtime/javaheapprof/javaheapsampler.h
@@ -26,22 +26,10 @@ namespace art HIDDEN {
 
 class HeapSampler {
  public:
-  HeapSampler() : rng_(/*seed=*/std::minstd_rand::default_seed),
-                  geo_dist_(1.0 / /*expected value=4KB*/ 4096),
-                  geo_dist_rng_lock_("Heap Sampler RNG Geometric Dist lock",
-                                     art::LockLevel::kGenericBottomLock) {}
+  HeapSampler()
+      : rng_(/*seed=*/std::minstd_rand::default_seed),
+        rng_lock_("Heap Sampler RNG lock", art::LockLevel::kGenericBottomLock) {}
 
-  // Set the bytes until sample.
-  void SetBytesUntilSample(size_t bytes) {
-    *GetBytesUntilSample() = bytes;
-  }
-  // Get the bytes until sample.
-  size_t* GetBytesUntilSample() {
-    // Initialization should happen only once the first time the function is called.
-    // However there will always be a slot allocated for it at thread creation.
-    thread_local size_t bytes_until_sample = 0;
-    return &bytes_until_sample;
-  }
   void SetHeapID(uint32_t heap_id) {
     perfetto_heap_id_ = heap_id;
   }
@@ -51,47 +39,57 @@ class HeapSampler {
   void DisableHeapSampler() {
     enabled_.store(false, std::memory_order_release);
   }
-  // Report a sample to Perfetto.
-  void ReportSample(art::mirror::Object* obj, size_t allocation_size);
-  // Check whether we should take a sample or not at this allocation, and return the
-  // number of bytes from current pos to the next sample to use in the expand Tlab
-  // calculation.
-  // Update state of both take_sample and temp_bytes_until_sample.
-  // tlab_used = pos - start
-  // Note: we do not update bytes until sample here. It will be saved after the allocation
-  // happens. This function can be called before the actual allocation happens.
-  size_t GetSampleOffset(size_t alloc_size,
-                         size_t tlab_used,
-                         bool* take_sample,
-                         size_t* temp_bytes_until_sample) REQUIRES(!geo_dist_rng_lock_);
-  // Adjust the sample offset value with the adjustment usually (pos - start)
-  // of new Tlab after Reset.
-  void AdjustSampleOffset(size_t adjustment);
+
+  // Reports an allocation of the given size to perfetto.  This should be
+  // called for all allocations. Sampling is done internally to reduce the
+  // performance overhead based on the sampling interval.
+  EXPORT void ReportAllocation(art::mirror::Object* obj, size_t allocation_size);
+
+  // Report a tlab allocation. This will adjust the allocation size based on
+  // the number of bytes allocated in the thread local buffer since the last
+  // sample was reported.
+  // The allocation_size should be the size of the object being allocated.
+  // pre_tlab_size should be the TlabSize() before the allocation, and
+  // post_tlab_size should be the TlabSize() after the allocation.
+  void ReportTlabAllocation(art::mirror::Object* obj,
+                            size_t allocation_size,
+                            size_t pre_tlab_size,
+                            size_t post_tlab_size);
+
+  // Computes and records the next TLAB allocation size to use based on the
+  // given target size. If profiling is enabled, the size is chosen randomly
+  // based on the current sampling interval, otherwise the target is returned
+  // directly.
+  size_t NextTlabSize(size_t target) REQUIRES(!rng_lock_) {
+    return IsEnabled() ? NextRandomTlabSize(target) : target;
+  }
+
+  // Computes and records the next TLAB allocation size to use based on the
+  // given target size, assuming profiling is enabled. The size is chosen
+  // randomly based on the current sampling interval.
+  size_t NextRandomTlabSize(size_t target) REQUIRES(!rng_lock_);
+
   // Is heap sampler enabled?
   bool IsEnabled() { return enabled_.load(std::memory_order_acquire); }
   // Set the sampling interval.
-  void SetSamplingInterval(int sampling_interval) REQUIRES(!geo_dist_rng_lock_);
+  void SetSamplingInterval(size_t sampling_interval);
   // Return the sampling interval.
-  int GetSamplingInterval();
+  size_t GetSamplingInterval();
 
  private:
-  size_t NextGeoDistRandSample() REQUIRES(!geo_dist_rng_lock_);
-  // Choose, save, and return the number of bytes until the next sample,
-  // possibly decreasing sample intervals by sample_adj_bytes.
-  size_t PickAndAdjustNextSample(size_t sample_adj_bytes = 0) REQUIRES(!geo_dist_rng_lock_);
+  // The number of bytes remaining in the thread local buffer that we have not
+  // sampled yet.
+  static thread_local size_t tlab_unsampled_bytes_;
 
   std::atomic<bool> enabled_{false};
   // Default sampling interval is 4kb.
-  // Writes guarded by geo_dist_rng_lock_.
-  std::atomic<int> p_sampling_interval_{4 * 1024};
+  std::atomic<size_t> p_sampling_interval_{4 * 1024};
   uint32_t perfetto_heap_id_ = 0;
   // std random number generator.
-  std::minstd_rand rng_ GUARDED_BY(geo_dist_rng_lock_);  // Holds the state
-  // std geometric distribution
-  std::geometric_distribution</*result_type=*/size_t> geo_dist_ GUARDED_BY(geo_dist_rng_lock_);
-  // Multiple threads can access the geometric distribution and the random number
-  // generator concurrently and thus geo_dist_rng_lock_ is used for thread safety.
-  art::Mutex geo_dist_rng_lock_;
+  std::minstd_rand rng_ GUARDED_BY(rng_lock_);  // Holds the state
+  // Multiple threads can access the random number generator concurrently and
+  // thus rng_lock_ is used for thread safety.
+  art::Mutex rng_lock_;
 };
 
 }  // namespace art
diff --git a/runtime/jit/debugger_interface.cc b/runtime/jit/debugger_interface.cc
index 674e1e1b23..7ae5ca83bb 100644
--- a/runtime/jit/debugger_interface.cc
+++ b/runtime/jit/debugger_interface.cc
@@ -428,7 +428,6 @@ void AddNativeDebugInfoForDex(Thread* self, const DexFile* dexfile) {
   DCHECK(dexfile != nullptr);
   // Container dex files (v41) may store data past the size defined in the header.
   uint32_t size = dexfile->SizeIncludingSharedData();
-  CHECK(!dexfile->IsCompactDexFile());
   const ArrayRef<const uint8_t> symfile(dexfile->Begin(), size);
   CreateJITCodeEntryInternal<DexNativeInfo>(symfile);
 }
diff --git a/runtime/jit/jit_code_cache.cc b/runtime/jit/jit_code_cache.cc
index 022532a54d..ac7f1000d9 100644
--- a/runtime/jit/jit_code_cache.cc
+++ b/runtime/jit/jit_code_cache.cc
@@ -909,6 +909,7 @@ bool JitCodeCache::RemoveMethodLocked(ArtMethod* method, bool release_memory) {
           FreeCodeAndData(it->first);
         }
         VLOG(jit) << "JIT removed " << it->second->PrettyMethod() << ": " << it->first;
+        zombie_code_.erase(it->first);
         it = method_code_map_.erase(it);
       } else {
         ++it;
diff --git a/runtime/jit/profile_saver.cc b/runtime/jit/profile_saver.cc
index 733ad47ef3..1a6b9e6f40 100644
--- a/runtime/jit/profile_saver.cc
+++ b/runtime/jit/profile_saver.cc
@@ -64,6 +64,9 @@ static_assert(ProfileCompilationInfo::kIndividualInlineCacheSize ==
 // At what priority to schedule the saver threads. 9 is the lowest foreground priority on device.
 static constexpr int kProfileSaverPthreadPriority = 9;
 
+// The valid window time for responding to profile delay signal.
+static constexpr uint64_t kProfileDelaySignalValidWindowMs = 2000;
+
 static void SetProfileSaverThreadPriority(pthread_t thread, int priority) {
 #if defined(ART_TARGET_ANDROID)
   int result = setpriority(PRIO_PROCESS, pthread_gettid_np(thread), priority);
@@ -104,7 +107,8 @@ ProfileSaver::ProfileSaver(const ProfileSaverOptions& options, jit::JitCodeCache
       total_ns_of_work_(0),
       total_number_of_hot_spikes_(0),
       total_number_of_wake_ups_(0),
-      options_(options) {
+      options_(options),
+      notify_delay_time_(0) {
   DCHECK(options_.IsEnabled());
 }
 
@@ -124,6 +128,16 @@ void ProfileSaver::NotifyStartupCompleted() {
   instance_->period_condition_.Signal(self);
 }
 
+void ProfileSaver::NotifyDelayProfileSaving() {
+  Thread* self = Thread::Current();
+  MutexLock mu(self, *Locks::profiler_lock_);
+  if (instance_ == nullptr || instance_->shutting_down_) {
+    return;
+  }
+  instance_->notify_delay_time_.store(MilliTime(), std::memory_order_relaxed);
+  VLOG(profiler) << "Profile saving might be delayed";
+}
+
 void ProfileSaver::Run() {
   Thread* self = Thread::Current();
 
@@ -184,10 +198,26 @@ void ProfileSaver::Run() {
     // a reasonable margin).
     uint64_t min_save_period_ns = MsToNs(force_first_save ? options_.GetMinFirstSaveMs() :
                                                                   options_.GetMinSavePeriodMs());
-    while (min_save_period_ns * 0.9 > sleep_time) {
+    // When the delay signal is valid (the notification delay time is within
+    // kProfileDelaySignalValidWindowMs), period_condition_.TimedWait is used to wait for
+    // the remaining window time to delay the processing of the profile. When there are multiple
+    // consecutive delays, the maximum sleep time does not exceed min_save_period_ns * 2.
+    // When profiling the boot class path, the delay mechanism is always disabled to ensure the
+    // profile collection is not impacted.
+    do {
+      uint64_t time_since_notify = MilliTime() - notify_delay_time_.load(std::memory_order_relaxed);
+      bool should_delay = !options_.GetProfileBootClassPath() &&
+                          (time_since_notify < kProfileDelaySignalValidWindowMs);
+
+      if (min_save_period_ns * 0.9 <= sleep_time &&
+          !(should_delay && min_save_period_ns * 2 > sleep_time)) {
+        break;
+      }
+      uint64_t wait_time = should_delay ? kProfileDelaySignalValidWindowMs - time_since_notify
+                                        : NsToMs(min_save_period_ns - sleep_time);
       {
         MutexLock mu(self, wait_lock_);
-        period_condition_.TimedWait(self, NsToMs(min_save_period_ns - sleep_time), 0);
+        period_condition_.TimedWait(self, wait_time, 0);
         sleep_time = NanoTime() - sleep_start;
       }
       // Check if the thread was woken up for shutdown.
@@ -195,7 +225,8 @@ void ProfileSaver::Run() {
         break;
       }
       total_number_of_wake_ups_++;
-    }
+    } while (true);
+
     total_ms_of_sleep_ += NsToMs(NanoTime() - sleep_start);
 
     if (ShuttingDown(self)) {
@@ -376,7 +407,6 @@ class ProfileSaver::GetClassesAndMethodsHelper {
   struct ClassRecord {
     dex::TypeIndex type_index;
     uint16_t array_dimension;
-    uint32_t copied_methods_start;
     LengthPrefixedArray<ArtMethod>* methods;
   };
 
@@ -477,9 +507,7 @@ void ProfileSaver::GetClassesAndMethodsHelper::CollectInternal(
         return true;
       }
 
-      // Attribute the array class to the defining dex file of the element class.
-      DCHECK_EQ(klass->GetCopiedMethodsStartOffset(), 0u);
-      DCHECK(klass->GetMethodsPtr() == nullptr);
+      DCHECK_EQ(klass->NumMethods(), 0u);
     } else {
       // Non-array class. There is no need to collect primitive types.
       DCHECK(kBootClassLoader || !k->IsPrimitive());
@@ -496,12 +524,7 @@ void ProfileSaver::GetClassesAndMethodsHelper::CollectInternal(
 
     const DexFile& dex_file = k->GetDexFile();
     dex::TypeIndex type_index = k->GetDexTypeIndex();
-    uint32_t copied_methods_start = klass->GetCopiedMethodsStartOffset();
     LengthPrefixedArray<ArtMethod>* methods = klass->GetMethodsPtr();
-    if (methods != nullptr) {
-      CHECK_LE(copied_methods_start, methods->size()) << k->PrettyClass();
-    }
-
     DexFileRecords* dex_file_records;
     auto it = dex_file_records_map_.find(&dex_file);
     if (it != dex_file_records_map_.end()) {
@@ -510,8 +533,7 @@ void ProfileSaver::GetClassesAndMethodsHelper::CollectInternal(
       dex_file_records = new (&allocator_) DexFileRecords(&allocator_);
       dex_file_records_map_.insert(std::make_pair(&dex_file, dex_file_records));
     }
-    dex_file_records->class_records.push_back(
-        ClassRecord{type_index, dim, copied_methods_start, methods});
+    dex_file_records->class_records.push_back(ClassRecord{type_index, dim, methods});
     return true;
   });
 }
@@ -547,12 +569,12 @@ void ProfileSaver::GetClassesAndMethodsHelper::CollectClasses(Thread* self) {
         continue;
       }
       const size_t methods_size = methods->size();
-      CHECK_LE(class_record.copied_methods_start, methods_size)
-          << dex_file->PrettyType(class_record.type_index);
-      for (size_t index = class_record.copied_methods_start; index != methods_size; ++index) {
+      for (size_t index = methods_size; index != 0u; --index) {
         // Note: Using `ArtMethod` array with implicit `kRuntimePointerSize`.
-        ArtMethod& method = methods->At(index);
-        CHECK(method.IsCopied()) << dex_file->PrettyType(class_record.type_index);
+        ArtMethod& method = methods->At(index - 1);
+        if (!method.IsCopied()) {
+          break;
+        }
         CHECK(!method.IsNative()) << dex_file->PrettyType(class_record.type_index);
         if (method.IsInvokable()) {
           const DexFile* method_dex_file = method.GetDexFile();
@@ -628,7 +650,7 @@ void ProfileSaver::GetClassesAndMethodsHelper::UpdateProfile(const std::set<std:
     for (const ClassRecord& class_record : dex_file_records->class_records) {
       if (class_record.array_dimension != 0u) {
         DCHECK(ShouldCollectClasses(startup));
-        DCHECK(class_record.methods == nullptr);  // No methods to process.
+        DCHECK_EQ(class_record.methods->size(), 0u);  // No methods to process.
         array_class_descriptor.assign(class_record.array_dimension, '[');
         array_class_descriptor += dex_file->GetTypeDescriptorView(class_record.type_index);
         dex::TypeIndex type_index =
@@ -641,18 +663,21 @@ void ProfileSaver::GetClassesAndMethodsHelper::UpdateProfile(const std::set<std:
         if (ShouldCollectClasses(startup)) {
           profile_info->AddClass(profile_index, class_record.type_index);
         }
-        const size_t num_declared_methods = class_record.copied_methods_start;
         LengthPrefixedArray<ArtMethod>* methods = class_record.methods;
-        for (size_t index = 0; index != num_declared_methods; ++index) {
-          // Note: Using `ArtMethod` array with implicit `kRuntimePointerSize`.
-          ArtMethod& method = methods->At(index);
-          DCHECK(!method.IsCopied());
-          // We do not record native methods. Once we AOT-compile the app,
-          // all native methods shall have their JNI stubs compiled.
-          if (method.IsInvokable() && !method.IsNative()) {
-            ProfileCompilationInfo::MethodHotness::Flag flags = get_method_flags(method);
-            if (flags != 0u) {
-              profile_info->AddMethod(profile_index, method.GetDexMethodIndex(), flags);
+        if (methods != nullptr) {
+          for (size_t index = 0, size = methods->size(); index != size; ++index) {
+            // Note: Using `ArtMethod` array with implicit `kRuntimePointerSize`.
+            ArtMethod& method = methods->At(index);
+            if (method.IsCopied()) {
+              break;
+            }
+            // We do not record native methods. Once we AOT-compile the app,
+            // all native methods shall have their JNI stubs compiled.
+            if (method.IsInvokable() && !method.IsNative()) {
+              ProfileCompilationInfo::MethodHotness::Flag flags = get_method_flags(method);
+              if (flags != 0u) {
+                profile_info->AddMethod(profile_index, method.GetDexMethodIndex(), flags);
+              }
             }
           }
         }
diff --git a/runtime/jit/profile_saver.h b/runtime/jit/profile_saver.h
index c37545fa4b..892dcda69d 100644
--- a/runtime/jit/profile_saver.h
+++ b/runtime/jit/profile_saver.h
@@ -62,6 +62,9 @@ class ProfileSaver {
   // Notify that startup has completed.
   static void NotifyStartupCompleted() REQUIRES(!Locks::profiler_lock_, !instance_->wait_lock_);
 
+  // Notify to delay the profile saving.
+  static void NotifyDelayProfileSaving() REQUIRES(!Locks::profiler_lock_);
+
  private:
   // Helper classes for collecting classes and methods.
   class GetClassesAndMethodsHelper;
@@ -179,6 +182,12 @@ class ProfileSaver {
 
   const ProfileSaverOptions options_;
 
+  // Notification time for delayed profile saving in milliseconds (ms).
+  // It is used in two functions: NotifyDelayProfileSaving and ProfileSaver::Run. The former is
+  // protected by profiler_lock_, while the latter releases the profiler_lock_ at the beginning of
+  // the function. Therefore, it is defined as an atomic variable.
+  std::atomic<uint64_t> notify_delay_time_;
+
   friend class ProfileSaverTest;
   friend class ProfileSaverForBootTest;
 
diff --git a/runtime/jit/profiling_info_test.cc b/runtime/jit/profiling_info_test.cc
index 41f75b87fc..212efbce52 100644
--- a/runtime/jit/profiling_info_test.cc
+++ b/runtime/jit/profiling_info_test.cc
@@ -57,8 +57,10 @@ class ProfileCompilationInfoTest : public CommonRuntimeTest {
 
     const auto pointer_size = class_linker->GetImagePointerSize();
     std::vector<ArtMethod*> methods;
-    for (auto& m : klass->GetVirtualMethods(pointer_size)) {
-      methods.push_back(&m);
+    for (auto& m : klass->GetMethods(pointer_size)) {
+      if (m.IsVirtual()) {
+        methods.push_back(&m);
+      }
     }
     return methods;
   }
diff --git a/runtime/jni/check_jni.cc b/runtime/jni/check_jni.cc
index a05a3e97f2..24e3256d81 100644
--- a/runtime/jni/check_jni.cc
+++ b/runtime/jni/check_jni.cc
@@ -747,10 +747,10 @@ class ScopedCheck {
     return true;
   }
 
-  bool CheckInstantiableNonArray(ScopedObjectAccess& soa, jclass jc)
+  bool CheckNonArray(ScopedObjectAccess& soa, jclass jc)
       REQUIRES_SHARED(Locks::mutator_lock_) {
     ObjPtr<mirror::Class> c = soa.Decode<mirror::Class>(jc);
-    if (!c->IsInstantiableNonArray()) {
+    if (c->IsArrayClass()) {
       AbortF("can't make objects of type %s: %p", c->PrettyDescriptor().c_str(), c.Ptr());
       return false;
     }
@@ -2195,7 +2195,7 @@ class CheckJNI {
     ScopedObjectAccess soa(env);
     ScopedCheck sc(kFlag_Default, __FUNCTION__);
     JniValueType args[2] = {{.E = env}, {.c = c}};
-    if (sc.Check(soa, true, "Ec", args) && sc.CheckInstantiableNonArray(soa, c)) {
+    if (sc.Check(soa, true, "Ec", args) && sc.CheckNonArray(soa, c)) {
       JniValueType result;
       result.L = baseEnv(env)->AllocObject(env, c);
       if (sc.Check(soa, false, "L", &result)) {
@@ -2211,7 +2211,7 @@ class CheckJNI {
     ScopedCheck sc(kFlag_Default, __FUNCTION__);
     VarArgs rest(mid, vargs);
     JniValueType args[4] = {{.E = env}, {.c = c}, {.m = mid}, {.va = &rest}};
-    if (sc.Check(soa, true, "Ecm.", args) && sc.CheckInstantiableNonArray(soa, c) &&
+    if (sc.Check(soa, true, "Ecm.", args) && sc.CheckNonArray(soa, c) &&
         sc.CheckConstructor(mid)) {
       JniValueType result;
       result.L = baseEnv(env)->NewObjectV(env, c, mid, vargs);
@@ -2237,7 +2237,7 @@ class CheckJNI {
     ScopedCheck sc(kFlag_Default, __FUNCTION__);
     VarArgs rest(mid, vargs);
     JniValueType args[4] = {{.E = env}, {.c = c}, {.m = mid}, {.va = &rest}};
-    if (sc.Check(soa, true, "Ecm.", args) && sc.CheckInstantiableNonArray(soa, c) &&
+    if (sc.Check(soa, true, "Ecm.", args) && sc.CheckNonArray(soa, c) &&
         sc.CheckConstructor(mid)) {
       JniValueType result;
       result.L = baseEnv(env)->NewObjectA(env, c, mid, vargs);
diff --git a/runtime/jni/jni_internal.cc b/runtime/jni/jni_internal.cc
index 1dde2de741..91c834fc5f 100644
--- a/runtime/jni/jni_internal.cc
+++ b/runtime/jni/jni_internal.cc
@@ -36,6 +36,7 @@
 #include "class_linker-inl.h"
 #include "class_root-inl.h"
 #include "dex/dex_file-inl.h"
+#include "dex/primitive.h"
 #include "dex/utf-inl.h"
 #include "fault_handler.h"
 #include "gc/accounting/card_table-inl.h"
@@ -52,7 +53,7 @@
 #include "mirror/class-inl.h"
 #include "mirror/class_loader.h"
 #include "mirror/dex_cache-inl.h"
-#include "mirror/field.h"
+#include "mirror/field-inl.h"
 #include "mirror/method.h"
 #include "mirror/object-inl.h"
 #include "mirror/object_array-alloc-inl.h"
@@ -922,6 +923,13 @@ class JNI {
     if (c == nullptr) {
       return nullptr;
     }
+    if (UNLIKELY(!c->IsInstantiable())) {
+      soa.Self()->ThrowNewExceptionF(
+          "Ljava/lang/InstantiationException;", "Can't instantiate %s %s",
+          c->IsInterface() ? "interface" : "abstract class",
+          c->PrettyDescriptor().c_str());
+      return nullptr;
+    }
     if (c->IsStringClass()) {
       gc::AllocatorType allocator_type = Runtime::Current()->GetHeap()->GetCurrentAllocator();
       return soa.AddLocalReference<jobject>(
@@ -949,6 +957,13 @@ class JNI {
     if (c == nullptr) {
       return nullptr;
     }
+    if (UNLIKELY(!c->IsInstantiable())) {
+      soa.Self()->ThrowNewExceptionF(
+          "Ljava/lang/InstantiationException;", "Can't instantiate %s %s",
+          c->IsInterface() ? "interface" : "abstract class",
+          c->PrettyDescriptor().c_str());
+      return nullptr;
+    }
     if (c->IsStringClass()) {
       // Replace calls to String.<init> with equivalent StringFactory call.
       jmethodID sf_mid = jni::EncodeArtMethod<kEnableIndexIds>(
@@ -975,6 +990,13 @@ class JNI {
     if (c == nullptr) {
       return nullptr;
     }
+    if (UNLIKELY(!c->IsInstantiable())) {
+      soa.Self()->ThrowNewExceptionF(
+          "Ljava/lang/InstantiationException;", "Can't instantiate %s %s",
+          c->IsInterface() ? "interface" : "abstract class",
+          c->PrettyDescriptor().c_str());
+      return nullptr;
+    }
     if (c->IsStringClass()) {
       // Replace calls to String.<init> with equivalent StringFactory call.
       jmethodID sf_mid = jni::EncodeArtMethod<kEnableIndexIds>(
@@ -1582,6 +1604,19 @@ class JNI {
     CHECK_NON_NULL_ARGUMENT_RETURN_VOID(fid);
     ScopedObjectAccess soa(env);
     ArtField* f = jni::DecodeArtField<kEnableIndexIds>(fid);
+    ObjPtr<mirror::Field> reflect_field =
+        mirror::Field::CreateFromArtField(soa.Self(), f, /*force_resolve=*/ true);
+    // Android Studio needs to be able to overwrite newly introduced fields in class redefinition
+    // process.
+    if (Runtime::Current()->IsJavaDebuggableAtInit() &&
+        reflect_field->IsMonotonic() &&
+        !f->GetObject(f->GetDeclaringClass()).IsNull()) {
+      LOG(FATAL) << "Can't overwrite value of already initialized " << f->PrettyField();
+    } else {
+      if (reflect_field->IsMonotonic()) {
+        LOG(FATAL) << "Can't overwrite value of " << f->PrettyField();
+      }
+    }
     NotifySetObjectField(f, nullptr, java_value);
     ObjPtr<mirror::Object> v = soa.Decode<mirror::Object>(java_value);
     f->SetObject<false>(f->GetDeclaringClass(), v);
@@ -1612,10 +1647,52 @@ class JNI {
   ObjPtr<mirror::Object> o = soa.Decode<mirror::Object>(instance); \
   f->Set ##fn <false>(o, value)
 
+  static bool IsZero(ArtField* f) REQUIRES_SHARED(Locks::mutator_lock_) {
+    DCHECK(f->IsStatic());
+
+    switch (f->GetTypeAsPrimitiveType()) {
+      case Primitive::Type::kPrimBoolean:
+        return f->GetBoolean(f->GetDeclaringClass()) == 0;
+      case Primitive::kPrimByte:
+        return f->GetByte(f->GetDeclaringClass()) == 0;
+      case Primitive::kPrimChar:
+        return f->GetChar(f->GetDeclaringClass()) == 0;
+      case Primitive::kPrimShort:
+        return f->GetShort(f->GetDeclaringClass()) == 0;
+      case Primitive::kPrimInt:
+        return f->GetInt(f->GetDeclaringClass()) == 0;
+      case Primitive::kPrimLong:
+        return f->GetLong(f->GetDeclaringClass()) == 0;
+      case Primitive::kPrimFloat:
+        return f->GetFloat(f->GetDeclaringClass()) == 0.0f;
+      case Primitive::kPrimDouble:
+        return f->GetDouble(f->GetDeclaringClass()) == 0.0;
+      case Primitive::kPrimVoid:
+      case Primitive::kPrimNot:
+        LOG(FATAL) << f->PrettyField()
+                   << " expected to be primitive, but is "
+                   << f->GetTypeAsPrimitiveType();
+        UNREACHABLE();
+    }
+  }
+
 #define SET_STATIC_PRIMITIVE_FIELD(fn, value) \
   CHECK_NON_NULL_ARGUMENT_RETURN_VOID(fid); \
   ScopedObjectAccess soa(env); \
   ArtField* f = jni::DecodeArtField<kEnableIndexIds>(fid); \
+  ObjPtr<mirror::Field> reflect_field = \
+    mirror::Field::CreateFromArtField(soa.Self(), f, /*force_resolve=*/ true); \
+  /* Android Studio needs to be able to overwrite newly introduced fields in class redefinition */ \
+  /* process. */ \
+  if (Runtime::Current()->IsJavaDebuggableAtInit()) { \
+    if (reflect_field->IsMonotonic() && !IsZero(f)) { \
+      LOG(FATAL) << "Can't overwrite value of already initialized " << f->PrettyField(); \
+    } \
+  } else { \
+    if (reflect_field->IsMonotonic()) { \
+      LOG(FATAL) << "Can't overwrite value of " << f->PrettyField(); \
+    } \
+  } \
   NotifySetPrimitiveField(f, nullptr, JValue::FromPrimitive<decltype(value)>(value)); \
   f->Set ##fn <false>(f->GetDeclaringClass(), value)
 
@@ -2933,7 +3010,8 @@ class JNI {
     return array;
   }
 
-  static bool IsClassLoaderNamespaceNativelyBridged(JNIEnv* env, jobject jclass_loader) {
+  static bool IsClassLoaderNamespaceNativelyBridged(JNIEnv* env, jobject jclass_loader)
+      REQUIRES(!Locks::mutator_lock_) {
 #if defined(ART_TARGET_ANDROID)
     android::NativeLoaderNamespace* ns =
         android::FindNativeLoaderNamespaceByClassLoader(env, jclass_loader);
diff --git a/runtime/method_handles.cc b/runtime/method_handles.cc
index ea7b3d39e1..07fc391725 100644
--- a/runtime/method_handles.cc
+++ b/runtime/method_handles.cc
@@ -481,14 +481,10 @@ ArtMethod* RefineTargetMethod(Thread* self,
     if (referrer_class == declaring_class) {
       return target_method;
     }
-    if (declaring_class->IsInterface()) {
-      if (target_method->IsAbstract()) {
-        std::string msg =
-            "Method " + target_method->PrettyMethod() + " is abstract interface method!";
-        ThrowIllegalAccessException(msg.c_str());
-        return nullptr;
-      }
-    } else {
+    CHECK(!target_method->IsAbstract())
+        << "invoke-super MethodHandle can't target abstract methods: "
+        << target_method->PrettyMethod();
+    if (!declaring_class->IsInterface()) {
       ObjPtr<mirror::Class> super_class = referrer_class->GetSuperClass();
       uint16_t vtable_index = target_method->GetMethodIndex();
       DCHECK(super_class != nullptr);
diff --git a/runtime/metrics/statsd.cc b/runtime/metrics/statsd.cc
index 26f6c20664..a52a913cb7 100644
--- a/runtime/metrics/statsd.cc
+++ b/runtime/metrics/statsd.cc
@@ -342,21 +342,6 @@ constexpr int32_t EncodeGcCollectorType(gc::CollectorType collector_type) {
   }
 }
 
-int32_t EncodeUffdMinorFaultSupport() {
-  auto [uffd_supported, minor_fault_supported] = gc::collector::MarkCompact::GetUffdAndMinorFault();
-
-  if (uffd_supported) {
-    if (minor_fault_supported) {
-      return statsd::ART_DATUM_REPORTED__UFFD_SUPPORT__ART_UFFD_SUPPORT_MINOR_FAULT_MODE_SUPPORTED;
-    } else {
-      return statsd::
-          ART_DATUM_REPORTED__UFFD_SUPPORT__ART_UFFD_SUPPORT_MINOR_FAULT_MODE_NOT_SUPPORTED;
-    }
-  } else {
-    return statsd::ART_DATUM_REPORTED__UFFD_SUPPORT__ART_UFFD_SUPPORT_UFFD_NOT_SUPPORTED;
-  }
-}
-
 class StatsdBackend : public MetricsBackend {
  public:
   void BeginOrUpdateSession(const SessionData& session_data) override {
@@ -403,7 +388,7 @@ class StatsdBackend : public MetricsBackend {
         0,  // deprecated - was ArtApkType
         EncodeInstructionSet(kRuntimeISA),
         EncodeGcCollectorType(Runtime::Current()->GetHeap()->GetForegroundCollectorType()),
-        EncodeUffdMinorFaultSupport());
+        0);  // deprecated - was ArtUffdSupport
   }
 
   void ReportHistogram(DatumId /*histogram_type*/,
diff --git a/runtime/mirror/array-inl.h b/runtime/mirror/array-inl.h
index 36113d679f..fd80c9f6d0 100644
--- a/runtime/mirror/array-inl.h
+++ b/runtime/mirror/array-inl.h
@@ -39,7 +39,10 @@ inline uint32_t Array::ClassSize(PointerSize pointer_size) {
 
 template <VerifyObjectFlags kVerifyFlags>
 inline size_t Array::SizeOf(size_t component_size_shift) {
-  int32_t component_count = GetLength<kVerifyFlags>();
+  return SizeOf(component_size_shift, GetLength<kVerifyFlags>());
+}
+
+inline size_t Array::SizeOf(size_t component_size_shift, int32_t component_count) {
   // This is safe from overflow because the array was already allocated.
   size_t header_size = DataOffset(1U << component_size_shift).SizeValue();
   size_t data_size = component_count << component_size_shift;
@@ -152,44 +155,41 @@ inline void PrimitiveArray<T>::Memmove(int32_t dst_pos,
     Memcpy(dst_pos, src, src_pos, count);
   } else {
     // Handle copies within the same array using the appropriate direction copy.
-    void* dst_raw = GetRawData(sizeof(T), dst_pos);
-    const void* src_raw = src->GetRawData(sizeof(T), src_pos);
+    static_assert(sizeof(T) == sizeof(uint8_t) || sizeof(T) == sizeof(uint16_t) ||
+                  sizeof(T) == sizeof(uint32_t) || sizeof(T) == sizeof(uint64_t));
+    T* d = GetData(dst_pos);
+    const T* s = src->GetData(src_pos);
     if (sizeof(T) == sizeof(uint8_t)) {
-      uint8_t* d = reinterpret_cast<uint8_t*>(dst_raw);
-      const uint8_t* s = reinterpret_cast<const uint8_t*>(src_raw);
       memmove(d, s, count);
     } else {
       const bool copy_forward = (dst_pos < src_pos) || (dst_pos - src_pos >= count);
-      if (sizeof(T) == sizeof(uint16_t)) {
-        uint16_t* d = reinterpret_cast<uint16_t*>(dst_raw);
-        const uint16_t* s = reinterpret_cast<const uint16_t*>(src_raw);
-        if (copy_forward) {
-          ArrayForwardCopy<uint16_t>(d, s, count);
-        } else {
-          ArrayBackwardCopy<uint16_t>(d, s, count);
-        }
-      } else if (sizeof(T) == sizeof(uint32_t)) {
-        uint32_t* d = reinterpret_cast<uint32_t*>(dst_raw);
-        const uint32_t* s = reinterpret_cast<const uint32_t*>(src_raw);
-        if (copy_forward) {
-          ArrayForwardCopy<uint32_t>(d, s, count);
-        } else {
-          ArrayBackwardCopy<uint32_t>(d, s, count);
-        }
+      if (copy_forward) {
+        ArrayForwardCopy<T>(d, s, count);
       } else {
-        DCHECK_EQ(sizeof(T), sizeof(uint64_t));
-        uint64_t* d = reinterpret_cast<uint64_t*>(dst_raw);
-        const uint64_t* s = reinterpret_cast<const uint64_t*>(src_raw);
-        if (copy_forward) {
-          ArrayForwardCopy<uint64_t>(d, s, count);
-        } else {
-          ArrayBackwardCopy<uint64_t>(d, s, count);
-        }
+        ArrayBackwardCopy<T>(d, s, count);
       }
     }
   }
 }
 
+template <class T>
+inline static void ForwardCopy(T* dst_raw, const T* src_raw, int32_t count) {
+  // Note for non-byte copies we can't rely on standard libc functions like memcpy(3) and memmove(3)
+  // in our implementation, because they may copy byte-by-byte.
+  static_assert(sizeof(T) == sizeof(uint8_t) || sizeof(T) == sizeof(uint16_t) ||
+                sizeof(T) == sizeof(uint32_t) || sizeof(T) == sizeof(uint64_t));
+  if (sizeof(T) == sizeof(uint8_t)) {
+    memcpy(dst_raw, src_raw, count);
+  } else if (sizeof(T) == sizeof(uint32_t)) {
+    // b/392789466 Avoids copy using float registers on aarch64 for better performance.
+    uint32_t* d = reinterpret_cast<uint32_t*>(dst_raw);
+    const uint32_t* s = reinterpret_cast<const uint32_t*>(src_raw);
+    ArrayForwardCopy<uint32_t>(d, s, count);
+  } else {
+    ArrayForwardCopy<T>(dst_raw, src_raw, count);
+  }
+}
+
 template<class T>
 inline void PrimitiveArray<T>::Memcpy(int32_t dst_pos,
                                       ObjPtr<PrimitiveArray<T>> src,
@@ -207,24 +207,46 @@ inline void PrimitiveArray<T>::Memcpy(int32_t dst_pos,
   DCHECK_LT(src_pos, src->GetLength());
   DCHECK_LE(src_pos, src->GetLength() - count);
 
-  // Note for non-byte copies we can't rely on standard libc functions like memcpy(3) and memmove(3)
-  // in our implementation, because they may copy byte-by-byte.
-  static_assert(sizeof(T) == sizeof(uint8_t) || sizeof(T) == sizeof(uint16_t) ||
-                sizeof(T) == sizeof(uint32_t) || sizeof(T) == sizeof(uint64_t));
-  void* dst_raw = GetRawData(sizeof(T), dst_pos);
-  const void* src_raw = src->GetRawData(sizeof(T), src_pos);
-  if (sizeof(T) == sizeof(uint8_t)) {
-    memcpy(dst_raw, src_raw, count);
-  } else if (sizeof(T) == sizeof(uint32_t)) {
-    // b/392789466 Avoids copy using float registers on aarch64 for better performance.
-    uint32_t* d = reinterpret_cast<uint32_t*>(dst_raw);
-    const uint32_t* s = reinterpret_cast<const uint32_t*>(src_raw);
-    ArrayForwardCopy<uint32_t>(d, s, count);
-  } else {
-    T* d = reinterpret_cast<T*>(dst_raw);
-    const T* s = reinterpret_cast<const T*>(src_raw);
-    ArrayForwardCopy<T>(d, s, count);
+  T* d = GetData(dst_pos);
+  const T* s = src->GetData(src_pos);
+  ForwardCopy<T>(d, s, count);
+}
+
+template <class T>
+inline void PrimitiveArray<T>::Memcpy(int32_t dst_pos,
+                                      const T* src,
+                                      int32_t src_pos,
+                                      int32_t count) {
+  if (UNLIKELY(count == 0)) {
+    return;
+  }
+  DCHECK_GE(dst_pos, 0);
+  DCHECK_GE(src_pos, 0);
+  DCHECK_GT(count, 0);
+  DCHECK(src != nullptr);
+  DCHECK_LT(dst_pos, GetLength());
+  DCHECK_LE(dst_pos, GetLength() - count);
+
+  T* d = GetData(dst_pos);
+  const T* s = src + src_pos;
+  ForwardCopy<T>(d, s, count);
+}
+
+template <class T>
+inline void PrimitiveArray<T>::MemcpyTo(int32_t src_pos, T* dst, int32_t dst_pos, int32_t count) {
+  if (UNLIKELY(count == 0)) {
+    return;
   }
+  DCHECK_GE(dst_pos, 0);
+  DCHECK_GE(src_pos, 0);
+  DCHECK_GT(count, 0);
+  DCHECK(dst != nullptr);
+  DCHECK_LT(src_pos, GetLength());
+  DCHECK_LE(src_pos, GetLength() - count);
+
+  T* d = dst + dst_pos;
+  const T* s = GetData(src_pos);
+  ForwardCopy<T>(d, s, count);
 }
 
 template<typename T, PointerSize kPointerSize, VerifyObjectFlags kVerifyFlags>
diff --git a/runtime/mirror/array.h b/runtime/mirror/array.h
index 7a0976ab48..26a7f67ba4 100644
--- a/runtime/mirror/array.h
+++ b/runtime/mirror/array.h
@@ -59,8 +59,11 @@ class MANAGED Array : public Object {
       REQUIRES_SHARED(Locks::mutator_lock_)
       REQUIRES(!Roles::uninterruptible_);
 
+  static size_t SizeOf(size_t component_size_shift, int32_t component_count);
+
   template <VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   size_t SizeOf(size_t component_size_shift) REQUIRES_SHARED(Locks::mutator_lock_);
+
   template <VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags,
             ReadBarrierOption kReadBarrierOption = kWithoutReadBarrier>
   size_t SizeOf() REQUIRES_SHARED(Locks::mutator_lock_);
@@ -105,24 +108,12 @@ class MANAGED Array : public Object {
         + (index * component_size);
     return reinterpret_cast<void*>(data);
   }
-  template <size_t kComponentSize>
-  void* GetRawData(int32_t index) REQUIRES_SHARED(Locks::mutator_lock_) {
-    intptr_t data = reinterpret_cast<intptr_t>(this) + DataOffset<kComponentSize>().Int32Value() +
-        + (index * kComponentSize);
-    return reinterpret_cast<void*>(data);
-  }
 
   const void* GetRawData(size_t component_size, int32_t index) const {
     intptr_t data = reinterpret_cast<intptr_t>(this) + DataOffset(component_size).Int32Value() +
         + (index * component_size);
     return reinterpret_cast<void*>(data);
   }
-  template <size_t kComponentSize>
-  const void* GetRawData(int32_t index) const {
-    intptr_t data = reinterpret_cast<intptr_t>(this) + DataOffset<kComponentSize>().Int32Value() +
-        + (index * kComponentSize);
-    return reinterpret_cast<void*>(data);
-  }
 
   // Returns true if the index is valid. If not, throws an ArrayIndexOutOfBoundsException and
   // returns false.
@@ -173,11 +164,19 @@ class MANAGED PrimitiveArray : public Array {
       REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(!Roles::uninterruptible_);
 
   const T* GetData() const ALWAYS_INLINE  REQUIRES_SHARED(Locks::mutator_lock_) {
-    return reinterpret_cast<const T*>(GetRawData<sizeof(T)>(0));
+    return GetData(0);
+  }
+
+  T* GetData() ALWAYS_INLINE REQUIRES_SHARED(Locks::mutator_lock_) { return GetData(0); }
+
+  const T* GetData(int32_t index) const ALWAYS_INLINE REQUIRES_SHARED(Locks::mutator_lock_) {
+    intptr_t data = reinterpret_cast<intptr_t>(this) + DataOffset<sizeof(T)>().Int32Value();
+    return reinterpret_cast<T*>(data) + index;
   }
 
-  T* GetData() ALWAYS_INLINE REQUIRES_SHARED(Locks::mutator_lock_) {
-    return reinterpret_cast<T*>(GetRawData<sizeof(T)>(0));
+  T* GetData(int32_t index) ALWAYS_INLINE REQUIRES_SHARED(Locks::mutator_lock_) {
+    intptr_t data = reinterpret_cast<intptr_t>(this) + DataOffset<sizeof(T)>().Int32Value();
+    return reinterpret_cast<T*>(data) + index;
   }
 
   T Get(int32_t i) ALWAYS_INLINE REQUIRES_SHARED(Locks::mutator_lock_);
@@ -216,6 +215,10 @@ class MANAGED PrimitiveArray : public Array {
    */
   EXPORT void Memcpy(int32_t dst_pos, ObjPtr<PrimitiveArray<T>> src, int32_t src_pos, int32_t count)
       REQUIRES_SHARED(Locks::mutator_lock_);
+  EXPORT void Memcpy(int32_t dst_pos, const T* src, int32_t src_pos, int32_t count)
+      REQUIRES_SHARED(Locks::mutator_lock_);
+  EXPORT void MemcpyTo(int32_t src_pos, T* dst, int32_t dst_pos, int32_t count)
+      REQUIRES_SHARED(Locks::mutator_lock_);
 
  private:
   DISALLOW_IMPLICIT_CONSTRUCTORS(PrimitiveArray);
diff --git a/runtime/mirror/class-inl.h b/runtime/mirror/class-inl.h
index 06b5cf5e9f..8c3afa51da 100644
--- a/runtime/mirror/class-inl.h
+++ b/runtime/mirror/class-inl.h
@@ -50,7 +50,7 @@ template<VerifyObjectFlags kVerifyFlags>
 inline uint32_t Class::GetObjectSize() {
   // Note: Extra parentheses to avoid the comma being interpreted as macro parameter separator.
   DCHECK((!IsVariableSize<kVerifyFlags>())) << "class=" << PrettyTypeOf();
-  return GetField32(ObjectSizeOffset());
+  return GetObjectSizeUnchecked<kVerifyFlags>();
 }
 
 template<VerifyObjectFlags kVerifyFlags>
@@ -106,43 +106,6 @@ inline ObjPtr<DexCache> Class::GetDexCache() {
       OFFSET_OF_OBJECT_MEMBER(Class, dex_cache_));
 }
 
-inline uint32_t Class::GetCopiedMethodsStartOffset() {
-  // Object::GetFieldShort returns an int16_t value, but
-  // Class::copied_methods_offset_ is an uint16_t value; cast the
-  // latter to uint16_t before returning it as an uint32_t value, so
-  // that uint16_t values between 2^15 and 2^16-1 are correctly
-  // handled.
-  return static_cast<uint16_t>(
-      GetFieldShort(OFFSET_OF_OBJECT_MEMBER(Class, copied_methods_offset_)));
-}
-
-inline uint32_t Class::GetDirectMethodsStartOffset() {
-  return 0;
-}
-
-inline uint32_t Class::GetVirtualMethodsStartOffset() {
-  // Object::GetFieldShort returns an int16_t value, but
-  // Class::virtual_method_offset_ is an uint16_t value; cast the
-  // latter to uint16_t before returning it as an uint32_t value, so
-  // that uint16_t values between 2^15 and 2^16-1 are correctly
-  // handled.
-  return static_cast<uint16_t>(
-      GetFieldShort(OFFSET_OF_OBJECT_MEMBER(Class, virtual_methods_offset_)));
-}
-
-template<VerifyObjectFlags kVerifyFlags>
-inline ArraySlice<ArtMethod> Class::GetDirectMethodsSlice(PointerSize pointer_size) {
-  DCHECK(IsLoaded() || IsErroneous()) << GetStatus();
-  return GetDirectMethodsSliceUnchecked(pointer_size);
-}
-
-inline ArraySlice<ArtMethod> Class::GetDirectMethodsSliceUnchecked(PointerSize pointer_size) {
-  return GetMethodsSliceRangeUnchecked(GetMethodsPtr(),
-                                       pointer_size,
-                                       GetDirectMethodsStartOffset(),
-                                       GetVirtualMethodsStartOffset());
-}
-
 template<VerifyObjectFlags kVerifyFlags>
 inline ArraySlice<ArtMethod> Class::GetDeclaredMethodsSlice(PointerSize pointer_size) {
   DCHECK(IsLoaded() || IsErroneous()) << GetStatus();
@@ -152,36 +115,8 @@ inline ArraySlice<ArtMethod> Class::GetDeclaredMethodsSlice(PointerSize pointer_
 inline ArraySlice<ArtMethod> Class::GetDeclaredMethodsSliceUnchecked(PointerSize pointer_size) {
   return GetMethodsSliceRangeUnchecked(GetMethodsPtr(),
                                        pointer_size,
-                                       GetDirectMethodsStartOffset(),
-                                       GetCopiedMethodsStartOffset());
-}
-
-template<VerifyObjectFlags kVerifyFlags>
-inline ArraySlice<ArtMethod> Class::GetDeclaredVirtualMethodsSlice(PointerSize pointer_size) {
-  DCHECK(IsLoaded() || IsErroneous()) << GetStatus();
-  return GetDeclaredVirtualMethodsSliceUnchecked(pointer_size);
-}
-
-inline ArraySlice<ArtMethod> Class::GetDeclaredVirtualMethodsSliceUnchecked(
-    PointerSize pointer_size) {
-  return GetMethodsSliceRangeUnchecked(GetMethodsPtr(),
-                                       pointer_size,
-                                       GetVirtualMethodsStartOffset(),
-                                       GetCopiedMethodsStartOffset());
-}
-
-template<VerifyObjectFlags kVerifyFlags>
-inline ArraySlice<ArtMethod> Class::GetVirtualMethodsSlice(PointerSize pointer_size) {
-  DCHECK(IsLoaded() || IsErroneous());
-  return GetVirtualMethodsSliceUnchecked(pointer_size);
-}
-
-inline ArraySlice<ArtMethod> Class::GetVirtualMethodsSliceUnchecked(PointerSize pointer_size) {
-  LengthPrefixedArray<ArtMethod>* methods = GetMethodsPtr();
-  return GetMethodsSliceRangeUnchecked(methods,
-                                       pointer_size,
-                                       GetVirtualMethodsStartOffset(),
-                                       NumMethods(methods));
+                                       /* start_offset= */ 0u,
+                                       NumDeclaredMethods());
 }
 
 template<VerifyObjectFlags kVerifyFlags>
@@ -192,9 +127,10 @@ inline ArraySlice<ArtMethod> Class::GetCopiedMethodsSlice(PointerSize pointer_si
 
 inline ArraySlice<ArtMethod> Class::GetCopiedMethodsSliceUnchecked(PointerSize pointer_size) {
   LengthPrefixedArray<ArtMethod>* methods = GetMethodsPtr();
+  DCHECK_LE(NumDeclaredMethods(), NumMethods(methods)) << PrettyClass();
   return GetMethodsSliceRangeUnchecked(methods,
                                        pointer_size,
-                                       GetCopiedMethodsStartOffset(),
+                                       NumDeclaredMethods(),
                                        NumMethods(methods));
 }
 
@@ -206,6 +142,10 @@ inline LengthPrefixedArray<ArtMethod>* Class::GetMethodsPtr() {
 template<VerifyObjectFlags kVerifyFlags>
 inline ArraySlice<ArtMethod> Class::GetMethodsSlice(PointerSize pointer_size) {
   DCHECK(IsLoaded() || IsErroneous());
+  return GetMethodsSliceUnchecked(pointer_size);
+}
+
+inline ArraySlice<ArtMethod> Class::GetMethodsSliceUnchecked(PointerSize pointer_size) {
   LengthPrefixedArray<ArtMethod>* methods = GetMethodsPtr();
   return GetMethodsSliceRangeUnchecked(methods, pointer_size, 0, NumMethods(methods));
 }
@@ -232,21 +172,12 @@ inline ArraySlice<ArtMethod> Class::GetMethodsSliceRangeUnchecked(
 }
 
 inline uint32_t Class::NumMethods() {
+  DCHECK_NE(GetMethodsPtr(), nullptr);
   return NumMethods(GetMethodsPtr());
 }
 
 inline uint32_t Class::NumMethods(LengthPrefixedArray<ArtMethod>* methods) {
-  return (methods == nullptr) ? 0 : methods->size();
-}
-
-inline ArtMethod* Class::GetDirectMethodUnchecked(size_t i, PointerSize pointer_size) {
-  CheckPointerSize(pointer_size);
-  return &GetDirectMethodsSliceUnchecked(pointer_size)[i];
-}
-
-inline ArtMethod* Class::GetDirectMethod(size_t i, PointerSize pointer_size) {
-  CheckPointerSize(pointer_size);
-  return &GetDirectMethodsSlice(pointer_size)[i];
+  return methods->size();
 }
 
 inline void Class::SetMethodsPtr(LengthPrefixedArray<ArtMethod>* new_methods,
@@ -256,36 +187,12 @@ inline void Class::SetMethodsPtr(LengthPrefixedArray<ArtMethod>* new_methods,
   SetMethodsPtrUnchecked(new_methods, num_direct, num_virtual);
 }
 
-
 inline void Class::SetMethodsPtrUnchecked(LengthPrefixedArray<ArtMethod>* new_methods,
-                                          uint32_t num_direct,
-                                          uint32_t num_virtual) {
+                                          [[maybe_unused]] uint32_t num_direct,
+                                          [[maybe_unused]] uint32_t num_virtual) {
   DCHECK_LE(num_direct + num_virtual, (new_methods == nullptr) ? 0 : new_methods->size());
-  SetField64<false>(OFFSET_OF_OBJECT_MEMBER(Class, methods_),
-                    static_cast<uint64_t>(reinterpret_cast<uintptr_t>(new_methods)));
-  SetFieldShort<false>(OFFSET_OF_OBJECT_MEMBER(Class, copied_methods_offset_),
-                    dchecked_integral_cast<uint16_t>(num_direct + num_virtual));
-  SetFieldShort<false>(OFFSET_OF_OBJECT_MEMBER(Class, virtual_methods_offset_),
-                       dchecked_integral_cast<uint16_t>(num_direct));
-}
-
-template<VerifyObjectFlags kVerifyFlags>
-inline ArtMethod* Class::GetVirtualMethod(size_t i, PointerSize pointer_size) {
-  CheckPointerSize(pointer_size);
-  DCHECK(IsResolved<kVerifyFlags>() || IsErroneous<kVerifyFlags>())
-      << Class::PrettyClass() << " status=" << GetStatus();
-  return GetVirtualMethodUnchecked(i, pointer_size);
-}
-
-inline ArtMethod* Class::GetVirtualMethodDuringLinking(size_t i, PointerSize pointer_size) {
-  CheckPointerSize(pointer_size);
-  DCHECK(IsLoaded() || IsErroneous());
-  return GetVirtualMethodUnchecked(i, pointer_size);
-}
-
-inline ArtMethod* Class::GetVirtualMethodUnchecked(size_t i, PointerSize pointer_size) {
-  CheckPointerSize(pointer_size);
-  return &GetVirtualMethodsSliceUnchecked(pointer_size)[i];
+  SetField64<false, false>(OFFSET_OF_OBJECT_MEMBER(Class, methods_),
+                           static_cast<uint64_t>(reinterpret_cast<uintptr_t>(new_methods)));
 }
 
 template<VerifyObjectFlags kVerifyFlags, ReadBarrierOption kReadBarrierOption>
@@ -687,7 +594,7 @@ inline void Class::SetFieldsPtr(LengthPrefixedArray<ArtField>* new_fields) {
 }
 
 inline void Class::SetFieldsPtrUnchecked(LengthPrefixedArray<ArtField>* new_fields) {
-  SetFieldPtr<false, true, kVerifyNone>(OFFSET_OF_OBJECT_MEMBER(Class, fields_), new_fields);
+  SetFieldPtr<false, false, kVerifyNone>(OFFSET_OF_OBJECT_MEMBER(Class, fields_), new_fields);
 }
 
 inline LengthPrefixedArray<ArtField>* Class::GetFieldsPtrUnchecked() {
@@ -701,7 +608,7 @@ inline ArtField* Class::GetField(uint32_t i) {
 template<VerifyObjectFlags kVerifyFlags>
 inline uint32_t Class::GetReferenceInstanceOffsets() {
   DCHECK(IsResolved<kVerifyFlags>() || IsErroneous<kVerifyFlags>());
-  return GetField32<kVerifyFlags>(OFFSET_OF_OBJECT_MEMBER(Class, reference_instance_offsets_));
+  return GetReferenceInstanceOffsetsUnchecked<kVerifyFlags>();
 }
 
 inline void Class::SetClinitThreadId(pid_t new_clinit_thread_id) {
@@ -1049,9 +956,12 @@ inline void Class::SetAccessFlags(uint32_t new_access_flags) {
   }
 }
 
-inline void Class::SetClassFlags(uint32_t new_flags) {
-  SetField32</*kTransactionActive=*/ false, /*kCheckTransaction=*/ false>(
-      OFFSET_OF_OBJECT_MEMBER(Class, class_flags_), new_flags);
+inline void Class::AddRemoveClassFlags(uint32_t new_flags, uint32_t clear_flags) {
+  DCHECK_EQ(new_flags & clear_flags, 0u);
+  uint32_t flags = GetClassFlags();
+  flags &= ~clear_flags;
+  flags |= new_flags;
+  SetField32</*kTransactionActive=*/false, /*kCheckTransaction=*/false>(ClassFlagsOffset(), flags);
 }
 
 inline uint32_t Class::NumDirectInterfaces() {
@@ -1072,30 +982,15 @@ inline uint32_t Class::NumDirectInterfaces() {
   }
 }
 
-inline ArraySlice<ArtMethod> Class::GetDirectMethods(PointerSize pointer_size) {
-  CheckPointerSize(pointer_size);
-  return GetDirectMethodsSliceUnchecked(pointer_size);
-}
-
 inline ArraySlice<ArtMethod> Class::GetDeclaredMethods(PointerSize pointer_size) {
   return GetDeclaredMethodsSliceUnchecked(pointer_size);
 }
 
-inline ArraySlice<ArtMethod> Class::GetDeclaredVirtualMethods(PointerSize pointer_size) {
-  return GetDeclaredVirtualMethodsSliceUnchecked(pointer_size);
-}
-
-inline ArraySlice<ArtMethod> Class::GetVirtualMethods(PointerSize pointer_size) {
-  CheckPointerSize(pointer_size);
-  return GetVirtualMethodsSliceUnchecked(pointer_size);
-}
-
 inline ArraySlice<ArtMethod> Class::GetCopiedMethods(PointerSize pointer_size) {
   CheckPointerSize(pointer_size);
   return GetCopiedMethodsSliceUnchecked(pointer_size);
 }
 
-
 inline ArraySlice<ArtMethod> Class::GetMethods(PointerSize pointer_size) {
   CheckPointerSize(pointer_size);
   LengthPrefixedArray<ArtMethod>* methods = GetMethodsPtr();
@@ -1193,20 +1088,47 @@ inline bool Class::IsAssignableFrom(ObjPtr<Class> src) {
 }
 
 inline uint32_t Class::NumDirectMethods() {
-  return GetVirtualMethodsStartOffset();
+  if (IsProxyClass()) {
+    // Proxy classes have one constructor, and then only virtual methods.
+    return 1;
+  }
+  if (IsArrayClass() || IsPrimitive()) {
+    return 0u;
+  }
+  ClassAccessor accessor(GetDexFile(), GetDexClassDefIndex());
+  return accessor.NumDirectMethods();
 }
 
 inline uint32_t Class::NumDeclaredVirtualMethods() {
-  return GetCopiedMethodsStartOffset() - GetVirtualMethodsStartOffset();
+  if (IsProxyClass()) {
+    // Proxy classes have one constructor, and then only virtual methods.
+    return NumMethods() - 1;
+  }
+  if (IsArrayClass() || IsPrimitive()) {
+    return 0u;
+  }
+  ClassAccessor accessor(GetDexFile(), GetDexClassDefIndex());
+  return accessor.NumVirtualMethods();
 }
 
 inline uint32_t Class::NumVirtualMethods() {
-  return NumMethods() - GetVirtualMethodsStartOffset();
+  return NumMethods() - NumDirectMethods();
+}
+
+inline uint32_t Class::NumDeclaredMethods() {
+  if (IsProxyClass()) {
+    return NumMethods();
+  }
+  if (IsArrayClass() || IsPrimitive()) {
+    return 0u;
+  }
+  ClassAccessor accessor(GetDexFile(), GetDexClassDefIndex());
+  return accessor.NumDirectMethods() + accessor.NumVirtualMethods();
 }
 
 inline uint32_t Class::NumFields() {
-  LengthPrefixedArray<ArtField>* arr = GetFieldsPtrUnchecked();
-  return arr != nullptr ? arr->size() : 0u;
+  DCHECK_NE(GetFieldsPtrUnchecked(), nullptr) << PrettyClass();
+  return GetFieldsPtrUnchecked()->size();
 }
 
 inline bool Class::HasStaticFields() {
@@ -1366,33 +1288,82 @@ inline ImTable* Class::FindSuperImt(PointerSize pointer_size) {
   return nullptr;
 }
 
+template <bool kOnlyLookAtIndex>
 ALWAYS_INLINE FLATTEN inline ArtField* Class::FindDeclaredField(uint32_t dex_field_idx) {
-  size_t num_fields = NumFields();
-  if (num_fields > 0) {
-    // The field array is an ordered list of fields where there may be missing
-    // indices. For example, it could be [40, 42], but in 90% of cases cases we have
-    // [40, 41, 42]. The latter is the case we are optimizing for, where for
-    // example `dex_field_idx` is 41, and we can just substract it with the
-    // first field index (40) and directly access the array with that index (1).
-    uint32_t index = dex_field_idx - GetField(0)->GetDexFieldIndex();
-    if (index < num_fields) {
-      ArtField* field = GetField(index);
-      if (field->GetDexFieldIndex() == dex_field_idx) {
-        return field;
-      }
-    } else {
-      index = num_fields;
+  LengthPrefixedArray<ArtField>* array = GetFieldsPtrUnchecked();
+  size_t size = array->size();
+  if (size == 0) {
+    return nullptr;
+  }
+  // The field array is an ordered list of fields where there may be missing
+  // indices. For example, it could be [40, 42], but in 90% of cases cases we have
+  // [40, 41, 42]. The latter is the case we are optimizing for, where for
+  // example `dex_field_idx` is 41, and we can just substract it with the
+  // first field index (40) and directly access the array with that index (1).
+  uint32_t index = dex_field_idx - array->At(0).GetDexFieldIndex();
+  if (index < size) {
+    ArtField& field = array->At(index);
+    if (field.GetDexFieldIndex() == dex_field_idx) {
+      return &field;
     }
-    // If there is a field, it's down the array. The array is ordered by field
-    // index, so we know we can stop the search if `dex_field_idx` is greater
-    // than the current field's index.
-    for (; index > 0; --index) {
-      ArtField* field = GetField(index - 1);
-      if (field->GetDexFieldIndex() == dex_field_idx) {
-        return field;
-      } else if (field->GetDexFieldIndex() < dex_field_idx) {
-        break;
-      }
+  } else {
+    index = size;
+  }
+  if (kOnlyLookAtIndex) {
+    return nullptr;
+  }
+  // If there is a field, it's down the array. The array is ordered by field
+  // index, so we know we can stop the search if `dex_field_idx` is greater
+  // than the current field's index.
+  for (; index > 0; --index) {
+    ArtField& field = array->At(index - 1);
+    if (field.GetDexFieldIndex() == dex_field_idx) {
+      return &field;
+    } else if (field.GetDexFieldIndex() < dex_field_idx) {
+      break;
+    }
+  }
+  return nullptr;
+}
+
+template <bool kOnlyLookAtIndex, PointerSize kPointerSize>
+ALWAYS_INLINE FLATTEN inline ArtMethod* Class::FindDeclaredClassMethod(uint32_t dex_method_idx) {
+  LengthPrefixedArray<ArtMethod>* array = GetMethodsPtr();
+  static constexpr size_t kMethodAlignment = ArtMethod::Alignment(kPointerSize);
+  static constexpr size_t kMethodSize = ArtMethod::Size(kPointerSize);
+
+  size_t size = array->size();
+  if (size == 0) {
+    return nullptr;
+  }
+  // The method array is an ordered list of methods where there may be missing
+  // indices. For example, it could be [40, 42], but in 90% of cases cases we have
+  // [40, 41, 42]. The latter is the case we are optimizing for, where for
+  // example `dex_method_idx` is 41, and we can just substract it with the
+  // first method index (40) and directly access the array with that index (1).
+  uint32_t index = dex_method_idx - array->At(0, kMethodSize, kMethodAlignment).GetDexMethodIndex();
+  if (index < size) {
+    ArtMethod& method = array->At(index, kMethodSize, kMethodAlignment);
+    if (!method.IsCopied() && method.GetDexMethodIndex() == dex_method_idx) {
+      return &method;
+    }
+  } else {
+    index = size;
+  }
+  if (kOnlyLookAtIndex) {
+    return nullptr;
+  }
+  // If there is a method, it's down the array. The array is ordered by method
+  // index, so we know we can stop the search if `dex_method_idx` is greater
+  // than the current method's index.
+  for (; index > 0; --index) {
+    ArtMethod& method = array->At(index - 1, kMethodSize, kMethodAlignment);
+    if (method.IsCopied()) {
+      continue;
+    } else if (method.GetDexMethodIndex() == dex_method_idx) {
+      return &method;
+    } else if (method.GetDexMethodIndex() < dex_method_idx) {
+      break;
     }
   }
   return nullptr;
@@ -1402,3 +1373,5 @@ ALWAYS_INLINE FLATTEN inline ArtField* Class::FindDeclaredField(uint32_t dex_fie
 }  // namespace art
 
 #endif  // ART_RUNTIME_MIRROR_CLASS_INL_H_
+
+
diff --git a/runtime/mirror/class-refvisitor-inl.h b/runtime/mirror/class-refvisitor-inl.h
index 8de27de485..e26b9075c2 100644
--- a/runtime/mirror/class-refvisitor-inl.h
+++ b/runtime/mirror/class-refvisitor-inl.h
@@ -80,9 +80,12 @@ void Class::VisitNativeRoots(Visitor& visitor, PointerSize pointer_size) {
           << GetStatus() << field->GetDeclaringClass()->PrettyClass() << " != " << PrettyClass();
     }
   });
-  // Don't use VisitMethods because we don't want to hit the class-ext methods twice.
-  for (ArtMethod& method : GetMethods(pointer_size)) {
-    method.VisitRoots<kReadBarrierOption, kVisitProxyMethod>(visitor, pointer_size);
+  // The method array may be null when the class isn't resolved yet.
+  if (GetMethodsPtr() != nullptr) {
+    // Don't use VisitMethods because we don't want to hit the class-ext methods twice.
+    for (ArtMethod& method : GetMethods(pointer_size)) {
+      method.VisitRoots<kReadBarrierOption, kVisitProxyMethod>(visitor, pointer_size);
+    }
   }
   ObjPtr<ClassExt> ext(GetExtData<kDefaultVerifyFlags, kReadBarrierOption>());
   if (!ext.IsNull()) {
diff --git a/runtime/mirror/class.cc b/runtime/mirror/class.cc
index 01f8fb3201..f32e047eef 100644
--- a/runtime/mirror/class.cc
+++ b/runtime/mirror/class.cc
@@ -417,14 +417,9 @@ void Class::DumpClass(std::ostream& os, int flags) {
   } else {
     os << "  vtable (" << NumVirtualMethods() << " entries, "
         << (super != nullptr ? super->NumVirtualMethods() : 0) << " in super):\n";
-    for (size_t i = 0; i < NumVirtualMethods(); ++i) {
-      os << StringPrintf("    %2zd: %s\n", i, ArtMethod::PrettyMethod(
-          GetVirtualMethodDuringLinking(i, image_pointer_size)).c_str());
-    }
-    os << "  direct methods (" << NumDirectMethods() << " entries):\n";
-    for (size_t i = 0; i < NumDirectMethods(); ++i) {
-      os << StringPrintf("    %2zd: %s\n", i, ArtMethod::PrettyMethod(
-          GetDirectMethod(i, image_pointer_size)).c_str());
+    size_t index = 0;
+    for (ArtMethod& method : GetDeclaredMethods(image_pointer_size)) {
+      os << StringPrintf("    %2zd: %s\n", index++, method.PrettyMethod().c_str());
     }
     if (NumFields() > 0) {
       os << "  fields (" << NumFields() << " entries):\n";
@@ -520,7 +515,7 @@ bool Class::IsThrowableClass() {
 }
 
 template <typename SignatureType>
-static inline ArtMethod* FindInterfaceMethodWithSignature(ObjPtr<Class> klass,
+inline ArtMethod* Class::FindInterfaceMethodWithSignature(ObjPtr<Class> klass,
                                                           std::string_view name,
                                                           const SignatureType& signature,
                                                           PointerSize pointer_size)
@@ -543,8 +538,10 @@ static inline ArtMethod* FindInterfaceMethodWithSignature(ObjPtr<Class> klass,
   ObjPtr<IfTable> iftable = klass->GetIfTable();
   for (int32_t i = 0, iftable_count = iftable->Count(); i < iftable_count; ++i) {
     ObjPtr<Class> iface = iftable->GetInterface(i);
-    for (ArtMethod& method : iface->GetVirtualMethodsSlice(pointer_size)) {
-      if (method.GetNameView() == name && method.GetSignature() == signature) {
+    for (ArtMethod& method : iface->GetMethodsSlice(pointer_size)) {
+      if (method.IsVirtual() &&
+          method.GetNameView() == name &&
+          method.GetSignature() == signature) {
         return &method;
       }
     }
@@ -579,7 +576,27 @@ ArtMethod* Class::FindInterfaceMethod(std::string_view name,
 ArtMethod* Class::FindInterfaceMethod(ObjPtr<DexCache> dex_cache,
                                       uint32_t dex_method_idx,
                                       PointerSize pointer_size) {
-  // We always search by name and signature, ignoring the type index in the MethodId.
+  // First try to find a declared method by dex_method_idx if we have a dex_cache match.
+  if (GetDexCache() == dex_cache) {
+    ArtMethod* method = nullptr;
+    if (pointer_size == kRuntimePointerSize) {
+      method = FindDeclaredClassMethod</* kOnlyLookAtIndex= */ false, kRuntimePointerSize>(
+          dex_method_idx);
+    } else {
+      constexpr PointerSize kOtherPointerSize =
+          (kRuntimePointerSize == PointerSize::k64) ? PointerSize::k32 : PointerSize::k64;
+      method = FindDeclaredClassMethod</* kOnlyLookAtIndex */ false, kOtherPointerSize>(
+          dex_method_idx);
+    }
+    if (method != nullptr) {
+      // This method is only called for interface classes, except from
+      // `ClassLinker::FindIncompatibleMethod` where we have not found one.
+      DCHECK(IsInterface());
+      return method;
+    }
+  }
+
+  // Otherwise search by name and signature, ignoring the type index in the MethodId.
   const DexFile& dex_file = *dex_cache->GetDexFile();
   const dex::MethodId& method_id = dex_file.GetMethodId(dex_method_idx);
   std::string_view name = dex_file.GetStringView(method_id.name_idx_);
@@ -780,11 +797,11 @@ std::tuple<bool, uint32_t> ClassMemberBinarySearch(uint32_t begin,
   return {success, mid};
 }
 
-static std::tuple<bool, ArtMethod*> FindDeclaredClassMethod(ObjPtr<mirror::Class> klass,
-                                                            const DexFile& dex_file,
-                                                            std::string_view name,
-                                                            Signature signature,
-                                                            PointerSize pointer_size)
+static std::tuple<bool, ArtMethod*> FindDeclaredClassMethodInternal(ObjPtr<mirror::Class> klass,
+                                                                    const DexFile& dex_file,
+                                                                    std::string_view name,
+                                                                    Signature signature,
+                                                                    PointerSize pointer_size)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   DCHECK(&klass->GetDexFile() == &dex_file);
   DCHECK(!name.empty());
@@ -815,23 +832,15 @@ static std::tuple<bool, ArtMethod*> FindDeclaredClassMethod(ObjPtr<mirror::Class
     return method_id.name_idx_;
   };
 
-  // Use binary search in the sorted direct methods, then in the sorted virtual methods.
-  uint32_t num_direct_methods = klass->NumDirectMethods();
+  // Use binary search in the sorted methods.
   uint32_t num_declared_methods = dchecked_integral_cast<uint32_t>(declared_methods.size());
-  DCHECK_LE(num_direct_methods, num_declared_methods);
-  const uint32_t ranges[2][2] = {
-     {0u, num_direct_methods},                   // Declared direct methods.
-     {num_direct_methods, num_declared_methods}  // Declared virtual methods.
-  };
-  for (const uint32_t (&range)[2] : ranges) {
-    auto [success, mid] =
-        ClassMemberBinarySearch(range[0], range[1], name_cmp, signature_cmp, get_name_idx);
-    if (success) {
-      return {true, &declared_methods[mid]};
-    }
+  auto [success, mid] =
+      ClassMemberBinarySearch(0, num_declared_methods, name_cmp, signature_cmp, get_name_idx);
+  if (success) {
+    return {true, &declared_methods[mid]};
   }
 
-  // Did not find a declared method in either slice.
+  // Did not find a declared method.
   return {false, nullptr};
 }
 
@@ -845,12 +854,18 @@ ArtMethod* Class::FindClassMethod(ObjPtr<DexCache> dex_cache,
   // First try to find a declared method by dex_method_idx if we have a dex_cache match.
   ObjPtr<DexCache> this_dex_cache = GetDexCache();
   if (this_dex_cache == dex_cache) {
-    // Lookup is always performed in the class referenced by the MethodId.
-    DCHECK_EQ(dex_type_idx_, GetDexFile().GetMethodId(dex_method_idx).class_idx_.index_);
-    for (ArtMethod& method : GetDeclaredMethodsSlice(pointer_size)) {
-      if (method.GetDexMethodIndex() == dex_method_idx) {
-        return &method;
-      }
+    ArtMethod* method = nullptr;
+    if (pointer_size == kRuntimePointerSize) {
+      method = FindDeclaredClassMethod</* kOnlyLookAtIndex= */ false, kRuntimePointerSize>(
+          dex_method_idx);
+    } else {
+      constexpr PointerSize kOtherPointerSize =
+          (kRuntimePointerSize == PointerSize::k64) ? PointerSize::k32 : PointerSize::k64;
+      method = FindDeclaredClassMethod</* kOnlyLookAtIndex */ false, kOtherPointerSize>(
+          dex_method_idx);
+    }
+    if (method != nullptr) {
+      return method;
     }
   }
 
@@ -864,7 +879,7 @@ ArtMethod* Class::FindClassMethod(ObjPtr<DexCache> dex_cache,
   if (this_dex_cache != dex_cache && !GetDeclaredMethodsSlice(pointer_size).empty()) {
     DCHECK(name.empty());
     name = dex_file.GetMethodNameView(method_id);
-    auto [success, method] = FindDeclaredClassMethod(
+    auto [success, method] = FindDeclaredClassMethodInternal(
         this, *this_dex_cache->GetDexFile(), name, signature, pointer_size);
     DCHECK_EQ(success, method != nullptr);
     if (success) {
@@ -896,7 +911,7 @@ ArtMethod* Class::FindClassMethod(ObjPtr<DexCache> dex_cache,
       if (name.empty()) {
         name = dex_file.GetMethodNameView(method_id);
       }
-      auto [success, method] = FindDeclaredClassMethod(
+      auto [success, method] = FindDeclaredClassMethodInternal(
           klass, *klass_dex_cache->GetDexFile(), name, signature, pointer_size);
       DCHECK_EQ(success, method != nullptr);
       if (success) {
@@ -936,9 +951,10 @@ ArtMethod* Class::FindClassMethod(ObjPtr<DexCache> dex_cache,
 ArtMethod* Class::FindConstructor(std::string_view signature, PointerSize pointer_size) {
   // Internal helper, never called on proxy classes. We can skip GetInterfaceMethodIfProxy().
   DCHECK(!IsProxyClass());
-  std::string_view name("<init>");
-  for (ArtMethod& method : GetDirectMethodsSliceUnchecked(pointer_size)) {
-    if (method.GetName() == name && method.GetSignature() == signature) {
+  for (ArtMethod& method : GetDeclaredMethodsSliceUnchecked(pointer_size)) {
+    DCHECK_IMPLIES(method.IsConstructor(), !method.IsVirtual());
+    if (method.IsInstanceConstructor() && method.GetSignature() == signature) {
+      DCHECK(method.GetName() == std::string_view("<init>"));
       return &method;
     }
   }
@@ -946,20 +962,24 @@ ArtMethod* Class::FindConstructor(std::string_view signature, PointerSize pointe
 }
 
 ArtMethod* Class::FindDeclaredDirectMethodByName(std::string_view name, PointerSize pointer_size) {
-  for (auto& method : GetDirectMethods(pointer_size)) {
-    ArtMethod* const np_method = method.GetInterfaceMethodIfProxy(pointer_size);
-    if (name == np_method->GetName()) {
-      return &method;
+  for (auto& method : GetDeclaredMethods(pointer_size)) {
+    if (!method.IsVirtual()) {
+      ArtMethod* const np_method = method.GetInterfaceMethodIfProxy(pointer_size);
+      if (name == np_method->GetName()) {
+        return &method;
+      }
     }
   }
   return nullptr;
 }
 
 ArtMethod* Class::FindDeclaredVirtualMethodByName(std::string_view name, PointerSize pointer_size) {
-  for (auto& method : GetVirtualMethods(pointer_size)) {
-    ArtMethod* const np_method = method.GetInterfaceMethodIfProxy(pointer_size);
-    if (name == np_method->GetName()) {
-      return &method;
+  for (auto& method : GetDeclaredMethods(pointer_size)) {
+    if (method.IsVirtual()) {
+      ArtMethod* const np_method = method.GetInterfaceMethodIfProxy(pointer_size);
+      if (name == np_method->GetName()) {
+        return &method;
+      }
     }
   }
   return nullptr;
@@ -971,8 +991,8 @@ ArtMethod* Class::FindVirtualMethodForInterfaceSuper(ArtMethod* method, PointerS
   // Check if we have one defined on this interface first. This includes searching copied ones to
   // get any conflict methods. Conflict methods are copied into each subtype from the supertype. We
   // don't do any indirect method checks here.
-  for (ArtMethod& iface_method : GetVirtualMethods(pointer_size)) {
-    if (method->HasSameNameAndSignature(&iface_method)) {
+  for (ArtMethod& iface_method : GetMethods(pointer_size)) {
+    if (iface_method.IsVirtual() && method->HasSameNameAndSignature(&iface_method)) {
       return &iface_method;
     }
   }
@@ -995,7 +1015,10 @@ ArtMethod* Class::FindVirtualMethodForInterfaceSuper(ArtMethod* method, PointerS
     iface.Assign(iftable->GetInterface(k));
     // Iterate through every declared method on this interface. Each direct method's name/signature
     // is unique so the order of the inner loop doesn't matter.
-    for (auto& method_iter : iface->GetDeclaredVirtualMethods(pointer_size)) {
+    for (auto& method_iter : iface->GetDeclaredMethods(pointer_size)) {
+      if (!method_iter.IsVirtual()) {
+        continue;
+      }
       ArtMethod* current_method = &method_iter;
       if (current_method->HasSameNameAndSignature(method)) {
         if (current_method->IsDefault()) {
@@ -1031,7 +1054,7 @@ ArtMethod* Class::FindVirtualMethodForInterfaceSuper(ArtMethod* method, PointerS
 }
 
 ArtMethod* Class::FindClassInitializer(PointerSize pointer_size) {
-  for (ArtMethod& method : GetDirectMethods(pointer_size)) {
+  for (ArtMethod& method : GetDeclaredMethods(pointer_size)) {
     if (method.IsClassInitializer()) {
       DCHECK_STREQ(method.GetName(), "<clinit>");
       DCHECK_STREQ(method.GetSignature().ToString().c_str(), "()V");
@@ -1559,6 +1582,7 @@ void Class::PopulateEmbeddedVTable(PointerSize pointer_size) {
   CHECK(table != nullptr) << PrettyClass();
   const size_t table_length = table->GetLength();
   SetEmbeddedVTableLength(table_length);
+  AddRemoveClassFlags(kClassFlagHasEmbeddedVTable);
   for (size_t i = 0; i < table_length; i++) {
     SetEmbeddedVTableEntry(i, table->GetElementPtrSize<ArtMethod*>(i, pointer_size), pointer_size);
   }
@@ -1674,6 +1698,9 @@ void Class::PopulateReferenceOffsetBitmap() {
       ref_offsets = -overflow_bitmap_word_idx | kVisitReferencesSlowpathMask;
     }
   }
+  if ((GetClassFlags() & ~kClassFlagStaticRefInfo) == 0 && ref_offsets != 0) {
+    AddRemoveClassFlags(kClassFlagNormal);
+  }
   SetReferenceInstanceOffsets(ref_offsets);
 }
 
@@ -1866,17 +1893,16 @@ uint32_t Class::UpdateHashForProxyClass(uint32_t hash, ObjPtr<mirror::Class> pro
 // TODO: Move this to java_lang_Class.cc?
 ArtMethod* Class::GetDeclaredConstructor(
     Thread* self, Handle<ObjectArray<Class>> args, PointerSize pointer_size) {
-  for (auto& m : GetDirectMethods(pointer_size)) {
-    // Skip <clinit> which is a static constructor, as well as non constructors.
-    if (m.IsStatic() || !m.IsConstructor()) {
-      continue;
-    }
-    // May cause thread suspension and exceptions.
-    if (m.GetInterfaceMethodIfProxy(kRuntimePointerSize)->EqualParameters(args)) {
-      return &m;
-    }
-    if (UNLIKELY(self->IsExceptionPending())) {
-      return nullptr;
+  for (auto& m : GetDeclaredMethods(pointer_size)) {
+    if (m.IsInstanceConstructor()) {
+      // May cause thread suspension and exceptions.
+      if (m.GetInterfaceMethodIfProxy(kRuntimePointerSize)->EqualParameters(args)) {
+        DCHECK(!self->IsExceptionPending());
+        return &m;
+      }
+      if (UNLIKELY(self->IsExceptionPending())) {
+        return nullptr;
+      }
     }
   }
   return nullptr;
@@ -1958,8 +1984,8 @@ ObjPtr<Method> Class::GetDeclaredMethodInternal(
   constexpr hiddenapi::AccessMethod access_method = hiddenapi::AccessMethod::kCheckWithPolicy;
   ArtMethod* result = nullptr;
   bool result_hidden = false;
-  for (auto& m : h_klass->GetDeclaredVirtualMethods(kPointerSize)) {
-    if (m.IsMiranda()) {
+  for (auto& m : h_klass->GetDeclaredMethods(kPointerSize)) {
+    if (!m.IsVirtual() || m.IsMiranda()) {
       continue;
     }
     ArtMethod* np_method = m.GetInterfaceMethodIfProxy(kPointerSize);
@@ -1991,9 +2017,8 @@ ObjPtr<Method> Class::GetDeclaredMethodInternal(
     DCHECK(!result->IsDirect());
     DCHECK(result->IsSynthetic());
   } else {
-    for (auto& m : h_klass->GetDirectMethods(kPointerSize)) {
-      auto modifiers = m.GetAccessFlags();
-      if ((modifiers & kAccConstructor) != 0) {
+    for (auto& m : h_klass->GetDeclaredMethods(kPointerSize)) {
+      if (m.IsVirtual() || m.IsConstructor()) {
         continue;
       }
       ArtMethod* np_method = m.GetInterfaceMethodIfProxy(kPointerSize);
@@ -2310,8 +2335,9 @@ ArtMethod* Class::FindAccessibleInterfaceMethod(ArtMethod* implementation_method
   if (IsInterface()) {  // Interface class doesn't resolve methods into the iftable.
     for (int32_t i = 0, iftable_count = iftable->Count(); i < iftable_count; ++i) {
       ObjPtr<mirror::Class> iface = iftable->GetInterface(i);
-      for (ArtMethod& interface_method : iface->GetVirtualMethodsSlice(pointer_size)) {
-        if (implementation_method->HasSameNameAndSignature(&interface_method) &&
+      for (ArtMethod& interface_method : iface->GetMethodsSlice(pointer_size)) {
+        if (interface_method.IsVirtual() &&
+            implementation_method->HasSameNameAndSignature(&interface_method) &&
             IsInterfaceMethodAccessible(&interface_method)) {
           return &interface_method;
         }
@@ -2323,12 +2349,13 @@ ArtMethod* Class::FindAccessibleInterfaceMethod(ArtMethod* implementation_method
       if (methods == nullptr) {
         continue;
       }
-      for (size_t j = 0, count = iftable->GetMethodArrayCount(i); j < count; ++j) {
-        if (implementation_method == methods->GetElementPtrSize<ArtMethod*>(j, pointer_size)) {
-          ObjPtr<mirror::Class> iface = iftable->GetInterface(i);
-          ArtMethod* interface_method = &iface->GetVirtualMethodsSlice(pointer_size)[j];
-          if (IsInterfaceMethodAccessible(interface_method)) {
-            return interface_method;
+      ObjPtr<mirror::Class> iface = iftable->GetInterface(i);
+      for (ArtMethod& m : iface->GetDeclaredMethods(pointer_size)) {
+        if (m.IsVirtual() &&
+            methods->GetElementPtrSize<ArtMethod*>(m.GetMethodIndex(), pointer_size)
+                == implementation_method) {
+          if (IsInterfaceMethodAccessible(&m)) {
+            return &m;
           }
         }
       }
@@ -2337,6 +2364,20 @@ ArtMethod* Class::FindAccessibleInterfaceMethod(ArtMethod* implementation_method
   return nullptr;
 }
 
+size_t Class::GetProxyThrowsIndex(ArtMethod* method) REQUIRES_SHARED(Locks::mutator_lock_) {
+  CHECK(IsProxyClass());
+  size_t i = 0;
+  for (const auto& m : GetDeclaredMethods(kRuntimePointerSize)) {
+    if (m.IsVirtual()) {
+      if (&m == method) {
+        return i;
+      }
+      ++i;
+    }
+  }
+  return static_cast<size_t>(-1);
+}
+
 
 }  // namespace mirror
 }  // namespace art
diff --git a/runtime/mirror/class.h b/runtime/mirror/class.h
index c8b234b789..bf42298428 100644
--- a/runtime/mirror/class.h
+++ b/runtime/mirror/class.h
@@ -86,7 +86,7 @@ class EXPORT MANAGED Class final : public Object {
   // 'reference_instance_offsets_' may contain up to 31 reference offsets. If
   // more bits are required, then we set the most-significant bit and store the
   // number of 32-bit bitmap entries required in the remaining bits. All the
-  // required bitmap entries after stored after static fields (at the end of the class).
+  // required bitmap entries are stored after static fields (at the end of the class).
   static constexpr uint32_t kVisitReferencesSlowpathShift = 31;
   static constexpr uint32_t kVisitReferencesSlowpathMask = 1u << kVisitReferencesSlowpathShift;
 
@@ -229,11 +229,18 @@ class EXPORT MANAGED Class final : public Object {
     return OFFSET_OF_OBJECT_MEMBER(Class, access_flags_);
   }
 
+  static constexpr MemberOffset ClassFlagsOffset() {
+    return OFFSET_OF_OBJECT_MEMBER(Class, class_flags_);
+  }
+
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   ALWAYS_INLINE uint32_t GetClassFlags() REQUIRES_SHARED(Locks::mutator_lock_) {
-    return GetField32<kVerifyFlags>(OFFSET_OF_OBJECT_MEMBER(Class, class_flags_));
+    return GetField32<kVerifyFlags>(ClassFlagsOffset());
   }
-  void SetClassFlags(uint32_t new_flags) REQUIRES_SHARED(Locks::mutator_lock_);
+
+  // Adds new_flags and clears clear_flags from the existing set of class_flags
+  void AddRemoveClassFlags(uint32_t new_flags, uint32_t clear_flags = 0)
+      REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Set access flags during linking, these cannot be rolled back by a Transaction.
   void SetAccessFlagsDuringLinking(uint32_t new_access_flags) REQUIRES_SHARED(Locks::mutator_lock_);
@@ -304,16 +311,16 @@ class EXPORT MANAGED Class final : public Object {
   }
 
   ALWAYS_INLINE void SetStringClass() REQUIRES_SHARED(Locks::mutator_lock_) {
-    SetClassFlags(kClassFlagString | kClassFlagNoReferenceFields);
+    AddRemoveClassFlags(kClassFlagString | kClassFlagNoReferenceFields);
   }
 
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   ALWAYS_INLINE bool IsClassLoaderClass() REQUIRES_SHARED(Locks::mutator_lock_) {
-    return GetClassFlags<kVerifyFlags>() == kClassFlagClassLoader;
+    return (GetClassFlags<kVerifyFlags>() & kClassFlagClassLoader) != 0;
   }
 
   ALWAYS_INLINE void SetClassLoaderClass() REQUIRES_SHARED(Locks::mutator_lock_) {
-    SetClassFlags(kClassFlagClassLoader);
+    AddRemoveClassFlags(kClassFlagClassLoader, kClassFlagNormal);
   }
 
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
@@ -322,7 +329,7 @@ class EXPORT MANAGED Class final : public Object {
   }
 
   ALWAYS_INLINE void SetDexCacheClass() REQUIRES_SHARED(Locks::mutator_lock_) {
-    SetClassFlags(GetClassFlags() | kClassFlagDexCache);
+    AddRemoveClassFlags(kClassFlagDexCache, kClassFlagNormal);
   }
 
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
@@ -331,7 +338,7 @@ class EXPORT MANAGED Class final : public Object {
   }
 
   ALWAYS_INLINE void SetRecordClass() REQUIRES_SHARED(Locks::mutator_lock_) {
-    SetClassFlags(GetClassFlags() | kClassFlagRecord);
+    AddRemoveClassFlags(kClassFlagRecord, kClassFlagNormal);
   }
 
   // Returns true if the class is abstract.
@@ -368,22 +375,22 @@ class EXPORT MANAGED Class final : public Object {
 
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   bool IsWeakReferenceClass() REQUIRES_SHARED(Locks::mutator_lock_) {
-    return GetClassFlags<kVerifyFlags>() == kClassFlagWeakReference;
+    return (GetClassFlags<kVerifyFlags>() & kClassFlagWeakReference) != 0;
   }
 
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   bool IsSoftReferenceClass() REQUIRES_SHARED(Locks::mutator_lock_) {
-    return GetClassFlags<kVerifyFlags>() == kClassFlagSoftReference;
+    return (GetClassFlags<kVerifyFlags>() & kClassFlagSoftReference) != 0;
   }
 
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   bool IsFinalizerReferenceClass() REQUIRES_SHARED(Locks::mutator_lock_) {
-    return GetClassFlags<kVerifyFlags>() == kClassFlagFinalizerReference;
+    return (GetClassFlags<kVerifyFlags>() & kClassFlagFinalizerReference) != 0;
   }
 
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   bool IsPhantomReferenceClass() REQUIRES_SHARED(Locks::mutator_lock_) {
-    return GetClassFlags<kVerifyFlags>() == kClassFlagPhantomReference;
+    return (GetClassFlags<kVerifyFlags>() & kClassFlagPhantomReference) != 0;
   }
 
   // Can references of this type be assigned to by things of another type? For non-array types
@@ -557,14 +564,18 @@ class EXPORT MANAGED Class final : public Object {
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   ALWAYS_INLINE bool IsVariableSize() REQUIRES_SHARED(Locks::mutator_lock_);
 
+  static constexpr MemberOffset ClassSizeOffset() {
+    return OFFSET_OF_OBJECT_MEMBER(Class, class_size_);
+  }
+
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   uint32_t SizeOf() REQUIRES_SHARED(Locks::mutator_lock_) {
-    return GetField32<kVerifyFlags>(OFFSET_OF_OBJECT_MEMBER(Class, class_size_));
+    return GetClassSize<kVerifyFlags>();
   }
 
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   uint32_t GetClassSize() REQUIRES_SHARED(Locks::mutator_lock_) {
-    return GetField32<kVerifyFlags>(OFFSET_OF_OBJECT_MEMBER(Class, class_size_));
+    return GetField32<kVerifyFlags>(ClassSizeOffset());
   }
 
   void SetClassSize(uint32_t new_class_size)
@@ -599,6 +610,11 @@ class EXPORT MANAGED Class final : public Object {
     return ComputeClassSize(false, 0, 0, 0, 0, 0, 0, 0, pointer_size);
   }
 
+  template <VerifyObjectFlags kVerifyFlags>
+  uint32_t GetObjectSizeUnchecked() REQUIRES_SHARED(Locks::mutator_lock_) {
+    return GetField32<kVerifyFlags>(ObjectSizeOffset());
+  }
+
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   uint32_t GetObjectSize() REQUIRES_SHARED(Locks::mutator_lock_);
   static constexpr MemberOffset ObjectSizeOffset() {
@@ -725,9 +741,6 @@ class EXPORT MANAGED Class final : public Object {
   // Also updates the dex_cache_strings_ variable from new_dex_cache.
   void SetDexCache(ObjPtr<DexCache> new_dex_cache) REQUIRES_SHARED(Locks::mutator_lock_);
 
-  ALWAYS_INLINE ArraySlice<ArtMethod> GetDirectMethods(PointerSize pointer_size)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
   ALWAYS_INLINE LengthPrefixedArray<ArtMethod>* GetMethodsPtr()
       REQUIRES_SHARED(Locks::mutator_lock_);
 
@@ -748,19 +761,6 @@ class EXPORT MANAGED Class final : public Object {
                               uint32_t num_virtual)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
-  ALWAYS_INLINE ArraySlice<ArtMethod> GetDirectMethodsSlice(PointerSize pointer_size)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
-  ALWAYS_INLINE ArtMethod* GetDirectMethod(size_t i, PointerSize pointer_size)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
-  // Use only when we are allocating populating the method arrays.
-  ALWAYS_INLINE ArtMethod* GetDirectMethodUnchecked(size_t i, PointerSize pointer_size)
-        REQUIRES_SHARED(Locks::mutator_lock_);
-  ALWAYS_INLINE ArtMethod* GetVirtualMethodUnchecked(size_t i, PointerSize pointer_size)
-        REQUIRES_SHARED(Locks::mutator_lock_);
-
   // Returns the number of static, private, and constructor methods.
   ALWAYS_INLINE uint32_t NumDirectMethods() REQUIRES_SHARED(Locks::mutator_lock_);
 
@@ -791,17 +791,6 @@ class EXPORT MANAGED Class final : public Object {
                                                             ObjPtr<ObjectArray<Class>> args)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
-  ALWAYS_INLINE ArraySlice<ArtMethod> GetDeclaredVirtualMethodsSlice(PointerSize pointer_size)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
-  ALWAYS_INLINE ArraySlice<ArtMethod> GetDeclaredVirtualMethods(
-        PointerSize pointer_size)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
-  // The index in the methods_ array where the first copied method is.
-  ALWAYS_INLINE uint32_t GetCopiedMethodsStartOffset() REQUIRES_SHARED(Locks::mutator_lock_);
-
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   ALWAYS_INLINE ArraySlice<ArtMethod> GetCopiedMethodsSlice(PointerSize pointer_size)
       REQUIRES_SHARED(Locks::mutator_lock_);
@@ -809,14 +798,6 @@ class EXPORT MANAGED Class final : public Object {
   ALWAYS_INLINE ArraySlice<ArtMethod> GetCopiedMethods(PointerSize pointer_size)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
-  ALWAYS_INLINE ArraySlice<ArtMethod> GetVirtualMethodsSlice(PointerSize pointer_size)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
-  ALWAYS_INLINE ArraySlice<ArtMethod> GetVirtualMethods(
-      PointerSize pointer_size)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
   // Returns the number of non-inherited virtual methods (sum of declared and copied methods).
   ALWAYS_INLINE uint32_t NumVirtualMethods() REQUIRES_SHARED(Locks::mutator_lock_);
 
@@ -826,17 +807,13 @@ class EXPORT MANAGED Class final : public Object {
   // Returns the number of declared virtual methods.
   ALWAYS_INLINE uint32_t NumDeclaredVirtualMethods() REQUIRES_SHARED(Locks::mutator_lock_);
 
+  // Returns the number of declared methods.
+  ALWAYS_INLINE uint32_t NumDeclaredMethods() REQUIRES_SHARED(Locks::mutator_lock_);
+
   ALWAYS_INLINE uint32_t NumMethods() REQUIRES_SHARED(Locks::mutator_lock_);
   static ALWAYS_INLINE uint32_t NumMethods(LengthPrefixedArray<ArtMethod>* methods)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
-  ArtMethod* GetVirtualMethod(size_t i, PointerSize pointer_size)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
-  ArtMethod* GetVirtualMethodDuringLinking(size_t i, PointerSize pointer_size)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags,
            ReadBarrierOption kReadBarrierOption = kWithReadBarrier>
   ALWAYS_INLINE ObjPtr<PointerArray> GetVTable() REQUIRES_SHARED(Locks::mutator_lock_);
@@ -1006,6 +983,10 @@ class EXPORT MANAGED Class final : public Object {
                              PointerSize pointer_size)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
+  template <bool kOnlyLookAtIndex, PointerSize kPointerSize>
+  ArtMethod* FindDeclaredClassMethod(uint32_t dex_method_idx)
+      REQUIRES_SHARED(Locks::mutator_lock_);
+
   ArtMethod* FindConstructor(std::string_view signature, PointerSize pointer_size)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
@@ -1037,6 +1018,7 @@ class EXPORT MANAGED Class final : public Object {
 
   // Get fields of the class.
   LengthPrefixedArray<ArtField>* GetFieldsPtr() REQUIRES_SHARED(Locks::mutator_lock_);
+  LengthPrefixedArray<ArtField>* GetFieldsPtrUnchecked() REQUIRES_SHARED(Locks::mutator_lock_);
 
   ALWAYS_INLINE IterationRange<StrideIterator<ArtField>> GetFields()
       REQUIRES_SHARED(Locks::mutator_lock_);
@@ -1072,6 +1054,15 @@ class EXPORT MANAGED Class final : public Object {
     SetField32<false>(OFFSET_OF_OBJECT_MEMBER(Class, num_reference_instance_fields_), new_num);
   }
 
+  static constexpr MemberOffset ReferenceInstanceOffsetsOffset() {
+    return OFFSET_OF_OBJECT_MEMBER(Class, reference_instance_offsets_);
+  }
+
+  template <VerifyObjectFlags kVerifyFlags>
+  uint32_t GetReferenceInstanceOffsetsUnchecked() REQUIRES_SHARED(Locks::mutator_lock_) {
+    return GetField32<kVerifyFlags>(OFFSET_OF_OBJECT_MEMBER(Class, reference_instance_offsets_));
+  }
+
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   uint32_t GetReferenceInstanceOffsets() ALWAYS_INLINE REQUIRES_SHARED(Locks::mutator_lock_);
 
@@ -1084,21 +1075,33 @@ class EXPORT MANAGED Class final : public Object {
   MemberOffset GetFirstReferenceInstanceFieldOffset()
       REQUIRES_SHARED(Locks::mutator_lock_);
 
+  static constexpr MemberOffset NumReferenceStaticFieldsOffset() {
+    return OFFSET_OF_OBJECT_MEMBER(Class, num_reference_static_fields_);
+  }
+
+  template <VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
+  uint32_t NumReferenceStaticFieldsUnchecked() REQUIRES_SHARED(Locks::mutator_lock_) {
+    return GetField32<kVerifyFlags>(NumReferenceStaticFieldsOffset());
+  }
+
   // Returns the number of static fields containing reference types.
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   uint32_t NumReferenceStaticFields() REQUIRES_SHARED(Locks::mutator_lock_) {
     DCHECK(IsResolved<kVerifyFlags>());
-    return GetField32<kVerifyFlags>(OFFSET_OF_OBJECT_MEMBER(Class, num_reference_static_fields_));
+    return NumReferenceStaticFieldsUnchecked<kVerifyFlags>();
   }
 
   uint32_t NumReferenceStaticFieldsDuringLinking() REQUIRES_SHARED(Locks::mutator_lock_) {
     DCHECK(IsLoaded() || IsErroneous() || IsRetired());
-    return GetField32(OFFSET_OF_OBJECT_MEMBER(Class, num_reference_static_fields_));
+    return GetField32(NumReferenceStaticFieldsOffset());
   }
 
   void SetNumReferenceStaticFields(uint32_t new_num) REQUIRES_SHARED(Locks::mutator_lock_) {
+    if (new_num > 0) {
+      AddRemoveClassFlags(kClassFlagHasStaticRefs);
+    }
     // Not called within a transaction.
-    SetField32<false>(OFFSET_OF_OBJECT_MEMBER(Class, num_reference_static_fields_), new_num);
+    SetField32<false>(NumReferenceStaticFieldsOffset(), new_num);
   }
 
   // Get the offset of the first reference static field. Other reference static fields follow.
@@ -1126,6 +1129,7 @@ class EXPORT MANAGED Class final : public Object {
   ArtField* FindDeclaredField(ObjPtr<DexCache> dex_cache, uint32_t dex_field_idx)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
+  template <bool kOnlyLookAtIndex = false>
   ArtField* FindDeclaredField(uint32_t dex_field_idx)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
@@ -1331,19 +1335,12 @@ class EXPORT MANAGED Class final : public Object {
     return static_cast<size_t>(pointer_size);
   }
 
-  ALWAYS_INLINE ArraySlice<ArtMethod> GetDirectMethodsSliceUnchecked(PointerSize pointer_size)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
-  ALWAYS_INLINE ArraySlice<ArtMethod> GetVirtualMethodsSliceUnchecked(PointerSize pointer_size)
+  ALWAYS_INLINE ArraySlice<ArtMethod> GetMethodsSliceUnchecked(PointerSize pointer_size)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
   ALWAYS_INLINE ArraySlice<ArtMethod> GetDeclaredMethodsSliceUnchecked(PointerSize pointer_size)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  ALWAYS_INLINE ArraySlice<ArtMethod> GetDeclaredVirtualMethodsSliceUnchecked(
-      PointerSize pointer_size)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
   ALWAYS_INLINE ArraySlice<ArtMethod> GetCopiedMethodsSliceUnchecked(PointerSize pointer_size)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
@@ -1398,7 +1395,16 @@ class EXPORT MANAGED Class final : public Object {
   // See b/259501764.
   bool CheckIsVisibleWithTargetSdk(Thread* self) REQUIRES_SHARED(Locks::mutator_lock_);
 
+  size_t GetProxyThrowsIndex(ArtMethod* method) REQUIRES_SHARED(Locks::mutator_lock_);
+
  private:
+  template <typename SignatureType>
+  static ArtMethod* FindInterfaceMethodWithSignature(ObjPtr<Class> klass,
+                                                     std::string_view name,
+                                                     const SignatureType& signature,
+                                                     PointerSize pointer_size)
+    REQUIRES_SHARED(Locks::mutator_lock_);
+
   template <typename T, VerifyObjectFlags kVerifyFlags, typename Visitor>
   void FixupNativePointer(
       Class* dest, PointerSize pointer_size, const Visitor& visitor, MemberOffset member_offset)
@@ -1423,17 +1429,9 @@ class EXPORT MANAGED Class final : public Object {
 
   void CheckObjectAlloc() REQUIRES_SHARED(Locks::mutator_lock_);
 
-  // Unchecked editions is for root visiting.
-  LengthPrefixedArray<ArtField>* GetFieldsPtrUnchecked() REQUIRES_SHARED(Locks::mutator_lock_);
   IterationRange<StrideIterator<ArtField>> GetFieldsUnchecked()
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  // The index in the methods_ array where the first declared virtual method is.
-  ALWAYS_INLINE uint32_t GetVirtualMethodsStartOffset() REQUIRES_SHARED(Locks::mutator_lock_);
-
-  // The index in the methods_ array where the first direct method is.
-  ALWAYS_INLINE uint32_t GetDirectMethodsStartOffset() REQUIRES_SHARED(Locks::mutator_lock_);
-
   bool ProxyDescriptorEquals(ObjPtr<mirror::Class> match) REQUIRES_SHARED(Locks::mutator_lock_);
   bool ProxyDescriptorEquals(std::string_view match) REQUIRES_SHARED(Locks::mutator_lock_);
   static uint32_t UpdateHashForProxyClass(uint32_t hash, ObjPtr<mirror::Class> proxy_class)
@@ -1540,7 +1538,8 @@ class EXPORT MANAGED Class final : public Object {
   // Access flags; low 16 bits are defined by VM spec.
   uint32_t access_flags_;
 
-  // Class flags to help speed up visiting object references.
+  // Class flags to help speed up visiting object references. See class_flags.h
+  // for various possible flags.
   uint32_t class_flags_;
 
   // Total size of the Class instance; used when allocating storage on gc heap.
@@ -1586,14 +1585,6 @@ class EXPORT MANAGED Class final : public Object {
   // typeof(status_) is actually SubtypeCheckBitsAndStatus.
   uint32_t status_;
 
-  // The offset of the first virtual method that is copied from an interface. This includes miranda,
-  // default, and default-conflict methods. Having a hard limit of ((2 << 16) - 1) for methods
-  // defined on a single class is well established in Java so we will use only uint16_t's here.
-  uint16_t copied_methods_offset_;
-
-  // The offset of the first declared virtual methods in the methods_ array.
-  uint16_t virtual_methods_offset_;
-
   // The following data exist in real class objects.
   // Embedded Vtable length, for class object that's instantiable, fixed size.
   // uint32_t vtable_length_;
diff --git a/runtime/mirror/class_flags.h b/runtime/mirror/class_flags.h
index b3d38b0ba8..0171ae08bc 100644
--- a/runtime/mirror/class_flags.h
+++ b/runtime/mirror/class_flags.h
@@ -24,12 +24,16 @@
 namespace art HIDDEN {
 namespace mirror {
 
+// Reserved for CMC MOVE ioctl implementation. We don't want to have 0x0 as
+// class-flags for any object.
+static constexpr uint32_t kClassFlagReservedForCMC = 0x00000000;
+
 // Normal instance with at least one ref field other than the class.
-static constexpr uint32_t kClassFlagNormal             = 0x00000000;
+static constexpr uint32_t kClassFlagNormal = 0x00000001;
 
 // Only normal objects which have no reference fields, e.g. string or primitive array or normal
 // class instance with no fields other than klass.
-static constexpr uint32_t kClassFlagNoReferenceFields  = 0x00000001;
+static constexpr uint32_t kClassFlagNoReferenceFields = 0x00000002;
 
 // Class is java.lang.String.class.
 static constexpr uint32_t kClassFlagString             = 0x00000004;
@@ -64,6 +68,12 @@ static constexpr uint32_t kClassFlagRecord             = 0x00000800;
 // Class is a primitive array class.
 static constexpr uint32_t kClassFlagPrimitiveArray = 0x00001000;
 
+// Class has non-zero static references. Needed for CMC GC compaction phase.
+static constexpr uint32_t kClassFlagHasStaticRefs = 0x00002000;
+
+// Class has embedded vtable, which maybe 0 length. Needed for CMC GC compaction phase.
+static constexpr uint32_t kClassFlagHasEmbeddedVTable = 0x00004000;
+
 // NOTE: The most significant 2 bits are used to store the component size shift
 // for arrays (both primitive and object). See Primitive::ComponentSizeShift()
 // for size shift of different types.
@@ -77,6 +87,9 @@ static constexpr uint32_t kClassFlagReference =
     kClassFlagFinalizerReference |
     kClassFlagPhantomReference;
 
+static constexpr uint32_t kClassFlagStaticRefInfo =
+    kClassFlagHasStaticRefs | kClassFlagHasEmbeddedVTable;
+
 }  // namespace mirror
 }  // namespace art
 
diff --git a/runtime/mirror/field-inl.h b/runtime/mirror/field-inl.h
index d47e8c66c7..2943932257 100644
--- a/runtime/mirror/field-inl.h
+++ b/runtime/mirror/field-inl.h
@@ -23,6 +23,9 @@
 #include "class-alloc-inl.h"
 #include "class_root-inl.h"
 #include "object-inl.h"
+#include "base/sdk_version.h"
+#include "runtime.h"
+#include "well_known_classes.h"
 
 namespace art HIDDEN {
 
@@ -32,6 +35,50 @@ inline ObjPtr<mirror::Class> Field::GetDeclaringClass() REQUIRES_SHARED(Locks::m
   return GetFieldObject<Class>(OFFSET_OF_OBJECT_MEMBER(Field, declaring_class_));
 }
 
+inline bool Field::IsMonotonic() REQUIRES_SHARED(Locks::mutator_lock_) {
+  if (!IsFinal()) {
+    return false;
+  }
+
+  ObjPtr<mirror::Class> declaring_class = GetDeclaringClass();
+  DCHECK(declaring_class != nullptr);
+
+  if (declaring_class->IsRecordClass()) {
+    return true;
+  }
+
+  // Write-protected fields are `static final`, but can be modified nevertheless.
+  if (IsWriteProtected()) {
+    return false;
+  }
+
+  // Before and on Android B any field could be overwritten using reflection with final fields in
+  // record classes being the only exception. For compatibility purposes allow apps targeting B
+  // or an older release to overwrite such fields.
+  uint32_t target_sdk_version = Runtime::Current()->GetTargetSdkVersion();
+  if (IsSdkVersionSetAndAtMost(target_sdk_version, SdkVersion::kB)) {
+    return false;
+  }
+
+  // Make sure that OEMs code in bootclasspath won't be affected after ART module update.
+  uint32_t sdk_version = Runtime::Current()->GetSdkVersion();
+  if (IsSdkVersionSetAndAtMost(sdk_version, SdkVersion::kB)) {
+    return false;
+  }
+
+  return IsStatic() && IsFinal();
+}
+
+inline bool Field::IsWriteProtected() {
+  ArtField* art_field = GetArtField();
+  if (art_field == WellKnownClasses::java_lang_System_in ||
+      art_field == WellKnownClasses::java_lang_System_out ||
+      art_field == WellKnownClasses::java_lang_System_err) {
+    return true;
+  }
+  return false;
+}
+
 inline Primitive::Type Field::GetTypeAsPrimitiveType() {
   return GetType()->GetPrimitiveType();
 }
diff --git a/runtime/mirror/field.h b/runtime/mirror/field.h
index db7ec82861..cd23f60b9e 100644
--- a/runtime/mirror/field.h
+++ b/runtime/mirror/field.h
@@ -64,6 +64,13 @@ class MANAGED Field : public AccessibleObject {
     return (GetAccessFlags() & kAccVolatile) != 0;
   }
 
+  // Returns true if this field's value can change only once.
+  bool IsMonotonic() REQUIRES_SHARED(Locks::mutator_lock_);
+
+  // Write-protected are static final fields whose value can be changed. There are only 3 of them.
+  // See https://docs.oracle.com/javase/specs/jls/se24/html/jls-17.html#jls-17.5.4.
+  bool IsWriteProtected() REQUIRES_SHARED(Locks::mutator_lock_);
+
   ALWAYS_INLINE Primitive::Type GetTypeAsPrimitiveType() REQUIRES_SHARED(Locks::mutator_lock_);
 
   ObjPtr<mirror::Class> GetType() REQUIRES_SHARED(Locks::mutator_lock_);
diff --git a/runtime/mirror/method_handle_impl.h b/runtime/mirror/method_handle_impl.h
index a4a6e67a5a..a180236c1b 100644
--- a/runtime/mirror/method_handle_impl.h
+++ b/runtime/mirror/method_handle_impl.h
@@ -135,13 +135,13 @@ class MANAGED MethodHandleImpl : public MethodHandle {
       REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(!Roles::uninterruptible_);
 
   static MemberOffset TargetOffset() {
-    return MemberOffset(OFFSETOF_MEMBER(MethodHandleImpl, target_));
+    return MemberOffset(OFFSETOF_MEMBER(MethodHandleImpl, target_method_entry_));
   }
 
  private:
   HeapReference<mirror::Field> field_;
   HeapReference<mirror::Object> target_class_or_info_;  // Unused by the runtime.
-  uint64_t target_;
+  uint64_t target_method_entry_;
 
   friend struct art::MethodHandleImplOffsets;  // for verifying offset information
   DISALLOW_IMPLICIT_CONSTRUCTORS(MethodHandleImpl);
diff --git a/runtime/mirror/object-refvisitor-inl.h b/runtime/mirror/object-refvisitor-inl.h
index 140ef9f93b..ef209190e7 100644
--- a/runtime/mirror/object-refvisitor-inl.h
+++ b/runtime/mirror/object-refvisitor-inl.h
@@ -70,20 +70,23 @@ inline void Object::VisitReferences(const Visitor& visitor,
   visitor(this, ClassOffset(), /* is_static= */ false);
   ObjPtr<Class> klass = GetClass<kVerifyFlags, kReadBarrierOption>();
   const uint32_t class_flags = klass->GetClassFlags<kVerifyNone>();
-  if (LIKELY(class_flags == kClassFlagNormal) || class_flags == kClassFlagRecord) {
-    CheckNormalClass<kVerifyFlags>(klass);
-    DCHECK(klass->IsInstantiableNonArray()) << klass->PrettyDescriptor();
-    VisitInstanceFieldsReferences<kVerifyFlags, kReadBarrierOption>(klass, visitor);
+  DCHECK_NE(class_flags & (kClassFlagNormal | kClassFlagNoReferenceFields),
+            kClassFlagNormal | kClassFlagNoReferenceFields);
+  if ((class_flags & kClassFlagNoReferenceFields) != 0) {
+    CheckNoReferenceField<kVerifyFlags, kReadBarrierOption>(klass);
     return;
   }
+  DCHECK(!klass->IsStringClass<kVerifyFlags>());
 
-  if ((class_flags & kClassFlagNoReferenceFields) != 0) {
-    CheckNoReferenceField<kVerifyFlags, kReadBarrierOption>(klass);
+  // Record with no references will return from previous if block.
+  if ((class_flags & (kClassFlagNormal | kClassFlagRecord)) != 0) {
+    CheckNormalClass<kVerifyFlags>(klass);
+    DCHECK(klass->IsInstantiableNonArray()) << klass->PrettyDescriptor();
+    VisitInstanceFieldsReferences<kVerifyFlags, kReadBarrierOption>(klass, visitor);
     return;
   }
 
-  DCHECK(!klass->IsStringClass<kVerifyFlags>());
-  if (class_flags == kClassFlagClass) {
+  if ((class_flags & kClassFlagClass) != 0) {
     DCHECK(klass->IsClassClass<kVerifyFlags>());
     DCHECK(klass->IsInstantiableNonArray()) << klass->PrettyDescriptor();
     ObjPtr<Class> as_klass = AsClass<kVerifyNone>();
@@ -104,7 +107,7 @@ inline void Object::VisitReferences(const Visitor& visitor,
     return;
   }
 
-  if (class_flags == kClassFlagDexCache) {
+  if ((class_flags & kClassFlagDexCache) != 0) {
     DCHECK(klass->IsInstantiableNonArray()) << klass->PrettyDescriptor();
     DCHECK(klass->IsDexCacheClass<kVerifyFlags>());
     ObjPtr<mirror::DexCache> const dex_cache = AsDexCache<kVerifyFlags, kReadBarrierOption>();
@@ -114,7 +117,7 @@ inline void Object::VisitReferences(const Visitor& visitor,
     return;
   }
 
-  if (class_flags == kClassFlagClassLoader) {
+  if ((class_flags & kClassFlagClassLoader) != 0) {
     DCHECK(klass->IsInstantiableNonArray()) << klass->PrettyDescriptor();
     DCHECK(klass->IsClassLoaderClass<kVerifyFlags>());
     ObjPtr<mirror::ClassLoader> const class_loader =
@@ -129,79 +132,6 @@ inline void Object::VisitReferences(const Visitor& visitor,
             << " for " << klass->PrettyClass();
 }
 
-// Could be called with from-space address of the object as we access klass and
-// length (in case of arrays/strings) and we don't want to cause cascading faults.
-template <bool kFetchObjSize,
-          bool kVisitNativeRoots,
-          VerifyObjectFlags kVerifyFlags,
-          ReadBarrierOption kReadBarrierOption,
-          typename Visitor>
-inline size_t Object::VisitRefsForCompaction(const Visitor& visitor,
-                                             MemberOffset begin,
-                                             MemberOffset end) {
-  constexpr VerifyObjectFlags kSizeOfFlags = RemoveThisFlags(kVerifyFlags);
-  size_t size;
-  // We want to continue using pre-compact klass to avoid cascading faults.
-  ObjPtr<Class> klass = GetClass<kVerifyFlags, kReadBarrierOption>();
-  DCHECK(klass != nullptr) << "obj=" << this;
-  const uint32_t class_flags = klass->GetClassFlags<kVerifyNone>();
-  if (LIKELY(class_flags == kClassFlagNormal) || class_flags == kClassFlagRecord) {
-    CheckNormalClass<kVerifyFlags>(klass);
-    VisitInstanceFieldsReferences<kVerifyFlags, kReadBarrierOption>(klass, visitor);
-    size = kFetchObjSize ? klass->GetObjectSize<kSizeOfFlags>() : 0;
-  } else if ((class_flags & kClassFlagNoReferenceFields) != 0) {
-    if ((class_flags & kClassFlagString) != 0) {
-      size = kFetchObjSize ? static_cast<String*>(this)->SizeOf<kSizeOfFlags>() : 0;
-    } else if ((class_flags & kClassFlagPrimitiveArray) != 0) {
-      ObjPtr<Array> arr = ObjPtr<Array>::DownCast(this);
-      size = kFetchObjSize ?
-                 arr->SizeOf<kSizeOfFlags>(class_flags >> kArrayComponentSizeShiftShift) :
-                 0;
-    } else {
-      // Only possibility left is of a normal klass instance with no references.
-      size = kFetchObjSize ? klass->GetObjectSize<kSizeOfFlags>() : 0;
-    }
-  } else if (class_flags == kClassFlagClass) {
-    DCHECK(klass->IsClassClass<kVerifyFlags>());
-    ObjPtr<Class> as_klass = ObjPtr<Class>::DownCast(this);
-    as_klass->VisitReferences<kVisitNativeRoots, kVerifyFlags, kReadBarrierOption>(klass,
-                                                                                   visitor);
-    size = kFetchObjSize ? as_klass->SizeOf<kSizeOfFlags>() : 0;
-  } else if ((class_flags & kClassFlagObjectArray) != 0) {
-    ObjPtr<ObjectArray<Object>> obj_arr = ObjPtr<ObjectArray<Object>>::DownCast(this);
-    obj_arr->VisitReferences(visitor, begin, end);
-    size = kFetchObjSize ?
-               obj_arr->SizeOf<kSizeOfFlags>(class_flags >> kArrayComponentSizeShiftShift) :
-               0;
-  } else if ((class_flags & kClassFlagReference) != 0) {
-    VisitInstanceFieldsReferences<kVerifyFlags, kReadBarrierOption>(klass, visitor);
-    // Visit referent also as this is about updating the reference only.
-    // There is no reference processing happening here.
-    visitor(this, mirror::Reference::ReferentOffset(), /* is_static= */ false);
-    size = kFetchObjSize ? klass->GetObjectSize<kSizeOfFlags>() : 0;
-  } else if (class_flags == kClassFlagDexCache) {
-    DCHECK(klass->IsDexCacheClass<kVerifyFlags>());
-    ObjPtr<DexCache> const dex_cache = ObjPtr<DexCache>::DownCast(this);
-    dex_cache->VisitReferences<kVisitNativeRoots,
-                               kVerifyFlags,
-                               kReadBarrierOption>(klass, visitor);
-    size = kFetchObjSize ? klass->GetObjectSize<kSizeOfFlags>() : 0;
-  } else if (class_flags == kClassFlagClassLoader) {
-    DCHECK(klass->IsClassLoaderClass<kVerifyFlags>());
-    ObjPtr<ClassLoader> const class_loader = ObjPtr<ClassLoader>::DownCast(this);
-    class_loader->VisitReferences<kVisitNativeRoots,
-                                  kVerifyFlags,
-                                  kReadBarrierOption>(klass, visitor);
-    size = kFetchObjSize ? klass->GetObjectSize<kSizeOfFlags>() : 0;
-  } else {
-    LOG(FATAL) << "Unexpected class flags: " << std::hex << class_flags
-               << " for " << klass->PrettyClass();
-    size = -1;
-  }
-  visitor(this, ClassOffset(), /* is_static= */ false);
-  return size;
-}
-
 }  // namespace mirror
 }  // namespace art
 
diff --git a/runtime/mirror/object.h b/runtime/mirror/object.h
index 0fd9003e33..99d3c52a03 100644
--- a/runtime/mirror/object.h
+++ b/runtime/mirror/object.h
@@ -660,19 +660,8 @@ class EXPORT MANAGED LOCKABLE Object {
             ReadBarrierOption kReadBarrierOption = kWithReadBarrier,
             typename Visitor,
             typename JavaLangRefVisitor = VoidFunctor>
-  void VisitReferences(const Visitor& visitor, const JavaLangRefVisitor& ref_visitor)
-      NO_THREAD_SAFETY_ANALYSIS;
-  // VisitReferences version for compaction. It is invoked with from-space
-  // object so that portions of the object, like klass and length (for arrays),
-  // can be accessed without causing cascading faults.
-  template <bool kFetchObjSize = true,
-            bool kVisitNativeRoots = false,
-            VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags,
-            ReadBarrierOption kReadBarrierOption = kWithFromSpaceBarrier,
-            typename Visitor>
-  size_t VisitRefsForCompaction(const Visitor& visitor,
-                                MemberOffset begin,
-                                MemberOffset end) NO_THREAD_SAFETY_ANALYSIS;
+  void VisitReferences(const Visitor& visitor,
+                       const JavaLangRefVisitor& ref_visitor) NO_THREAD_SAFETY_ANALYSIS;
 
   ArtField* FindFieldByOffset(MemberOffset offset) REQUIRES_SHARED(Locks::mutator_lock_);
 
@@ -700,6 +689,12 @@ class EXPORT MANAGED LOCKABLE Object {
                                 size_t num_bytes)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
+  template <VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags,
+            ReadBarrierOption kReadBarrierOption = kWithReadBarrier,
+            typename Visitor>
+  void VisitInstanceFieldsReferences(ObjPtr<mirror::Class> klass, const Visitor& visitor) HOT_ATTR
+      REQUIRES_SHARED(Locks::mutator_lock_);
+
  protected:
   // Accessors for non-Java type fields
   template<class T, VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags, bool kIsVolatile = false>
@@ -725,12 +720,6 @@ class EXPORT MANAGED LOCKABLE Object {
     }
   }
 
-  template <VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags,
-            ReadBarrierOption kReadBarrierOption = kWithReadBarrier,
-            typename Visitor>
-  void VisitInstanceFieldsReferences(ObjPtr<mirror::Class> klass, const Visitor& visitor) HOT_ATTR
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
  private:
   template <bool kAllowInflation>
   int32_t IdentityHashCodeHelper() REQUIRES_SHARED(Locks::mutator_lock_)
diff --git a/runtime/mirror/object_test.cc b/runtime/mirror/object_test.cc
index 579ccdce01..c2c53e738f 100644
--- a/runtime/mirror/object_test.cc
+++ b/runtime/mirror/object_test.cc
@@ -545,23 +545,23 @@ TEST_F(ObjectTest, DescriptorCompare) {
   Handle<Class> klass2 = hs.NewHandle(FindClass("LProtoCompare2;", class_loader_2));
   ASSERT_TRUE(klass2 != nullptr);
 
-  ArtMethod* m1_1 = klass1->GetVirtualMethod(0, kRuntimePointerSize);
-  EXPECT_STREQ(m1_1->GetName(), "m1");
-  ArtMethod* m2_1 = klass1->GetVirtualMethod(1, kRuntimePointerSize);
-  EXPECT_STREQ(m2_1->GetName(), "m2");
-  ArtMethod* m3_1 = klass1->GetVirtualMethod(2, kRuntimePointerSize);
-  EXPECT_STREQ(m3_1->GetName(), "m3");
-  ArtMethod* m4_1 = klass1->GetVirtualMethod(3, kRuntimePointerSize);
-  EXPECT_STREQ(m4_1->GetName(), "m4");
-
-  ArtMethod* m1_2 = klass2->GetVirtualMethod(0, kRuntimePointerSize);
-  EXPECT_STREQ(m1_2->GetName(), "m1");
-  ArtMethod* m2_2 = klass2->GetVirtualMethod(1, kRuntimePointerSize);
-  EXPECT_STREQ(m2_2->GetName(), "m2");
-  ArtMethod* m3_2 = klass2->GetVirtualMethod(2, kRuntimePointerSize);
-  EXPECT_STREQ(m3_2->GetName(), "m3");
-  ArtMethod* m4_2 = klass2->GetVirtualMethod(3, kRuntimePointerSize);
-  EXPECT_STREQ(m4_2->GetName(), "m4");
+  ArtMethod& m1_1 = klass1->GetMethods(kRuntimePointerSize)[1];
+  EXPECT_STREQ(m1_1.GetName(), "m1");
+  ArtMethod& m2_1 = klass1->GetMethods(kRuntimePointerSize)[2];
+  EXPECT_STREQ(m2_1.GetName(), "m2");
+  ArtMethod& m3_1 = klass1->GetMethods(kRuntimePointerSize)[3];
+  EXPECT_STREQ(m3_1.GetName(), "m3");
+  ArtMethod& m4_1 = klass1->GetMethods(kRuntimePointerSize)[4];
+  EXPECT_STREQ(m4_1.GetName(), "m4");
+
+  ArtMethod& m1_2 = klass2->GetMethods(kRuntimePointerSize)[1];
+  EXPECT_STREQ(m1_2.GetName(), "m1");
+  ArtMethod& m2_2 = klass2->GetMethods(kRuntimePointerSize)[2];
+  EXPECT_STREQ(m2_2.GetName(), "m2");
+  ArtMethod& m3_2 = klass2->GetMethods(kRuntimePointerSize)[3];
+  EXPECT_STREQ(m3_2.GetName(), "m3");
+  ArtMethod& m4_2 = klass2->GetMethods(kRuntimePointerSize)[4];
+  EXPECT_STREQ(m4_2.GetName(), "m4");
 }
 
 TEST_F(ObjectTest, StringHashCode) {
diff --git a/runtime/mirror/string.h b/runtime/mirror/string.h
index 62890f58fb..60b5896e8b 100644
--- a/runtime/mirror/string.h
+++ b/runtime/mirror/string.h
@@ -74,13 +74,13 @@ class MANAGED String final : public Object {
     return &value_compressed_[0];
   }
 
-  template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
-  size_t SizeOf() REQUIRES_SHARED(Locks::mutator_lock_) {
+  static size_t SizeOf(int32_t count) {
     size_t size = sizeof(String);
-    if (IsCompressed()) {
-      size += (sizeof(uint8_t) * GetLength<kVerifyFlags>());
+    int32_t len = GetLengthFromCount(count);
+    if (kUseStringCompression && IsCompressed(count)) {
+      size += sizeof(uint8_t) * len;
     } else {
-      size += (sizeof(uint16_t) * GetLength<kVerifyFlags>());
+      size += sizeof(uint16_t) * len;
     }
     // String.equals() intrinsics assume zero-padding up to kObjectAlignment,
     // so make sure the zero-padding is actually copied around if GC compaction
@@ -89,6 +89,11 @@ class MANAGED String final : public Object {
     return RoundUp(size, kObjectAlignment);
   }
 
+  template <VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
+  size_t SizeOf() REQUIRES_SHARED(Locks::mutator_lock_) {
+    return SizeOf(GetCount<kVerifyFlags>());
+  }
+
   // Taking out the first/uppermost bit because it is not part of actual length value
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
   int32_t GetLength() REQUIRES_SHARED(Locks::mutator_lock_) {
diff --git a/runtime/monitor.cc b/runtime/monitor.cc
index 7aba2a4504..cd9beefc85 100644
--- a/runtime/monitor.cc
+++ b/runtime/monitor.cc
@@ -1112,11 +1112,18 @@ ObjPtr<mirror::Object> Monitor::MonitorEnter(Thread* self,
   obj = FakeLock(obj);
   uint32_t thread_id = self->GetThreadId();
   size_t contention_count = 0;
+  // Initial pure spin iterations before the GetMaxSpinsBeforeThinLockInflation() calls to
+  // sched_yield().
   constexpr size_t kExtraSpinIters = 100;
   int inflation_attempt = 1;
   StackHandleScope<1> hs(self);
   Handle<mirror::Object> h_obj(hs.NewHandle(obj));
   while (true) {
+    static constexpr int kMaxInflationAttempts = 100;  // Really > 5 should essentially never
+                                                       // happen.
+    if (UNLIKELY(inflation_attempt >= kMaxInflationAttempts)) {
+      LOG(FATAL) << "Too many inflation attempts";
+    }
     // We initially read the lockword with ordinary Java/relaxed semantics. When stronger
     // semantics are needed, we address it below. Since GetLockWord bottoms out to a relaxed load,
     // we can fix it later, in an infrequently executed case, with a fence.
@@ -1160,7 +1167,7 @@ ObjPtr<mirror::Object> Monitor::MonitorEnter(Thread* self,
             continue;  // Go again.
           } else {
             // We'd overflow the recursion count, so inflate the monitor.
-            InflateThinLocked(self, h_obj, lock_word, 0, inflation_attempt++);
+            InflateThinLocked(self, h_obj, lock_word, 0, std::min(inflation_attempt++, 4));
           }
         } else {
           if (trylock) {
@@ -1171,17 +1178,19 @@ ObjPtr<mirror::Object> Monitor::MonitorEnter(Thread* self,
           Runtime* runtime = Runtime::Current();
           if (contention_count
               <= kExtraSpinIters + runtime->GetMaxSpinsBeforeThinLockInflation()) {
-            // TODO: Consider switching the thread state to kWaitingForLockInflation when we are
-            // yielding.  Use sched_yield instead of NanoSleep since NanoSleep can wait much longer
+            // Use sched_yield instead of NanoSleep since NanoSleep can wait much longer
             // than the parameter you pass in. This can cause thread suspension to take excessively
             // long and make long pauses. See b/16307460.
             if (contention_count > kExtraSpinIters) {
+              self->CheckSuspend();
               sched_yield();
             }
           } else {
             contention_count = 0;
             // No ordering required for initial lockword read. Install rereads it anyway.
-            InflateThinLocked(self, h_obj, lock_word, 0, inflation_attempt++);
+            InflateThinLocked(self, h_obj, lock_word, 0, std::min(inflation_attempt++, 4));
+            // The above can fail without timing out of the owner exits. If that happens on the
+            // last attempt, we retry with attempt = 4.
           }
         }
         continue;  // Start from the beginning.
diff --git a/runtime/monitor.h b/runtime/monitor.h
index 876fe8790a..9ac0c3a324 100644
--- a/runtime/monitor.h
+++ b/runtime/monitor.h
@@ -159,6 +159,7 @@ class Monitor {
   // attempt_of_4 is in 1..4 inclusive or 0. A non-zero value indicates that we are retrying
   // up to 4 times, and should only abort on 4. Zero means we are only trying once, with the
   // full suspend timeout instead of a quarter.
+  // May temporarily drop and reacquire the mutator lock.
   static void InflateThinLocked(Thread* self,
                                 Handle<mirror::Object> obj,
                                 LockWord lock_word,
diff --git a/runtime/mutator_gc_coord.md b/runtime/mutator_gc_coord.md
index 35996a5f8e..63c1bc0ef8 100644
--- a/runtime/mutator_gc_coord.md
+++ b/runtime/mutator_gc_coord.md
@@ -427,12 +427,12 @@ all operations sequentially consistent. There does not appear to be an easy way
 to limit this overhead to just empty checkpoint execution. Thus it appears to be
 better to forego this "optimization".
 
-[^1]: In the most recent versions of ART, compiler-generated code loads through
-    the address at `tlsPtr_.suspend_trigger`. A thread suspension is requested
-    by setting this to null, triggering a `SIGSEGV`, causing that thread to
-    check for GC cooperation requests. The older mechanism instead sets an
-    appropriate `ThreadFlag` entry to request suspension or a checkpoint. Note
-    that the actual checkpoint function value is set, along with the flag, while
-    holding `suspend_count_lock_`. If the target thread notices that a
-    checkpoint is requested, it then acquires the `suspend_count_lock_` to read
-    the checkpoint function.
+[^1]: In the most recent versions of ART on arm64, compiler-generated code loads
+    through the address at `tlsPtr_.suspend_trigger`. A thread suspension is
+    requested by setting this to null, triggering a `SIGSEGV`, causing that
+    thread to check for GC cooperation requests. The older mechanism instead
+    sets an appropriate `ThreadFlag` entry to request suspension or a
+    checkpoint. Note that the actual checkpoint function value is set, along
+    with the flag, while holding `suspend_count_lock_`. If the target thread
+    notices that a checkpoint is requested, it then acquires the
+    `suspend_count_lock_` to read the checkpoint function.
diff --git a/runtime/native/dalvik_system_VMRuntime.cc b/runtime/native/dalvik_system_VMRuntime.cc
index 52a00c957c..769e981852 100644
--- a/runtime/native/dalvik_system_VMRuntime.cc
+++ b/runtime/native/dalvik_system_VMRuntime.cc
@@ -31,6 +31,7 @@ extern "C" void android_set_application_target_sdk_version(uint32_t version);
 #include "android-base/strings.h"
 #include "arch/instruction_set.h"
 #include "art_method-inl.h"
+#include "base/flags.h"
 #include "base/pointer_size.h"
 #include "base/sdk_version.h"
 #include "class_linker-inl.h"
@@ -429,21 +430,15 @@ static jstring VMRuntime_getCurrentInstructionSet(JNIEnv* env, jclass) {
   return env->NewStringUTF(GetInstructionSetString(kRuntimeISA));
 }
 
-static void VMRuntime_setSystemDaemonThreadPriority([[maybe_unused]] JNIEnv* env,
-                                                    [[maybe_unused]] jclass klass) {
-#ifdef ART_TARGET_ANDROID
-  Thread* self = Thread::Current();
-  DCHECK(self != nullptr);
-  pid_t tid = self->GetTid();
+static int VMRuntime_getSystemDaemonNiceness() {
   // We use a priority lower than the default for the system daemon threads (eg HeapTaskDaemon) to
-  // avoid jank due to CPU contentions between GC and other UI-related threads. b/36631902.
+  // avoid jank due to CPU contention between GC and other UI-related threads. b/36631902.
   // We may use a native priority that doesn't have a corresponding java.lang.Thread-level priority.
-  static constexpr int kSystemDaemonNiceValue = 4;  // priority 124
-  if (setpriority(PRIO_PROCESS, tid, kSystemDaemonNiceValue) != 0) {
-    PLOG(INFO) << *self << " setpriority(PRIO_PROCESS, " << tid << ", "
-               << kSystemDaemonNiceValue << ") failed";
-  }
-#endif
+  // Currently we use a niceness value between those corresponding to priority 4 and 5, which
+  // matches the traditional niceness 4 value with the traditional mapping.
+  static int systemDaemonNiceValue =
+      (6 * Thread::PriorityToNiceness(5) + 4 * Thread::PriorityToNiceness(4) + 5) / 10;
+  return systemDaemonNiceValue;
 }
 
 static void VMRuntime_setDedupeHiddenApiWarnings([[maybe_unused]] JNIEnv* env,
@@ -554,6 +549,11 @@ static jlong VMRuntime_getFullGcCount([[maybe_unused]] JNIEnv* env, [[maybe_unus
   return metrics->FullGcCount()->Value();
 }
 
+static jboolean VMRuntime_isArtTestRwFlagEnabled([[maybe_unused]] JNIEnv* env,
+                                                 [[maybe_unused]] jclass klass) {
+  return is_test_rw_flag_enabled();
+}
+
 static JNINativeMethod gMethods[] = {
     FAST_NATIVE_METHOD(VMRuntime, addressOf, "(Ljava/lang/Object;)J"),
     NATIVE_METHOD(VMRuntime, bootClassPath, "()Ljava/lang/String;"),
@@ -598,7 +598,7 @@ static JNINativeMethod gMethods[] = {
                   "(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;[Ljava/lang/String;I)V"),
     NATIVE_METHOD(VMRuntime, isBootClassPathOnDisk, "(Ljava/lang/String;)Z"),
     NATIVE_METHOD(VMRuntime, getCurrentInstructionSet, "()Ljava/lang/String;"),
-    NATIVE_METHOD(VMRuntime, setSystemDaemonThreadPriority, "()V"),
+    CRITICAL_NATIVE_METHOD(VMRuntime, getSystemDaemonNiceness, "()I"),
     NATIVE_METHOD(VMRuntime, setDedupeHiddenApiWarnings, "(Z)V"),
     NATIVE_METHOD(VMRuntime, setProcessPackageName, "(Ljava/lang/String;)V"),
     NATIVE_METHOD(VMRuntime, setProcessDataDirectory, "(Ljava/lang/String;)V"),
@@ -608,6 +608,7 @@ static JNINativeMethod gMethods[] = {
     NATIVE_METHOD(
         VMRuntime, getBaseApkOptimizationInfo, "()Ldalvik/system/DexFile$OptimizationInfo;"),
     NATIVE_METHOD(VMRuntime, getFullGcCount, "()J"),
+    NATIVE_METHOD(VMRuntime, isArtTestRwFlagEnabled, "()Z"),
 };
 
 void register_dalvik_system_VMRuntime(JNIEnv* env) {
diff --git a/runtime/native/java_lang_Class.cc b/runtime/native/java_lang_Class.cc
index efd52918af..9a4bda85d3 100644
--- a/runtime/native/java_lang_Class.cc
+++ b/runtime/native/java_lang_Class.cc
@@ -444,7 +444,7 @@ static jobjectArray Class_getDeclaredConstructorsInternal(
   }
   size_t constructor_count = 0;
   // Two pass approach for speed.
-  for (auto& m : h_klass->GetDirectMethods(kRuntimePointerSize)) {
+  for (auto& m : h_klass->GetMethods(kRuntimePointerSize)) {
     constructor_count += MethodMatchesConstructor(&m, public_only, hiddenapi_context) ? 1u : 0u;
   }
   auto h_constructors = hs.NewHandle(mirror::ObjectArray<mirror::Constructor>::Alloc(
@@ -454,7 +454,7 @@ static jobjectArray Class_getDeclaredConstructorsInternal(
     return nullptr;
   }
   constructor_count = 0;
-  for (auto& m : h_klass->GetDirectMethods(kRuntimePointerSize)) {
+  for (auto& m : h_klass->GetMethods(kRuntimePointerSize)) {
     if (MethodMatchesConstructor(&m, public_only, hiddenapi_context)) {
       DCHECK_EQ(Runtime::Current()->GetClassLinker()->GetImagePointerSize(), kRuntimePointerSize);
       DCHECK(!Runtime::Current()->IsActiveTransaction());
diff --git a/runtime/native/java_lang_Thread.cc b/runtime/native/java_lang_Thread.cc
index 2c79a38ca0..e215117df1 100644
--- a/runtime/native/java_lang_Thread.cc
+++ b/runtime/native/java_lang_Thread.cc
@@ -16,18 +16,44 @@
 
 #include "java_lang_Thread.h"
 
+#include "android-base/logging.h"
+#include "android-base/macros.h"
+#include "art_field-inl.h"
+#include "art_field.h"
+#include "art_method.h"
+#include "base/casts.h"
+#include "base/utils.h"
+#include "class_linker.h"
+#include "class_root-inl.h"
 #include "common_throws.h"
+#include "handle_scope-inl.h"
+#include "handle_scope.h"
+#include "interpreter/shadow_frame-inl.h"
+#include "interpreter/shadow_frame.h"
+#include "jni.h"
 #include "jni/jni_internal.h"
+#include "mirror/class-alloc-inl.h"
+#include "mirror/class.h"
+#include "mirror/object-inl.h"
 #include "mirror/object.h"
+#include "mirror/object_array-alloc-inl.h"
+#include "mirror/object_array-inl.h"
+#include "mirror/object_array.h"
+#include "mirror/object_reference.h"
 #include "monitor.h"
 #include "native_util.h"
 #include "nativehelper/jni_macros.h"
 #include "nativehelper/scoped_utf_chars.h"
+#include "obj_ptr.h"
+#include "runtime.h"
 #include "scoped_fast_native_object_access-inl.h"
 #include "scoped_thread_state_change-inl.h"
+#include "stack.h"
+#include "stack_reference.h"
 #include "thread.h"
 #include "thread_list.h"
 #include "verify_object.h"
+#include "well_known_classes.h"
 
 namespace art HIDDEN {
 
@@ -160,18 +186,42 @@ static void Thread_setNativeName(JNIEnv* env, jobject peer, jstring java_name) {
   }
 }
 
+/*
+ * Change Linux niceness priority for the given thread, returning errno.
+ */
+static int Thread_setNiceness0(JNIEnv* env, jobject java_thread, jint new_niceness) {
+  ScopedObjectAccess soa(env);
+  MutexLock mu(soa.Self(), *Locks::thread_list_lock_);
+  Thread* thread = Thread::FromManagedThread(soa, java_thread);
+  if (thread != nullptr) {
+    return thread->SetNativeNiceness(new_niceness);
+  }
+  return 0;
+}
+
 /*
  * Alter the priority of the specified thread.  "new_priority" will range
  * from Thread.MIN_PRIORITY to Thread.MAX_PRIORITY (1-10), with "normal"
- * threads at Thread.NORM_PRIORITY (5).
+ * threads at Thread.NORM_PRIORITY (5). Returns corresponding niceness.
  */
-static void Thread_setPriority0(JNIEnv* env, jobject java_thread, jint new_priority) {
+static int Thread_setPriority0(JNIEnv* env, jobject java_thread, jint new_priority) {
+  // We should just do the conversion and call the above. But that would bypass the
+  // Android S workaround in SetNativePriority. So we have a separate code path.
   ScopedObjectAccess soa(env);
   MutexLock mu(soa.Self(), *Locks::thread_list_lock_);
   Thread* thread = Thread::FromManagedThread(soa, java_thread);
   if (thread != nullptr) {
-    thread->SetNativePriority(new_priority);
+    return thread->SetNativePriority(new_priority);
   }
+  return Thread::PriorityToNiceness(new_priority);
+}
+
+static int Thread_priorityForNiceness(int niceness) {
+  return (Thread::NicenessToPriority(niceness));
+}
+
+static int Thread_nicenessForPriority(int priority) {
+  return (Thread::PriorityToNiceness(priority));
 }
 
 static void Thread_sleep(JNIEnv* env, jclass, jobject java_lock, jlong ms, jint ns) {
@@ -190,18 +240,175 @@ static void Thread_yield(JNIEnv*, jobject) {
   sched_yield();
 }
 
+enum PinningReason {
+  kNoReason = 0,
+  kNativeMethod = 1,
+  kMonitor = 2,
+  kUnsupportedFrame = 3,
+};
+
+struct VirtualThreadParkingVisitor final : public StackVisitor {
+  explicit VirtualThreadParkingVisitor(Thread* thread) REQUIRES_SHARED(Locks::mutator_lock_)
+      : StackVisitor(thread, nullptr, StackVisitor::StackWalkKind::kIncludeInlinedFrames, true),
+        shadow_frame_count_(0),
+        reason_(kNoReason) {}
+  bool VisitFrame() override REQUIRES_SHARED(Locks::mutator_lock_) {
+    ShadowFrame* shadow_frame = GetCurrentShadowFrame();
+
+    if (shadow_frame == nullptr) {
+      // Stack walking continues only if the only non-interpreted frame is
+      // Thread.parkVirtualInternal until JIT and AOT frame is supported.
+      ArtMethod** quick_frame = GetCurrentQuickFrame();
+      ArtMethod* method = quick_frame != nullptr ? *quick_frame : nullptr;
+      if (method != nullptr && method->IsNative()) {
+        if (method == WellKnownClasses::java_lang_Thread_parkVirtualInternal) {
+          return true;
+        }
+
+        reason_ = kNativeMethod;
+        return false;
+      }
+
+      reason_ = kUnsupportedFrame;
+      return false;
+    }
+
+    if (!shadow_frame->GetLockCountData().IsEmpty()) {
+      reason_ = kMonitor;
+      return false;
+    }
+
+    // If the verifier was able to verify the locks are balanced, the interpreter won't update the
+    // lock cound data. We need to walk the stack to find the locks here. Or should we just have an
+    // increment/decrement counter?
+    Monitor::VisitLocks(this, LockVisitingCallback, this);
+    if (reason_ == kMonitor) {
+      return false;
+    }
+    DCHECK(reason_ == kNoReason);
+
+    shadow_frame_count_++;
+    shadow_frames_.push_back(shadow_frame);
+
+    return true;
+  }
+
+  static void LockVisitingCallback(ObjPtr<mirror::Object> obj, void* visitor) {
+    DCHECK(!obj.IsNull());
+    reinterpret_cast<VirtualThreadParkingVisitor*>(visitor)->reason_ = kMonitor;
+  }
+
+  std::vector<ShadowFrame*> shadow_frames_;
+  size_t shadow_frame_count_;
+  PinningReason reason_;
+};
+
+static void Thread_parkVirtualInternal(
+    JNIEnv* env, jobject, jobject v_context, jobject parked_states, jobject vm_error) {
+  Thread* self = Thread::Current();
+  ScopedObjectAccess soa(env);
+  StackHandleScope<4> hs(soa.Self());
+  ClassLinker* cl = Runtime::Current()->GetClassLinker();
+
+  auto v_context_h = hs.NewHandle(soa.Decode<mirror::Object>(v_context));
+  auto parked_states_h = hs.NewHandle(soa.Decode<mirror::Object>(parked_states));
+  auto opeer_h = hs.NewHandle(self->GetPeer());
+
+  VirtualThreadParkingVisitor dump_visitor(self);
+  dump_visitor.WalkStack();
+
+  DCHECK_NE(dump_visitor.reason_, kUnsupportedFrame) << "JIT / AOT frame isn't supported.";
+  if (dump_visitor.reason_ != kNoReason) {
+    WellKnownClasses::dalvik_system_VirtualThreadContext_pinnedCarrierThread->SetObject<false>(
+        v_context_h.Get(), opeer_h.Get());
+    // Return to the java code to park the carrier thread
+    return;
+  }
+
+  size_t num_frames = dump_visitor.shadow_frames_.size();
+
+  DCHECK(!Runtime::Current()->IsActiveTransaction());
+
+  Runtime* runtime = Runtime::Current();
+  auto frames_h = hs.NewHandle(mirror::ObjectArray<mirror::Object>::Alloc(
+      self,
+      soa.Decode<mirror::Class>(WellKnownClasses::dalvik_system_VirtualThreadFrame__array),
+      num_frames));
+  for (size_t i = 0; i < num_frames; i++) {
+    const ShadowFrame* sf = dump_visitor.shadow_frames_[i];
+    DCHECK(sf != nullptr);
+    sf->CheckConsistentVRegs();
+
+    size_t num_vergs = sf->NumberOfVRegs();
+    int32_t non_vref_size = ShadowFrame::ComputeSizeWithoutReferences(num_vergs);
+
+    StackHandleScope<4> hs2(soa.Self());
+    auto vtf = hs2.NewHandle(WellKnownClasses::dalvik_system_VirtualThreadFrame.Get()->Alloc(
+        self, Runtime::Current()->GetHeap()->GetCurrentAllocator()));
+    auto frame_bytes = hs2.NewHandle(mirror::ByteArray::Alloc(self, non_vref_size));
+    auto declaring_class = hs2.NewHandle(sf->GetMethod()->GetDeclaringClass());
+    auto refs = hs2.NewHandle(mirror::ObjectArray<mirror::Object>::Alloc(
+        self, GetClassRoot<mirror::ObjectArray<mirror::Object>>(), num_vergs));
+    DCHECK(!vtf.IsNull());
+    DCHECK(!frame_bytes.IsNull());
+    DCHECK(!refs.IsNull());
+
+    bool areRefsAllNull = true;
+    for (uint32_t j = 0; j < sf->NumberOfVRegs(); j++) {
+      ObjPtr<mirror::Object> obj = sf->GetVRegReference(j);
+      if (obj != nullptr) {
+        refs->Set(j, obj);
+        areRefsAllNull = false;
+      }
+    }
+
+    if (!areRefsAllNull) {
+      WellKnownClasses::dalvik_system_VirtualThreadFrame_refs->SetObject<false>(vtf.Get(),
+                                                                                refs.Get());
+    }
+    frame_bytes->Memcpy(0, reinterpret_cast<const int8_t*>(sf), 0, non_vref_size);
+    WellKnownClasses::dalvik_system_VirtualThreadFrame_frame->SetObject<false>(vtf.Get(),
+                                                                               frame_bytes.Get());
+    WellKnownClasses::dalvik_system_VirtualThreadFrame_declaringClass->SetObject<false>(
+        vtf.Get(), declaring_class.Get());
+    frames_h->Set(i, vtf.Get());
+  }
+
+  if (self->IsExceptionPending()) {
+    // It's likely a OOME. Let's throw it at the java level.
+    // Consider handling it in another way, e.g. pinning virtual thread, in the future.
+    return;
+  }
+
+  WellKnownClasses::dalvik_system_VirtualThreadParkedStates_frames->SetObject<false>(
+      parked_states_h.Get(), frames_h.Get());
+  WellKnownClasses::dalvik_system_VirtualThreadContext_parkedStates->SetObject<false>(
+      v_context_h.Get(), parked_states_h.Get());
+  self->SetVirtualThreadFlags(VirtualThreadFlag::kParking, true);
+
+  // Throw a VirtualThreadParkingError to unwind the stack and park the virtual thread.
+  env->Throw(reinterpret_cast<jthrowable>(vm_error));
+}
+
 static JNINativeMethod gMethods[] = {
-  FAST_NATIVE_METHOD(Thread, currentThread, "()Ljava/lang/Thread;"),
-  FAST_NATIVE_METHOD(Thread, interrupted, "()Z"),
-  FAST_NATIVE_METHOD(Thread, isInterrupted, "()Z"),
-  NATIVE_METHOD(Thread, nativeCreate, "(Ljava/lang/Thread;JZ)V"),
-  NATIVE_METHOD(Thread, nativeGetStatus, "(Z)I"),
-  NATIVE_METHOD(Thread, holdsLock, "(Ljava/lang/Object;)Z"),
-  FAST_NATIVE_METHOD(Thread, interrupt0, "()V"),
-  NATIVE_METHOD(Thread, setNativeName, "(Ljava/lang/String;)V"),
-  NATIVE_METHOD(Thread, setPriority0, "(I)V"),
-  FAST_NATIVE_METHOD(Thread, sleep, "(Ljava/lang/Object;JI)V"),
-  NATIVE_METHOD(Thread, yield, "()V"),
+    FAST_NATIVE_METHOD(Thread, currentThread, "()Ljava/lang/Thread;"),
+    FAST_NATIVE_METHOD(Thread, interrupted, "()Z"),
+    FAST_NATIVE_METHOD(Thread, isInterrupted, "()Z"),
+    NATIVE_METHOD(Thread, nativeCreate, "(Ljava/lang/Thread;JZ)V"),
+    NATIVE_METHOD(Thread, nativeGetStatus, "(Z)I"),
+    CRITICAL_NATIVE_METHOD(Thread, nicenessForPriority, "(I)I"),
+    NATIVE_METHOD(Thread, holdsLock, "(Ljava/lang/Object;)Z"),
+    FAST_NATIVE_METHOD(Thread, interrupt0, "()V"),
+    CRITICAL_NATIVE_METHOD(Thread, priorityForNiceness, "(I)I"),
+    NATIVE_METHOD(Thread, setNativeName, "(Ljava/lang/String;)V"),
+    NATIVE_METHOD(Thread, setNiceness0, "(I)I"),
+    NATIVE_METHOD(Thread, setPriority0, "(I)I"),
+    FAST_NATIVE_METHOD(Thread, sleep, "(Ljava/lang/Object;JI)V"),
+    NATIVE_METHOD(Thread, yield, "()V"),
+    NATIVE_METHOD(Thread,
+                  parkVirtualInternal,
+                  "(Ldalvik/system/VirtualThreadContext;Ldalvik/system/"
+                  "VirtualThreadParkedStates;Ldalvik/system/VirtualThreadParkingError;)V"),
 };
 
 void register_java_lang_Thread(JNIEnv* env) {
diff --git a/runtime/native/java_lang_reflect_Field.cc b/runtime/native/java_lang_reflect_Field.cc
index 622b514ea3..04c1f9da96 100644
--- a/runtime/native/java_lang_reflect_Field.cc
+++ b/runtime/native/java_lang_reflect_Field.cc
@@ -340,22 +340,29 @@ ALWAYS_INLINE inline static void SetFieldValue(ObjPtr<mirror::Object> o,
   }
 }
 
-ALWAYS_INLINE inline static bool ThrowIAEIfRecordFinalField(ObjPtr<mirror::Field> field)
+ALWAYS_INLINE inline static bool ThrowIAEIfFieldIsNotOverwritable(ObjPtr<mirror::Field> field)
     REQUIRES_SHARED(Locks::mutator_lock_) {
-  if (!(field->IsFinal())) {
-    return false;
-  }
-  ObjPtr<mirror::Class> declaring_class = field->GetDeclaringClass();
-  DCHECK(declaring_class != nullptr);
-  if (!(declaring_class->IsRecordClass())) {
+  // Write-protected fields can be modified via System.setIn/setOut/setErr methods only.
+  // However, before Android C, reflection and JNI APIs were allowed to modify them.
+  if (field->IsWriteProtected()) {
+    // See Field::IsMonotonic.
+    uint32_t target_sdk_version = Runtime::Current()->GetTargetSdkVersion();
+    if (IsSdkVersionSetAndAtMost(target_sdk_version, SdkVersion::kB)) {
+      return false;
+    }
+
+    uint32_t sdk_version = Runtime::Current()->GetSdkVersion();
+    if (IsSdkVersionSetAndAtMost(sdk_version, SdkVersion::kB)) {
+      return false;
+    }
+  } else if (!field->IsMonotonic()) {
     return false;
   }
-
   ThrowIllegalAccessException(
-          StringPrintf("Cannot set %s field %s of record class %s",
+          StringPrintf("Cannot set %s field %s of class %s",
               PrettyJavaAccessFlags(field->GetAccessFlags()).c_str(),
               ArtField::PrettyField(field->GetArtField()).c_str(),
-              declaring_class->PrettyClass().c_str()).c_str());
+              field->GetDeclaringClass()->PrettyClass().c_str()).c_str());
 
   return true;
 }
@@ -369,7 +376,7 @@ static void Field_set(JNIEnv* env, jobject javaField, jobject javaObj, jobject j
     DCHECK(soa.Self()->IsExceptionPending());
     return;
   }
-  if (ThrowIAEIfRecordFinalField(f)) {
+  if (ThrowIAEIfFieldIsNotOverwritable(f)) {
     DCHECK(soa.Self()->IsExceptionPending());
     return;
   }
@@ -399,6 +406,7 @@ static void Field_set(JNIEnv* env, jobject javaField, jobject javaObj, jobject j
     DCHECK(soa.Self()->IsExceptionPending());
     return;
   }
+
   SetFieldValue(o, f, field_prim_type, true, unboxed_value);
 }
 
@@ -413,7 +421,7 @@ static void SetPrimitiveField(JNIEnv* env,
   if (!CheckReceiver(soa, javaObj, &f, &o)) {
     return;
   }
-  if (ThrowIAEIfRecordFinalField(f)) {
+  if (ThrowIAEIfFieldIsNotOverwritable(f)) {
     DCHECK(soa.Self()->IsExceptionPending());
     return;
   }
@@ -549,6 +557,13 @@ static jboolean Field_isAnnotationPresentNative(JNIEnv* env,
   return annotations::IsFieldAnnotationPresent(field, klass);
 }
 
+static jboolean Field_isMonotonic0(JNIEnv* env, jobject javaField) {
+  ScopedObjectAccess soa(env);
+  ObjPtr<mirror::Field> f = soa.Decode<mirror::Field>(javaField);
+
+  return f->IsMonotonic();
+}
+
 static JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Field, get,        "(Ljava/lang/Object;)Ljava/lang/Object;"),
   FAST_NATIVE_METHOD(Field, getBoolean, "(Ljava/lang/Object;)Z"),
@@ -566,6 +581,7 @@ static JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Field, getNameInternal, "()Ljava/lang/String;"),
   FAST_NATIVE_METHOD(Field, getShort,   "(Ljava/lang/Object;)S"),
   FAST_NATIVE_METHOD(Field, isAnnotationPresentNative, "(Ljava/lang/Class;)Z"),
+  NATIVE_METHOD(Field,      isMonotonic0, "()Z"),
   FAST_NATIVE_METHOD(Field, set,        "(Ljava/lang/Object;Ljava/lang/Object;)V"),
   FAST_NATIVE_METHOD(Field, setBoolean, "(Ljava/lang/Object;Z)V"),
   FAST_NATIVE_METHOD(Field, setByte,    "(Ljava/lang/Object;B)V"),
diff --git a/runtime/native/java_lang_reflect_Method.cc b/runtime/native/java_lang_reflect_Method.cc
index 64b4b17d4c..92161f8bcd 100644
--- a/runtime/native/java_lang_reflect_Method.cc
+++ b/runtime/native/java_lang_reflect_Method.cc
@@ -48,16 +48,8 @@ static jobjectArray Method_getExceptionTypes(JNIEnv* env, jobject javaMethod) {
   ArtMethod* method = ArtMethod::FromReflectedMethod(soa, javaMethod);
   if (method->GetDeclaringClass()->IsProxyClass()) {
     ObjPtr<mirror::Class> klass = method->GetDeclaringClass();
-    int throws_index = -1;
-    size_t i = 0;
-    for (const auto& m : klass->GetDeclaredVirtualMethods(kRuntimePointerSize)) {
-      if (&m == method) {
-        throws_index = i;
-        break;
-      }
-      ++i;
-    }
-    CHECK_NE(throws_index, -1);
+    size_t throws_index = klass->GetProxyThrowsIndex(method);
+    DCHECK_NE(throws_index, static_cast<size_t>(-1));
     StackHandleScope<1u> hs(soa.Self());
     Handle<mirror::ObjectArray<mirror::Class>> declared_exceptions =
         hs.NewHandle(klass->GetProxyThrows()->Get(throws_index));
diff --git a/runtime/native/sun_misc_Unsafe.cc b/runtime/native/sun_misc_Unsafe.cc
index 38fe72555c..b4de838a9e 100644
--- a/runtime/native/sun_misc_Unsafe.cc
+++ b/runtime/native/sun_misc_Unsafe.cc
@@ -251,6 +251,10 @@ static void Unsafe_setMemory(
   memset(reinterpret_cast<void*>(static_cast<uintptr_t>(address)), value, bytes);
 }
 
+static jobject Unsafe_allocateInstance(JNIEnv* env, jobject, jclass c) {
+  return env->AllocObject(c);
+}
+
 static jbyte Unsafe_getByteJ([[maybe_unused]] JNIEnv* env, jobject, jlong address) {
   return *reinterpret_cast<jbyte*>(address);
 }
@@ -577,6 +581,7 @@ static JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Unsafe, copyMemoryToPrimitiveArray, "(JLjava/lang/Object;JJ)V"),
   FAST_NATIVE_METHOD(Unsafe, copyMemoryFromPrimitiveArray, "(Ljava/lang/Object;JJJ)V"),
   FAST_NATIVE_METHOD(Unsafe, getBoolean, "(Ljava/lang/Object;J)Z"),
+  NATIVE_METHOD(Unsafe, allocateInstance, "(Ljava/lang/Class;)Ljava/lang/Object;"),
 
   FAST_NATIVE_METHOD(Unsafe, getByte, "(Ljava/lang/Object;J)B"),
   FAST_NATIVE_METHOD(Unsafe, getChar, "(Ljava/lang/Object;J)C"),
diff --git a/runtime/native_stack_dump.cc b/runtime/native_stack_dump.cc
index 5a6cee0630..b4204b168a 100644
--- a/runtime/native_stack_dump.cc
+++ b/runtime/native_stack_dump.cc
@@ -31,7 +31,6 @@
 
 #include <vector>
 
-#include <linux/unistd.h>
 #include <poll.h>
 #include <signal.h>
 #include <stdlib.h>
diff --git a/runtime/oat/oat.cc b/runtime/oat/oat.cc
index 9882b0fa51..8fb8a572c7 100644
--- a/runtime/oat/oat.cc
+++ b/runtime/oat/oat.cc
@@ -470,6 +470,17 @@ bool OatHeader::RequiresImage() const {
   return IsKeyEnabled(OatHeader::kRequiresImage);
 }
 
+bool OatHeader::HasAssumeValueSdkInt() const {
+  const char* key_value = GetStoreValueByKey(kAssumeValueSdkIntKey);
+  return key_value != nullptr;
+}
+
+uint32_t OatHeader::GetAssumeValueSdkInt() const {
+  const char* key_value = GetStoreValueByKey(kAssumeValueSdkIntKey);
+  CHECK(key_value != nullptr) << "assume-value-sdk-int not found in oat header";
+  return atoi(key_value);
+}
+
 CompilerFilter::Filter OatHeader::GetCompilerFilter() const {
   CompilerFilter::Filter filter;
   const char* key_value = GetStoreValueByKey(kCompilerFilter);
diff --git a/runtime/oat/oat.h b/runtime/oat/oat.h
index 2069b569e3..445d7f1616 100644
--- a/runtime/oat/oat.h
+++ b/runtime/oat/oat.h
@@ -47,8 +47,8 @@ std::ostream& operator<<(std::ostream& stream, StubType stub_type);
 class EXPORT PACKED(4) OatHeader {
  public:
   static constexpr std::array<uint8_t, 4> kOatMagic { { 'o', 'a', 't', '\n' } };
-  // Last oat version changed reason: Ensure oat checksum determinism across hosts and devices.
-  static constexpr std::array<uint8_t, 4> kOatVersion{{'2', '5', '9', '\0'}};
+  // Last oat version changed reason: class_flags changes required for using MOVE ioctl.
+  static constexpr std::array<uint8_t, 4> kOatVersion{{'2', '6', '2', '\0'}};
 
   static constexpr const char* kDex2OatCmdLineKey = "dex2oat-cmdline";
   static constexpr const char* kDebuggableKey = "debuggable";
@@ -61,6 +61,9 @@ class EXPORT PACKED(4) OatHeader {
   static constexpr const char* kConcurrentCopying = "concurrent-copying";
   static constexpr const char* kCompilationReasonKey = "compilation-reason";
   static constexpr const char* kRequiresImage = "requires-image";
+  // Note: If we add support for additional assumed values, we should generalize this key to support
+  // repeated descriptor:value pairings.
+  static constexpr const char* kAssumeValueSdkIntKey = "assume-value-sdk-int";
 
   // Fields listed here are key value store fields that are deterministic across hosts and devices,
   // meaning they should have exactly the same value when the oat file is generated on different
@@ -72,7 +75,7 @@ class EXPORT PACKED(4) OatHeader {
   // excluded from the oat checksum computation. This makes the oat checksum deterministic across
   // hosts and devices, which is important for Cloud Compilation, where we generate an oat file on a
   // host and use it on a device.
-  static constexpr std::array<std::string_view, 9> kDeterministicFields{
+  static constexpr std::array<std::string_view, 10> kDeterministicFields{
       kDebuggableKey,
       kNativeDebuggableKey,
       kCompilerFilter,
@@ -82,6 +85,7 @@ class EXPORT PACKED(4) OatHeader {
       kConcurrentCopying,
       kCompilationReasonKey,
       kRequiresImage,
+      kAssumeValueSdkIntKey
   };
 
   static constexpr std::array<std::pair<std::string_view, size_t>, 2>
@@ -186,6 +190,8 @@ class EXPORT PACKED(4) OatHeader {
   CompilerFilter::Filter GetCompilerFilter() const;
   bool IsConcurrentCopying() const;
   bool RequiresImage() const;
+  bool HasAssumeValueSdkInt() const;
+  uint32_t GetAssumeValueSdkInt() const;
 
   const uint8_t* GetOatAddress(StubType type) const;
 
diff --git a/runtime/oat/oat_file.cc b/runtime/oat/oat_file.cc
index ec845cd0c1..bf99d03f1a 100644
--- a/runtime/oat/oat_file.cc
+++ b/runtime/oat/oat_file.cc
@@ -97,7 +97,7 @@
 // dlopen_ext support from bionic.
 #ifdef ART_TARGET_ANDROID
 #include "android/dlext.h"
-#include "nativeloader/dlext_namespaces.h"
+#include "bionic/dlext_namespaces.h"
 #endif
 
 namespace art HIDDEN {
@@ -980,7 +980,9 @@ bool OatFileBase::Setup(int zip_fd,
       // expects this check to happen during oat file setup when the oat file
       // does not contain dex code.
       if (dex_file_checksum != external_dex_files_[i]->GetLocationChecksum()) {
-        CHECK(dex_file_sha1 != external_dex_files_[i]->GetSha1());
+        // The location checksum is affected by all dex files within container,
+        // so one dex file can match exactly by SHA1 and yet location checksum
+        // might differ for it if some other dex file in the container differs.
         *error_msg = ErrorPrintf("dex file checksum 0x%08x does not match"
                                      " checksum 0x%08x of external dex file '%s'",
                                  dex_file_checksum,
@@ -2331,9 +2333,6 @@ void OatDexFile::InitializeTypeLookupTable() {
       if (StandardDexFile::IsMagicValid(dex_header->magic_)) {
         dex_data -= dex_header->HeaderOffset();
       }
-      if (CompactDexFile::IsMagicValid(dex_header->magic_)) {
-        dex_data += dex_header->data_off_;
-      }
       lookup_table_ = TypeLookupTable::Open(dex_data, lookup_table_data_, num_class_defs);
     }
   }
diff --git a/runtime/oat/oat_file_assistant.cc b/runtime/oat/oat_file_assistant.cc
index 53f412908d..df719228e0 100644
--- a/runtime/oat/oat_file_assistant.cc
+++ b/runtime/oat/oat_file_assistant.cc
@@ -82,6 +82,9 @@ std::ostream& operator<<(std::ostream& stream, const OatFileAssistant::OatStatus
     case OatFileAssistant::kOatContextOutOfDate:
       stream << "kOatContextOutOfDate";
       break;
+    case OatFileAssistant::kOatAssumedValuesOutOfDate:
+      stream << "kOatAssumedValuesOutOfDate";
+      break;
   }
 
   return stream;
@@ -522,6 +525,14 @@ OatFileAssistant::OatStatus OatFileAssistant::GivenOatFileStatus(const OatFile&
     return kOatContextOutOfDate;
   }
 
+  if (file.GetOatHeader().HasAssumeValueSdkInt() &&
+      file.GetOatHeader().GetAssumeValueSdkInt() != GetRuntimeOptions().sdk_version) {
+    *error_msg = ART_FORMAT("Assumed value mismatch for SDK_INT (compiled='{}', runtime='{}')",
+                            file.GetOatHeader().GetAssumeValueSdkInt(),
+                            GetRuntimeOptions().sdk_version);
+    return kOatAssumedValuesOutOfDate;
+  }
+
   return kOatUpToDate;
 }
 
@@ -903,6 +914,7 @@ bool OatFileAssistant::OatFileInfo::IsUseable() {
     case kOatDexOutOfDate:
     case kOatContextOutOfDate:
     case kOatBootImageOutOfDate:
+    case kOatAssumedValuesOutOfDate:
       return false;
 
     case kOatUpToDate:
@@ -1294,6 +1306,7 @@ void OatFileAssistant::GetOptimizationStatus(std::string* out_odex_location,
     case kOatCannotOpen:
     case kOatBootImageOutOfDate:
     case kOatContextOutOfDate:
+    case kOatAssumedValuesOutOfDate:
       // These should never happen, but be robust.
       *out_compilation_filter = "unexpected";
       *out_compilation_reason = "unexpected";
diff --git a/runtime/oat/oat_file_assistant.h b/runtime/oat/oat_file_assistant.h
index 80eabba34b..f9f0e8573d 100644
--- a/runtime/oat/oat_file_assistant.h
+++ b/runtime/oat/oat_file_assistant.h
@@ -91,6 +91,10 @@ class OatFileAssistant {
     // respect to the class loader context.
     kOatContextOutOfDate,
 
+    // kOatAssumedValuesOutOfDate - The assumed compiler values in the oat file
+    // are of date with respect to the current runtime.
+    kOatAssumedValuesOutOfDate,
+
     // kOatUpToDate - The oat file is completely up to date with respect to
     // the dex file and boot image.
     kOatUpToDate,
diff --git a/runtime/oat/oat_file_assistant_context.cc b/runtime/oat/oat_file_assistant_context.cc
index 23ad19b652..cc369cd956 100644
--- a/runtime/oat/oat_file_assistant_context.cc
+++ b/runtime/oat/oat_file_assistant_context.cc
@@ -60,6 +60,7 @@ OatFileAssistantContext::OatFileAssistantContext(Runtime* runtime)
                                            runtime->GetBootClassPathFiles() :
                                            std::optional<ArrayRef<File>>(),
               .deny_art_apex_data_files = runtime->DenyArtApexDataFiles(),
+              .sdk_version = runtime->GetSdkVersion(),
           })) {
   // Fetch boot image info from the runtime.
   std::vector<BootImageInfo>& boot_image_info_list =
diff --git a/runtime/oat/oat_file_assistant_context.h b/runtime/oat/oat_file_assistant_context.h
index 28d5b49dc2..ae0a335c60 100644
--- a/runtime/oat/oat_file_assistant_context.h
+++ b/runtime/oat/oat_file_assistant_context.h
@@ -24,6 +24,7 @@
 
 #include "arch/instruction_set.h"
 #include "base/macros.h"
+#include "base/sdk_version.h"
 #include "runtime.h"
 
 namespace art HIDDEN {
@@ -48,6 +49,8 @@ class OatFileAssistantContext {
     std::optional<ArrayRef<File>> boot_class_path_files = {};
     // Optional. See `-Xdeny-art-apex-data-files`.
     const bool deny_art_apex_data_files = false;
+    // Optional. Determined from the context's `ro.build.version.sdk` sysprop.
+    const uint32_t sdk_version = static_cast<uint32_t>(SdkVersion::kUnset);
   };
 
   // Information about a boot image.
diff --git a/runtime/oat/oat_file_assistant_test.cc b/runtime/oat/oat_file_assistant_test.cc
index c3e2f63e24..e24b7ce896 100644
--- a/runtime/oat/oat_file_assistant_test.cc
+++ b/runtime/oat/oat_file_assistant_test.cc
@@ -201,6 +201,7 @@ class OatFileAssistantTest : public OatFileAssistantBaseTest,
                                              runtime_->GetBootClassPathFiles() :
                                              std::optional<ArrayRef<File>>(),
                 .deny_art_apex_data_files = runtime_->DenyArtApexDataFiles(),
+                .sdk_version = runtime_->GetSdkVersion(),
             }));
   }
 
@@ -233,6 +234,8 @@ class OatFileAssistantTest : public OatFileAssistantBaseTest,
     EXPECT_EQ(*has_dex_files, expected_value);
   }
 
+  void SetRuntimeSdkVersion(uint32_t sdk_version) { runtime_->SetSdkVersion(sdk_version); }
+
   std::unique_ptr<ClassLoaderContext> default_context_ = InitializeDefaultContext();
   bool with_runtime_;
   const OatFileAssistant::DexOptTrigger default_trigger_{
@@ -1412,6 +1415,57 @@ TEST_P(OatFileAssistantTest, LongDexExtension) {
   EXPECT_EQ(OatFileAssistant::kOatCannotOpen, oat_file_assistant.OatFileStatus());
 }
 
+// Case: Mismatch between assumed values for SDK_INT and runtime SDK_INT.
+// Expect: kOatAssumedValuesOutOfDate when 1) mismatched SDK versions, and 2) feature flag enabled.
+TEST_P(OatFileAssistantTest, AssumedValuesOutOfDate) {
+  std::string dex_location = GetScratchDir() + "/AssumedValuesOutOfDate.jar";
+  std::string odex_location = GetOdexDir() + "/AssumedValuesOutOfDate.odex";
+  Copy(GetDexSrc1(), dex_location);
+
+  GenerateOdexForTest(dex_location,
+                      odex_location,
+                      CompilerFilter::kSpeed,
+                      /*compilation_reason=*/nullptr,
+                      /*extra_args=*/{"--assume-value=Landroid/os/Build$VERSION;->SDK_INT:76"});
+
+  // Ensure both unset and differing runtime SDK versions yield out-of-date dexopt status.
+  for (uint32_t runtime_sdk_version : {static_cast<uint32_t>(SdkVersion::kUnset), 77u}) {
+    SCOPED_TRACE("Runtime SDK version: " + std::to_string(runtime_sdk_version));
+    // Override the runtime SDK version and recreate the OFA context to reflect this.
+    SetRuntimeSdkVersion(runtime_sdk_version);
+    ofa_context_ = CreateOatFileAssistantContext();
+
+    auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
+
+    OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str());
+    if (com::android::art::flags::compile_sdk_int_constant()) {
+      // When the runtime SDK_INT differs from the compiled SDK_INT, reject the ODEX file.
+      // Note that the VDEX remains usable.
+      EXPECT_EQ(OatFileAssistant::kOatAssumedValuesOutOfDate, oat_file_assistant.OdexFileStatus());
+      EXPECT_EQ(OatFileAssistant::kOatCannotOpen, oat_file_assistant.OatFileStatus());
+      VerifyGetDexOptNeededDefault(&oat_file_assistant,
+                                   CompilerFilter::kSpeed,
+                                   /*expected_dexopt_needed=*/true,
+                                   /*expected_is_vdex_usable=*/true,
+                                   /*expected_location=*/OatFileAssistant::kLocationOdex,
+                                   /*expected_legacy_result=*/-OatFileAssistant::kDex2OatForFilter);
+    } else {
+      // Otherwise, when assumed values for SDK_INT are disabled, ODEX compilation and loading are
+      // not affected.
+      EXPECT_EQ(OatFileAssistant::kOatUpToDate, oat_file_assistant.OdexFileStatus());
+      EXPECT_EQ(OatFileAssistant::kOatCannotOpen, oat_file_assistant.OatFileStatus());
+      VerifyGetDexOptNeededDefault(&oat_file_assistant,
+                                   CompilerFilter::kSpeed,
+                                   /*expected_dexopt_needed=*/false,
+                                   /*expected_is_vdex_usable=*/true,
+                                   /*expected_location=*/OatFileAssistant::kLocationOdex,
+                                   /*expected_legacy_result=*/OatFileAssistant::kNoDexOptNeeded);
+      EXPECT_EQ(OatFileAssistant::kNoDexOptNeeded,
+                oat_file_assistant.GetDexOptNeeded(CompilerFilter::kSpeed));
+    }
+  }
+}
+
 // A task to generate a dex location. Used by the RaceToGenerate test.
 class RaceGenerateTask : public Task {
  public:
diff --git a/runtime/oat/oat_file_test.cc b/runtime/oat/oat_file_test.cc
index f79c4cac26..59cd60d1a8 100644
--- a/runtime/oat/oat_file_test.cc
+++ b/runtime/oat/oat_file_test.cc
@@ -18,16 +18,41 @@
 
 #include <dlfcn.h>
 
+#include <algorithm>
+#include <cstring>
+#include <memory>
 #include <string>
+#include <string_view>
 
+#include "android-base/scopeguard.h"
+#include "base/file_utils.h"
+#include "base/os.h"
 #include "common_runtime_test.h"
 #include "dexopt_test.h"
+#include "gmock/gmock.h"
 #include "gtest/gtest.h"
 #include "scoped_thread_state_change-inl.h"
 #include "vdex_file.h"
 
 namespace art HIDDEN {
 
+using ::testing::HasSubstr;
+
+using std::string_view_literals::operator""sv;
+
+// Returns the offset of the first dex file in the vdex file.
+static void GetFirstDexFileOffset(const std::string& vdex_filename, /*out*/ size_t* offset) {
+  std::string error_msg;
+  std::unique_ptr<VdexFile> vdex_file =
+      VdexFile::Open(vdex_filename, /*low_4gb=*/false, &error_msg);
+  ASSERT_NE(vdex_file, nullptr) << error_msg;
+  const uint8_t* ptr = vdex_file->GetNextDexFileData(/*cursor=*/nullptr, /*dex_file_index=*/0);
+  ASSERT_NE(ptr, nullptr) << "No dex code in vdex";
+  ASSERT_GE(ptr, vdex_file->Begin());
+  ASSERT_LT(ptr, vdex_file->End());
+  *offset = ptr - vdex_file->Begin();
+}
+
 class OatFileTest : public DexoptTest {};
 
 TEST_F(OatFileTest, LoadOat) {
@@ -123,10 +148,17 @@ TEST_F(OatFileTest, DlOpenLoad) {
   if (!error_msg.empty()) {
     // If a valid oat file was returned but there was an error message, then dlopen failed
     // but the backup ART ELF loader successfully loaded the oat file.
-    // The only expected reason for this is a bug in glibc that prevents loading dynamic
-    // shared objects with a read-only dynamic section:
-    // https://sourceware.org/bugzilla/show_bug.cgi?id=28340.
-    ASSERT_TRUE(error_msg == "DlOpen does not support read-only .dynamic section.") << error_msg;
+    // There are a few expected reasons for this:
+    //   - a bug in glibc that prevents loading dynamic shared objects with a read-only dynamic
+    //     section https://sourceware.org/bugzilla/show_bug.cgi?id=28340.
+    //   - glibc >= 2.41 fails to dlopen shared objects that don't have GNU_STACK segment,
+    //     see https://lists.gnu.org/archive/html/info-gnu/2025-01/msg00014.html
+    ASSERT_TRUE(
+        error_msg == "DlOpen does not support read-only .dynamic section." ||
+        error_msg ==
+            "Failed to dlopen '" + oat_location + "': " + oat_location +
+                ": cannot enable executable stack as shared object requires: Invalid argument")
+        << error_msg;
     GTEST_SKIP() << error_msg;
   }
 #else
@@ -147,4 +179,57 @@ TEST_F(OatFileTest, DlOpenLoad) {
   EXPECT_STREQ(info.dli_sname, "oatdata") << info.dli_sname;
 }
 
+TEST_F(OatFileTest, RejectsCdex) {
+  std::string dex_location = GetScratchDir() + "/LoadOat.jar";
+  std::string odex_location = GetScratchDir() + "/LoadOat.odex";
+  std::string vdex_location = GetVdexFilename(odex_location);
+
+  Copy(GetDexSrc1(), dex_location);
+  ASSERT_NO_FATAL_FAILURE(GenerateOdexForTest(dex_location, odex_location, CompilerFilter::kSpeed));
+
+  // Patch the generated vdex file to simulate that it contains cdex.
+  {
+    size_t dex_offset;
+    ASSERT_NO_FATAL_FAILURE(GetFirstDexFileOffset(vdex_location, &dex_offset));
+    std::unique_ptr<File> vdex_file(OS::OpenFileReadWrite(vdex_location.c_str()));
+    ASSERT_NE(vdex_file, nullptr) << strerror(errno);
+    auto cleanup = android::base::make_scope_guard([&] { (void)vdex_file->FlushClose(); });
+    constexpr std::string_view kCdexMagic = "cdex001\0"sv;
+    ASSERT_LE(dex_offset + kCdexMagic.size(), vdex_file->GetLength()) << "Dex file too short";
+    bool success = vdex_file->PwriteFully(kCdexMagic.data(), kCdexMagic.size(), dex_offset);
+    ASSERT_TRUE(success) << strerror(errno);
+    cleanup.Disable();
+    ASSERT_EQ(vdex_file->FlushClose(), 0);
+  }
+
+  // Create `OatFile` from the vdex file together with the oat file. This should fail.
+  {
+    std::string error_msg;
+    std::unique_ptr<OatFile> odex_file(OatFile::Open(/*zip_fd=*/-1,
+                                                     odex_location,
+                                                     odex_location,
+                                                     /*executable=*/false,
+                                                     /*low_4gb=*/false,
+                                                     dex_location,
+                                                     &error_msg));
+    EXPECT_EQ(odex_file, nullptr) << "Cdex accepted unexpectedly";
+    EXPECT_THAT(error_msg, HasSubstr("invalid dex file magic"));
+  }
+
+  // Create `OatFile` from the vdex file alone. This should fail too.
+  {
+    std::string error_msg;
+    std::unique_ptr<VdexFile> vdex_file =
+        VdexFile::Open(vdex_location, /*low_4gb=*/false, &error_msg);
+    ASSERT_NE(vdex_file, nullptr);
+    std::unique_ptr<OatFile> odex_file(OatFile::OpenFromVdex(/*zip_fd=*/-1,
+                                                             std::move(vdex_file),
+                                                             vdex_location,
+                                                             /*context=*/nullptr,
+                                                             &error_msg));
+    EXPECT_EQ(odex_file, nullptr) << "Cdex accepted unexpectedly";
+    EXPECT_THAT(error_msg, HasSubstr("found dex file with invalid dex file version"));
+  }
+}
+
 }  // namespace art
diff --git a/runtime/proxy_test.h b/runtime/proxy_test.h
index 8c69a2e8c7..62b6d85009 100644
--- a/runtime/proxy_test.h
+++ b/runtime/proxy_test.h
@@ -95,10 +95,12 @@ inline ObjPtr<mirror::Class> GenerateProxyClass(ScopedObjectAccess& soa,
           mirror::Method::CreateFromArtMethod<kRuntimePointerSize>(soa.Self(), method)));
   // Now adds all interfaces virtual methods.
   for (Handle<mirror::Class> interface : interfaces) {
-    for (auto& m : interface->GetDeclaredVirtualMethods(kRuntimePointerSize)) {
-      soa.Env()->SetObjectArrayElement(
-          proxyClassMethods, array_index++, soa.AddLocalReference<jobject>(
-              mirror::Method::CreateFromArtMethod<kRuntimePointerSize>(soa.Self(), &m)));
+    for (auto& m : interface->GetDeclaredMethods(kRuntimePointerSize)) {
+      if (m.IsVirtual()) {
+        soa.Env()->SetObjectArrayElement(
+            proxyClassMethods, array_index++, soa.AddLocalReference<jobject>(
+                mirror::Method::CreateFromArtMethod<kRuntimePointerSize>(soa.Self(), &m)));
+      }
     }
   }
   CHECK_EQ(array_index, methods_count);
diff --git a/runtime/runtime.cc b/runtime/runtime.cc
index d6c11628fe..28b84a8970 100644
--- a/runtime/runtime.cc
+++ b/runtime/runtime.cc
@@ -58,7 +58,7 @@
 #include "base/dumpable.h"
 #include "base/file_utils.h"
 #include "base/flags.h"
-#include "base/malloc_arena_pool.h"
+#include "base/calloc_arena_pool.h"
 #include "base/mem_map_arena_pool.h"
 #include "base/memory_tool.h"
 #include "base/mutex.h"
@@ -290,6 +290,9 @@ Runtime::Runtime()
       active_transaction_(false),
       verify_(verifier::VerifyMode::kNone),
       target_sdk_version_(static_cast<uint32_t>(SdkVersion::kUnset)),
+      sdk_version_(
+          android::base::GetUintProperty(
+              "ro.build.version.sdk", static_cast<uint32_t>(SdkVersion::kUnset))),
       compat_framework_(),
       implicit_null_checks_(false),
       implicit_so_checks_(false),
@@ -1512,9 +1515,11 @@ std::string Runtime::GetApexVersions(ArrayRef<const std::string> boot_class_path
     if (info == apex_infos.end() || info->second->getIsFactory()) {
       result += '/';
     } else {
-      // In case lastUpdateMillis field is populated in apex-info-list.xml, we
-      // prefer to use it as version scheme. If the field is missing we
-      // fallback to the version code of the APEX.
+      // In case lastUpdateMillis field is populated in apex-info-list.xml, we prefer to use it as
+      // version scheme.
+      // If the field is missing we fallback to the version code of the APEX. This only happens
+      // when apexd cannot stat the APEX, which should be very rare in practice because we would
+      // probably have crashed already due to apexd failing to mount the APEX.
       uint64_t version = info->second->hasLastUpdateMillis()
           ? info->second->getLastUpdateMillis()
           : info->second->getVersionCode();
@@ -1882,8 +1887,8 @@ bool Runtime::Init(RuntimeArgumentMap&& runtime_options_in) {
   // can't be trimmed as easily.
   const bool use_malloc = IsAotCompiler();
   if (use_malloc) {
-    arena_pool_.reset(new MallocArenaPool());
-    jit_arena_pool_.reset(new MallocArenaPool());
+    arena_pool_.reset(new CallocArenaPool());
+    jit_arena_pool_.reset(new CallocArenaPool());
   } else {
     arena_pool_.reset(new MemMapArenaPool(/* low_4gb= */ false));
     jit_arena_pool_.reset(new MemMapArenaPool(/* low_4gb= */ false, "CompilerMetadata"));
@@ -3146,6 +3151,17 @@ void Runtime::UpdateProcessState(ProcessState process_state) {
   ProcessState old_process_state = process_state_;
   process_state_ = process_state;
   GetHeap()->UpdateProcessState(old_process_state, process_state);
+
+  // When the application switches to the foreground, lock contention on classlinker_classes_lock_
+  // and priority inversion occasionally occur. Delay profile saving on hot/warm startup can reduce
+  // the occurrence of the problem. This feature depends on whether the
+  // jank_perceptible_narrow flag is enabled, which is only implemented and enabled on Android 16
+  // and above.On older Android versions, the process is always in kProcessStateJankPerceptible as
+  // long as it is not cached, so the profile saving delay is not applicable.
+  if (IsSdkVersionSetAndAtLeast(sdk_version_, SdkVersion::kB) &&
+      process_state == kProcessStateJankPerceptible) {
+    ProfileSaver::NotifyDelayProfileSaving();
+  }
 }
 
 void Runtime::RegisterSensitiveThread() const {
diff --git a/runtime/runtime.h b/runtime/runtime.h
index 8f781cff34..20e6276f82 100644
--- a/runtime/runtime.h
+++ b/runtime/runtime.h
@@ -29,6 +29,7 @@
 #include <vector>
 
 #include "app_info.h"
+#include "base/length_prefixed_array.h"
 #include "base/locks.h"
 #include "base/macros.h"
 #include "base/mem_map.h"
@@ -740,6 +741,10 @@ class Runtime {
     return is_running_on_memory_tool_;
   }
 
+  uint32_t GetSdkVersion() const {
+    return sdk_version_;
+  }
+
   void SetTargetSdkVersion(uint32_t version) {
     target_sdk_version_ = version;
   }
@@ -760,8 +765,6 @@ class Runtime {
     return (experimental_flags_ & flags) != ExperimentalFlags::kNone;
   }
 
-  void CreateJitCodeCache(bool rwx_memory_allowed);
-
   // Create the JIT and instrumentation and code cache.
   void CreateJit();
 
@@ -1191,6 +1194,9 @@ class Runtime {
 
   void DCheckNoTransactionCheckAllowed();
 
+  // Only used for testing.
+  void SetSdkVersion(uint32_t version) { sdk_version_ = version; }
+
   // Don't use EXPORT ("default" visibility), because quick_entrypoints_x86.o
   // refers to this symbol and it can't link with R_386_PC32 relocation.
   // A pointer to the active runtime or null.
@@ -1363,6 +1369,18 @@ class Runtime {
   // Specifies target SDK version to allow workarounds for certain API levels.
   uint32_t target_sdk_version_;
 
+  // SDK version of the running OS.
+  // Field's value is equal to `ro.build.version.sdk` system property if it stores a valid integer
+  // or 0 (`SdkVersion::kUnset`) otherwise.
+  //
+  // Note that this value does not take into account pre-release SDK codenames. To take into account
+  // pre-release SDK codenames, also check `ro.build.version.codename`.
+  //
+  // For making compile-time decisions, DO NOT rely on this value because it may not be correct in
+  // the Pre-reboot Dexopt case. Instead, use `AssumeValueOptions::SdkInt`-related properties as
+  // provided by `CompilerOptions`.
+  uint32_t sdk_version_;
+
   // ART counterpart for the compat framework (go/compat-framework).
   CompatFramework compat_framework_;
 
@@ -1549,12 +1567,16 @@ class Runtime {
   metrics::ArtMetrics metrics_;
   std::unique_ptr<metrics::MetricsReporter> metrics_reporter_;
 
-  // Apex versions of boot classpath jars concatenated in a string. The format
-  // is of the type:
-  // '/apex1_version/apex2_version//'
+  // Apex timestamps of boot classpath jars concatenated in a string, one timestamp per jar, in the
+  // same order as the boot classpath. Each entry is a slash (`/`) followed by the mtime of the
+  // owning apex, in seconds, stringified without leading zeros, indicating the apex install time.
+  // - If an apex contributes multiple jars to the boot classpath, the apex timestamp is repeated.
+  // - If an apex is in the factory version, we only encode a slash (`/`) (like the third and fourth
+  //   entries in the example below).
+  // - If a jar is not owned by an apex, we don't encode it at all (not even a slash).
   //
-  // When the apex is the factory version, we don't encode it (for example in
-  // the third entry in the example above).
+  // The format is of the type:
+  // '/apex_timestamp_1/apex_timestamp_2//'
   std::string apex_versions_;
 
   // The info about the application code paths.
diff --git a/runtime/runtime_callbacks_test.cc b/runtime/runtime_callbacks_test.cc
index 0fd9efa65e..ad360f3b93 100644
--- a/runtime/runtime_callbacks_test.cc
+++ b/runtime/runtime_callbacks_test.cc
@@ -177,7 +177,12 @@ TEST_F(ThreadLifecycleCallbackRuntimeCallbacksTest, ThreadLifecycleCallbackJava)
       hs.NewHandle(soa.Decode<mirror::Object>(runtime_->GetMainThreadGroup()));
   Handle<mirror::Object> thread =
       WellKnownClasses::java_lang_Thread_init->NewObject<'L', 'L', 'I', 'Z'>(
-          hs, self, thread_group, thread_name, kMinThreadPriority, /*daemon=*/ false);
+          hs,
+          self,
+          thread_group,
+          thread_name,
+          Thread::PriorityToNiceness(kMinThreadPriority),
+          /*daemon=*/false);
   ASSERT_FALSE(self->IsExceptionPending());
   ASSERT_TRUE(thread != nullptr);
 
diff --git a/runtime/runtime_image.cc b/runtime/runtime_image.cc
index 28f311a6f9..2086300bbf 100644
--- a/runtime/runtime_image.cc
+++ b/runtime/runtime_image.cc
@@ -861,23 +861,24 @@ class RuntimeImageHelper {
   void CopyFieldArrays(ObjPtr<mirror::Class> cls, uint32_t class_image_address)
       REQUIRES_SHARED(Locks::mutator_lock_) {
     LengthPrefixedArray<ArtField>* cur_fields = cls->GetFieldsPtr();
-    if (cur_fields != nullptr) {
-      // Copy the array.
-      size_t number_of_fields = cur_fields->size();
-      size_t size = LengthPrefixedArray<ArtField>::ComputeSize(number_of_fields);
-      size_t offset = art_fields_.size();
-      art_fields_.resize(offset + size);
-      auto* dest_array =
-          reinterpret_cast<LengthPrefixedArray<ArtField>*>(art_fields_.data() + offset);
-      memcpy(dest_array, cur_fields, size);
-      native_relocations_.Put(cur_fields,
-                              std::make_pair(NativeRelocationKind::kArtFieldArray, offset));
-
-      // Update the class pointer of individual fields.
-      for (size_t i = 0; i != number_of_fields; ++i) {
-        dest_array->At(i).GetDeclaringClassAddressWithoutBarrier()->Assign(
-            reinterpret_cast<mirror::Class*>(class_image_address));
-      }
+    if (HasNativeRelocation(cur_fields) || IsInBootImage(cur_fields)) {
+      return;
+    }
+    // Copy the array.
+    size_t number_of_fields = cur_fields->size();
+    size_t size = LengthPrefixedArray<ArtField>::ComputeSize(number_of_fields);
+    size_t offset = art_fields_.size();
+    art_fields_.resize(offset + size);
+    auto* dest_array =
+        reinterpret_cast<LengthPrefixedArray<ArtField>*>(art_fields_.data() + offset);
+    memcpy(dest_array, cur_fields, size);
+    native_relocations_.Put(cur_fields,
+                            std::make_pair(NativeRelocationKind::kArtFieldArray, offset));
+
+    // Update the class pointer of individual fields.
+    for (size_t i = 0; i != number_of_fields; ++i) {
+      dest_array->At(i).GetDeclaringClassAddressWithoutBarrier()->Assign(
+          reinterpret_cast<mirror::Class*>(class_image_address));
     }
   }
 
@@ -885,22 +886,23 @@ class RuntimeImageHelper {
                         uint32_t class_image_address,
                         bool is_class_initialized)
       REQUIRES_SHARED(Locks::mutator_lock_) {
-    size_t number_of_methods = cls->NumMethods();
-    if (number_of_methods == 0) {
+    LengthPrefixedArray<ArtMethod>* cur_methods = cls->GetMethodsPtr();
+    if (HasNativeRelocation(cur_methods) || IsInBootImage(cur_methods)) {
       return;
     }
-
+    size_t number_of_methods = cls->NumMethods();
+    DCHECK_NE(number_of_methods, 0u);
     size_t size = LengthPrefixedArray<ArtMethod>::ComputeSize(number_of_methods);
     size_t offset = art_methods_.size();
     art_methods_.resize(offset + size);
     auto* dest_array =
         reinterpret_cast<LengthPrefixedArray<ArtMethod>*>(art_methods_.data() + offset);
     memcpy(dest_array, cls->GetMethodsPtr(), size);
-    native_relocations_.Put(cls->GetMethodsPtr(),
+    native_relocations_.Put(cur_methods,
                             std::make_pair(NativeRelocationKind::kArtMethodArray, offset));
 
     for (size_t i = 0; i != number_of_methods; ++i) {
-      ArtMethod* method = &cls->GetMethodsPtr()->At(i);
+      ArtMethod* method = &cur_methods->At(i);
       ArtMethod* copy = &dest_array->At(i);
 
       // Update the class pointer.
diff --git a/runtime/signal_catcher.cc b/runtime/signal_catcher.cc
index c7e297f3b7..bb969c9f91 100644
--- a/runtime/signal_catcher.cc
+++ b/runtime/signal_catcher.cc
@@ -16,6 +16,10 @@
 
 #include "signal_catcher.h"
 
+#ifdef ART_TARGET_ANDROID
+#include <android/api-level.h>
+#endif
+
 #include <android-base/file.h>
 #include <android-base/stringprintf.h>
 #include <fcntl.h>
@@ -143,7 +147,10 @@ void SignalCatcher::HandleSigQuit() {
   os << "Debug Store: " << DebugStoreGetString() << "\n";
 
   if (art_flags::always_enable_profile_code()) {
-    os << "LongRunningMethods: " << TraceProfiler::GetLongRunningMethodsString() << "\n";
+    std::string str = TraceProfiler::GetLongRunningMethodsString();
+    if (!str.empty()) {
+      os << "LongRunningMethods: " << str << "\n";
+    }
   }
 
   runtime->DumpForSigQuit(os);
@@ -159,20 +166,56 @@ void SignalCatcher::HandleSigQuit() {
   sigquit_nanotime_ = std::nullopt;
 }
 
+void SignalCatcher::HandleMultiplexedSigUsr1(siginfo_t* info) {
+  if (info->si_code != SI_QUEUE) {
+    // If this signal is not generated from sigqueue, fallback to forcing a GC trigger.
+    HandleSigUsr1();
+    return;
+  }
+  // Use the signal's tag to call the correct handler.
+  uint8_t tag = std::bit_cast<uintptr_t>(info->si_value) & 0xFu;
+  switch (tag) {
+    case 0:
+      HandleLongMethodTracing(info);
+      break;
+    default:
+      LOG(WARNING) << "SIGUSR1 received via sigqueue with unknown tag: " << static_cast<int>(tag);
+      break;
+  }
+}
+
 void SignalCatcher::HandleSigUsr1() {
   LOG(INFO) << "SIGUSR1 forcing GC (no HPROF) and profile save";
   Runtime::Current()->GetHeap()->CollectGarbage(/* clear_soft_references= */ false);
   ProfileSaver::ForceProcessProfiles();
 }
 
-int SignalCatcher::WaitForSignal(Thread* self, SignalSet& signals) {
+void SignalCatcher::HandleLongMethodTracing(siginfo_t* info) {
+  if (info == nullptr) {
+    // The signaler didn't attach a duration, treat this case as a no-op.
+    return;
+  }
+
+  // Extract duration from the signal info
+  // si_value.sival_int contains the integer passed with the signal
+  // This value represent the trace duration in ms
+  // It is safe to ignore the tag here as we durations that are divisible by 16
+  uint64_t duration_ms = static_cast<uint64_t>(info->si_value.sival_int);
+
+  LOG(INFO) << "Received SIGUSR1 signal with long method tracing tag, enabling long method tracing"
+            << " for duration :" << duration_ms << " millis";
+  // Start the long method tracing with the specified duration (in nanoseconds)
+  TraceProfiler::StartTraceLongRunningMethods(duration_ms * 1000000ULL);
+}
+
+int SignalCatcher::WaitForSignal(Thread* self, SignalSet& signals, siginfo_t* info) {
   ScopedThreadStateChange tsc(self, ThreadState::kWaitingInMainSignalCatcherLoop);
 
   // Signals for sigwait() must be blocked but not ignored.  We
   // block signals like SIGQUIT for all threads, so the condition
   // is met.  When the signal hits, we wake up, without any signal
   // handlers being invoked.
-  int signal_number = signals.Wait();
+  int signal_number = signals.Wait(info);
   if (!ShouldHalt()) {
     // Let the user know we got the signal, just in case the system's too screwed for us to
     // actually do what they want us to do...
@@ -207,7 +250,8 @@ void* SignalCatcher::Run(void* arg) {
   signals.Add(SIGUSR1);
 
   while (true) {
-    int signal_number = signal_catcher->WaitForSignal(self, signals);
+    siginfo_t info;
+    int signal_number = signal_catcher->WaitForSignal(self, signals, &info);
     if (signal_catcher->ShouldHalt()) {
       runtime->DetachCurrentThread();
       return nullptr;
@@ -218,7 +262,11 @@ void* SignalCatcher::Run(void* arg) {
       signal_catcher->HandleSigQuit();
       break;
     case SIGUSR1:
-      signal_catcher->HandleSigUsr1();
+      if (kIsTargetAndroid && art_flags::always_enable_profile_code()) {
+        signal_catcher->HandleMultiplexedSigUsr1(&info);
+      } else {
+        signal_catcher->HandleSigUsr1();
+      }
       break;
     default:
       LOG(ERROR) << "Unexpected signal %d" << signal_number;
diff --git a/runtime/signal_catcher.h b/runtime/signal_catcher.h
index 4d5d71ede8..f6a97407ec 100644
--- a/runtime/signal_catcher.h
+++ b/runtime/signal_catcher.h
@@ -17,11 +17,12 @@
 #ifndef ART_RUNTIME_SIGNAL_CATCHER_H_
 #define ART_RUNTIME_SIGNAL_CATCHER_H_
 
+#include <csignal>
 #include <optional>
 
 #include "android-base/unique_fd.h"
-#include "base/mutex.h"
 #include "base/macros.h"
+#include "base/mutex.h"
 
 namespace art HIDDEN {
 
@@ -49,10 +50,12 @@ class SignalCatcher {
   static void* Run(void* arg) NO_THREAD_SAFETY_ANALYSIS;
 
   void HandleSigUsr1();
+  void HandleMultiplexedSigUsr1(siginfo_t* info);
+  void HandleLongMethodTracing(siginfo_t* info);
   void Output(const std::string& s);
   void SetHaltFlag(bool new_value) REQUIRES(!lock_);
   bool ShouldHalt() REQUIRES(!lock_);
-  int WaitForSignal(Thread* self, SignalSet& signals) REQUIRES(!lock_);
+  int WaitForSignal(Thread* self, SignalSet& signals, siginfo_t* info) REQUIRES(!lock_);
 
   mutable Mutex lock_ DEFAULT_MUTEX_ACQUIRED_AFTER;
   ConditionVariable cond_ GUARDED_BY(lock_);
diff --git a/runtime/signal_set.h b/runtime/signal_set.h
index 470319c9cb..ceda92d458 100644
--- a/runtime/signal_set.h
+++ b/runtime/signal_set.h
@@ -29,6 +29,7 @@
 #define sigaddset64 sigaddset
 #define pthread_sigmask64 pthread_sigmask
 #define sigwait64 sigwait
+#define sigwaitinfo64 sigwaitinfo
 #endif
 
 namespace art HIDDEN {
@@ -53,14 +54,15 @@ class SignalSet {
     }
   }
 
-  int Wait() {
+  int Wait(siginfo_t* info) {
     // Sleep in sigwait() until a signal arrives. gdb causes EINTR failures.
-    int signal_number;
-    int rc = TEMP_FAILURE_RETRY(sigwait64(&set_, &signal_number));
-    if (rc != 0) {
-      PLOG(FATAL) << "sigwait failed";
+    while (true) {
+      int signal_number = TEMP_FAILURE_RETRY(sigwaitinfo64(&set_, info));
+      if (signal_number > 0) {
+        return signal_number;
+      }
+      PLOG(FATAL) << "sigwaitinfo failed";
     }
-    return signal_number;
   }
 
  private:
diff --git a/runtime/thread.cc b/runtime/thread.cc
index 8d37d4006a..b8913eb847 100644
--- a/runtime/thread.cc
+++ b/runtime/thread.cc
@@ -33,11 +33,9 @@
 #include <sstream>
 
 #include "android-base/file.h"
+#include "android-base/macros.h"
 #include "android-base/stringprintf.h"
 #include "android-base/strings.h"
-
-#include "unwindstack/AndroidUnwinder.h"
-
 #include "arch/context-inl.h"
 #include "arch/context.h"
 #include "art_field-inl.h"
@@ -72,6 +70,7 @@
 #include "gc/space/space-inl.h"
 #include "gc_root.h"
 #include "handle_scope-inl.h"
+#include "handle_scope.h"
 #include "indirect_reference_table-inl.h"
 #include "instrumentation.h"
 #include "intern_table.h"
@@ -82,6 +81,7 @@
 #include "jni/jni_internal.h"
 #include "mirror/class-alloc-inl.h"
 #include "mirror/class_loader.h"
+#include "mirror/object.h"
 #include "mirror/object_array-alloc-inl.h"
 #include "mirror/object_array-inl.h"
 #include "mirror/stack_frame_info.h"
@@ -96,6 +96,7 @@
 #include "oat/oat_quick_method_header.h"
 #include "oat/stack_map.h"
 #include "obj_ptr-inl.h"
+#include "obj_ptr.h"
 #include "object_lock.h"
 #include "palette/palette.h"
 #include "quick/quick_method_frame_info.h"
@@ -106,13 +107,14 @@
 #include "runtime-inl.h"
 #include "runtime.h"
 #include "runtime_callbacks.h"
-#include "scoped_thread_state_change-inl.h"
 #include "scoped_disable_public_sdk_checker.h"
+#include "scoped_thread_state_change-inl.h"
 #include "stack.h"
 #include "thread-inl.h"
 #include "thread_list.h"
 #include "trace.h"
 #include "trace_profile.h"
+#include "unwindstack/AndroidUnwinder.h"
 #include "verify_object.h"
 #include "well_known_classes-inl.h"
 
@@ -651,6 +653,7 @@ void* Thread::CreateCallback(void* arg) {
     self->DeleteJPeer(self->GetJniEnv());
     self->SetThreadName(self->GetThreadName()->ToModifiedUtf8().c_str());
 
+    // Use priority rather than niceness field to enable Android S workaround.
     ArtField* priorityField = WellKnownClasses::java_lang_Thread_priority;
     self->SetNativePriority(priorityField->GetInt(self->tlsPtr_.opeer));
 
@@ -671,10 +674,40 @@ void* Thread::CreateCallback(void* arg) {
     if (should_unpark) {
       self->Unpark();
     }
-    // Invoke the 'run' method of our java.lang.Thread.
+
     ObjPtr<mirror::Object> receiver = self->tlsPtr_.opeer;
-    WellKnownClasses::java_lang_Thread_run->InvokeVirtual<'V'>(self, receiver);
+    ObjPtr<mirror::Object> runnable =
+        WellKnownClasses::java_lang_Thread_target->GetObject(receiver);
+    // When the runnable is a VirtualThreadContext, don't run thread.run() and treat it as a virtual
+    // thread.
+    if (kIsVirtualThreadEnabled &&
+        UNLIKELY(
+            !runnable.IsNull() &&
+            runnable->InstanceOf(WellKnownClasses::dalvik_system_VirtualThreadContext.Get()))) {
+      self->SetVirtualThreadFlags(VirtualThreadFlag::kIsVirtual, true);
+      ObjPtr<mirror::Object> parked_states =
+          WellKnownClasses::dalvik_system_VirtualThreadContext_parkedStates->GetObject(runnable);
+      if (parked_states != nullptr) {
+        self->SetVirtualThreadFlags(VirtualThreadFlag::kUnparking, true);
+      }
+
+      // Invoke the Runnable.run() method to avoid holding a reference of opeer in the managed
+      // stack.
+      WellKnownClasses::java_lang_Runnable_run->InvokeInterface<'V'>(self, runnable);
+
+      // When a virtual thread is parked, we expect and clear the VirtualThreadParkingError used to
+      // unwind the native stack.
+      if (self->IsExceptionPending() && self->IsVirtualThreadParking()) {
+        DCHECK(self->GetException()->GetClass()->DescriptorEquals(
+            "Ldalvik/system/VirtualThreadParkingError;"));
+        self->ClearException();
+      }
+    } else {
+      // Invoke the 'run' method of our java.lang.Thread.
+      WellKnownClasses::java_lang_Thread_run->InvokeVirtual<'V'>(self, receiver);
+    }
   }
+
   // Detach and delete self.
   Runtime::Current()->GetThreadList()->Unregister(self, /* should_run_callbacks= */ true);
 
@@ -1211,7 +1244,7 @@ void Thread::CreatePeer(const char* name, bool as_daemon, jobject thread_group)
     CHECK(self->IsExceptionPending());
     return;
   }
-  jint thread_priority = GetNativePriority();
+  jint thread_niceness = GetNativeNiceness();
 
   DCHECK(WellKnownClasses::java_lang_Thread->IsInitialized());
   Handle<mirror::Object> peer =
@@ -1222,7 +1255,7 @@ void Thread::CreatePeer(const char* name, bool as_daemon, jobject thread_group)
   }
   tlsPtr_.opeer = peer.Get();
   WellKnownClasses::java_lang_Thread_init->InvokeInstance<'V', 'L', 'L', 'I', 'Z'>(
-      self, peer.Get(), thr_group.Get(), thread_name.Get(), thread_priority, as_daemon);
+      self, peer.Get(), thr_group.Get(), thread_name.Get(), thread_niceness, as_daemon);
   if (self->IsExceptionPending()) {
     return;
   }
@@ -1236,17 +1269,10 @@ void Thread::CreatePeer(const char* name, bool as_daemon, jobject thread_group)
     // available (in the compiler, in tests), we manually assign the
     // fields the constructor should have set.
     if (runtime->IsActiveTransaction()) {
-      InitPeer<true>(tlsPtr_.opeer,
-                     as_daemon,
-                     thr_group.Get(),
-                     thread_name.Get(),
-                     thread_priority);
+      InitPeer<true>(tlsPtr_.opeer, as_daemon, thr_group.Get(), thread_name.Get(), thread_niceness);
     } else {
-      InitPeer<false>(tlsPtr_.opeer,
-                      as_daemon,
-                      thr_group.Get(),
-                      thread_name.Get(),
-                      thread_priority);
+      InitPeer<false>(
+          tlsPtr_.opeer, as_daemon, thr_group.Get(), thread_name.Get(), thread_niceness);
     }
     peer_thread_name.Assign(GetThreadName());
   }
@@ -1276,7 +1302,8 @@ ObjPtr<mirror::Object> Thread::CreateCompileTimePeer(const char* name,
     CHECK(self->IsExceptionPending());
     return nullptr;
   }
-  jint thread_priority = kNormThreadPriority;  // Always normalize to NORM priority.
+  // Always normalize to NORM priority.
+  jint thread_niceness = PriorityToNiceness(kNormThreadPriority);
 
   DCHECK(WellKnownClasses::java_lang_Thread->IsInitialized());
   Handle<mirror::Object> peer = hs.NewHandle(
@@ -1293,33 +1320,27 @@ ObjPtr<mirror::Object> Thread::CreateCompileTimePeer(const char* name,
   // available (in the compiler, in tests), we manually assign the
   // fields the constructor should have set.
   if (runtime->IsActiveTransaction()) {
-    InitPeer<true>(peer.Get(),
-                   as_daemon,
-                   thr_group.Get(),
-                   thread_name.Get(),
-                   thread_priority);
+    InitPeer<true>(peer.Get(), as_daemon, thr_group.Get(), thread_name.Get(), thread_niceness);
   } else {
-    InitPeer<false>(peer.Get(),
-                    as_daemon,
-                    thr_group.Get(),
-                    thread_name.Get(),
-                    thread_priority);
+    InitPeer<false>(peer.Get(), as_daemon, thr_group.Get(), thread_name.Get(), thread_niceness);
   }
 
   return peer.Get();
 }
 
-template<bool kTransactionActive>
+template <bool kTransactionActive>
 void Thread::InitPeer(ObjPtr<mirror::Object> peer,
                       bool as_daemon,
                       ObjPtr<mirror::Object> thread_group,
                       ObjPtr<mirror::String> thread_name,
-                      jint thread_priority) {
+                      jint thread_niceness) {
   WellKnownClasses::java_lang_Thread_daemon->SetBoolean<kTransactionActive>(peer,
       static_cast<uint8_t>(as_daemon ? 1u : 0u));
   WellKnownClasses::java_lang_Thread_group->SetObject<kTransactionActive>(peer, thread_group);
   WellKnownClasses::java_lang_Thread_name->SetObject<kTransactionActive>(peer, thread_name);
-  WellKnownClasses::java_lang_Thread_priority->SetInt<kTransactionActive>(peer, thread_priority);
+  WellKnownClasses::java_lang_Thread_niceness->SetInt<kTransactionActive>(peer, thread_niceness);
+  WellKnownClasses::java_lang_Thread_priority->SetInt<kTransactionActive>(
+      peer, NicenessToPriority(thread_niceness));
 }
 
 void Thread::SetCachedThreadName(const char* name) {
@@ -2046,7 +2067,8 @@ void Thread::DumpState(std::ostream& os, const Thread* thread, pid_t tid) {
   // cause ScopedObjectAccessUnchecked to deadlock.
   if (gAborting == 0 && self != nullptr && thread != nullptr && thread->tlsPtr_.opeer != nullptr) {
     ScopedObjectAccessUnchecked soa(self);
-    priority = WellKnownClasses::java_lang_Thread_priority->GetInt(thread->tlsPtr_.opeer);
+    priority = NicenessToPriority(
+        WellKnownClasses::java_lang_Thread_niceness->GetInt(thread->tlsPtr_.opeer));
     is_daemon = WellKnownClasses::java_lang_Thread_daemon->GetBoolean(thread->tlsPtr_.opeer);
 
     ObjPtr<mirror::Object> thread_group =
@@ -2060,10 +2082,19 @@ void Thread::DumpState(std::ostream& os, const Thread* thread, pid_t tid) {
           : "<null>";
     }
   } else if (thread != nullptr) {
+    // This produces niceness translated to a Java priority, which may not match the cached Java
+    // priority, and may have no relation to real scheduling priority if this was bumped to
+    // real-time priority. Except in the palette-fake case, this is always what it did, so change
+    // seems risky.
     priority = thread->GetNativePriority();
   } else {
-    palette_status_t status = PaletteSchedGetPriority(tid, &priority);
-    CHECK(status == PALETTE_STATUS_OK || status == PALETTE_STATUS_CHECK_ERRNO);
+    errno = 0;
+    int niceness = getpriority(PRIO_PROCESS, static_cast<id_t>(tid));
+    if (niceness == -1 && errno != 0) {
+      priority = -1;  // A recognizably bogus value.
+    } else {
+      priority = NicenessToPriority(niceness);
+    }
   }
 
   std::string scheduler_group_name(GetSchedulerGroupName(tid));
@@ -3775,10 +3806,10 @@ void Thread::DumpFromGdb() const {
   std::string str(ss.str());
   // log to stderr for debugging command line processes
   std::cerr << str;
-#ifdef ART_TARGET_ANDROID
-  // log to logcat for debugging frameworks processes
-  LOG(INFO) << str;
-#endif
+  if (kIsTargetAndroid) {
+    // log to logcat for debugging frameworks processes
+    LOG(INFO) << str;
+  }
 }
 
 // Explicitly instantiate 32 and 64bit thread offset dumping support.
@@ -4026,7 +4057,11 @@ std::unique_ptr<Context> Thread::QuickDeliverException(bool skip_method_exit_cal
   // listeners are installed and frame pop feature is supported.
   bool needs_deopt =
       instrumentation->HasMethodExitListeners() && Runtime::Current()->AreNonStandardExitsEnabled();
-  if (Dbg::IsForcedInterpreterNeededForException(this) || IsForceInterpreter() || needs_deopt) {
+  // parkVirtualInternal throws an exception when parking a virtual thread. It's not a
+  // deoptimization request.
+  bool is_parking_vthread = this->IsVirtualThreadParking();
+  if (!is_parking_vthread &&
+      (Dbg::IsForcedInterpreterNeededForException(this) || IsForceInterpreter() || needs_deopt)) {
     NthCallerVisitor visitor(this, 0, false);
     visitor.WalkStack();
     if (visitor.GetCurrentQuickFrame() != nullptr) {
@@ -4690,14 +4725,6 @@ void Thread::SetTlab(uint8_t* start, uint8_t* end, uint8_t* limit) {
 
 void Thread::ResetTlab() {
   gc::Heap* const heap = Runtime::Current()->GetHeap();
-  if (heap->GetHeapSampler().IsEnabled()) {
-    // Note: We always ResetTlab before SetTlab, therefore we can do the sample
-    // offset adjustment here.
-    heap->AdjustSampleOffset(GetTlabPosOffset());
-    VLOG(heap) << "JHP: ResetTlab, Tid: " << GetTid()
-               << " adjustment = "
-               << (tlsPtr_.thread_local_pos - tlsPtr_.thread_local_start);
-  }
   SetTlab(nullptr, nullptr, nullptr);
 }
 
@@ -4885,16 +4912,214 @@ void Thread::ClearAllInterpreterCaches() {
   Runtime::Current()->GetThreadList()->RunCheckpoint(&closure);
 }
 
-void Thread::SetNativePriority(int new_priority) {
-  palette_status_t status = PaletteSchedSetPriority(GetTid(), new_priority);
-  CHECK(status == PALETTE_STATUS_OK || status == PALETTE_STATUS_CHECK_ERRNO);
-}
+static_assert(kMinThreadPriority >= 0);
+
+// Use PaletteSchedSetPriority on host for testing. This should set canSetPriority to false,
+// but not crash.
+static constexpr bool kUseFakeOnHost = false;
 
-int Thread::GetNativePriority() const {
-  int priority = 0;
-  palette_status_t status = PaletteSchedGetPriority(GetTid(), &priority);
-  CHECK(status == PALETTE_STATUS_OK || status == PALETTE_STATUS_CHECK_ERRNO);
-  return priority;
+static bool canSetPriority = true;  // If false, we skip attempting to set OS priority.
+
+// Android S, does more than setting niceness in PaletteSchedSetPriority, making it unsafe to use
+// that in Zygote, and making it desirable (for risk minimization, at least) to actually call it
+// when expected.
+inline bool NeedSWorkaround() {
+  static bool needSWorkaround = false;
+  static std::once_flag sWorkaroundInitialized;
+  std::call_once(sWorkaroundInitialized, []() {
+    if (kIsTargetAndroid) {
+      if (android::base::GetIntProperty("ro.build.version.sdk", 0) <= 32) {
+        needSWorkaround = true;
+      }
+    } else if (!kUseFakeOnHost) {
+      // Priority setting is often restricted on host. Just fake it, as for S.
+      // TODO: Fuchsia may require attention here.
+      needSWorkaround = true;
+      canSetPriority = false;
+    }
+  });
+  return needSWorkaround;
+}
+
+// Return an int array result, so that result[i] is the native priority, really
+// "niceness" corresponding to Java priority i. result[0] is unused.
+int* Thread::GetPriorityMap() {
+  static int priorityMap[kMaxThreadPriority + 1];
+  static std::once_flag priorityMapInitialized;
+  // For Android S, PaletteSchedSetPriority is unsafe in the zygote, since it leaves a file
+  // descriptor open, which must crash zygote. We could possibly postpone discovering the map, but
+  // that adds a few dozen system calls in each child. We instead simply assume the historical map
+  // that shipped with Android S.
+  // Deviating from this is questionable anyway, since a lot of Android code at all levels sets
+  // niceness directly without going through the Palette API.  On Android S, we continue to use
+  // PaletteSchedSetPriority to set Java priorities, but we cache the niceness values given here,
+  // and return those. Thus actual thread niceness values will not reflect those here, but pure
+  // Java behavior should be consistent. This is similar to what happens when framework code
+  // alters thread priorities externally, so we should be OK.
+  static int traditional_priority_map[] = {0 /*unused*/, 19, 16, 13, 10, 0, -2, -4, -5, -6, -8};
+  const char* failure_msg = nullptr;
+
+  if (NeedSWorkaround()) {
+    static bool warned = false;
+    if (!warned) {
+      LOG(WARNING) << "Using default priority map due to SDK version";
+      warned = true;
+    }
+    return traditional_priority_map;
+  }
+  std::call_once(priorityMapInitialized, [&pm = priorityMap, &failure_msg]() {
+  // CHECKs in this function should be avoided. Dump calls will invoke this recursively.
+#define CHECK_DEFERRED_ABORT(pred, msg) \
+  if (!(pred)) {                        \
+    failure_msg = (msg);                \
+    return;                             \
+  }
+    bool need_fake = false;  // Saw an anomaly requiring us to fake the map?
+    bool success = true;
+    bool saw_difference = true;  // Do we map to different niceness values?
+                                 // PaletteMapPriority always yields nontrivial mapping.
+    for (int p = kMinThreadPriority; p <= kMaxThreadPriority; ++p) {
+      palette_status_t result = PaletteMapPriority(p, &pm[p]);
+      if (result == PALETTE_STATUS_NOT_SUPPORTED) {
+        success = false;
+        break;
+      }
+      CHECK_DEFERRED_ABORT(result == PALETTE_STATUS_OK, "Bad PALLETTE_STATUS");
+    }
+    if (!success) {
+      // Discover the map the hard way.
+      int32_t me = static_cast<int32_t>(::art::GetTid());
+      bool map_consistent;
+      errno = 0;
+      int orig_niceness = getpriority(PRIO_PROCESS, 0 /* self */);
+      CHECK_DEFERRED_ABORT(orig_niceness != -1 || errno == 0, "getpriority() failed");
+      constexpr int kMaxIters = 10;
+      int iters = 0;
+      do {
+        map_consistent = true;
+        ++iters;
+        saw_difference = false;
+        CHECK_DEFERRED_ABORT(iters <= kMaxIters, "iters > kMaxIters");
+        // Start checking from higher priorities, since that is most likely to fail, and we may
+        // have trouble undoing the damage if we don't detect the problem immediately.
+        for (int p = kMaxThreadPriority; p >= kMinThreadPriority; --p) {
+          int ret = PaletteSchedSetPriority(me, p);
+          if (ret == PALETTE_STATUS_OK) {
+            errno = 0;
+            pm[p] = getpriority(PRIO_PROCESS, 0 /* self */);
+            // If we always get the same value, we're dealing with a fake, and need to fake a
+            // consistent result here.
+            if (!saw_difference && pm[p] != pm[kMaxThreadPriority]) {
+              if (p == kMaxThreadPriority - 1) {
+                saw_difference = true;
+              } else {
+                // We saw several identical values, which is wrong.
+                // Force a complete pass with checking.
+                map_consistent = false;
+                break;
+              }
+            }
+            CHECK_DEFERRED_ABORT(pm[p] != -1 || errno == 0, "2nd getpriority() failed");
+            VLOG(threads) << "Niceness[" << p << "] = " << pm[p];
+            if (saw_difference) {
+              // With a non-fake PaletteSchedSetPriority the map should be strictly monotonically
+              // decreasing.
+              if (p < kMaxThreadPriority && pm[p] <= pm[p + 1]) {
+                // Maybe somebody else mucked with our priority? Start over.
+                map_consistent = false;
+                break;
+              }
+            }
+          } else {
+            need_fake = true;
+            break;
+          }
+        }
+      } while (!map_consistent && !need_fake);
+      int ret = setpriority(PRIO_PROCESS, static_cast<id_t>(me), orig_niceness);
+      CHECK_DEFERRED_ABORT(ret == 0, "setpriority() failed");
+    }
+    if (!saw_difference || need_fake) {
+      // Palette calls don't impact getpriority(), as with the traditional
+      // PaletteSetSchedPriority fake on host.
+      canSetPriority = false;
+      LOG(WARNING) << "Failed to retrieve monotonic priority map : faking it";
+      // Make the map monotonic, so we can map priority to niceness and back without losing
+      // information.
+      for (int p = kMinThreadPriority; p <= kMaxThreadPriority; ++p) {
+        pm[p] = 5 - p;
+      }
+    }
+    std::ostringstream priority_map_string;
+    for (int p = kMinThreadPriority; p <= kMaxThreadPriority; ++p) {
+      if (p != kMinThreadPriority) {
+        priority_map_string << ", ";
+      }
+      priority_map_string << priorityMap[p];
+    }
+    LOG(INFO) << "Priority-to-niceness mapping: " << priority_map_string.str();
+#undef CHECK_DEFERRED_ABORT
+  });
+  if (failure_msg != nullptr) {
+    // Calls to GetPriorityMap() during dumping must return plausible values.
+    for (int p = kMinThreadPriority; p <= kMaxThreadPriority; ++p) {
+      priorityMap[p] = 5 - p;
+    }
+    LOG(FATAL) << failure_msg;
+  }
+  return priorityMap;
+}
+
+// Many niceness values don't correspond to a priority. Find and return a close one.
+int Thread::NicenessToPriority(int niceness) {
+  int* pm = GetPriorityMap();
+  int* bound = std::lower_bound(pm + kMinThreadPriority,
+                                pm + kMaxThreadPriority + 1,
+                                niceness,
+                                std::greater() /* niceness decreases */);
+  if (bound > pm + kMaxThreadPriority) {
+    return kMaxThreadPriority;
+  }
+  if (bound == pm + kMinThreadPriority) {
+    return kMinThreadPriority;
+  }
+  // The closest is either bound[0] or bound[-1].
+  DCHECK_LE(bound[0], niceness);
+  DCHECK_GT(bound[-1], niceness);
+  // Resolve ties towards the higher priority. This usually maps system daemon priority to Java
+  // normal priority, which is the traditional behavior we test for.
+  return static_cast<int>((niceness - bound[0] > bound[-1] - niceness) ? bound - pm - 1
+                                                                       : bound - pm);
+}
+
+int Thread::SetNativeNiceness(int niceness) {
+  int ret = setpriority(PRIO_PROCESS, static_cast<id_t>(GetTid()), niceness);
+  if (ret == 0) {
+    return 0;
+  }
+  LOG(WARNING) << "Cannot set niceness to " << niceness;
+  // TODO: With PaletteMapPriority we may want to do more here.
+  return errno;
+}
+
+int Thread::GetNativeNiceness() const {
+  errno = 0;
+  int niceness = getpriority(PRIO_PROCESS, static_cast<id_t>(GetTid()));
+  CHECK(niceness != -1 || errno == 0);
+  return niceness;
+}
+
+int Thread::SetNativePriority(int new_priority) {
+  int n = PriorityToNiceness(new_priority);
+  if (canSetPriority) {
+    if (UNLIKELY(NeedSWorkaround())) {
+      palette_status_t status = PaletteSchedSetPriority(GetTid(), new_priority);
+      CHECK(status == PALETTE_STATUS_OK || status == PALETTE_STATUS_CHECK_ERRNO);
+    } else {
+      SetNativeNiceness(n);
+    }
+  }
+  return n;
 }
 
 void Thread::AbortInThis(const std::string& message) {
diff --git a/runtime/thread.h b/runtime/thread.h
index 4a675d4c99..6c33bcc051 100644
--- a/runtime/thread.h
+++ b/runtime/thread.h
@@ -17,8 +17,11 @@
 #ifndef ART_RUNTIME_THREAD_H_
 #define ART_RUNTIME_THREAD_H_
 
+#include <android-base/properties.h>
+
 #include <atomic>
 #include <bitset>
+#include <cstdint>
 #include <deque>
 #include <iosfwd>
 #include <list>
@@ -33,6 +36,7 @@
 #include "base/pointer_size.h"
 #include "base/safe_map.h"
 #include "base/value_object.h"
+#include "com_android_art_flags.h"
 #include "entrypoints/jni/jni_entrypoints.h"
 #include "entrypoints/quick/quick_entrypoints.h"
 #include "handle.h"
@@ -200,6 +204,15 @@ enum class WeakRefAccessState : int32_t {
   kDisabled
 };
 
+enum VirtualThreadFlag : uint8_t {
+  // This flag is set only when a virtual thread is running on the given carrier thread.
+  kIsVirtual = 1u,
+  // The flag is set when a virtual thread is being parked and unmounted from the carrier thread.
+  kParking = 1u << 1,
+  // The flag is set when a virtual thread is being unparked and mounted from the carrier thread.
+  kUnparking = 1u << 2,
+};
+
 // ART uses two types of ABI/code: quick and native.
 //
 // Quick code includes:
@@ -239,6 +252,10 @@ static constexpr StackType kNativeStackType = StackType::kHardware;
 // kHardware stack.
 static constexpr StackType kQuickStackType = StackType::kHardware;
 
+static_assert(com::android::art::flags::virtual_thread_impl_v1() ==
+              COM_ANDROID_ART_FLAGS_VIRTUAL_THREAD_IMPL_V1);
+static constexpr bool kIsVirtualThreadEnabled = com::android::art::flags::virtual_thread_impl_v1();
+
 // See Thread.tlsPtr_.active_suspend1_barriers below for explanation.
 struct WrappedSuspend1Barrier {
   // TODO(b/323668816): At least weaken CHECKs to DCHECKs once the bug is fixed.
@@ -606,12 +623,31 @@ class EXPORT Thread {
   bool HoldsLock(ObjPtr<mirror::Object> object) const REQUIRES_SHARED(Locks::mutator_lock_);
 
   /*
-   * Changes the priority of this thread to match that of the java.lang.Thread object.
+   * Set native thread niceness to match the given Java priority.
    *
    * We map a priority value from 1-10 to Linux "nice" values, where lower
    * numbers indicate higher priority.
+   *
+   * Return the niceness value corresponding to the priority.
+   */
+  int SetNativePriority(int newPriority) REQUIRES_SHARED(Locks::mutator_lock_);
+
+  /*
+   * Same thing, but niceness is supplied directly.
+   *
+   * Return 0 on success, or errno.
+   */
+  int SetNativeNiceness(int newNiceness) REQUIRES_SHARED(Locks::mutator_lock_);
+
+  /*
+   * Convert Java priority to Posix niceness using palette information.
    */
-  void SetNativePriority(int newPriority);
+  static int PriorityToNiceness(int priority) { return GetPriorityMap()[priority]; }
+
+  /*
+   * Convert Posix niceness to the closest Java priority using palette information.
+   */
+  static int NicenessToPriority(int niceness);
 
   /*
    * Returns the priority of this thread by querying the system.
@@ -619,7 +655,14 @@ class EXPORT Thread {
    *
    * Returns a value from 1 to 10 (compatible with java.lang.Thread values).
    */
-  int GetNativePriority() const;
+  int GetNativePriority() const { return NicenessToPriority(GetNativeNiceness()); }
+
+  /*
+   * Return Posix niceness instead of Java priority. A very thin wrapper over getpriority().  May
+   * be inconsistent with PaletteSchedSetPriority, especially if that doesn't actually adjust
+   * priorities.
+   */
+  int GetNativeNiceness() const;
 
   // Guaranteed to be non-zero.
   uint32_t GetThreadId() const {
@@ -870,6 +913,28 @@ class EXPORT Thread {
   void Park(bool is_absolute, int64_t time) REQUIRES_SHARED(Locks::mutator_lock_);
   void Unpark();
 
+  ALWAYS_INLINE void SetVirtualThreadFlags(uint8_t flags_mask, bool enabled) {
+    if (enabled) {
+      virtual_thread_flags = virtual_thread_flags | flags_mask;
+    } else {
+      virtual_thread_flags = virtual_thread_flags & (~flags_mask);
+    }
+  }
+
+  ALWAYS_INLINE bool IsVirtualThreadParking() const {
+    return AreVirtualThreadFlagsEnabled(VirtualThreadFlag::kIsVirtual |
+                                        VirtualThreadFlag::kParking);
+  }
+
+  ALWAYS_INLINE bool IsVirtualThreadUnparking() const {
+    return AreVirtualThreadFlagsEnabled(VirtualThreadFlag::kIsVirtual |
+                                        VirtualThreadFlag::kUnparking);
+  }
+
+  ALWAYS_INLINE bool AreVirtualThreadFlagsEnabled(uint8_t flags_mask) const {
+    return (virtual_thread_flags & flags_mask) == flags_mask;
+  }
+
  private:
   void NotifyLocked(Thread* self) REQUIRES(wait_mutex_);
 
@@ -1519,6 +1584,11 @@ class EXPORT Thread {
     tlsPtr_.suspend_trigger.store(reinterpret_cast<uintptr_t*>(&tlsPtr_.suspend_trigger),
                                   std::memory_order_relaxed);
   }
+  // Check the suspend trigger value. This is not the way we normally check for suspension, but
+  // can be used to explicitly propagate the value to the suspend check register.
+  bool IsSuspendTriggerSet() {
+    return tlsPtr_.suspend_trigger.load(std::memory_order_relaxed) == nullptr;
+  }
 
   // Trigger a suspend check by making the suspend_trigger_ TLS value an invalid pointer.
   // The next time a suspend check is done, it will load from the value at this address
@@ -1577,7 +1647,8 @@ class EXPORT Thread {
   }
 
   bool IsForceInterpreter() const {
-    return tls32_.force_interpreter_count != 0;
+    return (tls32_.force_interpreter_count != 0) ||
+           AreVirtualThreadFlagsEnabled(VirtualThreadFlag::kIsVirtual);
   }
 
   bool IncrementMakeVisiblyInitializedCounter() {
@@ -1897,6 +1968,8 @@ class EXPORT Thread {
 
   static bool IsAotCompiler();
 
+  static int* GetPriorityMap();
+
   void SetCachedThreadName(const char* name);
 
   // Helper functions to get/set the tls stack pointer variables.
@@ -2200,48 +2273,49 @@ class EXPORT Thread {
   } tls64_;
 
   struct alignas(sizeof(void*)) tls_ptr_sized_values {
-      tls_ptr_sized_values() : card_table(nullptr),
-                               exception(nullptr),
-                               stack_end(nullptr),
-                               managed_stack(),
-                               suspend_trigger(nullptr),
-                               jni_env(nullptr),
-                               tmp_jni_env(nullptr),
-                               self(nullptr),
-                               opeer(nullptr),
-                               jpeer(nullptr),
-                               stack_begin(nullptr),
-                               stack_size(0),
-                               deps_or_stack_trace_sample(),
-                               wait_next(nullptr),
-                               monitor_enter_object(nullptr),
-                               top_handle_scope(nullptr),
-                               class_loader_override(nullptr),
-                               stacked_shadow_frame_record(nullptr),
-                               deoptimization_context_stack(nullptr),
-                               frame_id_to_shadow_frame(nullptr),
-                               name(nullptr),
-                               pthread_self(0),
-                               active_suspendall_barrier(nullptr),
-                               active_suspend1_barriers(nullptr),
-                               thread_local_pos(nullptr),
-                               thread_local_end(nullptr),
-                               thread_local_start(nullptr),
-                               thread_local_limit(nullptr),
-                               thread_local_objects(0),
-                               checkpoint_function(nullptr),
-                               thread_local_alloc_stack_top(nullptr),
-                               thread_local_alloc_stack_end(nullptr),
-                               mutator_lock(nullptr),
-                               flip_function(nullptr),
-                               thread_local_mark_stack(nullptr),
-                               async_exception(nullptr),
-                               top_reflective_handle_scope(nullptr),
-                               method_trace_buffer(nullptr),
-                               method_trace_buffer_curr_entry(nullptr),
-                               thread_exit_flags(nullptr),
-                               last_no_thread_suspension_cause(nullptr),
-                               last_no_transaction_checks_cause(nullptr) {
+    tls_ptr_sized_values()
+        : card_table(nullptr),
+          exception(nullptr),
+          stack_end(nullptr),
+          managed_stack(),
+          suspend_trigger(nullptr),
+          jni_env(nullptr),
+          tmp_jni_env(nullptr),
+          self(nullptr),
+          opeer(nullptr),
+          jpeer(nullptr),
+          stack_begin(nullptr),
+          stack_size(0),
+          deps_or_stack_trace_sample(),
+          wait_next(nullptr),
+          monitor_enter_object(nullptr),
+          top_handle_scope(nullptr),
+          class_loader_override(nullptr),
+          stacked_shadow_frame_record(nullptr),
+          deoptimization_context_stack(nullptr),
+          frame_id_to_shadow_frame(nullptr),
+          name(nullptr),
+          pthread_self(0),
+          active_suspendall_barrier(nullptr),
+          active_suspend1_barriers(nullptr),
+          thread_local_pos(nullptr),
+          thread_local_end(nullptr),
+          thread_local_start(nullptr),
+          thread_local_limit(nullptr),
+          thread_local_objects(0),
+          checkpoint_function(nullptr),
+          thread_local_alloc_stack_top(nullptr),
+          thread_local_alloc_stack_end(nullptr),
+          mutator_lock(nullptr),
+          flip_function(nullptr),
+          thread_local_mark_stack(nullptr),
+          async_exception(nullptr),
+          top_reflective_handle_scope(nullptr),
+          method_trace_buffer(nullptr),
+          method_trace_buffer_curr_entry(nullptr),
+          thread_exit_flags(nullptr),
+          last_no_thread_suspension_cause(nullptr),
+          last_no_transaction_checks_cause(nullptr) {
       std::fill(held_mutexes, held_mutexes + kLockLevelCount, nullptr);
     }
 
@@ -2446,6 +2520,12 @@ class EXPORT Thread {
   // Debug disable read barrier count, only is checked for debug builds and only in the runtime.
   uint8_t debug_disallow_read_barrier_ = 0;
 
+  // The flag value should only be accessed by the carrier thread itself.
+  // When a virtual thread is mounted onto this carrier thread, this flag value is
+  // non-zero. See VirtualThreadFlag for the details.
+  // For a regular java thread, this value is always zero.
+  uint8_t virtual_thread_flags = 0;
+
   // Counters used only for debugging and error reporting.  Likely to wrap.  Small to avoid
   // increasing Thread size.
   // We currently maintain these unconditionally, since it doesn't cost much, and we seem to have
diff --git a/runtime/thread_list.cc b/runtime/thread_list.cc
index 60e3d727f3..ae8043b129 100644
--- a/runtime/thread_list.cc
+++ b/runtime/thread_list.cc
@@ -786,7 +786,8 @@ std::optional<std::string> ThreadList::WaitForSuspendBarrier(AtomicInteger* barr
   if (attempt_of_4 != 1) {
     // TODO: RequestSynchronousCheckpoint routinely passes attempt_of_4 = 0. Can
     // we avoid the getpriority() call?
-    if (getpriority(PRIO_PROCESS, 0 /* this thread */) > 0) {
+    if (getpriority(PRIO_PROCESS, 0 /* this thread */) >
+        Thread::PriorityToNiceness(kNormThreadPriority)) {
       // We're a low priority thread, and thus have a longer ANR timeout. Increase the suspend
       // timeout.
       avg_wait_multiplier = 3;
@@ -797,6 +798,7 @@ std::optional<std::string> ThreadList::WaitForSuspendBarrier(AtomicInteger* barr
     wait_multiplier = attempt_of_4 == 4 ? 2 * avg_wait_multiplier - 1 : avg_wait_multiplier;
     timeout_ns *= wait_multiplier;
   }
+  DCHECK_LE(attempt_of_4, 4);
   bool collect_state = (t != 0 && (attempt_of_4 == 0 || attempt_of_4 == 4));
   int32_t cur_val = barrier->load(std::memory_order_acquire);
   if (cur_val <= 0) {
diff --git a/runtime/thread_list.h b/runtime/thread_list.h
index a1f823572a..9bc5ef6a8b 100644
--- a/runtime/thread_list.h
+++ b/runtime/thread_list.h
@@ -90,6 +90,7 @@ class ThreadList {
 
   // Suspend a thread using a peer, typically used by the debugger. Returns the thread on success,
   // else null. The peer is used to identify the thread to avoid races with the thread terminating.
+  // Aborts on timeout.
   EXPORT Thread* SuspendThreadByPeer(jobject peer, SuspendReason reason)
       REQUIRES(!Locks::mutator_lock_,
                !Locks::thread_list_lock_,
@@ -98,8 +99,9 @@ class ThreadList {
   // Suspend a thread using its thread id, typically used by lock/monitor inflation. Returns the
   // thread on success else null. The thread id is used to identify the thread to avoid races with
   // the thread terminating. Note that as thread ids are recycled this may not suspend the expected
-  // thread, that may be terminating. 'attempt_of_4' is zero if this is the only
-  // attempt, or 1..4 to try 4 times with fractional timeouts.
+  // thread, that may be terminating. 'attempt_of_4' is zero if this is the only attempt, or 1..4
+  // to try 4 times with fractional timeouts. Aborts on timeout during the final attempt, but not
+  // if thread exited in the meantime.
   // TODO: Reconsider the use of thread_id, now that we have ThreadExitFlag.
   Thread* SuspendThreadByThreadId(uint32_t thread_id, SuspendReason reason, int attempt_of_4 = 0)
       REQUIRES(!Locks::mutator_lock_,
@@ -279,7 +281,8 @@ class ThreadList {
   // the caller guarantees that *thread is valid until that is released.  We "release the mutator
   // lock", by switching to self_state.  'attempt_of_4' is 0 if we only attempt once, and 1..4 if
   // we are going to try 4 times with a quarter of the full timeout. 'func_name' is used only to
-  // identify ourselves for logging.
+  // identify ourselves for logging. Aborts if we time out on the final (attempt_of_4 is 0 or 4)
+  // attempt.
   bool SuspendThread(Thread* self,
                      Thread* thread,
                      SuspendReason reason,
diff --git a/runtime/thread_pool.cc b/runtime/thread_pool.cc
index 80fd3587ae..fa6d5cfc62 100644
--- a/runtime/thread_pool.cc
+++ b/runtime/thread_pool.cc
@@ -154,8 +154,7 @@ void* ThreadPoolWorker::Callback(void* arg) {
   runtime->DetachCurrentThread(/* should_run_callbacks= */ false);
   // On zygote fork, we wait for this thread to exit completely. Set to highest Java priority
   // to speed that up.
-  constexpr int kJavaMaxPrioNiceness = -8;
-  SetPriorityForTid(0 /* this thread */, kJavaMaxPrioNiceness);
+  SetPriorityForTid(0 /* this thread */, Thread::PriorityToNiceness(kMaxThreadPriority));
   return nullptr;
 }
 
diff --git a/runtime/thread_priorities.md b/runtime/thread_priorities.md
new file mode 100644
index 0000000000..ec74c56852
--- /dev/null
+++ b/runtime/thread_priorities.md
@@ -0,0 +1,220 @@
+Thread Priorities in Android
+----------------------------
+
+[TODO(b/389104950) Some of the following is currently more aspirational than fact. Implementation
+is in progress. ]
+
+Unfortunately, this topic relies on several different kinds of thread "priority", which we will
+try to distinguish as follows:
+
+1. The priority argument passed to Posix `setpriority()`, which affects Linux threads using
+   `SCHED_OTHER` scheduling. These range in value from -20 to 19, with larger values correspond to
+   *lower* scheduling priority. We will refer to these as "niceness". A difference of 1 in
+   niceness corresponds to something like a factor of 1.25 in processor share when cores are
+   overcommitted. Details may change as the Linux scheduler changes.
+
+2. Java thread priorities. In practice, these range from 1 to 10, with higher values corresponding
+   to higher scheduling priority. We refer to these as "java-priorities".
+
+3. The value of the `sched_priority` field, for example in the `sched_param` struct passed to
+   `sched_setparam()`. This affects only `SCHED_FIFO` and `SCHED_RR` scheduling policies. The
+   latter is not normally used in Android. The former has some limited uses as described below.
+   These threads always have higher scheduling priority than normal `SCHED_OTHER` threads, and
+   they can effectively preclude even essential system functions from running. Larger values
+   correspond to higher scheduling priority. We will refer to these as "rt-priorities".
+
+ART initially assumed that Java threads always used `SCHED_OTHER` Linux thread scheduling. Under
+this discipline all threads get a chance to run, even when cores are overcommitted. However the
+share of cpu time allocated to a particular thread varies greatly, by up to a factor of thousands,
+depending on its "niceness", as set by `setpriority()` (or the swiss-army-knife `sched_setattr()`).
+
+In general java-priorities are mapped, via the platform supplied `libartpalette` to corresponding
+niceness values for `SCHED_OTHER` threads, These niceness values are then installed with
+`setpriority()`.
+
+The above assumptions are still generally true, but there are two important exceptions:
+
+1. ART occasionally assigns niceness values for internal daemon threads that are obtained by
+   interpolating between java-priorities.  These niceness values do not map perfectly back onto
+   java-priorities. Thread.getPriority() rounds them to the nearest java-priority.
+
+2. Frameworks application management code may temporarily promote certain threads to `SCHED_FIFO`
+   scheduling with an associated rt-priority. As mentioned above, such threads cannot be preempted
+   by `SCHED_OTHER` threads. There are currently no clear rules about which threads may run under
+   `SCHED_FIFO` scheduling. In order to prevent deadlocks, all ART, libcore, and frameworks code
+   should be written so that it can safely run under `SCHED_FIFO`. This means that wait loops that
+   either spin consuming CPU time, or attempt to release it by calling `sched_yield()` or
+   `Thread.yield()` are *incorrect*. At a minimum they should eventually sleep for short periods.
+   Preferably, they should call an OS waiting primitive. Mutex acquisition, or Java's
+   `Object.wait()` do so internally.
+
+How to set priorities
+---------------------
+
+Java code is arguably allowed to expect that `Thread.getPriority()` returns the last value passed
+to `Thread.setPriority()`, at least unless the application itself calls Android-specific code or
+native code documented to alter thread priorities. This argues that `Thread.getPriority()` should
+cache the current priority. Historically, this has also been done for performance reasons. We
+continue to do so, though we now cache niceness rather than java-priority.
+
+In order to keep this cache accurate, applications should try to set priorities via
+`Thread.setPriority(int)`. `android.os.Process.setThreadPriority(int)` may be used to set any
+niceness value, and interacts correctly with the `Thread.getPriority()`. (This treatment
+of `android.os.Process.setThreadPriority(int)` is currently still future work.)
+
+All other mechanisms, including `android.os.Process.setThreadPriority(int, int)` (with a thread id
+parameter) bypass this cache. This has two effects:
+
+1. The change of priority is invisible to `Thread.getPriority()`.
+
+2. If the effect is to set a positive (> 0, i.e. low priority) niceness value, that effect may be
+   temporary, since ART, or the Java appliction may undo the effect.
+
+Avoiding races in setting priorities
+------------------------------------
+
+Priorities may be set either from within the process itself, or by external frameworks components.
+If multiple threads or external components try to set the priority of a thread at the same time,
+this is prone to races. For example:
+
+Consider a thread *A* that temporarily wants to raise its own priority from *x* to *y* and then
+restore it to the initial priority *x*. If another thread happens to also raise its priority to
+*y* while it is already temporarily raised to *y*, resulting in no immediate change, *A* will have
+no way to tell that it should not lower the priority back to *x*. The update by the second thread
+can thus be lost.
+
+In order to minimize such issues, all components should avoid racing priority updates and play by
+the following rules:
+
+1. The application is entitled to adjust its own thread priorities. Everything else should
+   interfere with that as little as possible. The application should adjust priorities using
+   either `Thread.setPriority(int)` or `android.os.Process.setThreadPriority(int)`. JNI code that
+   updates the thread priority only for the duration of the call, may be able to adjust its
+   own priorities using OS calls, using something like the ART protocol outlined below,
+   so long as the JNI code does not call back into Java code sensitive to thread priorities.
+   The application should generally restore its priority to an expected value, such as a saved
+   value previously read via `Thread.getPriority()`, so that the value it restores is unaffected by
+   by priority adjustment from another process.
+
+2. ART may temporarily change the priority of an application thread with positive niceness to a
+   lower non-negative niceness value (usually zero), and then restore it it to the cached value as
+   detailed below.  This happens only from within that thread itself.  This is necessary, since
+   threads with large positive niceness may become esentially unresponsive under heavy load, and
+   may thus block essential runtime operations.
+
+3. External process priority updates, usually by another process, should not use mechanisms that
+   affect the cached value, specifically `android.os.Process.setThreadPriority(int)` or
+   `Thread.setPriority(int)`, so that such temporary updates do not result in the application
+   restoring the wrong value after a temprary priority adjustment. This is normally automatic,
+   since a cross-process niceness change doesn't affect the cached value.
+
+4. Priority updates using means other than `android.os.Process.setThreadPriority(int)` or
+   `Thread.setPriority(int)` should set a positive niceness value only when it is OK for ART (or
+   the application) to overwrite that with the value cached by Java at any point. ART will not
+   overwrite negative niceness values, unless the negative value is installed exactly between ART's
+   priority check and priority update. (There is no atomic priority update in Linux.) Hence,
+   ideally:
+
+5. In order to minimize interference with applications directly using OS primitive to
+   adjust priorities, external processes should avoid updating priorities of threads
+   executing managed code, or that may therwise be concurrently adjusting its own priorities.
+   If they do, ART can at any moment overwrite their updates and restore its cached value.
+   That is good, because it is not always clear how to enforce such timing constraints.
+
+ART may use the following to temporarily increase priority by lowering niceness to zero:
+
+```
+  current_niceness = getpriority(<me>);
+  if (current_niceness <= 0 || current_niceness != <cached niceness>)
+    <do not change niceness>;
+  else
+    setpriority(<me>, 0);
+  <do high priority stuff that does not alter priority / niceness>
+  if (<we changed niceness> && getpriority(<me>) == 0) {
+    // Either there were no external intervening priority/niceness changes, or an
+    // external process restored our 0 niceness in the interim. Thread.setPriority(0)
+    // may have been called by another thread in the interim. In any of those cases,
+    // it is safe to restore priority with:
+    setpriority(<me>, <niceness cached by Java>);
+    // setpriority(<me>, current_niceness) would be incorrect with an intervening
+    // Thread.setPriority(0).
+  }
+```
+
+The above requires sufficient concurrency control to ensure that no `Thread.setPriority()`
+calls overlap with the priority restoration at the end.
+
+Java priority to niceness mapping
+---------------------------------
+
+The priority mapping is mostly determined by libartpalette, which is a platform component
+outside ART. Traditionally that provided `PaletteGetSchedPriority` and `PaletteSetSchedPriority`,
+which returned and took Java priority arguments, thus attempting to entirely hide the mapping.
+This turned out to be impractical, because some ART components, e.g. ART's system daemons,
+were tuned to use niceness values that did not correspond to Java priorities, and thus used
+Posix/Linux `setpriority` calls directly. This scheme also prevents us from easily caching
+priorities set by `android.os.Process.setThreadPriority(int)`, which traffics in Linux niceness
+values.
+
+Thus libartpalette now also provides `PaletteMapPriority()`, which is used in connection
+with Linux `setpriority(), and is preferred when available.
+
+Kernel-supported priority inheritance
+------------------------------------
+
+ART often suffers from priority inversions: A high priority thread *A* waits for a lower
+priority thread *B*. This may happen because *B* holds a lock that *A* needs, because *A* is
+waiting for a condition variable that *B* will eventually signal, or because *A* is waiting
+for *B* to pass a barrier, etc. The reason ART needs to explicitly adjust priorities is
+mostly to work around this kind of situation.
+
+In the case of locks, there is a well-established technique for addressing this, which is
+partially supported by the Linux kernel: If *A* blocks on a lock held by *B*, we can temporarily
+raise *B*'s priority to *A*'s until the lock is released. The Linux kernel provides
+`FUTEX_LOCK_PI` and pthreads provides `PTHREAD_PRIO_INHERIT` to support this. The implementation
+avoids the race issues we discussed above, along with some others. It is thus quite attractive.
+
+We plan to make use of this facility in the future. There are ongoing discussions about expanding
+it to cover some non-lock cases, which are equally important to us. However, there are currently
+some obstacles in the way of such use:
+
+1. The kernel supports priority inheritance for non-real-time-threads only in 6.12 and later. Pure
+   real-time priority inheritance does not help us much, since it is, for good reason, rarely used.
+
+2. User-level support currently only exists for `pthread_mutex`, and not for various kinds of locks
+   used internally by ART, nor those by Java synchronized blocks, nor the Java-implemented locks
+   used by java.util.concurrent. For the former two, this would require at least a major redesign,
+   possibly with some negative performance consequences. For the latter, it's currently impossible
+   because the implementation builds directly on `park()` / `unpark()`, which do not expose the
+   thread being waited for to the kernel.
+
+3. Even for `pthread_mutex_lock()` we would have to be consistent about using
+  `PTHREAD_PRIO_INHERIT`everywhere. This may have negative performance consequences, and the Posix
+  standard appears to prohibit defaulting to it.
+
+Thus, at the moment, we basically do not use kernel-supported priority inheritance. But stay
+tuned.
+
+Historical Notes
+----------------
+
+In the past (and still as of April 2025) `android.os.Process.setThreadPriority(int)` was not
+reflected in the `Thread.getpriority()` cache. This is viewed as a misfeature to be corrected.
+Unfortunately it can only be corrected for the version that sets the current thread's priority.
+
+Before Android T (a.k.a. 13), `PaletteSetSchedPriority()`, invoked by `Thread.setPriority()`,  but
+not by `android.os.Process.setThreadPriority(int)`, was only partially safe for use in frameworks
+code that might be called from the zygote process. It appears that under some circumstances,
+`SetTaskProfiles()` would be invoked and leave a file descriptor open, which is unacceptable for
+zygote code. As of April 2025, this still matters for changes to the ART module, and requires an
+unpleasant workaround. Even more recently, `android.os.Process.setThreadPriority(int)` and
+`Thread.setPriority()` differed in their effect on Linux cgroups. Now neither has an effect
+beyond setting niceness.
+
+On Android S, we assume a default Java priority to niceness mapping, and insure that
+Thread.setPriority() calls continue to result in `PaletteSetSchedPriority()` calls, generally
+preserving the platform behavior. (In unusual cases, the cached niceness value may not reflect
+what `PaletteSetSchedPriority()` actually sets, and ART attempts to temporarily raise priority may
+have no impact due to the mismatch.) On later API versions, we just call setpriority with the
+correct niceness value.
+
diff --git a/runtime/thread_suspension_timeouts.md b/runtime/thread_suspension_timeouts.md
index ca0efa183f..224e19d8e3 100644
--- a/runtime/thread_suspension_timeouts.md
+++ b/runtime/thread_suspension_timeouts.md
@@ -1,7 +1,10 @@
 Thread Suspension timeouts in ART
 ---------------------------------
 ART occasionally needs to "suspend" threads for a variety of reasons. "Suspended" threads may
-continue to run, but may not access data structures related to the Java heap. Please see
+continue to run, but may not access data structures related to the Java heap. Threads that are
+blocked indefinitely in ways understood by ART, e.g. because they are waiting on a monitor,
+waiting for Java IO, or in normal (not `@CriticalNative` or `@FastNative`) JNI calls are
+considered to be already suspended, and not subject to this process. Please see
 `mutator_gc_coord.md` for details.
 
 The suspension process usually involves setting a flag for the thread to be "suspended", possibly
@@ -73,6 +76,10 @@ include:
   may again involve removing `@FastNative` or `@CriticalNative` annotations. For ART internal
   code, break up `ScopedObjectAccess` sections or the like, being careful to not hold native
   pointers to Java heap objects across such sections.
+- Avoid calling problem code that is known to not be as responsive to suspension requests as we
+  would like. The most common example of this is generation of Java stack traces. Usually this is
+  fine, but if done very frequently, especially from low priority threads, you may see a
+  non-negligible suspension timeout failure rate, or at least add appreciable jank.
 - Avoid excessive parallelism that is causing some threads to starve.
 - Reduce differences in thread priorities and, if necessary, avoid very low priority threads, for
   the same reason.
diff --git a/runtime/trace_profile.cc b/runtime/trace_profile.cc
index 6eff2cdfe1..5da8fcc03a 100644
--- a/runtime/trace_profile.cc
+++ b/runtime/trace_profile.cc
@@ -62,6 +62,9 @@ static constexpr size_t kAlwaysOnTraceHeaderSize = 12;
 static constexpr size_t kAlwaysOnMethodInfoHeaderSize = 11;
 static constexpr size_t kAlwaysOnThreadInfoHeaderSize = 7;
 
+// Default duration for long-running method traces, currently 2 seconds (an arbitrary value)
+static constexpr uint64_t kDefaultTraceDurationNs = 2 * 1000 * 1000 * 1000;
+
 bool TraceProfiler::profile_in_progress_ = false;
 
 TraceData* TraceProfiler::trace_data_ = nullptr;
@@ -229,6 +232,53 @@ void DumpThreadMethodInfo(const std::unordered_map<size_t, std::string>& traced_
     os.write(method_line.c_str(), method_line_length);
   }
 }
+
+std::string Base64Encode(std::string_view input) {
+  // Encoding alphabet, see RFC4648 "Table 1: The Base 64 Alphabet"
+  static constexpr char kTable[] =
+      "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
+      "abcdefghijklmnopqrstuvwxyz"
+      "0123456789+/";
+  // 6 bits: 0b00111111
+  static constexpr int kBase64Mask = 0x3F;
+  std::string encoded_string;
+  // The output size is roughly 4/3 the input size.
+  encoded_string.reserve(((input.size() + 2) / 3) * 4);
+
+  std::uint32_t accumulator = 0;
+  // Tracks available bits. Starts at -6 because we need 6 bits for a usual-path extraction.
+  int bits_in_temp = -6;
+
+  for (unsigned char c : input) {
+    accumulator = (accumulator << 8) | c;
+    bits_in_temp += 8;
+
+    // While enough bits are available (>= 6) to extract a Base64 character
+    while (bits_in_temp >= 0) {
+      // Extract the most significant 6 available bits
+      encoded_string.push_back(kTable[(accumulator >> bits_in_temp) & kBase64Mask]);
+      bits_in_temp -= 6;
+    }
+  }
+
+  // Handle any remaining bits
+  // note that the remaining bits equal 6 + bits_in_temp, so, for instance,
+  // a bits_in_temp with the value -2 means 4 bits left, and that the only possible
+  // values of bits_in_temp at this points are -6, -4 and -2 (ie, 0, 2 or 4 bits remain)
+  if (bits_in_temp > -6) {
+    // This shifts the remaining bits to the MSB side of the 6-bit chunk,
+    // effectively padding with zeros on the right as per RFC 4648.
+    encoded_string.push_back(kTable[(accumulator << (-bits_in_temp)) & kBase64Mask]);
+  }
+
+  // Add padding characters to make the size a multiple of 4, per the RFC.
+  while (encoded_string.size() % 4 != 0) {
+    encoded_string.push_back('=');
+  }
+
+  return encoded_string;
+}
+
 }  // namespace
 
 class TraceStopTask : public gc::HeapTask {
@@ -270,6 +320,11 @@ void TraceProfiler::Start(LowOverheadTraceType trace_type, uint64_t trace_durati
   Thread* self = Thread::Current();
   uint64_t new_end_time = 0;
   bool add_trace_end_task = false;
+
+  if (trace_duration_ns == 0 && trace_type == LowOverheadTraceType::kLongRunningMethods) {
+    trace_duration_ns = kDefaultTraceDurationNs;
+  }
+
   {
     MutexLock mu(self, *Locks::trace_lock_);
     if (Trace::IsTracingEnabledLocked()) {
@@ -650,7 +705,7 @@ std::string TraceProfiler::GetLongRunningMethodsString() {
 
   std::ostringstream os;
   Dump(std::unique_ptr<File>(), os);
-  return os.str();
+  return Base64Encode(os.view());
 }
 
 void TraceDumpCheckpoint::Run(Thread* thread) {
diff --git a/runtime/vdex_file.cc b/runtime/vdex_file.cc
index 64ce9e84e1..f1ce3c095e 100644
--- a/runtime/vdex_file.cc
+++ b/runtime/vdex_file.cc
@@ -204,23 +204,6 @@ std::unique_ptr<VdexFile> VdexFile::OpenFromDm(const std::string& filename,
   return vdex_file;
 }
 
-bool VdexFile::IsValid() const {
-  if (mmap_.Size() < sizeof(VdexFileHeader) || !GetVdexFileHeader().IsValid()) {
-    return false;
-  }
-
-  // Invalidate vdex files that contain dex files in the no longer supported
-  // compact dex format. Revert this whenever the vdex version is bumped.
-  size_t i = 0;
-  for (const uint8_t* dex_file_start = GetNextDexFileData(nullptr, i); dex_file_start != nullptr;
-       dex_file_start = GetNextDexFileData(dex_file_start, ++i)) {
-    if (!DexFileLoader::IsMagicValid(dex_file_start)) {
-      return false;
-    }
-  }
-  return true;
-}
-
 const uint8_t* VdexFile::GetNextDexFileData(const uint8_t* cursor, uint32_t dex_file_index) const {
   DCHECK(cursor == nullptr || (cursor > Begin() && cursor <= End()));
   if (cursor == nullptr) {
@@ -253,31 +236,6 @@ const uint8_t* VdexFile::GetNextTypeLookupTableData(const uint8_t* cursor,
   }
 }
 
-bool VdexFile::OpenAllDexFiles(std::vector<std::unique_ptr<const DexFile>>* dex_files,
-                               std::string* error_msg) const {
-  size_t i = 0;
-  auto dex_file_container = std::make_shared<MemoryDexFileContainer>(Begin(), End());
-  for (const uint8_t* dex_file_start = GetNextDexFileData(nullptr, i);
-       dex_file_start != nullptr;
-       dex_file_start = GetNextDexFileData(dex_file_start, ++i)) {
-    // TODO: Supply the location information for a vdex file.
-    static constexpr char kVdexLocation[] = "";
-    std::string location = DexFileLoader::GetMultiDexLocation(i, kVdexLocation);
-    ArtDexFileLoader dex_file_loader(dex_file_container, location);
-    std::unique_ptr<const DexFile> dex(dex_file_loader.OpenOne(dex_file_start - Begin(),
-                                                               GetLocationChecksum(i),
-                                                               /*oat_dex_file=*/nullptr,
-                                                               /*verify=*/false,
-                                                               /*verify_checksum=*/false,
-                                                               error_msg));
-    if (dex == nullptr) {
-      return false;
-    }
-    dex_files->push_back(std::move(dex));
-  }
-  return true;
-}
-
 static bool CreateDirectories(const std::string& child_path, /* out */ std::string* error_msg) {
   size_t last_slash_pos = child_path.find_last_of('/');
   CHECK_NE(last_slash_pos, std::string::npos) << "Invalid path: " << child_path;
diff --git a/runtime/vdex_file.h b/runtime/vdex_file.h
index f192b7f466..1a16befa86 100644
--- a/runtime/vdex_file.h
+++ b/runtime/vdex_file.h
@@ -27,7 +27,6 @@
 #include "base/mem_map.h"
 #include "base/os.h"
 #include "class_status.h"
-#include "dex/compact_offset_table.h"
 #include "dex/dex_file.h"
 #include "handle.h"
 
@@ -252,7 +251,9 @@ class VdexFile {
         GetSectionHeader(VdexSection::kVerifierDepsSection).section_size);
   }
 
-  EXPORT bool IsValid() const;
+  bool IsValid() const {
+    return mmap_.Size() >= sizeof(VdexFileHeader) && GetVdexFileHeader().IsValid();
+  }
 
   // This method is for iterating over the dex files in the vdex. If `cursor` is null,
   // the first dex file is returned. If `cursor` is not null, it must point to a dex
@@ -268,10 +269,6 @@ class VdexFile {
     return GetDexChecksumAt(dex_file_index);
   }
 
-  // Open all the dex files contained in this vdex file.
-  EXPORT bool OpenAllDexFiles(std::vector<std::unique_ptr<const DexFile>>* dex_files,
-                              std::string* error_msg) const;
-
   // Writes a vdex into `path` and returns true on success.
   // The vdex will not contain a dex section but will store checksums of `dex_files`,
   // encoded `verifier_deps`, as well as the current boot class path cheksum and
diff --git a/runtime/verifier/method_verifier.cc b/runtime/verifier/method_verifier.cc
index 91e46365cf..8f8431f266 100644
--- a/runtime/verifier/method_verifier.cc
+++ b/runtime/verifier/method_verifier.cc
@@ -284,20 +284,20 @@ class MethodVerifierImpl : public ::art::verifier::MethodVerifier {
                                        uint16_t inst_data);
 
   /* Ensure that the register index is valid for this code item. */
-  bool CheckRegisterIndex(uint32_t idx) {
-    if (UNLIKELY(idx >= code_item_accessor_.RegistersSize())) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "register index out of range (" << idx << " >= "
-                                        << code_item_accessor_.RegistersSize() << ")";
+  ALWAYS_INLINE bool CheckRegisterIndex(uint32_t idx) {
+    uint32_t registers_size = code_item_accessor_.RegistersSize();
+    if (UNLIKELY(idx >= registers_size)) {
+      FailBadRegisterIndex(idx, registers_size);
       return false;
     }
     return true;
   }
 
   /* Ensure that the wide register index is valid for this code item. */
-  bool CheckWideRegisterIndex(uint32_t idx) {
-    if (UNLIKELY(idx + 1 >= code_item_accessor_.RegistersSize())) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "wide register index out of range (" << idx
-                                        << "+1 >= " << code_item_accessor_.RegistersSize() << ")";
+  ALWAYS_INLINE bool CheckWideRegisterIndex(uint32_t idx) {
+    uint32_t registers_size = code_item_accessor_.RegistersSize();
+    if (UNLIKELY(idx + 1 >= registers_size)) {
+      FailBadWideRegisterIndex(idx, registers_size);
       return false;
     }
     return true;
@@ -305,11 +305,10 @@ class MethodVerifierImpl : public ::art::verifier::MethodVerifier {
 
   // Perform static checks on an instruction referencing a CallSite. All we do here is ensure that
   // the call site index is in the valid range.
-  bool CheckCallSiteIndex(uint32_t idx) {
+  ALWAYS_INLINE bool CheckCallSiteIndex(uint32_t idx) {
     uint32_t limit = dex_file_->NumCallSiteIds();
     if (UNLIKELY(idx >= limit)) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "bad call site index " << idx << " (max "
-                                        << limit << ")";
+      FailBadCallSiteIndex(idx, limit);
       return false;
     }
     return true;
@@ -400,11 +399,7 @@ class MethodVerifierImpl : public ::art::verifier::MethodVerifier {
     std::pair<char, char> permitted = kPermittedDescriptors[opcode - kMinFieldAccessOpcode];
     const char* descriptor = dex_file_->GetFieldTypeDescriptor(field_idx);
     if (UNLIKELY(descriptor[0] != permitted.first && descriptor[0] != permitted.second)) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD)
-          << "expected field " << dex_file_->PrettyField(field_idx)
-          << " to have type descritor starting with '" << permitted.first
-          << (permitted.second != permitted.first ? std::string("' or '") + permitted.second : "")
-          << "' but found '" << descriptor[0] << "' in " << opcode;
+      FailBadFieldDescriptor(field_idx, permitted.first, permitted.second, descriptor[0], opcode);
       return false;
     }
     return true;
@@ -422,7 +417,7 @@ class MethodVerifierImpl : public ::art::verifier::MethodVerifier {
 
   // Perform static checks on an instruction referencing a constant method handle. All we do here
   // is ensure that the method index is in the valid range.
-  bool CheckMethodHandleIndex(uint32_t idx) {
+  ALWAYS_INLINE bool CheckMethodHandleIndex(uint32_t idx) {
     if (UNLIKELY(idx >= dex_file_->NumMethodHandles())) {
       FailBadMethodHandleIndex(idx);
       return false;
@@ -432,7 +427,7 @@ class MethodVerifierImpl : public ::art::verifier::MethodVerifier {
 
   // Perform static checks on a prototype indexing instruction. All we do here is ensure that the
   // prototype index is in the valid range.
-  bool CheckPrototypeIndex(uint32_t idx) {
+  ALWAYS_INLINE bool CheckPrototypeIndex(uint32_t idx) {
     if (UNLIKELY(idx >= dex_file_->NumProtoIds())) {
       FailBadPrototypeIndex(idx);
       return false;
@@ -441,7 +436,7 @@ class MethodVerifierImpl : public ::art::verifier::MethodVerifier {
   }
 
   /* Ensure that the string index is in the valid range. */
-  bool CheckStringIndex(uint32_t idx) {
+  ALWAYS_INLINE bool CheckStringIndex(uint32_t idx) {
     if (UNLIKELY(idx >= dex_file_->NumStringIds())) {
       FailBadStringIndex(idx);
       return false;
@@ -451,7 +446,7 @@ class MethodVerifierImpl : public ::art::verifier::MethodVerifier {
 
   // Perform static checks on an instruction that takes a class constant. Ensure that the class
   // index is in the valid range.
-  bool CheckTypeIndex(dex::TypeIndex idx) {
+  ALWAYS_INLINE bool CheckTypeIndex(dex::TypeIndex idx) {
     if (UNLIKELY(idx.index_ >= dex_file_->GetHeader().type_ids_size_)) {
       FailBadTypeIndex(idx);
       return false;
@@ -538,13 +533,12 @@ class MethodVerifierImpl : public ::art::verifier::MethodVerifier {
   // Check the register indices used in a "vararg/range" instruction, such as invoke-virtual/range
   // or filled-new-array/range.
   // - vA holds word count, vC holds index of first reg.
-  bool CheckVarArgRangeRegs(uint32_t vA, uint32_t vC) {
-    uint16_t registers_size = code_item_accessor_.RegistersSize();
+  ALWAYS_INLINE bool CheckVarArgRangeRegs(uint32_t vA, uint32_t vC) {
+    uint32_t registers_size = code_item_accessor_.RegistersSize();
     // vA/vC are unsigned 8-bit/16-bit quantities for /range instructions, so there's no risk of
     // integer overflow when adding them here.
     if (UNLIKELY(vA + vC > registers_size)) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid reg index " << vA << "+" << vC
-                                        << " in range invoke (> " << registers_size << ")";
+      FailBadVarArgsRangeRegs(vA, vC, registers_size);
       return false;
     }
     return true;
@@ -712,11 +706,31 @@ class MethodVerifierImpl : public ::art::verifier::MethodVerifier {
     return Instruction::MOVE_RESULT <= opcode && opcode <= Instruction::MOVE_EXCEPTION;
   }
 
+  NO_INLINE void FailBadRegisterIndex(uint32_t idx, uint32_t registers_size) {
+      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "register index out of range (" << idx << " >= "
+                                        << registers_size << ")";
+  }
+
+  NO_INLINE void FailBadWideRegisterIndex(uint32_t idx, uint32_t registers_size) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "wide register index out of range (" << idx
+                                      << "+1 >= " << registers_size << ")";
+  }
+
+  NO_INLINE void FailBadCallSiteIndex(uint32_t idx, uint32_t limit) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "bad call site index " << idx << " (max "
+                                      << limit << ")";
+  }
+
   NO_INLINE void FailInvalidArgCount(const Instruction* inst, uint32_t arg_count) {
     Fail(VERIFY_ERROR_BAD_CLASS_HARD)
         << "invalid arg count (" << arg_count << ") in " << inst->Name();
   }
 
+  NO_INLINE void FailBadVarArgsRangeRegs(uint32_t vA, uint32_t vC, uint32_t registers_size) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid reg index " << vA << "+" << vC
+                                      << " in range invoke (> " << registers_size << ")";
+  }
+
   NO_INLINE void FailUnexpectedOpcode(const Instruction* inst) {
     Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "unexpected opcode " << inst->Name();
   }
@@ -726,6 +740,18 @@ class MethodVerifierImpl : public ::art::verifier::MethodVerifier {
         << "bad field index " << field_idx << " (max " << dex_file_->NumFieldIds() << ")";
   }
 
+  NO_INLINE void FailBadFieldDescriptor(uint32_t field_idx,
+                                        char permitted1,
+                                        char permitted2,
+                                        char descriptor,
+                                        Instruction::Code opcode) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "expected field " << dex_file_->PrettyField(field_idx)
+        << " to have type descriptor starting with '" << permitted1
+        << (permitted2 != permitted1 ? std::string("' or '") + permitted1 : "")
+        << "' but found '" << descriptor << "' in " << opcode;
+  }
+
   NO_INLINE void FailBadMethodIndex(uint32_t method_idx) {
     Fail(VERIFY_ERROR_BAD_CLASS_HARD)
         << "bad method index " << method_idx << " (max " << dex_file_->NumMethodIds() << ")";
@@ -751,6 +777,10 @@ class MethodVerifierImpl : public ::art::verifier::MethodVerifier {
         << "bad type index " << idx.index_ << " (max " << dex_file_->NumTypeIds() << ")";
   }
 
+  NO_INLINE void FailBadNewInstanceDescriptor(std::string_view descriptor) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "can't call new-instance on type '" << descriptor << "'";
+  }
+
   NO_INLINE void FailBadNewArrayNotArray(const char* descriptor) {
     Fail(VERIFY_ERROR_BAD_CLASS_HARD)
         << "can't new-array class '" << descriptor << "' (not an array)";
@@ -2086,7 +2116,7 @@ inline bool MethodVerifierImpl::CheckNewInstance(dex::TypeIndex idx) {
   // We don't need the actual class, just a pointer to the class name.
   const std::string_view descriptor = dex_file_->GetTypeDescriptorView(idx);
   if (UNLIKELY(descriptor[0] != 'L')) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "can't call new-instance on type '" << descriptor << "'";
+    FailBadNewInstanceDescriptor(descriptor);
     return false;
   } else if (UNLIKELY(descriptor == "Ljava/lang/Class;")) {
     // An unlikely new instance on Class is not allowed.
diff --git a/runtime/well_known_classes.cc b/runtime/well_known_classes.cc
index 14b0feecb6..1c189d4900 100644
--- a/runtime/well_known_classes.cc
+++ b/runtime/well_known_classes.cc
@@ -16,14 +16,16 @@
 
 #include "well_known_classes.h"
 
+#include <android-base/logging.h>
+#include <android-base/stringprintf.h>
 #include <stdlib.h>
 
+#include <cstddef>
 #include <sstream>
 
-#include <android-base/logging.h>
-#include <android-base/stringprintf.h>
-
+#include "art_field.h"
 #include "art_method-inl.h"
+#include "art_method.h"
 #include "base/casts.h"
 #include "base/pointer_size.h"
 #include "class_linker.h"
@@ -49,6 +51,7 @@ jclass WellKnownClasses::dalvik_annotation_optimization_CriticalNative;
 jclass WellKnownClasses::dalvik_annotation_optimization_FastNative;
 jclass WellKnownClasses::dalvik_annotation_optimization_NeverCompile;
 jclass WellKnownClasses::dalvik_annotation_optimization_NeverInline;
+jclass WellKnownClasses::dalvik_system_VirtualThreadFrame__array;
 jclass WellKnownClasses::java_lang_annotation_Annotation__array;
 jclass WellKnownClasses::java_lang_ClassValue;
 jclass WellKnownClasses::java_lang_Record;
@@ -83,6 +86,7 @@ ArtMethod* WellKnownClasses::java_lang_Integer_valueOf;
 ArtMethod* WellKnownClasses::java_lang_Long_valueOf;
 ArtMethod* WellKnownClasses::java_lang_NoClassDefFoundError_init;
 ArtMethod* WellKnownClasses::java_lang_OutOfMemoryError_init;
+ArtMethod* WellKnownClasses::java_lang_Runnable_run;
 ArtMethod* WellKnownClasses::java_lang_Runtime_nativeLoad;
 ArtMethod* WellKnownClasses::java_lang_RuntimeException_init;
 ArtMethod* WellKnownClasses::java_lang_Short_valueOf;
@@ -90,6 +94,7 @@ ArtMethod* WellKnownClasses::java_lang_StackOverflowError_init;
 ArtMethod* WellKnownClasses::java_lang_String_charAt;
 ArtMethod* WellKnownClasses::java_lang_Thread_dispatchUncaughtException;
 ArtMethod* WellKnownClasses::java_lang_Thread_init;
+ArtMethod* WellKnownClasses::java_lang_Thread_parkVirtualInternal;
 ArtMethod* WellKnownClasses::java_lang_Thread_run;
 ArtMethod* WellKnownClasses::java_lang_ThreadGroup_add;
 ArtMethod* WellKnownClasses::java_lang_ThreadGroup_threadTerminated;
@@ -126,21 +131,32 @@ ArtField* WellKnownClasses::dalvik_system_DexFile_fileName;
 ArtField* WellKnownClasses::dalvik_system_DexPathList_dexElements;
 ArtField* WellKnownClasses::dalvik_system_DexPathList__Element_dexFile;
 ArtField* WellKnownClasses::dalvik_system_VMRuntime_nonSdkApiUsageConsumer;
+ArtField* WellKnownClasses::dalvik_system_VirtualThreadContext_parkedStates;
+ArtField* WellKnownClasses::dalvik_system_VirtualThreadContext_pinnedCarrierThread;
+ArtField* WellKnownClasses::dalvik_system_VirtualThreadParkedStates_frames;
+ArtField* WellKnownClasses::dalvik_system_VirtualThreadFrame_frame;
+ArtField* WellKnownClasses::dalvik_system_VirtualThreadFrame_refs;
+ArtField* WellKnownClasses::dalvik_system_VirtualThreadFrame_declaringClass;
 ArtField* WellKnownClasses::java_io_FileDescriptor_descriptor;
 ArtField* WellKnownClasses::java_lang_ref_Reference_disableIntrinsic;
 ArtField* WellKnownClasses::java_lang_ref_Reference_slowPathEnabled;
 ArtField* WellKnownClasses::java_lang_ClassLoader_parent;
 ArtField* WellKnownClasses::java_lang_Object_shadowKlass;
+ArtField* WellKnownClasses::java_lang_System_in;
+ArtField* WellKnownClasses::java_lang_System_out;
+ArtField* WellKnownClasses::java_lang_System_err;
 ArtField* WellKnownClasses::java_lang_String_EMPTY;
 ArtField* WellKnownClasses::java_lang_Thread_parkBlocker;
 ArtField* WellKnownClasses::java_lang_Thread_daemon;
 ArtField* WellKnownClasses::java_lang_Thread_group;
 ArtField* WellKnownClasses::java_lang_Thread_lock;
 ArtField* WellKnownClasses::java_lang_Thread_name;
-ArtField* WellKnownClasses::java_lang_Thread_priority;
+ArtField* WellKnownClasses::java_lang_Thread_niceness;
 ArtField* WellKnownClasses::java_lang_Thread_nativePeer;
+ArtField* WellKnownClasses::java_lang_Thread_priority;
 ArtField* WellKnownClasses::java_lang_Thread_systemDaemon;
 ArtField* WellKnownClasses::java_lang_Thread_unparkedBeforeStart;
+ArtField* WellKnownClasses::java_lang_Thread_target;
 ArtField* WellKnownClasses::java_lang_ThreadGroup_groups;
 ArtField* WellKnownClasses::java_lang_ThreadGroup_ngroups;
 ArtField* WellKnownClasses::java_lang_ThreadGroup_mainThreadGroup;
@@ -365,6 +381,7 @@ void WellKnownClasses::Init(JNIEnv* env) {
       CacheClass(env, "dalvik/annotation/optimization/NeverCompile");
   dalvik_annotation_optimization_NeverInline =
       CacheClass(env, "dalvik/annotation/optimization/NeverInline");
+  dalvik_system_VirtualThreadFrame__array = CacheClass(env, "[Ldalvik/system/VirtualThreadFrame;");
 
   java_lang_annotation_Annotation__array = CacheClass(env, "[Ljava/lang/annotation/Annotation;");
   java_lang_ClassValue = CacheClass(env, "java/lang/ClassValue");
@@ -431,7 +448,7 @@ void WellKnownClasses::InitFieldsAndMethodsOnly(JNIEnv* env) {
   java_lang_Long_value = CacheValueInBoxField(
       class_linker, self, "Ljava/lang/Long;", "J");
 
-  StackHandleScope<45u> hs(self);
+  StackHandleScope<49u> hs(self);
   Handle<mirror::Class> d_s_bdcl =
       hs.NewHandle(FindSystemClass(class_linker, self, "Ldalvik/system/BaseDexClassLoader;"));
   Handle<mirror::Class> d_s_dlcl =
@@ -450,6 +467,12 @@ void WellKnownClasses::InitFieldsAndMethodsOnly(JNIEnv* env) {
       hs.NewHandle(FindSystemClass(class_linker, self, "Ldalvik/system/PathClassLoader;"));
   Handle<mirror::Class> d_s_vmr =
       hs.NewHandle(FindSystemClass(class_linker, self, "Ldalvik/system/VMRuntime;"));
+  Handle<mirror::Class> d_s_vtc =
+      hs.NewHandle(FindSystemClass(class_linker, self, "Ldalvik/system/VirtualThreadContext;"));
+  Handle<mirror::Class> d_s_vtps = hs.NewHandle(
+      FindSystemClass(class_linker, self, "Ldalvik/system/VirtualThreadParkedStates;"));
+  Handle<mirror::Class> d_s_vtf =
+      hs.NewHandle(FindSystemClass(class_linker, self, "Ldalvik/system/VirtualThreadFrame;"));
   Handle<mirror::Class> j_i_fd =
       hs.NewHandle(FindSystemClass(class_linker, self, "Ljava/io/FileDescriptor;"));
   Handle<mirror::Class> j_l_bcl =
@@ -468,6 +491,8 @@ void WellKnownClasses::InitFieldsAndMethodsOnly(JNIEnv* env) {
       hs.NewHandle(FindSystemClass(class_linker, self, "Ljava/lang/NoClassDefFoundError;"));
   Handle<mirror::Class> j_l_OutOfMemoryError =
       hs.NewHandle(FindSystemClass(class_linker, self, "Ljava/lang/OutOfMemoryError;"));
+  Handle<mirror::Class> j_l_Runnable =
+      hs.NewHandle(FindSystemClass(class_linker, self, "Ljava/lang/Runnable;"));
   Handle<mirror::Class> j_l_RuntimeException =
       hs.NewHandle(FindSystemClass(class_linker, self, "Ljava/lang/RuntimeException;"));
   Handle<mirror::Class> j_l_StackOverflowError =
@@ -602,6 +627,8 @@ void WellKnownClasses::InitFieldsAndMethodsOnly(JNIEnv* env) {
       j_l_NoClassDefFoundError.Get(), /*is_static=*/ false, "<init>", "()V", pointer_size);
   java_lang_OutOfMemoryError_init = CacheMethod(
       j_l_OutOfMemoryError.Get(), /*is_static=*/ false, "<init>", "()V", pointer_size);
+  java_lang_Runnable_run =
+      CacheMethod(j_l_Runnable.Get(), /*is_static=*/false, "run", "()V", pointer_size);
   java_lang_RuntimeException_init = CacheMethod(
       j_l_RuntimeException.Get(), /*is_static=*/ false, "<init>", "()V", pointer_size);
   java_lang_StackOverflowError_init = CacheMethod(
@@ -623,6 +650,13 @@ void WellKnownClasses::InitFieldsAndMethodsOnly(JNIEnv* env) {
       "<init>",
       "(Ljava/lang/ThreadGroup;Ljava/lang/String;IZ)V",
       pointer_size);
+  java_lang_Thread_parkVirtualInternal =
+      CacheMethod(j_l_Thread.Get(),
+                  /*is_static=*/true,
+                  "parkVirtualInternal",
+                  "(Ldalvik/system/VirtualThreadContext;Ldalvik/system/"
+                  "VirtualThreadParkedStates;Ldalvik/system/VirtualThreadParkingError;)V",
+                  pointer_size);
   java_lang_Thread_run = CacheMethod(
       j_l_Thread.Get(), /*is_static=*/ false, "run", "()V", pointer_size);
   java_lang_ThreadGroup_add = CacheMethod(
@@ -789,6 +823,32 @@ void WellKnownClasses::InitFieldsAndMethodsOnly(JNIEnv* env) {
       /*is_static=*/ true,
       "nonSdkApiUsageConsumer",
       "Ljava/util/function/Consumer;");
+  dalvik_system_VirtualThreadContext_parkedStates =
+      CacheField(d_s_vtc.Get(),
+                 /*is_static=*/false,
+                 "parkedStates",
+                 "Ldalvik/system/VirtualThreadParkedStates;");
+  dalvik_system_VirtualThreadContext_pinnedCarrierThread = CacheField(d_s_vtc.Get(),
+                                                                      /*is_static=*/false,
+                                                                      "pinnedCarrierThread",
+                                                                      "Ljava/lang/Thread;");
+  dalvik_system_VirtualThreadParkedStates_frames =
+      CacheField(d_s_vtps.Get(),
+                 /*is_static=*/false,
+                 "frames",
+                 "[Ldalvik/system/VirtualThreadFrame;");
+  dalvik_system_VirtualThreadFrame_frame = CacheField(d_s_vtf.Get(),
+                                                      /*is_static=*/false,
+                                                      "frame",
+                                                      "[B");
+  dalvik_system_VirtualThreadFrame_refs = CacheField(d_s_vtf.Get(),
+                                                     /*is_static=*/false,
+                                                     "refs",
+                                                     "[Ljava/lang/Object;");
+  dalvik_system_VirtualThreadFrame_declaringClass = CacheField(d_s_vtf.Get(),
+                                                               /*is_static=*/false,
+                                                               "declaringClass",
+                                                               "Ljava/lang/Class;");
 
   java_io_FileDescriptor_descriptor = CacheField(
       j_i_fd.Get(), /*is_static=*/ false, "descriptor", "I");
@@ -797,7 +857,15 @@ void WellKnownClasses::InitFieldsAndMethodsOnly(JNIEnv* env) {
       j_l_cl.Get(), /*is_static=*/ false, "parent", "Ljava/lang/ClassLoader;");
 
   java_lang_String_EMPTY =
-      CacheField(j_l_String, /*is_static=*/true, "EMPTY", "Ljava/lang/String;");
+      CacheField(j_l_String, /*is_static=*/ true, "EMPTY", "Ljava/lang/String;");
+
+  java_lang_System_in =
+      CacheField(ToClass(java_lang_System), /*is_static=*/ true, "in", "Ljava/io/InputStream;");
+  java_lang_System_out =
+      CacheField(ToClass(java_lang_System), /*is_static=*/ true, "out", "Ljava/io/PrintStream;");
+  java_lang_System_err =
+      CacheField(ToClass(java_lang_System), /*is_static=*/ true, "err", "Ljava/io/PrintStream;");
+
   java_lang_Thread_parkBlocker =
       CacheField(j_l_Thread.Get(), /*is_static=*/ false, "parkBlocker", "Ljava/lang/Object;");
   java_lang_Thread_daemon = CacheField(j_l_Thread.Get(), /*is_static=*/ false, "daemon", "Z");
@@ -807,13 +875,16 @@ void WellKnownClasses::InitFieldsAndMethodsOnly(JNIEnv* env) {
       CacheField(j_l_Thread.Get(), /*is_static=*/ false, "lock", "Ljava/lang/Object;");
   java_lang_Thread_name =
       CacheField(j_l_Thread.Get(), /*is_static=*/ false, "name", "Ljava/lang/String;");
-  java_lang_Thread_priority = CacheField(j_l_Thread.Get(), /*is_static=*/ false, "priority", "I");
+  java_lang_Thread_niceness = CacheField(j_l_Thread.Get(), /*is_static=*/false, "niceness", "I");
   java_lang_Thread_nativePeer =
       CacheField(j_l_Thread.Get(), /*is_static=*/ false, "nativePeer", "J");
+  java_lang_Thread_priority = CacheField(j_l_Thread.Get(), /*is_static=*/false, "priority", "I");
   java_lang_Thread_systemDaemon =
       CacheField(j_l_Thread.Get(), /*is_static=*/ false, "systemDaemon", "Z");
   java_lang_Thread_unparkedBeforeStart =
       CacheField(j_l_Thread.Get(), /*is_static=*/ false, "unparkedBeforeStart", "Z");
+  java_lang_Thread_target =
+      CacheField(j_l_Thread.Get(), /*is_static=*/false, "target", "Ljava/lang/Runnable;");
 
   java_lang_ThreadGroup_groups =
       CacheField(j_l_tg.Get(), /*is_static=*/ false, "groups", "[Ljava/lang/ThreadGroup;");
@@ -914,6 +985,7 @@ void WellKnownClasses::Clear() {
   dalvik_annotation_optimization_FastNative = nullptr;
   dalvik_annotation_optimization_NeverCompile = nullptr;
   dalvik_annotation_optimization_NeverInline = nullptr;
+  dalvik_system_VirtualThreadFrame__array = nullptr;
   java_lang_annotation_Annotation__array = nullptr;
   java_lang_ClassValue = nullptr;
   java_lang_Record = nullptr;
@@ -949,6 +1021,7 @@ void WellKnownClasses::Clear() {
   java_lang_Long_valueOf = nullptr;
   java_lang_NoClassDefFoundError_init = nullptr;
   java_lang_OutOfMemoryError_init = nullptr;
+  java_lang_Runnable_run = nullptr;
   java_lang_Runtime_nativeLoad = nullptr;
   java_lang_RuntimeException_init = nullptr;
   java_lang_Short_valueOf = nullptr;
@@ -956,6 +1029,7 @@ void WellKnownClasses::Clear() {
   java_lang_String_charAt = nullptr;
   java_lang_Thread_dispatchUncaughtException = nullptr;
   java_lang_Thread_init = nullptr;
+  java_lang_Thread_parkVirtualInternal = nullptr;
   java_lang_Thread_run = nullptr;
   java_lang_ThreadGroup_add = nullptr;
   java_lang_ThreadGroup_threadTerminated = nullptr;
@@ -994,13 +1068,17 @@ void WellKnownClasses::Clear() {
   java_lang_ClassLoader_parent = nullptr;
   java_lang_Object_shadowKlass = nullptr;
   java_lang_String_EMPTY = nullptr;
+  java_lang_System_in = nullptr;
+  java_lang_System_out = nullptr;
+  java_lang_System_err = nullptr;
   java_lang_Thread_parkBlocker = nullptr;
   java_lang_Thread_daemon = nullptr;
   java_lang_Thread_group = nullptr;
   java_lang_Thread_lock = nullptr;
   java_lang_Thread_name = nullptr;
-  java_lang_Thread_priority = nullptr;
+  java_lang_Thread_niceness = nullptr;
   java_lang_Thread_nativePeer = nullptr;
+  java_lang_Thread_priority = nullptr;
   java_lang_ThreadGroup_groups = nullptr;
   java_lang_ThreadGroup_ngroups = nullptr;
   java_lang_ThreadGroup_mainThreadGroup = nullptr;
diff --git a/runtime/well_known_classes.h b/runtime/well_known_classes.h
index 8798744448..a3b456a8ff 100644
--- a/runtime/well_known_classes.h
+++ b/runtime/well_known_classes.h
@@ -95,6 +95,7 @@ struct EXPORT WellKnownClasses {
   static jclass dalvik_annotation_optimization_FastNative;
   static jclass dalvik_annotation_optimization_NeverCompile;
   static jclass dalvik_annotation_optimization_NeverInline;
+  static jclass dalvik_system_VirtualThreadFrame__array;
   static jclass java_lang_annotation_Annotation__array;
   static jclass java_lang_ClassValue;
   static jclass java_lang_Record;
@@ -129,6 +130,7 @@ struct EXPORT WellKnownClasses {
   static ArtMethod* java_lang_Long_valueOf;
   static ArtMethod* java_lang_NoClassDefFoundError_init;  // Only for the declaring class.
   static ArtMethod* java_lang_OutOfMemoryError_init;  // Only for the declaring class.
+  static ArtMethod* java_lang_Runnable_run;
   static ArtMethod* java_lang_Runtime_nativeLoad;
   static ArtMethod* java_lang_RuntimeException_init;  // Only for the declaring class.
   static ArtMethod* java_lang_Short_valueOf;
@@ -136,6 +138,7 @@ struct EXPORT WellKnownClasses {
   static ArtMethod* java_lang_String_charAt;
   static ArtMethod* java_lang_Thread_dispatchUncaughtException;
   static ArtMethod* java_lang_Thread_init;
+  static ArtMethod* java_lang_Thread_parkVirtualInternal;
   static ArtMethod* java_lang_Thread_run;
   static ArtMethod* java_lang_ThreadGroup_add;
   static ArtMethod* java_lang_ThreadGroup_threadTerminated;
@@ -172,20 +175,32 @@ struct EXPORT WellKnownClasses {
   static ArtField* dalvik_system_DexPathList_dexElements;
   static ArtField* dalvik_system_DexPathList__Element_dexFile;
   static ArtField* dalvik_system_VMRuntime_nonSdkApiUsageConsumer;
+  static ArtField* dalvik_system_VirtualThreadContext_parkedStates;
+  static ArtField* dalvik_system_VirtualThreadContext_pinnedCarrierThread;
+  static ArtField* dalvik_system_VirtualThreadParkedStates_frames;
+  // TODO: Consider using a mirror class for VirtualThreadFrame.
+  static ArtField* dalvik_system_VirtualThreadFrame_frame;
+  static ArtField* dalvik_system_VirtualThreadFrame_refs;
+  static ArtField* dalvik_system_VirtualThreadFrame_declaringClass;
   static ArtField* java_io_FileDescriptor_descriptor;
   static ArtField* java_lang_ref_Reference_disableIntrinsic;
   static ArtField* java_lang_ref_Reference_slowPathEnabled;
   static ArtField* java_lang_ClassLoader_parent;
   static ArtField* java_lang_Object_shadowKlass;
   static ArtField* java_lang_String_EMPTY;
+  static ArtField* java_lang_System_in;
+  static ArtField* java_lang_System_out;
+  static ArtField* java_lang_System_err;
   static ArtField* java_lang_Thread_parkBlocker;
   static ArtField* java_lang_Thread_daemon;
   static ArtField* java_lang_Thread_group;
   static ArtField* java_lang_Thread_lock;
   static ArtField* java_lang_Thread_name;
-  static ArtField* java_lang_Thread_priority;
+  static ArtField* java_lang_Thread_niceness;
   static ArtField* java_lang_Thread_nativePeer;
+  static ArtField* java_lang_Thread_priority;
   static ArtField* java_lang_Thread_systemDaemon;
+  static ArtField* java_lang_Thread_target;
   static ArtField* java_lang_Thread_unparkedBeforeStart;
   static ArtField* java_lang_ThreadGroup_groups;
   static ArtField* java_lang_ThreadGroup_ngroups;
@@ -245,6 +260,12 @@ struct EXPORT WellKnownClasses {
       dalvik_system_InMemoryDexClassLoader;
   static constexpr ClassFromMethod<&dalvik_system_PathClassLoader_init>
       dalvik_system_PathClassLoader;
+  static constexpr ClassFromField<&dalvik_system_VirtualThreadContext_parkedStates>
+      dalvik_system_VirtualThreadContext;
+  static constexpr ClassFromField<&dalvik_system_VirtualThreadParkedStates_frames>
+      dalvik_system_VirtualThreadParkedStates;
+  static constexpr ClassFromField<&dalvik_system_VirtualThreadFrame_frame>
+      dalvik_system_VirtualThreadFrame;
   static constexpr ClassFromMethod<&java_lang_BootClassLoader_init> java_lang_BootClassLoader;
   static constexpr ClassFromField<&java_lang_ClassLoader_parent> java_lang_ClassLoader;
   static constexpr ClassFromMethod<&java_lang_Daemons_start> java_lang_Daemons;
diff --git a/sigchainlib/sigchain.cc b/sigchainlib/sigchain.cc
index e2e96df99b..aaaa49f369 100644
--- a/sigchainlib/sigchain.cc
+++ b/sigchainlib/sigchain.cc
@@ -525,18 +525,27 @@ void SignalChain::Handler(int signo, siginfo_t* siginfo, void* ucontext_raw) {
     chains[signo].action_.sa_sigaction(signo, siginfo, ucontext_raw);
   } else {
     auto handler = chains[signo].action_.sa_handler;
-    if (handler == SIG_IGN) {
-      return;
-    } else if (handler == SIG_DFL) {
+    if (handler == SIG_DFL ||
+        (handler == SIG_IGN && signo == SIGSYS && siginfo->si_code == SYS_SECCOMP)) {
       // We'll only get here if debuggerd is disabled. In that case, whatever next tries to handle
       // the crash will have no way to know our ucontext, and thus no way to dump the original crash
       // stack (since we're on an alternate stack.) Let's remove our handler and return. Then the
       // pre-crash state is restored, the crash happens again, and the next handler gets a chance.
+      //
+      // If we receive SIGSYS from SECCOMP filter and the chained handler is SIG_IGN then we can't
+      // ignore as the kernel forces signal in that case. So we have to fallback to SIG_DFL.
       LogError("reverting to SIG_DFL handler for signal %d, ucontext %p", signo, ucontext);
       LogStack();
       struct sigaction dfl = {};
       dfl.sa_handler = SIG_DFL;
       linked_sigaction(signo, &dfl, nullptr);
+      if (handler != SIG_DFL) {
+        // In case of SECCOMP, we cannot rely on the syscall to be re-tried as SECCOMP sets up
+        // things as if the syscall has already returned. So we must raise the signal.
+        raise(signo);
+      }
+      return;
+    } else if (handler == SIG_IGN) {
       return;
     } else {
       handler(signo);
diff --git a/simulator/Android.bp b/simulator/Android.bp
index dd0b11da4b..96f4067c23 100644
--- a/simulator/Android.bp
+++ b/simulator/Android.bp
@@ -49,6 +49,53 @@ cc_defaults {
     header_libs: ["libart_simulator_headers"],
 }
 
+cc_defaults {
+    name: "libart_simulator_static_base_defaults",
+    defaults: [
+        // Ideally this defaults should only be added for host, but Soong
+        // doesn't support that. This is good enough - in the worst case it adds
+        // some unnecessary dependencies.
+        "art_liblog_static_defaults",
+    ],
+    target: {
+        host: {
+            whole_static_libs: [
+                "libbase",
+            ],
+        },
+    },
+}
+
+cc_defaults {
+    name: "libart-simulator-for-test_static_defaults",
+    defaults: [
+        // See comment in libart_simulator_static_base_defaults.
+        "libart-for-test_static_defaults",
+        "libart_simulator_static_base_defaults",
+        "libartbase_static_defaults",
+    ],
+    target: {
+        host: {
+            whole_static_libs: ["libart-simulator"],
+        },
+    },
+}
+
+cc_defaults {
+    name: "libartd-simulator-for-test_static_defaults",
+    defaults: [
+        // See comment in libart_simulator_static_base_defaults.
+        "libartd-for-test_static_defaults",
+        "libart_simulator_static_base_defaults",
+        "libartbased_static_defaults",
+    ],
+    target: {
+        host: {
+            whole_static_libs: ["libartd-simulator"],
+        },
+    },
+}
+
 art_cc_library {
     name: "libart-simulator",
     defaults: ["libart_simulator_defaults"],
@@ -96,6 +143,13 @@ cc_defaults {
 art_cc_library_static {
     name: "libart-simulator-container",
     defaults: ["libart_simulator_container_defaults"],
+    target: {
+        host: {
+            runtime_libs: [
+                "libart-simulator",
+            ],
+        },
+    },
 }
 
 art_cc_library_static {
@@ -104,4 +158,11 @@ art_cc_library_static {
         "art_debug_defaults",
         "libart_simulator_container_defaults",
     ],
+    target: {
+        host: {
+            runtime_libs: [
+                "libartd-simulator",
+            ],
+        },
+    },
 }
diff --git a/simulator/include/code_simulator.h b/simulator/include/code_simulator.h
index 256ab23aa4..a11d873a97 100644
--- a/simulator/include/code_simulator.h
+++ b/simulator/include/code_simulator.h
@@ -39,7 +39,12 @@ class CodeSimulator {
   DISALLOW_COPY_AND_ASSIGN(CodeSimulator);
 };
 
+// libart(d)-simulator is only included as a dependency on device targets.
+#ifndef ART_TARGET
 extern "C" CodeSimulator* CreateCodeSimulator(InstructionSet target_isa);
+#else
+[[maybe_unused]] static CodeSimulator* CreateCodeSimulator(InstructionSet) { return nullptr; }
+#endif
 
 }  // namespace art
 
diff --git a/test/051-thread/src/Main.java b/test/051-thread/src/Main.java
index fe1cafef56..3977c6bb55 100644
--- a/test/051-thread/src/Main.java
+++ b/test/051-thread/src/Main.java
@@ -144,21 +144,24 @@ public class Main {
     private static void testThreadPriorities() throws Exception {
         System.out.print("testThreadPriorities starting\n");
 
-        PriorityStoringThread t1 = new PriorityStoringThread(false);
-        t1.setPriority(Thread.MAX_PRIORITY);
-        t1.start();
-        t1.join();
-        if (supportsThreadPriorities() && (t1.getNativePriority() != Thread.MAX_PRIORITY)) {
-            System.out.print("thread priority for t1 was " + t1.getNativePriority() +
-                " [expected Thread.MAX_PRIORITY]\n");
-        }
-
-        PriorityStoringThread t2 = new PriorityStoringThread(true);
-        t2.start();
-        t2.join();
-        if (supportsThreadPriorities() && (t2.getNativePriority() != Thread.MAX_PRIORITY)) {
-            System.out.print("thread priority for t2 was " + t2.getNativePriority() +
-                " [expected Thread.MAX_PRIORITY]\n");
+        if (supportsThreadPriorities()) {
+          PriorityStoringThread t1 = new PriorityStoringThread(false);
+          t1.setPriority(Thread.MAX_PRIORITY);
+          t1.start();
+          t1.join();
+          if (t1.getNativePriority() != Thread.MAX_PRIORITY) {
+              System.out.print("thread priority for t1 was " + t1.getNativePriority() +
+                  " [expected Thread.MAX_PRIORITY]\n");
+              System.out.println(getPriorityInfo());
+          }
+          PriorityStoringThread t2 = new PriorityStoringThread(true);
+          t2.start();
+          t2.join();
+          if (t2.getNativePriority() != Thread.MAX_PRIORITY) {
+              System.out.print("thread priority for t2 was " + t2.getNativePriority() +
+                  " [expected Thread.MAX_PRIORITY]\n");
+              System.out.println(getPriorityInfo());
+          }
         }
 
         System.out.print("testThreadPriorities finished\n");
@@ -209,6 +212,7 @@ public class Main {
 
     private static native int getNativePriority();
     private static native boolean supportsThreadPriorities();
+    private static native String getPriorityInfo();
 
     static class PriorityStoringThread extends Thread {
         private final boolean setPriority;
diff --git a/test/051-thread/thread_test.cc b/test/051-thread/thread_test.cc
index 6c7092396a..61b3957c75 100644
--- a/test/051-thread/thread_test.cc
+++ b/test/051-thread/thread_test.cc
@@ -16,8 +16,14 @@
 
 #include "base/macros.h"
 #include "jni.h"
+#include "scoped_thread_state_change-inl.h"
 #include "thread-inl.h"
 
+#include <errno.h>
+#include <sstream>
+#include <string.h>
+#include <sys/resource.h>
+
 namespace art {
 
 extern "C" JNIEXPORT jint JNICALL Java_Main_getNativePriority(JNIEnv* env,
@@ -27,11 +33,70 @@ extern "C" JNIEXPORT jint JNICALL Java_Main_getNativePriority(JNIEnv* env,
 
 extern "C" JNIEXPORT jboolean JNICALL Java_Main_supportsThreadPriorities(
     [[maybe_unused]] JNIEnv* env, [[maybe_unused]] jclass clazz) {
-#if defined(ART_TARGET_ANDROID)
-  return JNI_TRUE;
-#else
-  return JNI_FALSE;
-#endif
+  int my_niceness = getpriority(PRIO_PROCESS, 0 /* self */);
+  int my_priority = Thread::NicenessToPriority(my_niceness);
+  int new_priority = (my_priority == kMaxThreadPriority ? my_priority - 2 : my_priority + 1);
+  {
+    ScopedObjectAccess soa(env);
+    Thread::Current()->SetNativePriority(new_priority);
+  }
+  int new_niceness = getpriority(PRIO_PROCESS, 0 /* self */);
+  {
+    ScopedObjectAccess soa(env);
+    Thread::Current()->SetNativePriority(my_priority);
+  }
+  if (new_niceness == my_niceness) {
+    // Had no effect.
+    return JNI_FALSE;
+  } else {
+    // Assum3e it did the right thing.
+    return JNI_TRUE;
+  }
+}
+
+// Returns a description of Java to Posix priority mapping, and information about how we obtained
+// it. Used only for test failure reporting, and hence a bit sloppy in terms of error checks.
+extern "C" JNIEXPORT jstring JNICALL Java_Main_getPriorityInfo(JNIEnv* env,
+                                                               [[maybe_unused]] jclass clazz) {
+  std::ostringstream result;
+
+  result << "Java priorities mapping: ";
+  for (int p = kMinThreadPriority; p <= kMaxThreadPriority; ++p) {
+    if (p != kMinThreadPriority) {
+      result << ", ";
+    }
+    result << p << ": " << Thread::PriorityToNiceness(p);
+  }
+  result << ";\nSetpriority effects: ";
+  int32_t me = static_cast<int32_t>(::art::GetTid());
+  int my_niceness = getpriority(PRIO_PROCESS, 0 /* self */);
+  for (int p = -20; p <= 19; p += 3) {
+    if (p != -20) {
+      result << ", ";
+    }
+    int ret = setpriority(PRIO_PROCESS, 0 /* self */, p);
+    result << p << ": ";
+    if (ret == 0) {
+      result << getpriority(PRIO_PROCESS, 0 /* self */);
+    } else {
+      result << "failed:" << strerror(errno);
+    }
+  }
+  // Test whether SetNativePriority has any impact. If not, that suggests that either setpriority
+  // doesn't work in this range, or we've decided that setpriority cannot be relied upon for other
+  // reasons. See `canSetPriority` in thread.cc. Intentionally slightly different from
+  // supportsThreadPriorities test, so that we have a chance of detecting inconsistencies.
+  {
+    ScopedObjectAccess soa(env);
+    Thread::Current()->SetNativePriority(6);
+  }
+  if (getpriority(PRIO_PROCESS, 0 /* self */) != Thread::PriorityToNiceness(6)) {
+    result << ";\nPriority setting not working";
+  }
+  result << ";\nSDK version is " << android::base::GetIntProperty("ro.build.version.sdk", 0);
+  // Restore priority to something plausible, if possible.
+  setpriority(PRIO_PROCESS, 0 /* self */, my_niceness);
+  return env->NewStringUTF(result.str().c_str());
 }
 
 }  // namespace art
diff --git a/test/100-reflect2/expected-stdout.txt b/test/100-reflect2/expected-stdout.txt
index 7c548e8fa6..42c5210809 100644
--- a/test/100-reflect2/expected-stdout.txt
+++ b/test/100-reflect2/expected-stdout.txt
@@ -33,7 +33,7 @@ z (class java.lang.Character)
 14 (class java.lang.Short)
 [java.lang.String(byte[],byte), java.lang.String(int,int,char[]), public java.lang.String(), public java.lang.String(byte[]), public java.lang.String(byte[],int), public java.lang.String(byte[],int,int), public java.lang.String(byte[],int,int,int), public java.lang.String(byte[],int,int,java.lang.String) throws java.io.UnsupportedEncodingException, public java.lang.String(byte[],int,int,java.nio.charset.Charset), public java.lang.String(byte[],java.lang.String) throws java.io.UnsupportedEncodingException, public java.lang.String(byte[],java.nio.charset.Charset), public java.lang.String(char[]), public java.lang.String(char[],int,int), public java.lang.String(int[],int,int), public java.lang.String(java.lang.String), public java.lang.String(java.lang.StringBuffer), public java.lang.String(java.lang.StringBuilder)]
 [private final int java.lang.String.count, private int java.lang.String.hash, private static final java.io.ObjectStreamField[] java.lang.String.serialPersistentFields, private static final long java.lang.String.serialVersionUID, public static final java.lang.String java.lang.String.EMPTY, public static final java.util.Comparator java.lang.String.CASE_INSENSITIVE_ORDER, static final boolean java.lang.String.COMPACT_STRINGS, static final byte java.lang.String.CODER_LATIN1, static final byte java.lang.String.CODER_UTF16]
-[byte java.lang.String.coder(), native void java.lang.String.getCharsNoCheck(int,int,char[],int), private boolean java.lang.String.nonSyncContentEquals(java.lang.AbstractStringBuilder), private int java.lang.String.indexOfNonWhitespace(), private int java.lang.String.indexOfSupplementary(int,int), private int java.lang.String.lastIndexOfNonWhitespace(), private int java.lang.String.lastIndexOfSupplementary(int,int), private native java.lang.String java.lang.String.doRepeat(int), private native java.lang.String java.lang.String.doReplace(char,char), private native java.lang.String java.lang.String.fastSubstring(int,int), private native void java.lang.String.fillBytesLatin1(byte[],int), private native void java.lang.String.fillBytesUTF16(byte[],int), private static int java.lang.String.indexOf(java.lang.String,java.lang.String,int), private static int java.lang.String.lastIndexOf(java.lang.String,java.lang.String,int), private static int java.lang.String.outdent(java.util.List), public boolean java.lang.String.contains(java.lang.CharSequence), public boolean java.lang.String.contentEquals(java.lang.CharSequence), public boolean java.lang.String.contentEquals(java.lang.StringBuffer), public boolean java.lang.String.endsWith(java.lang.String), public boolean java.lang.String.equals(java.lang.Object), public boolean java.lang.String.equalsIgnoreCase(java.lang.String), public boolean java.lang.String.isBlank(), public boolean java.lang.String.isEmpty(), public boolean java.lang.String.matches(java.lang.String), public boolean java.lang.String.regionMatches(boolean,int,java.lang.String,int,int), public boolean java.lang.String.regionMatches(int,java.lang.String,int,int), public boolean java.lang.String.startsWith(java.lang.String), public boolean java.lang.String.startsWith(java.lang.String,int), public byte[] java.lang.String.getBytes(), public byte[] java.lang.String.getBytes(java.lang.String) throws java.io.UnsupportedEncodingException, public byte[] java.lang.String.getBytes(java.nio.charset.Charset), public int java.lang.String.codePointAt(int), public int java.lang.String.codePointBefore(int), public int java.lang.String.codePointCount(int,int), public int java.lang.String.compareTo(java.lang.Object), public int java.lang.String.compareToIgnoreCase(java.lang.String), public int java.lang.String.hashCode(), public int java.lang.String.indexOf(int), public int java.lang.String.indexOf(int,int), public int java.lang.String.indexOf(java.lang.String), public int java.lang.String.indexOf(java.lang.String,int), public int java.lang.String.lastIndexOf(int), public int java.lang.String.lastIndexOf(int,int), public int java.lang.String.lastIndexOf(java.lang.String), public int java.lang.String.lastIndexOf(java.lang.String,int), public int java.lang.String.length(), public int java.lang.String.offsetByCodePoints(int,int), public java.lang.CharSequence java.lang.String.subSequence(int,int), public java.lang.Object java.lang.String.resolveConstantDesc(java.lang.invoke.MethodHandles$Lookup) throws java.lang.ReflectiveOperationException, public java.lang.Object java.lang.String.transform(java.util.function.Function), public java.lang.String java.lang.String.formatted(java.lang.Object[]), public java.lang.String java.lang.String.indent(int), public java.lang.String java.lang.String.repeat(int), public java.lang.String java.lang.String.replace(char,char), public java.lang.String java.lang.String.replace(java.lang.CharSequence,java.lang.CharSequence), public java.lang.String java.lang.String.replaceAll(java.lang.String,java.lang.String), public java.lang.String java.lang.String.replaceFirst(java.lang.String,java.lang.String), public java.lang.String java.lang.String.resolveConstantDesc(java.lang.invoke.MethodHandles$Lookup), public java.lang.String java.lang.String.strip(), public java.lang.String java.lang.String.stripIndent(), public java.lang.String java.lang.String.stripLeading(), public java.lang.String java.lang.String.stripTrailing(), public java.lang.String java.lang.String.substring(int), public java.lang.String java.lang.String.substring(int,int), public java.lang.String java.lang.String.toLowerCase(), public java.lang.String java.lang.String.toLowerCase(java.util.Locale), public java.lang.String java.lang.String.toString(), public java.lang.String java.lang.String.toUpperCase(), public java.lang.String java.lang.String.toUpperCase(java.util.Locale), public java.lang.String java.lang.String.translateEscapes(), public java.lang.String java.lang.String.trim(), public java.lang.String[] java.lang.String.split(java.lang.String), public java.lang.String[] java.lang.String.split(java.lang.String,int), public java.util.Optional java.lang.String.describeConstable(), public java.util.stream.IntStream java.lang.String.chars(), public java.util.stream.IntStream java.lang.String.codePoints(), public java.util.stream.Stream java.lang.String.lines(), public native char java.lang.String.charAt(int), public native char[] java.lang.String.toCharArray(), public native int java.lang.String.compareTo(java.lang.String), public native java.lang.String java.lang.String.concat(java.lang.String), public native java.lang.String java.lang.String.intern(), public static java.lang.String java.lang.String.copyValueOf(char[]), public static java.lang.String java.lang.String.copyValueOf(char[],int,int), public static java.lang.String java.lang.String.format(java.lang.String,java.lang.Object[]), public static java.lang.String java.lang.String.format(java.util.Locale,java.lang.String,java.lang.Object[]), public static java.lang.String java.lang.String.join(java.lang.CharSequence,java.lang.CharSequence[]), public static java.lang.String java.lang.String.join(java.lang.CharSequence,java.lang.Iterable), public static java.lang.String java.lang.String.valueOf(boolean), public static java.lang.String java.lang.String.valueOf(char), public static java.lang.String java.lang.String.valueOf(char[]), public static java.lang.String java.lang.String.valueOf(char[],int,int), public static java.lang.String java.lang.String.valueOf(double), public static java.lang.String java.lang.String.valueOf(float), public static java.lang.String java.lang.String.valueOf(int), public static java.lang.String java.lang.String.valueOf(java.lang.Object), public static java.lang.String java.lang.String.valueOf(long), public void java.lang.String.getBytes(int,int,byte[],int), public void java.lang.String.getChars(int,int,char[],int), static int java.lang.String.indexOf(byte[],byte,int,java.lang.String,int), static int java.lang.String.lastIndexOf(byte[],byte,int,java.lang.String,int), static int java.lang.String.lastIndexOf(char[],int,int,char[],int,int,int), static java.lang.String java.lang.String.lambda$indent$0(java.lang.String,java.lang.String), static java.lang.String java.lang.String.lambda$indent$1(java.lang.String), static java.lang.String java.lang.String.lambda$indent$2(int,java.lang.String), static java.lang.String java.lang.String.lambda$stripIndent$3(int,java.lang.String), static java.lang.String java.lang.String.valueOfCodePoint(int), static void java.lang.String.checkBoundsBeginEnd(int,int,int), static void java.lang.String.checkBoundsOffCount(int,int,int), static void java.lang.String.checkIndex(int,int), static void java.lang.String.checkOffset(int,int), void java.lang.String.fillBytes(byte[],int,byte), void java.lang.String.getChars(char[],int)]
+[byte java.lang.String.coder(), native void java.lang.String.getCharsNoCheck(int,int,char[],int), private boolean java.lang.String.nonSyncContentEquals(java.lang.AbstractStringBuilder), private int java.lang.String.indexOfNonWhitespace(), private int java.lang.String.indexOfSupplementary(int,int), private int java.lang.String.lastIndexOfNonWhitespace(), private int java.lang.String.lastIndexOfSupplementary(int,int), private native java.lang.String java.lang.String.doRepeat(int), private native java.lang.String java.lang.String.doReplace(char,char), private native java.lang.String java.lang.String.fastSubstring(int,int), private native void java.lang.String.fillBytesLatin1(byte[],int), private native void java.lang.String.fillBytesUTF16(byte[],int), private static byte[] java.lang.String.encodeWithEncoder(java.nio.charset.Charset,java.lang.String,boolean), private static byte[] java.lang.String.getBytesNoRepl1(java.lang.String,java.nio.charset.Charset), private static byte[] java.lang.String.safeTrim(byte[],int,boolean), private static int java.lang.String.decodeWithDecoder(java.nio.charset.CharsetDecoder,char[],byte[],int,int) throws java.nio.charset.CharacterCodingException, private static int java.lang.String.indexOf(java.lang.String,java.lang.String,int), private static int java.lang.String.lastIndexOf(java.lang.String,java.lang.String,int), private static int java.lang.String.outdent(java.util.List), private static int java.lang.String.scale(int,float), private static java.lang.String java.lang.String.newStringNoRepl1(byte[],java.nio.charset.Charset), public boolean java.lang.String.contains(java.lang.CharSequence), public boolean java.lang.String.contentEquals(java.lang.CharSequence), public boolean java.lang.String.contentEquals(java.lang.StringBuffer), public boolean java.lang.String.endsWith(java.lang.String), public boolean java.lang.String.equals(java.lang.Object), public boolean java.lang.String.equalsIgnoreCase(java.lang.String), public boolean java.lang.String.isBlank(), public boolean java.lang.String.isEmpty(), public boolean java.lang.String.matches(java.lang.String), public boolean java.lang.String.regionMatches(boolean,int,java.lang.String,int,int), public boolean java.lang.String.regionMatches(int,java.lang.String,int,int), public boolean java.lang.String.startsWith(java.lang.String), public boolean java.lang.String.startsWith(java.lang.String,int), public byte[] java.lang.String.getBytes(), public byte[] java.lang.String.getBytes(java.lang.String) throws java.io.UnsupportedEncodingException, public byte[] java.lang.String.getBytes(java.nio.charset.Charset), public int java.lang.String.codePointAt(int), public int java.lang.String.codePointBefore(int), public int java.lang.String.codePointCount(int,int), public int java.lang.String.compareTo(java.lang.Object), public int java.lang.String.compareToIgnoreCase(java.lang.String), public int java.lang.String.hashCode(), public int java.lang.String.indexOf(int), public int java.lang.String.indexOf(int,int), public int java.lang.String.indexOf(java.lang.String), public int java.lang.String.indexOf(java.lang.String,int), public int java.lang.String.lastIndexOf(int), public int java.lang.String.lastIndexOf(int,int), public int java.lang.String.lastIndexOf(java.lang.String), public int java.lang.String.lastIndexOf(java.lang.String,int), public int java.lang.String.length(), public int java.lang.String.offsetByCodePoints(int,int), public java.lang.CharSequence java.lang.String.subSequence(int,int), public java.lang.Object java.lang.String.resolveConstantDesc(java.lang.invoke.MethodHandles$Lookup) throws java.lang.ReflectiveOperationException, public java.lang.Object java.lang.String.transform(java.util.function.Function), public java.lang.String java.lang.String.formatted(java.lang.Object[]), public java.lang.String java.lang.String.indent(int), public java.lang.String java.lang.String.repeat(int), public java.lang.String java.lang.String.replace(char,char), public java.lang.String java.lang.String.replace(java.lang.CharSequence,java.lang.CharSequence), public java.lang.String java.lang.String.replaceAll(java.lang.String,java.lang.String), public java.lang.String java.lang.String.replaceFirst(java.lang.String,java.lang.String), public java.lang.String java.lang.String.resolveConstantDesc(java.lang.invoke.MethodHandles$Lookup), public java.lang.String java.lang.String.strip(), public java.lang.String java.lang.String.stripIndent(), public java.lang.String java.lang.String.stripLeading(), public java.lang.String java.lang.String.stripTrailing(), public java.lang.String java.lang.String.substring(int), public java.lang.String java.lang.String.substring(int,int), public java.lang.String java.lang.String.toLowerCase(), public java.lang.String java.lang.String.toLowerCase(java.util.Locale), public java.lang.String java.lang.String.toString(), public java.lang.String java.lang.String.toUpperCase(), public java.lang.String java.lang.String.toUpperCase(java.util.Locale), public java.lang.String java.lang.String.translateEscapes(), public java.lang.String java.lang.String.trim(), public java.lang.String[] java.lang.String.split(java.lang.String), public java.lang.String[] java.lang.String.split(java.lang.String,int), public java.util.Optional java.lang.String.describeConstable(), public java.util.stream.IntStream java.lang.String.chars(), public java.util.stream.IntStream java.lang.String.codePoints(), public java.util.stream.Stream java.lang.String.lines(), public native char java.lang.String.charAt(int), public native char[] java.lang.String.toCharArray(), public native int java.lang.String.compareTo(java.lang.String), public native java.lang.String java.lang.String.concat(java.lang.String), public native java.lang.String java.lang.String.intern(), public static java.lang.String java.lang.String.copyValueOf(char[]), public static java.lang.String java.lang.String.copyValueOf(char[],int,int), public static java.lang.String java.lang.String.format(java.lang.String,java.lang.Object[]), public static java.lang.String java.lang.String.format(java.util.Locale,java.lang.String,java.lang.Object[]), public static java.lang.String java.lang.String.join(java.lang.CharSequence,java.lang.CharSequence[]), public static java.lang.String java.lang.String.join(java.lang.CharSequence,java.lang.Iterable), public static java.lang.String java.lang.String.valueOf(boolean), public static java.lang.String java.lang.String.valueOf(char), public static java.lang.String java.lang.String.valueOf(char[]), public static java.lang.String java.lang.String.valueOf(char[],int,int), public static java.lang.String java.lang.String.valueOf(double), public static java.lang.String java.lang.String.valueOf(float), public static java.lang.String java.lang.String.valueOf(int), public static java.lang.String java.lang.String.valueOf(java.lang.Object), public static java.lang.String java.lang.String.valueOf(long), public void java.lang.String.getBytes(int,int,byte[],int), public void java.lang.String.getChars(int,int,char[],int), static byte[] java.lang.String.getBytesNoRepl(java.lang.String,java.nio.charset.Charset) throws java.nio.charset.CharacterCodingException, static int java.lang.String.indexOf(byte[],byte,int,java.lang.String,int), static int java.lang.String.lastIndexOf(byte[],byte,int,java.lang.String,int), static int java.lang.String.lastIndexOf(char[],int,int,char[],int,int,int), static java.lang.String java.lang.String.newStringNoRepl(byte[],java.nio.charset.Charset) throws java.nio.charset.CharacterCodingException, static java.lang.String java.lang.String.valueOfCodePoint(int), static void java.lang.String.checkBoundsBeginEnd(int,int,int), static void java.lang.String.checkBoundsOffCount(int,int,int), static void java.lang.String.checkIndex(int,int), static void java.lang.String.checkOffset(int,int), void java.lang.String.fillBytes(byte[],int,byte), void java.lang.String.getChars(char[],int)]
 []
 [interface java.io.Serializable, interface java.lang.Comparable, interface java.lang.CharSequence, interface java.lang.constant.Constable, interface java.lang.constant.ConstantDesc]
 0
diff --git a/test/100-reflect2/src/Main.java b/test/100-reflect2/src/Main.java
index 56b4a82f84..c5d9f5e54c 100644
--- a/test/100-reflect2/src/Main.java
+++ b/test/100-reflect2/src/Main.java
@@ -16,6 +16,7 @@
 
 import java.lang.reflect.*;
 import java.util.*;
+import java.util.function.Predicate;
 
 class Main {
   private static boolean z = true;
@@ -175,10 +176,20 @@ class Main {
       }
     };
 
+    // Filter out methods that D8 or R8 may generate for internal use.
+    Predicate<Method> filterGenerated = new Predicate<Method>() {
+      public boolean test(Method m) {
+        String name = m.getName();
+        return !name.startsWith("lambda$") && !name.startsWith("$r8$");
+      }
+    };
+
     // Sort the return values by their string values since the order is undefined by the spec.
     System.out.println(Arrays.toString(sort(String.class.getDeclaredConstructors(), comp)));
     System.out.println(Arrays.toString(sort(String.class.getDeclaredFields(), comp)));
-    System.out.println(Arrays.toString(sort(String.class.getDeclaredMethods(), comp)));
+    System.out.println(Arrays.toString(
+            sort(Arrays.stream(String.class.getDeclaredMethods()).filter(filterGenerated).toArray(),
+                    comp)));
 
     System.out.println(Arrays.toString(Main.class.getInterfaces()));
     System.out.println(Arrays.toString(String.class.getInterfaces()));
diff --git a/test/160-read-barrier-stress/info.txt b/test/160-read-barrier-stress/info.txt
index 505fe33338..77e7edc60d 100644
--- a/test/160-read-barrier-stress/info.txt
+++ b/test/160-read-barrier-stress/info.txt
@@ -1 +1,2 @@
-Test stressing read barriers for CC GC.
+Test stressing read barriers for CC GC. In case of CMC GC, stresses MOVE ioctl
+based compaction.
diff --git a/test/160-read-barrier-stress/src/Main.java b/test/160-read-barrier-stress/src/Main.java
index df403db5a7..9cb5185d57 100644
--- a/test/160-read-barrier-stress/src/Main.java
+++ b/test/160-read-barrier-stress/src/Main.java
@@ -500,6 +500,519 @@ public class Main {
 }
 
 class ManyFields extends ManyFieldsBase3 {
+    public static long fillClass0 = 0;
+    public static long fillClass1 = 1;
+    public static long fillClass2 = 2;
+    public static long fillClass3 = 3;
+    public static long fillClass4 = 4;
+    public static long fillClass5 = 5;
+    public static long fillClass6 = 6;
+    public static long fillClass7 = 7;
+    public static long fillClass8 = 8;
+    public static long fillClass9 = 9;
+    public static long fillClass10 = 10;
+    public static long fillClass11 = 11;
+    public static long fillClass12 = 12;
+    public static long fillClass13 = 13;
+    public static long fillClass14 = 14;
+    public static long fillClass15 = 15;
+    public static long fillClass16 = 16;
+    public static long fillClass17 = 17;
+    public static long fillClass18 = 18;
+    public static long fillClass19 = 19;
+    public static long fillClass20 = 20;
+    public static long fillClass21 = 21;
+    public static long fillClass22 = 22;
+    public static long fillClass23 = 23;
+    public static long fillClass24 = 24;
+    public static long fillClass25 = 25;
+    public static long fillClass26 = 26;
+    public static long fillClass27 = 27;
+    public static long fillClass28 = 28;
+    public static long fillClass29 = 29;
+    public static long fillClass30 = 30;
+    public static long fillClass31 = 31;
+    public static long fillClass32 = 32;
+    public static long fillClass33 = 33;
+    public static long fillClass34 = 34;
+    public static long fillClass35 = 35;
+    public static long fillClass36 = 36;
+    public static long fillClass37 = 37;
+    public static long fillClass38 = 38;
+    public static long fillClass39 = 39;
+    public static long fillClass40 = 40;
+    public static long fillClass41 = 41;
+    public static long fillClass42 = 42;
+    public static long fillClass43 = 43;
+    public static long fillClass44 = 44;
+    public static long fillClass45 = 45;
+    public static long fillClass46 = 46;
+    public static long fillClass47 = 47;
+    public static long fillClass48 = 48;
+    public static long fillClass49 = 49;
+    public static long fillClass50 = 50;
+    public static long fillClass51 = 51;
+    public static long fillClass52 = 52;
+    public static long fillClass53 = 53;
+    public static long fillClass54 = 54;
+    public static long fillClass55 = 55;
+    public static long fillClass56 = 56;
+    public static long fillClass57 = 57;
+    public static long fillClass58 = 58;
+    public static long fillClass59 = 59;
+    public static long fillClass60 = 60;
+    public static long fillClass61 = 61;
+    public static long fillClass62 = 62;
+    public static long fillClass63 = 63;
+    public static long fillClass64 = 64;
+    public static long fillClass65 = 65;
+    public static long fillClass66 = 66;
+    public static long fillClass67 = 67;
+    public static long fillClass68 = 68;
+    public static long fillClass69 = 69;
+    public static long fillClass70 = 70;
+    public static long fillClass71 = 71;
+    public static long fillClass72 = 72;
+    public static long fillClass73 = 73;
+    public static long fillClass74 = 74;
+    public static long fillClass75 = 75;
+    public static long fillClass76 = 76;
+    public static long fillClass77 = 77;
+    public static long fillClass78 = 78;
+    public static long fillClass79 = 79;
+    public static long fillClass80 = 80;
+    public static long fillClass81 = 81;
+    public static long fillClass82 = 82;
+    public static long fillClass83 = 83;
+    public static long fillClass84 = 84;
+    public static long fillClass85 = 85;
+    public static long fillClass86 = 86;
+    public static long fillClass87 = 87;
+    public static long fillClass88 = 88;
+    public static long fillClass89 = 89;
+    public static long fillClass90 = 90;
+    public static long fillClass91 = 91;
+    public static long fillClass92 = 92;
+    public static long fillClass93 = 93;
+    public static long fillClass94 = 94;
+    public static long fillClass95 = 95;
+    public static long fillClass96 = 96;
+    public static long fillClass97 = 97;
+    public static long fillClass98 = 98;
+    public static long fillClass99 = 99;
+    public static long fillClass100 = 100;
+    public static long fillClass101 = 101;
+    public static long fillClass102 = 102;
+    public static long fillClass103 = 103;
+    public static long fillClass104 = 104;
+    public static long fillClass105 = 105;
+    public static long fillClass106 = 106;
+    public static long fillClass107 = 107;
+    public static long fillClass108 = 108;
+    public static long fillClass109 = 109;
+    public static long fillClass110 = 110;
+    public static long fillClass111 = 111;
+    public static long fillClass112 = 112;
+    public static long fillClass113 = 113;
+    public static long fillClass114 = 114;
+    public static long fillClass115 = 115;
+    public static long fillClass116 = 116;
+    public static long fillClass117 = 117;
+    public static long fillClass118 = 118;
+    public static long fillClass119 = 119;
+    public static long fillClass120 = 120;
+    public static long fillClass121 = 121;
+    public static long fillClass122 = 122;
+    public static long fillClass123 = 123;
+    public static long fillClass124 = 124;
+    public static long fillClass125 = 125;
+    public static long fillClass126 = 126;
+    public static long fillClass127 = 127;
+    public static long fillClass128 = 128;
+    public static long fillClass129 = 129;
+    public static long fillClass130 = 130;
+    public static long fillClass131 = 131;
+    public static long fillClass132 = 132;
+    public static long fillClass133 = 133;
+    public static long fillClass134 = 134;
+    public static long fillClass135 = 135;
+    public static long fillClass136 = 136;
+    public static long fillClass137 = 137;
+    public static long fillClass138 = 138;
+    public static long fillClass139 = 139;
+    public static long fillClass140 = 140;
+    public static long fillClass141 = 141;
+    public static long fillClass142 = 142;
+    public static long fillClass143 = 143;
+    public static long fillClass144 = 144;
+    public static long fillClass145 = 145;
+    public static long fillClass146 = 146;
+    public static long fillClass147 = 147;
+    public static long fillClass148 = 148;
+    public static long fillClass149 = 149;
+    public static long fillClass150 = 150;
+    public static long fillClass151 = 151;
+    public static long fillClass152 = 152;
+    public static long fillClass153 = 153;
+    public static long fillClass154 = 154;
+    public static long fillClass155 = 155;
+    public static long fillClass156 = 156;
+    public static long fillClass157 = 157;
+    public static long fillClass158 = 158;
+    public static long fillClass159 = 159;
+    public static long fillClass160 = 160;
+    public static long fillClass161 = 161;
+    public static long fillClass162 = 162;
+    public static long fillClass163 = 163;
+    public static long fillClass164 = 164;
+    public static long fillClass165 = 165;
+    public static long fillClass166 = 166;
+    public static long fillClass167 = 167;
+    public static long fillClass168 = 168;
+    public static long fillClass169 = 169;
+    public static long fillClass170 = 170;
+    public static long fillClass171 = 171;
+    public static long fillClass172 = 172;
+    public static long fillClass173 = 173;
+    public static long fillClass174 = 174;
+    public static long fillClass175 = 175;
+    public static long fillClass176 = 176;
+    public static long fillClass177 = 177;
+    public static long fillClass178 = 178;
+    public static long fillClass179 = 179;
+    public static long fillClass180 = 180;
+    public static long fillClass181 = 181;
+    public static long fillClass182 = 182;
+    public static long fillClass183 = 183;
+    public static long fillClass184 = 184;
+    public static long fillClass185 = 185;
+    public static long fillClass186 = 186;
+    public static long fillClass187 = 187;
+    public static long fillClass188 = 188;
+    public static long fillClass189 = 189;
+    public static long fillClass190 = 190;
+    public static long fillClass191 = 191;
+    public static long fillClass192 = 192;
+    public static long fillClass193 = 193;
+    public static long fillClass194 = 194;
+    public static long fillClass195 = 195;
+    public static long fillClass196 = 196;
+    public static long fillClass197 = 197;
+    public static long fillClass198 = 198;
+    public static long fillClass199 = 199;
+    public static long fillClass200 = 200;
+    public static long fillClass201 = 201;
+    public static long fillClass202 = 202;
+    public static long fillClass203 = 203;
+    public static long fillClass204 = 204;
+    public static long fillClass205 = 205;
+    public static long fillClass206 = 206;
+    public static long fillClass207 = 207;
+    public static long fillClass208 = 208;
+    public static long fillClass209 = 209;
+    public static long fillClass210 = 210;
+    public static long fillClass211 = 211;
+    public static long fillClass212 = 212;
+    public static long fillClass213 = 213;
+    public static long fillClass214 = 214;
+    public static long fillClass215 = 215;
+    public static long fillClass216 = 216;
+    public static long fillClass217 = 217;
+    public static long fillClass218 = 218;
+    public static long fillClass219 = 219;
+    public static long fillClass220 = 220;
+    public static long fillClass221 = 221;
+    public static long fillClass222 = 222;
+    public static long fillClass223 = 223;
+    public static long fillClass224 = 224;
+    public static long fillClass225 = 225;
+    public static long fillClass226 = 226;
+    public static long fillClass227 = 227;
+    public static long fillClass228 = 228;
+    public static long fillClass229 = 229;
+    public static long fillClass230 = 230;
+    public static long fillClass231 = 231;
+    public static long fillClass232 = 232;
+    public static long fillClass233 = 233;
+    public static long fillClass234 = 234;
+    public static long fillClass235 = 235;
+    public static long fillClass236 = 236;
+    public static long fillClass237 = 237;
+    public static long fillClass238 = 238;
+    public static long fillClass239 = 239;
+    public static long fillClass240 = 240;
+    public static long fillClass241 = 241;
+    public static long fillClass242 = 242;
+    public static long fillClass243 = 243;
+    public static long fillClass244 = 244;
+    public static long fillClass245 = 245;
+    public static long fillClass246 = 246;
+    public static long fillClass247 = 247;
+    public static long fillClass248 = 248;
+    public static long fillClass249 = 249;
+    public static long fillClass250 = 250;
+    public static long fillClass251 = 251;
+    public static long fillClass252 = 252;
+    public static long fillClass253 = 253;
+    public static long fillClass254 = 254;
+    public static long fillClass255 = 255;
+    public static long fillClass256 = 256;
+    public static long fillClass257 = 257;
+    public static long fillClass258 = 258;
+    public static long fillClass259 = 259;
+    public static long fillClass260 = 260;
+    public static long fillClass261 = 261;
+    public static long fillClass262 = 262;
+    public static long fillClass263 = 263;
+    public static long fillClass264 = 264;
+    public static long fillClass265 = 265;
+    public static long fillClass266 = 266;
+    public static long fillClass267 = 267;
+    public static long fillClass268 = 268;
+    public static long fillClass269 = 269;
+    public static long fillClass270 = 270;
+    public static long fillClass271 = 271;
+    public static long fillClass272 = 272;
+    public static long fillClass273 = 273;
+    public static long fillClass274 = 274;
+    public static long fillClass275 = 275;
+    public static long fillClass276 = 276;
+    public static long fillClass277 = 277;
+    public static long fillClass278 = 278;
+    public static long fillClass279 = 279;
+    public static long fillClass280 = 280;
+    public static long fillClass281 = 281;
+    public static long fillClass282 = 282;
+    public static long fillClass283 = 283;
+    public static long fillClass284 = 284;
+    public static long fillClass285 = 285;
+    public static long fillClass286 = 286;
+    public static long fillClass287 = 287;
+    public static long fillClass288 = 288;
+    public static long fillClass289 = 289;
+    public static long fillClass290 = 290;
+    public static long fillClass291 = 291;
+    public static long fillClass292 = 292;
+    public static long fillClass293 = 293;
+    public static long fillClass294 = 294;
+    public static long fillClass295 = 295;
+    public static long fillClass296 = 296;
+    public static long fillClass297 = 297;
+    public static long fillClass298 = 298;
+    public static long fillClass299 = 299;
+    public static long fillClass300 = 300;
+    public static long fillClass301 = 301;
+    public static long fillClass302 = 302;
+    public static long fillClass303 = 303;
+    public static long fillClass304 = 304;
+    public static long fillClass305 = 305;
+    public static long fillClass306 = 306;
+    public static long fillClass307 = 307;
+    public static long fillClass308 = 308;
+    public static long fillClass309 = 309;
+    public static long fillClass310 = 310;
+    public static long fillClass311 = 311;
+    public static long fillClass312 = 312;
+    public static long fillClass313 = 313;
+    public static long fillClass314 = 314;
+    public static long fillClass315 = 315;
+    public static long fillClass316 = 316;
+    public static long fillClass317 = 317;
+    public static long fillClass318 = 318;
+    public static long fillClass319 = 319;
+    public static long fillClass320 = 320;
+    public static long fillClass321 = 321;
+    public static long fillClass322 = 322;
+    public static long fillClass323 = 323;
+    public static long fillClass324 = 324;
+    public static long fillClass325 = 325;
+    public static long fillClass326 = 326;
+    public static long fillClass327 = 327;
+    public static long fillClass328 = 328;
+    public static long fillClass329 = 329;
+    public static long fillClass330 = 330;
+    public static long fillClass331 = 331;
+    public static long fillClass332 = 332;
+    public static long fillClass333 = 333;
+    public static long fillClass334 = 334;
+    public static long fillClass335 = 335;
+    public static long fillClass336 = 336;
+    public static long fillClass337 = 337;
+    public static long fillClass338 = 338;
+    public static long fillClass339 = 339;
+    public static long fillClass340 = 340;
+    public static long fillClass341 = 341;
+    public static long fillClass342 = 342;
+    public static long fillClass343 = 343;
+    public static long fillClass344 = 344;
+    public static long fillClass345 = 345;
+    public static long fillClass346 = 346;
+    public static long fillClass347 = 347;
+    public static long fillClass348 = 348;
+    public static long fillClass349 = 349;
+    public static long fillClass350 = 350;
+    public static long fillClass351 = 351;
+    public static long fillClass352 = 352;
+    public static long fillClass353 = 353;
+    public static long fillClass354 = 354;
+    public static long fillClass355 = 355;
+    public static long fillClass356 = 356;
+    public static long fillClass357 = 357;
+    public static long fillClass358 = 358;
+    public static long fillClass359 = 359;
+    public static long fillClass360 = 360;
+    public static long fillClass361 = 361;
+    public static long fillClass362 = 362;
+    public static long fillClass363 = 363;
+    public static long fillClass364 = 364;
+    public static long fillClass365 = 365;
+    public static long fillClass366 = 366;
+    public static long fillClass367 = 367;
+    public static long fillClass368 = 368;
+    public static long fillClass369 = 369;
+    public static long fillClass370 = 370;
+    public static long fillClass371 = 371;
+    public static long fillClass372 = 372;
+    public static long fillClass373 = 373;
+    public static long fillClass374 = 374;
+    public static long fillClass375 = 375;
+    public static long fillClass376 = 376;
+    public static long fillClass377 = 377;
+    public static long fillClass378 = 378;
+    public static long fillClass379 = 379;
+    public static long fillClass380 = 380;
+    public static long fillClass381 = 381;
+    public static long fillClass382 = 382;
+    public static long fillClass383 = 383;
+    public static long fillClass384 = 384;
+    public static long fillClass385 = 385;
+    public static long fillClass386 = 386;
+    public static long fillClass387 = 387;
+    public static long fillClass388 = 388;
+    public static long fillClass389 = 389;
+    public static long fillClass390 = 390;
+    public static long fillClass391 = 391;
+    public static long fillClass392 = 392;
+    public static long fillClass393 = 393;
+    public static long fillClass394 = 394;
+    public static long fillClass395 = 395;
+    public static long fillClass396 = 396;
+    public static long fillClass397 = 397;
+    public static long fillClass398 = 398;
+    public static long fillClass399 = 399;
+    public static long fillClass400 = 400;
+    public static long fillClass401 = 401;
+    public static long fillClass402 = 402;
+    public static long fillClass403 = 403;
+    public static long fillClass404 = 404;
+    public static long fillClass405 = 405;
+    public static long fillClass406 = 406;
+    public static long fillClass407 = 407;
+    public static long fillClass408 = 408;
+    public static long fillClass409 = 409;
+    public static long fillClass410 = 410;
+    public static long fillClass411 = 411;
+    public static long fillClass412 = 412;
+    public static long fillClass413 = 413;
+    public static long fillClass414 = 414;
+    public static long fillClass415 = 415;
+    public static long fillClass416 = 416;
+    public static long fillClass417 = 417;
+    public static long fillClass418 = 418;
+    public static long fillClass419 = 419;
+    public static long fillClass420 = 420;
+    public static long fillClass421 = 421;
+    public static long fillClass422 = 422;
+    public static long fillClass423 = 423;
+    public static long fillClass424 = 424;
+    public static long fillClass425 = 425;
+    public static long fillClass426 = 426;
+    public static long fillClass427 = 427;
+    public static long fillClass428 = 428;
+    public static long fillClass429 = 429;
+    public static long fillClass430 = 430;
+    public static long fillClass431 = 431;
+    public static long fillClass432 = 432;
+    public static long fillClass433 = 433;
+    public static long fillClass434 = 434;
+    public static long fillClass435 = 435;
+    public static long fillClass436 = 436;
+    public static long fillClass437 = 437;
+    public static long fillClass438 = 438;
+    public static long fillClass439 = 439;
+    public static long fillClass440 = 440;
+    public static long fillClass441 = 441;
+    public static long fillClass442 = 442;
+    public static long fillClass443 = 443;
+    public static long fillClass444 = 444;
+    public static long fillClass445 = 445;
+    public static long fillClass446 = 446;
+    public static long fillClass447 = 447;
+    public static long fillClass448 = 448;
+    public static long fillClass449 = 449;
+    public static long fillClass450 = 450;
+    public static long fillClass451 = 451;
+    public static long fillClass452 = 452;
+    public static long fillClass453 = 453;
+    public static long fillClass454 = 454;
+    public static long fillClass455 = 455;
+    public static long fillClass456 = 456;
+    public static long fillClass457 = 457;
+    public static long fillClass458 = 458;
+    public static long fillClass459 = 459;
+    public static long fillClass460 = 460;
+    public static long fillClass461 = 461;
+    public static long fillClass462 = 462;
+    public static long fillClass463 = 463;
+    public static long fillClass464 = 464;
+    public static long fillClass465 = 465;
+    public static long fillClass466 = 466;
+    public static long fillClass467 = 467;
+    public static long fillClass468 = 468;
+    public static long fillClass469 = 469;
+    public static long fillClass470 = 470;
+    public static long fillClass471 = 471;
+    public static long fillClass472 = 472;
+    public static long fillClass473 = 473;
+    public static long fillClass474 = 474;
+    public static long fillClass475 = 475;
+    public static long fillClass476 = 476;
+    public static long fillClass477 = 477;
+    public static long fillClass478 = 478;
+    public static long fillClass479 = 479;
+    public static long fillClass480 = 480;
+    public static long fillClass481 = 481;
+    public static long fillClass482 = 482;
+    public static long fillClass483 = 483;
+    public static long fillClass484 = 484;
+    public static long fillClass485 = 485;
+    public static long fillClass486 = 486;
+    public static long fillClass487 = 487;
+    public static long fillClass488 = 488;
+    public static long fillClass489 = 489;
+    public static long fillClass490 = 490;
+    public static long fillClass491 = 491;
+    public static long fillClass492 = 492;
+    public static long fillClass493 = 493;
+    public static long fillClass494 = 494;
+    public static long fillClass495 = 495;
+    public static long fillClass496 = 496;
+    public static long fillClass497 = 497;
+    public static long fillClass498 = 498;
+    public static long fillClass499 = 499;
+    public static long fillClass500 = 500;
+    public static long fillClass501 = 501;
+    public static long fillClass502 = 502;
+    public static long fillClass503 = 503;
+    public static long fillClass504 = 504;
+    public static long fillClass505 = 505;
+    public static long fillClass506 = 506;
+    public static long fillClass507 = 507;
+    public static long fillClass508 = 508;
+    public static long fillClass509 = 509;
+    public static long fillClass510 = 510;
+    public static long fillClass511 = 511;
+
     public Object testField4000 = new Integer(4000);
     public Object testField4001 = new Integer(4001);
     public Object testField4002 = new Integer(4002);
diff --git a/test/160-read-barrier-stress/src/ManyFieldsBase0.java b/test/160-read-barrier-stress/src/ManyFieldsBase0.java
index 1b6c2a608b..98220b94fe 100644
--- a/test/160-read-barrier-stress/src/ManyFieldsBase0.java
+++ b/test/160-read-barrier-stress/src/ManyFieldsBase0.java
@@ -1015,4 +1015,5005 @@ class ManyFieldsBase0 {
     public Object testField0997 = new Integer(997);
     public Object testField0998 = new Integer(998);
     public Object testField0999 = new Integer(999);
+
+    public long fillRefBitmap0 = 0;
+    public long fillRefBitmap1 = 1;
+    public long fillRefBitmap2 = 2;
+    public long fillRefBitmap3 = 3;
+    public long fillRefBitmap4 = 4;
+    public long fillRefBitmap5 = 5;
+    public long fillRefBitmap6 = 6;
+    public long fillRefBitmap7 = 7;
+    public long fillRefBitmap8 = 8;
+    public long fillRefBitmap9 = 9;
+    public long fillRefBitmap10 = 10;
+    public long fillRefBitmap11 = 11;
+    public long fillRefBitmap12 = 12;
+    public long fillRefBitmap13 = 13;
+    public long fillRefBitmap14 = 14;
+    public long fillRefBitmap15 = 15;
+    public long fillRefBitmap16 = 16;
+    public long fillRefBitmap17 = 17;
+    public long fillRefBitmap18 = 18;
+    public long fillRefBitmap19 = 19;
+    public long fillRefBitmap20 = 20;
+    public long fillRefBitmap21 = 21;
+    public long fillRefBitmap22 = 22;
+    public long fillRefBitmap23 = 23;
+    public long fillRefBitmap24 = 24;
+    public long fillRefBitmap25 = 25;
+    public long fillRefBitmap26 = 26;
+    public long fillRefBitmap27 = 27;
+    public long fillRefBitmap28 = 28;
+    public long fillRefBitmap29 = 29;
+    public long fillRefBitmap30 = 30;
+    public long fillRefBitmap31 = 31;
+    public long fillRefBitmap32 = 32;
+    public long fillRefBitmap33 = 33;
+    public long fillRefBitmap34 = 34;
+    public long fillRefBitmap35 = 35;
+    public long fillRefBitmap36 = 36;
+    public long fillRefBitmap37 = 37;
+    public long fillRefBitmap38 = 38;
+    public long fillRefBitmap39 = 39;
+    public long fillRefBitmap40 = 40;
+    public long fillRefBitmap41 = 41;
+    public long fillRefBitmap42 = 42;
+    public long fillRefBitmap43 = 43;
+    public long fillRefBitmap44 = 44;
+    public long fillRefBitmap45 = 45;
+    public long fillRefBitmap46 = 46;
+    public long fillRefBitmap47 = 47;
+    public long fillRefBitmap48 = 48;
+    public long fillRefBitmap49 = 49;
+    public long fillRefBitmap50 = 50;
+    public long fillRefBitmap51 = 51;
+    public long fillRefBitmap52 = 52;
+    public long fillRefBitmap53 = 53;
+    public long fillRefBitmap54 = 54;
+    public long fillRefBitmap55 = 55;
+    public long fillRefBitmap56 = 56;
+    public long fillRefBitmap57 = 57;
+    public long fillRefBitmap58 = 58;
+    public long fillRefBitmap59 = 59;
+    public long fillRefBitmap60 = 60;
+    public long fillRefBitmap61 = 61;
+    public long fillRefBitmap62 = 62;
+    public long fillRefBitmap63 = 63;
+    public long fillRefBitmap64 = 64;
+    public long fillRefBitmap65 = 65;
+    public long fillRefBitmap66 = 66;
+    public long fillRefBitmap67 = 67;
+    public long fillRefBitmap68 = 68;
+    public long fillRefBitmap69 = 69;
+    public long fillRefBitmap70 = 70;
+    public long fillRefBitmap71 = 71;
+    public long fillRefBitmap72 = 72;
+    public long fillRefBitmap73 = 73;
+    public long fillRefBitmap74 = 74;
+    public long fillRefBitmap75 = 75;
+    public long fillRefBitmap76 = 76;
+    public long fillRefBitmap77 = 77;
+    public long fillRefBitmap78 = 78;
+    public long fillRefBitmap79 = 79;
+    public long fillRefBitmap80 = 80;
+    public long fillRefBitmap81 = 81;
+    public long fillRefBitmap82 = 82;
+    public long fillRefBitmap83 = 83;
+    public long fillRefBitmap84 = 84;
+    public long fillRefBitmap85 = 85;
+    public long fillRefBitmap86 = 86;
+    public long fillRefBitmap87 = 87;
+    public long fillRefBitmap88 = 88;
+    public long fillRefBitmap89 = 89;
+    public long fillRefBitmap90 = 90;
+    public long fillRefBitmap91 = 91;
+    public long fillRefBitmap92 = 92;
+    public long fillRefBitmap93 = 93;
+    public long fillRefBitmap94 = 94;
+    public long fillRefBitmap95 = 95;
+    public long fillRefBitmap96 = 96;
+    public long fillRefBitmap97 = 97;
+    public long fillRefBitmap98 = 98;
+    public long fillRefBitmap99 = 99;
+    public long fillRefBitmap100 = 100;
+    public long fillRefBitmap101 = 101;
+    public long fillRefBitmap102 = 102;
+    public long fillRefBitmap103 = 103;
+    public long fillRefBitmap104 = 104;
+    public long fillRefBitmap105 = 105;
+    public long fillRefBitmap106 = 106;
+    public long fillRefBitmap107 = 107;
+    public long fillRefBitmap108 = 108;
+    public long fillRefBitmap109 = 109;
+    public long fillRefBitmap110 = 110;
+    public long fillRefBitmap111 = 111;
+    public long fillRefBitmap112 = 112;
+    public long fillRefBitmap113 = 113;
+    public long fillRefBitmap114 = 114;
+    public long fillRefBitmap115 = 115;
+    public long fillRefBitmap116 = 116;
+    public long fillRefBitmap117 = 117;
+    public long fillRefBitmap118 = 118;
+    public long fillRefBitmap119 = 119;
+    public long fillRefBitmap120 = 120;
+    public long fillRefBitmap121 = 121;
+    public long fillRefBitmap122 = 122;
+    public long fillRefBitmap123 = 123;
+    public long fillRefBitmap124 = 124;
+    public long fillRefBitmap125 = 125;
+    public long fillRefBitmap126 = 126;
+    public long fillRefBitmap127 = 127;
+    public long fillRefBitmap128 = 128;
+    public long fillRefBitmap129 = 129;
+    public long fillRefBitmap130 = 130;
+    public long fillRefBitmap131 = 131;
+    public long fillRefBitmap132 = 132;
+    public long fillRefBitmap133 = 133;
+    public long fillRefBitmap134 = 134;
+    public long fillRefBitmap135 = 135;
+    public long fillRefBitmap136 = 136;
+    public long fillRefBitmap137 = 137;
+    public long fillRefBitmap138 = 138;
+    public long fillRefBitmap139 = 139;
+    public long fillRefBitmap140 = 140;
+    public long fillRefBitmap141 = 141;
+    public long fillRefBitmap142 = 142;
+    public long fillRefBitmap143 = 143;
+    public long fillRefBitmap144 = 144;
+    public long fillRefBitmap145 = 145;
+    public long fillRefBitmap146 = 146;
+    public long fillRefBitmap147 = 147;
+    public long fillRefBitmap148 = 148;
+    public long fillRefBitmap149 = 149;
+    public long fillRefBitmap150 = 150;
+    public long fillRefBitmap151 = 151;
+    public long fillRefBitmap152 = 152;
+    public long fillRefBitmap153 = 153;
+    public long fillRefBitmap154 = 154;
+    public long fillRefBitmap155 = 155;
+    public long fillRefBitmap156 = 156;
+    public long fillRefBitmap157 = 157;
+    public long fillRefBitmap158 = 158;
+    public long fillRefBitmap159 = 159;
+    public long fillRefBitmap160 = 160;
+    public long fillRefBitmap161 = 161;
+    public long fillRefBitmap162 = 162;
+    public long fillRefBitmap163 = 163;
+    public long fillRefBitmap164 = 164;
+    public long fillRefBitmap165 = 165;
+    public long fillRefBitmap166 = 166;
+    public long fillRefBitmap167 = 167;
+    public long fillRefBitmap168 = 168;
+    public long fillRefBitmap169 = 169;
+    public long fillRefBitmap170 = 170;
+    public long fillRefBitmap171 = 171;
+    public long fillRefBitmap172 = 172;
+    public long fillRefBitmap173 = 173;
+    public long fillRefBitmap174 = 174;
+    public long fillRefBitmap175 = 175;
+    public long fillRefBitmap176 = 176;
+    public long fillRefBitmap177 = 177;
+    public long fillRefBitmap178 = 178;
+    public long fillRefBitmap179 = 179;
+    public long fillRefBitmap180 = 180;
+    public long fillRefBitmap181 = 181;
+    public long fillRefBitmap182 = 182;
+    public long fillRefBitmap183 = 183;
+    public long fillRefBitmap184 = 184;
+    public long fillRefBitmap185 = 185;
+    public long fillRefBitmap186 = 186;
+    public long fillRefBitmap187 = 187;
+    public long fillRefBitmap188 = 188;
+    public long fillRefBitmap189 = 189;
+    public long fillRefBitmap190 = 190;
+    public long fillRefBitmap191 = 191;
+    public long fillRefBitmap192 = 192;
+    public long fillRefBitmap193 = 193;
+    public long fillRefBitmap194 = 194;
+    public long fillRefBitmap195 = 195;
+    public long fillRefBitmap196 = 196;
+    public long fillRefBitmap197 = 197;
+    public long fillRefBitmap198 = 198;
+    public long fillRefBitmap199 = 199;
+    public long fillRefBitmap200 = 200;
+    public long fillRefBitmap201 = 201;
+    public long fillRefBitmap202 = 202;
+    public long fillRefBitmap203 = 203;
+    public long fillRefBitmap204 = 204;
+    public long fillRefBitmap205 = 205;
+    public long fillRefBitmap206 = 206;
+    public long fillRefBitmap207 = 207;
+    public long fillRefBitmap208 = 208;
+    public long fillRefBitmap209 = 209;
+    public long fillRefBitmap210 = 210;
+    public long fillRefBitmap211 = 211;
+    public long fillRefBitmap212 = 212;
+    public long fillRefBitmap213 = 213;
+    public long fillRefBitmap214 = 214;
+    public long fillRefBitmap215 = 215;
+    public long fillRefBitmap216 = 216;
+    public long fillRefBitmap217 = 217;
+    public long fillRefBitmap218 = 218;
+    public long fillRefBitmap219 = 219;
+    public long fillRefBitmap220 = 220;
+    public long fillRefBitmap221 = 221;
+    public long fillRefBitmap222 = 222;
+    public long fillRefBitmap223 = 223;
+    public long fillRefBitmap224 = 224;
+    public long fillRefBitmap225 = 225;
+    public long fillRefBitmap226 = 226;
+    public long fillRefBitmap227 = 227;
+    public long fillRefBitmap228 = 228;
+    public long fillRefBitmap229 = 229;
+    public long fillRefBitmap230 = 230;
+    public long fillRefBitmap231 = 231;
+    public long fillRefBitmap232 = 232;
+    public long fillRefBitmap233 = 233;
+    public long fillRefBitmap234 = 234;
+    public long fillRefBitmap235 = 235;
+    public long fillRefBitmap236 = 236;
+    public long fillRefBitmap237 = 237;
+    public long fillRefBitmap238 = 238;
+    public long fillRefBitmap239 = 239;
+    public long fillRefBitmap240 = 240;
+    public long fillRefBitmap241 = 241;
+    public long fillRefBitmap242 = 242;
+    public long fillRefBitmap243 = 243;
+    public long fillRefBitmap244 = 244;
+    public long fillRefBitmap245 = 245;
+    public long fillRefBitmap246 = 246;
+    public long fillRefBitmap247 = 247;
+    public long fillRefBitmap248 = 248;
+    public long fillRefBitmap249 = 249;
+    public long fillRefBitmap250 = 250;
+    public long fillRefBitmap251 = 251;
+    public long fillRefBitmap252 = 252;
+    public long fillRefBitmap253 = 253;
+    public long fillRefBitmap254 = 254;
+    public long fillRefBitmap255 = 255;
+    public long fillRefBitmap256 = 256;
+    public long fillRefBitmap257 = 257;
+    public long fillRefBitmap258 = 258;
+    public long fillRefBitmap259 = 259;
+    public long fillRefBitmap260 = 260;
+    public long fillRefBitmap261 = 261;
+    public long fillRefBitmap262 = 262;
+    public long fillRefBitmap263 = 263;
+    public long fillRefBitmap264 = 264;
+    public long fillRefBitmap265 = 265;
+    public long fillRefBitmap266 = 266;
+    public long fillRefBitmap267 = 267;
+    public long fillRefBitmap268 = 268;
+    public long fillRefBitmap269 = 269;
+    public long fillRefBitmap270 = 270;
+    public long fillRefBitmap271 = 271;
+    public long fillRefBitmap272 = 272;
+    public long fillRefBitmap273 = 273;
+    public long fillRefBitmap274 = 274;
+    public long fillRefBitmap275 = 275;
+    public long fillRefBitmap276 = 276;
+    public long fillRefBitmap277 = 277;
+    public long fillRefBitmap278 = 278;
+    public long fillRefBitmap279 = 279;
+    public long fillRefBitmap280 = 280;
+    public long fillRefBitmap281 = 281;
+    public long fillRefBitmap282 = 282;
+    public long fillRefBitmap283 = 283;
+    public long fillRefBitmap284 = 284;
+    public long fillRefBitmap285 = 285;
+    public long fillRefBitmap286 = 286;
+    public long fillRefBitmap287 = 287;
+    public long fillRefBitmap288 = 288;
+    public long fillRefBitmap289 = 289;
+    public long fillRefBitmap290 = 290;
+    public long fillRefBitmap291 = 291;
+    public long fillRefBitmap292 = 292;
+    public long fillRefBitmap293 = 293;
+    public long fillRefBitmap294 = 294;
+    public long fillRefBitmap295 = 295;
+    public long fillRefBitmap296 = 296;
+    public long fillRefBitmap297 = 297;
+    public long fillRefBitmap298 = 298;
+    public long fillRefBitmap299 = 299;
+    public long fillRefBitmap300 = 300;
+    public long fillRefBitmap301 = 301;
+    public long fillRefBitmap302 = 302;
+    public long fillRefBitmap303 = 303;
+    public long fillRefBitmap304 = 304;
+    public long fillRefBitmap305 = 305;
+    public long fillRefBitmap306 = 306;
+    public long fillRefBitmap307 = 307;
+    public long fillRefBitmap308 = 308;
+    public long fillRefBitmap309 = 309;
+    public long fillRefBitmap310 = 310;
+    public long fillRefBitmap311 = 311;
+    public long fillRefBitmap312 = 312;
+    public long fillRefBitmap313 = 313;
+    public long fillRefBitmap314 = 314;
+    public long fillRefBitmap315 = 315;
+    public long fillRefBitmap316 = 316;
+    public long fillRefBitmap317 = 317;
+    public long fillRefBitmap318 = 318;
+    public long fillRefBitmap319 = 319;
+    public long fillRefBitmap320 = 320;
+    public long fillRefBitmap321 = 321;
+    public long fillRefBitmap322 = 322;
+    public long fillRefBitmap323 = 323;
+    public long fillRefBitmap324 = 324;
+    public long fillRefBitmap325 = 325;
+    public long fillRefBitmap326 = 326;
+    public long fillRefBitmap327 = 327;
+    public long fillRefBitmap328 = 328;
+    public long fillRefBitmap329 = 329;
+    public long fillRefBitmap330 = 330;
+    public long fillRefBitmap331 = 331;
+    public long fillRefBitmap332 = 332;
+    public long fillRefBitmap333 = 333;
+    public long fillRefBitmap334 = 334;
+    public long fillRefBitmap335 = 335;
+    public long fillRefBitmap336 = 336;
+    public long fillRefBitmap337 = 337;
+    public long fillRefBitmap338 = 338;
+    public long fillRefBitmap339 = 339;
+    public long fillRefBitmap340 = 340;
+    public long fillRefBitmap341 = 341;
+    public long fillRefBitmap342 = 342;
+    public long fillRefBitmap343 = 343;
+    public long fillRefBitmap344 = 344;
+    public long fillRefBitmap345 = 345;
+    public long fillRefBitmap346 = 346;
+    public long fillRefBitmap347 = 347;
+    public long fillRefBitmap348 = 348;
+    public long fillRefBitmap349 = 349;
+    public long fillRefBitmap350 = 350;
+    public long fillRefBitmap351 = 351;
+    public long fillRefBitmap352 = 352;
+    public long fillRefBitmap353 = 353;
+    public long fillRefBitmap354 = 354;
+    public long fillRefBitmap355 = 355;
+    public long fillRefBitmap356 = 356;
+    public long fillRefBitmap357 = 357;
+    public long fillRefBitmap358 = 358;
+    public long fillRefBitmap359 = 359;
+    public long fillRefBitmap360 = 360;
+    public long fillRefBitmap361 = 361;
+    public long fillRefBitmap362 = 362;
+    public long fillRefBitmap363 = 363;
+    public long fillRefBitmap364 = 364;
+    public long fillRefBitmap365 = 365;
+    public long fillRefBitmap366 = 366;
+    public long fillRefBitmap367 = 367;
+    public long fillRefBitmap368 = 368;
+    public long fillRefBitmap369 = 369;
+    public long fillRefBitmap370 = 370;
+    public long fillRefBitmap371 = 371;
+    public long fillRefBitmap372 = 372;
+    public long fillRefBitmap373 = 373;
+    public long fillRefBitmap374 = 374;
+    public long fillRefBitmap375 = 375;
+    public long fillRefBitmap376 = 376;
+    public long fillRefBitmap377 = 377;
+    public long fillRefBitmap378 = 378;
+    public long fillRefBitmap379 = 379;
+    public long fillRefBitmap380 = 380;
+    public long fillRefBitmap381 = 381;
+    public long fillRefBitmap382 = 382;
+    public long fillRefBitmap383 = 383;
+    public long fillRefBitmap384 = 384;
+    public long fillRefBitmap385 = 385;
+    public long fillRefBitmap386 = 386;
+    public long fillRefBitmap387 = 387;
+    public long fillRefBitmap388 = 388;
+    public long fillRefBitmap389 = 389;
+    public long fillRefBitmap390 = 390;
+    public long fillRefBitmap391 = 391;
+    public long fillRefBitmap392 = 392;
+    public long fillRefBitmap393 = 393;
+    public long fillRefBitmap394 = 394;
+    public long fillRefBitmap395 = 395;
+    public long fillRefBitmap396 = 396;
+    public long fillRefBitmap397 = 397;
+    public long fillRefBitmap398 = 398;
+    public long fillRefBitmap399 = 399;
+    public long fillRefBitmap400 = 400;
+    public long fillRefBitmap401 = 401;
+    public long fillRefBitmap402 = 402;
+    public long fillRefBitmap403 = 403;
+    public long fillRefBitmap404 = 404;
+    public long fillRefBitmap405 = 405;
+    public long fillRefBitmap406 = 406;
+    public long fillRefBitmap407 = 407;
+    public long fillRefBitmap408 = 408;
+    public long fillRefBitmap409 = 409;
+    public long fillRefBitmap410 = 410;
+    public long fillRefBitmap411 = 411;
+    public long fillRefBitmap412 = 412;
+    public long fillRefBitmap413 = 413;
+    public long fillRefBitmap414 = 414;
+    public long fillRefBitmap415 = 415;
+    public long fillRefBitmap416 = 416;
+    public long fillRefBitmap417 = 417;
+    public long fillRefBitmap418 = 418;
+    public long fillRefBitmap419 = 419;
+    public long fillRefBitmap420 = 420;
+    public long fillRefBitmap421 = 421;
+    public long fillRefBitmap422 = 422;
+    public long fillRefBitmap423 = 423;
+    public long fillRefBitmap424 = 424;
+    public long fillRefBitmap425 = 425;
+    public long fillRefBitmap426 = 426;
+    public long fillRefBitmap427 = 427;
+    public long fillRefBitmap428 = 428;
+    public long fillRefBitmap429 = 429;
+    public long fillRefBitmap430 = 430;
+    public long fillRefBitmap431 = 431;
+    public long fillRefBitmap432 = 432;
+    public long fillRefBitmap433 = 433;
+    public long fillRefBitmap434 = 434;
+    public long fillRefBitmap435 = 435;
+    public long fillRefBitmap436 = 436;
+    public long fillRefBitmap437 = 437;
+    public long fillRefBitmap438 = 438;
+    public long fillRefBitmap439 = 439;
+    public long fillRefBitmap440 = 440;
+    public long fillRefBitmap441 = 441;
+    public long fillRefBitmap442 = 442;
+    public long fillRefBitmap443 = 443;
+    public long fillRefBitmap444 = 444;
+    public long fillRefBitmap445 = 445;
+    public long fillRefBitmap446 = 446;
+    public long fillRefBitmap447 = 447;
+    public long fillRefBitmap448 = 448;
+    public long fillRefBitmap449 = 449;
+    public long fillRefBitmap450 = 450;
+    public long fillRefBitmap451 = 451;
+    public long fillRefBitmap452 = 452;
+    public long fillRefBitmap453 = 453;
+    public long fillRefBitmap454 = 454;
+    public long fillRefBitmap455 = 455;
+    public long fillRefBitmap456 = 456;
+    public long fillRefBitmap457 = 457;
+    public long fillRefBitmap458 = 458;
+    public long fillRefBitmap459 = 459;
+    public long fillRefBitmap460 = 460;
+    public long fillRefBitmap461 = 461;
+    public long fillRefBitmap462 = 462;
+    public long fillRefBitmap463 = 463;
+    public long fillRefBitmap464 = 464;
+    public long fillRefBitmap465 = 465;
+    public long fillRefBitmap466 = 466;
+    public long fillRefBitmap467 = 467;
+    public long fillRefBitmap468 = 468;
+    public long fillRefBitmap469 = 469;
+    public long fillRefBitmap470 = 470;
+    public long fillRefBitmap471 = 471;
+    public long fillRefBitmap472 = 472;
+    public long fillRefBitmap473 = 473;
+    public long fillRefBitmap474 = 474;
+    public long fillRefBitmap475 = 475;
+    public long fillRefBitmap476 = 476;
+    public long fillRefBitmap477 = 477;
+    public long fillRefBitmap478 = 478;
+    public long fillRefBitmap479 = 479;
+    public long fillRefBitmap480 = 480;
+    public long fillRefBitmap481 = 481;
+    public long fillRefBitmap482 = 482;
+    public long fillRefBitmap483 = 483;
+    public long fillRefBitmap484 = 484;
+    public long fillRefBitmap485 = 485;
+    public long fillRefBitmap486 = 486;
+    public long fillRefBitmap487 = 487;
+    public long fillRefBitmap488 = 488;
+    public long fillRefBitmap489 = 489;
+    public long fillRefBitmap490 = 490;
+    public long fillRefBitmap491 = 491;
+    public long fillRefBitmap492 = 492;
+    public long fillRefBitmap493 = 493;
+    public long fillRefBitmap494 = 494;
+    public long fillRefBitmap495 = 495;
+    public long fillRefBitmap496 = 496;
+    public long fillRefBitmap497 = 497;
+    public long fillRefBitmap498 = 498;
+    public long fillRefBitmap499 = 499;
+    public long fillRefBitmap500 = 500;
+    public long fillRefBitmap501 = 501;
+    public long fillRefBitmap502 = 502;
+    public long fillRefBitmap503 = 503;
+    public long fillRefBitmap504 = 504;
+    public long fillRefBitmap505 = 505;
+    public long fillRefBitmap506 = 506;
+    public long fillRefBitmap507 = 507;
+    public long fillRefBitmap508 = 508;
+    public long fillRefBitmap509 = 509;
+    public long fillRefBitmap510 = 510;
+    public long fillRefBitmap511 = 511;
+    public long fillRefBitmap512 = 512;
+    public long fillRefBitmap513 = 513;
+    public long fillRefBitmap514 = 514;
+    public long fillRefBitmap515 = 515;
+    public long fillRefBitmap516 = 516;
+    public long fillRefBitmap517 = 517;
+    public long fillRefBitmap518 = 518;
+    public long fillRefBitmap519 = 519;
+    public long fillRefBitmap520 = 520;
+    public long fillRefBitmap521 = 521;
+    public long fillRefBitmap522 = 522;
+    public long fillRefBitmap523 = 523;
+    public long fillRefBitmap524 = 524;
+    public long fillRefBitmap525 = 525;
+    public long fillRefBitmap526 = 526;
+    public long fillRefBitmap527 = 527;
+    public long fillRefBitmap528 = 528;
+    public long fillRefBitmap529 = 529;
+    public long fillRefBitmap530 = 530;
+    public long fillRefBitmap531 = 531;
+    public long fillRefBitmap532 = 532;
+    public long fillRefBitmap533 = 533;
+    public long fillRefBitmap534 = 534;
+    public long fillRefBitmap535 = 535;
+    public long fillRefBitmap536 = 536;
+    public long fillRefBitmap537 = 537;
+    public long fillRefBitmap538 = 538;
+    public long fillRefBitmap539 = 539;
+    public long fillRefBitmap540 = 540;
+    public long fillRefBitmap541 = 541;
+    public long fillRefBitmap542 = 542;
+    public long fillRefBitmap543 = 543;
+    public long fillRefBitmap544 = 544;
+    public long fillRefBitmap545 = 545;
+    public long fillRefBitmap546 = 546;
+    public long fillRefBitmap547 = 547;
+    public long fillRefBitmap548 = 548;
+    public long fillRefBitmap549 = 549;
+    public long fillRefBitmap550 = 550;
+    public long fillRefBitmap551 = 551;
+    public long fillRefBitmap552 = 552;
+    public long fillRefBitmap553 = 553;
+    public long fillRefBitmap554 = 554;
+    public long fillRefBitmap555 = 555;
+    public long fillRefBitmap556 = 556;
+    public long fillRefBitmap557 = 557;
+    public long fillRefBitmap558 = 558;
+    public long fillRefBitmap559 = 559;
+    public long fillRefBitmap560 = 560;
+    public long fillRefBitmap561 = 561;
+    public long fillRefBitmap562 = 562;
+    public long fillRefBitmap563 = 563;
+    public long fillRefBitmap564 = 564;
+    public long fillRefBitmap565 = 565;
+    public long fillRefBitmap566 = 566;
+    public long fillRefBitmap567 = 567;
+    public long fillRefBitmap568 = 568;
+    public long fillRefBitmap569 = 569;
+    public long fillRefBitmap570 = 570;
+    public long fillRefBitmap571 = 571;
+    public long fillRefBitmap572 = 572;
+    public long fillRefBitmap573 = 573;
+    public long fillRefBitmap574 = 574;
+    public long fillRefBitmap575 = 575;
+    public long fillRefBitmap576 = 576;
+    public long fillRefBitmap577 = 577;
+    public long fillRefBitmap578 = 578;
+    public long fillRefBitmap579 = 579;
+    public long fillRefBitmap580 = 580;
+    public long fillRefBitmap581 = 581;
+    public long fillRefBitmap582 = 582;
+    public long fillRefBitmap583 = 583;
+    public long fillRefBitmap584 = 584;
+    public long fillRefBitmap585 = 585;
+    public long fillRefBitmap586 = 586;
+    public long fillRefBitmap587 = 587;
+    public long fillRefBitmap588 = 588;
+    public long fillRefBitmap589 = 589;
+    public long fillRefBitmap590 = 590;
+    public long fillRefBitmap591 = 591;
+    public long fillRefBitmap592 = 592;
+    public long fillRefBitmap593 = 593;
+    public long fillRefBitmap594 = 594;
+    public long fillRefBitmap595 = 595;
+    public long fillRefBitmap596 = 596;
+    public long fillRefBitmap597 = 597;
+    public long fillRefBitmap598 = 598;
+    public long fillRefBitmap599 = 599;
+    public long fillRefBitmap600 = 600;
+    public long fillRefBitmap601 = 601;
+    public long fillRefBitmap602 = 602;
+    public long fillRefBitmap603 = 603;
+    public long fillRefBitmap604 = 604;
+    public long fillRefBitmap605 = 605;
+    public long fillRefBitmap606 = 606;
+    public long fillRefBitmap607 = 607;
+    public long fillRefBitmap608 = 608;
+    public long fillRefBitmap609 = 609;
+    public long fillRefBitmap610 = 610;
+    public long fillRefBitmap611 = 611;
+    public long fillRefBitmap612 = 612;
+    public long fillRefBitmap613 = 613;
+    public long fillRefBitmap614 = 614;
+    public long fillRefBitmap615 = 615;
+    public long fillRefBitmap616 = 616;
+    public long fillRefBitmap617 = 617;
+    public long fillRefBitmap618 = 618;
+    public long fillRefBitmap619 = 619;
+    public long fillRefBitmap620 = 620;
+    public long fillRefBitmap621 = 621;
+    public long fillRefBitmap622 = 622;
+    public long fillRefBitmap623 = 623;
+    public long fillRefBitmap624 = 624;
+    public long fillRefBitmap625 = 625;
+    public long fillRefBitmap626 = 626;
+    public long fillRefBitmap627 = 627;
+    public long fillRefBitmap628 = 628;
+    public long fillRefBitmap629 = 629;
+    public long fillRefBitmap630 = 630;
+    public long fillRefBitmap631 = 631;
+    public long fillRefBitmap632 = 632;
+    public long fillRefBitmap633 = 633;
+    public long fillRefBitmap634 = 634;
+    public long fillRefBitmap635 = 635;
+    public long fillRefBitmap636 = 636;
+    public long fillRefBitmap637 = 637;
+    public long fillRefBitmap638 = 638;
+    public long fillRefBitmap639 = 639;
+    public long fillRefBitmap640 = 640;
+    public long fillRefBitmap641 = 641;
+    public long fillRefBitmap642 = 642;
+    public long fillRefBitmap643 = 643;
+    public long fillRefBitmap644 = 644;
+    public long fillRefBitmap645 = 645;
+    public long fillRefBitmap646 = 646;
+    public long fillRefBitmap647 = 647;
+    public long fillRefBitmap648 = 648;
+    public long fillRefBitmap649 = 649;
+    public long fillRefBitmap650 = 650;
+    public long fillRefBitmap651 = 651;
+    public long fillRefBitmap652 = 652;
+    public long fillRefBitmap653 = 653;
+    public long fillRefBitmap654 = 654;
+    public long fillRefBitmap655 = 655;
+    public long fillRefBitmap656 = 656;
+    public long fillRefBitmap657 = 657;
+    public long fillRefBitmap658 = 658;
+    public long fillRefBitmap659 = 659;
+    public long fillRefBitmap660 = 660;
+    public long fillRefBitmap661 = 661;
+    public long fillRefBitmap662 = 662;
+    public long fillRefBitmap663 = 663;
+    public long fillRefBitmap664 = 664;
+    public long fillRefBitmap665 = 665;
+    public long fillRefBitmap666 = 666;
+    public long fillRefBitmap667 = 667;
+    public long fillRefBitmap668 = 668;
+    public long fillRefBitmap669 = 669;
+    public long fillRefBitmap670 = 670;
+    public long fillRefBitmap671 = 671;
+    public long fillRefBitmap672 = 672;
+    public long fillRefBitmap673 = 673;
+    public long fillRefBitmap674 = 674;
+    public long fillRefBitmap675 = 675;
+    public long fillRefBitmap676 = 676;
+    public long fillRefBitmap677 = 677;
+    public long fillRefBitmap678 = 678;
+    public long fillRefBitmap679 = 679;
+    public long fillRefBitmap680 = 680;
+    public long fillRefBitmap681 = 681;
+    public long fillRefBitmap682 = 682;
+    public long fillRefBitmap683 = 683;
+    public long fillRefBitmap684 = 684;
+    public long fillRefBitmap685 = 685;
+    public long fillRefBitmap686 = 686;
+    public long fillRefBitmap687 = 687;
+    public long fillRefBitmap688 = 688;
+    public long fillRefBitmap689 = 689;
+    public long fillRefBitmap690 = 690;
+    public long fillRefBitmap691 = 691;
+    public long fillRefBitmap692 = 692;
+    public long fillRefBitmap693 = 693;
+    public long fillRefBitmap694 = 694;
+    public long fillRefBitmap695 = 695;
+    public long fillRefBitmap696 = 696;
+    public long fillRefBitmap697 = 697;
+    public long fillRefBitmap698 = 698;
+    public long fillRefBitmap699 = 699;
+    public long fillRefBitmap700 = 700;
+    public long fillRefBitmap701 = 701;
+    public long fillRefBitmap702 = 702;
+    public long fillRefBitmap703 = 703;
+    public long fillRefBitmap704 = 704;
+    public long fillRefBitmap705 = 705;
+    public long fillRefBitmap706 = 706;
+    public long fillRefBitmap707 = 707;
+    public long fillRefBitmap708 = 708;
+    public long fillRefBitmap709 = 709;
+    public long fillRefBitmap710 = 710;
+    public long fillRefBitmap711 = 711;
+    public long fillRefBitmap712 = 712;
+    public long fillRefBitmap713 = 713;
+    public long fillRefBitmap714 = 714;
+    public long fillRefBitmap715 = 715;
+    public long fillRefBitmap716 = 716;
+    public long fillRefBitmap717 = 717;
+    public long fillRefBitmap718 = 718;
+    public long fillRefBitmap719 = 719;
+    public long fillRefBitmap720 = 720;
+    public long fillRefBitmap721 = 721;
+    public long fillRefBitmap722 = 722;
+    public long fillRefBitmap723 = 723;
+    public long fillRefBitmap724 = 724;
+    public long fillRefBitmap725 = 725;
+    public long fillRefBitmap726 = 726;
+    public long fillRefBitmap727 = 727;
+    public long fillRefBitmap728 = 728;
+    public long fillRefBitmap729 = 729;
+    public long fillRefBitmap730 = 730;
+    public long fillRefBitmap731 = 731;
+    public long fillRefBitmap732 = 732;
+    public long fillRefBitmap733 = 733;
+    public long fillRefBitmap734 = 734;
+    public long fillRefBitmap735 = 735;
+    public long fillRefBitmap736 = 736;
+    public long fillRefBitmap737 = 737;
+    public long fillRefBitmap738 = 738;
+    public long fillRefBitmap739 = 739;
+    public long fillRefBitmap740 = 740;
+    public long fillRefBitmap741 = 741;
+    public long fillRefBitmap742 = 742;
+    public long fillRefBitmap743 = 743;
+    public long fillRefBitmap744 = 744;
+    public long fillRefBitmap745 = 745;
+    public long fillRefBitmap746 = 746;
+    public long fillRefBitmap747 = 747;
+    public long fillRefBitmap748 = 748;
+    public long fillRefBitmap749 = 749;
+    public long fillRefBitmap750 = 750;
+    public long fillRefBitmap751 = 751;
+    public long fillRefBitmap752 = 752;
+    public long fillRefBitmap753 = 753;
+    public long fillRefBitmap754 = 754;
+    public long fillRefBitmap755 = 755;
+    public long fillRefBitmap756 = 756;
+    public long fillRefBitmap757 = 757;
+    public long fillRefBitmap758 = 758;
+    public long fillRefBitmap759 = 759;
+    public long fillRefBitmap760 = 760;
+    public long fillRefBitmap761 = 761;
+    public long fillRefBitmap762 = 762;
+    public long fillRefBitmap763 = 763;
+    public long fillRefBitmap764 = 764;
+    public long fillRefBitmap765 = 765;
+    public long fillRefBitmap766 = 766;
+    public long fillRefBitmap767 = 767;
+    public long fillRefBitmap768 = 768;
+    public long fillRefBitmap769 = 769;
+    public long fillRefBitmap770 = 770;
+    public long fillRefBitmap771 = 771;
+    public long fillRefBitmap772 = 772;
+    public long fillRefBitmap773 = 773;
+    public long fillRefBitmap774 = 774;
+    public long fillRefBitmap775 = 775;
+    public long fillRefBitmap776 = 776;
+    public long fillRefBitmap777 = 777;
+    public long fillRefBitmap778 = 778;
+    public long fillRefBitmap779 = 779;
+    public long fillRefBitmap780 = 780;
+    public long fillRefBitmap781 = 781;
+    public long fillRefBitmap782 = 782;
+    public long fillRefBitmap783 = 783;
+    public long fillRefBitmap784 = 784;
+    public long fillRefBitmap785 = 785;
+    public long fillRefBitmap786 = 786;
+    public long fillRefBitmap787 = 787;
+    public long fillRefBitmap788 = 788;
+    public long fillRefBitmap789 = 789;
+    public long fillRefBitmap790 = 790;
+    public long fillRefBitmap791 = 791;
+    public long fillRefBitmap792 = 792;
+    public long fillRefBitmap793 = 793;
+    public long fillRefBitmap794 = 794;
+    public long fillRefBitmap795 = 795;
+    public long fillRefBitmap796 = 796;
+    public long fillRefBitmap797 = 797;
+    public long fillRefBitmap798 = 798;
+    public long fillRefBitmap799 = 799;
+    public long fillRefBitmap800 = 800;
+    public long fillRefBitmap801 = 801;
+    public long fillRefBitmap802 = 802;
+    public long fillRefBitmap803 = 803;
+    public long fillRefBitmap804 = 804;
+    public long fillRefBitmap805 = 805;
+    public long fillRefBitmap806 = 806;
+    public long fillRefBitmap807 = 807;
+    public long fillRefBitmap808 = 808;
+    public long fillRefBitmap809 = 809;
+    public long fillRefBitmap810 = 810;
+    public long fillRefBitmap811 = 811;
+    public long fillRefBitmap812 = 812;
+    public long fillRefBitmap813 = 813;
+    public long fillRefBitmap814 = 814;
+    public long fillRefBitmap815 = 815;
+    public long fillRefBitmap816 = 816;
+    public long fillRefBitmap817 = 817;
+    public long fillRefBitmap818 = 818;
+    public long fillRefBitmap819 = 819;
+    public long fillRefBitmap820 = 820;
+    public long fillRefBitmap821 = 821;
+    public long fillRefBitmap822 = 822;
+    public long fillRefBitmap823 = 823;
+    public long fillRefBitmap824 = 824;
+    public long fillRefBitmap825 = 825;
+    public long fillRefBitmap826 = 826;
+    public long fillRefBitmap827 = 827;
+    public long fillRefBitmap828 = 828;
+    public long fillRefBitmap829 = 829;
+    public long fillRefBitmap830 = 830;
+    public long fillRefBitmap831 = 831;
+    public long fillRefBitmap832 = 832;
+    public long fillRefBitmap833 = 833;
+    public long fillRefBitmap834 = 834;
+    public long fillRefBitmap835 = 835;
+    public long fillRefBitmap836 = 836;
+    public long fillRefBitmap837 = 837;
+    public long fillRefBitmap838 = 838;
+    public long fillRefBitmap839 = 839;
+    public long fillRefBitmap840 = 840;
+    public long fillRefBitmap841 = 841;
+    public long fillRefBitmap842 = 842;
+    public long fillRefBitmap843 = 843;
+    public long fillRefBitmap844 = 844;
+    public long fillRefBitmap845 = 845;
+    public long fillRefBitmap846 = 846;
+    public long fillRefBitmap847 = 847;
+    public long fillRefBitmap848 = 848;
+    public long fillRefBitmap849 = 849;
+    public long fillRefBitmap850 = 850;
+    public long fillRefBitmap851 = 851;
+    public long fillRefBitmap852 = 852;
+    public long fillRefBitmap853 = 853;
+    public long fillRefBitmap854 = 854;
+    public long fillRefBitmap855 = 855;
+    public long fillRefBitmap856 = 856;
+    public long fillRefBitmap857 = 857;
+    public long fillRefBitmap858 = 858;
+    public long fillRefBitmap859 = 859;
+    public long fillRefBitmap860 = 860;
+    public long fillRefBitmap861 = 861;
+    public long fillRefBitmap862 = 862;
+    public long fillRefBitmap863 = 863;
+    public long fillRefBitmap864 = 864;
+    public long fillRefBitmap865 = 865;
+    public long fillRefBitmap866 = 866;
+    public long fillRefBitmap867 = 867;
+    public long fillRefBitmap868 = 868;
+    public long fillRefBitmap869 = 869;
+    public long fillRefBitmap870 = 870;
+    public long fillRefBitmap871 = 871;
+    public long fillRefBitmap872 = 872;
+    public long fillRefBitmap873 = 873;
+    public long fillRefBitmap874 = 874;
+    public long fillRefBitmap875 = 875;
+    public long fillRefBitmap876 = 876;
+    public long fillRefBitmap877 = 877;
+    public long fillRefBitmap878 = 878;
+    public long fillRefBitmap879 = 879;
+    public long fillRefBitmap880 = 880;
+    public long fillRefBitmap881 = 881;
+    public long fillRefBitmap882 = 882;
+    public long fillRefBitmap883 = 883;
+    public long fillRefBitmap884 = 884;
+    public long fillRefBitmap885 = 885;
+    public long fillRefBitmap886 = 886;
+    public long fillRefBitmap887 = 887;
+    public long fillRefBitmap888 = 888;
+    public long fillRefBitmap889 = 889;
+    public long fillRefBitmap890 = 890;
+    public long fillRefBitmap891 = 891;
+    public long fillRefBitmap892 = 892;
+    public long fillRefBitmap893 = 893;
+    public long fillRefBitmap894 = 894;
+    public long fillRefBitmap895 = 895;
+    public long fillRefBitmap896 = 896;
+    public long fillRefBitmap897 = 897;
+    public long fillRefBitmap898 = 898;
+    public long fillRefBitmap899 = 899;
+    public long fillRefBitmap900 = 900;
+    public long fillRefBitmap901 = 901;
+    public long fillRefBitmap902 = 902;
+    public long fillRefBitmap903 = 903;
+    public long fillRefBitmap904 = 904;
+    public long fillRefBitmap905 = 905;
+    public long fillRefBitmap906 = 906;
+    public long fillRefBitmap907 = 907;
+    public long fillRefBitmap908 = 908;
+    public long fillRefBitmap909 = 909;
+    public long fillRefBitmap910 = 910;
+    public long fillRefBitmap911 = 911;
+    public long fillRefBitmap912 = 912;
+    public long fillRefBitmap913 = 913;
+    public long fillRefBitmap914 = 914;
+    public long fillRefBitmap915 = 915;
+    public long fillRefBitmap916 = 916;
+    public long fillRefBitmap917 = 917;
+    public long fillRefBitmap918 = 918;
+    public long fillRefBitmap919 = 919;
+    public long fillRefBitmap920 = 920;
+    public long fillRefBitmap921 = 921;
+    public long fillRefBitmap922 = 922;
+    public long fillRefBitmap923 = 923;
+    public long fillRefBitmap924 = 924;
+    public long fillRefBitmap925 = 925;
+    public long fillRefBitmap926 = 926;
+    public long fillRefBitmap927 = 927;
+    public long fillRefBitmap928 = 928;
+    public long fillRefBitmap929 = 929;
+    public long fillRefBitmap930 = 930;
+    public long fillRefBitmap931 = 931;
+    public long fillRefBitmap932 = 932;
+    public long fillRefBitmap933 = 933;
+    public long fillRefBitmap934 = 934;
+    public long fillRefBitmap935 = 935;
+    public long fillRefBitmap936 = 936;
+    public long fillRefBitmap937 = 937;
+    public long fillRefBitmap938 = 938;
+    public long fillRefBitmap939 = 939;
+    public long fillRefBitmap940 = 940;
+    public long fillRefBitmap941 = 941;
+    public long fillRefBitmap942 = 942;
+    public long fillRefBitmap943 = 943;
+    public long fillRefBitmap944 = 944;
+    public long fillRefBitmap945 = 945;
+    public long fillRefBitmap946 = 946;
+    public long fillRefBitmap947 = 947;
+    public long fillRefBitmap948 = 948;
+    public long fillRefBitmap949 = 949;
+    public long fillRefBitmap950 = 950;
+    public long fillRefBitmap951 = 951;
+    public long fillRefBitmap952 = 952;
+    public long fillRefBitmap953 = 953;
+    public long fillRefBitmap954 = 954;
+    public long fillRefBitmap955 = 955;
+    public long fillRefBitmap956 = 956;
+    public long fillRefBitmap957 = 957;
+    public long fillRefBitmap958 = 958;
+    public long fillRefBitmap959 = 959;
+    public long fillRefBitmap960 = 960;
+    public long fillRefBitmap961 = 961;
+    public long fillRefBitmap962 = 962;
+    public long fillRefBitmap963 = 963;
+    public long fillRefBitmap964 = 964;
+    public long fillRefBitmap965 = 965;
+    public long fillRefBitmap966 = 966;
+    public long fillRefBitmap967 = 967;
+    public long fillRefBitmap968 = 968;
+    public long fillRefBitmap969 = 969;
+    public long fillRefBitmap970 = 970;
+    public long fillRefBitmap971 = 971;
+    public long fillRefBitmap972 = 972;
+    public long fillRefBitmap973 = 973;
+    public long fillRefBitmap974 = 974;
+    public long fillRefBitmap975 = 975;
+    public long fillRefBitmap976 = 976;
+    public long fillRefBitmap977 = 977;
+    public long fillRefBitmap978 = 978;
+    public long fillRefBitmap979 = 979;
+    public long fillRefBitmap980 = 980;
+    public long fillRefBitmap981 = 981;
+    public long fillRefBitmap982 = 982;
+    public long fillRefBitmap983 = 983;
+    public long fillRefBitmap984 = 984;
+    public long fillRefBitmap985 = 985;
+    public long fillRefBitmap986 = 986;
+    public long fillRefBitmap987 = 987;
+    public long fillRefBitmap988 = 988;
+    public long fillRefBitmap989 = 989;
+    public long fillRefBitmap990 = 990;
+    public long fillRefBitmap991 = 991;
+    public long fillRefBitmap992 = 992;
+    public long fillRefBitmap993 = 993;
+    public long fillRefBitmap994 = 994;
+    public long fillRefBitmap995 = 995;
+    public long fillRefBitmap996 = 996;
+    public long fillRefBitmap997 = 997;
+    public long fillRefBitmap998 = 998;
+    public long fillRefBitmap999 = 999;
+    public long fillRefBitmap1000 = 1000;
+    public long fillRefBitmap1001 = 1001;
+    public long fillRefBitmap1002 = 1002;
+    public long fillRefBitmap1003 = 1003;
+    public long fillRefBitmap1004 = 1004;
+    public long fillRefBitmap1005 = 1005;
+    public long fillRefBitmap1006 = 1006;
+    public long fillRefBitmap1007 = 1007;
+    public long fillRefBitmap1008 = 1008;
+    public long fillRefBitmap1009 = 1009;
+    public long fillRefBitmap1010 = 1010;
+    public long fillRefBitmap1011 = 1011;
+    public long fillRefBitmap1012 = 1012;
+    public long fillRefBitmap1013 = 1013;
+    public long fillRefBitmap1014 = 1014;
+    public long fillRefBitmap1015 = 1015;
+    public long fillRefBitmap1016 = 1016;
+    public long fillRefBitmap1017 = 1017;
+    public long fillRefBitmap1018 = 1018;
+    public long fillRefBitmap1019 = 1019;
+    public long fillRefBitmap1020 = 1020;
+    public long fillRefBitmap1021 = 1021;
+    public long fillRefBitmap1022 = 1022;
+    public long fillRefBitmap1023 = 1023;
+    public long fillRefBitmap1024 = 1024;
+    public long fillRefBitmap1025 = 1025;
+    public long fillRefBitmap1026 = 1026;
+    public long fillRefBitmap1027 = 1027;
+    public long fillRefBitmap1028 = 1028;
+    public long fillRefBitmap1029 = 1029;
+    public long fillRefBitmap1030 = 1030;
+    public long fillRefBitmap1031 = 1031;
+    public long fillRefBitmap1032 = 1032;
+    public long fillRefBitmap1033 = 1033;
+    public long fillRefBitmap1034 = 1034;
+    public long fillRefBitmap1035 = 1035;
+    public long fillRefBitmap1036 = 1036;
+    public long fillRefBitmap1037 = 1037;
+    public long fillRefBitmap1038 = 1038;
+    public long fillRefBitmap1039 = 1039;
+    public long fillRefBitmap1040 = 1040;
+    public long fillRefBitmap1041 = 1041;
+    public long fillRefBitmap1042 = 1042;
+    public long fillRefBitmap1043 = 1043;
+    public long fillRefBitmap1044 = 1044;
+    public long fillRefBitmap1045 = 1045;
+    public long fillRefBitmap1046 = 1046;
+    public long fillRefBitmap1047 = 1047;
+    public long fillRefBitmap1048 = 1048;
+    public long fillRefBitmap1049 = 1049;
+    public long fillRefBitmap1050 = 1050;
+    public long fillRefBitmap1051 = 1051;
+    public long fillRefBitmap1052 = 1052;
+    public long fillRefBitmap1053 = 1053;
+    public long fillRefBitmap1054 = 1054;
+    public long fillRefBitmap1055 = 1055;
+    public long fillRefBitmap1056 = 1056;
+    public long fillRefBitmap1057 = 1057;
+    public long fillRefBitmap1058 = 1058;
+    public long fillRefBitmap1059 = 1059;
+    public long fillRefBitmap1060 = 1060;
+    public long fillRefBitmap1061 = 1061;
+    public long fillRefBitmap1062 = 1062;
+    public long fillRefBitmap1063 = 1063;
+    public long fillRefBitmap1064 = 1064;
+    public long fillRefBitmap1065 = 1065;
+    public long fillRefBitmap1066 = 1066;
+    public long fillRefBitmap1067 = 1067;
+    public long fillRefBitmap1068 = 1068;
+    public long fillRefBitmap1069 = 1069;
+    public long fillRefBitmap1070 = 1070;
+    public long fillRefBitmap1071 = 1071;
+    public long fillRefBitmap1072 = 1072;
+    public long fillRefBitmap1073 = 1073;
+    public long fillRefBitmap1074 = 1074;
+    public long fillRefBitmap1075 = 1075;
+    public long fillRefBitmap1076 = 1076;
+    public long fillRefBitmap1077 = 1077;
+    public long fillRefBitmap1078 = 1078;
+    public long fillRefBitmap1079 = 1079;
+    public long fillRefBitmap1080 = 1080;
+    public long fillRefBitmap1081 = 1081;
+    public long fillRefBitmap1082 = 1082;
+    public long fillRefBitmap1083 = 1083;
+    public long fillRefBitmap1084 = 1084;
+    public long fillRefBitmap1085 = 1085;
+    public long fillRefBitmap1086 = 1086;
+    public long fillRefBitmap1087 = 1087;
+    public long fillRefBitmap1088 = 1088;
+    public long fillRefBitmap1089 = 1089;
+    public long fillRefBitmap1090 = 1090;
+    public long fillRefBitmap1091 = 1091;
+    public long fillRefBitmap1092 = 1092;
+    public long fillRefBitmap1093 = 1093;
+    public long fillRefBitmap1094 = 1094;
+    public long fillRefBitmap1095 = 1095;
+    public long fillRefBitmap1096 = 1096;
+    public long fillRefBitmap1097 = 1097;
+    public long fillRefBitmap1098 = 1098;
+    public long fillRefBitmap1099 = 1099;
+    public long fillRefBitmap1100 = 1100;
+    public long fillRefBitmap1101 = 1101;
+    public long fillRefBitmap1102 = 1102;
+    public long fillRefBitmap1103 = 1103;
+    public long fillRefBitmap1104 = 1104;
+    public long fillRefBitmap1105 = 1105;
+    public long fillRefBitmap1106 = 1106;
+    public long fillRefBitmap1107 = 1107;
+    public long fillRefBitmap1108 = 1108;
+    public long fillRefBitmap1109 = 1109;
+    public long fillRefBitmap1110 = 1110;
+    public long fillRefBitmap1111 = 1111;
+    public long fillRefBitmap1112 = 1112;
+    public long fillRefBitmap1113 = 1113;
+    public long fillRefBitmap1114 = 1114;
+    public long fillRefBitmap1115 = 1115;
+    public long fillRefBitmap1116 = 1116;
+    public long fillRefBitmap1117 = 1117;
+    public long fillRefBitmap1118 = 1118;
+    public long fillRefBitmap1119 = 1119;
+    public long fillRefBitmap1120 = 1120;
+    public long fillRefBitmap1121 = 1121;
+    public long fillRefBitmap1122 = 1122;
+    public long fillRefBitmap1123 = 1123;
+    public long fillRefBitmap1124 = 1124;
+    public long fillRefBitmap1125 = 1125;
+    public long fillRefBitmap1126 = 1126;
+    public long fillRefBitmap1127 = 1127;
+    public long fillRefBitmap1128 = 1128;
+    public long fillRefBitmap1129 = 1129;
+    public long fillRefBitmap1130 = 1130;
+    public long fillRefBitmap1131 = 1131;
+    public long fillRefBitmap1132 = 1132;
+    public long fillRefBitmap1133 = 1133;
+    public long fillRefBitmap1134 = 1134;
+    public long fillRefBitmap1135 = 1135;
+    public long fillRefBitmap1136 = 1136;
+    public long fillRefBitmap1137 = 1137;
+    public long fillRefBitmap1138 = 1138;
+    public long fillRefBitmap1139 = 1139;
+    public long fillRefBitmap1140 = 1140;
+    public long fillRefBitmap1141 = 1141;
+    public long fillRefBitmap1142 = 1142;
+    public long fillRefBitmap1143 = 1143;
+    public long fillRefBitmap1144 = 1144;
+    public long fillRefBitmap1145 = 1145;
+    public long fillRefBitmap1146 = 1146;
+    public long fillRefBitmap1147 = 1147;
+    public long fillRefBitmap1148 = 1148;
+    public long fillRefBitmap1149 = 1149;
+    public long fillRefBitmap1150 = 1150;
+    public long fillRefBitmap1151 = 1151;
+    public long fillRefBitmap1152 = 1152;
+    public long fillRefBitmap1153 = 1153;
+    public long fillRefBitmap1154 = 1154;
+    public long fillRefBitmap1155 = 1155;
+    public long fillRefBitmap1156 = 1156;
+    public long fillRefBitmap1157 = 1157;
+    public long fillRefBitmap1158 = 1158;
+    public long fillRefBitmap1159 = 1159;
+    public long fillRefBitmap1160 = 1160;
+    public long fillRefBitmap1161 = 1161;
+    public long fillRefBitmap1162 = 1162;
+    public long fillRefBitmap1163 = 1163;
+    public long fillRefBitmap1164 = 1164;
+    public long fillRefBitmap1165 = 1165;
+    public long fillRefBitmap1166 = 1166;
+    public long fillRefBitmap1167 = 1167;
+    public long fillRefBitmap1168 = 1168;
+    public long fillRefBitmap1169 = 1169;
+    public long fillRefBitmap1170 = 1170;
+    public long fillRefBitmap1171 = 1171;
+    public long fillRefBitmap1172 = 1172;
+    public long fillRefBitmap1173 = 1173;
+    public long fillRefBitmap1174 = 1174;
+    public long fillRefBitmap1175 = 1175;
+    public long fillRefBitmap1176 = 1176;
+    public long fillRefBitmap1177 = 1177;
+    public long fillRefBitmap1178 = 1178;
+    public long fillRefBitmap1179 = 1179;
+    public long fillRefBitmap1180 = 1180;
+    public long fillRefBitmap1181 = 1181;
+    public long fillRefBitmap1182 = 1182;
+    public long fillRefBitmap1183 = 1183;
+    public long fillRefBitmap1184 = 1184;
+    public long fillRefBitmap1185 = 1185;
+    public long fillRefBitmap1186 = 1186;
+    public long fillRefBitmap1187 = 1187;
+    public long fillRefBitmap1188 = 1188;
+    public long fillRefBitmap1189 = 1189;
+    public long fillRefBitmap1190 = 1190;
+    public long fillRefBitmap1191 = 1191;
+    public long fillRefBitmap1192 = 1192;
+    public long fillRefBitmap1193 = 1193;
+    public long fillRefBitmap1194 = 1194;
+    public long fillRefBitmap1195 = 1195;
+    public long fillRefBitmap1196 = 1196;
+    public long fillRefBitmap1197 = 1197;
+    public long fillRefBitmap1198 = 1198;
+    public long fillRefBitmap1199 = 1199;
+    public long fillRefBitmap1200 = 1200;
+    public long fillRefBitmap1201 = 1201;
+    public long fillRefBitmap1202 = 1202;
+    public long fillRefBitmap1203 = 1203;
+    public long fillRefBitmap1204 = 1204;
+    public long fillRefBitmap1205 = 1205;
+    public long fillRefBitmap1206 = 1206;
+    public long fillRefBitmap1207 = 1207;
+    public long fillRefBitmap1208 = 1208;
+    public long fillRefBitmap1209 = 1209;
+    public long fillRefBitmap1210 = 1210;
+    public long fillRefBitmap1211 = 1211;
+    public long fillRefBitmap1212 = 1212;
+    public long fillRefBitmap1213 = 1213;
+    public long fillRefBitmap1214 = 1214;
+    public long fillRefBitmap1215 = 1215;
+    public long fillRefBitmap1216 = 1216;
+    public long fillRefBitmap1217 = 1217;
+    public long fillRefBitmap1218 = 1218;
+    public long fillRefBitmap1219 = 1219;
+    public long fillRefBitmap1220 = 1220;
+    public long fillRefBitmap1221 = 1221;
+    public long fillRefBitmap1222 = 1222;
+    public long fillRefBitmap1223 = 1223;
+    public long fillRefBitmap1224 = 1224;
+    public long fillRefBitmap1225 = 1225;
+    public long fillRefBitmap1226 = 1226;
+    public long fillRefBitmap1227 = 1227;
+    public long fillRefBitmap1228 = 1228;
+    public long fillRefBitmap1229 = 1229;
+    public long fillRefBitmap1230 = 1230;
+    public long fillRefBitmap1231 = 1231;
+    public long fillRefBitmap1232 = 1232;
+    public long fillRefBitmap1233 = 1233;
+    public long fillRefBitmap1234 = 1234;
+    public long fillRefBitmap1235 = 1235;
+    public long fillRefBitmap1236 = 1236;
+    public long fillRefBitmap1237 = 1237;
+    public long fillRefBitmap1238 = 1238;
+    public long fillRefBitmap1239 = 1239;
+    public long fillRefBitmap1240 = 1240;
+    public long fillRefBitmap1241 = 1241;
+    public long fillRefBitmap1242 = 1242;
+    public long fillRefBitmap1243 = 1243;
+    public long fillRefBitmap1244 = 1244;
+    public long fillRefBitmap1245 = 1245;
+    public long fillRefBitmap1246 = 1246;
+    public long fillRefBitmap1247 = 1247;
+    public long fillRefBitmap1248 = 1248;
+    public long fillRefBitmap1249 = 1249;
+    public long fillRefBitmap1250 = 1250;
+    public long fillRefBitmap1251 = 1251;
+    public long fillRefBitmap1252 = 1252;
+    public long fillRefBitmap1253 = 1253;
+    public long fillRefBitmap1254 = 1254;
+    public long fillRefBitmap1255 = 1255;
+    public long fillRefBitmap1256 = 1256;
+    public long fillRefBitmap1257 = 1257;
+    public long fillRefBitmap1258 = 1258;
+    public long fillRefBitmap1259 = 1259;
+    public long fillRefBitmap1260 = 1260;
+    public long fillRefBitmap1261 = 1261;
+    public long fillRefBitmap1262 = 1262;
+    public long fillRefBitmap1263 = 1263;
+    public long fillRefBitmap1264 = 1264;
+    public long fillRefBitmap1265 = 1265;
+    public long fillRefBitmap1266 = 1266;
+    public long fillRefBitmap1267 = 1267;
+    public long fillRefBitmap1268 = 1268;
+    public long fillRefBitmap1269 = 1269;
+    public long fillRefBitmap1270 = 1270;
+    public long fillRefBitmap1271 = 1271;
+    public long fillRefBitmap1272 = 1272;
+    public long fillRefBitmap1273 = 1273;
+    public long fillRefBitmap1274 = 1274;
+    public long fillRefBitmap1275 = 1275;
+    public long fillRefBitmap1276 = 1276;
+    public long fillRefBitmap1277 = 1277;
+    public long fillRefBitmap1278 = 1278;
+    public long fillRefBitmap1279 = 1279;
+    public long fillRefBitmap1280 = 1280;
+    public long fillRefBitmap1281 = 1281;
+    public long fillRefBitmap1282 = 1282;
+    public long fillRefBitmap1283 = 1283;
+    public long fillRefBitmap1284 = 1284;
+    public long fillRefBitmap1285 = 1285;
+    public long fillRefBitmap1286 = 1286;
+    public long fillRefBitmap1287 = 1287;
+    public long fillRefBitmap1288 = 1288;
+    public long fillRefBitmap1289 = 1289;
+    public long fillRefBitmap1290 = 1290;
+    public long fillRefBitmap1291 = 1291;
+    public long fillRefBitmap1292 = 1292;
+    public long fillRefBitmap1293 = 1293;
+    public long fillRefBitmap1294 = 1294;
+    public long fillRefBitmap1295 = 1295;
+    public long fillRefBitmap1296 = 1296;
+    public long fillRefBitmap1297 = 1297;
+    public long fillRefBitmap1298 = 1298;
+    public long fillRefBitmap1299 = 1299;
+    public long fillRefBitmap1300 = 1300;
+    public long fillRefBitmap1301 = 1301;
+    public long fillRefBitmap1302 = 1302;
+    public long fillRefBitmap1303 = 1303;
+    public long fillRefBitmap1304 = 1304;
+    public long fillRefBitmap1305 = 1305;
+    public long fillRefBitmap1306 = 1306;
+    public long fillRefBitmap1307 = 1307;
+    public long fillRefBitmap1308 = 1308;
+    public long fillRefBitmap1309 = 1309;
+    public long fillRefBitmap1310 = 1310;
+    public long fillRefBitmap1311 = 1311;
+    public long fillRefBitmap1312 = 1312;
+    public long fillRefBitmap1313 = 1313;
+    public long fillRefBitmap1314 = 1314;
+    public long fillRefBitmap1315 = 1315;
+    public long fillRefBitmap1316 = 1316;
+    public long fillRefBitmap1317 = 1317;
+    public long fillRefBitmap1318 = 1318;
+    public long fillRefBitmap1319 = 1319;
+    public long fillRefBitmap1320 = 1320;
+    public long fillRefBitmap1321 = 1321;
+    public long fillRefBitmap1322 = 1322;
+    public long fillRefBitmap1323 = 1323;
+    public long fillRefBitmap1324 = 1324;
+    public long fillRefBitmap1325 = 1325;
+    public long fillRefBitmap1326 = 1326;
+    public long fillRefBitmap1327 = 1327;
+    public long fillRefBitmap1328 = 1328;
+    public long fillRefBitmap1329 = 1329;
+    public long fillRefBitmap1330 = 1330;
+    public long fillRefBitmap1331 = 1331;
+    public long fillRefBitmap1332 = 1332;
+    public long fillRefBitmap1333 = 1333;
+    public long fillRefBitmap1334 = 1334;
+    public long fillRefBitmap1335 = 1335;
+    public long fillRefBitmap1336 = 1336;
+    public long fillRefBitmap1337 = 1337;
+    public long fillRefBitmap1338 = 1338;
+    public long fillRefBitmap1339 = 1339;
+    public long fillRefBitmap1340 = 1340;
+    public long fillRefBitmap1341 = 1341;
+    public long fillRefBitmap1342 = 1342;
+    public long fillRefBitmap1343 = 1343;
+    public long fillRefBitmap1344 = 1344;
+    public long fillRefBitmap1345 = 1345;
+    public long fillRefBitmap1346 = 1346;
+    public long fillRefBitmap1347 = 1347;
+    public long fillRefBitmap1348 = 1348;
+    public long fillRefBitmap1349 = 1349;
+    public long fillRefBitmap1350 = 1350;
+    public long fillRefBitmap1351 = 1351;
+    public long fillRefBitmap1352 = 1352;
+    public long fillRefBitmap1353 = 1353;
+    public long fillRefBitmap1354 = 1354;
+    public long fillRefBitmap1355 = 1355;
+    public long fillRefBitmap1356 = 1356;
+    public long fillRefBitmap1357 = 1357;
+    public long fillRefBitmap1358 = 1358;
+    public long fillRefBitmap1359 = 1359;
+    public long fillRefBitmap1360 = 1360;
+    public long fillRefBitmap1361 = 1361;
+    public long fillRefBitmap1362 = 1362;
+    public long fillRefBitmap1363 = 1363;
+    public long fillRefBitmap1364 = 1364;
+    public long fillRefBitmap1365 = 1365;
+    public long fillRefBitmap1366 = 1366;
+    public long fillRefBitmap1367 = 1367;
+    public long fillRefBitmap1368 = 1368;
+    public long fillRefBitmap1369 = 1369;
+    public long fillRefBitmap1370 = 1370;
+    public long fillRefBitmap1371 = 1371;
+    public long fillRefBitmap1372 = 1372;
+    public long fillRefBitmap1373 = 1373;
+    public long fillRefBitmap1374 = 1374;
+    public long fillRefBitmap1375 = 1375;
+    public long fillRefBitmap1376 = 1376;
+    public long fillRefBitmap1377 = 1377;
+    public long fillRefBitmap1378 = 1378;
+    public long fillRefBitmap1379 = 1379;
+    public long fillRefBitmap1380 = 1380;
+    public long fillRefBitmap1381 = 1381;
+    public long fillRefBitmap1382 = 1382;
+    public long fillRefBitmap1383 = 1383;
+    public long fillRefBitmap1384 = 1384;
+    public long fillRefBitmap1385 = 1385;
+    public long fillRefBitmap1386 = 1386;
+    public long fillRefBitmap1387 = 1387;
+    public long fillRefBitmap1388 = 1388;
+    public long fillRefBitmap1389 = 1389;
+    public long fillRefBitmap1390 = 1390;
+    public long fillRefBitmap1391 = 1391;
+    public long fillRefBitmap1392 = 1392;
+    public long fillRefBitmap1393 = 1393;
+    public long fillRefBitmap1394 = 1394;
+    public long fillRefBitmap1395 = 1395;
+    public long fillRefBitmap1396 = 1396;
+    public long fillRefBitmap1397 = 1397;
+    public long fillRefBitmap1398 = 1398;
+    public long fillRefBitmap1399 = 1399;
+    public long fillRefBitmap1400 = 1400;
+    public long fillRefBitmap1401 = 1401;
+    public long fillRefBitmap1402 = 1402;
+    public long fillRefBitmap1403 = 1403;
+    public long fillRefBitmap1404 = 1404;
+    public long fillRefBitmap1405 = 1405;
+    public long fillRefBitmap1406 = 1406;
+    public long fillRefBitmap1407 = 1407;
+    public long fillRefBitmap1408 = 1408;
+    public long fillRefBitmap1409 = 1409;
+    public long fillRefBitmap1410 = 1410;
+    public long fillRefBitmap1411 = 1411;
+    public long fillRefBitmap1412 = 1412;
+    public long fillRefBitmap1413 = 1413;
+    public long fillRefBitmap1414 = 1414;
+    public long fillRefBitmap1415 = 1415;
+    public long fillRefBitmap1416 = 1416;
+    public long fillRefBitmap1417 = 1417;
+    public long fillRefBitmap1418 = 1418;
+    public long fillRefBitmap1419 = 1419;
+    public long fillRefBitmap1420 = 1420;
+    public long fillRefBitmap1421 = 1421;
+    public long fillRefBitmap1422 = 1422;
+    public long fillRefBitmap1423 = 1423;
+    public long fillRefBitmap1424 = 1424;
+    public long fillRefBitmap1425 = 1425;
+    public long fillRefBitmap1426 = 1426;
+    public long fillRefBitmap1427 = 1427;
+    public long fillRefBitmap1428 = 1428;
+    public long fillRefBitmap1429 = 1429;
+    public long fillRefBitmap1430 = 1430;
+    public long fillRefBitmap1431 = 1431;
+    public long fillRefBitmap1432 = 1432;
+    public long fillRefBitmap1433 = 1433;
+    public long fillRefBitmap1434 = 1434;
+    public long fillRefBitmap1435 = 1435;
+    public long fillRefBitmap1436 = 1436;
+    public long fillRefBitmap1437 = 1437;
+    public long fillRefBitmap1438 = 1438;
+    public long fillRefBitmap1439 = 1439;
+    public long fillRefBitmap1440 = 1440;
+    public long fillRefBitmap1441 = 1441;
+    public long fillRefBitmap1442 = 1442;
+    public long fillRefBitmap1443 = 1443;
+    public long fillRefBitmap1444 = 1444;
+    public long fillRefBitmap1445 = 1445;
+    public long fillRefBitmap1446 = 1446;
+    public long fillRefBitmap1447 = 1447;
+    public long fillRefBitmap1448 = 1448;
+    public long fillRefBitmap1449 = 1449;
+    public long fillRefBitmap1450 = 1450;
+    public long fillRefBitmap1451 = 1451;
+    public long fillRefBitmap1452 = 1452;
+    public long fillRefBitmap1453 = 1453;
+    public long fillRefBitmap1454 = 1454;
+    public long fillRefBitmap1455 = 1455;
+    public long fillRefBitmap1456 = 1456;
+    public long fillRefBitmap1457 = 1457;
+    public long fillRefBitmap1458 = 1458;
+    public long fillRefBitmap1459 = 1459;
+    public long fillRefBitmap1460 = 1460;
+    public long fillRefBitmap1461 = 1461;
+    public long fillRefBitmap1462 = 1462;
+    public long fillRefBitmap1463 = 1463;
+    public long fillRefBitmap1464 = 1464;
+    public long fillRefBitmap1465 = 1465;
+    public long fillRefBitmap1466 = 1466;
+    public long fillRefBitmap1467 = 1467;
+    public long fillRefBitmap1468 = 1468;
+    public long fillRefBitmap1469 = 1469;
+    public long fillRefBitmap1470 = 1470;
+    public long fillRefBitmap1471 = 1471;
+    public long fillRefBitmap1472 = 1472;
+    public long fillRefBitmap1473 = 1473;
+    public long fillRefBitmap1474 = 1474;
+    public long fillRefBitmap1475 = 1475;
+    public long fillRefBitmap1476 = 1476;
+    public long fillRefBitmap1477 = 1477;
+    public long fillRefBitmap1478 = 1478;
+    public long fillRefBitmap1479 = 1479;
+    public long fillRefBitmap1480 = 1480;
+    public long fillRefBitmap1481 = 1481;
+    public long fillRefBitmap1482 = 1482;
+    public long fillRefBitmap1483 = 1483;
+    public long fillRefBitmap1484 = 1484;
+    public long fillRefBitmap1485 = 1485;
+    public long fillRefBitmap1486 = 1486;
+    public long fillRefBitmap1487 = 1487;
+    public long fillRefBitmap1488 = 1488;
+    public long fillRefBitmap1489 = 1489;
+    public long fillRefBitmap1490 = 1490;
+    public long fillRefBitmap1491 = 1491;
+    public long fillRefBitmap1492 = 1492;
+    public long fillRefBitmap1493 = 1493;
+    public long fillRefBitmap1494 = 1494;
+    public long fillRefBitmap1495 = 1495;
+    public long fillRefBitmap1496 = 1496;
+    public long fillRefBitmap1497 = 1497;
+    public long fillRefBitmap1498 = 1498;
+    public long fillRefBitmap1499 = 1499;
+    public long fillRefBitmap1500 = 1500;
+    public long fillRefBitmap1501 = 1501;
+    public long fillRefBitmap1502 = 1502;
+    public long fillRefBitmap1503 = 1503;
+    public long fillRefBitmap1504 = 1504;
+    public long fillRefBitmap1505 = 1505;
+    public long fillRefBitmap1506 = 1506;
+    public long fillRefBitmap1507 = 1507;
+    public long fillRefBitmap1508 = 1508;
+    public long fillRefBitmap1509 = 1509;
+    public long fillRefBitmap1510 = 1510;
+    public long fillRefBitmap1511 = 1511;
+    public long fillRefBitmap1512 = 1512;
+    public long fillRefBitmap1513 = 1513;
+    public long fillRefBitmap1514 = 1514;
+    public long fillRefBitmap1515 = 1515;
+    public long fillRefBitmap1516 = 1516;
+    public long fillRefBitmap1517 = 1517;
+    public long fillRefBitmap1518 = 1518;
+    public long fillRefBitmap1519 = 1519;
+    public long fillRefBitmap1520 = 1520;
+    public long fillRefBitmap1521 = 1521;
+    public long fillRefBitmap1522 = 1522;
+    public long fillRefBitmap1523 = 1523;
+    public long fillRefBitmap1524 = 1524;
+    public long fillRefBitmap1525 = 1525;
+    public long fillRefBitmap1526 = 1526;
+    public long fillRefBitmap1527 = 1527;
+    public long fillRefBitmap1528 = 1528;
+    public long fillRefBitmap1529 = 1529;
+    public long fillRefBitmap1530 = 1530;
+    public long fillRefBitmap1531 = 1531;
+    public long fillRefBitmap1532 = 1532;
+    public long fillRefBitmap1533 = 1533;
+    public long fillRefBitmap1534 = 1534;
+    public long fillRefBitmap1535 = 1535;
+    public long fillRefBitmap1536 = 1536;
+    public long fillRefBitmap1537 = 1537;
+    public long fillRefBitmap1538 = 1538;
+    public long fillRefBitmap1539 = 1539;
+    public long fillRefBitmap1540 = 1540;
+    public long fillRefBitmap1541 = 1541;
+    public long fillRefBitmap1542 = 1542;
+    public long fillRefBitmap1543 = 1543;
+    public long fillRefBitmap1544 = 1544;
+    public long fillRefBitmap1545 = 1545;
+    public long fillRefBitmap1546 = 1546;
+    public long fillRefBitmap1547 = 1547;
+    public long fillRefBitmap1548 = 1548;
+    public long fillRefBitmap1549 = 1549;
+    public long fillRefBitmap1550 = 1550;
+    public long fillRefBitmap1551 = 1551;
+    public long fillRefBitmap1552 = 1552;
+    public long fillRefBitmap1553 = 1553;
+    public long fillRefBitmap1554 = 1554;
+    public long fillRefBitmap1555 = 1555;
+    public long fillRefBitmap1556 = 1556;
+    public long fillRefBitmap1557 = 1557;
+    public long fillRefBitmap1558 = 1558;
+    public long fillRefBitmap1559 = 1559;
+    public long fillRefBitmap1560 = 1560;
+    public long fillRefBitmap1561 = 1561;
+    public long fillRefBitmap1562 = 1562;
+    public long fillRefBitmap1563 = 1563;
+    public long fillRefBitmap1564 = 1564;
+    public long fillRefBitmap1565 = 1565;
+    public long fillRefBitmap1566 = 1566;
+    public long fillRefBitmap1567 = 1567;
+    public long fillRefBitmap1568 = 1568;
+    public long fillRefBitmap1569 = 1569;
+    public long fillRefBitmap1570 = 1570;
+    public long fillRefBitmap1571 = 1571;
+    public long fillRefBitmap1572 = 1572;
+    public long fillRefBitmap1573 = 1573;
+    public long fillRefBitmap1574 = 1574;
+    public long fillRefBitmap1575 = 1575;
+    public long fillRefBitmap1576 = 1576;
+    public long fillRefBitmap1577 = 1577;
+    public long fillRefBitmap1578 = 1578;
+    public long fillRefBitmap1579 = 1579;
+    public long fillRefBitmap1580 = 1580;
+    public long fillRefBitmap1581 = 1581;
+    public long fillRefBitmap1582 = 1582;
+    public long fillRefBitmap1583 = 1583;
+    public long fillRefBitmap1584 = 1584;
+    public long fillRefBitmap1585 = 1585;
+    public long fillRefBitmap1586 = 1586;
+    public long fillRefBitmap1587 = 1587;
+    public long fillRefBitmap1588 = 1588;
+    public long fillRefBitmap1589 = 1589;
+    public long fillRefBitmap1590 = 1590;
+    public long fillRefBitmap1591 = 1591;
+    public long fillRefBitmap1592 = 1592;
+    public long fillRefBitmap1593 = 1593;
+    public long fillRefBitmap1594 = 1594;
+    public long fillRefBitmap1595 = 1595;
+    public long fillRefBitmap1596 = 1596;
+    public long fillRefBitmap1597 = 1597;
+    public long fillRefBitmap1598 = 1598;
+    public long fillRefBitmap1599 = 1599;
+    public long fillRefBitmap1600 = 1600;
+    public long fillRefBitmap1601 = 1601;
+    public long fillRefBitmap1602 = 1602;
+    public long fillRefBitmap1603 = 1603;
+    public long fillRefBitmap1604 = 1604;
+    public long fillRefBitmap1605 = 1605;
+    public long fillRefBitmap1606 = 1606;
+    public long fillRefBitmap1607 = 1607;
+    public long fillRefBitmap1608 = 1608;
+    public long fillRefBitmap1609 = 1609;
+    public long fillRefBitmap1610 = 1610;
+    public long fillRefBitmap1611 = 1611;
+    public long fillRefBitmap1612 = 1612;
+    public long fillRefBitmap1613 = 1613;
+    public long fillRefBitmap1614 = 1614;
+    public long fillRefBitmap1615 = 1615;
+    public long fillRefBitmap1616 = 1616;
+    public long fillRefBitmap1617 = 1617;
+    public long fillRefBitmap1618 = 1618;
+    public long fillRefBitmap1619 = 1619;
+    public long fillRefBitmap1620 = 1620;
+    public long fillRefBitmap1621 = 1621;
+    public long fillRefBitmap1622 = 1622;
+    public long fillRefBitmap1623 = 1623;
+    public long fillRefBitmap1624 = 1624;
+    public long fillRefBitmap1625 = 1625;
+    public long fillRefBitmap1626 = 1626;
+    public long fillRefBitmap1627 = 1627;
+    public long fillRefBitmap1628 = 1628;
+    public long fillRefBitmap1629 = 1629;
+    public long fillRefBitmap1630 = 1630;
+    public long fillRefBitmap1631 = 1631;
+    public long fillRefBitmap1632 = 1632;
+    public long fillRefBitmap1633 = 1633;
+    public long fillRefBitmap1634 = 1634;
+    public long fillRefBitmap1635 = 1635;
+    public long fillRefBitmap1636 = 1636;
+    public long fillRefBitmap1637 = 1637;
+    public long fillRefBitmap1638 = 1638;
+    public long fillRefBitmap1639 = 1639;
+    public long fillRefBitmap1640 = 1640;
+    public long fillRefBitmap1641 = 1641;
+    public long fillRefBitmap1642 = 1642;
+    public long fillRefBitmap1643 = 1643;
+    public long fillRefBitmap1644 = 1644;
+    public long fillRefBitmap1645 = 1645;
+    public long fillRefBitmap1646 = 1646;
+    public long fillRefBitmap1647 = 1647;
+    public long fillRefBitmap1648 = 1648;
+    public long fillRefBitmap1649 = 1649;
+    public long fillRefBitmap1650 = 1650;
+    public long fillRefBitmap1651 = 1651;
+    public long fillRefBitmap1652 = 1652;
+    public long fillRefBitmap1653 = 1653;
+    public long fillRefBitmap1654 = 1654;
+    public long fillRefBitmap1655 = 1655;
+    public long fillRefBitmap1656 = 1656;
+    public long fillRefBitmap1657 = 1657;
+    public long fillRefBitmap1658 = 1658;
+    public long fillRefBitmap1659 = 1659;
+    public long fillRefBitmap1660 = 1660;
+    public long fillRefBitmap1661 = 1661;
+    public long fillRefBitmap1662 = 1662;
+    public long fillRefBitmap1663 = 1663;
+    public long fillRefBitmap1664 = 1664;
+    public long fillRefBitmap1665 = 1665;
+    public long fillRefBitmap1666 = 1666;
+    public long fillRefBitmap1667 = 1667;
+    public long fillRefBitmap1668 = 1668;
+    public long fillRefBitmap1669 = 1669;
+    public long fillRefBitmap1670 = 1670;
+    public long fillRefBitmap1671 = 1671;
+    public long fillRefBitmap1672 = 1672;
+    public long fillRefBitmap1673 = 1673;
+    public long fillRefBitmap1674 = 1674;
+    public long fillRefBitmap1675 = 1675;
+    public long fillRefBitmap1676 = 1676;
+    public long fillRefBitmap1677 = 1677;
+    public long fillRefBitmap1678 = 1678;
+    public long fillRefBitmap1679 = 1679;
+    public long fillRefBitmap1680 = 1680;
+    public long fillRefBitmap1681 = 1681;
+    public long fillRefBitmap1682 = 1682;
+    public long fillRefBitmap1683 = 1683;
+    public long fillRefBitmap1684 = 1684;
+    public long fillRefBitmap1685 = 1685;
+    public long fillRefBitmap1686 = 1686;
+    public long fillRefBitmap1687 = 1687;
+    public long fillRefBitmap1688 = 1688;
+    public long fillRefBitmap1689 = 1689;
+    public long fillRefBitmap1690 = 1690;
+    public long fillRefBitmap1691 = 1691;
+    public long fillRefBitmap1692 = 1692;
+    public long fillRefBitmap1693 = 1693;
+    public long fillRefBitmap1694 = 1694;
+    public long fillRefBitmap1695 = 1695;
+    public long fillRefBitmap1696 = 1696;
+    public long fillRefBitmap1697 = 1697;
+    public long fillRefBitmap1698 = 1698;
+    public long fillRefBitmap1699 = 1699;
+    public long fillRefBitmap1700 = 1700;
+    public long fillRefBitmap1701 = 1701;
+    public long fillRefBitmap1702 = 1702;
+    public long fillRefBitmap1703 = 1703;
+    public long fillRefBitmap1704 = 1704;
+    public long fillRefBitmap1705 = 1705;
+    public long fillRefBitmap1706 = 1706;
+    public long fillRefBitmap1707 = 1707;
+    public long fillRefBitmap1708 = 1708;
+    public long fillRefBitmap1709 = 1709;
+    public long fillRefBitmap1710 = 1710;
+    public long fillRefBitmap1711 = 1711;
+    public long fillRefBitmap1712 = 1712;
+    public long fillRefBitmap1713 = 1713;
+    public long fillRefBitmap1714 = 1714;
+    public long fillRefBitmap1715 = 1715;
+    public long fillRefBitmap1716 = 1716;
+    public long fillRefBitmap1717 = 1717;
+    public long fillRefBitmap1718 = 1718;
+    public long fillRefBitmap1719 = 1719;
+    public long fillRefBitmap1720 = 1720;
+    public long fillRefBitmap1721 = 1721;
+    public long fillRefBitmap1722 = 1722;
+    public long fillRefBitmap1723 = 1723;
+    public long fillRefBitmap1724 = 1724;
+    public long fillRefBitmap1725 = 1725;
+    public long fillRefBitmap1726 = 1726;
+    public long fillRefBitmap1727 = 1727;
+    public long fillRefBitmap1728 = 1728;
+    public long fillRefBitmap1729 = 1729;
+    public long fillRefBitmap1730 = 1730;
+    public long fillRefBitmap1731 = 1731;
+    public long fillRefBitmap1732 = 1732;
+    public long fillRefBitmap1733 = 1733;
+    public long fillRefBitmap1734 = 1734;
+    public long fillRefBitmap1735 = 1735;
+    public long fillRefBitmap1736 = 1736;
+    public long fillRefBitmap1737 = 1737;
+    public long fillRefBitmap1738 = 1738;
+    public long fillRefBitmap1739 = 1739;
+    public long fillRefBitmap1740 = 1740;
+    public long fillRefBitmap1741 = 1741;
+    public long fillRefBitmap1742 = 1742;
+    public long fillRefBitmap1743 = 1743;
+    public long fillRefBitmap1744 = 1744;
+    public long fillRefBitmap1745 = 1745;
+    public long fillRefBitmap1746 = 1746;
+    public long fillRefBitmap1747 = 1747;
+    public long fillRefBitmap1748 = 1748;
+    public long fillRefBitmap1749 = 1749;
+    public long fillRefBitmap1750 = 1750;
+    public long fillRefBitmap1751 = 1751;
+    public long fillRefBitmap1752 = 1752;
+    public long fillRefBitmap1753 = 1753;
+    public long fillRefBitmap1754 = 1754;
+    public long fillRefBitmap1755 = 1755;
+    public long fillRefBitmap1756 = 1756;
+    public long fillRefBitmap1757 = 1757;
+    public long fillRefBitmap1758 = 1758;
+    public long fillRefBitmap1759 = 1759;
+    public long fillRefBitmap1760 = 1760;
+    public long fillRefBitmap1761 = 1761;
+    public long fillRefBitmap1762 = 1762;
+    public long fillRefBitmap1763 = 1763;
+    public long fillRefBitmap1764 = 1764;
+    public long fillRefBitmap1765 = 1765;
+    public long fillRefBitmap1766 = 1766;
+    public long fillRefBitmap1767 = 1767;
+    public long fillRefBitmap1768 = 1768;
+    public long fillRefBitmap1769 = 1769;
+    public long fillRefBitmap1770 = 1770;
+    public long fillRefBitmap1771 = 1771;
+    public long fillRefBitmap1772 = 1772;
+    public long fillRefBitmap1773 = 1773;
+    public long fillRefBitmap1774 = 1774;
+    public long fillRefBitmap1775 = 1775;
+    public long fillRefBitmap1776 = 1776;
+    public long fillRefBitmap1777 = 1777;
+    public long fillRefBitmap1778 = 1778;
+    public long fillRefBitmap1779 = 1779;
+    public long fillRefBitmap1780 = 1780;
+    public long fillRefBitmap1781 = 1781;
+    public long fillRefBitmap1782 = 1782;
+    public long fillRefBitmap1783 = 1783;
+    public long fillRefBitmap1784 = 1784;
+    public long fillRefBitmap1785 = 1785;
+    public long fillRefBitmap1786 = 1786;
+    public long fillRefBitmap1787 = 1787;
+    public long fillRefBitmap1788 = 1788;
+    public long fillRefBitmap1789 = 1789;
+    public long fillRefBitmap1790 = 1790;
+    public long fillRefBitmap1791 = 1791;
+    public long fillRefBitmap1792 = 1792;
+    public long fillRefBitmap1793 = 1793;
+    public long fillRefBitmap1794 = 1794;
+    public long fillRefBitmap1795 = 1795;
+    public long fillRefBitmap1796 = 1796;
+    public long fillRefBitmap1797 = 1797;
+    public long fillRefBitmap1798 = 1798;
+    public long fillRefBitmap1799 = 1799;
+    public long fillRefBitmap1800 = 1800;
+    public long fillRefBitmap1801 = 1801;
+    public long fillRefBitmap1802 = 1802;
+    public long fillRefBitmap1803 = 1803;
+    public long fillRefBitmap1804 = 1804;
+    public long fillRefBitmap1805 = 1805;
+    public long fillRefBitmap1806 = 1806;
+    public long fillRefBitmap1807 = 1807;
+    public long fillRefBitmap1808 = 1808;
+    public long fillRefBitmap1809 = 1809;
+    public long fillRefBitmap1810 = 1810;
+    public long fillRefBitmap1811 = 1811;
+    public long fillRefBitmap1812 = 1812;
+    public long fillRefBitmap1813 = 1813;
+    public long fillRefBitmap1814 = 1814;
+    public long fillRefBitmap1815 = 1815;
+    public long fillRefBitmap1816 = 1816;
+    public long fillRefBitmap1817 = 1817;
+    public long fillRefBitmap1818 = 1818;
+    public long fillRefBitmap1819 = 1819;
+    public long fillRefBitmap1820 = 1820;
+    public long fillRefBitmap1821 = 1821;
+    public long fillRefBitmap1822 = 1822;
+    public long fillRefBitmap1823 = 1823;
+    public long fillRefBitmap1824 = 1824;
+    public long fillRefBitmap1825 = 1825;
+    public long fillRefBitmap1826 = 1826;
+    public long fillRefBitmap1827 = 1827;
+    public long fillRefBitmap1828 = 1828;
+    public long fillRefBitmap1829 = 1829;
+    public long fillRefBitmap1830 = 1830;
+    public long fillRefBitmap1831 = 1831;
+    public long fillRefBitmap1832 = 1832;
+    public long fillRefBitmap1833 = 1833;
+    public long fillRefBitmap1834 = 1834;
+    public long fillRefBitmap1835 = 1835;
+    public long fillRefBitmap1836 = 1836;
+    public long fillRefBitmap1837 = 1837;
+    public long fillRefBitmap1838 = 1838;
+    public long fillRefBitmap1839 = 1839;
+    public long fillRefBitmap1840 = 1840;
+    public long fillRefBitmap1841 = 1841;
+    public long fillRefBitmap1842 = 1842;
+    public long fillRefBitmap1843 = 1843;
+    public long fillRefBitmap1844 = 1844;
+    public long fillRefBitmap1845 = 1845;
+    public long fillRefBitmap1846 = 1846;
+    public long fillRefBitmap1847 = 1847;
+    public long fillRefBitmap1848 = 1848;
+    public long fillRefBitmap1849 = 1849;
+    public long fillRefBitmap1850 = 1850;
+    public long fillRefBitmap1851 = 1851;
+    public long fillRefBitmap1852 = 1852;
+    public long fillRefBitmap1853 = 1853;
+    public long fillRefBitmap1854 = 1854;
+    public long fillRefBitmap1855 = 1855;
+    public long fillRefBitmap1856 = 1856;
+    public long fillRefBitmap1857 = 1857;
+    public long fillRefBitmap1858 = 1858;
+    public long fillRefBitmap1859 = 1859;
+    public long fillRefBitmap1860 = 1860;
+    public long fillRefBitmap1861 = 1861;
+    public long fillRefBitmap1862 = 1862;
+    public long fillRefBitmap1863 = 1863;
+    public long fillRefBitmap1864 = 1864;
+    public long fillRefBitmap1865 = 1865;
+    public long fillRefBitmap1866 = 1866;
+    public long fillRefBitmap1867 = 1867;
+    public long fillRefBitmap1868 = 1868;
+    public long fillRefBitmap1869 = 1869;
+    public long fillRefBitmap1870 = 1870;
+    public long fillRefBitmap1871 = 1871;
+    public long fillRefBitmap1872 = 1872;
+    public long fillRefBitmap1873 = 1873;
+    public long fillRefBitmap1874 = 1874;
+    public long fillRefBitmap1875 = 1875;
+    public long fillRefBitmap1876 = 1876;
+    public long fillRefBitmap1877 = 1877;
+    public long fillRefBitmap1878 = 1878;
+    public long fillRefBitmap1879 = 1879;
+    public long fillRefBitmap1880 = 1880;
+    public long fillRefBitmap1881 = 1881;
+    public long fillRefBitmap1882 = 1882;
+    public long fillRefBitmap1883 = 1883;
+    public long fillRefBitmap1884 = 1884;
+    public long fillRefBitmap1885 = 1885;
+    public long fillRefBitmap1886 = 1886;
+    public long fillRefBitmap1887 = 1887;
+    public long fillRefBitmap1888 = 1888;
+    public long fillRefBitmap1889 = 1889;
+    public long fillRefBitmap1890 = 1890;
+    public long fillRefBitmap1891 = 1891;
+    public long fillRefBitmap1892 = 1892;
+    public long fillRefBitmap1893 = 1893;
+    public long fillRefBitmap1894 = 1894;
+    public long fillRefBitmap1895 = 1895;
+    public long fillRefBitmap1896 = 1896;
+    public long fillRefBitmap1897 = 1897;
+    public long fillRefBitmap1898 = 1898;
+    public long fillRefBitmap1899 = 1899;
+    public long fillRefBitmap1900 = 1900;
+    public long fillRefBitmap1901 = 1901;
+    public long fillRefBitmap1902 = 1902;
+    public long fillRefBitmap1903 = 1903;
+    public long fillRefBitmap1904 = 1904;
+    public long fillRefBitmap1905 = 1905;
+    public long fillRefBitmap1906 = 1906;
+    public long fillRefBitmap1907 = 1907;
+    public long fillRefBitmap1908 = 1908;
+    public long fillRefBitmap1909 = 1909;
+    public long fillRefBitmap1910 = 1910;
+    public long fillRefBitmap1911 = 1911;
+    public long fillRefBitmap1912 = 1912;
+    public long fillRefBitmap1913 = 1913;
+    public long fillRefBitmap1914 = 1914;
+    public long fillRefBitmap1915 = 1915;
+    public long fillRefBitmap1916 = 1916;
+    public long fillRefBitmap1917 = 1917;
+    public long fillRefBitmap1918 = 1918;
+    public long fillRefBitmap1919 = 1919;
+    public long fillRefBitmap1920 = 1920;
+    public long fillRefBitmap1921 = 1921;
+    public long fillRefBitmap1922 = 1922;
+    public long fillRefBitmap1923 = 1923;
+    public long fillRefBitmap1924 = 1924;
+    public long fillRefBitmap1925 = 1925;
+    public long fillRefBitmap1926 = 1926;
+    public long fillRefBitmap1927 = 1927;
+    public long fillRefBitmap1928 = 1928;
+    public long fillRefBitmap1929 = 1929;
+    public long fillRefBitmap1930 = 1930;
+    public long fillRefBitmap1931 = 1931;
+    public long fillRefBitmap1932 = 1932;
+    public long fillRefBitmap1933 = 1933;
+    public long fillRefBitmap1934 = 1934;
+    public long fillRefBitmap1935 = 1935;
+    public long fillRefBitmap1936 = 1936;
+    public long fillRefBitmap1937 = 1937;
+    public long fillRefBitmap1938 = 1938;
+    public long fillRefBitmap1939 = 1939;
+    public long fillRefBitmap1940 = 1940;
+    public long fillRefBitmap1941 = 1941;
+    public long fillRefBitmap1942 = 1942;
+    public long fillRefBitmap1943 = 1943;
+    public long fillRefBitmap1944 = 1944;
+    public long fillRefBitmap1945 = 1945;
+    public long fillRefBitmap1946 = 1946;
+    public long fillRefBitmap1947 = 1947;
+    public long fillRefBitmap1948 = 1948;
+    public long fillRefBitmap1949 = 1949;
+    public long fillRefBitmap1950 = 1950;
+    public long fillRefBitmap1951 = 1951;
+    public long fillRefBitmap1952 = 1952;
+    public long fillRefBitmap1953 = 1953;
+    public long fillRefBitmap1954 = 1954;
+    public long fillRefBitmap1955 = 1955;
+    public long fillRefBitmap1956 = 1956;
+    public long fillRefBitmap1957 = 1957;
+    public long fillRefBitmap1958 = 1958;
+    public long fillRefBitmap1959 = 1959;
+    public long fillRefBitmap1960 = 1960;
+    public long fillRefBitmap1961 = 1961;
+    public long fillRefBitmap1962 = 1962;
+    public long fillRefBitmap1963 = 1963;
+    public long fillRefBitmap1964 = 1964;
+    public long fillRefBitmap1965 = 1965;
+    public long fillRefBitmap1966 = 1966;
+    public long fillRefBitmap1967 = 1967;
+    public long fillRefBitmap1968 = 1968;
+    public long fillRefBitmap1969 = 1969;
+    public long fillRefBitmap1970 = 1970;
+    public long fillRefBitmap1971 = 1971;
+    public long fillRefBitmap1972 = 1972;
+    public long fillRefBitmap1973 = 1973;
+    public long fillRefBitmap1974 = 1974;
+    public long fillRefBitmap1975 = 1975;
+    public long fillRefBitmap1976 = 1976;
+    public long fillRefBitmap1977 = 1977;
+    public long fillRefBitmap1978 = 1978;
+    public long fillRefBitmap1979 = 1979;
+    public long fillRefBitmap1980 = 1980;
+    public long fillRefBitmap1981 = 1981;
+    public long fillRefBitmap1982 = 1982;
+    public long fillRefBitmap1983 = 1983;
+    public long fillRefBitmap1984 = 1984;
+    public long fillRefBitmap1985 = 1985;
+    public long fillRefBitmap1986 = 1986;
+    public long fillRefBitmap1987 = 1987;
+    public long fillRefBitmap1988 = 1988;
+    public long fillRefBitmap1989 = 1989;
+    public long fillRefBitmap1990 = 1990;
+    public long fillRefBitmap1991 = 1991;
+    public long fillRefBitmap1992 = 1992;
+    public long fillRefBitmap1993 = 1993;
+    public long fillRefBitmap1994 = 1994;
+    public long fillRefBitmap1995 = 1995;
+    public long fillRefBitmap1996 = 1996;
+    public long fillRefBitmap1997 = 1997;
+    public long fillRefBitmap1998 = 1998;
+    public long fillRefBitmap1999 = 1999;
+    public long fillRefBitmap2000 = 2000;
+    public long fillRefBitmap2001 = 2001;
+    public long fillRefBitmap2002 = 2002;
+    public long fillRefBitmap2003 = 2003;
+    public long fillRefBitmap2004 = 2004;
+    public long fillRefBitmap2005 = 2005;
+    public long fillRefBitmap2006 = 2006;
+    public long fillRefBitmap2007 = 2007;
+    public long fillRefBitmap2008 = 2008;
+    public long fillRefBitmap2009 = 2009;
+    public long fillRefBitmap2010 = 2010;
+    public long fillRefBitmap2011 = 2011;
+    public long fillRefBitmap2012 = 2012;
+    public long fillRefBitmap2013 = 2013;
+    public long fillRefBitmap2014 = 2014;
+    public long fillRefBitmap2015 = 2015;
+    public long fillRefBitmap2016 = 2016;
+    public long fillRefBitmap2017 = 2017;
+    public long fillRefBitmap2018 = 2018;
+    public long fillRefBitmap2019 = 2019;
+    public long fillRefBitmap2020 = 2020;
+    public long fillRefBitmap2021 = 2021;
+    public long fillRefBitmap2022 = 2022;
+    public long fillRefBitmap2023 = 2023;
+    public long fillRefBitmap2024 = 2024;
+    public long fillRefBitmap2025 = 2025;
+    public long fillRefBitmap2026 = 2026;
+    public long fillRefBitmap2027 = 2027;
+    public long fillRefBitmap2028 = 2028;
+    public long fillRefBitmap2029 = 2029;
+    public long fillRefBitmap2030 = 2030;
+    public long fillRefBitmap2031 = 2031;
+    public long fillRefBitmap2032 = 2032;
+    public long fillRefBitmap2033 = 2033;
+    public long fillRefBitmap2034 = 2034;
+    public long fillRefBitmap2035 = 2035;
+    public long fillRefBitmap2036 = 2036;
+    public long fillRefBitmap2037 = 2037;
+    public long fillRefBitmap2038 = 2038;
+    public long fillRefBitmap2039 = 2039;
+    public long fillRefBitmap2040 = 2040;
+    public long fillRefBitmap2041 = 2041;
+    public long fillRefBitmap2042 = 2042;
+    public long fillRefBitmap2043 = 2043;
+    public long fillRefBitmap2044 = 2044;
+    public long fillRefBitmap2045 = 2045;
+    public long fillRefBitmap2046 = 2046;
+    public long fillRefBitmap2047 = 2047;
+    public long fillRefBitmap2048 = 2048;
+    public long fillRefBitmap2049 = 2049;
+    public long fillRefBitmap2050 = 2050;
+    public long fillRefBitmap2051 = 2051;
+    public long fillRefBitmap2052 = 2052;
+    public long fillRefBitmap2053 = 2053;
+    public long fillRefBitmap2054 = 2054;
+    public long fillRefBitmap2055 = 2055;
+    public long fillRefBitmap2056 = 2056;
+    public long fillRefBitmap2057 = 2057;
+    public long fillRefBitmap2058 = 2058;
+    public long fillRefBitmap2059 = 2059;
+    public long fillRefBitmap2060 = 2060;
+    public long fillRefBitmap2061 = 2061;
+    public long fillRefBitmap2062 = 2062;
+    public long fillRefBitmap2063 = 2063;
+    public long fillRefBitmap2064 = 2064;
+    public long fillRefBitmap2065 = 2065;
+    public long fillRefBitmap2066 = 2066;
+    public long fillRefBitmap2067 = 2067;
+    public long fillRefBitmap2068 = 2068;
+    public long fillRefBitmap2069 = 2069;
+    public long fillRefBitmap2070 = 2070;
+    public long fillRefBitmap2071 = 2071;
+    public long fillRefBitmap2072 = 2072;
+    public long fillRefBitmap2073 = 2073;
+    public long fillRefBitmap2074 = 2074;
+    public long fillRefBitmap2075 = 2075;
+    public long fillRefBitmap2076 = 2076;
+    public long fillRefBitmap2077 = 2077;
+    public long fillRefBitmap2078 = 2078;
+    public long fillRefBitmap2079 = 2079;
+    public long fillRefBitmap2080 = 2080;
+    public long fillRefBitmap2081 = 2081;
+    public long fillRefBitmap2082 = 2082;
+    public long fillRefBitmap2083 = 2083;
+    public long fillRefBitmap2084 = 2084;
+    public long fillRefBitmap2085 = 2085;
+    public long fillRefBitmap2086 = 2086;
+    public long fillRefBitmap2087 = 2087;
+    public long fillRefBitmap2088 = 2088;
+    public long fillRefBitmap2089 = 2089;
+    public long fillRefBitmap2090 = 2090;
+    public long fillRefBitmap2091 = 2091;
+    public long fillRefBitmap2092 = 2092;
+    public long fillRefBitmap2093 = 2093;
+    public long fillRefBitmap2094 = 2094;
+    public long fillRefBitmap2095 = 2095;
+    public long fillRefBitmap2096 = 2096;
+    public long fillRefBitmap2097 = 2097;
+    public long fillRefBitmap2098 = 2098;
+    public long fillRefBitmap2099 = 2099;
+    public long fillRefBitmap2100 = 2100;
+    public long fillRefBitmap2101 = 2101;
+    public long fillRefBitmap2102 = 2102;
+    public long fillRefBitmap2103 = 2103;
+    public long fillRefBitmap2104 = 2104;
+    public long fillRefBitmap2105 = 2105;
+    public long fillRefBitmap2106 = 2106;
+    public long fillRefBitmap2107 = 2107;
+    public long fillRefBitmap2108 = 2108;
+    public long fillRefBitmap2109 = 2109;
+    public long fillRefBitmap2110 = 2110;
+    public long fillRefBitmap2111 = 2111;
+    public long fillRefBitmap2112 = 2112;
+    public long fillRefBitmap2113 = 2113;
+    public long fillRefBitmap2114 = 2114;
+    public long fillRefBitmap2115 = 2115;
+    public long fillRefBitmap2116 = 2116;
+    public long fillRefBitmap2117 = 2117;
+    public long fillRefBitmap2118 = 2118;
+    public long fillRefBitmap2119 = 2119;
+    public long fillRefBitmap2120 = 2120;
+    public long fillRefBitmap2121 = 2121;
+    public long fillRefBitmap2122 = 2122;
+    public long fillRefBitmap2123 = 2123;
+    public long fillRefBitmap2124 = 2124;
+    public long fillRefBitmap2125 = 2125;
+    public long fillRefBitmap2126 = 2126;
+    public long fillRefBitmap2127 = 2127;
+    public long fillRefBitmap2128 = 2128;
+    public long fillRefBitmap2129 = 2129;
+    public long fillRefBitmap2130 = 2130;
+    public long fillRefBitmap2131 = 2131;
+    public long fillRefBitmap2132 = 2132;
+    public long fillRefBitmap2133 = 2133;
+    public long fillRefBitmap2134 = 2134;
+    public long fillRefBitmap2135 = 2135;
+    public long fillRefBitmap2136 = 2136;
+    public long fillRefBitmap2137 = 2137;
+    public long fillRefBitmap2138 = 2138;
+    public long fillRefBitmap2139 = 2139;
+    public long fillRefBitmap2140 = 2140;
+    public long fillRefBitmap2141 = 2141;
+    public long fillRefBitmap2142 = 2142;
+    public long fillRefBitmap2143 = 2143;
+    public long fillRefBitmap2144 = 2144;
+    public long fillRefBitmap2145 = 2145;
+    public long fillRefBitmap2146 = 2146;
+    public long fillRefBitmap2147 = 2147;
+    public long fillRefBitmap2148 = 2148;
+    public long fillRefBitmap2149 = 2149;
+    public long fillRefBitmap2150 = 2150;
+    public long fillRefBitmap2151 = 2151;
+    public long fillRefBitmap2152 = 2152;
+    public long fillRefBitmap2153 = 2153;
+    public long fillRefBitmap2154 = 2154;
+    public long fillRefBitmap2155 = 2155;
+    public long fillRefBitmap2156 = 2156;
+    public long fillRefBitmap2157 = 2157;
+    public long fillRefBitmap2158 = 2158;
+    public long fillRefBitmap2159 = 2159;
+    public long fillRefBitmap2160 = 2160;
+    public long fillRefBitmap2161 = 2161;
+    public long fillRefBitmap2162 = 2162;
+    public long fillRefBitmap2163 = 2163;
+    public long fillRefBitmap2164 = 2164;
+    public long fillRefBitmap2165 = 2165;
+    public long fillRefBitmap2166 = 2166;
+    public long fillRefBitmap2167 = 2167;
+    public long fillRefBitmap2168 = 2168;
+    public long fillRefBitmap2169 = 2169;
+    public long fillRefBitmap2170 = 2170;
+    public long fillRefBitmap2171 = 2171;
+    public long fillRefBitmap2172 = 2172;
+    public long fillRefBitmap2173 = 2173;
+    public long fillRefBitmap2174 = 2174;
+    public long fillRefBitmap2175 = 2175;
+    public long fillRefBitmap2176 = 2176;
+    public long fillRefBitmap2177 = 2177;
+    public long fillRefBitmap2178 = 2178;
+    public long fillRefBitmap2179 = 2179;
+    public long fillRefBitmap2180 = 2180;
+    public long fillRefBitmap2181 = 2181;
+    public long fillRefBitmap2182 = 2182;
+    public long fillRefBitmap2183 = 2183;
+    public long fillRefBitmap2184 = 2184;
+    public long fillRefBitmap2185 = 2185;
+    public long fillRefBitmap2186 = 2186;
+    public long fillRefBitmap2187 = 2187;
+    public long fillRefBitmap2188 = 2188;
+    public long fillRefBitmap2189 = 2189;
+    public long fillRefBitmap2190 = 2190;
+    public long fillRefBitmap2191 = 2191;
+    public long fillRefBitmap2192 = 2192;
+    public long fillRefBitmap2193 = 2193;
+    public long fillRefBitmap2194 = 2194;
+    public long fillRefBitmap2195 = 2195;
+    public long fillRefBitmap2196 = 2196;
+    public long fillRefBitmap2197 = 2197;
+    public long fillRefBitmap2198 = 2198;
+    public long fillRefBitmap2199 = 2199;
+    public long fillRefBitmap2200 = 2200;
+    public long fillRefBitmap2201 = 2201;
+    public long fillRefBitmap2202 = 2202;
+    public long fillRefBitmap2203 = 2203;
+    public long fillRefBitmap2204 = 2204;
+    public long fillRefBitmap2205 = 2205;
+    public long fillRefBitmap2206 = 2206;
+    public long fillRefBitmap2207 = 2207;
+    public long fillRefBitmap2208 = 2208;
+    public long fillRefBitmap2209 = 2209;
+    public long fillRefBitmap2210 = 2210;
+    public long fillRefBitmap2211 = 2211;
+    public long fillRefBitmap2212 = 2212;
+    public long fillRefBitmap2213 = 2213;
+    public long fillRefBitmap2214 = 2214;
+    public long fillRefBitmap2215 = 2215;
+    public long fillRefBitmap2216 = 2216;
+    public long fillRefBitmap2217 = 2217;
+    public long fillRefBitmap2218 = 2218;
+    public long fillRefBitmap2219 = 2219;
+    public long fillRefBitmap2220 = 2220;
+    public long fillRefBitmap2221 = 2221;
+    public long fillRefBitmap2222 = 2222;
+    public long fillRefBitmap2223 = 2223;
+    public long fillRefBitmap2224 = 2224;
+    public long fillRefBitmap2225 = 2225;
+    public long fillRefBitmap2226 = 2226;
+    public long fillRefBitmap2227 = 2227;
+    public long fillRefBitmap2228 = 2228;
+    public long fillRefBitmap2229 = 2229;
+    public long fillRefBitmap2230 = 2230;
+    public long fillRefBitmap2231 = 2231;
+    public long fillRefBitmap2232 = 2232;
+    public long fillRefBitmap2233 = 2233;
+    public long fillRefBitmap2234 = 2234;
+    public long fillRefBitmap2235 = 2235;
+    public long fillRefBitmap2236 = 2236;
+    public long fillRefBitmap2237 = 2237;
+    public long fillRefBitmap2238 = 2238;
+    public long fillRefBitmap2239 = 2239;
+    public long fillRefBitmap2240 = 2240;
+    public long fillRefBitmap2241 = 2241;
+    public long fillRefBitmap2242 = 2242;
+    public long fillRefBitmap2243 = 2243;
+    public long fillRefBitmap2244 = 2244;
+    public long fillRefBitmap2245 = 2245;
+    public long fillRefBitmap2246 = 2246;
+    public long fillRefBitmap2247 = 2247;
+    public long fillRefBitmap2248 = 2248;
+    public long fillRefBitmap2249 = 2249;
+    public long fillRefBitmap2250 = 2250;
+    public long fillRefBitmap2251 = 2251;
+    public long fillRefBitmap2252 = 2252;
+    public long fillRefBitmap2253 = 2253;
+    public long fillRefBitmap2254 = 2254;
+    public long fillRefBitmap2255 = 2255;
+    public long fillRefBitmap2256 = 2256;
+    public long fillRefBitmap2257 = 2257;
+    public long fillRefBitmap2258 = 2258;
+    public long fillRefBitmap2259 = 2259;
+    public long fillRefBitmap2260 = 2260;
+    public long fillRefBitmap2261 = 2261;
+    public long fillRefBitmap2262 = 2262;
+    public long fillRefBitmap2263 = 2263;
+    public long fillRefBitmap2264 = 2264;
+    public long fillRefBitmap2265 = 2265;
+    public long fillRefBitmap2266 = 2266;
+    public long fillRefBitmap2267 = 2267;
+    public long fillRefBitmap2268 = 2268;
+    public long fillRefBitmap2269 = 2269;
+    public long fillRefBitmap2270 = 2270;
+    public long fillRefBitmap2271 = 2271;
+    public long fillRefBitmap2272 = 2272;
+    public long fillRefBitmap2273 = 2273;
+    public long fillRefBitmap2274 = 2274;
+    public long fillRefBitmap2275 = 2275;
+    public long fillRefBitmap2276 = 2276;
+    public long fillRefBitmap2277 = 2277;
+    public long fillRefBitmap2278 = 2278;
+    public long fillRefBitmap2279 = 2279;
+    public long fillRefBitmap2280 = 2280;
+    public long fillRefBitmap2281 = 2281;
+    public long fillRefBitmap2282 = 2282;
+    public long fillRefBitmap2283 = 2283;
+    public long fillRefBitmap2284 = 2284;
+    public long fillRefBitmap2285 = 2285;
+    public long fillRefBitmap2286 = 2286;
+    public long fillRefBitmap2287 = 2287;
+    public long fillRefBitmap2288 = 2288;
+    public long fillRefBitmap2289 = 2289;
+    public long fillRefBitmap2290 = 2290;
+    public long fillRefBitmap2291 = 2291;
+    public long fillRefBitmap2292 = 2292;
+    public long fillRefBitmap2293 = 2293;
+    public long fillRefBitmap2294 = 2294;
+    public long fillRefBitmap2295 = 2295;
+    public long fillRefBitmap2296 = 2296;
+    public long fillRefBitmap2297 = 2297;
+    public long fillRefBitmap2298 = 2298;
+    public long fillRefBitmap2299 = 2299;
+    public long fillRefBitmap2300 = 2300;
+    public long fillRefBitmap2301 = 2301;
+    public long fillRefBitmap2302 = 2302;
+    public long fillRefBitmap2303 = 2303;
+    public long fillRefBitmap2304 = 2304;
+    public long fillRefBitmap2305 = 2305;
+    public long fillRefBitmap2306 = 2306;
+    public long fillRefBitmap2307 = 2307;
+    public long fillRefBitmap2308 = 2308;
+    public long fillRefBitmap2309 = 2309;
+    public long fillRefBitmap2310 = 2310;
+    public long fillRefBitmap2311 = 2311;
+    public long fillRefBitmap2312 = 2312;
+    public long fillRefBitmap2313 = 2313;
+    public long fillRefBitmap2314 = 2314;
+    public long fillRefBitmap2315 = 2315;
+    public long fillRefBitmap2316 = 2316;
+    public long fillRefBitmap2317 = 2317;
+    public long fillRefBitmap2318 = 2318;
+    public long fillRefBitmap2319 = 2319;
+    public long fillRefBitmap2320 = 2320;
+    public long fillRefBitmap2321 = 2321;
+    public long fillRefBitmap2322 = 2322;
+    public long fillRefBitmap2323 = 2323;
+    public long fillRefBitmap2324 = 2324;
+    public long fillRefBitmap2325 = 2325;
+    public long fillRefBitmap2326 = 2326;
+    public long fillRefBitmap2327 = 2327;
+    public long fillRefBitmap2328 = 2328;
+    public long fillRefBitmap2329 = 2329;
+    public long fillRefBitmap2330 = 2330;
+    public long fillRefBitmap2331 = 2331;
+    public long fillRefBitmap2332 = 2332;
+    public long fillRefBitmap2333 = 2333;
+    public long fillRefBitmap2334 = 2334;
+    public long fillRefBitmap2335 = 2335;
+    public long fillRefBitmap2336 = 2336;
+    public long fillRefBitmap2337 = 2337;
+    public long fillRefBitmap2338 = 2338;
+    public long fillRefBitmap2339 = 2339;
+    public long fillRefBitmap2340 = 2340;
+    public long fillRefBitmap2341 = 2341;
+    public long fillRefBitmap2342 = 2342;
+    public long fillRefBitmap2343 = 2343;
+    public long fillRefBitmap2344 = 2344;
+    public long fillRefBitmap2345 = 2345;
+    public long fillRefBitmap2346 = 2346;
+    public long fillRefBitmap2347 = 2347;
+    public long fillRefBitmap2348 = 2348;
+    public long fillRefBitmap2349 = 2349;
+    public long fillRefBitmap2350 = 2350;
+    public long fillRefBitmap2351 = 2351;
+    public long fillRefBitmap2352 = 2352;
+    public long fillRefBitmap2353 = 2353;
+    public long fillRefBitmap2354 = 2354;
+    public long fillRefBitmap2355 = 2355;
+    public long fillRefBitmap2356 = 2356;
+    public long fillRefBitmap2357 = 2357;
+    public long fillRefBitmap2358 = 2358;
+    public long fillRefBitmap2359 = 2359;
+    public long fillRefBitmap2360 = 2360;
+    public long fillRefBitmap2361 = 2361;
+    public long fillRefBitmap2362 = 2362;
+    public long fillRefBitmap2363 = 2363;
+    public long fillRefBitmap2364 = 2364;
+    public long fillRefBitmap2365 = 2365;
+    public long fillRefBitmap2366 = 2366;
+    public long fillRefBitmap2367 = 2367;
+    public long fillRefBitmap2368 = 2368;
+    public long fillRefBitmap2369 = 2369;
+    public long fillRefBitmap2370 = 2370;
+    public long fillRefBitmap2371 = 2371;
+    public long fillRefBitmap2372 = 2372;
+    public long fillRefBitmap2373 = 2373;
+    public long fillRefBitmap2374 = 2374;
+    public long fillRefBitmap2375 = 2375;
+    public long fillRefBitmap2376 = 2376;
+    public long fillRefBitmap2377 = 2377;
+    public long fillRefBitmap2378 = 2378;
+    public long fillRefBitmap2379 = 2379;
+    public long fillRefBitmap2380 = 2380;
+    public long fillRefBitmap2381 = 2381;
+    public long fillRefBitmap2382 = 2382;
+    public long fillRefBitmap2383 = 2383;
+    public long fillRefBitmap2384 = 2384;
+    public long fillRefBitmap2385 = 2385;
+    public long fillRefBitmap2386 = 2386;
+    public long fillRefBitmap2387 = 2387;
+    public long fillRefBitmap2388 = 2388;
+    public long fillRefBitmap2389 = 2389;
+    public long fillRefBitmap2390 = 2390;
+    public long fillRefBitmap2391 = 2391;
+    public long fillRefBitmap2392 = 2392;
+    public long fillRefBitmap2393 = 2393;
+    public long fillRefBitmap2394 = 2394;
+    public long fillRefBitmap2395 = 2395;
+    public long fillRefBitmap2396 = 2396;
+    public long fillRefBitmap2397 = 2397;
+    public long fillRefBitmap2398 = 2398;
+    public long fillRefBitmap2399 = 2399;
+    public long fillRefBitmap2400 = 2400;
+    public long fillRefBitmap2401 = 2401;
+    public long fillRefBitmap2402 = 2402;
+    public long fillRefBitmap2403 = 2403;
+    public long fillRefBitmap2404 = 2404;
+    public long fillRefBitmap2405 = 2405;
+    public long fillRefBitmap2406 = 2406;
+    public long fillRefBitmap2407 = 2407;
+    public long fillRefBitmap2408 = 2408;
+    public long fillRefBitmap2409 = 2409;
+    public long fillRefBitmap2410 = 2410;
+    public long fillRefBitmap2411 = 2411;
+    public long fillRefBitmap2412 = 2412;
+    public long fillRefBitmap2413 = 2413;
+    public long fillRefBitmap2414 = 2414;
+    public long fillRefBitmap2415 = 2415;
+    public long fillRefBitmap2416 = 2416;
+    public long fillRefBitmap2417 = 2417;
+    public long fillRefBitmap2418 = 2418;
+    public long fillRefBitmap2419 = 2419;
+    public long fillRefBitmap2420 = 2420;
+    public long fillRefBitmap2421 = 2421;
+    public long fillRefBitmap2422 = 2422;
+    public long fillRefBitmap2423 = 2423;
+    public long fillRefBitmap2424 = 2424;
+    public long fillRefBitmap2425 = 2425;
+    public long fillRefBitmap2426 = 2426;
+    public long fillRefBitmap2427 = 2427;
+    public long fillRefBitmap2428 = 2428;
+    public long fillRefBitmap2429 = 2429;
+    public long fillRefBitmap2430 = 2430;
+    public long fillRefBitmap2431 = 2431;
+    public long fillRefBitmap2432 = 2432;
+    public long fillRefBitmap2433 = 2433;
+    public long fillRefBitmap2434 = 2434;
+    public long fillRefBitmap2435 = 2435;
+    public long fillRefBitmap2436 = 2436;
+    public long fillRefBitmap2437 = 2437;
+    public long fillRefBitmap2438 = 2438;
+    public long fillRefBitmap2439 = 2439;
+    public long fillRefBitmap2440 = 2440;
+    public long fillRefBitmap2441 = 2441;
+    public long fillRefBitmap2442 = 2442;
+    public long fillRefBitmap2443 = 2443;
+    public long fillRefBitmap2444 = 2444;
+    public long fillRefBitmap2445 = 2445;
+    public long fillRefBitmap2446 = 2446;
+    public long fillRefBitmap2447 = 2447;
+    public long fillRefBitmap2448 = 2448;
+    public long fillRefBitmap2449 = 2449;
+    public long fillRefBitmap2450 = 2450;
+    public long fillRefBitmap2451 = 2451;
+    public long fillRefBitmap2452 = 2452;
+    public long fillRefBitmap2453 = 2453;
+    public long fillRefBitmap2454 = 2454;
+    public long fillRefBitmap2455 = 2455;
+    public long fillRefBitmap2456 = 2456;
+    public long fillRefBitmap2457 = 2457;
+    public long fillRefBitmap2458 = 2458;
+    public long fillRefBitmap2459 = 2459;
+    public long fillRefBitmap2460 = 2460;
+    public long fillRefBitmap2461 = 2461;
+    public long fillRefBitmap2462 = 2462;
+    public long fillRefBitmap2463 = 2463;
+    public long fillRefBitmap2464 = 2464;
+    public long fillRefBitmap2465 = 2465;
+    public long fillRefBitmap2466 = 2466;
+    public long fillRefBitmap2467 = 2467;
+    public long fillRefBitmap2468 = 2468;
+    public long fillRefBitmap2469 = 2469;
+    public long fillRefBitmap2470 = 2470;
+    public long fillRefBitmap2471 = 2471;
+    public long fillRefBitmap2472 = 2472;
+    public long fillRefBitmap2473 = 2473;
+    public long fillRefBitmap2474 = 2474;
+    public long fillRefBitmap2475 = 2475;
+    public long fillRefBitmap2476 = 2476;
+    public long fillRefBitmap2477 = 2477;
+    public long fillRefBitmap2478 = 2478;
+    public long fillRefBitmap2479 = 2479;
+    public long fillRefBitmap2480 = 2480;
+    public long fillRefBitmap2481 = 2481;
+    public long fillRefBitmap2482 = 2482;
+    public long fillRefBitmap2483 = 2483;
+    public long fillRefBitmap2484 = 2484;
+    public long fillRefBitmap2485 = 2485;
+    public long fillRefBitmap2486 = 2486;
+    public long fillRefBitmap2487 = 2487;
+    public long fillRefBitmap2488 = 2488;
+    public long fillRefBitmap2489 = 2489;
+    public long fillRefBitmap2490 = 2490;
+    public long fillRefBitmap2491 = 2491;
+    public long fillRefBitmap2492 = 2492;
+    public long fillRefBitmap2493 = 2493;
+    public long fillRefBitmap2494 = 2494;
+    public long fillRefBitmap2495 = 2495;
+    public long fillRefBitmap2496 = 2496;
+    public long fillRefBitmap2497 = 2497;
+    public long fillRefBitmap2498 = 2498;
+    public long fillRefBitmap2499 = 2499;
+    public long fillRefBitmap2500 = 2500;
+    public long fillRefBitmap2501 = 2501;
+    public long fillRefBitmap2502 = 2502;
+    public long fillRefBitmap2503 = 2503;
+    public long fillRefBitmap2504 = 2504;
+    public long fillRefBitmap2505 = 2505;
+    public long fillRefBitmap2506 = 2506;
+    public long fillRefBitmap2507 = 2507;
+    public long fillRefBitmap2508 = 2508;
+    public long fillRefBitmap2509 = 2509;
+    public long fillRefBitmap2510 = 2510;
+    public long fillRefBitmap2511 = 2511;
+    public long fillRefBitmap2512 = 2512;
+    public long fillRefBitmap2513 = 2513;
+    public long fillRefBitmap2514 = 2514;
+    public long fillRefBitmap2515 = 2515;
+    public long fillRefBitmap2516 = 2516;
+    public long fillRefBitmap2517 = 2517;
+    public long fillRefBitmap2518 = 2518;
+    public long fillRefBitmap2519 = 2519;
+    public long fillRefBitmap2520 = 2520;
+    public long fillRefBitmap2521 = 2521;
+    public long fillRefBitmap2522 = 2522;
+    public long fillRefBitmap2523 = 2523;
+    public long fillRefBitmap2524 = 2524;
+    public long fillRefBitmap2525 = 2525;
+    public long fillRefBitmap2526 = 2526;
+    public long fillRefBitmap2527 = 2527;
+    public long fillRefBitmap2528 = 2528;
+    public long fillRefBitmap2529 = 2529;
+    public long fillRefBitmap2530 = 2530;
+    public long fillRefBitmap2531 = 2531;
+    public long fillRefBitmap2532 = 2532;
+    public long fillRefBitmap2533 = 2533;
+    public long fillRefBitmap2534 = 2534;
+    public long fillRefBitmap2535 = 2535;
+    public long fillRefBitmap2536 = 2536;
+    public long fillRefBitmap2537 = 2537;
+    public long fillRefBitmap2538 = 2538;
+    public long fillRefBitmap2539 = 2539;
+    public long fillRefBitmap2540 = 2540;
+    public long fillRefBitmap2541 = 2541;
+    public long fillRefBitmap2542 = 2542;
+    public long fillRefBitmap2543 = 2543;
+    public long fillRefBitmap2544 = 2544;
+    public long fillRefBitmap2545 = 2545;
+    public long fillRefBitmap2546 = 2546;
+    public long fillRefBitmap2547 = 2547;
+    public long fillRefBitmap2548 = 2548;
+    public long fillRefBitmap2549 = 2549;
+    public long fillRefBitmap2550 = 2550;
+    public long fillRefBitmap2551 = 2551;
+    public long fillRefBitmap2552 = 2552;
+    public long fillRefBitmap2553 = 2553;
+    public long fillRefBitmap2554 = 2554;
+    public long fillRefBitmap2555 = 2555;
+    public long fillRefBitmap2556 = 2556;
+    public long fillRefBitmap2557 = 2557;
+    public long fillRefBitmap2558 = 2558;
+    public long fillRefBitmap2559 = 2559;
+    public long fillRefBitmap2560 = 2560;
+    public long fillRefBitmap2561 = 2561;
+    public long fillRefBitmap2562 = 2562;
+    public long fillRefBitmap2563 = 2563;
+    public long fillRefBitmap2564 = 2564;
+    public long fillRefBitmap2565 = 2565;
+    public long fillRefBitmap2566 = 2566;
+    public long fillRefBitmap2567 = 2567;
+    public long fillRefBitmap2568 = 2568;
+    public long fillRefBitmap2569 = 2569;
+    public long fillRefBitmap2570 = 2570;
+    public long fillRefBitmap2571 = 2571;
+    public long fillRefBitmap2572 = 2572;
+    public long fillRefBitmap2573 = 2573;
+    public long fillRefBitmap2574 = 2574;
+    public long fillRefBitmap2575 = 2575;
+    public long fillRefBitmap2576 = 2576;
+    public long fillRefBitmap2577 = 2577;
+    public long fillRefBitmap2578 = 2578;
+    public long fillRefBitmap2579 = 2579;
+    public long fillRefBitmap2580 = 2580;
+    public long fillRefBitmap2581 = 2581;
+    public long fillRefBitmap2582 = 2582;
+    public long fillRefBitmap2583 = 2583;
+    public long fillRefBitmap2584 = 2584;
+    public long fillRefBitmap2585 = 2585;
+    public long fillRefBitmap2586 = 2586;
+    public long fillRefBitmap2587 = 2587;
+    public long fillRefBitmap2588 = 2588;
+    public long fillRefBitmap2589 = 2589;
+    public long fillRefBitmap2590 = 2590;
+    public long fillRefBitmap2591 = 2591;
+    public long fillRefBitmap2592 = 2592;
+    public long fillRefBitmap2593 = 2593;
+    public long fillRefBitmap2594 = 2594;
+    public long fillRefBitmap2595 = 2595;
+    public long fillRefBitmap2596 = 2596;
+    public long fillRefBitmap2597 = 2597;
+    public long fillRefBitmap2598 = 2598;
+    public long fillRefBitmap2599 = 2599;
+    public long fillRefBitmap2600 = 2600;
+    public long fillRefBitmap2601 = 2601;
+    public long fillRefBitmap2602 = 2602;
+    public long fillRefBitmap2603 = 2603;
+    public long fillRefBitmap2604 = 2604;
+    public long fillRefBitmap2605 = 2605;
+    public long fillRefBitmap2606 = 2606;
+    public long fillRefBitmap2607 = 2607;
+    public long fillRefBitmap2608 = 2608;
+    public long fillRefBitmap2609 = 2609;
+    public long fillRefBitmap2610 = 2610;
+    public long fillRefBitmap2611 = 2611;
+    public long fillRefBitmap2612 = 2612;
+    public long fillRefBitmap2613 = 2613;
+    public long fillRefBitmap2614 = 2614;
+    public long fillRefBitmap2615 = 2615;
+    public long fillRefBitmap2616 = 2616;
+    public long fillRefBitmap2617 = 2617;
+    public long fillRefBitmap2618 = 2618;
+    public long fillRefBitmap2619 = 2619;
+    public long fillRefBitmap2620 = 2620;
+    public long fillRefBitmap2621 = 2621;
+    public long fillRefBitmap2622 = 2622;
+    public long fillRefBitmap2623 = 2623;
+    public long fillRefBitmap2624 = 2624;
+    public long fillRefBitmap2625 = 2625;
+    public long fillRefBitmap2626 = 2626;
+    public long fillRefBitmap2627 = 2627;
+    public long fillRefBitmap2628 = 2628;
+    public long fillRefBitmap2629 = 2629;
+    public long fillRefBitmap2630 = 2630;
+    public long fillRefBitmap2631 = 2631;
+    public long fillRefBitmap2632 = 2632;
+    public long fillRefBitmap2633 = 2633;
+    public long fillRefBitmap2634 = 2634;
+    public long fillRefBitmap2635 = 2635;
+    public long fillRefBitmap2636 = 2636;
+    public long fillRefBitmap2637 = 2637;
+    public long fillRefBitmap2638 = 2638;
+    public long fillRefBitmap2639 = 2639;
+    public long fillRefBitmap2640 = 2640;
+    public long fillRefBitmap2641 = 2641;
+    public long fillRefBitmap2642 = 2642;
+    public long fillRefBitmap2643 = 2643;
+    public long fillRefBitmap2644 = 2644;
+    public long fillRefBitmap2645 = 2645;
+    public long fillRefBitmap2646 = 2646;
+    public long fillRefBitmap2647 = 2647;
+    public long fillRefBitmap2648 = 2648;
+    public long fillRefBitmap2649 = 2649;
+    public long fillRefBitmap2650 = 2650;
+    public long fillRefBitmap2651 = 2651;
+    public long fillRefBitmap2652 = 2652;
+    public long fillRefBitmap2653 = 2653;
+    public long fillRefBitmap2654 = 2654;
+    public long fillRefBitmap2655 = 2655;
+    public long fillRefBitmap2656 = 2656;
+    public long fillRefBitmap2657 = 2657;
+    public long fillRefBitmap2658 = 2658;
+    public long fillRefBitmap2659 = 2659;
+    public long fillRefBitmap2660 = 2660;
+    public long fillRefBitmap2661 = 2661;
+    public long fillRefBitmap2662 = 2662;
+    public long fillRefBitmap2663 = 2663;
+    public long fillRefBitmap2664 = 2664;
+    public long fillRefBitmap2665 = 2665;
+    public long fillRefBitmap2666 = 2666;
+    public long fillRefBitmap2667 = 2667;
+    public long fillRefBitmap2668 = 2668;
+    public long fillRefBitmap2669 = 2669;
+    public long fillRefBitmap2670 = 2670;
+    public long fillRefBitmap2671 = 2671;
+    public long fillRefBitmap2672 = 2672;
+    public long fillRefBitmap2673 = 2673;
+    public long fillRefBitmap2674 = 2674;
+    public long fillRefBitmap2675 = 2675;
+    public long fillRefBitmap2676 = 2676;
+    public long fillRefBitmap2677 = 2677;
+    public long fillRefBitmap2678 = 2678;
+    public long fillRefBitmap2679 = 2679;
+    public long fillRefBitmap2680 = 2680;
+    public long fillRefBitmap2681 = 2681;
+    public long fillRefBitmap2682 = 2682;
+    public long fillRefBitmap2683 = 2683;
+    public long fillRefBitmap2684 = 2684;
+    public long fillRefBitmap2685 = 2685;
+    public long fillRefBitmap2686 = 2686;
+    public long fillRefBitmap2687 = 2687;
+    public long fillRefBitmap2688 = 2688;
+    public long fillRefBitmap2689 = 2689;
+    public long fillRefBitmap2690 = 2690;
+    public long fillRefBitmap2691 = 2691;
+    public long fillRefBitmap2692 = 2692;
+    public long fillRefBitmap2693 = 2693;
+    public long fillRefBitmap2694 = 2694;
+    public long fillRefBitmap2695 = 2695;
+    public long fillRefBitmap2696 = 2696;
+    public long fillRefBitmap2697 = 2697;
+    public long fillRefBitmap2698 = 2698;
+    public long fillRefBitmap2699 = 2699;
+    public long fillRefBitmap2700 = 2700;
+    public long fillRefBitmap2701 = 2701;
+    public long fillRefBitmap2702 = 2702;
+    public long fillRefBitmap2703 = 2703;
+    public long fillRefBitmap2704 = 2704;
+    public long fillRefBitmap2705 = 2705;
+    public long fillRefBitmap2706 = 2706;
+    public long fillRefBitmap2707 = 2707;
+    public long fillRefBitmap2708 = 2708;
+    public long fillRefBitmap2709 = 2709;
+    public long fillRefBitmap2710 = 2710;
+    public long fillRefBitmap2711 = 2711;
+    public long fillRefBitmap2712 = 2712;
+    public long fillRefBitmap2713 = 2713;
+    public long fillRefBitmap2714 = 2714;
+    public long fillRefBitmap2715 = 2715;
+    public long fillRefBitmap2716 = 2716;
+    public long fillRefBitmap2717 = 2717;
+    public long fillRefBitmap2718 = 2718;
+    public long fillRefBitmap2719 = 2719;
+    public long fillRefBitmap2720 = 2720;
+    public long fillRefBitmap2721 = 2721;
+    public long fillRefBitmap2722 = 2722;
+    public long fillRefBitmap2723 = 2723;
+    public long fillRefBitmap2724 = 2724;
+    public long fillRefBitmap2725 = 2725;
+    public long fillRefBitmap2726 = 2726;
+    public long fillRefBitmap2727 = 2727;
+    public long fillRefBitmap2728 = 2728;
+    public long fillRefBitmap2729 = 2729;
+    public long fillRefBitmap2730 = 2730;
+    public long fillRefBitmap2731 = 2731;
+    public long fillRefBitmap2732 = 2732;
+    public long fillRefBitmap2733 = 2733;
+    public long fillRefBitmap2734 = 2734;
+    public long fillRefBitmap2735 = 2735;
+    public long fillRefBitmap2736 = 2736;
+    public long fillRefBitmap2737 = 2737;
+    public long fillRefBitmap2738 = 2738;
+    public long fillRefBitmap2739 = 2739;
+    public long fillRefBitmap2740 = 2740;
+    public long fillRefBitmap2741 = 2741;
+    public long fillRefBitmap2742 = 2742;
+    public long fillRefBitmap2743 = 2743;
+    public long fillRefBitmap2744 = 2744;
+    public long fillRefBitmap2745 = 2745;
+    public long fillRefBitmap2746 = 2746;
+    public long fillRefBitmap2747 = 2747;
+    public long fillRefBitmap2748 = 2748;
+    public long fillRefBitmap2749 = 2749;
+    public long fillRefBitmap2750 = 2750;
+    public long fillRefBitmap2751 = 2751;
+    public long fillRefBitmap2752 = 2752;
+    public long fillRefBitmap2753 = 2753;
+    public long fillRefBitmap2754 = 2754;
+    public long fillRefBitmap2755 = 2755;
+    public long fillRefBitmap2756 = 2756;
+    public long fillRefBitmap2757 = 2757;
+    public long fillRefBitmap2758 = 2758;
+    public long fillRefBitmap2759 = 2759;
+    public long fillRefBitmap2760 = 2760;
+    public long fillRefBitmap2761 = 2761;
+    public long fillRefBitmap2762 = 2762;
+    public long fillRefBitmap2763 = 2763;
+    public long fillRefBitmap2764 = 2764;
+    public long fillRefBitmap2765 = 2765;
+    public long fillRefBitmap2766 = 2766;
+    public long fillRefBitmap2767 = 2767;
+    public long fillRefBitmap2768 = 2768;
+    public long fillRefBitmap2769 = 2769;
+    public long fillRefBitmap2770 = 2770;
+    public long fillRefBitmap2771 = 2771;
+    public long fillRefBitmap2772 = 2772;
+    public long fillRefBitmap2773 = 2773;
+    public long fillRefBitmap2774 = 2774;
+    public long fillRefBitmap2775 = 2775;
+    public long fillRefBitmap2776 = 2776;
+    public long fillRefBitmap2777 = 2777;
+    public long fillRefBitmap2778 = 2778;
+    public long fillRefBitmap2779 = 2779;
+    public long fillRefBitmap2780 = 2780;
+    public long fillRefBitmap2781 = 2781;
+    public long fillRefBitmap2782 = 2782;
+    public long fillRefBitmap2783 = 2783;
+    public long fillRefBitmap2784 = 2784;
+    public long fillRefBitmap2785 = 2785;
+    public long fillRefBitmap2786 = 2786;
+    public long fillRefBitmap2787 = 2787;
+    public long fillRefBitmap2788 = 2788;
+    public long fillRefBitmap2789 = 2789;
+    public long fillRefBitmap2790 = 2790;
+    public long fillRefBitmap2791 = 2791;
+    public long fillRefBitmap2792 = 2792;
+    public long fillRefBitmap2793 = 2793;
+    public long fillRefBitmap2794 = 2794;
+    public long fillRefBitmap2795 = 2795;
+    public long fillRefBitmap2796 = 2796;
+    public long fillRefBitmap2797 = 2797;
+    public long fillRefBitmap2798 = 2798;
+    public long fillRefBitmap2799 = 2799;
+    public long fillRefBitmap2800 = 2800;
+    public long fillRefBitmap2801 = 2801;
+    public long fillRefBitmap2802 = 2802;
+    public long fillRefBitmap2803 = 2803;
+    public long fillRefBitmap2804 = 2804;
+    public long fillRefBitmap2805 = 2805;
+    public long fillRefBitmap2806 = 2806;
+    public long fillRefBitmap2807 = 2807;
+    public long fillRefBitmap2808 = 2808;
+    public long fillRefBitmap2809 = 2809;
+    public long fillRefBitmap2810 = 2810;
+    public long fillRefBitmap2811 = 2811;
+    public long fillRefBitmap2812 = 2812;
+    public long fillRefBitmap2813 = 2813;
+    public long fillRefBitmap2814 = 2814;
+    public long fillRefBitmap2815 = 2815;
+    public long fillRefBitmap2816 = 2816;
+    public long fillRefBitmap2817 = 2817;
+    public long fillRefBitmap2818 = 2818;
+    public long fillRefBitmap2819 = 2819;
+    public long fillRefBitmap2820 = 2820;
+    public long fillRefBitmap2821 = 2821;
+    public long fillRefBitmap2822 = 2822;
+    public long fillRefBitmap2823 = 2823;
+    public long fillRefBitmap2824 = 2824;
+    public long fillRefBitmap2825 = 2825;
+    public long fillRefBitmap2826 = 2826;
+    public long fillRefBitmap2827 = 2827;
+    public long fillRefBitmap2828 = 2828;
+    public long fillRefBitmap2829 = 2829;
+    public long fillRefBitmap2830 = 2830;
+    public long fillRefBitmap2831 = 2831;
+    public long fillRefBitmap2832 = 2832;
+    public long fillRefBitmap2833 = 2833;
+    public long fillRefBitmap2834 = 2834;
+    public long fillRefBitmap2835 = 2835;
+    public long fillRefBitmap2836 = 2836;
+    public long fillRefBitmap2837 = 2837;
+    public long fillRefBitmap2838 = 2838;
+    public long fillRefBitmap2839 = 2839;
+    public long fillRefBitmap2840 = 2840;
+    public long fillRefBitmap2841 = 2841;
+    public long fillRefBitmap2842 = 2842;
+    public long fillRefBitmap2843 = 2843;
+    public long fillRefBitmap2844 = 2844;
+    public long fillRefBitmap2845 = 2845;
+    public long fillRefBitmap2846 = 2846;
+    public long fillRefBitmap2847 = 2847;
+    public long fillRefBitmap2848 = 2848;
+    public long fillRefBitmap2849 = 2849;
+    public long fillRefBitmap2850 = 2850;
+    public long fillRefBitmap2851 = 2851;
+    public long fillRefBitmap2852 = 2852;
+    public long fillRefBitmap2853 = 2853;
+    public long fillRefBitmap2854 = 2854;
+    public long fillRefBitmap2855 = 2855;
+    public long fillRefBitmap2856 = 2856;
+    public long fillRefBitmap2857 = 2857;
+    public long fillRefBitmap2858 = 2858;
+    public long fillRefBitmap2859 = 2859;
+    public long fillRefBitmap2860 = 2860;
+    public long fillRefBitmap2861 = 2861;
+    public long fillRefBitmap2862 = 2862;
+    public long fillRefBitmap2863 = 2863;
+    public long fillRefBitmap2864 = 2864;
+    public long fillRefBitmap2865 = 2865;
+    public long fillRefBitmap2866 = 2866;
+    public long fillRefBitmap2867 = 2867;
+    public long fillRefBitmap2868 = 2868;
+    public long fillRefBitmap2869 = 2869;
+    public long fillRefBitmap2870 = 2870;
+    public long fillRefBitmap2871 = 2871;
+    public long fillRefBitmap2872 = 2872;
+    public long fillRefBitmap2873 = 2873;
+    public long fillRefBitmap2874 = 2874;
+    public long fillRefBitmap2875 = 2875;
+    public long fillRefBitmap2876 = 2876;
+    public long fillRefBitmap2877 = 2877;
+    public long fillRefBitmap2878 = 2878;
+    public long fillRefBitmap2879 = 2879;
+    public long fillRefBitmap2880 = 2880;
+    public long fillRefBitmap2881 = 2881;
+    public long fillRefBitmap2882 = 2882;
+    public long fillRefBitmap2883 = 2883;
+    public long fillRefBitmap2884 = 2884;
+    public long fillRefBitmap2885 = 2885;
+    public long fillRefBitmap2886 = 2886;
+    public long fillRefBitmap2887 = 2887;
+    public long fillRefBitmap2888 = 2888;
+    public long fillRefBitmap2889 = 2889;
+    public long fillRefBitmap2890 = 2890;
+    public long fillRefBitmap2891 = 2891;
+    public long fillRefBitmap2892 = 2892;
+    public long fillRefBitmap2893 = 2893;
+    public long fillRefBitmap2894 = 2894;
+    public long fillRefBitmap2895 = 2895;
+    public long fillRefBitmap2896 = 2896;
+    public long fillRefBitmap2897 = 2897;
+    public long fillRefBitmap2898 = 2898;
+    public long fillRefBitmap2899 = 2899;
+    public long fillRefBitmap2900 = 2900;
+    public long fillRefBitmap2901 = 2901;
+    public long fillRefBitmap2902 = 2902;
+    public long fillRefBitmap2903 = 2903;
+    public long fillRefBitmap2904 = 2904;
+    public long fillRefBitmap2905 = 2905;
+    public long fillRefBitmap2906 = 2906;
+    public long fillRefBitmap2907 = 2907;
+    public long fillRefBitmap2908 = 2908;
+    public long fillRefBitmap2909 = 2909;
+    public long fillRefBitmap2910 = 2910;
+    public long fillRefBitmap2911 = 2911;
+    public long fillRefBitmap2912 = 2912;
+    public long fillRefBitmap2913 = 2913;
+    public long fillRefBitmap2914 = 2914;
+    public long fillRefBitmap2915 = 2915;
+    public long fillRefBitmap2916 = 2916;
+    public long fillRefBitmap2917 = 2917;
+    public long fillRefBitmap2918 = 2918;
+    public long fillRefBitmap2919 = 2919;
+    public long fillRefBitmap2920 = 2920;
+    public long fillRefBitmap2921 = 2921;
+    public long fillRefBitmap2922 = 2922;
+    public long fillRefBitmap2923 = 2923;
+    public long fillRefBitmap2924 = 2924;
+    public long fillRefBitmap2925 = 2925;
+    public long fillRefBitmap2926 = 2926;
+    public long fillRefBitmap2927 = 2927;
+    public long fillRefBitmap2928 = 2928;
+    public long fillRefBitmap2929 = 2929;
+    public long fillRefBitmap2930 = 2930;
+    public long fillRefBitmap2931 = 2931;
+    public long fillRefBitmap2932 = 2932;
+    public long fillRefBitmap2933 = 2933;
+    public long fillRefBitmap2934 = 2934;
+    public long fillRefBitmap2935 = 2935;
+    public long fillRefBitmap2936 = 2936;
+    public long fillRefBitmap2937 = 2937;
+    public long fillRefBitmap2938 = 2938;
+    public long fillRefBitmap2939 = 2939;
+    public long fillRefBitmap2940 = 2940;
+    public long fillRefBitmap2941 = 2941;
+    public long fillRefBitmap2942 = 2942;
+    public long fillRefBitmap2943 = 2943;
+    public long fillRefBitmap2944 = 2944;
+    public long fillRefBitmap2945 = 2945;
+    public long fillRefBitmap2946 = 2946;
+    public long fillRefBitmap2947 = 2947;
+    public long fillRefBitmap2948 = 2948;
+    public long fillRefBitmap2949 = 2949;
+    public long fillRefBitmap2950 = 2950;
+    public long fillRefBitmap2951 = 2951;
+    public long fillRefBitmap2952 = 2952;
+    public long fillRefBitmap2953 = 2953;
+    public long fillRefBitmap2954 = 2954;
+    public long fillRefBitmap2955 = 2955;
+    public long fillRefBitmap2956 = 2956;
+    public long fillRefBitmap2957 = 2957;
+    public long fillRefBitmap2958 = 2958;
+    public long fillRefBitmap2959 = 2959;
+    public long fillRefBitmap2960 = 2960;
+    public long fillRefBitmap2961 = 2961;
+    public long fillRefBitmap2962 = 2962;
+    public long fillRefBitmap2963 = 2963;
+    public long fillRefBitmap2964 = 2964;
+    public long fillRefBitmap2965 = 2965;
+    public long fillRefBitmap2966 = 2966;
+    public long fillRefBitmap2967 = 2967;
+    public long fillRefBitmap2968 = 2968;
+    public long fillRefBitmap2969 = 2969;
+    public long fillRefBitmap2970 = 2970;
+    public long fillRefBitmap2971 = 2971;
+    public long fillRefBitmap2972 = 2972;
+    public long fillRefBitmap2973 = 2973;
+    public long fillRefBitmap2974 = 2974;
+    public long fillRefBitmap2975 = 2975;
+    public long fillRefBitmap2976 = 2976;
+    public long fillRefBitmap2977 = 2977;
+    public long fillRefBitmap2978 = 2978;
+    public long fillRefBitmap2979 = 2979;
+    public long fillRefBitmap2980 = 2980;
+    public long fillRefBitmap2981 = 2981;
+    public long fillRefBitmap2982 = 2982;
+    public long fillRefBitmap2983 = 2983;
+    public long fillRefBitmap2984 = 2984;
+    public long fillRefBitmap2985 = 2985;
+    public long fillRefBitmap2986 = 2986;
+    public long fillRefBitmap2987 = 2987;
+    public long fillRefBitmap2988 = 2988;
+    public long fillRefBitmap2989 = 2989;
+    public long fillRefBitmap2990 = 2990;
+    public long fillRefBitmap2991 = 2991;
+    public long fillRefBitmap2992 = 2992;
+    public long fillRefBitmap2993 = 2993;
+    public long fillRefBitmap2994 = 2994;
+    public long fillRefBitmap2995 = 2995;
+    public long fillRefBitmap2996 = 2996;
+    public long fillRefBitmap2997 = 2997;
+    public long fillRefBitmap2998 = 2998;
+    public long fillRefBitmap2999 = 2999;
+    public long fillRefBitmap3000 = 3000;
+    public long fillRefBitmap3001 = 3001;
+    public long fillRefBitmap3002 = 3002;
+    public long fillRefBitmap3003 = 3003;
+    public long fillRefBitmap3004 = 3004;
+    public long fillRefBitmap3005 = 3005;
+    public long fillRefBitmap3006 = 3006;
+    public long fillRefBitmap3007 = 3007;
+    public long fillRefBitmap3008 = 3008;
+    public long fillRefBitmap3009 = 3009;
+    public long fillRefBitmap3010 = 3010;
+    public long fillRefBitmap3011 = 3011;
+    public long fillRefBitmap3012 = 3012;
+    public long fillRefBitmap3013 = 3013;
+    public long fillRefBitmap3014 = 3014;
+    public long fillRefBitmap3015 = 3015;
+    public long fillRefBitmap3016 = 3016;
+    public long fillRefBitmap3017 = 3017;
+    public long fillRefBitmap3018 = 3018;
+    public long fillRefBitmap3019 = 3019;
+    public long fillRefBitmap3020 = 3020;
+    public long fillRefBitmap3021 = 3021;
+    public long fillRefBitmap3022 = 3022;
+    public long fillRefBitmap3023 = 3023;
+    public long fillRefBitmap3024 = 3024;
+    public long fillRefBitmap3025 = 3025;
+    public long fillRefBitmap3026 = 3026;
+    public long fillRefBitmap3027 = 3027;
+    public long fillRefBitmap3028 = 3028;
+    public long fillRefBitmap3029 = 3029;
+    public long fillRefBitmap3030 = 3030;
+    public long fillRefBitmap3031 = 3031;
+    public long fillRefBitmap3032 = 3032;
+    public long fillRefBitmap3033 = 3033;
+    public long fillRefBitmap3034 = 3034;
+    public long fillRefBitmap3035 = 3035;
+    public long fillRefBitmap3036 = 3036;
+    public long fillRefBitmap3037 = 3037;
+    public long fillRefBitmap3038 = 3038;
+    public long fillRefBitmap3039 = 3039;
+    public long fillRefBitmap3040 = 3040;
+    public long fillRefBitmap3041 = 3041;
+    public long fillRefBitmap3042 = 3042;
+    public long fillRefBitmap3043 = 3043;
+    public long fillRefBitmap3044 = 3044;
+    public long fillRefBitmap3045 = 3045;
+    public long fillRefBitmap3046 = 3046;
+    public long fillRefBitmap3047 = 3047;
+    public long fillRefBitmap3048 = 3048;
+    public long fillRefBitmap3049 = 3049;
+    public long fillRefBitmap3050 = 3050;
+    public long fillRefBitmap3051 = 3051;
+    public long fillRefBitmap3052 = 3052;
+    public long fillRefBitmap3053 = 3053;
+    public long fillRefBitmap3054 = 3054;
+    public long fillRefBitmap3055 = 3055;
+    public long fillRefBitmap3056 = 3056;
+    public long fillRefBitmap3057 = 3057;
+    public long fillRefBitmap3058 = 3058;
+    public long fillRefBitmap3059 = 3059;
+    public long fillRefBitmap3060 = 3060;
+    public long fillRefBitmap3061 = 3061;
+    public long fillRefBitmap3062 = 3062;
+    public long fillRefBitmap3063 = 3063;
+    public long fillRefBitmap3064 = 3064;
+    public long fillRefBitmap3065 = 3065;
+    public long fillRefBitmap3066 = 3066;
+    public long fillRefBitmap3067 = 3067;
+    public long fillRefBitmap3068 = 3068;
+    public long fillRefBitmap3069 = 3069;
+    public long fillRefBitmap3070 = 3070;
+    public long fillRefBitmap3071 = 3071;
+    public long fillRefBitmap3072 = 3072;
+    public long fillRefBitmap3073 = 3073;
+    public long fillRefBitmap3074 = 3074;
+    public long fillRefBitmap3075 = 3075;
+    public long fillRefBitmap3076 = 3076;
+    public long fillRefBitmap3077 = 3077;
+    public long fillRefBitmap3078 = 3078;
+    public long fillRefBitmap3079 = 3079;
+    public long fillRefBitmap3080 = 3080;
+    public long fillRefBitmap3081 = 3081;
+    public long fillRefBitmap3082 = 3082;
+    public long fillRefBitmap3083 = 3083;
+    public long fillRefBitmap3084 = 3084;
+    public long fillRefBitmap3085 = 3085;
+    public long fillRefBitmap3086 = 3086;
+    public long fillRefBitmap3087 = 3087;
+    public long fillRefBitmap3088 = 3088;
+    public long fillRefBitmap3089 = 3089;
+    public long fillRefBitmap3090 = 3090;
+    public long fillRefBitmap3091 = 3091;
+    public long fillRefBitmap3092 = 3092;
+    public long fillRefBitmap3093 = 3093;
+    public long fillRefBitmap3094 = 3094;
+    public long fillRefBitmap3095 = 3095;
+    public long fillRefBitmap3096 = 3096;
+    public long fillRefBitmap3097 = 3097;
+    public long fillRefBitmap3098 = 3098;
+    public long fillRefBitmap3099 = 3099;
+    public long fillRefBitmap3100 = 3100;
+    public long fillRefBitmap3101 = 3101;
+    public long fillRefBitmap3102 = 3102;
+    public long fillRefBitmap3103 = 3103;
+    public long fillRefBitmap3104 = 3104;
+    public long fillRefBitmap3105 = 3105;
+    public long fillRefBitmap3106 = 3106;
+    public long fillRefBitmap3107 = 3107;
+    public long fillRefBitmap3108 = 3108;
+    public long fillRefBitmap3109 = 3109;
+    public long fillRefBitmap3110 = 3110;
+    public long fillRefBitmap3111 = 3111;
+    public long fillRefBitmap3112 = 3112;
+    public long fillRefBitmap3113 = 3113;
+    public long fillRefBitmap3114 = 3114;
+    public long fillRefBitmap3115 = 3115;
+    public long fillRefBitmap3116 = 3116;
+    public long fillRefBitmap3117 = 3117;
+    public long fillRefBitmap3118 = 3118;
+    public long fillRefBitmap3119 = 3119;
+    public long fillRefBitmap3120 = 3120;
+    public long fillRefBitmap3121 = 3121;
+    public long fillRefBitmap3122 = 3122;
+    public long fillRefBitmap3123 = 3123;
+    public long fillRefBitmap3124 = 3124;
+    public long fillRefBitmap3125 = 3125;
+    public long fillRefBitmap3126 = 3126;
+    public long fillRefBitmap3127 = 3127;
+    public long fillRefBitmap3128 = 3128;
+    public long fillRefBitmap3129 = 3129;
+    public long fillRefBitmap3130 = 3130;
+    public long fillRefBitmap3131 = 3131;
+    public long fillRefBitmap3132 = 3132;
+    public long fillRefBitmap3133 = 3133;
+    public long fillRefBitmap3134 = 3134;
+    public long fillRefBitmap3135 = 3135;
+    public long fillRefBitmap3136 = 3136;
+    public long fillRefBitmap3137 = 3137;
+    public long fillRefBitmap3138 = 3138;
+    public long fillRefBitmap3139 = 3139;
+    public long fillRefBitmap3140 = 3140;
+    public long fillRefBitmap3141 = 3141;
+    public long fillRefBitmap3142 = 3142;
+    public long fillRefBitmap3143 = 3143;
+    public long fillRefBitmap3144 = 3144;
+    public long fillRefBitmap3145 = 3145;
+    public long fillRefBitmap3146 = 3146;
+    public long fillRefBitmap3147 = 3147;
+    public long fillRefBitmap3148 = 3148;
+    public long fillRefBitmap3149 = 3149;
+    public long fillRefBitmap3150 = 3150;
+    public long fillRefBitmap3151 = 3151;
+    public long fillRefBitmap3152 = 3152;
+    public long fillRefBitmap3153 = 3153;
+    public long fillRefBitmap3154 = 3154;
+    public long fillRefBitmap3155 = 3155;
+    public long fillRefBitmap3156 = 3156;
+    public long fillRefBitmap3157 = 3157;
+    public long fillRefBitmap3158 = 3158;
+    public long fillRefBitmap3159 = 3159;
+    public long fillRefBitmap3160 = 3160;
+    public long fillRefBitmap3161 = 3161;
+    public long fillRefBitmap3162 = 3162;
+    public long fillRefBitmap3163 = 3163;
+    public long fillRefBitmap3164 = 3164;
+    public long fillRefBitmap3165 = 3165;
+    public long fillRefBitmap3166 = 3166;
+    public long fillRefBitmap3167 = 3167;
+    public long fillRefBitmap3168 = 3168;
+    public long fillRefBitmap3169 = 3169;
+    public long fillRefBitmap3170 = 3170;
+    public long fillRefBitmap3171 = 3171;
+    public long fillRefBitmap3172 = 3172;
+    public long fillRefBitmap3173 = 3173;
+    public long fillRefBitmap3174 = 3174;
+    public long fillRefBitmap3175 = 3175;
+    public long fillRefBitmap3176 = 3176;
+    public long fillRefBitmap3177 = 3177;
+    public long fillRefBitmap3178 = 3178;
+    public long fillRefBitmap3179 = 3179;
+    public long fillRefBitmap3180 = 3180;
+    public long fillRefBitmap3181 = 3181;
+    public long fillRefBitmap3182 = 3182;
+    public long fillRefBitmap3183 = 3183;
+    public long fillRefBitmap3184 = 3184;
+    public long fillRefBitmap3185 = 3185;
+    public long fillRefBitmap3186 = 3186;
+    public long fillRefBitmap3187 = 3187;
+    public long fillRefBitmap3188 = 3188;
+    public long fillRefBitmap3189 = 3189;
+    public long fillRefBitmap3190 = 3190;
+    public long fillRefBitmap3191 = 3191;
+    public long fillRefBitmap3192 = 3192;
+    public long fillRefBitmap3193 = 3193;
+    public long fillRefBitmap3194 = 3194;
+    public long fillRefBitmap3195 = 3195;
+    public long fillRefBitmap3196 = 3196;
+    public long fillRefBitmap3197 = 3197;
+    public long fillRefBitmap3198 = 3198;
+    public long fillRefBitmap3199 = 3199;
+    public long fillRefBitmap3200 = 3200;
+    public long fillRefBitmap3201 = 3201;
+    public long fillRefBitmap3202 = 3202;
+    public long fillRefBitmap3203 = 3203;
+    public long fillRefBitmap3204 = 3204;
+    public long fillRefBitmap3205 = 3205;
+    public long fillRefBitmap3206 = 3206;
+    public long fillRefBitmap3207 = 3207;
+    public long fillRefBitmap3208 = 3208;
+    public long fillRefBitmap3209 = 3209;
+    public long fillRefBitmap3210 = 3210;
+    public long fillRefBitmap3211 = 3211;
+    public long fillRefBitmap3212 = 3212;
+    public long fillRefBitmap3213 = 3213;
+    public long fillRefBitmap3214 = 3214;
+    public long fillRefBitmap3215 = 3215;
+    public long fillRefBitmap3216 = 3216;
+    public long fillRefBitmap3217 = 3217;
+    public long fillRefBitmap3218 = 3218;
+    public long fillRefBitmap3219 = 3219;
+    public long fillRefBitmap3220 = 3220;
+    public long fillRefBitmap3221 = 3221;
+    public long fillRefBitmap3222 = 3222;
+    public long fillRefBitmap3223 = 3223;
+    public long fillRefBitmap3224 = 3224;
+    public long fillRefBitmap3225 = 3225;
+    public long fillRefBitmap3226 = 3226;
+    public long fillRefBitmap3227 = 3227;
+    public long fillRefBitmap3228 = 3228;
+    public long fillRefBitmap3229 = 3229;
+    public long fillRefBitmap3230 = 3230;
+    public long fillRefBitmap3231 = 3231;
+    public long fillRefBitmap3232 = 3232;
+    public long fillRefBitmap3233 = 3233;
+    public long fillRefBitmap3234 = 3234;
+    public long fillRefBitmap3235 = 3235;
+    public long fillRefBitmap3236 = 3236;
+    public long fillRefBitmap3237 = 3237;
+    public long fillRefBitmap3238 = 3238;
+    public long fillRefBitmap3239 = 3239;
+    public long fillRefBitmap3240 = 3240;
+    public long fillRefBitmap3241 = 3241;
+    public long fillRefBitmap3242 = 3242;
+    public long fillRefBitmap3243 = 3243;
+    public long fillRefBitmap3244 = 3244;
+    public long fillRefBitmap3245 = 3245;
+    public long fillRefBitmap3246 = 3246;
+    public long fillRefBitmap3247 = 3247;
+    public long fillRefBitmap3248 = 3248;
+    public long fillRefBitmap3249 = 3249;
+    public long fillRefBitmap3250 = 3250;
+    public long fillRefBitmap3251 = 3251;
+    public long fillRefBitmap3252 = 3252;
+    public long fillRefBitmap3253 = 3253;
+    public long fillRefBitmap3254 = 3254;
+    public long fillRefBitmap3255 = 3255;
+    public long fillRefBitmap3256 = 3256;
+    public long fillRefBitmap3257 = 3257;
+    public long fillRefBitmap3258 = 3258;
+    public long fillRefBitmap3259 = 3259;
+    public long fillRefBitmap3260 = 3260;
+    public long fillRefBitmap3261 = 3261;
+    public long fillRefBitmap3262 = 3262;
+    public long fillRefBitmap3263 = 3263;
+    public long fillRefBitmap3264 = 3264;
+    public long fillRefBitmap3265 = 3265;
+    public long fillRefBitmap3266 = 3266;
+    public long fillRefBitmap3267 = 3267;
+    public long fillRefBitmap3268 = 3268;
+    public long fillRefBitmap3269 = 3269;
+    public long fillRefBitmap3270 = 3270;
+    public long fillRefBitmap3271 = 3271;
+    public long fillRefBitmap3272 = 3272;
+    public long fillRefBitmap3273 = 3273;
+    public long fillRefBitmap3274 = 3274;
+    public long fillRefBitmap3275 = 3275;
+    public long fillRefBitmap3276 = 3276;
+    public long fillRefBitmap3277 = 3277;
+    public long fillRefBitmap3278 = 3278;
+    public long fillRefBitmap3279 = 3279;
+    public long fillRefBitmap3280 = 3280;
+    public long fillRefBitmap3281 = 3281;
+    public long fillRefBitmap3282 = 3282;
+    public long fillRefBitmap3283 = 3283;
+    public long fillRefBitmap3284 = 3284;
+    public long fillRefBitmap3285 = 3285;
+    public long fillRefBitmap3286 = 3286;
+    public long fillRefBitmap3287 = 3287;
+    public long fillRefBitmap3288 = 3288;
+    public long fillRefBitmap3289 = 3289;
+    public long fillRefBitmap3290 = 3290;
+    public long fillRefBitmap3291 = 3291;
+    public long fillRefBitmap3292 = 3292;
+    public long fillRefBitmap3293 = 3293;
+    public long fillRefBitmap3294 = 3294;
+    public long fillRefBitmap3295 = 3295;
+    public long fillRefBitmap3296 = 3296;
+    public long fillRefBitmap3297 = 3297;
+    public long fillRefBitmap3298 = 3298;
+    public long fillRefBitmap3299 = 3299;
+    public long fillRefBitmap3300 = 3300;
+    public long fillRefBitmap3301 = 3301;
+    public long fillRefBitmap3302 = 3302;
+    public long fillRefBitmap3303 = 3303;
+    public long fillRefBitmap3304 = 3304;
+    public long fillRefBitmap3305 = 3305;
+    public long fillRefBitmap3306 = 3306;
+    public long fillRefBitmap3307 = 3307;
+    public long fillRefBitmap3308 = 3308;
+    public long fillRefBitmap3309 = 3309;
+    public long fillRefBitmap3310 = 3310;
+    public long fillRefBitmap3311 = 3311;
+    public long fillRefBitmap3312 = 3312;
+    public long fillRefBitmap3313 = 3313;
+    public long fillRefBitmap3314 = 3314;
+    public long fillRefBitmap3315 = 3315;
+    public long fillRefBitmap3316 = 3316;
+    public long fillRefBitmap3317 = 3317;
+    public long fillRefBitmap3318 = 3318;
+    public long fillRefBitmap3319 = 3319;
+    public long fillRefBitmap3320 = 3320;
+    public long fillRefBitmap3321 = 3321;
+    public long fillRefBitmap3322 = 3322;
+    public long fillRefBitmap3323 = 3323;
+    public long fillRefBitmap3324 = 3324;
+    public long fillRefBitmap3325 = 3325;
+    public long fillRefBitmap3326 = 3326;
+    public long fillRefBitmap3327 = 3327;
+    public long fillRefBitmap3328 = 3328;
+    public long fillRefBitmap3329 = 3329;
+    public long fillRefBitmap3330 = 3330;
+    public long fillRefBitmap3331 = 3331;
+    public long fillRefBitmap3332 = 3332;
+    public long fillRefBitmap3333 = 3333;
+    public long fillRefBitmap3334 = 3334;
+    public long fillRefBitmap3335 = 3335;
+    public long fillRefBitmap3336 = 3336;
+    public long fillRefBitmap3337 = 3337;
+    public long fillRefBitmap3338 = 3338;
+    public long fillRefBitmap3339 = 3339;
+    public long fillRefBitmap3340 = 3340;
+    public long fillRefBitmap3341 = 3341;
+    public long fillRefBitmap3342 = 3342;
+    public long fillRefBitmap3343 = 3343;
+    public long fillRefBitmap3344 = 3344;
+    public long fillRefBitmap3345 = 3345;
+    public long fillRefBitmap3346 = 3346;
+    public long fillRefBitmap3347 = 3347;
+    public long fillRefBitmap3348 = 3348;
+    public long fillRefBitmap3349 = 3349;
+    public long fillRefBitmap3350 = 3350;
+    public long fillRefBitmap3351 = 3351;
+    public long fillRefBitmap3352 = 3352;
+    public long fillRefBitmap3353 = 3353;
+    public long fillRefBitmap3354 = 3354;
+    public long fillRefBitmap3355 = 3355;
+    public long fillRefBitmap3356 = 3356;
+    public long fillRefBitmap3357 = 3357;
+    public long fillRefBitmap3358 = 3358;
+    public long fillRefBitmap3359 = 3359;
+    public long fillRefBitmap3360 = 3360;
+    public long fillRefBitmap3361 = 3361;
+    public long fillRefBitmap3362 = 3362;
+    public long fillRefBitmap3363 = 3363;
+    public long fillRefBitmap3364 = 3364;
+    public long fillRefBitmap3365 = 3365;
+    public long fillRefBitmap3366 = 3366;
+    public long fillRefBitmap3367 = 3367;
+    public long fillRefBitmap3368 = 3368;
+    public long fillRefBitmap3369 = 3369;
+    public long fillRefBitmap3370 = 3370;
+    public long fillRefBitmap3371 = 3371;
+    public long fillRefBitmap3372 = 3372;
+    public long fillRefBitmap3373 = 3373;
+    public long fillRefBitmap3374 = 3374;
+    public long fillRefBitmap3375 = 3375;
+    public long fillRefBitmap3376 = 3376;
+    public long fillRefBitmap3377 = 3377;
+    public long fillRefBitmap3378 = 3378;
+    public long fillRefBitmap3379 = 3379;
+    public long fillRefBitmap3380 = 3380;
+    public long fillRefBitmap3381 = 3381;
+    public long fillRefBitmap3382 = 3382;
+    public long fillRefBitmap3383 = 3383;
+    public long fillRefBitmap3384 = 3384;
+    public long fillRefBitmap3385 = 3385;
+    public long fillRefBitmap3386 = 3386;
+    public long fillRefBitmap3387 = 3387;
+    public long fillRefBitmap3388 = 3388;
+    public long fillRefBitmap3389 = 3389;
+    public long fillRefBitmap3390 = 3390;
+    public long fillRefBitmap3391 = 3391;
+    public long fillRefBitmap3392 = 3392;
+    public long fillRefBitmap3393 = 3393;
+    public long fillRefBitmap3394 = 3394;
+    public long fillRefBitmap3395 = 3395;
+    public long fillRefBitmap3396 = 3396;
+    public long fillRefBitmap3397 = 3397;
+    public long fillRefBitmap3398 = 3398;
+    public long fillRefBitmap3399 = 3399;
+    public long fillRefBitmap3400 = 3400;
+    public long fillRefBitmap3401 = 3401;
+    public long fillRefBitmap3402 = 3402;
+    public long fillRefBitmap3403 = 3403;
+    public long fillRefBitmap3404 = 3404;
+    public long fillRefBitmap3405 = 3405;
+    public long fillRefBitmap3406 = 3406;
+    public long fillRefBitmap3407 = 3407;
+    public long fillRefBitmap3408 = 3408;
+    public long fillRefBitmap3409 = 3409;
+    public long fillRefBitmap3410 = 3410;
+    public long fillRefBitmap3411 = 3411;
+    public long fillRefBitmap3412 = 3412;
+    public long fillRefBitmap3413 = 3413;
+    public long fillRefBitmap3414 = 3414;
+    public long fillRefBitmap3415 = 3415;
+    public long fillRefBitmap3416 = 3416;
+    public long fillRefBitmap3417 = 3417;
+    public long fillRefBitmap3418 = 3418;
+    public long fillRefBitmap3419 = 3419;
+    public long fillRefBitmap3420 = 3420;
+    public long fillRefBitmap3421 = 3421;
+    public long fillRefBitmap3422 = 3422;
+    public long fillRefBitmap3423 = 3423;
+    public long fillRefBitmap3424 = 3424;
+    public long fillRefBitmap3425 = 3425;
+    public long fillRefBitmap3426 = 3426;
+    public long fillRefBitmap3427 = 3427;
+    public long fillRefBitmap3428 = 3428;
+    public long fillRefBitmap3429 = 3429;
+    public long fillRefBitmap3430 = 3430;
+    public long fillRefBitmap3431 = 3431;
+    public long fillRefBitmap3432 = 3432;
+    public long fillRefBitmap3433 = 3433;
+    public long fillRefBitmap3434 = 3434;
+    public long fillRefBitmap3435 = 3435;
+    public long fillRefBitmap3436 = 3436;
+    public long fillRefBitmap3437 = 3437;
+    public long fillRefBitmap3438 = 3438;
+    public long fillRefBitmap3439 = 3439;
+    public long fillRefBitmap3440 = 3440;
+    public long fillRefBitmap3441 = 3441;
+    public long fillRefBitmap3442 = 3442;
+    public long fillRefBitmap3443 = 3443;
+    public long fillRefBitmap3444 = 3444;
+    public long fillRefBitmap3445 = 3445;
+    public long fillRefBitmap3446 = 3446;
+    public long fillRefBitmap3447 = 3447;
+    public long fillRefBitmap3448 = 3448;
+    public long fillRefBitmap3449 = 3449;
+    public long fillRefBitmap3450 = 3450;
+    public long fillRefBitmap3451 = 3451;
+    public long fillRefBitmap3452 = 3452;
+    public long fillRefBitmap3453 = 3453;
+    public long fillRefBitmap3454 = 3454;
+    public long fillRefBitmap3455 = 3455;
+    public long fillRefBitmap3456 = 3456;
+    public long fillRefBitmap3457 = 3457;
+    public long fillRefBitmap3458 = 3458;
+    public long fillRefBitmap3459 = 3459;
+    public long fillRefBitmap3460 = 3460;
+    public long fillRefBitmap3461 = 3461;
+    public long fillRefBitmap3462 = 3462;
+    public long fillRefBitmap3463 = 3463;
+    public long fillRefBitmap3464 = 3464;
+    public long fillRefBitmap3465 = 3465;
+    public long fillRefBitmap3466 = 3466;
+    public long fillRefBitmap3467 = 3467;
+    public long fillRefBitmap3468 = 3468;
+    public long fillRefBitmap3469 = 3469;
+    public long fillRefBitmap3470 = 3470;
+    public long fillRefBitmap3471 = 3471;
+    public long fillRefBitmap3472 = 3472;
+    public long fillRefBitmap3473 = 3473;
+    public long fillRefBitmap3474 = 3474;
+    public long fillRefBitmap3475 = 3475;
+    public long fillRefBitmap3476 = 3476;
+    public long fillRefBitmap3477 = 3477;
+    public long fillRefBitmap3478 = 3478;
+    public long fillRefBitmap3479 = 3479;
+    public long fillRefBitmap3480 = 3480;
+    public long fillRefBitmap3481 = 3481;
+    public long fillRefBitmap3482 = 3482;
+    public long fillRefBitmap3483 = 3483;
+    public long fillRefBitmap3484 = 3484;
+    public long fillRefBitmap3485 = 3485;
+    public long fillRefBitmap3486 = 3486;
+    public long fillRefBitmap3487 = 3487;
+    public long fillRefBitmap3488 = 3488;
+    public long fillRefBitmap3489 = 3489;
+    public long fillRefBitmap3490 = 3490;
+    public long fillRefBitmap3491 = 3491;
+    public long fillRefBitmap3492 = 3492;
+    public long fillRefBitmap3493 = 3493;
+    public long fillRefBitmap3494 = 3494;
+    public long fillRefBitmap3495 = 3495;
+    public long fillRefBitmap3496 = 3496;
+    public long fillRefBitmap3497 = 3497;
+    public long fillRefBitmap3498 = 3498;
+    public long fillRefBitmap3499 = 3499;
+    public long fillRefBitmap3500 = 3500;
+    public long fillRefBitmap3501 = 3501;
+    public long fillRefBitmap3502 = 3502;
+    public long fillRefBitmap3503 = 3503;
+    public long fillRefBitmap3504 = 3504;
+    public long fillRefBitmap3505 = 3505;
+    public long fillRefBitmap3506 = 3506;
+    public long fillRefBitmap3507 = 3507;
+    public long fillRefBitmap3508 = 3508;
+    public long fillRefBitmap3509 = 3509;
+    public long fillRefBitmap3510 = 3510;
+    public long fillRefBitmap3511 = 3511;
+    public long fillRefBitmap3512 = 3512;
+    public long fillRefBitmap3513 = 3513;
+    public long fillRefBitmap3514 = 3514;
+    public long fillRefBitmap3515 = 3515;
+    public long fillRefBitmap3516 = 3516;
+    public long fillRefBitmap3517 = 3517;
+    public long fillRefBitmap3518 = 3518;
+    public long fillRefBitmap3519 = 3519;
+    public long fillRefBitmap3520 = 3520;
+    public long fillRefBitmap3521 = 3521;
+    public long fillRefBitmap3522 = 3522;
+    public long fillRefBitmap3523 = 3523;
+    public long fillRefBitmap3524 = 3524;
+    public long fillRefBitmap3525 = 3525;
+    public long fillRefBitmap3526 = 3526;
+    public long fillRefBitmap3527 = 3527;
+    public long fillRefBitmap3528 = 3528;
+    public long fillRefBitmap3529 = 3529;
+    public long fillRefBitmap3530 = 3530;
+    public long fillRefBitmap3531 = 3531;
+    public long fillRefBitmap3532 = 3532;
+    public long fillRefBitmap3533 = 3533;
+    public long fillRefBitmap3534 = 3534;
+    public long fillRefBitmap3535 = 3535;
+    public long fillRefBitmap3536 = 3536;
+    public long fillRefBitmap3537 = 3537;
+    public long fillRefBitmap3538 = 3538;
+    public long fillRefBitmap3539 = 3539;
+    public long fillRefBitmap3540 = 3540;
+    public long fillRefBitmap3541 = 3541;
+    public long fillRefBitmap3542 = 3542;
+    public long fillRefBitmap3543 = 3543;
+    public long fillRefBitmap3544 = 3544;
+    public long fillRefBitmap3545 = 3545;
+    public long fillRefBitmap3546 = 3546;
+    public long fillRefBitmap3547 = 3547;
+    public long fillRefBitmap3548 = 3548;
+    public long fillRefBitmap3549 = 3549;
+    public long fillRefBitmap3550 = 3550;
+    public long fillRefBitmap3551 = 3551;
+    public long fillRefBitmap3552 = 3552;
+    public long fillRefBitmap3553 = 3553;
+    public long fillRefBitmap3554 = 3554;
+    public long fillRefBitmap3555 = 3555;
+    public long fillRefBitmap3556 = 3556;
+    public long fillRefBitmap3557 = 3557;
+    public long fillRefBitmap3558 = 3558;
+    public long fillRefBitmap3559 = 3559;
+    public long fillRefBitmap3560 = 3560;
+    public long fillRefBitmap3561 = 3561;
+    public long fillRefBitmap3562 = 3562;
+    public long fillRefBitmap3563 = 3563;
+    public long fillRefBitmap3564 = 3564;
+    public long fillRefBitmap3565 = 3565;
+    public long fillRefBitmap3566 = 3566;
+    public long fillRefBitmap3567 = 3567;
+    public long fillRefBitmap3568 = 3568;
+    public long fillRefBitmap3569 = 3569;
+    public long fillRefBitmap3570 = 3570;
+    public long fillRefBitmap3571 = 3571;
+    public long fillRefBitmap3572 = 3572;
+    public long fillRefBitmap3573 = 3573;
+    public long fillRefBitmap3574 = 3574;
+    public long fillRefBitmap3575 = 3575;
+    public long fillRefBitmap3576 = 3576;
+    public long fillRefBitmap3577 = 3577;
+    public long fillRefBitmap3578 = 3578;
+    public long fillRefBitmap3579 = 3579;
+    public long fillRefBitmap3580 = 3580;
+    public long fillRefBitmap3581 = 3581;
+    public long fillRefBitmap3582 = 3582;
+    public long fillRefBitmap3583 = 3583;
+    public long fillRefBitmap3584 = 3584;
+    public long fillRefBitmap3585 = 3585;
+    public long fillRefBitmap3586 = 3586;
+    public long fillRefBitmap3587 = 3587;
+    public long fillRefBitmap3588 = 3588;
+    public long fillRefBitmap3589 = 3589;
+    public long fillRefBitmap3590 = 3590;
+    public long fillRefBitmap3591 = 3591;
+    public long fillRefBitmap3592 = 3592;
+    public long fillRefBitmap3593 = 3593;
+    public long fillRefBitmap3594 = 3594;
+    public long fillRefBitmap3595 = 3595;
+    public long fillRefBitmap3596 = 3596;
+    public long fillRefBitmap3597 = 3597;
+    public long fillRefBitmap3598 = 3598;
+    public long fillRefBitmap3599 = 3599;
+    public long fillRefBitmap3600 = 3600;
+    public long fillRefBitmap3601 = 3601;
+    public long fillRefBitmap3602 = 3602;
+    public long fillRefBitmap3603 = 3603;
+    public long fillRefBitmap3604 = 3604;
+    public long fillRefBitmap3605 = 3605;
+    public long fillRefBitmap3606 = 3606;
+    public long fillRefBitmap3607 = 3607;
+    public long fillRefBitmap3608 = 3608;
+    public long fillRefBitmap3609 = 3609;
+    public long fillRefBitmap3610 = 3610;
+    public long fillRefBitmap3611 = 3611;
+    public long fillRefBitmap3612 = 3612;
+    public long fillRefBitmap3613 = 3613;
+    public long fillRefBitmap3614 = 3614;
+    public long fillRefBitmap3615 = 3615;
+    public long fillRefBitmap3616 = 3616;
+    public long fillRefBitmap3617 = 3617;
+    public long fillRefBitmap3618 = 3618;
+    public long fillRefBitmap3619 = 3619;
+    public long fillRefBitmap3620 = 3620;
+    public long fillRefBitmap3621 = 3621;
+    public long fillRefBitmap3622 = 3622;
+    public long fillRefBitmap3623 = 3623;
+    public long fillRefBitmap3624 = 3624;
+    public long fillRefBitmap3625 = 3625;
+    public long fillRefBitmap3626 = 3626;
+    public long fillRefBitmap3627 = 3627;
+    public long fillRefBitmap3628 = 3628;
+    public long fillRefBitmap3629 = 3629;
+    public long fillRefBitmap3630 = 3630;
+    public long fillRefBitmap3631 = 3631;
+    public long fillRefBitmap3632 = 3632;
+    public long fillRefBitmap3633 = 3633;
+    public long fillRefBitmap3634 = 3634;
+    public long fillRefBitmap3635 = 3635;
+    public long fillRefBitmap3636 = 3636;
+    public long fillRefBitmap3637 = 3637;
+    public long fillRefBitmap3638 = 3638;
+    public long fillRefBitmap3639 = 3639;
+    public long fillRefBitmap3640 = 3640;
+    public long fillRefBitmap3641 = 3641;
+    public long fillRefBitmap3642 = 3642;
+    public long fillRefBitmap3643 = 3643;
+    public long fillRefBitmap3644 = 3644;
+    public long fillRefBitmap3645 = 3645;
+    public long fillRefBitmap3646 = 3646;
+    public long fillRefBitmap3647 = 3647;
+    public long fillRefBitmap3648 = 3648;
+    public long fillRefBitmap3649 = 3649;
+    public long fillRefBitmap3650 = 3650;
+    public long fillRefBitmap3651 = 3651;
+    public long fillRefBitmap3652 = 3652;
+    public long fillRefBitmap3653 = 3653;
+    public long fillRefBitmap3654 = 3654;
+    public long fillRefBitmap3655 = 3655;
+    public long fillRefBitmap3656 = 3656;
+    public long fillRefBitmap3657 = 3657;
+    public long fillRefBitmap3658 = 3658;
+    public long fillRefBitmap3659 = 3659;
+    public long fillRefBitmap3660 = 3660;
+    public long fillRefBitmap3661 = 3661;
+    public long fillRefBitmap3662 = 3662;
+    public long fillRefBitmap3663 = 3663;
+    public long fillRefBitmap3664 = 3664;
+    public long fillRefBitmap3665 = 3665;
+    public long fillRefBitmap3666 = 3666;
+    public long fillRefBitmap3667 = 3667;
+    public long fillRefBitmap3668 = 3668;
+    public long fillRefBitmap3669 = 3669;
+    public long fillRefBitmap3670 = 3670;
+    public long fillRefBitmap3671 = 3671;
+    public long fillRefBitmap3672 = 3672;
+    public long fillRefBitmap3673 = 3673;
+    public long fillRefBitmap3674 = 3674;
+    public long fillRefBitmap3675 = 3675;
+    public long fillRefBitmap3676 = 3676;
+    public long fillRefBitmap3677 = 3677;
+    public long fillRefBitmap3678 = 3678;
+    public long fillRefBitmap3679 = 3679;
+    public long fillRefBitmap3680 = 3680;
+    public long fillRefBitmap3681 = 3681;
+    public long fillRefBitmap3682 = 3682;
+    public long fillRefBitmap3683 = 3683;
+    public long fillRefBitmap3684 = 3684;
+    public long fillRefBitmap3685 = 3685;
+    public long fillRefBitmap3686 = 3686;
+    public long fillRefBitmap3687 = 3687;
+    public long fillRefBitmap3688 = 3688;
+    public long fillRefBitmap3689 = 3689;
+    public long fillRefBitmap3690 = 3690;
+    public long fillRefBitmap3691 = 3691;
+    public long fillRefBitmap3692 = 3692;
+    public long fillRefBitmap3693 = 3693;
+    public long fillRefBitmap3694 = 3694;
+    public long fillRefBitmap3695 = 3695;
+    public long fillRefBitmap3696 = 3696;
+    public long fillRefBitmap3697 = 3697;
+    public long fillRefBitmap3698 = 3698;
+    public long fillRefBitmap3699 = 3699;
+    public long fillRefBitmap3700 = 3700;
+    public long fillRefBitmap3701 = 3701;
+    public long fillRefBitmap3702 = 3702;
+    public long fillRefBitmap3703 = 3703;
+    public long fillRefBitmap3704 = 3704;
+    public long fillRefBitmap3705 = 3705;
+    public long fillRefBitmap3706 = 3706;
+    public long fillRefBitmap3707 = 3707;
+    public long fillRefBitmap3708 = 3708;
+    public long fillRefBitmap3709 = 3709;
+    public long fillRefBitmap3710 = 3710;
+    public long fillRefBitmap3711 = 3711;
+    public long fillRefBitmap3712 = 3712;
+    public long fillRefBitmap3713 = 3713;
+    public long fillRefBitmap3714 = 3714;
+    public long fillRefBitmap3715 = 3715;
+    public long fillRefBitmap3716 = 3716;
+    public long fillRefBitmap3717 = 3717;
+    public long fillRefBitmap3718 = 3718;
+    public long fillRefBitmap3719 = 3719;
+    public long fillRefBitmap3720 = 3720;
+    public long fillRefBitmap3721 = 3721;
+    public long fillRefBitmap3722 = 3722;
+    public long fillRefBitmap3723 = 3723;
+    public long fillRefBitmap3724 = 3724;
+    public long fillRefBitmap3725 = 3725;
+    public long fillRefBitmap3726 = 3726;
+    public long fillRefBitmap3727 = 3727;
+    public long fillRefBitmap3728 = 3728;
+    public long fillRefBitmap3729 = 3729;
+    public long fillRefBitmap3730 = 3730;
+    public long fillRefBitmap3731 = 3731;
+    public long fillRefBitmap3732 = 3732;
+    public long fillRefBitmap3733 = 3733;
+    public long fillRefBitmap3734 = 3734;
+    public long fillRefBitmap3735 = 3735;
+    public long fillRefBitmap3736 = 3736;
+    public long fillRefBitmap3737 = 3737;
+    public long fillRefBitmap3738 = 3738;
+    public long fillRefBitmap3739 = 3739;
+    public long fillRefBitmap3740 = 3740;
+    public long fillRefBitmap3741 = 3741;
+    public long fillRefBitmap3742 = 3742;
+    public long fillRefBitmap3743 = 3743;
+    public long fillRefBitmap3744 = 3744;
+    public long fillRefBitmap3745 = 3745;
+    public long fillRefBitmap3746 = 3746;
+    public long fillRefBitmap3747 = 3747;
+    public long fillRefBitmap3748 = 3748;
+    public long fillRefBitmap3749 = 3749;
+    public long fillRefBitmap3750 = 3750;
+    public long fillRefBitmap3751 = 3751;
+    public long fillRefBitmap3752 = 3752;
+    public long fillRefBitmap3753 = 3753;
+    public long fillRefBitmap3754 = 3754;
+    public long fillRefBitmap3755 = 3755;
+    public long fillRefBitmap3756 = 3756;
+    public long fillRefBitmap3757 = 3757;
+    public long fillRefBitmap3758 = 3758;
+    public long fillRefBitmap3759 = 3759;
+    public long fillRefBitmap3760 = 3760;
+    public long fillRefBitmap3761 = 3761;
+    public long fillRefBitmap3762 = 3762;
+    public long fillRefBitmap3763 = 3763;
+    public long fillRefBitmap3764 = 3764;
+    public long fillRefBitmap3765 = 3765;
+    public long fillRefBitmap3766 = 3766;
+    public long fillRefBitmap3767 = 3767;
+    public long fillRefBitmap3768 = 3768;
+    public long fillRefBitmap3769 = 3769;
+    public long fillRefBitmap3770 = 3770;
+    public long fillRefBitmap3771 = 3771;
+    public long fillRefBitmap3772 = 3772;
+    public long fillRefBitmap3773 = 3773;
+    public long fillRefBitmap3774 = 3774;
+    public long fillRefBitmap3775 = 3775;
+    public long fillRefBitmap3776 = 3776;
+    public long fillRefBitmap3777 = 3777;
+    public long fillRefBitmap3778 = 3778;
+    public long fillRefBitmap3779 = 3779;
+    public long fillRefBitmap3780 = 3780;
+    public long fillRefBitmap3781 = 3781;
+    public long fillRefBitmap3782 = 3782;
+    public long fillRefBitmap3783 = 3783;
+    public long fillRefBitmap3784 = 3784;
+    public long fillRefBitmap3785 = 3785;
+    public long fillRefBitmap3786 = 3786;
+    public long fillRefBitmap3787 = 3787;
+    public long fillRefBitmap3788 = 3788;
+    public long fillRefBitmap3789 = 3789;
+    public long fillRefBitmap3790 = 3790;
+    public long fillRefBitmap3791 = 3791;
+    public long fillRefBitmap3792 = 3792;
+    public long fillRefBitmap3793 = 3793;
+    public long fillRefBitmap3794 = 3794;
+    public long fillRefBitmap3795 = 3795;
+    public long fillRefBitmap3796 = 3796;
+    public long fillRefBitmap3797 = 3797;
+    public long fillRefBitmap3798 = 3798;
+    public long fillRefBitmap3799 = 3799;
+    public long fillRefBitmap3800 = 3800;
+    public long fillRefBitmap3801 = 3801;
+    public long fillRefBitmap3802 = 3802;
+    public long fillRefBitmap3803 = 3803;
+    public long fillRefBitmap3804 = 3804;
+    public long fillRefBitmap3805 = 3805;
+    public long fillRefBitmap3806 = 3806;
+    public long fillRefBitmap3807 = 3807;
+    public long fillRefBitmap3808 = 3808;
+    public long fillRefBitmap3809 = 3809;
+    public long fillRefBitmap3810 = 3810;
+    public long fillRefBitmap3811 = 3811;
+    public long fillRefBitmap3812 = 3812;
+    public long fillRefBitmap3813 = 3813;
+    public long fillRefBitmap3814 = 3814;
+    public long fillRefBitmap3815 = 3815;
+    public long fillRefBitmap3816 = 3816;
+    public long fillRefBitmap3817 = 3817;
+    public long fillRefBitmap3818 = 3818;
+    public long fillRefBitmap3819 = 3819;
+    public long fillRefBitmap3820 = 3820;
+    public long fillRefBitmap3821 = 3821;
+    public long fillRefBitmap3822 = 3822;
+    public long fillRefBitmap3823 = 3823;
+    public long fillRefBitmap3824 = 3824;
+    public long fillRefBitmap3825 = 3825;
+    public long fillRefBitmap3826 = 3826;
+    public long fillRefBitmap3827 = 3827;
+    public long fillRefBitmap3828 = 3828;
+    public long fillRefBitmap3829 = 3829;
+    public long fillRefBitmap3830 = 3830;
+    public long fillRefBitmap3831 = 3831;
+    public long fillRefBitmap3832 = 3832;
+    public long fillRefBitmap3833 = 3833;
+    public long fillRefBitmap3834 = 3834;
+    public long fillRefBitmap3835 = 3835;
+    public long fillRefBitmap3836 = 3836;
+    public long fillRefBitmap3837 = 3837;
+    public long fillRefBitmap3838 = 3838;
+    public long fillRefBitmap3839 = 3839;
+    public long fillRefBitmap3840 = 3840;
+    public long fillRefBitmap3841 = 3841;
+    public long fillRefBitmap3842 = 3842;
+    public long fillRefBitmap3843 = 3843;
+    public long fillRefBitmap3844 = 3844;
+    public long fillRefBitmap3845 = 3845;
+    public long fillRefBitmap3846 = 3846;
+    public long fillRefBitmap3847 = 3847;
+    public long fillRefBitmap3848 = 3848;
+    public long fillRefBitmap3849 = 3849;
+    public long fillRefBitmap3850 = 3850;
+    public long fillRefBitmap3851 = 3851;
+    public long fillRefBitmap3852 = 3852;
+    public long fillRefBitmap3853 = 3853;
+    public long fillRefBitmap3854 = 3854;
+    public long fillRefBitmap3855 = 3855;
+    public long fillRefBitmap3856 = 3856;
+    public long fillRefBitmap3857 = 3857;
+    public long fillRefBitmap3858 = 3858;
+    public long fillRefBitmap3859 = 3859;
+    public long fillRefBitmap3860 = 3860;
+    public long fillRefBitmap3861 = 3861;
+    public long fillRefBitmap3862 = 3862;
+    public long fillRefBitmap3863 = 3863;
+    public long fillRefBitmap3864 = 3864;
+    public long fillRefBitmap3865 = 3865;
+    public long fillRefBitmap3866 = 3866;
+    public long fillRefBitmap3867 = 3867;
+    public long fillRefBitmap3868 = 3868;
+    public long fillRefBitmap3869 = 3869;
+    public long fillRefBitmap3870 = 3870;
+    public long fillRefBitmap3871 = 3871;
+    public long fillRefBitmap3872 = 3872;
+    public long fillRefBitmap3873 = 3873;
+    public long fillRefBitmap3874 = 3874;
+    public long fillRefBitmap3875 = 3875;
+    public long fillRefBitmap3876 = 3876;
+    public long fillRefBitmap3877 = 3877;
+    public long fillRefBitmap3878 = 3878;
+    public long fillRefBitmap3879 = 3879;
+    public long fillRefBitmap3880 = 3880;
+    public long fillRefBitmap3881 = 3881;
+    public long fillRefBitmap3882 = 3882;
+    public long fillRefBitmap3883 = 3883;
+    public long fillRefBitmap3884 = 3884;
+    public long fillRefBitmap3885 = 3885;
+    public long fillRefBitmap3886 = 3886;
+    public long fillRefBitmap3887 = 3887;
+    public long fillRefBitmap3888 = 3888;
+    public long fillRefBitmap3889 = 3889;
+    public long fillRefBitmap3890 = 3890;
+    public long fillRefBitmap3891 = 3891;
+    public long fillRefBitmap3892 = 3892;
+    public long fillRefBitmap3893 = 3893;
+    public long fillRefBitmap3894 = 3894;
+    public long fillRefBitmap3895 = 3895;
+    public long fillRefBitmap3896 = 3896;
+    public long fillRefBitmap3897 = 3897;
+    public long fillRefBitmap3898 = 3898;
+    public long fillRefBitmap3899 = 3899;
+    public long fillRefBitmap3900 = 3900;
+    public long fillRefBitmap3901 = 3901;
+    public long fillRefBitmap3902 = 3902;
+    public long fillRefBitmap3903 = 3903;
+    public long fillRefBitmap3904 = 3904;
+    public long fillRefBitmap3905 = 3905;
+    public long fillRefBitmap3906 = 3906;
+    public long fillRefBitmap3907 = 3907;
+    public long fillRefBitmap3908 = 3908;
+    public long fillRefBitmap3909 = 3909;
+    public long fillRefBitmap3910 = 3910;
+    public long fillRefBitmap3911 = 3911;
+    public long fillRefBitmap3912 = 3912;
+    public long fillRefBitmap3913 = 3913;
+    public long fillRefBitmap3914 = 3914;
+    public long fillRefBitmap3915 = 3915;
+    public long fillRefBitmap3916 = 3916;
+    public long fillRefBitmap3917 = 3917;
+    public long fillRefBitmap3918 = 3918;
+    public long fillRefBitmap3919 = 3919;
+    public long fillRefBitmap3920 = 3920;
+    public long fillRefBitmap3921 = 3921;
+    public long fillRefBitmap3922 = 3922;
+    public long fillRefBitmap3923 = 3923;
+    public long fillRefBitmap3924 = 3924;
+    public long fillRefBitmap3925 = 3925;
+    public long fillRefBitmap3926 = 3926;
+    public long fillRefBitmap3927 = 3927;
+    public long fillRefBitmap3928 = 3928;
+    public long fillRefBitmap3929 = 3929;
+    public long fillRefBitmap3930 = 3930;
+    public long fillRefBitmap3931 = 3931;
+    public long fillRefBitmap3932 = 3932;
+    public long fillRefBitmap3933 = 3933;
+    public long fillRefBitmap3934 = 3934;
+    public long fillRefBitmap3935 = 3935;
+    public long fillRefBitmap3936 = 3936;
+    public long fillRefBitmap3937 = 3937;
+    public long fillRefBitmap3938 = 3938;
+    public long fillRefBitmap3939 = 3939;
+    public long fillRefBitmap3940 = 3940;
+    public long fillRefBitmap3941 = 3941;
+    public long fillRefBitmap3942 = 3942;
+    public long fillRefBitmap3943 = 3943;
+    public long fillRefBitmap3944 = 3944;
+    public long fillRefBitmap3945 = 3945;
+    public long fillRefBitmap3946 = 3946;
+    public long fillRefBitmap3947 = 3947;
+    public long fillRefBitmap3948 = 3948;
+    public long fillRefBitmap3949 = 3949;
+    public long fillRefBitmap3950 = 3950;
+    public long fillRefBitmap3951 = 3951;
+    public long fillRefBitmap3952 = 3952;
+    public long fillRefBitmap3953 = 3953;
+    public long fillRefBitmap3954 = 3954;
+    public long fillRefBitmap3955 = 3955;
+    public long fillRefBitmap3956 = 3956;
+    public long fillRefBitmap3957 = 3957;
+    public long fillRefBitmap3958 = 3958;
+    public long fillRefBitmap3959 = 3959;
+    public long fillRefBitmap3960 = 3960;
+    public long fillRefBitmap3961 = 3961;
+    public long fillRefBitmap3962 = 3962;
+    public long fillRefBitmap3963 = 3963;
+    public long fillRefBitmap3964 = 3964;
+    public long fillRefBitmap3965 = 3965;
+    public long fillRefBitmap3966 = 3966;
+    public long fillRefBitmap3967 = 3967;
+    public long fillRefBitmap3968 = 3968;
+    public long fillRefBitmap3969 = 3969;
+    public long fillRefBitmap3970 = 3970;
+    public long fillRefBitmap3971 = 3971;
+    public long fillRefBitmap3972 = 3972;
+    public long fillRefBitmap3973 = 3973;
+    public long fillRefBitmap3974 = 3974;
+    public long fillRefBitmap3975 = 3975;
+    public long fillRefBitmap3976 = 3976;
+    public long fillRefBitmap3977 = 3977;
+    public long fillRefBitmap3978 = 3978;
+    public long fillRefBitmap3979 = 3979;
+    public long fillRefBitmap3980 = 3980;
+    public long fillRefBitmap3981 = 3981;
+    public long fillRefBitmap3982 = 3982;
+    public long fillRefBitmap3983 = 3983;
+    public long fillRefBitmap3984 = 3984;
+    public long fillRefBitmap3985 = 3985;
+    public long fillRefBitmap3986 = 3986;
+    public long fillRefBitmap3987 = 3987;
+    public long fillRefBitmap3988 = 3988;
+    public long fillRefBitmap3989 = 3989;
+    public long fillRefBitmap3990 = 3990;
+    public long fillRefBitmap3991 = 3991;
+    public long fillRefBitmap3992 = 3992;
+    public long fillRefBitmap3993 = 3993;
+    public long fillRefBitmap3994 = 3994;
+    public long fillRefBitmap3995 = 3995;
+    public long fillRefBitmap3996 = 3996;
+    public long fillRefBitmap3997 = 3997;
+    public long fillRefBitmap3998 = 3998;
+    public long fillRefBitmap3999 = 3999;
+    public long fillRefBitmap4000 = 4000;
+    public long fillRefBitmap4001 = 4001;
+    public long fillRefBitmap4002 = 4002;
+    public long fillRefBitmap4003 = 4003;
+    public long fillRefBitmap4004 = 4004;
+    public long fillRefBitmap4005 = 4005;
+    public long fillRefBitmap4006 = 4006;
+    public long fillRefBitmap4007 = 4007;
+    public long fillRefBitmap4008 = 4008;
+    public long fillRefBitmap4009 = 4009;
+    public long fillRefBitmap4010 = 4010;
+    public long fillRefBitmap4011 = 4011;
+    public long fillRefBitmap4012 = 4012;
+    public long fillRefBitmap4013 = 4013;
+    public long fillRefBitmap4014 = 4014;
+    public long fillRefBitmap4015 = 4015;
+    public long fillRefBitmap4016 = 4016;
+    public long fillRefBitmap4017 = 4017;
+    public long fillRefBitmap4018 = 4018;
+    public long fillRefBitmap4019 = 4019;
+    public long fillRefBitmap4020 = 4020;
+    public long fillRefBitmap4021 = 4021;
+    public long fillRefBitmap4022 = 4022;
+    public long fillRefBitmap4023 = 4023;
+    public long fillRefBitmap4024 = 4024;
+    public long fillRefBitmap4025 = 4025;
+    public long fillRefBitmap4026 = 4026;
+    public long fillRefBitmap4027 = 4027;
+    public long fillRefBitmap4028 = 4028;
+    public long fillRefBitmap4029 = 4029;
+    public long fillRefBitmap4030 = 4030;
+    public long fillRefBitmap4031 = 4031;
+    public long fillRefBitmap4032 = 4032;
+    public long fillRefBitmap4033 = 4033;
+    public long fillRefBitmap4034 = 4034;
+    public long fillRefBitmap4035 = 4035;
+    public long fillRefBitmap4036 = 4036;
+    public long fillRefBitmap4037 = 4037;
+    public long fillRefBitmap4038 = 4038;
+    public long fillRefBitmap4039 = 4039;
+    public long fillRefBitmap4040 = 4040;
+    public long fillRefBitmap4041 = 4041;
+    public long fillRefBitmap4042 = 4042;
+    public long fillRefBitmap4043 = 4043;
+    public long fillRefBitmap4044 = 4044;
+    public long fillRefBitmap4045 = 4045;
+    public long fillRefBitmap4046 = 4046;
+    public long fillRefBitmap4047 = 4047;
+    public long fillRefBitmap4048 = 4048;
+    public long fillRefBitmap4049 = 4049;
+    public long fillRefBitmap4050 = 4050;
+    public long fillRefBitmap4051 = 4051;
+    public long fillRefBitmap4052 = 4052;
+    public long fillRefBitmap4053 = 4053;
+    public long fillRefBitmap4054 = 4054;
+    public long fillRefBitmap4055 = 4055;
+    public long fillRefBitmap4056 = 4056;
+    public long fillRefBitmap4057 = 4057;
+    public long fillRefBitmap4058 = 4058;
+    public long fillRefBitmap4059 = 4059;
+    public long fillRefBitmap4060 = 4060;
+    public long fillRefBitmap4061 = 4061;
+    public long fillRefBitmap4062 = 4062;
+    public long fillRefBitmap4063 = 4063;
+    public long fillRefBitmap4064 = 4064;
+    public long fillRefBitmap4065 = 4065;
+    public long fillRefBitmap4066 = 4066;
+    public long fillRefBitmap4067 = 4067;
+    public long fillRefBitmap4068 = 4068;
+    public long fillRefBitmap4069 = 4069;
+    public long fillRefBitmap4070 = 4070;
+    public long fillRefBitmap4071 = 4071;
+    public long fillRefBitmap4072 = 4072;
+    public long fillRefBitmap4073 = 4073;
+    public long fillRefBitmap4074 = 4074;
+    public long fillRefBitmap4075 = 4075;
+    public long fillRefBitmap4076 = 4076;
+    public long fillRefBitmap4077 = 4077;
+    public long fillRefBitmap4078 = 4078;
+    public long fillRefBitmap4079 = 4079;
+    public long fillRefBitmap4080 = 4080;
+    public long fillRefBitmap4081 = 4081;
+    public long fillRefBitmap4082 = 4082;
+    public long fillRefBitmap4083 = 4083;
+    public long fillRefBitmap4084 = 4084;
+    public long fillRefBitmap4085 = 4085;
+    public long fillRefBitmap4086 = 4086;
+    public long fillRefBitmap4087 = 4087;
+    public long fillRefBitmap4088 = 4088;
+    public long fillRefBitmap4089 = 4089;
+    public long fillRefBitmap4090 = 4090;
+    public long fillRefBitmap4091 = 4091;
+    public long fillRefBitmap4092 = 4092;
+    public long fillRefBitmap4093 = 4093;
+    public long fillRefBitmap4094 = 4094;
+    public long fillRefBitmap4095 = 4095;
+    public long fillRefBitmap4096 = 4096;
+    public long fillRefBitmap4097 = 4097;
+    public long fillRefBitmap4098 = 4098;
+    public long fillRefBitmap4099 = 4099;
+    public long fillRefBitmap4100 = 4100;
+    public long fillRefBitmap4101 = 4101;
+    public long fillRefBitmap4102 = 4102;
+    public long fillRefBitmap4103 = 4103;
+    public long fillRefBitmap4104 = 4104;
+    public long fillRefBitmap4105 = 4105;
+    public long fillRefBitmap4106 = 4106;
+    public long fillRefBitmap4107 = 4107;
+    public long fillRefBitmap4108 = 4108;
+    public long fillRefBitmap4109 = 4109;
+    public long fillRefBitmap4110 = 4110;
+    public long fillRefBitmap4111 = 4111;
+    public long fillRefBitmap4112 = 4112;
+    public long fillRefBitmap4113 = 4113;
+    public long fillRefBitmap4114 = 4114;
+    public long fillRefBitmap4115 = 4115;
+    public long fillRefBitmap4116 = 4116;
+    public long fillRefBitmap4117 = 4117;
+    public long fillRefBitmap4118 = 4118;
+    public long fillRefBitmap4119 = 4119;
+    public long fillRefBitmap4120 = 4120;
+    public long fillRefBitmap4121 = 4121;
+    public long fillRefBitmap4122 = 4122;
+    public long fillRefBitmap4123 = 4123;
+    public long fillRefBitmap4124 = 4124;
+    public long fillRefBitmap4125 = 4125;
+    public long fillRefBitmap4126 = 4126;
+    public long fillRefBitmap4127 = 4127;
+    public long fillRefBitmap4128 = 4128;
+    public long fillRefBitmap4129 = 4129;
+    public long fillRefBitmap4130 = 4130;
+    public long fillRefBitmap4131 = 4131;
+    public long fillRefBitmap4132 = 4132;
+    public long fillRefBitmap4133 = 4133;
+    public long fillRefBitmap4134 = 4134;
+    public long fillRefBitmap4135 = 4135;
+    public long fillRefBitmap4136 = 4136;
+    public long fillRefBitmap4137 = 4137;
+    public long fillRefBitmap4138 = 4138;
+    public long fillRefBitmap4139 = 4139;
+    public long fillRefBitmap4140 = 4140;
+    public long fillRefBitmap4141 = 4141;
+    public long fillRefBitmap4142 = 4142;
+    public long fillRefBitmap4143 = 4143;
+    public long fillRefBitmap4144 = 4144;
+    public long fillRefBitmap4145 = 4145;
+    public long fillRefBitmap4146 = 4146;
+    public long fillRefBitmap4147 = 4147;
+    public long fillRefBitmap4148 = 4148;
+    public long fillRefBitmap4149 = 4149;
+    public long fillRefBitmap4150 = 4150;
+    public long fillRefBitmap4151 = 4151;
+    public long fillRefBitmap4152 = 4152;
+    public long fillRefBitmap4153 = 4153;
+    public long fillRefBitmap4154 = 4154;
+    public long fillRefBitmap4155 = 4155;
+    public long fillRefBitmap4156 = 4156;
+    public long fillRefBitmap4157 = 4157;
+    public long fillRefBitmap4158 = 4158;
+    public long fillRefBitmap4159 = 4159;
+    public long fillRefBitmap4160 = 4160;
+    public long fillRefBitmap4161 = 4161;
+    public long fillRefBitmap4162 = 4162;
+    public long fillRefBitmap4163 = 4163;
+    public long fillRefBitmap4164 = 4164;
+    public long fillRefBitmap4165 = 4165;
+    public long fillRefBitmap4166 = 4166;
+    public long fillRefBitmap4167 = 4167;
+    public long fillRefBitmap4168 = 4168;
+    public long fillRefBitmap4169 = 4169;
+    public long fillRefBitmap4170 = 4170;
+    public long fillRefBitmap4171 = 4171;
+    public long fillRefBitmap4172 = 4172;
+    public long fillRefBitmap4173 = 4173;
+    public long fillRefBitmap4174 = 4174;
+    public long fillRefBitmap4175 = 4175;
+    public long fillRefBitmap4176 = 4176;
+    public long fillRefBitmap4177 = 4177;
+    public long fillRefBitmap4178 = 4178;
+    public long fillRefBitmap4179 = 4179;
+    public long fillRefBitmap4180 = 4180;
+    public long fillRefBitmap4181 = 4181;
+    public long fillRefBitmap4182 = 4182;
+    public long fillRefBitmap4183 = 4183;
+    public long fillRefBitmap4184 = 4184;
+    public long fillRefBitmap4185 = 4185;
+    public long fillRefBitmap4186 = 4186;
+    public long fillRefBitmap4187 = 4187;
+    public long fillRefBitmap4188 = 4188;
+    public long fillRefBitmap4189 = 4189;
+    public long fillRefBitmap4190 = 4190;
+    public long fillRefBitmap4191 = 4191;
+    public long fillRefBitmap4192 = 4192;
+    public long fillRefBitmap4193 = 4193;
+    public long fillRefBitmap4194 = 4194;
+    public long fillRefBitmap4195 = 4195;
+    public long fillRefBitmap4196 = 4196;
+    public long fillRefBitmap4197 = 4197;
+    public long fillRefBitmap4198 = 4198;
+    public long fillRefBitmap4199 = 4199;
+    public long fillRefBitmap4200 = 4200;
+    public long fillRefBitmap4201 = 4201;
+    public long fillRefBitmap4202 = 4202;
+    public long fillRefBitmap4203 = 4203;
+    public long fillRefBitmap4204 = 4204;
+    public long fillRefBitmap4205 = 4205;
+    public long fillRefBitmap4206 = 4206;
+    public long fillRefBitmap4207 = 4207;
+    public long fillRefBitmap4208 = 4208;
+    public long fillRefBitmap4209 = 4209;
+    public long fillRefBitmap4210 = 4210;
+    public long fillRefBitmap4211 = 4211;
+    public long fillRefBitmap4212 = 4212;
+    public long fillRefBitmap4213 = 4213;
+    public long fillRefBitmap4214 = 4214;
+    public long fillRefBitmap4215 = 4215;
+    public long fillRefBitmap4216 = 4216;
+    public long fillRefBitmap4217 = 4217;
+    public long fillRefBitmap4218 = 4218;
+    public long fillRefBitmap4219 = 4219;
+    public long fillRefBitmap4220 = 4220;
+    public long fillRefBitmap4221 = 4221;
+    public long fillRefBitmap4222 = 4222;
+    public long fillRefBitmap4223 = 4223;
+    public long fillRefBitmap4224 = 4224;
+    public long fillRefBitmap4225 = 4225;
+    public long fillRefBitmap4226 = 4226;
+    public long fillRefBitmap4227 = 4227;
+    public long fillRefBitmap4228 = 4228;
+    public long fillRefBitmap4229 = 4229;
+    public long fillRefBitmap4230 = 4230;
+    public long fillRefBitmap4231 = 4231;
+    public long fillRefBitmap4232 = 4232;
+    public long fillRefBitmap4233 = 4233;
+    public long fillRefBitmap4234 = 4234;
+    public long fillRefBitmap4235 = 4235;
+    public long fillRefBitmap4236 = 4236;
+    public long fillRefBitmap4237 = 4237;
+    public long fillRefBitmap4238 = 4238;
+    public long fillRefBitmap4239 = 4239;
+    public long fillRefBitmap4240 = 4240;
+    public long fillRefBitmap4241 = 4241;
+    public long fillRefBitmap4242 = 4242;
+    public long fillRefBitmap4243 = 4243;
+    public long fillRefBitmap4244 = 4244;
+    public long fillRefBitmap4245 = 4245;
+    public long fillRefBitmap4246 = 4246;
+    public long fillRefBitmap4247 = 4247;
+    public long fillRefBitmap4248 = 4248;
+    public long fillRefBitmap4249 = 4249;
+    public long fillRefBitmap4250 = 4250;
+    public long fillRefBitmap4251 = 4251;
+    public long fillRefBitmap4252 = 4252;
+    public long fillRefBitmap4253 = 4253;
+    public long fillRefBitmap4254 = 4254;
+    public long fillRefBitmap4255 = 4255;
+    public long fillRefBitmap4256 = 4256;
+    public long fillRefBitmap4257 = 4257;
+    public long fillRefBitmap4258 = 4258;
+    public long fillRefBitmap4259 = 4259;
+    public long fillRefBitmap4260 = 4260;
+    public long fillRefBitmap4261 = 4261;
+    public long fillRefBitmap4262 = 4262;
+    public long fillRefBitmap4263 = 4263;
+    public long fillRefBitmap4264 = 4264;
+    public long fillRefBitmap4265 = 4265;
+    public long fillRefBitmap4266 = 4266;
+    public long fillRefBitmap4267 = 4267;
+    public long fillRefBitmap4268 = 4268;
+    public long fillRefBitmap4269 = 4269;
+    public long fillRefBitmap4270 = 4270;
+    public long fillRefBitmap4271 = 4271;
+    public long fillRefBitmap4272 = 4272;
+    public long fillRefBitmap4273 = 4273;
+    public long fillRefBitmap4274 = 4274;
+    public long fillRefBitmap4275 = 4275;
+    public long fillRefBitmap4276 = 4276;
+    public long fillRefBitmap4277 = 4277;
+    public long fillRefBitmap4278 = 4278;
+    public long fillRefBitmap4279 = 4279;
+    public long fillRefBitmap4280 = 4280;
+    public long fillRefBitmap4281 = 4281;
+    public long fillRefBitmap4282 = 4282;
+    public long fillRefBitmap4283 = 4283;
+    public long fillRefBitmap4284 = 4284;
+    public long fillRefBitmap4285 = 4285;
+    public long fillRefBitmap4286 = 4286;
+    public long fillRefBitmap4287 = 4287;
+    public long fillRefBitmap4288 = 4288;
+    public long fillRefBitmap4289 = 4289;
+    public long fillRefBitmap4290 = 4290;
+    public long fillRefBitmap4291 = 4291;
+    public long fillRefBitmap4292 = 4292;
+    public long fillRefBitmap4293 = 4293;
+    public long fillRefBitmap4294 = 4294;
+    public long fillRefBitmap4295 = 4295;
+    public long fillRefBitmap4296 = 4296;
+    public long fillRefBitmap4297 = 4297;
+    public long fillRefBitmap4298 = 4298;
+    public long fillRefBitmap4299 = 4299;
+    public long fillRefBitmap4300 = 4300;
+    public long fillRefBitmap4301 = 4301;
+    public long fillRefBitmap4302 = 4302;
+    public long fillRefBitmap4303 = 4303;
+    public long fillRefBitmap4304 = 4304;
+    public long fillRefBitmap4305 = 4305;
+    public long fillRefBitmap4306 = 4306;
+    public long fillRefBitmap4307 = 4307;
+    public long fillRefBitmap4308 = 4308;
+    public long fillRefBitmap4309 = 4309;
+    public long fillRefBitmap4310 = 4310;
+    public long fillRefBitmap4311 = 4311;
+    public long fillRefBitmap4312 = 4312;
+    public long fillRefBitmap4313 = 4313;
+    public long fillRefBitmap4314 = 4314;
+    public long fillRefBitmap4315 = 4315;
+    public long fillRefBitmap4316 = 4316;
+    public long fillRefBitmap4317 = 4317;
+    public long fillRefBitmap4318 = 4318;
+    public long fillRefBitmap4319 = 4319;
+    public long fillRefBitmap4320 = 4320;
+    public long fillRefBitmap4321 = 4321;
+    public long fillRefBitmap4322 = 4322;
+    public long fillRefBitmap4323 = 4323;
+    public long fillRefBitmap4324 = 4324;
+    public long fillRefBitmap4325 = 4325;
+    public long fillRefBitmap4326 = 4326;
+    public long fillRefBitmap4327 = 4327;
+    public long fillRefBitmap4328 = 4328;
+    public long fillRefBitmap4329 = 4329;
+    public long fillRefBitmap4330 = 4330;
+    public long fillRefBitmap4331 = 4331;
+    public long fillRefBitmap4332 = 4332;
+    public long fillRefBitmap4333 = 4333;
+    public long fillRefBitmap4334 = 4334;
+    public long fillRefBitmap4335 = 4335;
+    public long fillRefBitmap4336 = 4336;
+    public long fillRefBitmap4337 = 4337;
+    public long fillRefBitmap4338 = 4338;
+    public long fillRefBitmap4339 = 4339;
+    public long fillRefBitmap4340 = 4340;
+    public long fillRefBitmap4341 = 4341;
+    public long fillRefBitmap4342 = 4342;
+    public long fillRefBitmap4343 = 4343;
+    public long fillRefBitmap4344 = 4344;
+    public long fillRefBitmap4345 = 4345;
+    public long fillRefBitmap4346 = 4346;
+    public long fillRefBitmap4347 = 4347;
+    public long fillRefBitmap4348 = 4348;
+    public long fillRefBitmap4349 = 4349;
+    public long fillRefBitmap4350 = 4350;
+    public long fillRefBitmap4351 = 4351;
+    public long fillRefBitmap4352 = 4352;
+    public long fillRefBitmap4353 = 4353;
+    public long fillRefBitmap4354 = 4354;
+    public long fillRefBitmap4355 = 4355;
+    public long fillRefBitmap4356 = 4356;
+    public long fillRefBitmap4357 = 4357;
+    public long fillRefBitmap4358 = 4358;
+    public long fillRefBitmap4359 = 4359;
+    public long fillRefBitmap4360 = 4360;
+    public long fillRefBitmap4361 = 4361;
+    public long fillRefBitmap4362 = 4362;
+    public long fillRefBitmap4363 = 4363;
+    public long fillRefBitmap4364 = 4364;
+    public long fillRefBitmap4365 = 4365;
+    public long fillRefBitmap4366 = 4366;
+    public long fillRefBitmap4367 = 4367;
+    public long fillRefBitmap4368 = 4368;
+    public long fillRefBitmap4369 = 4369;
+    public long fillRefBitmap4370 = 4370;
+    public long fillRefBitmap4371 = 4371;
+    public long fillRefBitmap4372 = 4372;
+    public long fillRefBitmap4373 = 4373;
+    public long fillRefBitmap4374 = 4374;
+    public long fillRefBitmap4375 = 4375;
+    public long fillRefBitmap4376 = 4376;
+    public long fillRefBitmap4377 = 4377;
+    public long fillRefBitmap4378 = 4378;
+    public long fillRefBitmap4379 = 4379;
+    public long fillRefBitmap4380 = 4380;
+    public long fillRefBitmap4381 = 4381;
+    public long fillRefBitmap4382 = 4382;
+    public long fillRefBitmap4383 = 4383;
+    public long fillRefBitmap4384 = 4384;
+    public long fillRefBitmap4385 = 4385;
+    public long fillRefBitmap4386 = 4386;
+    public long fillRefBitmap4387 = 4387;
+    public long fillRefBitmap4388 = 4388;
+    public long fillRefBitmap4389 = 4389;
+    public long fillRefBitmap4390 = 4390;
+    public long fillRefBitmap4391 = 4391;
+    public long fillRefBitmap4392 = 4392;
+    public long fillRefBitmap4393 = 4393;
+    public long fillRefBitmap4394 = 4394;
+    public long fillRefBitmap4395 = 4395;
+    public long fillRefBitmap4396 = 4396;
+    public long fillRefBitmap4397 = 4397;
+    public long fillRefBitmap4398 = 4398;
+    public long fillRefBitmap4399 = 4399;
+    public long fillRefBitmap4400 = 4400;
+    public long fillRefBitmap4401 = 4401;
+    public long fillRefBitmap4402 = 4402;
+    public long fillRefBitmap4403 = 4403;
+    public long fillRefBitmap4404 = 4404;
+    public long fillRefBitmap4405 = 4405;
+    public long fillRefBitmap4406 = 4406;
+    public long fillRefBitmap4407 = 4407;
+    public long fillRefBitmap4408 = 4408;
+    public long fillRefBitmap4409 = 4409;
+    public long fillRefBitmap4410 = 4410;
+    public long fillRefBitmap4411 = 4411;
+    public long fillRefBitmap4412 = 4412;
+    public long fillRefBitmap4413 = 4413;
+    public long fillRefBitmap4414 = 4414;
+    public long fillRefBitmap4415 = 4415;
+    public long fillRefBitmap4416 = 4416;
+    public long fillRefBitmap4417 = 4417;
+    public long fillRefBitmap4418 = 4418;
+    public long fillRefBitmap4419 = 4419;
+    public long fillRefBitmap4420 = 4420;
+    public long fillRefBitmap4421 = 4421;
+    public long fillRefBitmap4422 = 4422;
+    public long fillRefBitmap4423 = 4423;
+    public long fillRefBitmap4424 = 4424;
+    public long fillRefBitmap4425 = 4425;
+    public long fillRefBitmap4426 = 4426;
+    public long fillRefBitmap4427 = 4427;
+    public long fillRefBitmap4428 = 4428;
+    public long fillRefBitmap4429 = 4429;
+    public long fillRefBitmap4430 = 4430;
+    public long fillRefBitmap4431 = 4431;
+    public long fillRefBitmap4432 = 4432;
+    public long fillRefBitmap4433 = 4433;
+    public long fillRefBitmap4434 = 4434;
+    public long fillRefBitmap4435 = 4435;
+    public long fillRefBitmap4436 = 4436;
+    public long fillRefBitmap4437 = 4437;
+    public long fillRefBitmap4438 = 4438;
+    public long fillRefBitmap4439 = 4439;
+    public long fillRefBitmap4440 = 4440;
+    public long fillRefBitmap4441 = 4441;
+    public long fillRefBitmap4442 = 4442;
+    public long fillRefBitmap4443 = 4443;
+    public long fillRefBitmap4444 = 4444;
+    public long fillRefBitmap4445 = 4445;
+    public long fillRefBitmap4446 = 4446;
+    public long fillRefBitmap4447 = 4447;
+    public long fillRefBitmap4448 = 4448;
+    public long fillRefBitmap4449 = 4449;
+    public long fillRefBitmap4450 = 4450;
+    public long fillRefBitmap4451 = 4451;
+    public long fillRefBitmap4452 = 4452;
+    public long fillRefBitmap4453 = 4453;
+    public long fillRefBitmap4454 = 4454;
+    public long fillRefBitmap4455 = 4455;
+    public long fillRefBitmap4456 = 4456;
+    public long fillRefBitmap4457 = 4457;
+    public long fillRefBitmap4458 = 4458;
+    public long fillRefBitmap4459 = 4459;
+    public long fillRefBitmap4460 = 4460;
+    public long fillRefBitmap4461 = 4461;
+    public long fillRefBitmap4462 = 4462;
+    public long fillRefBitmap4463 = 4463;
+    public long fillRefBitmap4464 = 4464;
+    public long fillRefBitmap4465 = 4465;
+    public long fillRefBitmap4466 = 4466;
+    public long fillRefBitmap4467 = 4467;
+    public long fillRefBitmap4468 = 4468;
+    public long fillRefBitmap4469 = 4469;
+    public long fillRefBitmap4470 = 4470;
+    public long fillRefBitmap4471 = 4471;
+    public long fillRefBitmap4472 = 4472;
+    public long fillRefBitmap4473 = 4473;
+    public long fillRefBitmap4474 = 4474;
+    public long fillRefBitmap4475 = 4475;
+    public long fillRefBitmap4476 = 4476;
+    public long fillRefBitmap4477 = 4477;
+    public long fillRefBitmap4478 = 4478;
+    public long fillRefBitmap4479 = 4479;
+    public long fillRefBitmap4480 = 4480;
+    public long fillRefBitmap4481 = 4481;
+    public long fillRefBitmap4482 = 4482;
+    public long fillRefBitmap4483 = 4483;
+    public long fillRefBitmap4484 = 4484;
+    public long fillRefBitmap4485 = 4485;
+    public long fillRefBitmap4486 = 4486;
+    public long fillRefBitmap4487 = 4487;
+    public long fillRefBitmap4488 = 4488;
+    public long fillRefBitmap4489 = 4489;
+    public long fillRefBitmap4490 = 4490;
+    public long fillRefBitmap4491 = 4491;
+    public long fillRefBitmap4492 = 4492;
+    public long fillRefBitmap4493 = 4493;
+    public long fillRefBitmap4494 = 4494;
+    public long fillRefBitmap4495 = 4495;
+    public long fillRefBitmap4496 = 4496;
+    public long fillRefBitmap4497 = 4497;
+    public long fillRefBitmap4498 = 4498;
+    public long fillRefBitmap4499 = 4499;
+    public long fillRefBitmap4500 = 4500;
+    public long fillRefBitmap4501 = 4501;
+    public long fillRefBitmap4502 = 4502;
+    public long fillRefBitmap4503 = 4503;
+    public long fillRefBitmap4504 = 4504;
+    public long fillRefBitmap4505 = 4505;
+    public long fillRefBitmap4506 = 4506;
+    public long fillRefBitmap4507 = 4507;
+    public long fillRefBitmap4508 = 4508;
+    public long fillRefBitmap4509 = 4509;
+    public long fillRefBitmap4510 = 4510;
+    public long fillRefBitmap4511 = 4511;
+    public long fillRefBitmap4512 = 4512;
+    public long fillRefBitmap4513 = 4513;
+    public long fillRefBitmap4514 = 4514;
+    public long fillRefBitmap4515 = 4515;
+    public long fillRefBitmap4516 = 4516;
+    public long fillRefBitmap4517 = 4517;
+    public long fillRefBitmap4518 = 4518;
+    public long fillRefBitmap4519 = 4519;
+    public long fillRefBitmap4520 = 4520;
+    public long fillRefBitmap4521 = 4521;
+    public long fillRefBitmap4522 = 4522;
+    public long fillRefBitmap4523 = 4523;
+    public long fillRefBitmap4524 = 4524;
+    public long fillRefBitmap4525 = 4525;
+    public long fillRefBitmap4526 = 4526;
+    public long fillRefBitmap4527 = 4527;
+    public long fillRefBitmap4528 = 4528;
+    public long fillRefBitmap4529 = 4529;
+    public long fillRefBitmap4530 = 4530;
+    public long fillRefBitmap4531 = 4531;
+    public long fillRefBitmap4532 = 4532;
+    public long fillRefBitmap4533 = 4533;
+    public long fillRefBitmap4534 = 4534;
+    public long fillRefBitmap4535 = 4535;
+    public long fillRefBitmap4536 = 4536;
+    public long fillRefBitmap4537 = 4537;
+    public long fillRefBitmap4538 = 4538;
+    public long fillRefBitmap4539 = 4539;
+    public long fillRefBitmap4540 = 4540;
+    public long fillRefBitmap4541 = 4541;
+    public long fillRefBitmap4542 = 4542;
+    public long fillRefBitmap4543 = 4543;
+    public long fillRefBitmap4544 = 4544;
+    public long fillRefBitmap4545 = 4545;
+    public long fillRefBitmap4546 = 4546;
+    public long fillRefBitmap4547 = 4547;
+    public long fillRefBitmap4548 = 4548;
+    public long fillRefBitmap4549 = 4549;
+    public long fillRefBitmap4550 = 4550;
+    public long fillRefBitmap4551 = 4551;
+    public long fillRefBitmap4552 = 4552;
+    public long fillRefBitmap4553 = 4553;
+    public long fillRefBitmap4554 = 4554;
+    public long fillRefBitmap4555 = 4555;
+    public long fillRefBitmap4556 = 4556;
+    public long fillRefBitmap4557 = 4557;
+    public long fillRefBitmap4558 = 4558;
+    public long fillRefBitmap4559 = 4559;
+    public long fillRefBitmap4560 = 4560;
+    public long fillRefBitmap4561 = 4561;
+    public long fillRefBitmap4562 = 4562;
+    public long fillRefBitmap4563 = 4563;
+    public long fillRefBitmap4564 = 4564;
+    public long fillRefBitmap4565 = 4565;
+    public long fillRefBitmap4566 = 4566;
+    public long fillRefBitmap4567 = 4567;
+    public long fillRefBitmap4568 = 4568;
+    public long fillRefBitmap4569 = 4569;
+    public long fillRefBitmap4570 = 4570;
+    public long fillRefBitmap4571 = 4571;
+    public long fillRefBitmap4572 = 4572;
+    public long fillRefBitmap4573 = 4573;
+    public long fillRefBitmap4574 = 4574;
+    public long fillRefBitmap4575 = 4575;
+    public long fillRefBitmap4576 = 4576;
+    public long fillRefBitmap4577 = 4577;
+    public long fillRefBitmap4578 = 4578;
+    public long fillRefBitmap4579 = 4579;
+    public long fillRefBitmap4580 = 4580;
+    public long fillRefBitmap4581 = 4581;
+    public long fillRefBitmap4582 = 4582;
+    public long fillRefBitmap4583 = 4583;
+    public long fillRefBitmap4584 = 4584;
+    public long fillRefBitmap4585 = 4585;
+    public long fillRefBitmap4586 = 4586;
+    public long fillRefBitmap4587 = 4587;
+    public long fillRefBitmap4588 = 4588;
+    public long fillRefBitmap4589 = 4589;
+    public long fillRefBitmap4590 = 4590;
+    public long fillRefBitmap4591 = 4591;
+    public long fillRefBitmap4592 = 4592;
+    public long fillRefBitmap4593 = 4593;
+    public long fillRefBitmap4594 = 4594;
+    public long fillRefBitmap4595 = 4595;
+    public long fillRefBitmap4596 = 4596;
+    public long fillRefBitmap4597 = 4597;
+    public long fillRefBitmap4598 = 4598;
+    public long fillRefBitmap4599 = 4599;
+    public long fillRefBitmap4600 = 4600;
+    public long fillRefBitmap4601 = 4601;
+    public long fillRefBitmap4602 = 4602;
+    public long fillRefBitmap4603 = 4603;
+    public long fillRefBitmap4604 = 4604;
+    public long fillRefBitmap4605 = 4605;
+    public long fillRefBitmap4606 = 4606;
+    public long fillRefBitmap4607 = 4607;
+    public long fillRefBitmap4608 = 4608;
+    public long fillRefBitmap4609 = 4609;
+    public long fillRefBitmap4610 = 4610;
+    public long fillRefBitmap4611 = 4611;
+    public long fillRefBitmap4612 = 4612;
+    public long fillRefBitmap4613 = 4613;
+    public long fillRefBitmap4614 = 4614;
+    public long fillRefBitmap4615 = 4615;
+    public long fillRefBitmap4616 = 4616;
+    public long fillRefBitmap4617 = 4617;
+    public long fillRefBitmap4618 = 4618;
+    public long fillRefBitmap4619 = 4619;
+    public long fillRefBitmap4620 = 4620;
+    public long fillRefBitmap4621 = 4621;
+    public long fillRefBitmap4622 = 4622;
+    public long fillRefBitmap4623 = 4623;
+    public long fillRefBitmap4624 = 4624;
+    public long fillRefBitmap4625 = 4625;
+    public long fillRefBitmap4626 = 4626;
+    public long fillRefBitmap4627 = 4627;
+    public long fillRefBitmap4628 = 4628;
+    public long fillRefBitmap4629 = 4629;
+    public long fillRefBitmap4630 = 4630;
+    public long fillRefBitmap4631 = 4631;
+    public long fillRefBitmap4632 = 4632;
+    public long fillRefBitmap4633 = 4633;
+    public long fillRefBitmap4634 = 4634;
+    public long fillRefBitmap4635 = 4635;
+    public long fillRefBitmap4636 = 4636;
+    public long fillRefBitmap4637 = 4637;
+    public long fillRefBitmap4638 = 4638;
+    public long fillRefBitmap4639 = 4639;
+    public long fillRefBitmap4640 = 4640;
+    public long fillRefBitmap4641 = 4641;
+    public long fillRefBitmap4642 = 4642;
+    public long fillRefBitmap4643 = 4643;
+    public long fillRefBitmap4644 = 4644;
+    public long fillRefBitmap4645 = 4645;
+    public long fillRefBitmap4646 = 4646;
+    public long fillRefBitmap4647 = 4647;
+    public long fillRefBitmap4648 = 4648;
+    public long fillRefBitmap4649 = 4649;
+    public long fillRefBitmap4650 = 4650;
+    public long fillRefBitmap4651 = 4651;
+    public long fillRefBitmap4652 = 4652;
+    public long fillRefBitmap4653 = 4653;
+    public long fillRefBitmap4654 = 4654;
+    public long fillRefBitmap4655 = 4655;
+    public long fillRefBitmap4656 = 4656;
+    public long fillRefBitmap4657 = 4657;
+    public long fillRefBitmap4658 = 4658;
+    public long fillRefBitmap4659 = 4659;
+    public long fillRefBitmap4660 = 4660;
+    public long fillRefBitmap4661 = 4661;
+    public long fillRefBitmap4662 = 4662;
+    public long fillRefBitmap4663 = 4663;
+    public long fillRefBitmap4664 = 4664;
+    public long fillRefBitmap4665 = 4665;
+    public long fillRefBitmap4666 = 4666;
+    public long fillRefBitmap4667 = 4667;
+    public long fillRefBitmap4668 = 4668;
+    public long fillRefBitmap4669 = 4669;
+    public long fillRefBitmap4670 = 4670;
+    public long fillRefBitmap4671 = 4671;
+    public long fillRefBitmap4672 = 4672;
+    public long fillRefBitmap4673 = 4673;
+    public long fillRefBitmap4674 = 4674;
+    public long fillRefBitmap4675 = 4675;
+    public long fillRefBitmap4676 = 4676;
+    public long fillRefBitmap4677 = 4677;
+    public long fillRefBitmap4678 = 4678;
+    public long fillRefBitmap4679 = 4679;
+    public long fillRefBitmap4680 = 4680;
+    public long fillRefBitmap4681 = 4681;
+    public long fillRefBitmap4682 = 4682;
+    public long fillRefBitmap4683 = 4683;
+    public long fillRefBitmap4684 = 4684;
+    public long fillRefBitmap4685 = 4685;
+    public long fillRefBitmap4686 = 4686;
+    public long fillRefBitmap4687 = 4687;
+    public long fillRefBitmap4688 = 4688;
+    public long fillRefBitmap4689 = 4689;
+    public long fillRefBitmap4690 = 4690;
+    public long fillRefBitmap4691 = 4691;
+    public long fillRefBitmap4692 = 4692;
+    public long fillRefBitmap4693 = 4693;
+    public long fillRefBitmap4694 = 4694;
+    public long fillRefBitmap4695 = 4695;
+    public long fillRefBitmap4696 = 4696;
+    public long fillRefBitmap4697 = 4697;
+    public long fillRefBitmap4698 = 4698;
+    public long fillRefBitmap4699 = 4699;
+    public long fillRefBitmap4700 = 4700;
+    public long fillRefBitmap4701 = 4701;
+    public long fillRefBitmap4702 = 4702;
+    public long fillRefBitmap4703 = 4703;
+    public long fillRefBitmap4704 = 4704;
+    public long fillRefBitmap4705 = 4705;
+    public long fillRefBitmap4706 = 4706;
+    public long fillRefBitmap4707 = 4707;
+    public long fillRefBitmap4708 = 4708;
+    public long fillRefBitmap4709 = 4709;
+    public long fillRefBitmap4710 = 4710;
+    public long fillRefBitmap4711 = 4711;
+    public long fillRefBitmap4712 = 4712;
+    public long fillRefBitmap4713 = 4713;
+    public long fillRefBitmap4714 = 4714;
+    public long fillRefBitmap4715 = 4715;
+    public long fillRefBitmap4716 = 4716;
+    public long fillRefBitmap4717 = 4717;
+    public long fillRefBitmap4718 = 4718;
+    public long fillRefBitmap4719 = 4719;
+    public long fillRefBitmap4720 = 4720;
+    public long fillRefBitmap4721 = 4721;
+    public long fillRefBitmap4722 = 4722;
+    public long fillRefBitmap4723 = 4723;
+    public long fillRefBitmap4724 = 4724;
+    public long fillRefBitmap4725 = 4725;
+    public long fillRefBitmap4726 = 4726;
+    public long fillRefBitmap4727 = 4727;
+    public long fillRefBitmap4728 = 4728;
+    public long fillRefBitmap4729 = 4729;
+    public long fillRefBitmap4730 = 4730;
+    public long fillRefBitmap4731 = 4731;
+    public long fillRefBitmap4732 = 4732;
+    public long fillRefBitmap4733 = 4733;
+    public long fillRefBitmap4734 = 4734;
+    public long fillRefBitmap4735 = 4735;
+    public long fillRefBitmap4736 = 4736;
+    public long fillRefBitmap4737 = 4737;
+    public long fillRefBitmap4738 = 4738;
+    public long fillRefBitmap4739 = 4739;
+    public long fillRefBitmap4740 = 4740;
+    public long fillRefBitmap4741 = 4741;
+    public long fillRefBitmap4742 = 4742;
+    public long fillRefBitmap4743 = 4743;
+    public long fillRefBitmap4744 = 4744;
+    public long fillRefBitmap4745 = 4745;
+    public long fillRefBitmap4746 = 4746;
+    public long fillRefBitmap4747 = 4747;
+    public long fillRefBitmap4748 = 4748;
+    public long fillRefBitmap4749 = 4749;
+    public long fillRefBitmap4750 = 4750;
+    public long fillRefBitmap4751 = 4751;
+    public long fillRefBitmap4752 = 4752;
+    public long fillRefBitmap4753 = 4753;
+    public long fillRefBitmap4754 = 4754;
+    public long fillRefBitmap4755 = 4755;
+    public long fillRefBitmap4756 = 4756;
+    public long fillRefBitmap4757 = 4757;
+    public long fillRefBitmap4758 = 4758;
+    public long fillRefBitmap4759 = 4759;
+    public long fillRefBitmap4760 = 4760;
+    public long fillRefBitmap4761 = 4761;
+    public long fillRefBitmap4762 = 4762;
+    public long fillRefBitmap4763 = 4763;
+    public long fillRefBitmap4764 = 4764;
+    public long fillRefBitmap4765 = 4765;
+    public long fillRefBitmap4766 = 4766;
+    public long fillRefBitmap4767 = 4767;
+    public long fillRefBitmap4768 = 4768;
+    public long fillRefBitmap4769 = 4769;
+    public long fillRefBitmap4770 = 4770;
+    public long fillRefBitmap4771 = 4771;
+    public long fillRefBitmap4772 = 4772;
+    public long fillRefBitmap4773 = 4773;
+    public long fillRefBitmap4774 = 4774;
+    public long fillRefBitmap4775 = 4775;
+    public long fillRefBitmap4776 = 4776;
+    public long fillRefBitmap4777 = 4777;
+    public long fillRefBitmap4778 = 4778;
+    public long fillRefBitmap4779 = 4779;
+    public long fillRefBitmap4780 = 4780;
+    public long fillRefBitmap4781 = 4781;
+    public long fillRefBitmap4782 = 4782;
+    public long fillRefBitmap4783 = 4783;
+    public long fillRefBitmap4784 = 4784;
+    public long fillRefBitmap4785 = 4785;
+    public long fillRefBitmap4786 = 4786;
+    public long fillRefBitmap4787 = 4787;
+    public long fillRefBitmap4788 = 4788;
+    public long fillRefBitmap4789 = 4789;
+    public long fillRefBitmap4790 = 4790;
+    public long fillRefBitmap4791 = 4791;
+    public long fillRefBitmap4792 = 4792;
+    public long fillRefBitmap4793 = 4793;
+    public long fillRefBitmap4794 = 4794;
+    public long fillRefBitmap4795 = 4795;
+    public long fillRefBitmap4796 = 4796;
+    public long fillRefBitmap4797 = 4797;
+    public long fillRefBitmap4798 = 4798;
+    public long fillRefBitmap4799 = 4799;
+    public long fillRefBitmap4800 = 4800;
+    public long fillRefBitmap4801 = 4801;
+    public long fillRefBitmap4802 = 4802;
+    public long fillRefBitmap4803 = 4803;
+    public long fillRefBitmap4804 = 4804;
+    public long fillRefBitmap4805 = 4805;
+    public long fillRefBitmap4806 = 4806;
+    public long fillRefBitmap4807 = 4807;
+    public long fillRefBitmap4808 = 4808;
+    public long fillRefBitmap4809 = 4809;
+    public long fillRefBitmap4810 = 4810;
+    public long fillRefBitmap4811 = 4811;
+    public long fillRefBitmap4812 = 4812;
+    public long fillRefBitmap4813 = 4813;
+    public long fillRefBitmap4814 = 4814;
+    public long fillRefBitmap4815 = 4815;
+    public long fillRefBitmap4816 = 4816;
+    public long fillRefBitmap4817 = 4817;
+    public long fillRefBitmap4818 = 4818;
+    public long fillRefBitmap4819 = 4819;
+    public long fillRefBitmap4820 = 4820;
+    public long fillRefBitmap4821 = 4821;
+    public long fillRefBitmap4822 = 4822;
+    public long fillRefBitmap4823 = 4823;
+    public long fillRefBitmap4824 = 4824;
+    public long fillRefBitmap4825 = 4825;
+    public long fillRefBitmap4826 = 4826;
+    public long fillRefBitmap4827 = 4827;
+    public long fillRefBitmap4828 = 4828;
+    public long fillRefBitmap4829 = 4829;
+    public long fillRefBitmap4830 = 4830;
+    public long fillRefBitmap4831 = 4831;
+    public long fillRefBitmap4832 = 4832;
+    public long fillRefBitmap4833 = 4833;
+    public long fillRefBitmap4834 = 4834;
+    public long fillRefBitmap4835 = 4835;
+    public long fillRefBitmap4836 = 4836;
+    public long fillRefBitmap4837 = 4837;
+    public long fillRefBitmap4838 = 4838;
+    public long fillRefBitmap4839 = 4839;
+    public long fillRefBitmap4840 = 4840;
+    public long fillRefBitmap4841 = 4841;
+    public long fillRefBitmap4842 = 4842;
+    public long fillRefBitmap4843 = 4843;
+    public long fillRefBitmap4844 = 4844;
+    public long fillRefBitmap4845 = 4845;
+    public long fillRefBitmap4846 = 4846;
+    public long fillRefBitmap4847 = 4847;
+    public long fillRefBitmap4848 = 4848;
+    public long fillRefBitmap4849 = 4849;
+    public long fillRefBitmap4850 = 4850;
+    public long fillRefBitmap4851 = 4851;
+    public long fillRefBitmap4852 = 4852;
+    public long fillRefBitmap4853 = 4853;
+    public long fillRefBitmap4854 = 4854;
+    public long fillRefBitmap4855 = 4855;
+    public long fillRefBitmap4856 = 4856;
+    public long fillRefBitmap4857 = 4857;
+    public long fillRefBitmap4858 = 4858;
+    public long fillRefBitmap4859 = 4859;
+    public long fillRefBitmap4860 = 4860;
+    public long fillRefBitmap4861 = 4861;
+    public long fillRefBitmap4862 = 4862;
+    public long fillRefBitmap4863 = 4863;
+    public long fillRefBitmap4864 = 4864;
+    public long fillRefBitmap4865 = 4865;
+    public long fillRefBitmap4866 = 4866;
+    public long fillRefBitmap4867 = 4867;
+    public long fillRefBitmap4868 = 4868;
+    public long fillRefBitmap4869 = 4869;
+    public long fillRefBitmap4870 = 4870;
+    public long fillRefBitmap4871 = 4871;
+    public long fillRefBitmap4872 = 4872;
+    public long fillRefBitmap4873 = 4873;
+    public long fillRefBitmap4874 = 4874;
+    public long fillRefBitmap4875 = 4875;
+    public long fillRefBitmap4876 = 4876;
+    public long fillRefBitmap4877 = 4877;
+    public long fillRefBitmap4878 = 4878;
+    public long fillRefBitmap4879 = 4879;
+    public long fillRefBitmap4880 = 4880;
+    public long fillRefBitmap4881 = 4881;
+    public long fillRefBitmap4882 = 4882;
+    public long fillRefBitmap4883 = 4883;
+    public long fillRefBitmap4884 = 4884;
+    public long fillRefBitmap4885 = 4885;
+    public long fillRefBitmap4886 = 4886;
+    public long fillRefBitmap4887 = 4887;
+    public long fillRefBitmap4888 = 4888;
+    public long fillRefBitmap4889 = 4889;
+    public long fillRefBitmap4890 = 4890;
+    public long fillRefBitmap4891 = 4891;
+    public long fillRefBitmap4892 = 4892;
+    public long fillRefBitmap4893 = 4893;
+    public long fillRefBitmap4894 = 4894;
+    public long fillRefBitmap4895 = 4895;
+    public long fillRefBitmap4896 = 4896;
+    public long fillRefBitmap4897 = 4897;
+    public long fillRefBitmap4898 = 4898;
+    public long fillRefBitmap4899 = 4899;
+    public long fillRefBitmap4900 = 4900;
+    public long fillRefBitmap4901 = 4901;
+    public long fillRefBitmap4902 = 4902;
+    public long fillRefBitmap4903 = 4903;
+    public long fillRefBitmap4904 = 4904;
+    public long fillRefBitmap4905 = 4905;
+    public long fillRefBitmap4906 = 4906;
+    public long fillRefBitmap4907 = 4907;
+    public long fillRefBitmap4908 = 4908;
+    public long fillRefBitmap4909 = 4909;
+    public long fillRefBitmap4910 = 4910;
+    public long fillRefBitmap4911 = 4911;
+    public long fillRefBitmap4912 = 4912;
+    public long fillRefBitmap4913 = 4913;
+    public long fillRefBitmap4914 = 4914;
+    public long fillRefBitmap4915 = 4915;
+    public long fillRefBitmap4916 = 4916;
+    public long fillRefBitmap4917 = 4917;
+    public long fillRefBitmap4918 = 4918;
+    public long fillRefBitmap4919 = 4919;
+    public long fillRefBitmap4920 = 4920;
+    public long fillRefBitmap4921 = 4921;
+    public long fillRefBitmap4922 = 4922;
+    public long fillRefBitmap4923 = 4923;
+    public long fillRefBitmap4924 = 4924;
+    public long fillRefBitmap4925 = 4925;
+    public long fillRefBitmap4926 = 4926;
+    public long fillRefBitmap4927 = 4927;
+    public long fillRefBitmap4928 = 4928;
+    public long fillRefBitmap4929 = 4929;
+    public long fillRefBitmap4930 = 4930;
+    public long fillRefBitmap4931 = 4931;
+    public long fillRefBitmap4932 = 4932;
+    public long fillRefBitmap4933 = 4933;
+    public long fillRefBitmap4934 = 4934;
+    public long fillRefBitmap4935 = 4935;
+    public long fillRefBitmap4936 = 4936;
+    public long fillRefBitmap4937 = 4937;
+    public long fillRefBitmap4938 = 4938;
+    public long fillRefBitmap4939 = 4939;
+    public long fillRefBitmap4940 = 4940;
+    public long fillRefBitmap4941 = 4941;
+    public long fillRefBitmap4942 = 4942;
+    public long fillRefBitmap4943 = 4943;
+    public long fillRefBitmap4944 = 4944;
+    public long fillRefBitmap4945 = 4945;
+    public long fillRefBitmap4946 = 4946;
+    public long fillRefBitmap4947 = 4947;
+    public long fillRefBitmap4948 = 4948;
+    public long fillRefBitmap4949 = 4949;
+    public long fillRefBitmap4950 = 4950;
+    public long fillRefBitmap4951 = 4951;
+    public long fillRefBitmap4952 = 4952;
+    public long fillRefBitmap4953 = 4953;
+    public long fillRefBitmap4954 = 4954;
+    public long fillRefBitmap4955 = 4955;
+    public long fillRefBitmap4956 = 4956;
+    public long fillRefBitmap4957 = 4957;
+    public long fillRefBitmap4958 = 4958;
+    public long fillRefBitmap4959 = 4959;
+    public long fillRefBitmap4960 = 4960;
+    public long fillRefBitmap4961 = 4961;
+    public long fillRefBitmap4962 = 4962;
+    public long fillRefBitmap4963 = 4963;
+    public long fillRefBitmap4964 = 4964;
+    public long fillRefBitmap4965 = 4965;
+    public long fillRefBitmap4966 = 4966;
+    public long fillRefBitmap4967 = 4967;
+    public long fillRefBitmap4968 = 4968;
+    public long fillRefBitmap4969 = 4969;
+    public long fillRefBitmap4970 = 4970;
+    public long fillRefBitmap4971 = 4971;
+    public long fillRefBitmap4972 = 4972;
+    public long fillRefBitmap4973 = 4973;
+    public long fillRefBitmap4974 = 4974;
+    public long fillRefBitmap4975 = 4975;
+    public long fillRefBitmap4976 = 4976;
+    public long fillRefBitmap4977 = 4977;
+    public long fillRefBitmap4978 = 4978;
+    public long fillRefBitmap4979 = 4979;
+    public long fillRefBitmap4980 = 4980;
+    public long fillRefBitmap4981 = 4981;
+    public long fillRefBitmap4982 = 4982;
+    public long fillRefBitmap4983 = 4983;
+    public long fillRefBitmap4984 = 4984;
+    public long fillRefBitmap4985 = 4985;
+    public long fillRefBitmap4986 = 4986;
+    public long fillRefBitmap4987 = 4987;
+    public long fillRefBitmap4988 = 4988;
+    public long fillRefBitmap4989 = 4989;
+    public long fillRefBitmap4990 = 4990;
+    public long fillRefBitmap4991 = 4991;
+    public long fillRefBitmap4992 = 4992;
+    public long fillRefBitmap4993 = 4993;
+    public long fillRefBitmap4994 = 4994;
+    public long fillRefBitmap4995 = 4995;
+    public long fillRefBitmap4996 = 4996;
+    public long fillRefBitmap4997 = 4997;
+    public long fillRefBitmap4998 = 4998;
+    public long fillRefBitmap4999 = 4999;
 }
diff --git a/test/160-read-barrier-stress/src/ManyFieldsBase1.java b/test/160-read-barrier-stress/src/ManyFieldsBase1.java
index 8680c6bb03..38cea2fa31 100644
--- a/test/160-read-barrier-stress/src/ManyFieldsBase1.java
+++ b/test/160-read-barrier-stress/src/ManyFieldsBase1.java
@@ -1015,4 +1015,5004 @@ class ManyFieldsBase1 extends ManyFieldsBase0 {
     public Object testField1997 = new Integer(1997);
     public Object testField1998 = new Integer(1998);
     public Object testField1999 = new Integer(1999);
+
+    public long fillRefBitmap5001 = 5001;
+    public long fillRefBitmap5002 = 5002;
+    public long fillRefBitmap5003 = 5003;
+    public long fillRefBitmap5004 = 5004;
+    public long fillRefBitmap5005 = 5005;
+    public long fillRefBitmap5006 = 5006;
+    public long fillRefBitmap5007 = 5007;
+    public long fillRefBitmap5008 = 5008;
+    public long fillRefBitmap5009 = 5009;
+    public long fillRefBitmap5010 = 5010;
+    public long fillRefBitmap5011 = 5011;
+    public long fillRefBitmap5012 = 5012;
+    public long fillRefBitmap5013 = 5013;
+    public long fillRefBitmap5014 = 5014;
+    public long fillRefBitmap5015 = 5015;
+    public long fillRefBitmap5016 = 5016;
+    public long fillRefBitmap5017 = 5017;
+    public long fillRefBitmap5018 = 5018;
+    public long fillRefBitmap5019 = 5019;
+    public long fillRefBitmap5020 = 5020;
+    public long fillRefBitmap5021 = 5021;
+    public long fillRefBitmap5022 = 5022;
+    public long fillRefBitmap5023 = 5023;
+    public long fillRefBitmap5024 = 5024;
+    public long fillRefBitmap5025 = 5025;
+    public long fillRefBitmap5026 = 5026;
+    public long fillRefBitmap5027 = 5027;
+    public long fillRefBitmap5028 = 5028;
+    public long fillRefBitmap5029 = 5029;
+    public long fillRefBitmap5030 = 5030;
+    public long fillRefBitmap5031 = 5031;
+    public long fillRefBitmap5032 = 5032;
+    public long fillRefBitmap5033 = 5033;
+    public long fillRefBitmap5034 = 5034;
+    public long fillRefBitmap5035 = 5035;
+    public long fillRefBitmap5036 = 5036;
+    public long fillRefBitmap5037 = 5037;
+    public long fillRefBitmap5038 = 5038;
+    public long fillRefBitmap5039 = 5039;
+    public long fillRefBitmap5040 = 5040;
+    public long fillRefBitmap5041 = 5041;
+    public long fillRefBitmap5042 = 5042;
+    public long fillRefBitmap5043 = 5043;
+    public long fillRefBitmap5044 = 5044;
+    public long fillRefBitmap5045 = 5045;
+    public long fillRefBitmap5046 = 5046;
+    public long fillRefBitmap5047 = 5047;
+    public long fillRefBitmap5048 = 5048;
+    public long fillRefBitmap5049 = 5049;
+    public long fillRefBitmap5050 = 5050;
+    public long fillRefBitmap5051 = 5051;
+    public long fillRefBitmap5052 = 5052;
+    public long fillRefBitmap5053 = 5053;
+    public long fillRefBitmap5054 = 5054;
+    public long fillRefBitmap5055 = 5055;
+    public long fillRefBitmap5056 = 5056;
+    public long fillRefBitmap5057 = 5057;
+    public long fillRefBitmap5058 = 5058;
+    public long fillRefBitmap5059 = 5059;
+    public long fillRefBitmap5060 = 5060;
+    public long fillRefBitmap5061 = 5061;
+    public long fillRefBitmap5062 = 5062;
+    public long fillRefBitmap5063 = 5063;
+    public long fillRefBitmap5064 = 5064;
+    public long fillRefBitmap5065 = 5065;
+    public long fillRefBitmap5066 = 5066;
+    public long fillRefBitmap5067 = 5067;
+    public long fillRefBitmap5068 = 5068;
+    public long fillRefBitmap5069 = 5069;
+    public long fillRefBitmap5070 = 5070;
+    public long fillRefBitmap5071 = 5071;
+    public long fillRefBitmap5072 = 5072;
+    public long fillRefBitmap5073 = 5073;
+    public long fillRefBitmap5074 = 5074;
+    public long fillRefBitmap5075 = 5075;
+    public long fillRefBitmap5076 = 5076;
+    public long fillRefBitmap5077 = 5077;
+    public long fillRefBitmap5078 = 5078;
+    public long fillRefBitmap5079 = 5079;
+    public long fillRefBitmap5080 = 5080;
+    public long fillRefBitmap5081 = 5081;
+    public long fillRefBitmap5082 = 5082;
+    public long fillRefBitmap5083 = 5083;
+    public long fillRefBitmap5084 = 5084;
+    public long fillRefBitmap5085 = 5085;
+    public long fillRefBitmap5086 = 5086;
+    public long fillRefBitmap5087 = 5087;
+    public long fillRefBitmap5088 = 5088;
+    public long fillRefBitmap5089 = 5089;
+    public long fillRefBitmap5090 = 5090;
+    public long fillRefBitmap5091 = 5091;
+    public long fillRefBitmap5092 = 5092;
+    public long fillRefBitmap5093 = 5093;
+    public long fillRefBitmap5094 = 5094;
+    public long fillRefBitmap5095 = 5095;
+    public long fillRefBitmap5096 = 5096;
+    public long fillRefBitmap5097 = 5097;
+    public long fillRefBitmap5098 = 5098;
+    public long fillRefBitmap5099 = 5099;
+    public long fillRefBitmap5100 = 5100;
+    public long fillRefBitmap5101 = 5101;
+    public long fillRefBitmap5102 = 5102;
+    public long fillRefBitmap5103 = 5103;
+    public long fillRefBitmap5104 = 5104;
+    public long fillRefBitmap5105 = 5105;
+    public long fillRefBitmap5106 = 5106;
+    public long fillRefBitmap5107 = 5107;
+    public long fillRefBitmap5108 = 5108;
+    public long fillRefBitmap5109 = 5109;
+    public long fillRefBitmap5110 = 5110;
+    public long fillRefBitmap5111 = 5111;
+    public long fillRefBitmap5112 = 5112;
+    public long fillRefBitmap5113 = 5113;
+    public long fillRefBitmap5114 = 5114;
+    public long fillRefBitmap5115 = 5115;
+    public long fillRefBitmap5116 = 5116;
+    public long fillRefBitmap5117 = 5117;
+    public long fillRefBitmap5118 = 5118;
+    public long fillRefBitmap5119 = 5119;
+    public long fillRefBitmap5120 = 5120;
+    public long fillRefBitmap5121 = 5121;
+    public long fillRefBitmap5122 = 5122;
+    public long fillRefBitmap5123 = 5123;
+    public long fillRefBitmap5124 = 5124;
+    public long fillRefBitmap5125 = 5125;
+    public long fillRefBitmap5126 = 5126;
+    public long fillRefBitmap5127 = 5127;
+    public long fillRefBitmap5128 = 5128;
+    public long fillRefBitmap5129 = 5129;
+    public long fillRefBitmap5130 = 5130;
+    public long fillRefBitmap5131 = 5131;
+    public long fillRefBitmap5132 = 5132;
+    public long fillRefBitmap5133 = 5133;
+    public long fillRefBitmap5134 = 5134;
+    public long fillRefBitmap5135 = 5135;
+    public long fillRefBitmap5136 = 5136;
+    public long fillRefBitmap5137 = 5137;
+    public long fillRefBitmap5138 = 5138;
+    public long fillRefBitmap5139 = 5139;
+    public long fillRefBitmap5140 = 5140;
+    public long fillRefBitmap5141 = 5141;
+    public long fillRefBitmap5142 = 5142;
+    public long fillRefBitmap5143 = 5143;
+    public long fillRefBitmap5144 = 5144;
+    public long fillRefBitmap5145 = 5145;
+    public long fillRefBitmap5146 = 5146;
+    public long fillRefBitmap5147 = 5147;
+    public long fillRefBitmap5148 = 5148;
+    public long fillRefBitmap5149 = 5149;
+    public long fillRefBitmap5150 = 5150;
+    public long fillRefBitmap5151 = 5151;
+    public long fillRefBitmap5152 = 5152;
+    public long fillRefBitmap5153 = 5153;
+    public long fillRefBitmap5154 = 5154;
+    public long fillRefBitmap5155 = 5155;
+    public long fillRefBitmap5156 = 5156;
+    public long fillRefBitmap5157 = 5157;
+    public long fillRefBitmap5158 = 5158;
+    public long fillRefBitmap5159 = 5159;
+    public long fillRefBitmap5160 = 5160;
+    public long fillRefBitmap5161 = 5161;
+    public long fillRefBitmap5162 = 5162;
+    public long fillRefBitmap5163 = 5163;
+    public long fillRefBitmap5164 = 5164;
+    public long fillRefBitmap5165 = 5165;
+    public long fillRefBitmap5166 = 5166;
+    public long fillRefBitmap5167 = 5167;
+    public long fillRefBitmap5168 = 5168;
+    public long fillRefBitmap5169 = 5169;
+    public long fillRefBitmap5170 = 5170;
+    public long fillRefBitmap5171 = 5171;
+    public long fillRefBitmap5172 = 5172;
+    public long fillRefBitmap5173 = 5173;
+    public long fillRefBitmap5174 = 5174;
+    public long fillRefBitmap5175 = 5175;
+    public long fillRefBitmap5176 = 5176;
+    public long fillRefBitmap5177 = 5177;
+    public long fillRefBitmap5178 = 5178;
+    public long fillRefBitmap5179 = 5179;
+    public long fillRefBitmap5180 = 5180;
+    public long fillRefBitmap5181 = 5181;
+    public long fillRefBitmap5182 = 5182;
+    public long fillRefBitmap5183 = 5183;
+    public long fillRefBitmap5184 = 5184;
+    public long fillRefBitmap5185 = 5185;
+    public long fillRefBitmap5186 = 5186;
+    public long fillRefBitmap5187 = 5187;
+    public long fillRefBitmap5188 = 5188;
+    public long fillRefBitmap5189 = 5189;
+    public long fillRefBitmap5190 = 5190;
+    public long fillRefBitmap5191 = 5191;
+    public long fillRefBitmap5192 = 5192;
+    public long fillRefBitmap5193 = 5193;
+    public long fillRefBitmap5194 = 5194;
+    public long fillRefBitmap5195 = 5195;
+    public long fillRefBitmap5196 = 5196;
+    public long fillRefBitmap5197 = 5197;
+    public long fillRefBitmap5198 = 5198;
+    public long fillRefBitmap5199 = 5199;
+    public long fillRefBitmap5200 = 5200;
+    public long fillRefBitmap5201 = 5201;
+    public long fillRefBitmap5202 = 5202;
+    public long fillRefBitmap5203 = 5203;
+    public long fillRefBitmap5204 = 5204;
+    public long fillRefBitmap5205 = 5205;
+    public long fillRefBitmap5206 = 5206;
+    public long fillRefBitmap5207 = 5207;
+    public long fillRefBitmap5208 = 5208;
+    public long fillRefBitmap5209 = 5209;
+    public long fillRefBitmap5210 = 5210;
+    public long fillRefBitmap5211 = 5211;
+    public long fillRefBitmap5212 = 5212;
+    public long fillRefBitmap5213 = 5213;
+    public long fillRefBitmap5214 = 5214;
+    public long fillRefBitmap5215 = 5215;
+    public long fillRefBitmap5216 = 5216;
+    public long fillRefBitmap5217 = 5217;
+    public long fillRefBitmap5218 = 5218;
+    public long fillRefBitmap5219 = 5219;
+    public long fillRefBitmap5220 = 5220;
+    public long fillRefBitmap5221 = 5221;
+    public long fillRefBitmap5222 = 5222;
+    public long fillRefBitmap5223 = 5223;
+    public long fillRefBitmap5224 = 5224;
+    public long fillRefBitmap5225 = 5225;
+    public long fillRefBitmap5226 = 5226;
+    public long fillRefBitmap5227 = 5227;
+    public long fillRefBitmap5228 = 5228;
+    public long fillRefBitmap5229 = 5229;
+    public long fillRefBitmap5230 = 5230;
+    public long fillRefBitmap5231 = 5231;
+    public long fillRefBitmap5232 = 5232;
+    public long fillRefBitmap5233 = 5233;
+    public long fillRefBitmap5234 = 5234;
+    public long fillRefBitmap5235 = 5235;
+    public long fillRefBitmap5236 = 5236;
+    public long fillRefBitmap5237 = 5237;
+    public long fillRefBitmap5238 = 5238;
+    public long fillRefBitmap5239 = 5239;
+    public long fillRefBitmap5240 = 5240;
+    public long fillRefBitmap5241 = 5241;
+    public long fillRefBitmap5242 = 5242;
+    public long fillRefBitmap5243 = 5243;
+    public long fillRefBitmap5244 = 5244;
+    public long fillRefBitmap5245 = 5245;
+    public long fillRefBitmap5246 = 5246;
+    public long fillRefBitmap5247 = 5247;
+    public long fillRefBitmap5248 = 5248;
+    public long fillRefBitmap5249 = 5249;
+    public long fillRefBitmap5250 = 5250;
+    public long fillRefBitmap5251 = 5251;
+    public long fillRefBitmap5252 = 5252;
+    public long fillRefBitmap5253 = 5253;
+    public long fillRefBitmap5254 = 5254;
+    public long fillRefBitmap5255 = 5255;
+    public long fillRefBitmap5256 = 5256;
+    public long fillRefBitmap5257 = 5257;
+    public long fillRefBitmap5258 = 5258;
+    public long fillRefBitmap5259 = 5259;
+    public long fillRefBitmap5260 = 5260;
+    public long fillRefBitmap5261 = 5261;
+    public long fillRefBitmap5262 = 5262;
+    public long fillRefBitmap5263 = 5263;
+    public long fillRefBitmap5264 = 5264;
+    public long fillRefBitmap5265 = 5265;
+    public long fillRefBitmap5266 = 5266;
+    public long fillRefBitmap5267 = 5267;
+    public long fillRefBitmap5268 = 5268;
+    public long fillRefBitmap5269 = 5269;
+    public long fillRefBitmap5270 = 5270;
+    public long fillRefBitmap5271 = 5271;
+    public long fillRefBitmap5272 = 5272;
+    public long fillRefBitmap5273 = 5273;
+    public long fillRefBitmap5274 = 5274;
+    public long fillRefBitmap5275 = 5275;
+    public long fillRefBitmap5276 = 5276;
+    public long fillRefBitmap5277 = 5277;
+    public long fillRefBitmap5278 = 5278;
+    public long fillRefBitmap5279 = 5279;
+    public long fillRefBitmap5280 = 5280;
+    public long fillRefBitmap5281 = 5281;
+    public long fillRefBitmap5282 = 5282;
+    public long fillRefBitmap5283 = 5283;
+    public long fillRefBitmap5284 = 5284;
+    public long fillRefBitmap5285 = 5285;
+    public long fillRefBitmap5286 = 5286;
+    public long fillRefBitmap5287 = 5287;
+    public long fillRefBitmap5288 = 5288;
+    public long fillRefBitmap5289 = 5289;
+    public long fillRefBitmap5290 = 5290;
+    public long fillRefBitmap5291 = 5291;
+    public long fillRefBitmap5292 = 5292;
+    public long fillRefBitmap5293 = 5293;
+    public long fillRefBitmap5294 = 5294;
+    public long fillRefBitmap5295 = 5295;
+    public long fillRefBitmap5296 = 5296;
+    public long fillRefBitmap5297 = 5297;
+    public long fillRefBitmap5298 = 5298;
+    public long fillRefBitmap5299 = 5299;
+    public long fillRefBitmap5300 = 5300;
+    public long fillRefBitmap5301 = 5301;
+    public long fillRefBitmap5302 = 5302;
+    public long fillRefBitmap5303 = 5303;
+    public long fillRefBitmap5304 = 5304;
+    public long fillRefBitmap5305 = 5305;
+    public long fillRefBitmap5306 = 5306;
+    public long fillRefBitmap5307 = 5307;
+    public long fillRefBitmap5308 = 5308;
+    public long fillRefBitmap5309 = 5309;
+    public long fillRefBitmap5310 = 5310;
+    public long fillRefBitmap5311 = 5311;
+    public long fillRefBitmap5312 = 5312;
+    public long fillRefBitmap5313 = 5313;
+    public long fillRefBitmap5314 = 5314;
+    public long fillRefBitmap5315 = 5315;
+    public long fillRefBitmap5316 = 5316;
+    public long fillRefBitmap5317 = 5317;
+    public long fillRefBitmap5318 = 5318;
+    public long fillRefBitmap5319 = 5319;
+    public long fillRefBitmap5320 = 5320;
+    public long fillRefBitmap5321 = 5321;
+    public long fillRefBitmap5322 = 5322;
+    public long fillRefBitmap5323 = 5323;
+    public long fillRefBitmap5324 = 5324;
+    public long fillRefBitmap5325 = 5325;
+    public long fillRefBitmap5326 = 5326;
+    public long fillRefBitmap5327 = 5327;
+    public long fillRefBitmap5328 = 5328;
+    public long fillRefBitmap5329 = 5329;
+    public long fillRefBitmap5330 = 5330;
+    public long fillRefBitmap5331 = 5331;
+    public long fillRefBitmap5332 = 5332;
+    public long fillRefBitmap5333 = 5333;
+    public long fillRefBitmap5334 = 5334;
+    public long fillRefBitmap5335 = 5335;
+    public long fillRefBitmap5336 = 5336;
+    public long fillRefBitmap5337 = 5337;
+    public long fillRefBitmap5338 = 5338;
+    public long fillRefBitmap5339 = 5339;
+    public long fillRefBitmap5340 = 5340;
+    public long fillRefBitmap5341 = 5341;
+    public long fillRefBitmap5342 = 5342;
+    public long fillRefBitmap5343 = 5343;
+    public long fillRefBitmap5344 = 5344;
+    public long fillRefBitmap5345 = 5345;
+    public long fillRefBitmap5346 = 5346;
+    public long fillRefBitmap5347 = 5347;
+    public long fillRefBitmap5348 = 5348;
+    public long fillRefBitmap5349 = 5349;
+    public long fillRefBitmap5350 = 5350;
+    public long fillRefBitmap5351 = 5351;
+    public long fillRefBitmap5352 = 5352;
+    public long fillRefBitmap5353 = 5353;
+    public long fillRefBitmap5354 = 5354;
+    public long fillRefBitmap5355 = 5355;
+    public long fillRefBitmap5356 = 5356;
+    public long fillRefBitmap5357 = 5357;
+    public long fillRefBitmap5358 = 5358;
+    public long fillRefBitmap5359 = 5359;
+    public long fillRefBitmap5360 = 5360;
+    public long fillRefBitmap5361 = 5361;
+    public long fillRefBitmap5362 = 5362;
+    public long fillRefBitmap5363 = 5363;
+    public long fillRefBitmap5364 = 5364;
+    public long fillRefBitmap5365 = 5365;
+    public long fillRefBitmap5366 = 5366;
+    public long fillRefBitmap5367 = 5367;
+    public long fillRefBitmap5368 = 5368;
+    public long fillRefBitmap5369 = 5369;
+    public long fillRefBitmap5370 = 5370;
+    public long fillRefBitmap5371 = 5371;
+    public long fillRefBitmap5372 = 5372;
+    public long fillRefBitmap5373 = 5373;
+    public long fillRefBitmap5374 = 5374;
+    public long fillRefBitmap5375 = 5375;
+    public long fillRefBitmap5376 = 5376;
+    public long fillRefBitmap5377 = 5377;
+    public long fillRefBitmap5378 = 5378;
+    public long fillRefBitmap5379 = 5379;
+    public long fillRefBitmap5380 = 5380;
+    public long fillRefBitmap5381 = 5381;
+    public long fillRefBitmap5382 = 5382;
+    public long fillRefBitmap5383 = 5383;
+    public long fillRefBitmap5384 = 5384;
+    public long fillRefBitmap5385 = 5385;
+    public long fillRefBitmap5386 = 5386;
+    public long fillRefBitmap5387 = 5387;
+    public long fillRefBitmap5388 = 5388;
+    public long fillRefBitmap5389 = 5389;
+    public long fillRefBitmap5390 = 5390;
+    public long fillRefBitmap5391 = 5391;
+    public long fillRefBitmap5392 = 5392;
+    public long fillRefBitmap5393 = 5393;
+    public long fillRefBitmap5394 = 5394;
+    public long fillRefBitmap5395 = 5395;
+    public long fillRefBitmap5396 = 5396;
+    public long fillRefBitmap5397 = 5397;
+    public long fillRefBitmap5398 = 5398;
+    public long fillRefBitmap5399 = 5399;
+    public long fillRefBitmap5400 = 5400;
+    public long fillRefBitmap5401 = 5401;
+    public long fillRefBitmap5402 = 5402;
+    public long fillRefBitmap5403 = 5403;
+    public long fillRefBitmap5404 = 5404;
+    public long fillRefBitmap5405 = 5405;
+    public long fillRefBitmap5406 = 5406;
+    public long fillRefBitmap5407 = 5407;
+    public long fillRefBitmap5408 = 5408;
+    public long fillRefBitmap5409 = 5409;
+    public long fillRefBitmap5410 = 5410;
+    public long fillRefBitmap5411 = 5411;
+    public long fillRefBitmap5412 = 5412;
+    public long fillRefBitmap5413 = 5413;
+    public long fillRefBitmap5414 = 5414;
+    public long fillRefBitmap5415 = 5415;
+    public long fillRefBitmap5416 = 5416;
+    public long fillRefBitmap5417 = 5417;
+    public long fillRefBitmap5418 = 5418;
+    public long fillRefBitmap5419 = 5419;
+    public long fillRefBitmap5420 = 5420;
+    public long fillRefBitmap5421 = 5421;
+    public long fillRefBitmap5422 = 5422;
+    public long fillRefBitmap5423 = 5423;
+    public long fillRefBitmap5424 = 5424;
+    public long fillRefBitmap5425 = 5425;
+    public long fillRefBitmap5426 = 5426;
+    public long fillRefBitmap5427 = 5427;
+    public long fillRefBitmap5428 = 5428;
+    public long fillRefBitmap5429 = 5429;
+    public long fillRefBitmap5430 = 5430;
+    public long fillRefBitmap5431 = 5431;
+    public long fillRefBitmap5432 = 5432;
+    public long fillRefBitmap5433 = 5433;
+    public long fillRefBitmap5434 = 5434;
+    public long fillRefBitmap5435 = 5435;
+    public long fillRefBitmap5436 = 5436;
+    public long fillRefBitmap5437 = 5437;
+    public long fillRefBitmap5438 = 5438;
+    public long fillRefBitmap5439 = 5439;
+    public long fillRefBitmap5440 = 5440;
+    public long fillRefBitmap5441 = 5441;
+    public long fillRefBitmap5442 = 5442;
+    public long fillRefBitmap5443 = 5443;
+    public long fillRefBitmap5444 = 5444;
+    public long fillRefBitmap5445 = 5445;
+    public long fillRefBitmap5446 = 5446;
+    public long fillRefBitmap5447 = 5447;
+    public long fillRefBitmap5448 = 5448;
+    public long fillRefBitmap5449 = 5449;
+    public long fillRefBitmap5450 = 5450;
+    public long fillRefBitmap5451 = 5451;
+    public long fillRefBitmap5452 = 5452;
+    public long fillRefBitmap5453 = 5453;
+    public long fillRefBitmap5454 = 5454;
+    public long fillRefBitmap5455 = 5455;
+    public long fillRefBitmap5456 = 5456;
+    public long fillRefBitmap5457 = 5457;
+    public long fillRefBitmap5458 = 5458;
+    public long fillRefBitmap5459 = 5459;
+    public long fillRefBitmap5460 = 5460;
+    public long fillRefBitmap5461 = 5461;
+    public long fillRefBitmap5462 = 5462;
+    public long fillRefBitmap5463 = 5463;
+    public long fillRefBitmap5464 = 5464;
+    public long fillRefBitmap5465 = 5465;
+    public long fillRefBitmap5466 = 5466;
+    public long fillRefBitmap5467 = 5467;
+    public long fillRefBitmap5468 = 5468;
+    public long fillRefBitmap5469 = 5469;
+    public long fillRefBitmap5470 = 5470;
+    public long fillRefBitmap5471 = 5471;
+    public long fillRefBitmap5472 = 5472;
+    public long fillRefBitmap5473 = 5473;
+    public long fillRefBitmap5474 = 5474;
+    public long fillRefBitmap5475 = 5475;
+    public long fillRefBitmap5476 = 5476;
+    public long fillRefBitmap5477 = 5477;
+    public long fillRefBitmap5478 = 5478;
+    public long fillRefBitmap5479 = 5479;
+    public long fillRefBitmap5480 = 5480;
+    public long fillRefBitmap5481 = 5481;
+    public long fillRefBitmap5482 = 5482;
+    public long fillRefBitmap5483 = 5483;
+    public long fillRefBitmap5484 = 5484;
+    public long fillRefBitmap5485 = 5485;
+    public long fillRefBitmap5486 = 5486;
+    public long fillRefBitmap5487 = 5487;
+    public long fillRefBitmap5488 = 5488;
+    public long fillRefBitmap5489 = 5489;
+    public long fillRefBitmap5490 = 5490;
+    public long fillRefBitmap5491 = 5491;
+    public long fillRefBitmap5492 = 5492;
+    public long fillRefBitmap5493 = 5493;
+    public long fillRefBitmap5494 = 5494;
+    public long fillRefBitmap5495 = 5495;
+    public long fillRefBitmap5496 = 5496;
+    public long fillRefBitmap5497 = 5497;
+    public long fillRefBitmap5498 = 5498;
+    public long fillRefBitmap5499 = 5499;
+    public long fillRefBitmap5500 = 5500;
+    public long fillRefBitmap5501 = 5501;
+    public long fillRefBitmap5502 = 5502;
+    public long fillRefBitmap5503 = 5503;
+    public long fillRefBitmap5504 = 5504;
+    public long fillRefBitmap5505 = 5505;
+    public long fillRefBitmap5506 = 5506;
+    public long fillRefBitmap5507 = 5507;
+    public long fillRefBitmap5508 = 5508;
+    public long fillRefBitmap5509 = 5509;
+    public long fillRefBitmap5510 = 5510;
+    public long fillRefBitmap5511 = 5511;
+    public long fillRefBitmap5512 = 5512;
+    public long fillRefBitmap5513 = 5513;
+    public long fillRefBitmap5514 = 5514;
+    public long fillRefBitmap5515 = 5515;
+    public long fillRefBitmap5516 = 5516;
+    public long fillRefBitmap5517 = 5517;
+    public long fillRefBitmap5518 = 5518;
+    public long fillRefBitmap5519 = 5519;
+    public long fillRefBitmap5520 = 5520;
+    public long fillRefBitmap5521 = 5521;
+    public long fillRefBitmap5522 = 5522;
+    public long fillRefBitmap5523 = 5523;
+    public long fillRefBitmap5524 = 5524;
+    public long fillRefBitmap5525 = 5525;
+    public long fillRefBitmap5526 = 5526;
+    public long fillRefBitmap5527 = 5527;
+    public long fillRefBitmap5528 = 5528;
+    public long fillRefBitmap5529 = 5529;
+    public long fillRefBitmap5530 = 5530;
+    public long fillRefBitmap5531 = 5531;
+    public long fillRefBitmap5532 = 5532;
+    public long fillRefBitmap5533 = 5533;
+    public long fillRefBitmap5534 = 5534;
+    public long fillRefBitmap5535 = 5535;
+    public long fillRefBitmap5536 = 5536;
+    public long fillRefBitmap5537 = 5537;
+    public long fillRefBitmap5538 = 5538;
+    public long fillRefBitmap5539 = 5539;
+    public long fillRefBitmap5540 = 5540;
+    public long fillRefBitmap5541 = 5541;
+    public long fillRefBitmap5542 = 5542;
+    public long fillRefBitmap5543 = 5543;
+    public long fillRefBitmap5544 = 5544;
+    public long fillRefBitmap5545 = 5545;
+    public long fillRefBitmap5546 = 5546;
+    public long fillRefBitmap5547 = 5547;
+    public long fillRefBitmap5548 = 5548;
+    public long fillRefBitmap5549 = 5549;
+    public long fillRefBitmap5550 = 5550;
+    public long fillRefBitmap5551 = 5551;
+    public long fillRefBitmap5552 = 5552;
+    public long fillRefBitmap5553 = 5553;
+    public long fillRefBitmap5554 = 5554;
+    public long fillRefBitmap5555 = 5555;
+    public long fillRefBitmap5556 = 5556;
+    public long fillRefBitmap5557 = 5557;
+    public long fillRefBitmap5558 = 5558;
+    public long fillRefBitmap5559 = 5559;
+    public long fillRefBitmap5560 = 5560;
+    public long fillRefBitmap5561 = 5561;
+    public long fillRefBitmap5562 = 5562;
+    public long fillRefBitmap5563 = 5563;
+    public long fillRefBitmap5564 = 5564;
+    public long fillRefBitmap5565 = 5565;
+    public long fillRefBitmap5566 = 5566;
+    public long fillRefBitmap5567 = 5567;
+    public long fillRefBitmap5568 = 5568;
+    public long fillRefBitmap5569 = 5569;
+    public long fillRefBitmap5570 = 5570;
+    public long fillRefBitmap5571 = 5571;
+    public long fillRefBitmap5572 = 5572;
+    public long fillRefBitmap5573 = 5573;
+    public long fillRefBitmap5574 = 5574;
+    public long fillRefBitmap5575 = 5575;
+    public long fillRefBitmap5576 = 5576;
+    public long fillRefBitmap5577 = 5577;
+    public long fillRefBitmap5578 = 5578;
+    public long fillRefBitmap5579 = 5579;
+    public long fillRefBitmap5580 = 5580;
+    public long fillRefBitmap5581 = 5581;
+    public long fillRefBitmap5582 = 5582;
+    public long fillRefBitmap5583 = 5583;
+    public long fillRefBitmap5584 = 5584;
+    public long fillRefBitmap5585 = 5585;
+    public long fillRefBitmap5586 = 5586;
+    public long fillRefBitmap5587 = 5587;
+    public long fillRefBitmap5588 = 5588;
+    public long fillRefBitmap5589 = 5589;
+    public long fillRefBitmap5590 = 5590;
+    public long fillRefBitmap5591 = 5591;
+    public long fillRefBitmap5592 = 5592;
+    public long fillRefBitmap5593 = 5593;
+    public long fillRefBitmap5594 = 5594;
+    public long fillRefBitmap5595 = 5595;
+    public long fillRefBitmap5596 = 5596;
+    public long fillRefBitmap5597 = 5597;
+    public long fillRefBitmap5598 = 5598;
+    public long fillRefBitmap5599 = 5599;
+    public long fillRefBitmap5600 = 5600;
+    public long fillRefBitmap5601 = 5601;
+    public long fillRefBitmap5602 = 5602;
+    public long fillRefBitmap5603 = 5603;
+    public long fillRefBitmap5604 = 5604;
+    public long fillRefBitmap5605 = 5605;
+    public long fillRefBitmap5606 = 5606;
+    public long fillRefBitmap5607 = 5607;
+    public long fillRefBitmap5608 = 5608;
+    public long fillRefBitmap5609 = 5609;
+    public long fillRefBitmap5610 = 5610;
+    public long fillRefBitmap5611 = 5611;
+    public long fillRefBitmap5612 = 5612;
+    public long fillRefBitmap5613 = 5613;
+    public long fillRefBitmap5614 = 5614;
+    public long fillRefBitmap5615 = 5615;
+    public long fillRefBitmap5616 = 5616;
+    public long fillRefBitmap5617 = 5617;
+    public long fillRefBitmap5618 = 5618;
+    public long fillRefBitmap5619 = 5619;
+    public long fillRefBitmap5620 = 5620;
+    public long fillRefBitmap5621 = 5621;
+    public long fillRefBitmap5622 = 5622;
+    public long fillRefBitmap5623 = 5623;
+    public long fillRefBitmap5624 = 5624;
+    public long fillRefBitmap5625 = 5625;
+    public long fillRefBitmap5626 = 5626;
+    public long fillRefBitmap5627 = 5627;
+    public long fillRefBitmap5628 = 5628;
+    public long fillRefBitmap5629 = 5629;
+    public long fillRefBitmap5630 = 5630;
+    public long fillRefBitmap5631 = 5631;
+    public long fillRefBitmap5632 = 5632;
+    public long fillRefBitmap5633 = 5633;
+    public long fillRefBitmap5634 = 5634;
+    public long fillRefBitmap5635 = 5635;
+    public long fillRefBitmap5636 = 5636;
+    public long fillRefBitmap5637 = 5637;
+    public long fillRefBitmap5638 = 5638;
+    public long fillRefBitmap5639 = 5639;
+    public long fillRefBitmap5640 = 5640;
+    public long fillRefBitmap5641 = 5641;
+    public long fillRefBitmap5642 = 5642;
+    public long fillRefBitmap5643 = 5643;
+    public long fillRefBitmap5644 = 5644;
+    public long fillRefBitmap5645 = 5645;
+    public long fillRefBitmap5646 = 5646;
+    public long fillRefBitmap5647 = 5647;
+    public long fillRefBitmap5648 = 5648;
+    public long fillRefBitmap5649 = 5649;
+    public long fillRefBitmap5650 = 5650;
+    public long fillRefBitmap5651 = 5651;
+    public long fillRefBitmap5652 = 5652;
+    public long fillRefBitmap5653 = 5653;
+    public long fillRefBitmap5654 = 5654;
+    public long fillRefBitmap5655 = 5655;
+    public long fillRefBitmap5656 = 5656;
+    public long fillRefBitmap5657 = 5657;
+    public long fillRefBitmap5658 = 5658;
+    public long fillRefBitmap5659 = 5659;
+    public long fillRefBitmap5660 = 5660;
+    public long fillRefBitmap5661 = 5661;
+    public long fillRefBitmap5662 = 5662;
+    public long fillRefBitmap5663 = 5663;
+    public long fillRefBitmap5664 = 5664;
+    public long fillRefBitmap5665 = 5665;
+    public long fillRefBitmap5666 = 5666;
+    public long fillRefBitmap5667 = 5667;
+    public long fillRefBitmap5668 = 5668;
+    public long fillRefBitmap5669 = 5669;
+    public long fillRefBitmap5670 = 5670;
+    public long fillRefBitmap5671 = 5671;
+    public long fillRefBitmap5672 = 5672;
+    public long fillRefBitmap5673 = 5673;
+    public long fillRefBitmap5674 = 5674;
+    public long fillRefBitmap5675 = 5675;
+    public long fillRefBitmap5676 = 5676;
+    public long fillRefBitmap5677 = 5677;
+    public long fillRefBitmap5678 = 5678;
+    public long fillRefBitmap5679 = 5679;
+    public long fillRefBitmap5680 = 5680;
+    public long fillRefBitmap5681 = 5681;
+    public long fillRefBitmap5682 = 5682;
+    public long fillRefBitmap5683 = 5683;
+    public long fillRefBitmap5684 = 5684;
+    public long fillRefBitmap5685 = 5685;
+    public long fillRefBitmap5686 = 5686;
+    public long fillRefBitmap5687 = 5687;
+    public long fillRefBitmap5688 = 5688;
+    public long fillRefBitmap5689 = 5689;
+    public long fillRefBitmap5690 = 5690;
+    public long fillRefBitmap5691 = 5691;
+    public long fillRefBitmap5692 = 5692;
+    public long fillRefBitmap5693 = 5693;
+    public long fillRefBitmap5694 = 5694;
+    public long fillRefBitmap5695 = 5695;
+    public long fillRefBitmap5696 = 5696;
+    public long fillRefBitmap5697 = 5697;
+    public long fillRefBitmap5698 = 5698;
+    public long fillRefBitmap5699 = 5699;
+    public long fillRefBitmap5700 = 5700;
+    public long fillRefBitmap5701 = 5701;
+    public long fillRefBitmap5702 = 5702;
+    public long fillRefBitmap5703 = 5703;
+    public long fillRefBitmap5704 = 5704;
+    public long fillRefBitmap5705 = 5705;
+    public long fillRefBitmap5706 = 5706;
+    public long fillRefBitmap5707 = 5707;
+    public long fillRefBitmap5708 = 5708;
+    public long fillRefBitmap5709 = 5709;
+    public long fillRefBitmap5710 = 5710;
+    public long fillRefBitmap5711 = 5711;
+    public long fillRefBitmap5712 = 5712;
+    public long fillRefBitmap5713 = 5713;
+    public long fillRefBitmap5714 = 5714;
+    public long fillRefBitmap5715 = 5715;
+    public long fillRefBitmap5716 = 5716;
+    public long fillRefBitmap5717 = 5717;
+    public long fillRefBitmap5718 = 5718;
+    public long fillRefBitmap5719 = 5719;
+    public long fillRefBitmap5720 = 5720;
+    public long fillRefBitmap5721 = 5721;
+    public long fillRefBitmap5722 = 5722;
+    public long fillRefBitmap5723 = 5723;
+    public long fillRefBitmap5724 = 5724;
+    public long fillRefBitmap5725 = 5725;
+    public long fillRefBitmap5726 = 5726;
+    public long fillRefBitmap5727 = 5727;
+    public long fillRefBitmap5728 = 5728;
+    public long fillRefBitmap5729 = 5729;
+    public long fillRefBitmap5730 = 5730;
+    public long fillRefBitmap5731 = 5731;
+    public long fillRefBitmap5732 = 5732;
+    public long fillRefBitmap5733 = 5733;
+    public long fillRefBitmap5734 = 5734;
+    public long fillRefBitmap5735 = 5735;
+    public long fillRefBitmap5736 = 5736;
+    public long fillRefBitmap5737 = 5737;
+    public long fillRefBitmap5738 = 5738;
+    public long fillRefBitmap5739 = 5739;
+    public long fillRefBitmap5740 = 5740;
+    public long fillRefBitmap5741 = 5741;
+    public long fillRefBitmap5742 = 5742;
+    public long fillRefBitmap5743 = 5743;
+    public long fillRefBitmap5744 = 5744;
+    public long fillRefBitmap5745 = 5745;
+    public long fillRefBitmap5746 = 5746;
+    public long fillRefBitmap5747 = 5747;
+    public long fillRefBitmap5748 = 5748;
+    public long fillRefBitmap5749 = 5749;
+    public long fillRefBitmap5750 = 5750;
+    public long fillRefBitmap5751 = 5751;
+    public long fillRefBitmap5752 = 5752;
+    public long fillRefBitmap5753 = 5753;
+    public long fillRefBitmap5754 = 5754;
+    public long fillRefBitmap5755 = 5755;
+    public long fillRefBitmap5756 = 5756;
+    public long fillRefBitmap5757 = 5757;
+    public long fillRefBitmap5758 = 5758;
+    public long fillRefBitmap5759 = 5759;
+    public long fillRefBitmap5760 = 5760;
+    public long fillRefBitmap5761 = 5761;
+    public long fillRefBitmap5762 = 5762;
+    public long fillRefBitmap5763 = 5763;
+    public long fillRefBitmap5764 = 5764;
+    public long fillRefBitmap5765 = 5765;
+    public long fillRefBitmap5766 = 5766;
+    public long fillRefBitmap5767 = 5767;
+    public long fillRefBitmap5768 = 5768;
+    public long fillRefBitmap5769 = 5769;
+    public long fillRefBitmap5770 = 5770;
+    public long fillRefBitmap5771 = 5771;
+    public long fillRefBitmap5772 = 5772;
+    public long fillRefBitmap5773 = 5773;
+    public long fillRefBitmap5774 = 5774;
+    public long fillRefBitmap5775 = 5775;
+    public long fillRefBitmap5776 = 5776;
+    public long fillRefBitmap5777 = 5777;
+    public long fillRefBitmap5778 = 5778;
+    public long fillRefBitmap5779 = 5779;
+    public long fillRefBitmap5780 = 5780;
+    public long fillRefBitmap5781 = 5781;
+    public long fillRefBitmap5782 = 5782;
+    public long fillRefBitmap5783 = 5783;
+    public long fillRefBitmap5784 = 5784;
+    public long fillRefBitmap5785 = 5785;
+    public long fillRefBitmap5786 = 5786;
+    public long fillRefBitmap5787 = 5787;
+    public long fillRefBitmap5788 = 5788;
+    public long fillRefBitmap5789 = 5789;
+    public long fillRefBitmap5790 = 5790;
+    public long fillRefBitmap5791 = 5791;
+    public long fillRefBitmap5792 = 5792;
+    public long fillRefBitmap5793 = 5793;
+    public long fillRefBitmap5794 = 5794;
+    public long fillRefBitmap5795 = 5795;
+    public long fillRefBitmap5796 = 5796;
+    public long fillRefBitmap5797 = 5797;
+    public long fillRefBitmap5798 = 5798;
+    public long fillRefBitmap5799 = 5799;
+    public long fillRefBitmap5800 = 5800;
+    public long fillRefBitmap5801 = 5801;
+    public long fillRefBitmap5802 = 5802;
+    public long fillRefBitmap5803 = 5803;
+    public long fillRefBitmap5804 = 5804;
+    public long fillRefBitmap5805 = 5805;
+    public long fillRefBitmap5806 = 5806;
+    public long fillRefBitmap5807 = 5807;
+    public long fillRefBitmap5808 = 5808;
+    public long fillRefBitmap5809 = 5809;
+    public long fillRefBitmap5810 = 5810;
+    public long fillRefBitmap5811 = 5811;
+    public long fillRefBitmap5812 = 5812;
+    public long fillRefBitmap5813 = 5813;
+    public long fillRefBitmap5814 = 5814;
+    public long fillRefBitmap5815 = 5815;
+    public long fillRefBitmap5816 = 5816;
+    public long fillRefBitmap5817 = 5817;
+    public long fillRefBitmap5818 = 5818;
+    public long fillRefBitmap5819 = 5819;
+    public long fillRefBitmap5820 = 5820;
+    public long fillRefBitmap5821 = 5821;
+    public long fillRefBitmap5822 = 5822;
+    public long fillRefBitmap5823 = 5823;
+    public long fillRefBitmap5824 = 5824;
+    public long fillRefBitmap5825 = 5825;
+    public long fillRefBitmap5826 = 5826;
+    public long fillRefBitmap5827 = 5827;
+    public long fillRefBitmap5828 = 5828;
+    public long fillRefBitmap5829 = 5829;
+    public long fillRefBitmap5830 = 5830;
+    public long fillRefBitmap5831 = 5831;
+    public long fillRefBitmap5832 = 5832;
+    public long fillRefBitmap5833 = 5833;
+    public long fillRefBitmap5834 = 5834;
+    public long fillRefBitmap5835 = 5835;
+    public long fillRefBitmap5836 = 5836;
+    public long fillRefBitmap5837 = 5837;
+    public long fillRefBitmap5838 = 5838;
+    public long fillRefBitmap5839 = 5839;
+    public long fillRefBitmap5840 = 5840;
+    public long fillRefBitmap5841 = 5841;
+    public long fillRefBitmap5842 = 5842;
+    public long fillRefBitmap5843 = 5843;
+    public long fillRefBitmap5844 = 5844;
+    public long fillRefBitmap5845 = 5845;
+    public long fillRefBitmap5846 = 5846;
+    public long fillRefBitmap5847 = 5847;
+    public long fillRefBitmap5848 = 5848;
+    public long fillRefBitmap5849 = 5849;
+    public long fillRefBitmap5850 = 5850;
+    public long fillRefBitmap5851 = 5851;
+    public long fillRefBitmap5852 = 5852;
+    public long fillRefBitmap5853 = 5853;
+    public long fillRefBitmap5854 = 5854;
+    public long fillRefBitmap5855 = 5855;
+    public long fillRefBitmap5856 = 5856;
+    public long fillRefBitmap5857 = 5857;
+    public long fillRefBitmap5858 = 5858;
+    public long fillRefBitmap5859 = 5859;
+    public long fillRefBitmap5860 = 5860;
+    public long fillRefBitmap5861 = 5861;
+    public long fillRefBitmap5862 = 5862;
+    public long fillRefBitmap5863 = 5863;
+    public long fillRefBitmap5864 = 5864;
+    public long fillRefBitmap5865 = 5865;
+    public long fillRefBitmap5866 = 5866;
+    public long fillRefBitmap5867 = 5867;
+    public long fillRefBitmap5868 = 5868;
+    public long fillRefBitmap5869 = 5869;
+    public long fillRefBitmap5870 = 5870;
+    public long fillRefBitmap5871 = 5871;
+    public long fillRefBitmap5872 = 5872;
+    public long fillRefBitmap5873 = 5873;
+    public long fillRefBitmap5874 = 5874;
+    public long fillRefBitmap5875 = 5875;
+    public long fillRefBitmap5876 = 5876;
+    public long fillRefBitmap5877 = 5877;
+    public long fillRefBitmap5878 = 5878;
+    public long fillRefBitmap5879 = 5879;
+    public long fillRefBitmap5880 = 5880;
+    public long fillRefBitmap5881 = 5881;
+    public long fillRefBitmap5882 = 5882;
+    public long fillRefBitmap5883 = 5883;
+    public long fillRefBitmap5884 = 5884;
+    public long fillRefBitmap5885 = 5885;
+    public long fillRefBitmap5886 = 5886;
+    public long fillRefBitmap5887 = 5887;
+    public long fillRefBitmap5888 = 5888;
+    public long fillRefBitmap5889 = 5889;
+    public long fillRefBitmap5890 = 5890;
+    public long fillRefBitmap5891 = 5891;
+    public long fillRefBitmap5892 = 5892;
+    public long fillRefBitmap5893 = 5893;
+    public long fillRefBitmap5894 = 5894;
+    public long fillRefBitmap5895 = 5895;
+    public long fillRefBitmap5896 = 5896;
+    public long fillRefBitmap5897 = 5897;
+    public long fillRefBitmap5898 = 5898;
+    public long fillRefBitmap5899 = 5899;
+    public long fillRefBitmap5900 = 5900;
+    public long fillRefBitmap5901 = 5901;
+    public long fillRefBitmap5902 = 5902;
+    public long fillRefBitmap5903 = 5903;
+    public long fillRefBitmap5904 = 5904;
+    public long fillRefBitmap5905 = 5905;
+    public long fillRefBitmap5906 = 5906;
+    public long fillRefBitmap5907 = 5907;
+    public long fillRefBitmap5908 = 5908;
+    public long fillRefBitmap5909 = 5909;
+    public long fillRefBitmap5910 = 5910;
+    public long fillRefBitmap5911 = 5911;
+    public long fillRefBitmap5912 = 5912;
+    public long fillRefBitmap5913 = 5913;
+    public long fillRefBitmap5914 = 5914;
+    public long fillRefBitmap5915 = 5915;
+    public long fillRefBitmap5916 = 5916;
+    public long fillRefBitmap5917 = 5917;
+    public long fillRefBitmap5918 = 5918;
+    public long fillRefBitmap5919 = 5919;
+    public long fillRefBitmap5920 = 5920;
+    public long fillRefBitmap5921 = 5921;
+    public long fillRefBitmap5922 = 5922;
+    public long fillRefBitmap5923 = 5923;
+    public long fillRefBitmap5924 = 5924;
+    public long fillRefBitmap5925 = 5925;
+    public long fillRefBitmap5926 = 5926;
+    public long fillRefBitmap5927 = 5927;
+    public long fillRefBitmap5928 = 5928;
+    public long fillRefBitmap5929 = 5929;
+    public long fillRefBitmap5930 = 5930;
+    public long fillRefBitmap5931 = 5931;
+    public long fillRefBitmap5932 = 5932;
+    public long fillRefBitmap5933 = 5933;
+    public long fillRefBitmap5934 = 5934;
+    public long fillRefBitmap5935 = 5935;
+    public long fillRefBitmap5936 = 5936;
+    public long fillRefBitmap5937 = 5937;
+    public long fillRefBitmap5938 = 5938;
+    public long fillRefBitmap5939 = 5939;
+    public long fillRefBitmap5940 = 5940;
+    public long fillRefBitmap5941 = 5941;
+    public long fillRefBitmap5942 = 5942;
+    public long fillRefBitmap5943 = 5943;
+    public long fillRefBitmap5944 = 5944;
+    public long fillRefBitmap5945 = 5945;
+    public long fillRefBitmap5946 = 5946;
+    public long fillRefBitmap5947 = 5947;
+    public long fillRefBitmap5948 = 5948;
+    public long fillRefBitmap5949 = 5949;
+    public long fillRefBitmap5950 = 5950;
+    public long fillRefBitmap5951 = 5951;
+    public long fillRefBitmap5952 = 5952;
+    public long fillRefBitmap5953 = 5953;
+    public long fillRefBitmap5954 = 5954;
+    public long fillRefBitmap5955 = 5955;
+    public long fillRefBitmap5956 = 5956;
+    public long fillRefBitmap5957 = 5957;
+    public long fillRefBitmap5958 = 5958;
+    public long fillRefBitmap5959 = 5959;
+    public long fillRefBitmap5960 = 5960;
+    public long fillRefBitmap5961 = 5961;
+    public long fillRefBitmap5962 = 5962;
+    public long fillRefBitmap5963 = 5963;
+    public long fillRefBitmap5964 = 5964;
+    public long fillRefBitmap5965 = 5965;
+    public long fillRefBitmap5966 = 5966;
+    public long fillRefBitmap5967 = 5967;
+    public long fillRefBitmap5968 = 5968;
+    public long fillRefBitmap5969 = 5969;
+    public long fillRefBitmap5970 = 5970;
+    public long fillRefBitmap5971 = 5971;
+    public long fillRefBitmap5972 = 5972;
+    public long fillRefBitmap5973 = 5973;
+    public long fillRefBitmap5974 = 5974;
+    public long fillRefBitmap5975 = 5975;
+    public long fillRefBitmap5976 = 5976;
+    public long fillRefBitmap5977 = 5977;
+    public long fillRefBitmap5978 = 5978;
+    public long fillRefBitmap5979 = 5979;
+    public long fillRefBitmap5980 = 5980;
+    public long fillRefBitmap5981 = 5981;
+    public long fillRefBitmap5982 = 5982;
+    public long fillRefBitmap5983 = 5983;
+    public long fillRefBitmap5984 = 5984;
+    public long fillRefBitmap5985 = 5985;
+    public long fillRefBitmap5986 = 5986;
+    public long fillRefBitmap5987 = 5987;
+    public long fillRefBitmap5988 = 5988;
+    public long fillRefBitmap5989 = 5989;
+    public long fillRefBitmap5990 = 5990;
+    public long fillRefBitmap5991 = 5991;
+    public long fillRefBitmap5992 = 5992;
+    public long fillRefBitmap5993 = 5993;
+    public long fillRefBitmap5994 = 5994;
+    public long fillRefBitmap5995 = 5995;
+    public long fillRefBitmap5996 = 5996;
+    public long fillRefBitmap5997 = 5997;
+    public long fillRefBitmap5998 = 5998;
+    public long fillRefBitmap5999 = 5999;
+    public long fillRefBitmap6000 = 6000;
+    public long fillRefBitmap6001 = 6001;
+    public long fillRefBitmap6002 = 6002;
+    public long fillRefBitmap6003 = 6003;
+    public long fillRefBitmap6004 = 6004;
+    public long fillRefBitmap6005 = 6005;
+    public long fillRefBitmap6006 = 6006;
+    public long fillRefBitmap6007 = 6007;
+    public long fillRefBitmap6008 = 6008;
+    public long fillRefBitmap6009 = 6009;
+    public long fillRefBitmap6010 = 6010;
+    public long fillRefBitmap6011 = 6011;
+    public long fillRefBitmap6012 = 6012;
+    public long fillRefBitmap6013 = 6013;
+    public long fillRefBitmap6014 = 6014;
+    public long fillRefBitmap6015 = 6015;
+    public long fillRefBitmap6016 = 6016;
+    public long fillRefBitmap6017 = 6017;
+    public long fillRefBitmap6018 = 6018;
+    public long fillRefBitmap6019 = 6019;
+    public long fillRefBitmap6020 = 6020;
+    public long fillRefBitmap6021 = 6021;
+    public long fillRefBitmap6022 = 6022;
+    public long fillRefBitmap6023 = 6023;
+    public long fillRefBitmap6024 = 6024;
+    public long fillRefBitmap6025 = 6025;
+    public long fillRefBitmap6026 = 6026;
+    public long fillRefBitmap6027 = 6027;
+    public long fillRefBitmap6028 = 6028;
+    public long fillRefBitmap6029 = 6029;
+    public long fillRefBitmap6030 = 6030;
+    public long fillRefBitmap6031 = 6031;
+    public long fillRefBitmap6032 = 6032;
+    public long fillRefBitmap6033 = 6033;
+    public long fillRefBitmap6034 = 6034;
+    public long fillRefBitmap6035 = 6035;
+    public long fillRefBitmap6036 = 6036;
+    public long fillRefBitmap6037 = 6037;
+    public long fillRefBitmap6038 = 6038;
+    public long fillRefBitmap6039 = 6039;
+    public long fillRefBitmap6040 = 6040;
+    public long fillRefBitmap6041 = 6041;
+    public long fillRefBitmap6042 = 6042;
+    public long fillRefBitmap6043 = 6043;
+    public long fillRefBitmap6044 = 6044;
+    public long fillRefBitmap6045 = 6045;
+    public long fillRefBitmap6046 = 6046;
+    public long fillRefBitmap6047 = 6047;
+    public long fillRefBitmap6048 = 6048;
+    public long fillRefBitmap6049 = 6049;
+    public long fillRefBitmap6050 = 6050;
+    public long fillRefBitmap6051 = 6051;
+    public long fillRefBitmap6052 = 6052;
+    public long fillRefBitmap6053 = 6053;
+    public long fillRefBitmap6054 = 6054;
+    public long fillRefBitmap6055 = 6055;
+    public long fillRefBitmap6056 = 6056;
+    public long fillRefBitmap6057 = 6057;
+    public long fillRefBitmap6058 = 6058;
+    public long fillRefBitmap6059 = 6059;
+    public long fillRefBitmap6060 = 6060;
+    public long fillRefBitmap6061 = 6061;
+    public long fillRefBitmap6062 = 6062;
+    public long fillRefBitmap6063 = 6063;
+    public long fillRefBitmap6064 = 6064;
+    public long fillRefBitmap6065 = 6065;
+    public long fillRefBitmap6066 = 6066;
+    public long fillRefBitmap6067 = 6067;
+    public long fillRefBitmap6068 = 6068;
+    public long fillRefBitmap6069 = 6069;
+    public long fillRefBitmap6070 = 6070;
+    public long fillRefBitmap6071 = 6071;
+    public long fillRefBitmap6072 = 6072;
+    public long fillRefBitmap6073 = 6073;
+    public long fillRefBitmap6074 = 6074;
+    public long fillRefBitmap6075 = 6075;
+    public long fillRefBitmap6076 = 6076;
+    public long fillRefBitmap6077 = 6077;
+    public long fillRefBitmap6078 = 6078;
+    public long fillRefBitmap6079 = 6079;
+    public long fillRefBitmap6080 = 6080;
+    public long fillRefBitmap6081 = 6081;
+    public long fillRefBitmap6082 = 6082;
+    public long fillRefBitmap6083 = 6083;
+    public long fillRefBitmap6084 = 6084;
+    public long fillRefBitmap6085 = 6085;
+    public long fillRefBitmap6086 = 6086;
+    public long fillRefBitmap6087 = 6087;
+    public long fillRefBitmap6088 = 6088;
+    public long fillRefBitmap6089 = 6089;
+    public long fillRefBitmap6090 = 6090;
+    public long fillRefBitmap6091 = 6091;
+    public long fillRefBitmap6092 = 6092;
+    public long fillRefBitmap6093 = 6093;
+    public long fillRefBitmap6094 = 6094;
+    public long fillRefBitmap6095 = 6095;
+    public long fillRefBitmap6096 = 6096;
+    public long fillRefBitmap6097 = 6097;
+    public long fillRefBitmap6098 = 6098;
+    public long fillRefBitmap6099 = 6099;
+    public long fillRefBitmap6100 = 6100;
+    public long fillRefBitmap6101 = 6101;
+    public long fillRefBitmap6102 = 6102;
+    public long fillRefBitmap6103 = 6103;
+    public long fillRefBitmap6104 = 6104;
+    public long fillRefBitmap6105 = 6105;
+    public long fillRefBitmap6106 = 6106;
+    public long fillRefBitmap6107 = 6107;
+    public long fillRefBitmap6108 = 6108;
+    public long fillRefBitmap6109 = 6109;
+    public long fillRefBitmap6110 = 6110;
+    public long fillRefBitmap6111 = 6111;
+    public long fillRefBitmap6112 = 6112;
+    public long fillRefBitmap6113 = 6113;
+    public long fillRefBitmap6114 = 6114;
+    public long fillRefBitmap6115 = 6115;
+    public long fillRefBitmap6116 = 6116;
+    public long fillRefBitmap6117 = 6117;
+    public long fillRefBitmap6118 = 6118;
+    public long fillRefBitmap6119 = 6119;
+    public long fillRefBitmap6120 = 6120;
+    public long fillRefBitmap6121 = 6121;
+    public long fillRefBitmap6122 = 6122;
+    public long fillRefBitmap6123 = 6123;
+    public long fillRefBitmap6124 = 6124;
+    public long fillRefBitmap6125 = 6125;
+    public long fillRefBitmap6126 = 6126;
+    public long fillRefBitmap6127 = 6127;
+    public long fillRefBitmap6128 = 6128;
+    public long fillRefBitmap6129 = 6129;
+    public long fillRefBitmap6130 = 6130;
+    public long fillRefBitmap6131 = 6131;
+    public long fillRefBitmap6132 = 6132;
+    public long fillRefBitmap6133 = 6133;
+    public long fillRefBitmap6134 = 6134;
+    public long fillRefBitmap6135 = 6135;
+    public long fillRefBitmap6136 = 6136;
+    public long fillRefBitmap6137 = 6137;
+    public long fillRefBitmap6138 = 6138;
+    public long fillRefBitmap6139 = 6139;
+    public long fillRefBitmap6140 = 6140;
+    public long fillRefBitmap6141 = 6141;
+    public long fillRefBitmap6142 = 6142;
+    public long fillRefBitmap6143 = 6143;
+    public long fillRefBitmap6144 = 6144;
+    public long fillRefBitmap6145 = 6145;
+    public long fillRefBitmap6146 = 6146;
+    public long fillRefBitmap6147 = 6147;
+    public long fillRefBitmap6148 = 6148;
+    public long fillRefBitmap6149 = 6149;
+    public long fillRefBitmap6150 = 6150;
+    public long fillRefBitmap6151 = 6151;
+    public long fillRefBitmap6152 = 6152;
+    public long fillRefBitmap6153 = 6153;
+    public long fillRefBitmap6154 = 6154;
+    public long fillRefBitmap6155 = 6155;
+    public long fillRefBitmap6156 = 6156;
+    public long fillRefBitmap6157 = 6157;
+    public long fillRefBitmap6158 = 6158;
+    public long fillRefBitmap6159 = 6159;
+    public long fillRefBitmap6160 = 6160;
+    public long fillRefBitmap6161 = 6161;
+    public long fillRefBitmap6162 = 6162;
+    public long fillRefBitmap6163 = 6163;
+    public long fillRefBitmap6164 = 6164;
+    public long fillRefBitmap6165 = 6165;
+    public long fillRefBitmap6166 = 6166;
+    public long fillRefBitmap6167 = 6167;
+    public long fillRefBitmap6168 = 6168;
+    public long fillRefBitmap6169 = 6169;
+    public long fillRefBitmap6170 = 6170;
+    public long fillRefBitmap6171 = 6171;
+    public long fillRefBitmap6172 = 6172;
+    public long fillRefBitmap6173 = 6173;
+    public long fillRefBitmap6174 = 6174;
+    public long fillRefBitmap6175 = 6175;
+    public long fillRefBitmap6176 = 6176;
+    public long fillRefBitmap6177 = 6177;
+    public long fillRefBitmap6178 = 6178;
+    public long fillRefBitmap6179 = 6179;
+    public long fillRefBitmap6180 = 6180;
+    public long fillRefBitmap6181 = 6181;
+    public long fillRefBitmap6182 = 6182;
+    public long fillRefBitmap6183 = 6183;
+    public long fillRefBitmap6184 = 6184;
+    public long fillRefBitmap6185 = 6185;
+    public long fillRefBitmap6186 = 6186;
+    public long fillRefBitmap6187 = 6187;
+    public long fillRefBitmap6188 = 6188;
+    public long fillRefBitmap6189 = 6189;
+    public long fillRefBitmap6190 = 6190;
+    public long fillRefBitmap6191 = 6191;
+    public long fillRefBitmap6192 = 6192;
+    public long fillRefBitmap6193 = 6193;
+    public long fillRefBitmap6194 = 6194;
+    public long fillRefBitmap6195 = 6195;
+    public long fillRefBitmap6196 = 6196;
+    public long fillRefBitmap6197 = 6197;
+    public long fillRefBitmap6198 = 6198;
+    public long fillRefBitmap6199 = 6199;
+    public long fillRefBitmap6200 = 6200;
+    public long fillRefBitmap6201 = 6201;
+    public long fillRefBitmap6202 = 6202;
+    public long fillRefBitmap6203 = 6203;
+    public long fillRefBitmap6204 = 6204;
+    public long fillRefBitmap6205 = 6205;
+    public long fillRefBitmap6206 = 6206;
+    public long fillRefBitmap6207 = 6207;
+    public long fillRefBitmap6208 = 6208;
+    public long fillRefBitmap6209 = 6209;
+    public long fillRefBitmap6210 = 6210;
+    public long fillRefBitmap6211 = 6211;
+    public long fillRefBitmap6212 = 6212;
+    public long fillRefBitmap6213 = 6213;
+    public long fillRefBitmap6214 = 6214;
+    public long fillRefBitmap6215 = 6215;
+    public long fillRefBitmap6216 = 6216;
+    public long fillRefBitmap6217 = 6217;
+    public long fillRefBitmap6218 = 6218;
+    public long fillRefBitmap6219 = 6219;
+    public long fillRefBitmap6220 = 6220;
+    public long fillRefBitmap6221 = 6221;
+    public long fillRefBitmap6222 = 6222;
+    public long fillRefBitmap6223 = 6223;
+    public long fillRefBitmap6224 = 6224;
+    public long fillRefBitmap6225 = 6225;
+    public long fillRefBitmap6226 = 6226;
+    public long fillRefBitmap6227 = 6227;
+    public long fillRefBitmap6228 = 6228;
+    public long fillRefBitmap6229 = 6229;
+    public long fillRefBitmap6230 = 6230;
+    public long fillRefBitmap6231 = 6231;
+    public long fillRefBitmap6232 = 6232;
+    public long fillRefBitmap6233 = 6233;
+    public long fillRefBitmap6234 = 6234;
+    public long fillRefBitmap6235 = 6235;
+    public long fillRefBitmap6236 = 6236;
+    public long fillRefBitmap6237 = 6237;
+    public long fillRefBitmap6238 = 6238;
+    public long fillRefBitmap6239 = 6239;
+    public long fillRefBitmap6240 = 6240;
+    public long fillRefBitmap6241 = 6241;
+    public long fillRefBitmap6242 = 6242;
+    public long fillRefBitmap6243 = 6243;
+    public long fillRefBitmap6244 = 6244;
+    public long fillRefBitmap6245 = 6245;
+    public long fillRefBitmap6246 = 6246;
+    public long fillRefBitmap6247 = 6247;
+    public long fillRefBitmap6248 = 6248;
+    public long fillRefBitmap6249 = 6249;
+    public long fillRefBitmap6250 = 6250;
+    public long fillRefBitmap6251 = 6251;
+    public long fillRefBitmap6252 = 6252;
+    public long fillRefBitmap6253 = 6253;
+    public long fillRefBitmap6254 = 6254;
+    public long fillRefBitmap6255 = 6255;
+    public long fillRefBitmap6256 = 6256;
+    public long fillRefBitmap6257 = 6257;
+    public long fillRefBitmap6258 = 6258;
+    public long fillRefBitmap6259 = 6259;
+    public long fillRefBitmap6260 = 6260;
+    public long fillRefBitmap6261 = 6261;
+    public long fillRefBitmap6262 = 6262;
+    public long fillRefBitmap6263 = 6263;
+    public long fillRefBitmap6264 = 6264;
+    public long fillRefBitmap6265 = 6265;
+    public long fillRefBitmap6266 = 6266;
+    public long fillRefBitmap6267 = 6267;
+    public long fillRefBitmap6268 = 6268;
+    public long fillRefBitmap6269 = 6269;
+    public long fillRefBitmap6270 = 6270;
+    public long fillRefBitmap6271 = 6271;
+    public long fillRefBitmap6272 = 6272;
+    public long fillRefBitmap6273 = 6273;
+    public long fillRefBitmap6274 = 6274;
+    public long fillRefBitmap6275 = 6275;
+    public long fillRefBitmap6276 = 6276;
+    public long fillRefBitmap6277 = 6277;
+    public long fillRefBitmap6278 = 6278;
+    public long fillRefBitmap6279 = 6279;
+    public long fillRefBitmap6280 = 6280;
+    public long fillRefBitmap6281 = 6281;
+    public long fillRefBitmap6282 = 6282;
+    public long fillRefBitmap6283 = 6283;
+    public long fillRefBitmap6284 = 6284;
+    public long fillRefBitmap6285 = 6285;
+    public long fillRefBitmap6286 = 6286;
+    public long fillRefBitmap6287 = 6287;
+    public long fillRefBitmap6288 = 6288;
+    public long fillRefBitmap6289 = 6289;
+    public long fillRefBitmap6290 = 6290;
+    public long fillRefBitmap6291 = 6291;
+    public long fillRefBitmap6292 = 6292;
+    public long fillRefBitmap6293 = 6293;
+    public long fillRefBitmap6294 = 6294;
+    public long fillRefBitmap6295 = 6295;
+    public long fillRefBitmap6296 = 6296;
+    public long fillRefBitmap6297 = 6297;
+    public long fillRefBitmap6298 = 6298;
+    public long fillRefBitmap6299 = 6299;
+    public long fillRefBitmap6300 = 6300;
+    public long fillRefBitmap6301 = 6301;
+    public long fillRefBitmap6302 = 6302;
+    public long fillRefBitmap6303 = 6303;
+    public long fillRefBitmap6304 = 6304;
+    public long fillRefBitmap6305 = 6305;
+    public long fillRefBitmap6306 = 6306;
+    public long fillRefBitmap6307 = 6307;
+    public long fillRefBitmap6308 = 6308;
+    public long fillRefBitmap6309 = 6309;
+    public long fillRefBitmap6310 = 6310;
+    public long fillRefBitmap6311 = 6311;
+    public long fillRefBitmap6312 = 6312;
+    public long fillRefBitmap6313 = 6313;
+    public long fillRefBitmap6314 = 6314;
+    public long fillRefBitmap6315 = 6315;
+    public long fillRefBitmap6316 = 6316;
+    public long fillRefBitmap6317 = 6317;
+    public long fillRefBitmap6318 = 6318;
+    public long fillRefBitmap6319 = 6319;
+    public long fillRefBitmap6320 = 6320;
+    public long fillRefBitmap6321 = 6321;
+    public long fillRefBitmap6322 = 6322;
+    public long fillRefBitmap6323 = 6323;
+    public long fillRefBitmap6324 = 6324;
+    public long fillRefBitmap6325 = 6325;
+    public long fillRefBitmap6326 = 6326;
+    public long fillRefBitmap6327 = 6327;
+    public long fillRefBitmap6328 = 6328;
+    public long fillRefBitmap6329 = 6329;
+    public long fillRefBitmap6330 = 6330;
+    public long fillRefBitmap6331 = 6331;
+    public long fillRefBitmap6332 = 6332;
+    public long fillRefBitmap6333 = 6333;
+    public long fillRefBitmap6334 = 6334;
+    public long fillRefBitmap6335 = 6335;
+    public long fillRefBitmap6336 = 6336;
+    public long fillRefBitmap6337 = 6337;
+    public long fillRefBitmap6338 = 6338;
+    public long fillRefBitmap6339 = 6339;
+    public long fillRefBitmap6340 = 6340;
+    public long fillRefBitmap6341 = 6341;
+    public long fillRefBitmap6342 = 6342;
+    public long fillRefBitmap6343 = 6343;
+    public long fillRefBitmap6344 = 6344;
+    public long fillRefBitmap6345 = 6345;
+    public long fillRefBitmap6346 = 6346;
+    public long fillRefBitmap6347 = 6347;
+    public long fillRefBitmap6348 = 6348;
+    public long fillRefBitmap6349 = 6349;
+    public long fillRefBitmap6350 = 6350;
+    public long fillRefBitmap6351 = 6351;
+    public long fillRefBitmap6352 = 6352;
+    public long fillRefBitmap6353 = 6353;
+    public long fillRefBitmap6354 = 6354;
+    public long fillRefBitmap6355 = 6355;
+    public long fillRefBitmap6356 = 6356;
+    public long fillRefBitmap6357 = 6357;
+    public long fillRefBitmap6358 = 6358;
+    public long fillRefBitmap6359 = 6359;
+    public long fillRefBitmap6360 = 6360;
+    public long fillRefBitmap6361 = 6361;
+    public long fillRefBitmap6362 = 6362;
+    public long fillRefBitmap6363 = 6363;
+    public long fillRefBitmap6364 = 6364;
+    public long fillRefBitmap6365 = 6365;
+    public long fillRefBitmap6366 = 6366;
+    public long fillRefBitmap6367 = 6367;
+    public long fillRefBitmap6368 = 6368;
+    public long fillRefBitmap6369 = 6369;
+    public long fillRefBitmap6370 = 6370;
+    public long fillRefBitmap6371 = 6371;
+    public long fillRefBitmap6372 = 6372;
+    public long fillRefBitmap6373 = 6373;
+    public long fillRefBitmap6374 = 6374;
+    public long fillRefBitmap6375 = 6375;
+    public long fillRefBitmap6376 = 6376;
+    public long fillRefBitmap6377 = 6377;
+    public long fillRefBitmap6378 = 6378;
+    public long fillRefBitmap6379 = 6379;
+    public long fillRefBitmap6380 = 6380;
+    public long fillRefBitmap6381 = 6381;
+    public long fillRefBitmap6382 = 6382;
+    public long fillRefBitmap6383 = 6383;
+    public long fillRefBitmap6384 = 6384;
+    public long fillRefBitmap6385 = 6385;
+    public long fillRefBitmap6386 = 6386;
+    public long fillRefBitmap6387 = 6387;
+    public long fillRefBitmap6388 = 6388;
+    public long fillRefBitmap6389 = 6389;
+    public long fillRefBitmap6390 = 6390;
+    public long fillRefBitmap6391 = 6391;
+    public long fillRefBitmap6392 = 6392;
+    public long fillRefBitmap6393 = 6393;
+    public long fillRefBitmap6394 = 6394;
+    public long fillRefBitmap6395 = 6395;
+    public long fillRefBitmap6396 = 6396;
+    public long fillRefBitmap6397 = 6397;
+    public long fillRefBitmap6398 = 6398;
+    public long fillRefBitmap6399 = 6399;
+    public long fillRefBitmap6400 = 6400;
+    public long fillRefBitmap6401 = 6401;
+    public long fillRefBitmap6402 = 6402;
+    public long fillRefBitmap6403 = 6403;
+    public long fillRefBitmap6404 = 6404;
+    public long fillRefBitmap6405 = 6405;
+    public long fillRefBitmap6406 = 6406;
+    public long fillRefBitmap6407 = 6407;
+    public long fillRefBitmap6408 = 6408;
+    public long fillRefBitmap6409 = 6409;
+    public long fillRefBitmap6410 = 6410;
+    public long fillRefBitmap6411 = 6411;
+    public long fillRefBitmap6412 = 6412;
+    public long fillRefBitmap6413 = 6413;
+    public long fillRefBitmap6414 = 6414;
+    public long fillRefBitmap6415 = 6415;
+    public long fillRefBitmap6416 = 6416;
+    public long fillRefBitmap6417 = 6417;
+    public long fillRefBitmap6418 = 6418;
+    public long fillRefBitmap6419 = 6419;
+    public long fillRefBitmap6420 = 6420;
+    public long fillRefBitmap6421 = 6421;
+    public long fillRefBitmap6422 = 6422;
+    public long fillRefBitmap6423 = 6423;
+    public long fillRefBitmap6424 = 6424;
+    public long fillRefBitmap6425 = 6425;
+    public long fillRefBitmap6426 = 6426;
+    public long fillRefBitmap6427 = 6427;
+    public long fillRefBitmap6428 = 6428;
+    public long fillRefBitmap6429 = 6429;
+    public long fillRefBitmap6430 = 6430;
+    public long fillRefBitmap6431 = 6431;
+    public long fillRefBitmap6432 = 6432;
+    public long fillRefBitmap6433 = 6433;
+    public long fillRefBitmap6434 = 6434;
+    public long fillRefBitmap6435 = 6435;
+    public long fillRefBitmap6436 = 6436;
+    public long fillRefBitmap6437 = 6437;
+    public long fillRefBitmap6438 = 6438;
+    public long fillRefBitmap6439 = 6439;
+    public long fillRefBitmap6440 = 6440;
+    public long fillRefBitmap6441 = 6441;
+    public long fillRefBitmap6442 = 6442;
+    public long fillRefBitmap6443 = 6443;
+    public long fillRefBitmap6444 = 6444;
+    public long fillRefBitmap6445 = 6445;
+    public long fillRefBitmap6446 = 6446;
+    public long fillRefBitmap6447 = 6447;
+    public long fillRefBitmap6448 = 6448;
+    public long fillRefBitmap6449 = 6449;
+    public long fillRefBitmap6450 = 6450;
+    public long fillRefBitmap6451 = 6451;
+    public long fillRefBitmap6452 = 6452;
+    public long fillRefBitmap6453 = 6453;
+    public long fillRefBitmap6454 = 6454;
+    public long fillRefBitmap6455 = 6455;
+    public long fillRefBitmap6456 = 6456;
+    public long fillRefBitmap6457 = 6457;
+    public long fillRefBitmap6458 = 6458;
+    public long fillRefBitmap6459 = 6459;
+    public long fillRefBitmap6460 = 6460;
+    public long fillRefBitmap6461 = 6461;
+    public long fillRefBitmap6462 = 6462;
+    public long fillRefBitmap6463 = 6463;
+    public long fillRefBitmap6464 = 6464;
+    public long fillRefBitmap6465 = 6465;
+    public long fillRefBitmap6466 = 6466;
+    public long fillRefBitmap6467 = 6467;
+    public long fillRefBitmap6468 = 6468;
+    public long fillRefBitmap6469 = 6469;
+    public long fillRefBitmap6470 = 6470;
+    public long fillRefBitmap6471 = 6471;
+    public long fillRefBitmap6472 = 6472;
+    public long fillRefBitmap6473 = 6473;
+    public long fillRefBitmap6474 = 6474;
+    public long fillRefBitmap6475 = 6475;
+    public long fillRefBitmap6476 = 6476;
+    public long fillRefBitmap6477 = 6477;
+    public long fillRefBitmap6478 = 6478;
+    public long fillRefBitmap6479 = 6479;
+    public long fillRefBitmap6480 = 6480;
+    public long fillRefBitmap6481 = 6481;
+    public long fillRefBitmap6482 = 6482;
+    public long fillRefBitmap6483 = 6483;
+    public long fillRefBitmap6484 = 6484;
+    public long fillRefBitmap6485 = 6485;
+    public long fillRefBitmap6486 = 6486;
+    public long fillRefBitmap6487 = 6487;
+    public long fillRefBitmap6488 = 6488;
+    public long fillRefBitmap6489 = 6489;
+    public long fillRefBitmap6490 = 6490;
+    public long fillRefBitmap6491 = 6491;
+    public long fillRefBitmap6492 = 6492;
+    public long fillRefBitmap6493 = 6493;
+    public long fillRefBitmap6494 = 6494;
+    public long fillRefBitmap6495 = 6495;
+    public long fillRefBitmap6496 = 6496;
+    public long fillRefBitmap6497 = 6497;
+    public long fillRefBitmap6498 = 6498;
+    public long fillRefBitmap6499 = 6499;
+    public long fillRefBitmap6500 = 6500;
+    public long fillRefBitmap6501 = 6501;
+    public long fillRefBitmap6502 = 6502;
+    public long fillRefBitmap6503 = 6503;
+    public long fillRefBitmap6504 = 6504;
+    public long fillRefBitmap6505 = 6505;
+    public long fillRefBitmap6506 = 6506;
+    public long fillRefBitmap6507 = 6507;
+    public long fillRefBitmap6508 = 6508;
+    public long fillRefBitmap6509 = 6509;
+    public long fillRefBitmap6510 = 6510;
+    public long fillRefBitmap6511 = 6511;
+    public long fillRefBitmap6512 = 6512;
+    public long fillRefBitmap6513 = 6513;
+    public long fillRefBitmap6514 = 6514;
+    public long fillRefBitmap6515 = 6515;
+    public long fillRefBitmap6516 = 6516;
+    public long fillRefBitmap6517 = 6517;
+    public long fillRefBitmap6518 = 6518;
+    public long fillRefBitmap6519 = 6519;
+    public long fillRefBitmap6520 = 6520;
+    public long fillRefBitmap6521 = 6521;
+    public long fillRefBitmap6522 = 6522;
+    public long fillRefBitmap6523 = 6523;
+    public long fillRefBitmap6524 = 6524;
+    public long fillRefBitmap6525 = 6525;
+    public long fillRefBitmap6526 = 6526;
+    public long fillRefBitmap6527 = 6527;
+    public long fillRefBitmap6528 = 6528;
+    public long fillRefBitmap6529 = 6529;
+    public long fillRefBitmap6530 = 6530;
+    public long fillRefBitmap6531 = 6531;
+    public long fillRefBitmap6532 = 6532;
+    public long fillRefBitmap6533 = 6533;
+    public long fillRefBitmap6534 = 6534;
+    public long fillRefBitmap6535 = 6535;
+    public long fillRefBitmap6536 = 6536;
+    public long fillRefBitmap6537 = 6537;
+    public long fillRefBitmap6538 = 6538;
+    public long fillRefBitmap6539 = 6539;
+    public long fillRefBitmap6540 = 6540;
+    public long fillRefBitmap6541 = 6541;
+    public long fillRefBitmap6542 = 6542;
+    public long fillRefBitmap6543 = 6543;
+    public long fillRefBitmap6544 = 6544;
+    public long fillRefBitmap6545 = 6545;
+    public long fillRefBitmap6546 = 6546;
+    public long fillRefBitmap6547 = 6547;
+    public long fillRefBitmap6548 = 6548;
+    public long fillRefBitmap6549 = 6549;
+    public long fillRefBitmap6550 = 6550;
+    public long fillRefBitmap6551 = 6551;
+    public long fillRefBitmap6552 = 6552;
+    public long fillRefBitmap6553 = 6553;
+    public long fillRefBitmap6554 = 6554;
+    public long fillRefBitmap6555 = 6555;
+    public long fillRefBitmap6556 = 6556;
+    public long fillRefBitmap6557 = 6557;
+    public long fillRefBitmap6558 = 6558;
+    public long fillRefBitmap6559 = 6559;
+    public long fillRefBitmap6560 = 6560;
+    public long fillRefBitmap6561 = 6561;
+    public long fillRefBitmap6562 = 6562;
+    public long fillRefBitmap6563 = 6563;
+    public long fillRefBitmap6564 = 6564;
+    public long fillRefBitmap6565 = 6565;
+    public long fillRefBitmap6566 = 6566;
+    public long fillRefBitmap6567 = 6567;
+    public long fillRefBitmap6568 = 6568;
+    public long fillRefBitmap6569 = 6569;
+    public long fillRefBitmap6570 = 6570;
+    public long fillRefBitmap6571 = 6571;
+    public long fillRefBitmap6572 = 6572;
+    public long fillRefBitmap6573 = 6573;
+    public long fillRefBitmap6574 = 6574;
+    public long fillRefBitmap6575 = 6575;
+    public long fillRefBitmap6576 = 6576;
+    public long fillRefBitmap6577 = 6577;
+    public long fillRefBitmap6578 = 6578;
+    public long fillRefBitmap6579 = 6579;
+    public long fillRefBitmap6580 = 6580;
+    public long fillRefBitmap6581 = 6581;
+    public long fillRefBitmap6582 = 6582;
+    public long fillRefBitmap6583 = 6583;
+    public long fillRefBitmap6584 = 6584;
+    public long fillRefBitmap6585 = 6585;
+    public long fillRefBitmap6586 = 6586;
+    public long fillRefBitmap6587 = 6587;
+    public long fillRefBitmap6588 = 6588;
+    public long fillRefBitmap6589 = 6589;
+    public long fillRefBitmap6590 = 6590;
+    public long fillRefBitmap6591 = 6591;
+    public long fillRefBitmap6592 = 6592;
+    public long fillRefBitmap6593 = 6593;
+    public long fillRefBitmap6594 = 6594;
+    public long fillRefBitmap6595 = 6595;
+    public long fillRefBitmap6596 = 6596;
+    public long fillRefBitmap6597 = 6597;
+    public long fillRefBitmap6598 = 6598;
+    public long fillRefBitmap6599 = 6599;
+    public long fillRefBitmap6600 = 6600;
+    public long fillRefBitmap6601 = 6601;
+    public long fillRefBitmap6602 = 6602;
+    public long fillRefBitmap6603 = 6603;
+    public long fillRefBitmap6604 = 6604;
+    public long fillRefBitmap6605 = 6605;
+    public long fillRefBitmap6606 = 6606;
+    public long fillRefBitmap6607 = 6607;
+    public long fillRefBitmap6608 = 6608;
+    public long fillRefBitmap6609 = 6609;
+    public long fillRefBitmap6610 = 6610;
+    public long fillRefBitmap6611 = 6611;
+    public long fillRefBitmap6612 = 6612;
+    public long fillRefBitmap6613 = 6613;
+    public long fillRefBitmap6614 = 6614;
+    public long fillRefBitmap6615 = 6615;
+    public long fillRefBitmap6616 = 6616;
+    public long fillRefBitmap6617 = 6617;
+    public long fillRefBitmap6618 = 6618;
+    public long fillRefBitmap6619 = 6619;
+    public long fillRefBitmap6620 = 6620;
+    public long fillRefBitmap6621 = 6621;
+    public long fillRefBitmap6622 = 6622;
+    public long fillRefBitmap6623 = 6623;
+    public long fillRefBitmap6624 = 6624;
+    public long fillRefBitmap6625 = 6625;
+    public long fillRefBitmap6626 = 6626;
+    public long fillRefBitmap6627 = 6627;
+    public long fillRefBitmap6628 = 6628;
+    public long fillRefBitmap6629 = 6629;
+    public long fillRefBitmap6630 = 6630;
+    public long fillRefBitmap6631 = 6631;
+    public long fillRefBitmap6632 = 6632;
+    public long fillRefBitmap6633 = 6633;
+    public long fillRefBitmap6634 = 6634;
+    public long fillRefBitmap6635 = 6635;
+    public long fillRefBitmap6636 = 6636;
+    public long fillRefBitmap6637 = 6637;
+    public long fillRefBitmap6638 = 6638;
+    public long fillRefBitmap6639 = 6639;
+    public long fillRefBitmap6640 = 6640;
+    public long fillRefBitmap6641 = 6641;
+    public long fillRefBitmap6642 = 6642;
+    public long fillRefBitmap6643 = 6643;
+    public long fillRefBitmap6644 = 6644;
+    public long fillRefBitmap6645 = 6645;
+    public long fillRefBitmap6646 = 6646;
+    public long fillRefBitmap6647 = 6647;
+    public long fillRefBitmap6648 = 6648;
+    public long fillRefBitmap6649 = 6649;
+    public long fillRefBitmap6650 = 6650;
+    public long fillRefBitmap6651 = 6651;
+    public long fillRefBitmap6652 = 6652;
+    public long fillRefBitmap6653 = 6653;
+    public long fillRefBitmap6654 = 6654;
+    public long fillRefBitmap6655 = 6655;
+    public long fillRefBitmap6656 = 6656;
+    public long fillRefBitmap6657 = 6657;
+    public long fillRefBitmap6658 = 6658;
+    public long fillRefBitmap6659 = 6659;
+    public long fillRefBitmap6660 = 6660;
+    public long fillRefBitmap6661 = 6661;
+    public long fillRefBitmap6662 = 6662;
+    public long fillRefBitmap6663 = 6663;
+    public long fillRefBitmap6664 = 6664;
+    public long fillRefBitmap6665 = 6665;
+    public long fillRefBitmap6666 = 6666;
+    public long fillRefBitmap6667 = 6667;
+    public long fillRefBitmap6668 = 6668;
+    public long fillRefBitmap6669 = 6669;
+    public long fillRefBitmap6670 = 6670;
+    public long fillRefBitmap6671 = 6671;
+    public long fillRefBitmap6672 = 6672;
+    public long fillRefBitmap6673 = 6673;
+    public long fillRefBitmap6674 = 6674;
+    public long fillRefBitmap6675 = 6675;
+    public long fillRefBitmap6676 = 6676;
+    public long fillRefBitmap6677 = 6677;
+    public long fillRefBitmap6678 = 6678;
+    public long fillRefBitmap6679 = 6679;
+    public long fillRefBitmap6680 = 6680;
+    public long fillRefBitmap6681 = 6681;
+    public long fillRefBitmap6682 = 6682;
+    public long fillRefBitmap6683 = 6683;
+    public long fillRefBitmap6684 = 6684;
+    public long fillRefBitmap6685 = 6685;
+    public long fillRefBitmap6686 = 6686;
+    public long fillRefBitmap6687 = 6687;
+    public long fillRefBitmap6688 = 6688;
+    public long fillRefBitmap6689 = 6689;
+    public long fillRefBitmap6690 = 6690;
+    public long fillRefBitmap6691 = 6691;
+    public long fillRefBitmap6692 = 6692;
+    public long fillRefBitmap6693 = 6693;
+    public long fillRefBitmap6694 = 6694;
+    public long fillRefBitmap6695 = 6695;
+    public long fillRefBitmap6696 = 6696;
+    public long fillRefBitmap6697 = 6697;
+    public long fillRefBitmap6698 = 6698;
+    public long fillRefBitmap6699 = 6699;
+    public long fillRefBitmap6700 = 6700;
+    public long fillRefBitmap6701 = 6701;
+    public long fillRefBitmap6702 = 6702;
+    public long fillRefBitmap6703 = 6703;
+    public long fillRefBitmap6704 = 6704;
+    public long fillRefBitmap6705 = 6705;
+    public long fillRefBitmap6706 = 6706;
+    public long fillRefBitmap6707 = 6707;
+    public long fillRefBitmap6708 = 6708;
+    public long fillRefBitmap6709 = 6709;
+    public long fillRefBitmap6710 = 6710;
+    public long fillRefBitmap6711 = 6711;
+    public long fillRefBitmap6712 = 6712;
+    public long fillRefBitmap6713 = 6713;
+    public long fillRefBitmap6714 = 6714;
+    public long fillRefBitmap6715 = 6715;
+    public long fillRefBitmap6716 = 6716;
+    public long fillRefBitmap6717 = 6717;
+    public long fillRefBitmap6718 = 6718;
+    public long fillRefBitmap6719 = 6719;
+    public long fillRefBitmap6720 = 6720;
+    public long fillRefBitmap6721 = 6721;
+    public long fillRefBitmap6722 = 6722;
+    public long fillRefBitmap6723 = 6723;
+    public long fillRefBitmap6724 = 6724;
+    public long fillRefBitmap6725 = 6725;
+    public long fillRefBitmap6726 = 6726;
+    public long fillRefBitmap6727 = 6727;
+    public long fillRefBitmap6728 = 6728;
+    public long fillRefBitmap6729 = 6729;
+    public long fillRefBitmap6730 = 6730;
+    public long fillRefBitmap6731 = 6731;
+    public long fillRefBitmap6732 = 6732;
+    public long fillRefBitmap6733 = 6733;
+    public long fillRefBitmap6734 = 6734;
+    public long fillRefBitmap6735 = 6735;
+    public long fillRefBitmap6736 = 6736;
+    public long fillRefBitmap6737 = 6737;
+    public long fillRefBitmap6738 = 6738;
+    public long fillRefBitmap6739 = 6739;
+    public long fillRefBitmap6740 = 6740;
+    public long fillRefBitmap6741 = 6741;
+    public long fillRefBitmap6742 = 6742;
+    public long fillRefBitmap6743 = 6743;
+    public long fillRefBitmap6744 = 6744;
+    public long fillRefBitmap6745 = 6745;
+    public long fillRefBitmap6746 = 6746;
+    public long fillRefBitmap6747 = 6747;
+    public long fillRefBitmap6748 = 6748;
+    public long fillRefBitmap6749 = 6749;
+    public long fillRefBitmap6750 = 6750;
+    public long fillRefBitmap6751 = 6751;
+    public long fillRefBitmap6752 = 6752;
+    public long fillRefBitmap6753 = 6753;
+    public long fillRefBitmap6754 = 6754;
+    public long fillRefBitmap6755 = 6755;
+    public long fillRefBitmap6756 = 6756;
+    public long fillRefBitmap6757 = 6757;
+    public long fillRefBitmap6758 = 6758;
+    public long fillRefBitmap6759 = 6759;
+    public long fillRefBitmap6760 = 6760;
+    public long fillRefBitmap6761 = 6761;
+    public long fillRefBitmap6762 = 6762;
+    public long fillRefBitmap6763 = 6763;
+    public long fillRefBitmap6764 = 6764;
+    public long fillRefBitmap6765 = 6765;
+    public long fillRefBitmap6766 = 6766;
+    public long fillRefBitmap6767 = 6767;
+    public long fillRefBitmap6768 = 6768;
+    public long fillRefBitmap6769 = 6769;
+    public long fillRefBitmap6770 = 6770;
+    public long fillRefBitmap6771 = 6771;
+    public long fillRefBitmap6772 = 6772;
+    public long fillRefBitmap6773 = 6773;
+    public long fillRefBitmap6774 = 6774;
+    public long fillRefBitmap6775 = 6775;
+    public long fillRefBitmap6776 = 6776;
+    public long fillRefBitmap6777 = 6777;
+    public long fillRefBitmap6778 = 6778;
+    public long fillRefBitmap6779 = 6779;
+    public long fillRefBitmap6780 = 6780;
+    public long fillRefBitmap6781 = 6781;
+    public long fillRefBitmap6782 = 6782;
+    public long fillRefBitmap6783 = 6783;
+    public long fillRefBitmap6784 = 6784;
+    public long fillRefBitmap6785 = 6785;
+    public long fillRefBitmap6786 = 6786;
+    public long fillRefBitmap6787 = 6787;
+    public long fillRefBitmap6788 = 6788;
+    public long fillRefBitmap6789 = 6789;
+    public long fillRefBitmap6790 = 6790;
+    public long fillRefBitmap6791 = 6791;
+    public long fillRefBitmap6792 = 6792;
+    public long fillRefBitmap6793 = 6793;
+    public long fillRefBitmap6794 = 6794;
+    public long fillRefBitmap6795 = 6795;
+    public long fillRefBitmap6796 = 6796;
+    public long fillRefBitmap6797 = 6797;
+    public long fillRefBitmap6798 = 6798;
+    public long fillRefBitmap6799 = 6799;
+    public long fillRefBitmap6800 = 6800;
+    public long fillRefBitmap6801 = 6801;
+    public long fillRefBitmap6802 = 6802;
+    public long fillRefBitmap6803 = 6803;
+    public long fillRefBitmap6804 = 6804;
+    public long fillRefBitmap6805 = 6805;
+    public long fillRefBitmap6806 = 6806;
+    public long fillRefBitmap6807 = 6807;
+    public long fillRefBitmap6808 = 6808;
+    public long fillRefBitmap6809 = 6809;
+    public long fillRefBitmap6810 = 6810;
+    public long fillRefBitmap6811 = 6811;
+    public long fillRefBitmap6812 = 6812;
+    public long fillRefBitmap6813 = 6813;
+    public long fillRefBitmap6814 = 6814;
+    public long fillRefBitmap6815 = 6815;
+    public long fillRefBitmap6816 = 6816;
+    public long fillRefBitmap6817 = 6817;
+    public long fillRefBitmap6818 = 6818;
+    public long fillRefBitmap6819 = 6819;
+    public long fillRefBitmap6820 = 6820;
+    public long fillRefBitmap6821 = 6821;
+    public long fillRefBitmap6822 = 6822;
+    public long fillRefBitmap6823 = 6823;
+    public long fillRefBitmap6824 = 6824;
+    public long fillRefBitmap6825 = 6825;
+    public long fillRefBitmap6826 = 6826;
+    public long fillRefBitmap6827 = 6827;
+    public long fillRefBitmap6828 = 6828;
+    public long fillRefBitmap6829 = 6829;
+    public long fillRefBitmap6830 = 6830;
+    public long fillRefBitmap6831 = 6831;
+    public long fillRefBitmap6832 = 6832;
+    public long fillRefBitmap6833 = 6833;
+    public long fillRefBitmap6834 = 6834;
+    public long fillRefBitmap6835 = 6835;
+    public long fillRefBitmap6836 = 6836;
+    public long fillRefBitmap6837 = 6837;
+    public long fillRefBitmap6838 = 6838;
+    public long fillRefBitmap6839 = 6839;
+    public long fillRefBitmap6840 = 6840;
+    public long fillRefBitmap6841 = 6841;
+    public long fillRefBitmap6842 = 6842;
+    public long fillRefBitmap6843 = 6843;
+    public long fillRefBitmap6844 = 6844;
+    public long fillRefBitmap6845 = 6845;
+    public long fillRefBitmap6846 = 6846;
+    public long fillRefBitmap6847 = 6847;
+    public long fillRefBitmap6848 = 6848;
+    public long fillRefBitmap6849 = 6849;
+    public long fillRefBitmap6850 = 6850;
+    public long fillRefBitmap6851 = 6851;
+    public long fillRefBitmap6852 = 6852;
+    public long fillRefBitmap6853 = 6853;
+    public long fillRefBitmap6854 = 6854;
+    public long fillRefBitmap6855 = 6855;
+    public long fillRefBitmap6856 = 6856;
+    public long fillRefBitmap6857 = 6857;
+    public long fillRefBitmap6858 = 6858;
+    public long fillRefBitmap6859 = 6859;
+    public long fillRefBitmap6860 = 6860;
+    public long fillRefBitmap6861 = 6861;
+    public long fillRefBitmap6862 = 6862;
+    public long fillRefBitmap6863 = 6863;
+    public long fillRefBitmap6864 = 6864;
+    public long fillRefBitmap6865 = 6865;
+    public long fillRefBitmap6866 = 6866;
+    public long fillRefBitmap6867 = 6867;
+    public long fillRefBitmap6868 = 6868;
+    public long fillRefBitmap6869 = 6869;
+    public long fillRefBitmap6870 = 6870;
+    public long fillRefBitmap6871 = 6871;
+    public long fillRefBitmap6872 = 6872;
+    public long fillRefBitmap6873 = 6873;
+    public long fillRefBitmap6874 = 6874;
+    public long fillRefBitmap6875 = 6875;
+    public long fillRefBitmap6876 = 6876;
+    public long fillRefBitmap6877 = 6877;
+    public long fillRefBitmap6878 = 6878;
+    public long fillRefBitmap6879 = 6879;
+    public long fillRefBitmap6880 = 6880;
+    public long fillRefBitmap6881 = 6881;
+    public long fillRefBitmap6882 = 6882;
+    public long fillRefBitmap6883 = 6883;
+    public long fillRefBitmap6884 = 6884;
+    public long fillRefBitmap6885 = 6885;
+    public long fillRefBitmap6886 = 6886;
+    public long fillRefBitmap6887 = 6887;
+    public long fillRefBitmap6888 = 6888;
+    public long fillRefBitmap6889 = 6889;
+    public long fillRefBitmap6890 = 6890;
+    public long fillRefBitmap6891 = 6891;
+    public long fillRefBitmap6892 = 6892;
+    public long fillRefBitmap6893 = 6893;
+    public long fillRefBitmap6894 = 6894;
+    public long fillRefBitmap6895 = 6895;
+    public long fillRefBitmap6896 = 6896;
+    public long fillRefBitmap6897 = 6897;
+    public long fillRefBitmap6898 = 6898;
+    public long fillRefBitmap6899 = 6899;
+    public long fillRefBitmap6900 = 6900;
+    public long fillRefBitmap6901 = 6901;
+    public long fillRefBitmap6902 = 6902;
+    public long fillRefBitmap6903 = 6903;
+    public long fillRefBitmap6904 = 6904;
+    public long fillRefBitmap6905 = 6905;
+    public long fillRefBitmap6906 = 6906;
+    public long fillRefBitmap6907 = 6907;
+    public long fillRefBitmap6908 = 6908;
+    public long fillRefBitmap6909 = 6909;
+    public long fillRefBitmap6910 = 6910;
+    public long fillRefBitmap6911 = 6911;
+    public long fillRefBitmap6912 = 6912;
+    public long fillRefBitmap6913 = 6913;
+    public long fillRefBitmap6914 = 6914;
+    public long fillRefBitmap6915 = 6915;
+    public long fillRefBitmap6916 = 6916;
+    public long fillRefBitmap6917 = 6917;
+    public long fillRefBitmap6918 = 6918;
+    public long fillRefBitmap6919 = 6919;
+    public long fillRefBitmap6920 = 6920;
+    public long fillRefBitmap6921 = 6921;
+    public long fillRefBitmap6922 = 6922;
+    public long fillRefBitmap6923 = 6923;
+    public long fillRefBitmap6924 = 6924;
+    public long fillRefBitmap6925 = 6925;
+    public long fillRefBitmap6926 = 6926;
+    public long fillRefBitmap6927 = 6927;
+    public long fillRefBitmap6928 = 6928;
+    public long fillRefBitmap6929 = 6929;
+    public long fillRefBitmap6930 = 6930;
+    public long fillRefBitmap6931 = 6931;
+    public long fillRefBitmap6932 = 6932;
+    public long fillRefBitmap6933 = 6933;
+    public long fillRefBitmap6934 = 6934;
+    public long fillRefBitmap6935 = 6935;
+    public long fillRefBitmap6936 = 6936;
+    public long fillRefBitmap6937 = 6937;
+    public long fillRefBitmap6938 = 6938;
+    public long fillRefBitmap6939 = 6939;
+    public long fillRefBitmap6940 = 6940;
+    public long fillRefBitmap6941 = 6941;
+    public long fillRefBitmap6942 = 6942;
+    public long fillRefBitmap6943 = 6943;
+    public long fillRefBitmap6944 = 6944;
+    public long fillRefBitmap6945 = 6945;
+    public long fillRefBitmap6946 = 6946;
+    public long fillRefBitmap6947 = 6947;
+    public long fillRefBitmap6948 = 6948;
+    public long fillRefBitmap6949 = 6949;
+    public long fillRefBitmap6950 = 6950;
+    public long fillRefBitmap6951 = 6951;
+    public long fillRefBitmap6952 = 6952;
+    public long fillRefBitmap6953 = 6953;
+    public long fillRefBitmap6954 = 6954;
+    public long fillRefBitmap6955 = 6955;
+    public long fillRefBitmap6956 = 6956;
+    public long fillRefBitmap6957 = 6957;
+    public long fillRefBitmap6958 = 6958;
+    public long fillRefBitmap6959 = 6959;
+    public long fillRefBitmap6960 = 6960;
+    public long fillRefBitmap6961 = 6961;
+    public long fillRefBitmap6962 = 6962;
+    public long fillRefBitmap6963 = 6963;
+    public long fillRefBitmap6964 = 6964;
+    public long fillRefBitmap6965 = 6965;
+    public long fillRefBitmap6966 = 6966;
+    public long fillRefBitmap6967 = 6967;
+    public long fillRefBitmap6968 = 6968;
+    public long fillRefBitmap6969 = 6969;
+    public long fillRefBitmap6970 = 6970;
+    public long fillRefBitmap6971 = 6971;
+    public long fillRefBitmap6972 = 6972;
+    public long fillRefBitmap6973 = 6973;
+    public long fillRefBitmap6974 = 6974;
+    public long fillRefBitmap6975 = 6975;
+    public long fillRefBitmap6976 = 6976;
+    public long fillRefBitmap6977 = 6977;
+    public long fillRefBitmap6978 = 6978;
+    public long fillRefBitmap6979 = 6979;
+    public long fillRefBitmap6980 = 6980;
+    public long fillRefBitmap6981 = 6981;
+    public long fillRefBitmap6982 = 6982;
+    public long fillRefBitmap6983 = 6983;
+    public long fillRefBitmap6984 = 6984;
+    public long fillRefBitmap6985 = 6985;
+    public long fillRefBitmap6986 = 6986;
+    public long fillRefBitmap6987 = 6987;
+    public long fillRefBitmap6988 = 6988;
+    public long fillRefBitmap6989 = 6989;
+    public long fillRefBitmap6990 = 6990;
+    public long fillRefBitmap6991 = 6991;
+    public long fillRefBitmap6992 = 6992;
+    public long fillRefBitmap6993 = 6993;
+    public long fillRefBitmap6994 = 6994;
+    public long fillRefBitmap6995 = 6995;
+    public long fillRefBitmap6996 = 6996;
+    public long fillRefBitmap6997 = 6997;
+    public long fillRefBitmap6998 = 6998;
+    public long fillRefBitmap6999 = 6999;
+    public long fillRefBitmap7000 = 7000;
+    public long fillRefBitmap7001 = 7001;
+    public long fillRefBitmap7002 = 7002;
+    public long fillRefBitmap7003 = 7003;
+    public long fillRefBitmap7004 = 7004;
+    public long fillRefBitmap7005 = 7005;
+    public long fillRefBitmap7006 = 7006;
+    public long fillRefBitmap7007 = 7007;
+    public long fillRefBitmap7008 = 7008;
+    public long fillRefBitmap7009 = 7009;
+    public long fillRefBitmap7010 = 7010;
+    public long fillRefBitmap7011 = 7011;
+    public long fillRefBitmap7012 = 7012;
+    public long fillRefBitmap7013 = 7013;
+    public long fillRefBitmap7014 = 7014;
+    public long fillRefBitmap7015 = 7015;
+    public long fillRefBitmap7016 = 7016;
+    public long fillRefBitmap7017 = 7017;
+    public long fillRefBitmap7018 = 7018;
+    public long fillRefBitmap7019 = 7019;
+    public long fillRefBitmap7020 = 7020;
+    public long fillRefBitmap7021 = 7021;
+    public long fillRefBitmap7022 = 7022;
+    public long fillRefBitmap7023 = 7023;
+    public long fillRefBitmap7024 = 7024;
+    public long fillRefBitmap7025 = 7025;
+    public long fillRefBitmap7026 = 7026;
+    public long fillRefBitmap7027 = 7027;
+    public long fillRefBitmap7028 = 7028;
+    public long fillRefBitmap7029 = 7029;
+    public long fillRefBitmap7030 = 7030;
+    public long fillRefBitmap7031 = 7031;
+    public long fillRefBitmap7032 = 7032;
+    public long fillRefBitmap7033 = 7033;
+    public long fillRefBitmap7034 = 7034;
+    public long fillRefBitmap7035 = 7035;
+    public long fillRefBitmap7036 = 7036;
+    public long fillRefBitmap7037 = 7037;
+    public long fillRefBitmap7038 = 7038;
+    public long fillRefBitmap7039 = 7039;
+    public long fillRefBitmap7040 = 7040;
+    public long fillRefBitmap7041 = 7041;
+    public long fillRefBitmap7042 = 7042;
+    public long fillRefBitmap7043 = 7043;
+    public long fillRefBitmap7044 = 7044;
+    public long fillRefBitmap7045 = 7045;
+    public long fillRefBitmap7046 = 7046;
+    public long fillRefBitmap7047 = 7047;
+    public long fillRefBitmap7048 = 7048;
+    public long fillRefBitmap7049 = 7049;
+    public long fillRefBitmap7050 = 7050;
+    public long fillRefBitmap7051 = 7051;
+    public long fillRefBitmap7052 = 7052;
+    public long fillRefBitmap7053 = 7053;
+    public long fillRefBitmap7054 = 7054;
+    public long fillRefBitmap7055 = 7055;
+    public long fillRefBitmap7056 = 7056;
+    public long fillRefBitmap7057 = 7057;
+    public long fillRefBitmap7058 = 7058;
+    public long fillRefBitmap7059 = 7059;
+    public long fillRefBitmap7060 = 7060;
+    public long fillRefBitmap7061 = 7061;
+    public long fillRefBitmap7062 = 7062;
+    public long fillRefBitmap7063 = 7063;
+    public long fillRefBitmap7064 = 7064;
+    public long fillRefBitmap7065 = 7065;
+    public long fillRefBitmap7066 = 7066;
+    public long fillRefBitmap7067 = 7067;
+    public long fillRefBitmap7068 = 7068;
+    public long fillRefBitmap7069 = 7069;
+    public long fillRefBitmap7070 = 7070;
+    public long fillRefBitmap7071 = 7071;
+    public long fillRefBitmap7072 = 7072;
+    public long fillRefBitmap7073 = 7073;
+    public long fillRefBitmap7074 = 7074;
+    public long fillRefBitmap7075 = 7075;
+    public long fillRefBitmap7076 = 7076;
+    public long fillRefBitmap7077 = 7077;
+    public long fillRefBitmap7078 = 7078;
+    public long fillRefBitmap7079 = 7079;
+    public long fillRefBitmap7080 = 7080;
+    public long fillRefBitmap7081 = 7081;
+    public long fillRefBitmap7082 = 7082;
+    public long fillRefBitmap7083 = 7083;
+    public long fillRefBitmap7084 = 7084;
+    public long fillRefBitmap7085 = 7085;
+    public long fillRefBitmap7086 = 7086;
+    public long fillRefBitmap7087 = 7087;
+    public long fillRefBitmap7088 = 7088;
+    public long fillRefBitmap7089 = 7089;
+    public long fillRefBitmap7090 = 7090;
+    public long fillRefBitmap7091 = 7091;
+    public long fillRefBitmap7092 = 7092;
+    public long fillRefBitmap7093 = 7093;
+    public long fillRefBitmap7094 = 7094;
+    public long fillRefBitmap7095 = 7095;
+    public long fillRefBitmap7096 = 7096;
+    public long fillRefBitmap7097 = 7097;
+    public long fillRefBitmap7098 = 7098;
+    public long fillRefBitmap7099 = 7099;
+    public long fillRefBitmap7100 = 7100;
+    public long fillRefBitmap7101 = 7101;
+    public long fillRefBitmap7102 = 7102;
+    public long fillRefBitmap7103 = 7103;
+    public long fillRefBitmap7104 = 7104;
+    public long fillRefBitmap7105 = 7105;
+    public long fillRefBitmap7106 = 7106;
+    public long fillRefBitmap7107 = 7107;
+    public long fillRefBitmap7108 = 7108;
+    public long fillRefBitmap7109 = 7109;
+    public long fillRefBitmap7110 = 7110;
+    public long fillRefBitmap7111 = 7111;
+    public long fillRefBitmap7112 = 7112;
+    public long fillRefBitmap7113 = 7113;
+    public long fillRefBitmap7114 = 7114;
+    public long fillRefBitmap7115 = 7115;
+    public long fillRefBitmap7116 = 7116;
+    public long fillRefBitmap7117 = 7117;
+    public long fillRefBitmap7118 = 7118;
+    public long fillRefBitmap7119 = 7119;
+    public long fillRefBitmap7120 = 7120;
+    public long fillRefBitmap7121 = 7121;
+    public long fillRefBitmap7122 = 7122;
+    public long fillRefBitmap7123 = 7123;
+    public long fillRefBitmap7124 = 7124;
+    public long fillRefBitmap7125 = 7125;
+    public long fillRefBitmap7126 = 7126;
+    public long fillRefBitmap7127 = 7127;
+    public long fillRefBitmap7128 = 7128;
+    public long fillRefBitmap7129 = 7129;
+    public long fillRefBitmap7130 = 7130;
+    public long fillRefBitmap7131 = 7131;
+    public long fillRefBitmap7132 = 7132;
+    public long fillRefBitmap7133 = 7133;
+    public long fillRefBitmap7134 = 7134;
+    public long fillRefBitmap7135 = 7135;
+    public long fillRefBitmap7136 = 7136;
+    public long fillRefBitmap7137 = 7137;
+    public long fillRefBitmap7138 = 7138;
+    public long fillRefBitmap7139 = 7139;
+    public long fillRefBitmap7140 = 7140;
+    public long fillRefBitmap7141 = 7141;
+    public long fillRefBitmap7142 = 7142;
+    public long fillRefBitmap7143 = 7143;
+    public long fillRefBitmap7144 = 7144;
+    public long fillRefBitmap7145 = 7145;
+    public long fillRefBitmap7146 = 7146;
+    public long fillRefBitmap7147 = 7147;
+    public long fillRefBitmap7148 = 7148;
+    public long fillRefBitmap7149 = 7149;
+    public long fillRefBitmap7150 = 7150;
+    public long fillRefBitmap7151 = 7151;
+    public long fillRefBitmap7152 = 7152;
+    public long fillRefBitmap7153 = 7153;
+    public long fillRefBitmap7154 = 7154;
+    public long fillRefBitmap7155 = 7155;
+    public long fillRefBitmap7156 = 7156;
+    public long fillRefBitmap7157 = 7157;
+    public long fillRefBitmap7158 = 7158;
+    public long fillRefBitmap7159 = 7159;
+    public long fillRefBitmap7160 = 7160;
+    public long fillRefBitmap7161 = 7161;
+    public long fillRefBitmap7162 = 7162;
+    public long fillRefBitmap7163 = 7163;
+    public long fillRefBitmap7164 = 7164;
+    public long fillRefBitmap7165 = 7165;
+    public long fillRefBitmap7166 = 7166;
+    public long fillRefBitmap7167 = 7167;
+    public long fillRefBitmap7168 = 7168;
+    public long fillRefBitmap7169 = 7169;
+    public long fillRefBitmap7170 = 7170;
+    public long fillRefBitmap7171 = 7171;
+    public long fillRefBitmap7172 = 7172;
+    public long fillRefBitmap7173 = 7173;
+    public long fillRefBitmap7174 = 7174;
+    public long fillRefBitmap7175 = 7175;
+    public long fillRefBitmap7176 = 7176;
+    public long fillRefBitmap7177 = 7177;
+    public long fillRefBitmap7178 = 7178;
+    public long fillRefBitmap7179 = 7179;
+    public long fillRefBitmap7180 = 7180;
+    public long fillRefBitmap7181 = 7181;
+    public long fillRefBitmap7182 = 7182;
+    public long fillRefBitmap7183 = 7183;
+    public long fillRefBitmap7184 = 7184;
+    public long fillRefBitmap7185 = 7185;
+    public long fillRefBitmap7186 = 7186;
+    public long fillRefBitmap7187 = 7187;
+    public long fillRefBitmap7188 = 7188;
+    public long fillRefBitmap7189 = 7189;
+    public long fillRefBitmap7190 = 7190;
+    public long fillRefBitmap7191 = 7191;
+    public long fillRefBitmap7192 = 7192;
+    public long fillRefBitmap7193 = 7193;
+    public long fillRefBitmap7194 = 7194;
+    public long fillRefBitmap7195 = 7195;
+    public long fillRefBitmap7196 = 7196;
+    public long fillRefBitmap7197 = 7197;
+    public long fillRefBitmap7198 = 7198;
+    public long fillRefBitmap7199 = 7199;
+    public long fillRefBitmap7200 = 7200;
+    public long fillRefBitmap7201 = 7201;
+    public long fillRefBitmap7202 = 7202;
+    public long fillRefBitmap7203 = 7203;
+    public long fillRefBitmap7204 = 7204;
+    public long fillRefBitmap7205 = 7205;
+    public long fillRefBitmap7206 = 7206;
+    public long fillRefBitmap7207 = 7207;
+    public long fillRefBitmap7208 = 7208;
+    public long fillRefBitmap7209 = 7209;
+    public long fillRefBitmap7210 = 7210;
+    public long fillRefBitmap7211 = 7211;
+    public long fillRefBitmap7212 = 7212;
+    public long fillRefBitmap7213 = 7213;
+    public long fillRefBitmap7214 = 7214;
+    public long fillRefBitmap7215 = 7215;
+    public long fillRefBitmap7216 = 7216;
+    public long fillRefBitmap7217 = 7217;
+    public long fillRefBitmap7218 = 7218;
+    public long fillRefBitmap7219 = 7219;
+    public long fillRefBitmap7220 = 7220;
+    public long fillRefBitmap7221 = 7221;
+    public long fillRefBitmap7222 = 7222;
+    public long fillRefBitmap7223 = 7223;
+    public long fillRefBitmap7224 = 7224;
+    public long fillRefBitmap7225 = 7225;
+    public long fillRefBitmap7226 = 7226;
+    public long fillRefBitmap7227 = 7227;
+    public long fillRefBitmap7228 = 7228;
+    public long fillRefBitmap7229 = 7229;
+    public long fillRefBitmap7230 = 7230;
+    public long fillRefBitmap7231 = 7231;
+    public long fillRefBitmap7232 = 7232;
+    public long fillRefBitmap7233 = 7233;
+    public long fillRefBitmap7234 = 7234;
+    public long fillRefBitmap7235 = 7235;
+    public long fillRefBitmap7236 = 7236;
+    public long fillRefBitmap7237 = 7237;
+    public long fillRefBitmap7238 = 7238;
+    public long fillRefBitmap7239 = 7239;
+    public long fillRefBitmap7240 = 7240;
+    public long fillRefBitmap7241 = 7241;
+    public long fillRefBitmap7242 = 7242;
+    public long fillRefBitmap7243 = 7243;
+    public long fillRefBitmap7244 = 7244;
+    public long fillRefBitmap7245 = 7245;
+    public long fillRefBitmap7246 = 7246;
+    public long fillRefBitmap7247 = 7247;
+    public long fillRefBitmap7248 = 7248;
+    public long fillRefBitmap7249 = 7249;
+    public long fillRefBitmap7250 = 7250;
+    public long fillRefBitmap7251 = 7251;
+    public long fillRefBitmap7252 = 7252;
+    public long fillRefBitmap7253 = 7253;
+    public long fillRefBitmap7254 = 7254;
+    public long fillRefBitmap7255 = 7255;
+    public long fillRefBitmap7256 = 7256;
+    public long fillRefBitmap7257 = 7257;
+    public long fillRefBitmap7258 = 7258;
+    public long fillRefBitmap7259 = 7259;
+    public long fillRefBitmap7260 = 7260;
+    public long fillRefBitmap7261 = 7261;
+    public long fillRefBitmap7262 = 7262;
+    public long fillRefBitmap7263 = 7263;
+    public long fillRefBitmap7264 = 7264;
+    public long fillRefBitmap7265 = 7265;
+    public long fillRefBitmap7266 = 7266;
+    public long fillRefBitmap7267 = 7267;
+    public long fillRefBitmap7268 = 7268;
+    public long fillRefBitmap7269 = 7269;
+    public long fillRefBitmap7270 = 7270;
+    public long fillRefBitmap7271 = 7271;
+    public long fillRefBitmap7272 = 7272;
+    public long fillRefBitmap7273 = 7273;
+    public long fillRefBitmap7274 = 7274;
+    public long fillRefBitmap7275 = 7275;
+    public long fillRefBitmap7276 = 7276;
+    public long fillRefBitmap7277 = 7277;
+    public long fillRefBitmap7278 = 7278;
+    public long fillRefBitmap7279 = 7279;
+    public long fillRefBitmap7280 = 7280;
+    public long fillRefBitmap7281 = 7281;
+    public long fillRefBitmap7282 = 7282;
+    public long fillRefBitmap7283 = 7283;
+    public long fillRefBitmap7284 = 7284;
+    public long fillRefBitmap7285 = 7285;
+    public long fillRefBitmap7286 = 7286;
+    public long fillRefBitmap7287 = 7287;
+    public long fillRefBitmap7288 = 7288;
+    public long fillRefBitmap7289 = 7289;
+    public long fillRefBitmap7290 = 7290;
+    public long fillRefBitmap7291 = 7291;
+    public long fillRefBitmap7292 = 7292;
+    public long fillRefBitmap7293 = 7293;
+    public long fillRefBitmap7294 = 7294;
+    public long fillRefBitmap7295 = 7295;
+    public long fillRefBitmap7296 = 7296;
+    public long fillRefBitmap7297 = 7297;
+    public long fillRefBitmap7298 = 7298;
+    public long fillRefBitmap7299 = 7299;
+    public long fillRefBitmap7300 = 7300;
+    public long fillRefBitmap7301 = 7301;
+    public long fillRefBitmap7302 = 7302;
+    public long fillRefBitmap7303 = 7303;
+    public long fillRefBitmap7304 = 7304;
+    public long fillRefBitmap7305 = 7305;
+    public long fillRefBitmap7306 = 7306;
+    public long fillRefBitmap7307 = 7307;
+    public long fillRefBitmap7308 = 7308;
+    public long fillRefBitmap7309 = 7309;
+    public long fillRefBitmap7310 = 7310;
+    public long fillRefBitmap7311 = 7311;
+    public long fillRefBitmap7312 = 7312;
+    public long fillRefBitmap7313 = 7313;
+    public long fillRefBitmap7314 = 7314;
+    public long fillRefBitmap7315 = 7315;
+    public long fillRefBitmap7316 = 7316;
+    public long fillRefBitmap7317 = 7317;
+    public long fillRefBitmap7318 = 7318;
+    public long fillRefBitmap7319 = 7319;
+    public long fillRefBitmap7320 = 7320;
+    public long fillRefBitmap7321 = 7321;
+    public long fillRefBitmap7322 = 7322;
+    public long fillRefBitmap7323 = 7323;
+    public long fillRefBitmap7324 = 7324;
+    public long fillRefBitmap7325 = 7325;
+    public long fillRefBitmap7326 = 7326;
+    public long fillRefBitmap7327 = 7327;
+    public long fillRefBitmap7328 = 7328;
+    public long fillRefBitmap7329 = 7329;
+    public long fillRefBitmap7330 = 7330;
+    public long fillRefBitmap7331 = 7331;
+    public long fillRefBitmap7332 = 7332;
+    public long fillRefBitmap7333 = 7333;
+    public long fillRefBitmap7334 = 7334;
+    public long fillRefBitmap7335 = 7335;
+    public long fillRefBitmap7336 = 7336;
+    public long fillRefBitmap7337 = 7337;
+    public long fillRefBitmap7338 = 7338;
+    public long fillRefBitmap7339 = 7339;
+    public long fillRefBitmap7340 = 7340;
+    public long fillRefBitmap7341 = 7341;
+    public long fillRefBitmap7342 = 7342;
+    public long fillRefBitmap7343 = 7343;
+    public long fillRefBitmap7344 = 7344;
+    public long fillRefBitmap7345 = 7345;
+    public long fillRefBitmap7346 = 7346;
+    public long fillRefBitmap7347 = 7347;
+    public long fillRefBitmap7348 = 7348;
+    public long fillRefBitmap7349 = 7349;
+    public long fillRefBitmap7350 = 7350;
+    public long fillRefBitmap7351 = 7351;
+    public long fillRefBitmap7352 = 7352;
+    public long fillRefBitmap7353 = 7353;
+    public long fillRefBitmap7354 = 7354;
+    public long fillRefBitmap7355 = 7355;
+    public long fillRefBitmap7356 = 7356;
+    public long fillRefBitmap7357 = 7357;
+    public long fillRefBitmap7358 = 7358;
+    public long fillRefBitmap7359 = 7359;
+    public long fillRefBitmap7360 = 7360;
+    public long fillRefBitmap7361 = 7361;
+    public long fillRefBitmap7362 = 7362;
+    public long fillRefBitmap7363 = 7363;
+    public long fillRefBitmap7364 = 7364;
+    public long fillRefBitmap7365 = 7365;
+    public long fillRefBitmap7366 = 7366;
+    public long fillRefBitmap7367 = 7367;
+    public long fillRefBitmap7368 = 7368;
+    public long fillRefBitmap7369 = 7369;
+    public long fillRefBitmap7370 = 7370;
+    public long fillRefBitmap7371 = 7371;
+    public long fillRefBitmap7372 = 7372;
+    public long fillRefBitmap7373 = 7373;
+    public long fillRefBitmap7374 = 7374;
+    public long fillRefBitmap7375 = 7375;
+    public long fillRefBitmap7376 = 7376;
+    public long fillRefBitmap7377 = 7377;
+    public long fillRefBitmap7378 = 7378;
+    public long fillRefBitmap7379 = 7379;
+    public long fillRefBitmap7380 = 7380;
+    public long fillRefBitmap7381 = 7381;
+    public long fillRefBitmap7382 = 7382;
+    public long fillRefBitmap7383 = 7383;
+    public long fillRefBitmap7384 = 7384;
+    public long fillRefBitmap7385 = 7385;
+    public long fillRefBitmap7386 = 7386;
+    public long fillRefBitmap7387 = 7387;
+    public long fillRefBitmap7388 = 7388;
+    public long fillRefBitmap7389 = 7389;
+    public long fillRefBitmap7390 = 7390;
+    public long fillRefBitmap7391 = 7391;
+    public long fillRefBitmap7392 = 7392;
+    public long fillRefBitmap7393 = 7393;
+    public long fillRefBitmap7394 = 7394;
+    public long fillRefBitmap7395 = 7395;
+    public long fillRefBitmap7396 = 7396;
+    public long fillRefBitmap7397 = 7397;
+    public long fillRefBitmap7398 = 7398;
+    public long fillRefBitmap7399 = 7399;
+    public long fillRefBitmap7400 = 7400;
+    public long fillRefBitmap7401 = 7401;
+    public long fillRefBitmap7402 = 7402;
+    public long fillRefBitmap7403 = 7403;
+    public long fillRefBitmap7404 = 7404;
+    public long fillRefBitmap7405 = 7405;
+    public long fillRefBitmap7406 = 7406;
+    public long fillRefBitmap7407 = 7407;
+    public long fillRefBitmap7408 = 7408;
+    public long fillRefBitmap7409 = 7409;
+    public long fillRefBitmap7410 = 7410;
+    public long fillRefBitmap7411 = 7411;
+    public long fillRefBitmap7412 = 7412;
+    public long fillRefBitmap7413 = 7413;
+    public long fillRefBitmap7414 = 7414;
+    public long fillRefBitmap7415 = 7415;
+    public long fillRefBitmap7416 = 7416;
+    public long fillRefBitmap7417 = 7417;
+    public long fillRefBitmap7418 = 7418;
+    public long fillRefBitmap7419 = 7419;
+    public long fillRefBitmap7420 = 7420;
+    public long fillRefBitmap7421 = 7421;
+    public long fillRefBitmap7422 = 7422;
+    public long fillRefBitmap7423 = 7423;
+    public long fillRefBitmap7424 = 7424;
+    public long fillRefBitmap7425 = 7425;
+    public long fillRefBitmap7426 = 7426;
+    public long fillRefBitmap7427 = 7427;
+    public long fillRefBitmap7428 = 7428;
+    public long fillRefBitmap7429 = 7429;
+    public long fillRefBitmap7430 = 7430;
+    public long fillRefBitmap7431 = 7431;
+    public long fillRefBitmap7432 = 7432;
+    public long fillRefBitmap7433 = 7433;
+    public long fillRefBitmap7434 = 7434;
+    public long fillRefBitmap7435 = 7435;
+    public long fillRefBitmap7436 = 7436;
+    public long fillRefBitmap7437 = 7437;
+    public long fillRefBitmap7438 = 7438;
+    public long fillRefBitmap7439 = 7439;
+    public long fillRefBitmap7440 = 7440;
+    public long fillRefBitmap7441 = 7441;
+    public long fillRefBitmap7442 = 7442;
+    public long fillRefBitmap7443 = 7443;
+    public long fillRefBitmap7444 = 7444;
+    public long fillRefBitmap7445 = 7445;
+    public long fillRefBitmap7446 = 7446;
+    public long fillRefBitmap7447 = 7447;
+    public long fillRefBitmap7448 = 7448;
+    public long fillRefBitmap7449 = 7449;
+    public long fillRefBitmap7450 = 7450;
+    public long fillRefBitmap7451 = 7451;
+    public long fillRefBitmap7452 = 7452;
+    public long fillRefBitmap7453 = 7453;
+    public long fillRefBitmap7454 = 7454;
+    public long fillRefBitmap7455 = 7455;
+    public long fillRefBitmap7456 = 7456;
+    public long fillRefBitmap7457 = 7457;
+    public long fillRefBitmap7458 = 7458;
+    public long fillRefBitmap7459 = 7459;
+    public long fillRefBitmap7460 = 7460;
+    public long fillRefBitmap7461 = 7461;
+    public long fillRefBitmap7462 = 7462;
+    public long fillRefBitmap7463 = 7463;
+    public long fillRefBitmap7464 = 7464;
+    public long fillRefBitmap7465 = 7465;
+    public long fillRefBitmap7466 = 7466;
+    public long fillRefBitmap7467 = 7467;
+    public long fillRefBitmap7468 = 7468;
+    public long fillRefBitmap7469 = 7469;
+    public long fillRefBitmap7470 = 7470;
+    public long fillRefBitmap7471 = 7471;
+    public long fillRefBitmap7472 = 7472;
+    public long fillRefBitmap7473 = 7473;
+    public long fillRefBitmap7474 = 7474;
+    public long fillRefBitmap7475 = 7475;
+    public long fillRefBitmap7476 = 7476;
+    public long fillRefBitmap7477 = 7477;
+    public long fillRefBitmap7478 = 7478;
+    public long fillRefBitmap7479 = 7479;
+    public long fillRefBitmap7480 = 7480;
+    public long fillRefBitmap7481 = 7481;
+    public long fillRefBitmap7482 = 7482;
+    public long fillRefBitmap7483 = 7483;
+    public long fillRefBitmap7484 = 7484;
+    public long fillRefBitmap7485 = 7485;
+    public long fillRefBitmap7486 = 7486;
+    public long fillRefBitmap7487 = 7487;
+    public long fillRefBitmap7488 = 7488;
+    public long fillRefBitmap7489 = 7489;
+    public long fillRefBitmap7490 = 7490;
+    public long fillRefBitmap7491 = 7491;
+    public long fillRefBitmap7492 = 7492;
+    public long fillRefBitmap7493 = 7493;
+    public long fillRefBitmap7494 = 7494;
+    public long fillRefBitmap7495 = 7495;
+    public long fillRefBitmap7496 = 7496;
+    public long fillRefBitmap7497 = 7497;
+    public long fillRefBitmap7498 = 7498;
+    public long fillRefBitmap7499 = 7499;
+    public long fillRefBitmap7500 = 7500;
+    public long fillRefBitmap7501 = 7501;
+    public long fillRefBitmap7502 = 7502;
+    public long fillRefBitmap7503 = 7503;
+    public long fillRefBitmap7504 = 7504;
+    public long fillRefBitmap7505 = 7505;
+    public long fillRefBitmap7506 = 7506;
+    public long fillRefBitmap7507 = 7507;
+    public long fillRefBitmap7508 = 7508;
+    public long fillRefBitmap7509 = 7509;
+    public long fillRefBitmap7510 = 7510;
+    public long fillRefBitmap7511 = 7511;
+    public long fillRefBitmap7512 = 7512;
+    public long fillRefBitmap7513 = 7513;
+    public long fillRefBitmap7514 = 7514;
+    public long fillRefBitmap7515 = 7515;
+    public long fillRefBitmap7516 = 7516;
+    public long fillRefBitmap7517 = 7517;
+    public long fillRefBitmap7518 = 7518;
+    public long fillRefBitmap7519 = 7519;
+    public long fillRefBitmap7520 = 7520;
+    public long fillRefBitmap7521 = 7521;
+    public long fillRefBitmap7522 = 7522;
+    public long fillRefBitmap7523 = 7523;
+    public long fillRefBitmap7524 = 7524;
+    public long fillRefBitmap7525 = 7525;
+    public long fillRefBitmap7526 = 7526;
+    public long fillRefBitmap7527 = 7527;
+    public long fillRefBitmap7528 = 7528;
+    public long fillRefBitmap7529 = 7529;
+    public long fillRefBitmap7530 = 7530;
+    public long fillRefBitmap7531 = 7531;
+    public long fillRefBitmap7532 = 7532;
+    public long fillRefBitmap7533 = 7533;
+    public long fillRefBitmap7534 = 7534;
+    public long fillRefBitmap7535 = 7535;
+    public long fillRefBitmap7536 = 7536;
+    public long fillRefBitmap7537 = 7537;
+    public long fillRefBitmap7538 = 7538;
+    public long fillRefBitmap7539 = 7539;
+    public long fillRefBitmap7540 = 7540;
+    public long fillRefBitmap7541 = 7541;
+    public long fillRefBitmap7542 = 7542;
+    public long fillRefBitmap7543 = 7543;
+    public long fillRefBitmap7544 = 7544;
+    public long fillRefBitmap7545 = 7545;
+    public long fillRefBitmap7546 = 7546;
+    public long fillRefBitmap7547 = 7547;
+    public long fillRefBitmap7548 = 7548;
+    public long fillRefBitmap7549 = 7549;
+    public long fillRefBitmap7550 = 7550;
+    public long fillRefBitmap7551 = 7551;
+    public long fillRefBitmap7552 = 7552;
+    public long fillRefBitmap7553 = 7553;
+    public long fillRefBitmap7554 = 7554;
+    public long fillRefBitmap7555 = 7555;
+    public long fillRefBitmap7556 = 7556;
+    public long fillRefBitmap7557 = 7557;
+    public long fillRefBitmap7558 = 7558;
+    public long fillRefBitmap7559 = 7559;
+    public long fillRefBitmap7560 = 7560;
+    public long fillRefBitmap7561 = 7561;
+    public long fillRefBitmap7562 = 7562;
+    public long fillRefBitmap7563 = 7563;
+    public long fillRefBitmap7564 = 7564;
+    public long fillRefBitmap7565 = 7565;
+    public long fillRefBitmap7566 = 7566;
+    public long fillRefBitmap7567 = 7567;
+    public long fillRefBitmap7568 = 7568;
+    public long fillRefBitmap7569 = 7569;
+    public long fillRefBitmap7570 = 7570;
+    public long fillRefBitmap7571 = 7571;
+    public long fillRefBitmap7572 = 7572;
+    public long fillRefBitmap7573 = 7573;
+    public long fillRefBitmap7574 = 7574;
+    public long fillRefBitmap7575 = 7575;
+    public long fillRefBitmap7576 = 7576;
+    public long fillRefBitmap7577 = 7577;
+    public long fillRefBitmap7578 = 7578;
+    public long fillRefBitmap7579 = 7579;
+    public long fillRefBitmap7580 = 7580;
+    public long fillRefBitmap7581 = 7581;
+    public long fillRefBitmap7582 = 7582;
+    public long fillRefBitmap7583 = 7583;
+    public long fillRefBitmap7584 = 7584;
+    public long fillRefBitmap7585 = 7585;
+    public long fillRefBitmap7586 = 7586;
+    public long fillRefBitmap7587 = 7587;
+    public long fillRefBitmap7588 = 7588;
+    public long fillRefBitmap7589 = 7589;
+    public long fillRefBitmap7590 = 7590;
+    public long fillRefBitmap7591 = 7591;
+    public long fillRefBitmap7592 = 7592;
+    public long fillRefBitmap7593 = 7593;
+    public long fillRefBitmap7594 = 7594;
+    public long fillRefBitmap7595 = 7595;
+    public long fillRefBitmap7596 = 7596;
+    public long fillRefBitmap7597 = 7597;
+    public long fillRefBitmap7598 = 7598;
+    public long fillRefBitmap7599 = 7599;
+    public long fillRefBitmap7600 = 7600;
+    public long fillRefBitmap7601 = 7601;
+    public long fillRefBitmap7602 = 7602;
+    public long fillRefBitmap7603 = 7603;
+    public long fillRefBitmap7604 = 7604;
+    public long fillRefBitmap7605 = 7605;
+    public long fillRefBitmap7606 = 7606;
+    public long fillRefBitmap7607 = 7607;
+    public long fillRefBitmap7608 = 7608;
+    public long fillRefBitmap7609 = 7609;
+    public long fillRefBitmap7610 = 7610;
+    public long fillRefBitmap7611 = 7611;
+    public long fillRefBitmap7612 = 7612;
+    public long fillRefBitmap7613 = 7613;
+    public long fillRefBitmap7614 = 7614;
+    public long fillRefBitmap7615 = 7615;
+    public long fillRefBitmap7616 = 7616;
+    public long fillRefBitmap7617 = 7617;
+    public long fillRefBitmap7618 = 7618;
+    public long fillRefBitmap7619 = 7619;
+    public long fillRefBitmap7620 = 7620;
+    public long fillRefBitmap7621 = 7621;
+    public long fillRefBitmap7622 = 7622;
+    public long fillRefBitmap7623 = 7623;
+    public long fillRefBitmap7624 = 7624;
+    public long fillRefBitmap7625 = 7625;
+    public long fillRefBitmap7626 = 7626;
+    public long fillRefBitmap7627 = 7627;
+    public long fillRefBitmap7628 = 7628;
+    public long fillRefBitmap7629 = 7629;
+    public long fillRefBitmap7630 = 7630;
+    public long fillRefBitmap7631 = 7631;
+    public long fillRefBitmap7632 = 7632;
+    public long fillRefBitmap7633 = 7633;
+    public long fillRefBitmap7634 = 7634;
+    public long fillRefBitmap7635 = 7635;
+    public long fillRefBitmap7636 = 7636;
+    public long fillRefBitmap7637 = 7637;
+    public long fillRefBitmap7638 = 7638;
+    public long fillRefBitmap7639 = 7639;
+    public long fillRefBitmap7640 = 7640;
+    public long fillRefBitmap7641 = 7641;
+    public long fillRefBitmap7642 = 7642;
+    public long fillRefBitmap7643 = 7643;
+    public long fillRefBitmap7644 = 7644;
+    public long fillRefBitmap7645 = 7645;
+    public long fillRefBitmap7646 = 7646;
+    public long fillRefBitmap7647 = 7647;
+    public long fillRefBitmap7648 = 7648;
+    public long fillRefBitmap7649 = 7649;
+    public long fillRefBitmap7650 = 7650;
+    public long fillRefBitmap7651 = 7651;
+    public long fillRefBitmap7652 = 7652;
+    public long fillRefBitmap7653 = 7653;
+    public long fillRefBitmap7654 = 7654;
+    public long fillRefBitmap7655 = 7655;
+    public long fillRefBitmap7656 = 7656;
+    public long fillRefBitmap7657 = 7657;
+    public long fillRefBitmap7658 = 7658;
+    public long fillRefBitmap7659 = 7659;
+    public long fillRefBitmap7660 = 7660;
+    public long fillRefBitmap7661 = 7661;
+    public long fillRefBitmap7662 = 7662;
+    public long fillRefBitmap7663 = 7663;
+    public long fillRefBitmap7664 = 7664;
+    public long fillRefBitmap7665 = 7665;
+    public long fillRefBitmap7666 = 7666;
+    public long fillRefBitmap7667 = 7667;
+    public long fillRefBitmap7668 = 7668;
+    public long fillRefBitmap7669 = 7669;
+    public long fillRefBitmap7670 = 7670;
+    public long fillRefBitmap7671 = 7671;
+    public long fillRefBitmap7672 = 7672;
+    public long fillRefBitmap7673 = 7673;
+    public long fillRefBitmap7674 = 7674;
+    public long fillRefBitmap7675 = 7675;
+    public long fillRefBitmap7676 = 7676;
+    public long fillRefBitmap7677 = 7677;
+    public long fillRefBitmap7678 = 7678;
+    public long fillRefBitmap7679 = 7679;
+    public long fillRefBitmap7680 = 7680;
+    public long fillRefBitmap7681 = 7681;
+    public long fillRefBitmap7682 = 7682;
+    public long fillRefBitmap7683 = 7683;
+    public long fillRefBitmap7684 = 7684;
+    public long fillRefBitmap7685 = 7685;
+    public long fillRefBitmap7686 = 7686;
+    public long fillRefBitmap7687 = 7687;
+    public long fillRefBitmap7688 = 7688;
+    public long fillRefBitmap7689 = 7689;
+    public long fillRefBitmap7690 = 7690;
+    public long fillRefBitmap7691 = 7691;
+    public long fillRefBitmap7692 = 7692;
+    public long fillRefBitmap7693 = 7693;
+    public long fillRefBitmap7694 = 7694;
+    public long fillRefBitmap7695 = 7695;
+    public long fillRefBitmap7696 = 7696;
+    public long fillRefBitmap7697 = 7697;
+    public long fillRefBitmap7698 = 7698;
+    public long fillRefBitmap7699 = 7699;
+    public long fillRefBitmap7700 = 7700;
+    public long fillRefBitmap7701 = 7701;
+    public long fillRefBitmap7702 = 7702;
+    public long fillRefBitmap7703 = 7703;
+    public long fillRefBitmap7704 = 7704;
+    public long fillRefBitmap7705 = 7705;
+    public long fillRefBitmap7706 = 7706;
+    public long fillRefBitmap7707 = 7707;
+    public long fillRefBitmap7708 = 7708;
+    public long fillRefBitmap7709 = 7709;
+    public long fillRefBitmap7710 = 7710;
+    public long fillRefBitmap7711 = 7711;
+    public long fillRefBitmap7712 = 7712;
+    public long fillRefBitmap7713 = 7713;
+    public long fillRefBitmap7714 = 7714;
+    public long fillRefBitmap7715 = 7715;
+    public long fillRefBitmap7716 = 7716;
+    public long fillRefBitmap7717 = 7717;
+    public long fillRefBitmap7718 = 7718;
+    public long fillRefBitmap7719 = 7719;
+    public long fillRefBitmap7720 = 7720;
+    public long fillRefBitmap7721 = 7721;
+    public long fillRefBitmap7722 = 7722;
+    public long fillRefBitmap7723 = 7723;
+    public long fillRefBitmap7724 = 7724;
+    public long fillRefBitmap7725 = 7725;
+    public long fillRefBitmap7726 = 7726;
+    public long fillRefBitmap7727 = 7727;
+    public long fillRefBitmap7728 = 7728;
+    public long fillRefBitmap7729 = 7729;
+    public long fillRefBitmap7730 = 7730;
+    public long fillRefBitmap7731 = 7731;
+    public long fillRefBitmap7732 = 7732;
+    public long fillRefBitmap7733 = 7733;
+    public long fillRefBitmap7734 = 7734;
+    public long fillRefBitmap7735 = 7735;
+    public long fillRefBitmap7736 = 7736;
+    public long fillRefBitmap7737 = 7737;
+    public long fillRefBitmap7738 = 7738;
+    public long fillRefBitmap7739 = 7739;
+    public long fillRefBitmap7740 = 7740;
+    public long fillRefBitmap7741 = 7741;
+    public long fillRefBitmap7742 = 7742;
+    public long fillRefBitmap7743 = 7743;
+    public long fillRefBitmap7744 = 7744;
+    public long fillRefBitmap7745 = 7745;
+    public long fillRefBitmap7746 = 7746;
+    public long fillRefBitmap7747 = 7747;
+    public long fillRefBitmap7748 = 7748;
+    public long fillRefBitmap7749 = 7749;
+    public long fillRefBitmap7750 = 7750;
+    public long fillRefBitmap7751 = 7751;
+    public long fillRefBitmap7752 = 7752;
+    public long fillRefBitmap7753 = 7753;
+    public long fillRefBitmap7754 = 7754;
+    public long fillRefBitmap7755 = 7755;
+    public long fillRefBitmap7756 = 7756;
+    public long fillRefBitmap7757 = 7757;
+    public long fillRefBitmap7758 = 7758;
+    public long fillRefBitmap7759 = 7759;
+    public long fillRefBitmap7760 = 7760;
+    public long fillRefBitmap7761 = 7761;
+    public long fillRefBitmap7762 = 7762;
+    public long fillRefBitmap7763 = 7763;
+    public long fillRefBitmap7764 = 7764;
+    public long fillRefBitmap7765 = 7765;
+    public long fillRefBitmap7766 = 7766;
+    public long fillRefBitmap7767 = 7767;
+    public long fillRefBitmap7768 = 7768;
+    public long fillRefBitmap7769 = 7769;
+    public long fillRefBitmap7770 = 7770;
+    public long fillRefBitmap7771 = 7771;
+    public long fillRefBitmap7772 = 7772;
+    public long fillRefBitmap7773 = 7773;
+    public long fillRefBitmap7774 = 7774;
+    public long fillRefBitmap7775 = 7775;
+    public long fillRefBitmap7776 = 7776;
+    public long fillRefBitmap7777 = 7777;
+    public long fillRefBitmap7778 = 7778;
+    public long fillRefBitmap7779 = 7779;
+    public long fillRefBitmap7780 = 7780;
+    public long fillRefBitmap7781 = 7781;
+    public long fillRefBitmap7782 = 7782;
+    public long fillRefBitmap7783 = 7783;
+    public long fillRefBitmap7784 = 7784;
+    public long fillRefBitmap7785 = 7785;
+    public long fillRefBitmap7786 = 7786;
+    public long fillRefBitmap7787 = 7787;
+    public long fillRefBitmap7788 = 7788;
+    public long fillRefBitmap7789 = 7789;
+    public long fillRefBitmap7790 = 7790;
+    public long fillRefBitmap7791 = 7791;
+    public long fillRefBitmap7792 = 7792;
+    public long fillRefBitmap7793 = 7793;
+    public long fillRefBitmap7794 = 7794;
+    public long fillRefBitmap7795 = 7795;
+    public long fillRefBitmap7796 = 7796;
+    public long fillRefBitmap7797 = 7797;
+    public long fillRefBitmap7798 = 7798;
+    public long fillRefBitmap7799 = 7799;
+    public long fillRefBitmap7800 = 7800;
+    public long fillRefBitmap7801 = 7801;
+    public long fillRefBitmap7802 = 7802;
+    public long fillRefBitmap7803 = 7803;
+    public long fillRefBitmap7804 = 7804;
+    public long fillRefBitmap7805 = 7805;
+    public long fillRefBitmap7806 = 7806;
+    public long fillRefBitmap7807 = 7807;
+    public long fillRefBitmap7808 = 7808;
+    public long fillRefBitmap7809 = 7809;
+    public long fillRefBitmap7810 = 7810;
+    public long fillRefBitmap7811 = 7811;
+    public long fillRefBitmap7812 = 7812;
+    public long fillRefBitmap7813 = 7813;
+    public long fillRefBitmap7814 = 7814;
+    public long fillRefBitmap7815 = 7815;
+    public long fillRefBitmap7816 = 7816;
+    public long fillRefBitmap7817 = 7817;
+    public long fillRefBitmap7818 = 7818;
+    public long fillRefBitmap7819 = 7819;
+    public long fillRefBitmap7820 = 7820;
+    public long fillRefBitmap7821 = 7821;
+    public long fillRefBitmap7822 = 7822;
+    public long fillRefBitmap7823 = 7823;
+    public long fillRefBitmap7824 = 7824;
+    public long fillRefBitmap7825 = 7825;
+    public long fillRefBitmap7826 = 7826;
+    public long fillRefBitmap7827 = 7827;
+    public long fillRefBitmap7828 = 7828;
+    public long fillRefBitmap7829 = 7829;
+    public long fillRefBitmap7830 = 7830;
+    public long fillRefBitmap7831 = 7831;
+    public long fillRefBitmap7832 = 7832;
+    public long fillRefBitmap7833 = 7833;
+    public long fillRefBitmap7834 = 7834;
+    public long fillRefBitmap7835 = 7835;
+    public long fillRefBitmap7836 = 7836;
+    public long fillRefBitmap7837 = 7837;
+    public long fillRefBitmap7838 = 7838;
+    public long fillRefBitmap7839 = 7839;
+    public long fillRefBitmap7840 = 7840;
+    public long fillRefBitmap7841 = 7841;
+    public long fillRefBitmap7842 = 7842;
+    public long fillRefBitmap7843 = 7843;
+    public long fillRefBitmap7844 = 7844;
+    public long fillRefBitmap7845 = 7845;
+    public long fillRefBitmap7846 = 7846;
+    public long fillRefBitmap7847 = 7847;
+    public long fillRefBitmap7848 = 7848;
+    public long fillRefBitmap7849 = 7849;
+    public long fillRefBitmap7850 = 7850;
+    public long fillRefBitmap7851 = 7851;
+    public long fillRefBitmap7852 = 7852;
+    public long fillRefBitmap7853 = 7853;
+    public long fillRefBitmap7854 = 7854;
+    public long fillRefBitmap7855 = 7855;
+    public long fillRefBitmap7856 = 7856;
+    public long fillRefBitmap7857 = 7857;
+    public long fillRefBitmap7858 = 7858;
+    public long fillRefBitmap7859 = 7859;
+    public long fillRefBitmap7860 = 7860;
+    public long fillRefBitmap7861 = 7861;
+    public long fillRefBitmap7862 = 7862;
+    public long fillRefBitmap7863 = 7863;
+    public long fillRefBitmap7864 = 7864;
+    public long fillRefBitmap7865 = 7865;
+    public long fillRefBitmap7866 = 7866;
+    public long fillRefBitmap7867 = 7867;
+    public long fillRefBitmap7868 = 7868;
+    public long fillRefBitmap7869 = 7869;
+    public long fillRefBitmap7870 = 7870;
+    public long fillRefBitmap7871 = 7871;
+    public long fillRefBitmap7872 = 7872;
+    public long fillRefBitmap7873 = 7873;
+    public long fillRefBitmap7874 = 7874;
+    public long fillRefBitmap7875 = 7875;
+    public long fillRefBitmap7876 = 7876;
+    public long fillRefBitmap7877 = 7877;
+    public long fillRefBitmap7878 = 7878;
+    public long fillRefBitmap7879 = 7879;
+    public long fillRefBitmap7880 = 7880;
+    public long fillRefBitmap7881 = 7881;
+    public long fillRefBitmap7882 = 7882;
+    public long fillRefBitmap7883 = 7883;
+    public long fillRefBitmap7884 = 7884;
+    public long fillRefBitmap7885 = 7885;
+    public long fillRefBitmap7886 = 7886;
+    public long fillRefBitmap7887 = 7887;
+    public long fillRefBitmap7888 = 7888;
+    public long fillRefBitmap7889 = 7889;
+    public long fillRefBitmap7890 = 7890;
+    public long fillRefBitmap7891 = 7891;
+    public long fillRefBitmap7892 = 7892;
+    public long fillRefBitmap7893 = 7893;
+    public long fillRefBitmap7894 = 7894;
+    public long fillRefBitmap7895 = 7895;
+    public long fillRefBitmap7896 = 7896;
+    public long fillRefBitmap7897 = 7897;
+    public long fillRefBitmap7898 = 7898;
+    public long fillRefBitmap7899 = 7899;
+    public long fillRefBitmap7900 = 7900;
+    public long fillRefBitmap7901 = 7901;
+    public long fillRefBitmap7902 = 7902;
+    public long fillRefBitmap7903 = 7903;
+    public long fillRefBitmap7904 = 7904;
+    public long fillRefBitmap7905 = 7905;
+    public long fillRefBitmap7906 = 7906;
+    public long fillRefBitmap7907 = 7907;
+    public long fillRefBitmap7908 = 7908;
+    public long fillRefBitmap7909 = 7909;
+    public long fillRefBitmap7910 = 7910;
+    public long fillRefBitmap7911 = 7911;
+    public long fillRefBitmap7912 = 7912;
+    public long fillRefBitmap7913 = 7913;
+    public long fillRefBitmap7914 = 7914;
+    public long fillRefBitmap7915 = 7915;
+    public long fillRefBitmap7916 = 7916;
+    public long fillRefBitmap7917 = 7917;
+    public long fillRefBitmap7918 = 7918;
+    public long fillRefBitmap7919 = 7919;
+    public long fillRefBitmap7920 = 7920;
+    public long fillRefBitmap7921 = 7921;
+    public long fillRefBitmap7922 = 7922;
+    public long fillRefBitmap7923 = 7923;
+    public long fillRefBitmap7924 = 7924;
+    public long fillRefBitmap7925 = 7925;
+    public long fillRefBitmap7926 = 7926;
+    public long fillRefBitmap7927 = 7927;
+    public long fillRefBitmap7928 = 7928;
+    public long fillRefBitmap7929 = 7929;
+    public long fillRefBitmap7930 = 7930;
+    public long fillRefBitmap7931 = 7931;
+    public long fillRefBitmap7932 = 7932;
+    public long fillRefBitmap7933 = 7933;
+    public long fillRefBitmap7934 = 7934;
+    public long fillRefBitmap7935 = 7935;
+    public long fillRefBitmap7936 = 7936;
+    public long fillRefBitmap7937 = 7937;
+    public long fillRefBitmap7938 = 7938;
+    public long fillRefBitmap7939 = 7939;
+    public long fillRefBitmap7940 = 7940;
+    public long fillRefBitmap7941 = 7941;
+    public long fillRefBitmap7942 = 7942;
+    public long fillRefBitmap7943 = 7943;
+    public long fillRefBitmap7944 = 7944;
+    public long fillRefBitmap7945 = 7945;
+    public long fillRefBitmap7946 = 7946;
+    public long fillRefBitmap7947 = 7947;
+    public long fillRefBitmap7948 = 7948;
+    public long fillRefBitmap7949 = 7949;
+    public long fillRefBitmap7950 = 7950;
+    public long fillRefBitmap7951 = 7951;
+    public long fillRefBitmap7952 = 7952;
+    public long fillRefBitmap7953 = 7953;
+    public long fillRefBitmap7954 = 7954;
+    public long fillRefBitmap7955 = 7955;
+    public long fillRefBitmap7956 = 7956;
+    public long fillRefBitmap7957 = 7957;
+    public long fillRefBitmap7958 = 7958;
+    public long fillRefBitmap7959 = 7959;
+    public long fillRefBitmap7960 = 7960;
+    public long fillRefBitmap7961 = 7961;
+    public long fillRefBitmap7962 = 7962;
+    public long fillRefBitmap7963 = 7963;
+    public long fillRefBitmap7964 = 7964;
+    public long fillRefBitmap7965 = 7965;
+    public long fillRefBitmap7966 = 7966;
+    public long fillRefBitmap7967 = 7967;
+    public long fillRefBitmap7968 = 7968;
+    public long fillRefBitmap7969 = 7969;
+    public long fillRefBitmap7970 = 7970;
+    public long fillRefBitmap7971 = 7971;
+    public long fillRefBitmap7972 = 7972;
+    public long fillRefBitmap7973 = 7973;
+    public long fillRefBitmap7974 = 7974;
+    public long fillRefBitmap7975 = 7975;
+    public long fillRefBitmap7976 = 7976;
+    public long fillRefBitmap7977 = 7977;
+    public long fillRefBitmap7978 = 7978;
+    public long fillRefBitmap7979 = 7979;
+    public long fillRefBitmap7980 = 7980;
+    public long fillRefBitmap7981 = 7981;
+    public long fillRefBitmap7982 = 7982;
+    public long fillRefBitmap7983 = 7983;
+    public long fillRefBitmap7984 = 7984;
+    public long fillRefBitmap7985 = 7985;
+    public long fillRefBitmap7986 = 7986;
+    public long fillRefBitmap7987 = 7987;
+    public long fillRefBitmap7988 = 7988;
+    public long fillRefBitmap7989 = 7989;
+    public long fillRefBitmap7990 = 7990;
+    public long fillRefBitmap7991 = 7991;
+    public long fillRefBitmap7992 = 7992;
+    public long fillRefBitmap7993 = 7993;
+    public long fillRefBitmap7994 = 7994;
+    public long fillRefBitmap7995 = 7995;
+    public long fillRefBitmap7996 = 7996;
+    public long fillRefBitmap7997 = 7997;
+    public long fillRefBitmap7998 = 7998;
+    public long fillRefBitmap7999 = 7999;
+    public long fillRefBitmap8000 = 8000;
+    public long fillRefBitmap8001 = 8001;
+    public long fillRefBitmap8002 = 8002;
+    public long fillRefBitmap8003 = 8003;
+    public long fillRefBitmap8004 = 8004;
+    public long fillRefBitmap8005 = 8005;
+    public long fillRefBitmap8006 = 8006;
+    public long fillRefBitmap8007 = 8007;
+    public long fillRefBitmap8008 = 8008;
+    public long fillRefBitmap8009 = 8009;
+    public long fillRefBitmap8010 = 8010;
+    public long fillRefBitmap8011 = 8011;
+    public long fillRefBitmap8012 = 8012;
+    public long fillRefBitmap8013 = 8013;
+    public long fillRefBitmap8014 = 8014;
+    public long fillRefBitmap8015 = 8015;
+    public long fillRefBitmap8016 = 8016;
+    public long fillRefBitmap8017 = 8017;
+    public long fillRefBitmap8018 = 8018;
+    public long fillRefBitmap8019 = 8019;
+    public long fillRefBitmap8020 = 8020;
+    public long fillRefBitmap8021 = 8021;
+    public long fillRefBitmap8022 = 8022;
+    public long fillRefBitmap8023 = 8023;
+    public long fillRefBitmap8024 = 8024;
+    public long fillRefBitmap8025 = 8025;
+    public long fillRefBitmap8026 = 8026;
+    public long fillRefBitmap8027 = 8027;
+    public long fillRefBitmap8028 = 8028;
+    public long fillRefBitmap8029 = 8029;
+    public long fillRefBitmap8030 = 8030;
+    public long fillRefBitmap8031 = 8031;
+    public long fillRefBitmap8032 = 8032;
+    public long fillRefBitmap8033 = 8033;
+    public long fillRefBitmap8034 = 8034;
+    public long fillRefBitmap8035 = 8035;
+    public long fillRefBitmap8036 = 8036;
+    public long fillRefBitmap8037 = 8037;
+    public long fillRefBitmap8038 = 8038;
+    public long fillRefBitmap8039 = 8039;
+    public long fillRefBitmap8040 = 8040;
+    public long fillRefBitmap8041 = 8041;
+    public long fillRefBitmap8042 = 8042;
+    public long fillRefBitmap8043 = 8043;
+    public long fillRefBitmap8044 = 8044;
+    public long fillRefBitmap8045 = 8045;
+    public long fillRefBitmap8046 = 8046;
+    public long fillRefBitmap8047 = 8047;
+    public long fillRefBitmap8048 = 8048;
+    public long fillRefBitmap8049 = 8049;
+    public long fillRefBitmap8050 = 8050;
+    public long fillRefBitmap8051 = 8051;
+    public long fillRefBitmap8052 = 8052;
+    public long fillRefBitmap8053 = 8053;
+    public long fillRefBitmap8054 = 8054;
+    public long fillRefBitmap8055 = 8055;
+    public long fillRefBitmap8056 = 8056;
+    public long fillRefBitmap8057 = 8057;
+    public long fillRefBitmap8058 = 8058;
+    public long fillRefBitmap8059 = 8059;
+    public long fillRefBitmap8060 = 8060;
+    public long fillRefBitmap8061 = 8061;
+    public long fillRefBitmap8062 = 8062;
+    public long fillRefBitmap8063 = 8063;
+    public long fillRefBitmap8064 = 8064;
+    public long fillRefBitmap8065 = 8065;
+    public long fillRefBitmap8066 = 8066;
+    public long fillRefBitmap8067 = 8067;
+    public long fillRefBitmap8068 = 8068;
+    public long fillRefBitmap8069 = 8069;
+    public long fillRefBitmap8070 = 8070;
+    public long fillRefBitmap8071 = 8071;
+    public long fillRefBitmap8072 = 8072;
+    public long fillRefBitmap8073 = 8073;
+    public long fillRefBitmap8074 = 8074;
+    public long fillRefBitmap8075 = 8075;
+    public long fillRefBitmap8076 = 8076;
+    public long fillRefBitmap8077 = 8077;
+    public long fillRefBitmap8078 = 8078;
+    public long fillRefBitmap8079 = 8079;
+    public long fillRefBitmap8080 = 8080;
+    public long fillRefBitmap8081 = 8081;
+    public long fillRefBitmap8082 = 8082;
+    public long fillRefBitmap8083 = 8083;
+    public long fillRefBitmap8084 = 8084;
+    public long fillRefBitmap8085 = 8085;
+    public long fillRefBitmap8086 = 8086;
+    public long fillRefBitmap8087 = 8087;
+    public long fillRefBitmap8088 = 8088;
+    public long fillRefBitmap8089 = 8089;
+    public long fillRefBitmap8090 = 8090;
+    public long fillRefBitmap8091 = 8091;
+    public long fillRefBitmap8092 = 8092;
+    public long fillRefBitmap8093 = 8093;
+    public long fillRefBitmap8094 = 8094;
+    public long fillRefBitmap8095 = 8095;
+    public long fillRefBitmap8096 = 8096;
+    public long fillRefBitmap8097 = 8097;
+    public long fillRefBitmap8098 = 8098;
+    public long fillRefBitmap8099 = 8099;
+    public long fillRefBitmap8100 = 8100;
+    public long fillRefBitmap8101 = 8101;
+    public long fillRefBitmap8102 = 8102;
+    public long fillRefBitmap8103 = 8103;
+    public long fillRefBitmap8104 = 8104;
+    public long fillRefBitmap8105 = 8105;
+    public long fillRefBitmap8106 = 8106;
+    public long fillRefBitmap8107 = 8107;
+    public long fillRefBitmap8108 = 8108;
+    public long fillRefBitmap8109 = 8109;
+    public long fillRefBitmap8110 = 8110;
+    public long fillRefBitmap8111 = 8111;
+    public long fillRefBitmap8112 = 8112;
+    public long fillRefBitmap8113 = 8113;
+    public long fillRefBitmap8114 = 8114;
+    public long fillRefBitmap8115 = 8115;
+    public long fillRefBitmap8116 = 8116;
+    public long fillRefBitmap8117 = 8117;
+    public long fillRefBitmap8118 = 8118;
+    public long fillRefBitmap8119 = 8119;
+    public long fillRefBitmap8120 = 8120;
+    public long fillRefBitmap8121 = 8121;
+    public long fillRefBitmap8122 = 8122;
+    public long fillRefBitmap8123 = 8123;
+    public long fillRefBitmap8124 = 8124;
+    public long fillRefBitmap8125 = 8125;
+    public long fillRefBitmap8126 = 8126;
+    public long fillRefBitmap8127 = 8127;
+    public long fillRefBitmap8128 = 8128;
+    public long fillRefBitmap8129 = 8129;
+    public long fillRefBitmap8130 = 8130;
+    public long fillRefBitmap8131 = 8131;
+    public long fillRefBitmap8132 = 8132;
+    public long fillRefBitmap8133 = 8133;
+    public long fillRefBitmap8134 = 8134;
+    public long fillRefBitmap8135 = 8135;
+    public long fillRefBitmap8136 = 8136;
+    public long fillRefBitmap8137 = 8137;
+    public long fillRefBitmap8138 = 8138;
+    public long fillRefBitmap8139 = 8139;
+    public long fillRefBitmap8140 = 8140;
+    public long fillRefBitmap8141 = 8141;
+    public long fillRefBitmap8142 = 8142;
+    public long fillRefBitmap8143 = 8143;
+    public long fillRefBitmap8144 = 8144;
+    public long fillRefBitmap8145 = 8145;
+    public long fillRefBitmap8146 = 8146;
+    public long fillRefBitmap8147 = 8147;
+    public long fillRefBitmap8148 = 8148;
+    public long fillRefBitmap8149 = 8149;
+    public long fillRefBitmap8150 = 8150;
+    public long fillRefBitmap8151 = 8151;
+    public long fillRefBitmap8152 = 8152;
+    public long fillRefBitmap8153 = 8153;
+    public long fillRefBitmap8154 = 8154;
+    public long fillRefBitmap8155 = 8155;
+    public long fillRefBitmap8156 = 8156;
+    public long fillRefBitmap8157 = 8157;
+    public long fillRefBitmap8158 = 8158;
+    public long fillRefBitmap8159 = 8159;
+    public long fillRefBitmap8160 = 8160;
+    public long fillRefBitmap8161 = 8161;
+    public long fillRefBitmap8162 = 8162;
+    public long fillRefBitmap8163 = 8163;
+    public long fillRefBitmap8164 = 8164;
+    public long fillRefBitmap8165 = 8165;
+    public long fillRefBitmap8166 = 8166;
+    public long fillRefBitmap8167 = 8167;
+    public long fillRefBitmap8168 = 8168;
+    public long fillRefBitmap8169 = 8169;
+    public long fillRefBitmap8170 = 8170;
+    public long fillRefBitmap8171 = 8171;
+    public long fillRefBitmap8172 = 8172;
+    public long fillRefBitmap8173 = 8173;
+    public long fillRefBitmap8174 = 8174;
+    public long fillRefBitmap8175 = 8175;
+    public long fillRefBitmap8176 = 8176;
+    public long fillRefBitmap8177 = 8177;
+    public long fillRefBitmap8178 = 8178;
+    public long fillRefBitmap8179 = 8179;
+    public long fillRefBitmap8180 = 8180;
+    public long fillRefBitmap8181 = 8181;
+    public long fillRefBitmap8182 = 8182;
+    public long fillRefBitmap8183 = 8183;
+    public long fillRefBitmap8184 = 8184;
+    public long fillRefBitmap8185 = 8185;
+    public long fillRefBitmap8186 = 8186;
+    public long fillRefBitmap8187 = 8187;
+    public long fillRefBitmap8188 = 8188;
+    public long fillRefBitmap8189 = 8189;
+    public long fillRefBitmap8190 = 8190;
+    public long fillRefBitmap8191 = 8191;
+    public long fillRefBitmap8192 = 8192;
+    public long fillRefBitmap8193 = 8193;
+    public long fillRefBitmap8194 = 8194;
+    public long fillRefBitmap8195 = 8195;
+    public long fillRefBitmap8196 = 8196;
+    public long fillRefBitmap8197 = 8197;
+    public long fillRefBitmap8198 = 8198;
+    public long fillRefBitmap8199 = 8199;
+    public long fillRefBitmap8200 = 8200;
+    public long fillRefBitmap8201 = 8201;
+    public long fillRefBitmap8202 = 8202;
+    public long fillRefBitmap8203 = 8203;
+    public long fillRefBitmap8204 = 8204;
+    public long fillRefBitmap8205 = 8205;
+    public long fillRefBitmap8206 = 8206;
+    public long fillRefBitmap8207 = 8207;
+    public long fillRefBitmap8208 = 8208;
+    public long fillRefBitmap8209 = 8209;
+    public long fillRefBitmap8210 = 8210;
+    public long fillRefBitmap8211 = 8211;
+    public long fillRefBitmap8212 = 8212;
+    public long fillRefBitmap8213 = 8213;
+    public long fillRefBitmap8214 = 8214;
+    public long fillRefBitmap8215 = 8215;
+    public long fillRefBitmap8216 = 8216;
+    public long fillRefBitmap8217 = 8217;
+    public long fillRefBitmap8218 = 8218;
+    public long fillRefBitmap8219 = 8219;
+    public long fillRefBitmap8220 = 8220;
+    public long fillRefBitmap8221 = 8221;
+    public long fillRefBitmap8222 = 8222;
+    public long fillRefBitmap8223 = 8223;
+    public long fillRefBitmap8224 = 8224;
+    public long fillRefBitmap8225 = 8225;
+    public long fillRefBitmap8226 = 8226;
+    public long fillRefBitmap8227 = 8227;
+    public long fillRefBitmap8228 = 8228;
+    public long fillRefBitmap8229 = 8229;
+    public long fillRefBitmap8230 = 8230;
+    public long fillRefBitmap8231 = 8231;
+    public long fillRefBitmap8232 = 8232;
+    public long fillRefBitmap8233 = 8233;
+    public long fillRefBitmap8234 = 8234;
+    public long fillRefBitmap8235 = 8235;
+    public long fillRefBitmap8236 = 8236;
+    public long fillRefBitmap8237 = 8237;
+    public long fillRefBitmap8238 = 8238;
+    public long fillRefBitmap8239 = 8239;
+    public long fillRefBitmap8240 = 8240;
+    public long fillRefBitmap8241 = 8241;
+    public long fillRefBitmap8242 = 8242;
+    public long fillRefBitmap8243 = 8243;
+    public long fillRefBitmap8244 = 8244;
+    public long fillRefBitmap8245 = 8245;
+    public long fillRefBitmap8246 = 8246;
+    public long fillRefBitmap8247 = 8247;
+    public long fillRefBitmap8248 = 8248;
+    public long fillRefBitmap8249 = 8249;
+    public long fillRefBitmap8250 = 8250;
+    public long fillRefBitmap8251 = 8251;
+    public long fillRefBitmap8252 = 8252;
+    public long fillRefBitmap8253 = 8253;
+    public long fillRefBitmap8254 = 8254;
+    public long fillRefBitmap8255 = 8255;
+    public long fillRefBitmap8256 = 8256;
+    public long fillRefBitmap8257 = 8257;
+    public long fillRefBitmap8258 = 8258;
+    public long fillRefBitmap8259 = 8259;
+    public long fillRefBitmap8260 = 8260;
+    public long fillRefBitmap8261 = 8261;
+    public long fillRefBitmap8262 = 8262;
+    public long fillRefBitmap8263 = 8263;
+    public long fillRefBitmap8264 = 8264;
+    public long fillRefBitmap8265 = 8265;
+    public long fillRefBitmap8266 = 8266;
+    public long fillRefBitmap8267 = 8267;
+    public long fillRefBitmap8268 = 8268;
+    public long fillRefBitmap8269 = 8269;
+    public long fillRefBitmap8270 = 8270;
+    public long fillRefBitmap8271 = 8271;
+    public long fillRefBitmap8272 = 8272;
+    public long fillRefBitmap8273 = 8273;
+    public long fillRefBitmap8274 = 8274;
+    public long fillRefBitmap8275 = 8275;
+    public long fillRefBitmap8276 = 8276;
+    public long fillRefBitmap8277 = 8277;
+    public long fillRefBitmap8278 = 8278;
+    public long fillRefBitmap8279 = 8279;
+    public long fillRefBitmap8280 = 8280;
+    public long fillRefBitmap8281 = 8281;
+    public long fillRefBitmap8282 = 8282;
+    public long fillRefBitmap8283 = 8283;
+    public long fillRefBitmap8284 = 8284;
+    public long fillRefBitmap8285 = 8285;
+    public long fillRefBitmap8286 = 8286;
+    public long fillRefBitmap8287 = 8287;
+    public long fillRefBitmap8288 = 8288;
+    public long fillRefBitmap8289 = 8289;
+    public long fillRefBitmap8290 = 8290;
+    public long fillRefBitmap8291 = 8291;
+    public long fillRefBitmap8292 = 8292;
+    public long fillRefBitmap8293 = 8293;
+    public long fillRefBitmap8294 = 8294;
+    public long fillRefBitmap8295 = 8295;
+    public long fillRefBitmap8296 = 8296;
+    public long fillRefBitmap8297 = 8297;
+    public long fillRefBitmap8298 = 8298;
+    public long fillRefBitmap8299 = 8299;
+    public long fillRefBitmap8300 = 8300;
+    public long fillRefBitmap8301 = 8301;
+    public long fillRefBitmap8302 = 8302;
+    public long fillRefBitmap8303 = 8303;
+    public long fillRefBitmap8304 = 8304;
+    public long fillRefBitmap8305 = 8305;
+    public long fillRefBitmap8306 = 8306;
+    public long fillRefBitmap8307 = 8307;
+    public long fillRefBitmap8308 = 8308;
+    public long fillRefBitmap8309 = 8309;
+    public long fillRefBitmap8310 = 8310;
+    public long fillRefBitmap8311 = 8311;
+    public long fillRefBitmap8312 = 8312;
+    public long fillRefBitmap8313 = 8313;
+    public long fillRefBitmap8314 = 8314;
+    public long fillRefBitmap8315 = 8315;
+    public long fillRefBitmap8316 = 8316;
+    public long fillRefBitmap8317 = 8317;
+    public long fillRefBitmap8318 = 8318;
+    public long fillRefBitmap8319 = 8319;
+    public long fillRefBitmap8320 = 8320;
+    public long fillRefBitmap8321 = 8321;
+    public long fillRefBitmap8322 = 8322;
+    public long fillRefBitmap8323 = 8323;
+    public long fillRefBitmap8324 = 8324;
+    public long fillRefBitmap8325 = 8325;
+    public long fillRefBitmap8326 = 8326;
+    public long fillRefBitmap8327 = 8327;
+    public long fillRefBitmap8328 = 8328;
+    public long fillRefBitmap8329 = 8329;
+    public long fillRefBitmap8330 = 8330;
+    public long fillRefBitmap8331 = 8331;
+    public long fillRefBitmap8332 = 8332;
+    public long fillRefBitmap8333 = 8333;
+    public long fillRefBitmap8334 = 8334;
+    public long fillRefBitmap8335 = 8335;
+    public long fillRefBitmap8336 = 8336;
+    public long fillRefBitmap8337 = 8337;
+    public long fillRefBitmap8338 = 8338;
+    public long fillRefBitmap8339 = 8339;
+    public long fillRefBitmap8340 = 8340;
+    public long fillRefBitmap8341 = 8341;
+    public long fillRefBitmap8342 = 8342;
+    public long fillRefBitmap8343 = 8343;
+    public long fillRefBitmap8344 = 8344;
+    public long fillRefBitmap8345 = 8345;
+    public long fillRefBitmap8346 = 8346;
+    public long fillRefBitmap8347 = 8347;
+    public long fillRefBitmap8348 = 8348;
+    public long fillRefBitmap8349 = 8349;
+    public long fillRefBitmap8350 = 8350;
+    public long fillRefBitmap8351 = 8351;
+    public long fillRefBitmap8352 = 8352;
+    public long fillRefBitmap8353 = 8353;
+    public long fillRefBitmap8354 = 8354;
+    public long fillRefBitmap8355 = 8355;
+    public long fillRefBitmap8356 = 8356;
+    public long fillRefBitmap8357 = 8357;
+    public long fillRefBitmap8358 = 8358;
+    public long fillRefBitmap8359 = 8359;
+    public long fillRefBitmap8360 = 8360;
+    public long fillRefBitmap8361 = 8361;
+    public long fillRefBitmap8362 = 8362;
+    public long fillRefBitmap8363 = 8363;
+    public long fillRefBitmap8364 = 8364;
+    public long fillRefBitmap8365 = 8365;
+    public long fillRefBitmap8366 = 8366;
+    public long fillRefBitmap8367 = 8367;
+    public long fillRefBitmap8368 = 8368;
+    public long fillRefBitmap8369 = 8369;
+    public long fillRefBitmap8370 = 8370;
+    public long fillRefBitmap8371 = 8371;
+    public long fillRefBitmap8372 = 8372;
+    public long fillRefBitmap8373 = 8373;
+    public long fillRefBitmap8374 = 8374;
+    public long fillRefBitmap8375 = 8375;
+    public long fillRefBitmap8376 = 8376;
+    public long fillRefBitmap8377 = 8377;
+    public long fillRefBitmap8378 = 8378;
+    public long fillRefBitmap8379 = 8379;
+    public long fillRefBitmap8380 = 8380;
+    public long fillRefBitmap8381 = 8381;
+    public long fillRefBitmap8382 = 8382;
+    public long fillRefBitmap8383 = 8383;
+    public long fillRefBitmap8384 = 8384;
+    public long fillRefBitmap8385 = 8385;
+    public long fillRefBitmap8386 = 8386;
+    public long fillRefBitmap8387 = 8387;
+    public long fillRefBitmap8388 = 8388;
+    public long fillRefBitmap8389 = 8389;
+    public long fillRefBitmap8390 = 8390;
+    public long fillRefBitmap8391 = 8391;
+    public long fillRefBitmap8392 = 8392;
+    public long fillRefBitmap8393 = 8393;
+    public long fillRefBitmap8394 = 8394;
+    public long fillRefBitmap8395 = 8395;
+    public long fillRefBitmap8396 = 8396;
+    public long fillRefBitmap8397 = 8397;
+    public long fillRefBitmap8398 = 8398;
+    public long fillRefBitmap8399 = 8399;
+    public long fillRefBitmap8400 = 8400;
+    public long fillRefBitmap8401 = 8401;
+    public long fillRefBitmap8402 = 8402;
+    public long fillRefBitmap8403 = 8403;
+    public long fillRefBitmap8404 = 8404;
+    public long fillRefBitmap8405 = 8405;
+    public long fillRefBitmap8406 = 8406;
+    public long fillRefBitmap8407 = 8407;
+    public long fillRefBitmap8408 = 8408;
+    public long fillRefBitmap8409 = 8409;
+    public long fillRefBitmap8410 = 8410;
+    public long fillRefBitmap8411 = 8411;
+    public long fillRefBitmap8412 = 8412;
+    public long fillRefBitmap8413 = 8413;
+    public long fillRefBitmap8414 = 8414;
+    public long fillRefBitmap8415 = 8415;
+    public long fillRefBitmap8416 = 8416;
+    public long fillRefBitmap8417 = 8417;
+    public long fillRefBitmap8418 = 8418;
+    public long fillRefBitmap8419 = 8419;
+    public long fillRefBitmap8420 = 8420;
+    public long fillRefBitmap8421 = 8421;
+    public long fillRefBitmap8422 = 8422;
+    public long fillRefBitmap8423 = 8423;
+    public long fillRefBitmap8424 = 8424;
+    public long fillRefBitmap8425 = 8425;
+    public long fillRefBitmap8426 = 8426;
+    public long fillRefBitmap8427 = 8427;
+    public long fillRefBitmap8428 = 8428;
+    public long fillRefBitmap8429 = 8429;
+    public long fillRefBitmap8430 = 8430;
+    public long fillRefBitmap8431 = 8431;
+    public long fillRefBitmap8432 = 8432;
+    public long fillRefBitmap8433 = 8433;
+    public long fillRefBitmap8434 = 8434;
+    public long fillRefBitmap8435 = 8435;
+    public long fillRefBitmap8436 = 8436;
+    public long fillRefBitmap8437 = 8437;
+    public long fillRefBitmap8438 = 8438;
+    public long fillRefBitmap8439 = 8439;
+    public long fillRefBitmap8440 = 8440;
+    public long fillRefBitmap8441 = 8441;
+    public long fillRefBitmap8442 = 8442;
+    public long fillRefBitmap8443 = 8443;
+    public long fillRefBitmap8444 = 8444;
+    public long fillRefBitmap8445 = 8445;
+    public long fillRefBitmap8446 = 8446;
+    public long fillRefBitmap8447 = 8447;
+    public long fillRefBitmap8448 = 8448;
+    public long fillRefBitmap8449 = 8449;
+    public long fillRefBitmap8450 = 8450;
+    public long fillRefBitmap8451 = 8451;
+    public long fillRefBitmap8452 = 8452;
+    public long fillRefBitmap8453 = 8453;
+    public long fillRefBitmap8454 = 8454;
+    public long fillRefBitmap8455 = 8455;
+    public long fillRefBitmap8456 = 8456;
+    public long fillRefBitmap8457 = 8457;
+    public long fillRefBitmap8458 = 8458;
+    public long fillRefBitmap8459 = 8459;
+    public long fillRefBitmap8460 = 8460;
+    public long fillRefBitmap8461 = 8461;
+    public long fillRefBitmap8462 = 8462;
+    public long fillRefBitmap8463 = 8463;
+    public long fillRefBitmap8464 = 8464;
+    public long fillRefBitmap8465 = 8465;
+    public long fillRefBitmap8466 = 8466;
+    public long fillRefBitmap8467 = 8467;
+    public long fillRefBitmap8468 = 8468;
+    public long fillRefBitmap8469 = 8469;
+    public long fillRefBitmap8470 = 8470;
+    public long fillRefBitmap8471 = 8471;
+    public long fillRefBitmap8472 = 8472;
+    public long fillRefBitmap8473 = 8473;
+    public long fillRefBitmap8474 = 8474;
+    public long fillRefBitmap8475 = 8475;
+    public long fillRefBitmap8476 = 8476;
+    public long fillRefBitmap8477 = 8477;
+    public long fillRefBitmap8478 = 8478;
+    public long fillRefBitmap8479 = 8479;
+    public long fillRefBitmap8480 = 8480;
+    public long fillRefBitmap8481 = 8481;
+    public long fillRefBitmap8482 = 8482;
+    public long fillRefBitmap8483 = 8483;
+    public long fillRefBitmap8484 = 8484;
+    public long fillRefBitmap8485 = 8485;
+    public long fillRefBitmap8486 = 8486;
+    public long fillRefBitmap8487 = 8487;
+    public long fillRefBitmap8488 = 8488;
+    public long fillRefBitmap8489 = 8489;
+    public long fillRefBitmap8490 = 8490;
+    public long fillRefBitmap8491 = 8491;
+    public long fillRefBitmap8492 = 8492;
+    public long fillRefBitmap8493 = 8493;
+    public long fillRefBitmap8494 = 8494;
+    public long fillRefBitmap8495 = 8495;
+    public long fillRefBitmap8496 = 8496;
+    public long fillRefBitmap8497 = 8497;
+    public long fillRefBitmap8498 = 8498;
+    public long fillRefBitmap8499 = 8499;
+    public long fillRefBitmap8500 = 8500;
+    public long fillRefBitmap8501 = 8501;
+    public long fillRefBitmap8502 = 8502;
+    public long fillRefBitmap8503 = 8503;
+    public long fillRefBitmap8504 = 8504;
+    public long fillRefBitmap8505 = 8505;
+    public long fillRefBitmap8506 = 8506;
+    public long fillRefBitmap8507 = 8507;
+    public long fillRefBitmap8508 = 8508;
+    public long fillRefBitmap8509 = 8509;
+    public long fillRefBitmap8510 = 8510;
+    public long fillRefBitmap8511 = 8511;
+    public long fillRefBitmap8512 = 8512;
+    public long fillRefBitmap8513 = 8513;
+    public long fillRefBitmap8514 = 8514;
+    public long fillRefBitmap8515 = 8515;
+    public long fillRefBitmap8516 = 8516;
+    public long fillRefBitmap8517 = 8517;
+    public long fillRefBitmap8518 = 8518;
+    public long fillRefBitmap8519 = 8519;
+    public long fillRefBitmap8520 = 8520;
+    public long fillRefBitmap8521 = 8521;
+    public long fillRefBitmap8522 = 8522;
+    public long fillRefBitmap8523 = 8523;
+    public long fillRefBitmap8524 = 8524;
+    public long fillRefBitmap8525 = 8525;
+    public long fillRefBitmap8526 = 8526;
+    public long fillRefBitmap8527 = 8527;
+    public long fillRefBitmap8528 = 8528;
+    public long fillRefBitmap8529 = 8529;
+    public long fillRefBitmap8530 = 8530;
+    public long fillRefBitmap8531 = 8531;
+    public long fillRefBitmap8532 = 8532;
+    public long fillRefBitmap8533 = 8533;
+    public long fillRefBitmap8534 = 8534;
+    public long fillRefBitmap8535 = 8535;
+    public long fillRefBitmap8536 = 8536;
+    public long fillRefBitmap8537 = 8537;
+    public long fillRefBitmap8538 = 8538;
+    public long fillRefBitmap8539 = 8539;
+    public long fillRefBitmap8540 = 8540;
+    public long fillRefBitmap8541 = 8541;
+    public long fillRefBitmap8542 = 8542;
+    public long fillRefBitmap8543 = 8543;
+    public long fillRefBitmap8544 = 8544;
+    public long fillRefBitmap8545 = 8545;
+    public long fillRefBitmap8546 = 8546;
+    public long fillRefBitmap8547 = 8547;
+    public long fillRefBitmap8548 = 8548;
+    public long fillRefBitmap8549 = 8549;
+    public long fillRefBitmap8550 = 8550;
+    public long fillRefBitmap8551 = 8551;
+    public long fillRefBitmap8552 = 8552;
+    public long fillRefBitmap8553 = 8553;
+    public long fillRefBitmap8554 = 8554;
+    public long fillRefBitmap8555 = 8555;
+    public long fillRefBitmap8556 = 8556;
+    public long fillRefBitmap8557 = 8557;
+    public long fillRefBitmap8558 = 8558;
+    public long fillRefBitmap8559 = 8559;
+    public long fillRefBitmap8560 = 8560;
+    public long fillRefBitmap8561 = 8561;
+    public long fillRefBitmap8562 = 8562;
+    public long fillRefBitmap8563 = 8563;
+    public long fillRefBitmap8564 = 8564;
+    public long fillRefBitmap8565 = 8565;
+    public long fillRefBitmap8566 = 8566;
+    public long fillRefBitmap8567 = 8567;
+    public long fillRefBitmap8568 = 8568;
+    public long fillRefBitmap8569 = 8569;
+    public long fillRefBitmap8570 = 8570;
+    public long fillRefBitmap8571 = 8571;
+    public long fillRefBitmap8572 = 8572;
+    public long fillRefBitmap8573 = 8573;
+    public long fillRefBitmap8574 = 8574;
+    public long fillRefBitmap8575 = 8575;
+    public long fillRefBitmap8576 = 8576;
+    public long fillRefBitmap8577 = 8577;
+    public long fillRefBitmap8578 = 8578;
+    public long fillRefBitmap8579 = 8579;
+    public long fillRefBitmap8580 = 8580;
+    public long fillRefBitmap8581 = 8581;
+    public long fillRefBitmap8582 = 8582;
+    public long fillRefBitmap8583 = 8583;
+    public long fillRefBitmap8584 = 8584;
+    public long fillRefBitmap8585 = 8585;
+    public long fillRefBitmap8586 = 8586;
+    public long fillRefBitmap8587 = 8587;
+    public long fillRefBitmap8588 = 8588;
+    public long fillRefBitmap8589 = 8589;
+    public long fillRefBitmap8590 = 8590;
+    public long fillRefBitmap8591 = 8591;
+    public long fillRefBitmap8592 = 8592;
+    public long fillRefBitmap8593 = 8593;
+    public long fillRefBitmap8594 = 8594;
+    public long fillRefBitmap8595 = 8595;
+    public long fillRefBitmap8596 = 8596;
+    public long fillRefBitmap8597 = 8597;
+    public long fillRefBitmap8598 = 8598;
+    public long fillRefBitmap8599 = 8599;
+    public long fillRefBitmap8600 = 8600;
+    public long fillRefBitmap8601 = 8601;
+    public long fillRefBitmap8602 = 8602;
+    public long fillRefBitmap8603 = 8603;
+    public long fillRefBitmap8604 = 8604;
+    public long fillRefBitmap8605 = 8605;
+    public long fillRefBitmap8606 = 8606;
+    public long fillRefBitmap8607 = 8607;
+    public long fillRefBitmap8608 = 8608;
+    public long fillRefBitmap8609 = 8609;
+    public long fillRefBitmap8610 = 8610;
+    public long fillRefBitmap8611 = 8611;
+    public long fillRefBitmap8612 = 8612;
+    public long fillRefBitmap8613 = 8613;
+    public long fillRefBitmap8614 = 8614;
+    public long fillRefBitmap8615 = 8615;
+    public long fillRefBitmap8616 = 8616;
+    public long fillRefBitmap8617 = 8617;
+    public long fillRefBitmap8618 = 8618;
+    public long fillRefBitmap8619 = 8619;
+    public long fillRefBitmap8620 = 8620;
+    public long fillRefBitmap8621 = 8621;
+    public long fillRefBitmap8622 = 8622;
+    public long fillRefBitmap8623 = 8623;
+    public long fillRefBitmap8624 = 8624;
+    public long fillRefBitmap8625 = 8625;
+    public long fillRefBitmap8626 = 8626;
+    public long fillRefBitmap8627 = 8627;
+    public long fillRefBitmap8628 = 8628;
+    public long fillRefBitmap8629 = 8629;
+    public long fillRefBitmap8630 = 8630;
+    public long fillRefBitmap8631 = 8631;
+    public long fillRefBitmap8632 = 8632;
+    public long fillRefBitmap8633 = 8633;
+    public long fillRefBitmap8634 = 8634;
+    public long fillRefBitmap8635 = 8635;
+    public long fillRefBitmap8636 = 8636;
+    public long fillRefBitmap8637 = 8637;
+    public long fillRefBitmap8638 = 8638;
+    public long fillRefBitmap8639 = 8639;
+    public long fillRefBitmap8640 = 8640;
+    public long fillRefBitmap8641 = 8641;
+    public long fillRefBitmap8642 = 8642;
+    public long fillRefBitmap8643 = 8643;
+    public long fillRefBitmap8644 = 8644;
+    public long fillRefBitmap8645 = 8645;
+    public long fillRefBitmap8646 = 8646;
+    public long fillRefBitmap8647 = 8647;
+    public long fillRefBitmap8648 = 8648;
+    public long fillRefBitmap8649 = 8649;
+    public long fillRefBitmap8650 = 8650;
+    public long fillRefBitmap8651 = 8651;
+    public long fillRefBitmap8652 = 8652;
+    public long fillRefBitmap8653 = 8653;
+    public long fillRefBitmap8654 = 8654;
+    public long fillRefBitmap8655 = 8655;
+    public long fillRefBitmap8656 = 8656;
+    public long fillRefBitmap8657 = 8657;
+    public long fillRefBitmap8658 = 8658;
+    public long fillRefBitmap8659 = 8659;
+    public long fillRefBitmap8660 = 8660;
+    public long fillRefBitmap8661 = 8661;
+    public long fillRefBitmap8662 = 8662;
+    public long fillRefBitmap8663 = 8663;
+    public long fillRefBitmap8664 = 8664;
+    public long fillRefBitmap8665 = 8665;
+    public long fillRefBitmap8666 = 8666;
+    public long fillRefBitmap8667 = 8667;
+    public long fillRefBitmap8668 = 8668;
+    public long fillRefBitmap8669 = 8669;
+    public long fillRefBitmap8670 = 8670;
+    public long fillRefBitmap8671 = 8671;
+    public long fillRefBitmap8672 = 8672;
+    public long fillRefBitmap8673 = 8673;
+    public long fillRefBitmap8674 = 8674;
+    public long fillRefBitmap8675 = 8675;
+    public long fillRefBitmap8676 = 8676;
+    public long fillRefBitmap8677 = 8677;
+    public long fillRefBitmap8678 = 8678;
+    public long fillRefBitmap8679 = 8679;
+    public long fillRefBitmap8680 = 8680;
+    public long fillRefBitmap8681 = 8681;
+    public long fillRefBitmap8682 = 8682;
+    public long fillRefBitmap8683 = 8683;
+    public long fillRefBitmap8684 = 8684;
+    public long fillRefBitmap8685 = 8685;
+    public long fillRefBitmap8686 = 8686;
+    public long fillRefBitmap8687 = 8687;
+    public long fillRefBitmap8688 = 8688;
+    public long fillRefBitmap8689 = 8689;
+    public long fillRefBitmap8690 = 8690;
+    public long fillRefBitmap8691 = 8691;
+    public long fillRefBitmap8692 = 8692;
+    public long fillRefBitmap8693 = 8693;
+    public long fillRefBitmap8694 = 8694;
+    public long fillRefBitmap8695 = 8695;
+    public long fillRefBitmap8696 = 8696;
+    public long fillRefBitmap8697 = 8697;
+    public long fillRefBitmap8698 = 8698;
+    public long fillRefBitmap8699 = 8699;
+    public long fillRefBitmap8700 = 8700;
+    public long fillRefBitmap8701 = 8701;
+    public long fillRefBitmap8702 = 8702;
+    public long fillRefBitmap8703 = 8703;
+    public long fillRefBitmap8704 = 8704;
+    public long fillRefBitmap8705 = 8705;
+    public long fillRefBitmap8706 = 8706;
+    public long fillRefBitmap8707 = 8707;
+    public long fillRefBitmap8708 = 8708;
+    public long fillRefBitmap8709 = 8709;
+    public long fillRefBitmap8710 = 8710;
+    public long fillRefBitmap8711 = 8711;
+    public long fillRefBitmap8712 = 8712;
+    public long fillRefBitmap8713 = 8713;
+    public long fillRefBitmap8714 = 8714;
+    public long fillRefBitmap8715 = 8715;
+    public long fillRefBitmap8716 = 8716;
+    public long fillRefBitmap8717 = 8717;
+    public long fillRefBitmap8718 = 8718;
+    public long fillRefBitmap8719 = 8719;
+    public long fillRefBitmap8720 = 8720;
+    public long fillRefBitmap8721 = 8721;
+    public long fillRefBitmap8722 = 8722;
+    public long fillRefBitmap8723 = 8723;
+    public long fillRefBitmap8724 = 8724;
+    public long fillRefBitmap8725 = 8725;
+    public long fillRefBitmap8726 = 8726;
+    public long fillRefBitmap8727 = 8727;
+    public long fillRefBitmap8728 = 8728;
+    public long fillRefBitmap8729 = 8729;
+    public long fillRefBitmap8730 = 8730;
+    public long fillRefBitmap8731 = 8731;
+    public long fillRefBitmap8732 = 8732;
+    public long fillRefBitmap8733 = 8733;
+    public long fillRefBitmap8734 = 8734;
+    public long fillRefBitmap8735 = 8735;
+    public long fillRefBitmap8736 = 8736;
+    public long fillRefBitmap8737 = 8737;
+    public long fillRefBitmap8738 = 8738;
+    public long fillRefBitmap8739 = 8739;
+    public long fillRefBitmap8740 = 8740;
+    public long fillRefBitmap8741 = 8741;
+    public long fillRefBitmap8742 = 8742;
+    public long fillRefBitmap8743 = 8743;
+    public long fillRefBitmap8744 = 8744;
+    public long fillRefBitmap8745 = 8745;
+    public long fillRefBitmap8746 = 8746;
+    public long fillRefBitmap8747 = 8747;
+    public long fillRefBitmap8748 = 8748;
+    public long fillRefBitmap8749 = 8749;
+    public long fillRefBitmap8750 = 8750;
+    public long fillRefBitmap8751 = 8751;
+    public long fillRefBitmap8752 = 8752;
+    public long fillRefBitmap8753 = 8753;
+    public long fillRefBitmap8754 = 8754;
+    public long fillRefBitmap8755 = 8755;
+    public long fillRefBitmap8756 = 8756;
+    public long fillRefBitmap8757 = 8757;
+    public long fillRefBitmap8758 = 8758;
+    public long fillRefBitmap8759 = 8759;
+    public long fillRefBitmap8760 = 8760;
+    public long fillRefBitmap8761 = 8761;
+    public long fillRefBitmap8762 = 8762;
+    public long fillRefBitmap8763 = 8763;
+    public long fillRefBitmap8764 = 8764;
+    public long fillRefBitmap8765 = 8765;
+    public long fillRefBitmap8766 = 8766;
+    public long fillRefBitmap8767 = 8767;
+    public long fillRefBitmap8768 = 8768;
+    public long fillRefBitmap8769 = 8769;
+    public long fillRefBitmap8770 = 8770;
+    public long fillRefBitmap8771 = 8771;
+    public long fillRefBitmap8772 = 8772;
+    public long fillRefBitmap8773 = 8773;
+    public long fillRefBitmap8774 = 8774;
+    public long fillRefBitmap8775 = 8775;
+    public long fillRefBitmap8776 = 8776;
+    public long fillRefBitmap8777 = 8777;
+    public long fillRefBitmap8778 = 8778;
+    public long fillRefBitmap8779 = 8779;
+    public long fillRefBitmap8780 = 8780;
+    public long fillRefBitmap8781 = 8781;
+    public long fillRefBitmap8782 = 8782;
+    public long fillRefBitmap8783 = 8783;
+    public long fillRefBitmap8784 = 8784;
+    public long fillRefBitmap8785 = 8785;
+    public long fillRefBitmap8786 = 8786;
+    public long fillRefBitmap8787 = 8787;
+    public long fillRefBitmap8788 = 8788;
+    public long fillRefBitmap8789 = 8789;
+    public long fillRefBitmap8790 = 8790;
+    public long fillRefBitmap8791 = 8791;
+    public long fillRefBitmap8792 = 8792;
+    public long fillRefBitmap8793 = 8793;
+    public long fillRefBitmap8794 = 8794;
+    public long fillRefBitmap8795 = 8795;
+    public long fillRefBitmap8796 = 8796;
+    public long fillRefBitmap8797 = 8797;
+    public long fillRefBitmap8798 = 8798;
+    public long fillRefBitmap8799 = 8799;
+    public long fillRefBitmap8800 = 8800;
+    public long fillRefBitmap8801 = 8801;
+    public long fillRefBitmap8802 = 8802;
+    public long fillRefBitmap8803 = 8803;
+    public long fillRefBitmap8804 = 8804;
+    public long fillRefBitmap8805 = 8805;
+    public long fillRefBitmap8806 = 8806;
+    public long fillRefBitmap8807 = 8807;
+    public long fillRefBitmap8808 = 8808;
+    public long fillRefBitmap8809 = 8809;
+    public long fillRefBitmap8810 = 8810;
+    public long fillRefBitmap8811 = 8811;
+    public long fillRefBitmap8812 = 8812;
+    public long fillRefBitmap8813 = 8813;
+    public long fillRefBitmap8814 = 8814;
+    public long fillRefBitmap8815 = 8815;
+    public long fillRefBitmap8816 = 8816;
+    public long fillRefBitmap8817 = 8817;
+    public long fillRefBitmap8818 = 8818;
+    public long fillRefBitmap8819 = 8819;
+    public long fillRefBitmap8820 = 8820;
+    public long fillRefBitmap8821 = 8821;
+    public long fillRefBitmap8822 = 8822;
+    public long fillRefBitmap8823 = 8823;
+    public long fillRefBitmap8824 = 8824;
+    public long fillRefBitmap8825 = 8825;
+    public long fillRefBitmap8826 = 8826;
+    public long fillRefBitmap8827 = 8827;
+    public long fillRefBitmap8828 = 8828;
+    public long fillRefBitmap8829 = 8829;
+    public long fillRefBitmap8830 = 8830;
+    public long fillRefBitmap8831 = 8831;
+    public long fillRefBitmap8832 = 8832;
+    public long fillRefBitmap8833 = 8833;
+    public long fillRefBitmap8834 = 8834;
+    public long fillRefBitmap8835 = 8835;
+    public long fillRefBitmap8836 = 8836;
+    public long fillRefBitmap8837 = 8837;
+    public long fillRefBitmap8838 = 8838;
+    public long fillRefBitmap8839 = 8839;
+    public long fillRefBitmap8840 = 8840;
+    public long fillRefBitmap8841 = 8841;
+    public long fillRefBitmap8842 = 8842;
+    public long fillRefBitmap8843 = 8843;
+    public long fillRefBitmap8844 = 8844;
+    public long fillRefBitmap8845 = 8845;
+    public long fillRefBitmap8846 = 8846;
+    public long fillRefBitmap8847 = 8847;
+    public long fillRefBitmap8848 = 8848;
+    public long fillRefBitmap8849 = 8849;
+    public long fillRefBitmap8850 = 8850;
+    public long fillRefBitmap8851 = 8851;
+    public long fillRefBitmap8852 = 8852;
+    public long fillRefBitmap8853 = 8853;
+    public long fillRefBitmap8854 = 8854;
+    public long fillRefBitmap8855 = 8855;
+    public long fillRefBitmap8856 = 8856;
+    public long fillRefBitmap8857 = 8857;
+    public long fillRefBitmap8858 = 8858;
+    public long fillRefBitmap8859 = 8859;
+    public long fillRefBitmap8860 = 8860;
+    public long fillRefBitmap8861 = 8861;
+    public long fillRefBitmap8862 = 8862;
+    public long fillRefBitmap8863 = 8863;
+    public long fillRefBitmap8864 = 8864;
+    public long fillRefBitmap8865 = 8865;
+    public long fillRefBitmap8866 = 8866;
+    public long fillRefBitmap8867 = 8867;
+    public long fillRefBitmap8868 = 8868;
+    public long fillRefBitmap8869 = 8869;
+    public long fillRefBitmap8870 = 8870;
+    public long fillRefBitmap8871 = 8871;
+    public long fillRefBitmap8872 = 8872;
+    public long fillRefBitmap8873 = 8873;
+    public long fillRefBitmap8874 = 8874;
+    public long fillRefBitmap8875 = 8875;
+    public long fillRefBitmap8876 = 8876;
+    public long fillRefBitmap8877 = 8877;
+    public long fillRefBitmap8878 = 8878;
+    public long fillRefBitmap8879 = 8879;
+    public long fillRefBitmap8880 = 8880;
+    public long fillRefBitmap8881 = 8881;
+    public long fillRefBitmap8882 = 8882;
+    public long fillRefBitmap8883 = 8883;
+    public long fillRefBitmap8884 = 8884;
+    public long fillRefBitmap8885 = 8885;
+    public long fillRefBitmap8886 = 8886;
+    public long fillRefBitmap8887 = 8887;
+    public long fillRefBitmap8888 = 8888;
+    public long fillRefBitmap8889 = 8889;
+    public long fillRefBitmap8890 = 8890;
+    public long fillRefBitmap8891 = 8891;
+    public long fillRefBitmap8892 = 8892;
+    public long fillRefBitmap8893 = 8893;
+    public long fillRefBitmap8894 = 8894;
+    public long fillRefBitmap8895 = 8895;
+    public long fillRefBitmap8896 = 8896;
+    public long fillRefBitmap8897 = 8897;
+    public long fillRefBitmap8898 = 8898;
+    public long fillRefBitmap8899 = 8899;
+    public long fillRefBitmap8900 = 8900;
+    public long fillRefBitmap8901 = 8901;
+    public long fillRefBitmap8902 = 8902;
+    public long fillRefBitmap8903 = 8903;
+    public long fillRefBitmap8904 = 8904;
+    public long fillRefBitmap8905 = 8905;
+    public long fillRefBitmap8906 = 8906;
+    public long fillRefBitmap8907 = 8907;
+    public long fillRefBitmap8908 = 8908;
+    public long fillRefBitmap8909 = 8909;
+    public long fillRefBitmap8910 = 8910;
+    public long fillRefBitmap8911 = 8911;
+    public long fillRefBitmap8912 = 8912;
+    public long fillRefBitmap8913 = 8913;
+    public long fillRefBitmap8914 = 8914;
+    public long fillRefBitmap8915 = 8915;
+    public long fillRefBitmap8916 = 8916;
+    public long fillRefBitmap8917 = 8917;
+    public long fillRefBitmap8918 = 8918;
+    public long fillRefBitmap8919 = 8919;
+    public long fillRefBitmap8920 = 8920;
+    public long fillRefBitmap8921 = 8921;
+    public long fillRefBitmap8922 = 8922;
+    public long fillRefBitmap8923 = 8923;
+    public long fillRefBitmap8924 = 8924;
+    public long fillRefBitmap8925 = 8925;
+    public long fillRefBitmap8926 = 8926;
+    public long fillRefBitmap8927 = 8927;
+    public long fillRefBitmap8928 = 8928;
+    public long fillRefBitmap8929 = 8929;
+    public long fillRefBitmap8930 = 8930;
+    public long fillRefBitmap8931 = 8931;
+    public long fillRefBitmap8932 = 8932;
+    public long fillRefBitmap8933 = 8933;
+    public long fillRefBitmap8934 = 8934;
+    public long fillRefBitmap8935 = 8935;
+    public long fillRefBitmap8936 = 8936;
+    public long fillRefBitmap8937 = 8937;
+    public long fillRefBitmap8938 = 8938;
+    public long fillRefBitmap8939 = 8939;
+    public long fillRefBitmap8940 = 8940;
+    public long fillRefBitmap8941 = 8941;
+    public long fillRefBitmap8942 = 8942;
+    public long fillRefBitmap8943 = 8943;
+    public long fillRefBitmap8944 = 8944;
+    public long fillRefBitmap8945 = 8945;
+    public long fillRefBitmap8946 = 8946;
+    public long fillRefBitmap8947 = 8947;
+    public long fillRefBitmap8948 = 8948;
+    public long fillRefBitmap8949 = 8949;
+    public long fillRefBitmap8950 = 8950;
+    public long fillRefBitmap8951 = 8951;
+    public long fillRefBitmap8952 = 8952;
+    public long fillRefBitmap8953 = 8953;
+    public long fillRefBitmap8954 = 8954;
+    public long fillRefBitmap8955 = 8955;
+    public long fillRefBitmap8956 = 8956;
+    public long fillRefBitmap8957 = 8957;
+    public long fillRefBitmap8958 = 8958;
+    public long fillRefBitmap8959 = 8959;
+    public long fillRefBitmap8960 = 8960;
+    public long fillRefBitmap8961 = 8961;
+    public long fillRefBitmap8962 = 8962;
+    public long fillRefBitmap8963 = 8963;
+    public long fillRefBitmap8964 = 8964;
+    public long fillRefBitmap8965 = 8965;
+    public long fillRefBitmap8966 = 8966;
+    public long fillRefBitmap8967 = 8967;
+    public long fillRefBitmap8968 = 8968;
+    public long fillRefBitmap8969 = 8969;
+    public long fillRefBitmap8970 = 8970;
+    public long fillRefBitmap8971 = 8971;
+    public long fillRefBitmap8972 = 8972;
+    public long fillRefBitmap8973 = 8973;
+    public long fillRefBitmap8974 = 8974;
+    public long fillRefBitmap8975 = 8975;
+    public long fillRefBitmap8976 = 8976;
+    public long fillRefBitmap8977 = 8977;
+    public long fillRefBitmap8978 = 8978;
+    public long fillRefBitmap8979 = 8979;
+    public long fillRefBitmap8980 = 8980;
+    public long fillRefBitmap8981 = 8981;
+    public long fillRefBitmap8982 = 8982;
+    public long fillRefBitmap8983 = 8983;
+    public long fillRefBitmap8984 = 8984;
+    public long fillRefBitmap8985 = 8985;
+    public long fillRefBitmap8986 = 8986;
+    public long fillRefBitmap8987 = 8987;
+    public long fillRefBitmap8988 = 8988;
+    public long fillRefBitmap8989 = 8989;
+    public long fillRefBitmap8990 = 8990;
+    public long fillRefBitmap8991 = 8991;
+    public long fillRefBitmap8992 = 8992;
+    public long fillRefBitmap8993 = 8993;
+    public long fillRefBitmap8994 = 8994;
+    public long fillRefBitmap8995 = 8995;
+    public long fillRefBitmap8996 = 8996;
+    public long fillRefBitmap8997 = 8997;
+    public long fillRefBitmap8998 = 8998;
+    public long fillRefBitmap8999 = 8999;
+    public long fillRefBitmap9000 = 9000;
+    public long fillRefBitmap9001 = 9001;
+    public long fillRefBitmap9002 = 9002;
+    public long fillRefBitmap9003 = 9003;
+    public long fillRefBitmap9004 = 9004;
+    public long fillRefBitmap9005 = 9005;
+    public long fillRefBitmap9006 = 9006;
+    public long fillRefBitmap9007 = 9007;
+    public long fillRefBitmap9008 = 9008;
+    public long fillRefBitmap9009 = 9009;
+    public long fillRefBitmap9010 = 9010;
+    public long fillRefBitmap9011 = 9011;
+    public long fillRefBitmap9012 = 9012;
+    public long fillRefBitmap9013 = 9013;
+    public long fillRefBitmap9014 = 9014;
+    public long fillRefBitmap9015 = 9015;
+    public long fillRefBitmap9016 = 9016;
+    public long fillRefBitmap9017 = 9017;
+    public long fillRefBitmap9018 = 9018;
+    public long fillRefBitmap9019 = 9019;
+    public long fillRefBitmap9020 = 9020;
+    public long fillRefBitmap9021 = 9021;
+    public long fillRefBitmap9022 = 9022;
+    public long fillRefBitmap9023 = 9023;
+    public long fillRefBitmap9024 = 9024;
+    public long fillRefBitmap9025 = 9025;
+    public long fillRefBitmap9026 = 9026;
+    public long fillRefBitmap9027 = 9027;
+    public long fillRefBitmap9028 = 9028;
+    public long fillRefBitmap9029 = 9029;
+    public long fillRefBitmap9030 = 9030;
+    public long fillRefBitmap9031 = 9031;
+    public long fillRefBitmap9032 = 9032;
+    public long fillRefBitmap9033 = 9033;
+    public long fillRefBitmap9034 = 9034;
+    public long fillRefBitmap9035 = 9035;
+    public long fillRefBitmap9036 = 9036;
+    public long fillRefBitmap9037 = 9037;
+    public long fillRefBitmap9038 = 9038;
+    public long fillRefBitmap9039 = 9039;
+    public long fillRefBitmap9040 = 9040;
+    public long fillRefBitmap9041 = 9041;
+    public long fillRefBitmap9042 = 9042;
+    public long fillRefBitmap9043 = 9043;
+    public long fillRefBitmap9044 = 9044;
+    public long fillRefBitmap9045 = 9045;
+    public long fillRefBitmap9046 = 9046;
+    public long fillRefBitmap9047 = 9047;
+    public long fillRefBitmap9048 = 9048;
+    public long fillRefBitmap9049 = 9049;
+    public long fillRefBitmap9050 = 9050;
+    public long fillRefBitmap9051 = 9051;
+    public long fillRefBitmap9052 = 9052;
+    public long fillRefBitmap9053 = 9053;
+    public long fillRefBitmap9054 = 9054;
+    public long fillRefBitmap9055 = 9055;
+    public long fillRefBitmap9056 = 9056;
+    public long fillRefBitmap9057 = 9057;
+    public long fillRefBitmap9058 = 9058;
+    public long fillRefBitmap9059 = 9059;
+    public long fillRefBitmap9060 = 9060;
+    public long fillRefBitmap9061 = 9061;
+    public long fillRefBitmap9062 = 9062;
+    public long fillRefBitmap9063 = 9063;
+    public long fillRefBitmap9064 = 9064;
+    public long fillRefBitmap9065 = 9065;
+    public long fillRefBitmap9066 = 9066;
+    public long fillRefBitmap9067 = 9067;
+    public long fillRefBitmap9068 = 9068;
+    public long fillRefBitmap9069 = 9069;
+    public long fillRefBitmap9070 = 9070;
+    public long fillRefBitmap9071 = 9071;
+    public long fillRefBitmap9072 = 9072;
+    public long fillRefBitmap9073 = 9073;
+    public long fillRefBitmap9074 = 9074;
+    public long fillRefBitmap9075 = 9075;
+    public long fillRefBitmap9076 = 9076;
+    public long fillRefBitmap9077 = 9077;
+    public long fillRefBitmap9078 = 9078;
+    public long fillRefBitmap9079 = 9079;
+    public long fillRefBitmap9080 = 9080;
+    public long fillRefBitmap9081 = 9081;
+    public long fillRefBitmap9082 = 9082;
+    public long fillRefBitmap9083 = 9083;
+    public long fillRefBitmap9084 = 9084;
+    public long fillRefBitmap9085 = 9085;
+    public long fillRefBitmap9086 = 9086;
+    public long fillRefBitmap9087 = 9087;
+    public long fillRefBitmap9088 = 9088;
+    public long fillRefBitmap9089 = 9089;
+    public long fillRefBitmap9090 = 9090;
+    public long fillRefBitmap9091 = 9091;
+    public long fillRefBitmap9092 = 9092;
+    public long fillRefBitmap9093 = 9093;
+    public long fillRefBitmap9094 = 9094;
+    public long fillRefBitmap9095 = 9095;
+    public long fillRefBitmap9096 = 9096;
+    public long fillRefBitmap9097 = 9097;
+    public long fillRefBitmap9098 = 9098;
+    public long fillRefBitmap9099 = 9099;
+    public long fillRefBitmap9100 = 9100;
+    public long fillRefBitmap9101 = 9101;
+    public long fillRefBitmap9102 = 9102;
+    public long fillRefBitmap9103 = 9103;
+    public long fillRefBitmap9104 = 9104;
+    public long fillRefBitmap9105 = 9105;
+    public long fillRefBitmap9106 = 9106;
+    public long fillRefBitmap9107 = 9107;
+    public long fillRefBitmap9108 = 9108;
+    public long fillRefBitmap9109 = 9109;
+    public long fillRefBitmap9110 = 9110;
+    public long fillRefBitmap9111 = 9111;
+    public long fillRefBitmap9112 = 9112;
+    public long fillRefBitmap9113 = 9113;
+    public long fillRefBitmap9114 = 9114;
+    public long fillRefBitmap9115 = 9115;
+    public long fillRefBitmap9116 = 9116;
+    public long fillRefBitmap9117 = 9117;
+    public long fillRefBitmap9118 = 9118;
+    public long fillRefBitmap9119 = 9119;
+    public long fillRefBitmap9120 = 9120;
+    public long fillRefBitmap9121 = 9121;
+    public long fillRefBitmap9122 = 9122;
+    public long fillRefBitmap9123 = 9123;
+    public long fillRefBitmap9124 = 9124;
+    public long fillRefBitmap9125 = 9125;
+    public long fillRefBitmap9126 = 9126;
+    public long fillRefBitmap9127 = 9127;
+    public long fillRefBitmap9128 = 9128;
+    public long fillRefBitmap9129 = 9129;
+    public long fillRefBitmap9130 = 9130;
+    public long fillRefBitmap9131 = 9131;
+    public long fillRefBitmap9132 = 9132;
+    public long fillRefBitmap9133 = 9133;
+    public long fillRefBitmap9134 = 9134;
+    public long fillRefBitmap9135 = 9135;
+    public long fillRefBitmap9136 = 9136;
+    public long fillRefBitmap9137 = 9137;
+    public long fillRefBitmap9138 = 9138;
+    public long fillRefBitmap9139 = 9139;
+    public long fillRefBitmap9140 = 9140;
+    public long fillRefBitmap9141 = 9141;
+    public long fillRefBitmap9142 = 9142;
+    public long fillRefBitmap9143 = 9143;
+    public long fillRefBitmap9144 = 9144;
+    public long fillRefBitmap9145 = 9145;
+    public long fillRefBitmap9146 = 9146;
+    public long fillRefBitmap9147 = 9147;
+    public long fillRefBitmap9148 = 9148;
+    public long fillRefBitmap9149 = 9149;
+    public long fillRefBitmap9150 = 9150;
+    public long fillRefBitmap9151 = 9151;
+    public long fillRefBitmap9152 = 9152;
+    public long fillRefBitmap9153 = 9153;
+    public long fillRefBitmap9154 = 9154;
+    public long fillRefBitmap9155 = 9155;
+    public long fillRefBitmap9156 = 9156;
+    public long fillRefBitmap9157 = 9157;
+    public long fillRefBitmap9158 = 9158;
+    public long fillRefBitmap9159 = 9159;
+    public long fillRefBitmap9160 = 9160;
+    public long fillRefBitmap9161 = 9161;
+    public long fillRefBitmap9162 = 9162;
+    public long fillRefBitmap9163 = 9163;
+    public long fillRefBitmap9164 = 9164;
+    public long fillRefBitmap9165 = 9165;
+    public long fillRefBitmap9166 = 9166;
+    public long fillRefBitmap9167 = 9167;
+    public long fillRefBitmap9168 = 9168;
+    public long fillRefBitmap9169 = 9169;
+    public long fillRefBitmap9170 = 9170;
+    public long fillRefBitmap9171 = 9171;
+    public long fillRefBitmap9172 = 9172;
+    public long fillRefBitmap9173 = 9173;
+    public long fillRefBitmap9174 = 9174;
+    public long fillRefBitmap9175 = 9175;
+    public long fillRefBitmap9176 = 9176;
+    public long fillRefBitmap9177 = 9177;
+    public long fillRefBitmap9178 = 9178;
+    public long fillRefBitmap9179 = 9179;
+    public long fillRefBitmap9180 = 9180;
+    public long fillRefBitmap9181 = 9181;
+    public long fillRefBitmap9182 = 9182;
+    public long fillRefBitmap9183 = 9183;
+    public long fillRefBitmap9184 = 9184;
+    public long fillRefBitmap9185 = 9185;
+    public long fillRefBitmap9186 = 9186;
+    public long fillRefBitmap9187 = 9187;
+    public long fillRefBitmap9188 = 9188;
+    public long fillRefBitmap9189 = 9189;
+    public long fillRefBitmap9190 = 9190;
+    public long fillRefBitmap9191 = 9191;
+    public long fillRefBitmap9192 = 9192;
+    public long fillRefBitmap9193 = 9193;
+    public long fillRefBitmap9194 = 9194;
+    public long fillRefBitmap9195 = 9195;
+    public long fillRefBitmap9196 = 9196;
+    public long fillRefBitmap9197 = 9197;
+    public long fillRefBitmap9198 = 9198;
+    public long fillRefBitmap9199 = 9199;
+    public long fillRefBitmap9200 = 9200;
+    public long fillRefBitmap9201 = 9201;
+    public long fillRefBitmap9202 = 9202;
+    public long fillRefBitmap9203 = 9203;
+    public long fillRefBitmap9204 = 9204;
+    public long fillRefBitmap9205 = 9205;
+    public long fillRefBitmap9206 = 9206;
+    public long fillRefBitmap9207 = 9207;
+    public long fillRefBitmap9208 = 9208;
+    public long fillRefBitmap9209 = 9209;
+    public long fillRefBitmap9210 = 9210;
+    public long fillRefBitmap9211 = 9211;
+    public long fillRefBitmap9212 = 9212;
+    public long fillRefBitmap9213 = 9213;
+    public long fillRefBitmap9214 = 9214;
+    public long fillRefBitmap9215 = 9215;
+    public long fillRefBitmap9216 = 9216;
+    public long fillRefBitmap9217 = 9217;
+    public long fillRefBitmap9218 = 9218;
+    public long fillRefBitmap9219 = 9219;
+    public long fillRefBitmap9220 = 9220;
+    public long fillRefBitmap9221 = 9221;
+    public long fillRefBitmap9222 = 9222;
+    public long fillRefBitmap9223 = 9223;
+    public long fillRefBitmap9224 = 9224;
+    public long fillRefBitmap9225 = 9225;
+    public long fillRefBitmap9226 = 9226;
+    public long fillRefBitmap9227 = 9227;
+    public long fillRefBitmap9228 = 9228;
+    public long fillRefBitmap9229 = 9229;
+    public long fillRefBitmap9230 = 9230;
+    public long fillRefBitmap9231 = 9231;
+    public long fillRefBitmap9232 = 9232;
+    public long fillRefBitmap9233 = 9233;
+    public long fillRefBitmap9234 = 9234;
+    public long fillRefBitmap9235 = 9235;
+    public long fillRefBitmap9236 = 9236;
+    public long fillRefBitmap9237 = 9237;
+    public long fillRefBitmap9238 = 9238;
+    public long fillRefBitmap9239 = 9239;
+    public long fillRefBitmap9240 = 9240;
+    public long fillRefBitmap9241 = 9241;
+    public long fillRefBitmap9242 = 9242;
+    public long fillRefBitmap9243 = 9243;
+    public long fillRefBitmap9244 = 9244;
+    public long fillRefBitmap9245 = 9245;
+    public long fillRefBitmap9246 = 9246;
+    public long fillRefBitmap9247 = 9247;
+    public long fillRefBitmap9248 = 9248;
+    public long fillRefBitmap9249 = 9249;
+    public long fillRefBitmap9250 = 9250;
+    public long fillRefBitmap9251 = 9251;
+    public long fillRefBitmap9252 = 9252;
+    public long fillRefBitmap9253 = 9253;
+    public long fillRefBitmap9254 = 9254;
+    public long fillRefBitmap9255 = 9255;
+    public long fillRefBitmap9256 = 9256;
+    public long fillRefBitmap9257 = 9257;
+    public long fillRefBitmap9258 = 9258;
+    public long fillRefBitmap9259 = 9259;
+    public long fillRefBitmap9260 = 9260;
+    public long fillRefBitmap9261 = 9261;
+    public long fillRefBitmap9262 = 9262;
+    public long fillRefBitmap9263 = 9263;
+    public long fillRefBitmap9264 = 9264;
+    public long fillRefBitmap9265 = 9265;
+    public long fillRefBitmap9266 = 9266;
+    public long fillRefBitmap9267 = 9267;
+    public long fillRefBitmap9268 = 9268;
+    public long fillRefBitmap9269 = 9269;
+    public long fillRefBitmap9270 = 9270;
+    public long fillRefBitmap9271 = 9271;
+    public long fillRefBitmap9272 = 9272;
+    public long fillRefBitmap9273 = 9273;
+    public long fillRefBitmap9274 = 9274;
+    public long fillRefBitmap9275 = 9275;
+    public long fillRefBitmap9276 = 9276;
+    public long fillRefBitmap9277 = 9277;
+    public long fillRefBitmap9278 = 9278;
+    public long fillRefBitmap9279 = 9279;
+    public long fillRefBitmap9280 = 9280;
+    public long fillRefBitmap9281 = 9281;
+    public long fillRefBitmap9282 = 9282;
+    public long fillRefBitmap9283 = 9283;
+    public long fillRefBitmap9284 = 9284;
+    public long fillRefBitmap9285 = 9285;
+    public long fillRefBitmap9286 = 9286;
+    public long fillRefBitmap9287 = 9287;
+    public long fillRefBitmap9288 = 9288;
+    public long fillRefBitmap9289 = 9289;
+    public long fillRefBitmap9290 = 9290;
+    public long fillRefBitmap9291 = 9291;
+    public long fillRefBitmap9292 = 9292;
+    public long fillRefBitmap9293 = 9293;
+    public long fillRefBitmap9294 = 9294;
+    public long fillRefBitmap9295 = 9295;
+    public long fillRefBitmap9296 = 9296;
+    public long fillRefBitmap9297 = 9297;
+    public long fillRefBitmap9298 = 9298;
+    public long fillRefBitmap9299 = 9299;
+    public long fillRefBitmap9300 = 9300;
+    public long fillRefBitmap9301 = 9301;
+    public long fillRefBitmap9302 = 9302;
+    public long fillRefBitmap9303 = 9303;
+    public long fillRefBitmap9304 = 9304;
+    public long fillRefBitmap9305 = 9305;
+    public long fillRefBitmap9306 = 9306;
+    public long fillRefBitmap9307 = 9307;
+    public long fillRefBitmap9308 = 9308;
+    public long fillRefBitmap9309 = 9309;
+    public long fillRefBitmap9310 = 9310;
+    public long fillRefBitmap9311 = 9311;
+    public long fillRefBitmap9312 = 9312;
+    public long fillRefBitmap9313 = 9313;
+    public long fillRefBitmap9314 = 9314;
+    public long fillRefBitmap9315 = 9315;
+    public long fillRefBitmap9316 = 9316;
+    public long fillRefBitmap9317 = 9317;
+    public long fillRefBitmap9318 = 9318;
+    public long fillRefBitmap9319 = 9319;
+    public long fillRefBitmap9320 = 9320;
+    public long fillRefBitmap9321 = 9321;
+    public long fillRefBitmap9322 = 9322;
+    public long fillRefBitmap9323 = 9323;
+    public long fillRefBitmap9324 = 9324;
+    public long fillRefBitmap9325 = 9325;
+    public long fillRefBitmap9326 = 9326;
+    public long fillRefBitmap9327 = 9327;
+    public long fillRefBitmap9328 = 9328;
+    public long fillRefBitmap9329 = 9329;
+    public long fillRefBitmap9330 = 9330;
+    public long fillRefBitmap9331 = 9331;
+    public long fillRefBitmap9332 = 9332;
+    public long fillRefBitmap9333 = 9333;
+    public long fillRefBitmap9334 = 9334;
+    public long fillRefBitmap9335 = 9335;
+    public long fillRefBitmap9336 = 9336;
+    public long fillRefBitmap9337 = 9337;
+    public long fillRefBitmap9338 = 9338;
+    public long fillRefBitmap9339 = 9339;
+    public long fillRefBitmap9340 = 9340;
+    public long fillRefBitmap9341 = 9341;
+    public long fillRefBitmap9342 = 9342;
+    public long fillRefBitmap9343 = 9343;
+    public long fillRefBitmap9344 = 9344;
+    public long fillRefBitmap9345 = 9345;
+    public long fillRefBitmap9346 = 9346;
+    public long fillRefBitmap9347 = 9347;
+    public long fillRefBitmap9348 = 9348;
+    public long fillRefBitmap9349 = 9349;
+    public long fillRefBitmap9350 = 9350;
+    public long fillRefBitmap9351 = 9351;
+    public long fillRefBitmap9352 = 9352;
+    public long fillRefBitmap9353 = 9353;
+    public long fillRefBitmap9354 = 9354;
+    public long fillRefBitmap9355 = 9355;
+    public long fillRefBitmap9356 = 9356;
+    public long fillRefBitmap9357 = 9357;
+    public long fillRefBitmap9358 = 9358;
+    public long fillRefBitmap9359 = 9359;
+    public long fillRefBitmap9360 = 9360;
+    public long fillRefBitmap9361 = 9361;
+    public long fillRefBitmap9362 = 9362;
+    public long fillRefBitmap9363 = 9363;
+    public long fillRefBitmap9364 = 9364;
+    public long fillRefBitmap9365 = 9365;
+    public long fillRefBitmap9366 = 9366;
+    public long fillRefBitmap9367 = 9367;
+    public long fillRefBitmap9368 = 9368;
+    public long fillRefBitmap9369 = 9369;
+    public long fillRefBitmap9370 = 9370;
+    public long fillRefBitmap9371 = 9371;
+    public long fillRefBitmap9372 = 9372;
+    public long fillRefBitmap9373 = 9373;
+    public long fillRefBitmap9374 = 9374;
+    public long fillRefBitmap9375 = 9375;
+    public long fillRefBitmap9376 = 9376;
+    public long fillRefBitmap9377 = 9377;
+    public long fillRefBitmap9378 = 9378;
+    public long fillRefBitmap9379 = 9379;
+    public long fillRefBitmap9380 = 9380;
+    public long fillRefBitmap9381 = 9381;
+    public long fillRefBitmap9382 = 9382;
+    public long fillRefBitmap9383 = 9383;
+    public long fillRefBitmap9384 = 9384;
+    public long fillRefBitmap9385 = 9385;
+    public long fillRefBitmap9386 = 9386;
+    public long fillRefBitmap9387 = 9387;
+    public long fillRefBitmap9388 = 9388;
+    public long fillRefBitmap9389 = 9389;
+    public long fillRefBitmap9390 = 9390;
+    public long fillRefBitmap9391 = 9391;
+    public long fillRefBitmap9392 = 9392;
+    public long fillRefBitmap9393 = 9393;
+    public long fillRefBitmap9394 = 9394;
+    public long fillRefBitmap9395 = 9395;
+    public long fillRefBitmap9396 = 9396;
+    public long fillRefBitmap9397 = 9397;
+    public long fillRefBitmap9398 = 9398;
+    public long fillRefBitmap9399 = 9399;
+    public long fillRefBitmap9400 = 9400;
+    public long fillRefBitmap9401 = 9401;
+    public long fillRefBitmap9402 = 9402;
+    public long fillRefBitmap9403 = 9403;
+    public long fillRefBitmap9404 = 9404;
+    public long fillRefBitmap9405 = 9405;
+    public long fillRefBitmap9406 = 9406;
+    public long fillRefBitmap9407 = 9407;
+    public long fillRefBitmap9408 = 9408;
+    public long fillRefBitmap9409 = 9409;
+    public long fillRefBitmap9410 = 9410;
+    public long fillRefBitmap9411 = 9411;
+    public long fillRefBitmap9412 = 9412;
+    public long fillRefBitmap9413 = 9413;
+    public long fillRefBitmap9414 = 9414;
+    public long fillRefBitmap9415 = 9415;
+    public long fillRefBitmap9416 = 9416;
+    public long fillRefBitmap9417 = 9417;
+    public long fillRefBitmap9418 = 9418;
+    public long fillRefBitmap9419 = 9419;
+    public long fillRefBitmap9420 = 9420;
+    public long fillRefBitmap9421 = 9421;
+    public long fillRefBitmap9422 = 9422;
+    public long fillRefBitmap9423 = 9423;
+    public long fillRefBitmap9424 = 9424;
+    public long fillRefBitmap9425 = 9425;
+    public long fillRefBitmap9426 = 9426;
+    public long fillRefBitmap9427 = 9427;
+    public long fillRefBitmap9428 = 9428;
+    public long fillRefBitmap9429 = 9429;
+    public long fillRefBitmap9430 = 9430;
+    public long fillRefBitmap9431 = 9431;
+    public long fillRefBitmap9432 = 9432;
+    public long fillRefBitmap9433 = 9433;
+    public long fillRefBitmap9434 = 9434;
+    public long fillRefBitmap9435 = 9435;
+    public long fillRefBitmap9436 = 9436;
+    public long fillRefBitmap9437 = 9437;
+    public long fillRefBitmap9438 = 9438;
+    public long fillRefBitmap9439 = 9439;
+    public long fillRefBitmap9440 = 9440;
+    public long fillRefBitmap9441 = 9441;
+    public long fillRefBitmap9442 = 9442;
+    public long fillRefBitmap9443 = 9443;
+    public long fillRefBitmap9444 = 9444;
+    public long fillRefBitmap9445 = 9445;
+    public long fillRefBitmap9446 = 9446;
+    public long fillRefBitmap9447 = 9447;
+    public long fillRefBitmap9448 = 9448;
+    public long fillRefBitmap9449 = 9449;
+    public long fillRefBitmap9450 = 9450;
+    public long fillRefBitmap9451 = 9451;
+    public long fillRefBitmap9452 = 9452;
+    public long fillRefBitmap9453 = 9453;
+    public long fillRefBitmap9454 = 9454;
+    public long fillRefBitmap9455 = 9455;
+    public long fillRefBitmap9456 = 9456;
+    public long fillRefBitmap9457 = 9457;
+    public long fillRefBitmap9458 = 9458;
+    public long fillRefBitmap9459 = 9459;
+    public long fillRefBitmap9460 = 9460;
+    public long fillRefBitmap9461 = 9461;
+    public long fillRefBitmap9462 = 9462;
+    public long fillRefBitmap9463 = 9463;
+    public long fillRefBitmap9464 = 9464;
+    public long fillRefBitmap9465 = 9465;
+    public long fillRefBitmap9466 = 9466;
+    public long fillRefBitmap9467 = 9467;
+    public long fillRefBitmap9468 = 9468;
+    public long fillRefBitmap9469 = 9469;
+    public long fillRefBitmap9470 = 9470;
+    public long fillRefBitmap9471 = 9471;
+    public long fillRefBitmap9472 = 9472;
+    public long fillRefBitmap9473 = 9473;
+    public long fillRefBitmap9474 = 9474;
+    public long fillRefBitmap9475 = 9475;
+    public long fillRefBitmap9476 = 9476;
+    public long fillRefBitmap9477 = 9477;
+    public long fillRefBitmap9478 = 9478;
+    public long fillRefBitmap9479 = 9479;
+    public long fillRefBitmap9480 = 9480;
+    public long fillRefBitmap9481 = 9481;
+    public long fillRefBitmap9482 = 9482;
+    public long fillRefBitmap9483 = 9483;
+    public long fillRefBitmap9484 = 9484;
+    public long fillRefBitmap9485 = 9485;
+    public long fillRefBitmap9486 = 9486;
+    public long fillRefBitmap9487 = 9487;
+    public long fillRefBitmap9488 = 9488;
+    public long fillRefBitmap9489 = 9489;
+    public long fillRefBitmap9490 = 9490;
+    public long fillRefBitmap9491 = 9491;
+    public long fillRefBitmap9492 = 9492;
+    public long fillRefBitmap9493 = 9493;
+    public long fillRefBitmap9494 = 9494;
+    public long fillRefBitmap9495 = 9495;
+    public long fillRefBitmap9496 = 9496;
+    public long fillRefBitmap9497 = 9497;
+    public long fillRefBitmap9498 = 9498;
+    public long fillRefBitmap9499 = 9499;
+    public long fillRefBitmap9500 = 9500;
+    public long fillRefBitmap9501 = 9501;
+    public long fillRefBitmap9502 = 9502;
+    public long fillRefBitmap9503 = 9503;
+    public long fillRefBitmap9504 = 9504;
+    public long fillRefBitmap9505 = 9505;
+    public long fillRefBitmap9506 = 9506;
+    public long fillRefBitmap9507 = 9507;
+    public long fillRefBitmap9508 = 9508;
+    public long fillRefBitmap9509 = 9509;
+    public long fillRefBitmap9510 = 9510;
+    public long fillRefBitmap9511 = 9511;
+    public long fillRefBitmap9512 = 9512;
+    public long fillRefBitmap9513 = 9513;
+    public long fillRefBitmap9514 = 9514;
+    public long fillRefBitmap9515 = 9515;
+    public long fillRefBitmap9516 = 9516;
+    public long fillRefBitmap9517 = 9517;
+    public long fillRefBitmap9518 = 9518;
+    public long fillRefBitmap9519 = 9519;
+    public long fillRefBitmap9520 = 9520;
+    public long fillRefBitmap9521 = 9521;
+    public long fillRefBitmap9522 = 9522;
+    public long fillRefBitmap9523 = 9523;
+    public long fillRefBitmap9524 = 9524;
+    public long fillRefBitmap9525 = 9525;
+    public long fillRefBitmap9526 = 9526;
+    public long fillRefBitmap9527 = 9527;
+    public long fillRefBitmap9528 = 9528;
+    public long fillRefBitmap9529 = 9529;
+    public long fillRefBitmap9530 = 9530;
+    public long fillRefBitmap9531 = 9531;
+    public long fillRefBitmap9532 = 9532;
+    public long fillRefBitmap9533 = 9533;
+    public long fillRefBitmap9534 = 9534;
+    public long fillRefBitmap9535 = 9535;
+    public long fillRefBitmap9536 = 9536;
+    public long fillRefBitmap9537 = 9537;
+    public long fillRefBitmap9538 = 9538;
+    public long fillRefBitmap9539 = 9539;
+    public long fillRefBitmap9540 = 9540;
+    public long fillRefBitmap9541 = 9541;
+    public long fillRefBitmap9542 = 9542;
+    public long fillRefBitmap9543 = 9543;
+    public long fillRefBitmap9544 = 9544;
+    public long fillRefBitmap9545 = 9545;
+    public long fillRefBitmap9546 = 9546;
+    public long fillRefBitmap9547 = 9547;
+    public long fillRefBitmap9548 = 9548;
+    public long fillRefBitmap9549 = 9549;
+    public long fillRefBitmap9550 = 9550;
+    public long fillRefBitmap9551 = 9551;
+    public long fillRefBitmap9552 = 9552;
+    public long fillRefBitmap9553 = 9553;
+    public long fillRefBitmap9554 = 9554;
+    public long fillRefBitmap9555 = 9555;
+    public long fillRefBitmap9556 = 9556;
+    public long fillRefBitmap9557 = 9557;
+    public long fillRefBitmap9558 = 9558;
+    public long fillRefBitmap9559 = 9559;
+    public long fillRefBitmap9560 = 9560;
+    public long fillRefBitmap9561 = 9561;
+    public long fillRefBitmap9562 = 9562;
+    public long fillRefBitmap9563 = 9563;
+    public long fillRefBitmap9564 = 9564;
+    public long fillRefBitmap9565 = 9565;
+    public long fillRefBitmap9566 = 9566;
+    public long fillRefBitmap9567 = 9567;
+    public long fillRefBitmap9568 = 9568;
+    public long fillRefBitmap9569 = 9569;
+    public long fillRefBitmap9570 = 9570;
+    public long fillRefBitmap9571 = 9571;
+    public long fillRefBitmap9572 = 9572;
+    public long fillRefBitmap9573 = 9573;
+    public long fillRefBitmap9574 = 9574;
+    public long fillRefBitmap9575 = 9575;
+    public long fillRefBitmap9576 = 9576;
+    public long fillRefBitmap9577 = 9577;
+    public long fillRefBitmap9578 = 9578;
+    public long fillRefBitmap9579 = 9579;
+    public long fillRefBitmap9580 = 9580;
+    public long fillRefBitmap9581 = 9581;
+    public long fillRefBitmap9582 = 9582;
+    public long fillRefBitmap9583 = 9583;
+    public long fillRefBitmap9584 = 9584;
+    public long fillRefBitmap9585 = 9585;
+    public long fillRefBitmap9586 = 9586;
+    public long fillRefBitmap9587 = 9587;
+    public long fillRefBitmap9588 = 9588;
+    public long fillRefBitmap9589 = 9589;
+    public long fillRefBitmap9590 = 9590;
+    public long fillRefBitmap9591 = 9591;
+    public long fillRefBitmap9592 = 9592;
+    public long fillRefBitmap9593 = 9593;
+    public long fillRefBitmap9594 = 9594;
+    public long fillRefBitmap9595 = 9595;
+    public long fillRefBitmap9596 = 9596;
+    public long fillRefBitmap9597 = 9597;
+    public long fillRefBitmap9598 = 9598;
+    public long fillRefBitmap9599 = 9599;
+    public long fillRefBitmap9600 = 9600;
+    public long fillRefBitmap9601 = 9601;
+    public long fillRefBitmap9602 = 9602;
+    public long fillRefBitmap9603 = 9603;
+    public long fillRefBitmap9604 = 9604;
+    public long fillRefBitmap9605 = 9605;
+    public long fillRefBitmap9606 = 9606;
+    public long fillRefBitmap9607 = 9607;
+    public long fillRefBitmap9608 = 9608;
+    public long fillRefBitmap9609 = 9609;
+    public long fillRefBitmap9610 = 9610;
+    public long fillRefBitmap9611 = 9611;
+    public long fillRefBitmap9612 = 9612;
+    public long fillRefBitmap9613 = 9613;
+    public long fillRefBitmap9614 = 9614;
+    public long fillRefBitmap9615 = 9615;
+    public long fillRefBitmap9616 = 9616;
+    public long fillRefBitmap9617 = 9617;
+    public long fillRefBitmap9618 = 9618;
+    public long fillRefBitmap9619 = 9619;
+    public long fillRefBitmap9620 = 9620;
+    public long fillRefBitmap9621 = 9621;
+    public long fillRefBitmap9622 = 9622;
+    public long fillRefBitmap9623 = 9623;
+    public long fillRefBitmap9624 = 9624;
+    public long fillRefBitmap9625 = 9625;
+    public long fillRefBitmap9626 = 9626;
+    public long fillRefBitmap9627 = 9627;
+    public long fillRefBitmap9628 = 9628;
+    public long fillRefBitmap9629 = 9629;
+    public long fillRefBitmap9630 = 9630;
+    public long fillRefBitmap9631 = 9631;
+    public long fillRefBitmap9632 = 9632;
+    public long fillRefBitmap9633 = 9633;
+    public long fillRefBitmap9634 = 9634;
+    public long fillRefBitmap9635 = 9635;
+    public long fillRefBitmap9636 = 9636;
+    public long fillRefBitmap9637 = 9637;
+    public long fillRefBitmap9638 = 9638;
+    public long fillRefBitmap9639 = 9639;
+    public long fillRefBitmap9640 = 9640;
+    public long fillRefBitmap9641 = 9641;
+    public long fillRefBitmap9642 = 9642;
+    public long fillRefBitmap9643 = 9643;
+    public long fillRefBitmap9644 = 9644;
+    public long fillRefBitmap9645 = 9645;
+    public long fillRefBitmap9646 = 9646;
+    public long fillRefBitmap9647 = 9647;
+    public long fillRefBitmap9648 = 9648;
+    public long fillRefBitmap9649 = 9649;
+    public long fillRefBitmap9650 = 9650;
+    public long fillRefBitmap9651 = 9651;
+    public long fillRefBitmap9652 = 9652;
+    public long fillRefBitmap9653 = 9653;
+    public long fillRefBitmap9654 = 9654;
+    public long fillRefBitmap9655 = 9655;
+    public long fillRefBitmap9656 = 9656;
+    public long fillRefBitmap9657 = 9657;
+    public long fillRefBitmap9658 = 9658;
+    public long fillRefBitmap9659 = 9659;
+    public long fillRefBitmap9660 = 9660;
+    public long fillRefBitmap9661 = 9661;
+    public long fillRefBitmap9662 = 9662;
+    public long fillRefBitmap9663 = 9663;
+    public long fillRefBitmap9664 = 9664;
+    public long fillRefBitmap9665 = 9665;
+    public long fillRefBitmap9666 = 9666;
+    public long fillRefBitmap9667 = 9667;
+    public long fillRefBitmap9668 = 9668;
+    public long fillRefBitmap9669 = 9669;
+    public long fillRefBitmap9670 = 9670;
+    public long fillRefBitmap9671 = 9671;
+    public long fillRefBitmap9672 = 9672;
+    public long fillRefBitmap9673 = 9673;
+    public long fillRefBitmap9674 = 9674;
+    public long fillRefBitmap9675 = 9675;
+    public long fillRefBitmap9676 = 9676;
+    public long fillRefBitmap9677 = 9677;
+    public long fillRefBitmap9678 = 9678;
+    public long fillRefBitmap9679 = 9679;
+    public long fillRefBitmap9680 = 9680;
+    public long fillRefBitmap9681 = 9681;
+    public long fillRefBitmap9682 = 9682;
+    public long fillRefBitmap9683 = 9683;
+    public long fillRefBitmap9684 = 9684;
+    public long fillRefBitmap9685 = 9685;
+    public long fillRefBitmap9686 = 9686;
+    public long fillRefBitmap9687 = 9687;
+    public long fillRefBitmap9688 = 9688;
+    public long fillRefBitmap9689 = 9689;
+    public long fillRefBitmap9690 = 9690;
+    public long fillRefBitmap9691 = 9691;
+    public long fillRefBitmap9692 = 9692;
+    public long fillRefBitmap9693 = 9693;
+    public long fillRefBitmap9694 = 9694;
+    public long fillRefBitmap9695 = 9695;
+    public long fillRefBitmap9696 = 9696;
+    public long fillRefBitmap9697 = 9697;
+    public long fillRefBitmap9698 = 9698;
+    public long fillRefBitmap9699 = 9699;
+    public long fillRefBitmap9700 = 9700;
+    public long fillRefBitmap9701 = 9701;
+    public long fillRefBitmap9702 = 9702;
+    public long fillRefBitmap9703 = 9703;
+    public long fillRefBitmap9704 = 9704;
+    public long fillRefBitmap9705 = 9705;
+    public long fillRefBitmap9706 = 9706;
+    public long fillRefBitmap9707 = 9707;
+    public long fillRefBitmap9708 = 9708;
+    public long fillRefBitmap9709 = 9709;
+    public long fillRefBitmap9710 = 9710;
+    public long fillRefBitmap9711 = 9711;
+    public long fillRefBitmap9712 = 9712;
+    public long fillRefBitmap9713 = 9713;
+    public long fillRefBitmap9714 = 9714;
+    public long fillRefBitmap9715 = 9715;
+    public long fillRefBitmap9716 = 9716;
+    public long fillRefBitmap9717 = 9717;
+    public long fillRefBitmap9718 = 9718;
+    public long fillRefBitmap9719 = 9719;
+    public long fillRefBitmap9720 = 9720;
+    public long fillRefBitmap9721 = 9721;
+    public long fillRefBitmap9722 = 9722;
+    public long fillRefBitmap9723 = 9723;
+    public long fillRefBitmap9724 = 9724;
+    public long fillRefBitmap9725 = 9725;
+    public long fillRefBitmap9726 = 9726;
+    public long fillRefBitmap9727 = 9727;
+    public long fillRefBitmap9728 = 9728;
+    public long fillRefBitmap9729 = 9729;
+    public long fillRefBitmap9730 = 9730;
+    public long fillRefBitmap9731 = 9731;
+    public long fillRefBitmap9732 = 9732;
+    public long fillRefBitmap9733 = 9733;
+    public long fillRefBitmap9734 = 9734;
+    public long fillRefBitmap9735 = 9735;
+    public long fillRefBitmap9736 = 9736;
+    public long fillRefBitmap9737 = 9737;
+    public long fillRefBitmap9738 = 9738;
+    public long fillRefBitmap9739 = 9739;
+    public long fillRefBitmap9740 = 9740;
+    public long fillRefBitmap9741 = 9741;
+    public long fillRefBitmap9742 = 9742;
+    public long fillRefBitmap9743 = 9743;
+    public long fillRefBitmap9744 = 9744;
+    public long fillRefBitmap9745 = 9745;
+    public long fillRefBitmap9746 = 9746;
+    public long fillRefBitmap9747 = 9747;
+    public long fillRefBitmap9748 = 9748;
+    public long fillRefBitmap9749 = 9749;
+    public long fillRefBitmap9750 = 9750;
+    public long fillRefBitmap9751 = 9751;
+    public long fillRefBitmap9752 = 9752;
+    public long fillRefBitmap9753 = 9753;
+    public long fillRefBitmap9754 = 9754;
+    public long fillRefBitmap9755 = 9755;
+    public long fillRefBitmap9756 = 9756;
+    public long fillRefBitmap9757 = 9757;
+    public long fillRefBitmap9758 = 9758;
+    public long fillRefBitmap9759 = 9759;
+    public long fillRefBitmap9760 = 9760;
+    public long fillRefBitmap9761 = 9761;
+    public long fillRefBitmap9762 = 9762;
+    public long fillRefBitmap9763 = 9763;
+    public long fillRefBitmap9764 = 9764;
+    public long fillRefBitmap9765 = 9765;
+    public long fillRefBitmap9766 = 9766;
+    public long fillRefBitmap9767 = 9767;
+    public long fillRefBitmap9768 = 9768;
+    public long fillRefBitmap9769 = 9769;
+    public long fillRefBitmap9770 = 9770;
+    public long fillRefBitmap9771 = 9771;
+    public long fillRefBitmap9772 = 9772;
+    public long fillRefBitmap9773 = 9773;
+    public long fillRefBitmap9774 = 9774;
+    public long fillRefBitmap9775 = 9775;
+    public long fillRefBitmap9776 = 9776;
+    public long fillRefBitmap9777 = 9777;
+    public long fillRefBitmap9778 = 9778;
+    public long fillRefBitmap9779 = 9779;
+    public long fillRefBitmap9780 = 9780;
+    public long fillRefBitmap9781 = 9781;
+    public long fillRefBitmap9782 = 9782;
+    public long fillRefBitmap9783 = 9783;
+    public long fillRefBitmap9784 = 9784;
+    public long fillRefBitmap9785 = 9785;
+    public long fillRefBitmap9786 = 9786;
+    public long fillRefBitmap9787 = 9787;
+    public long fillRefBitmap9788 = 9788;
+    public long fillRefBitmap9789 = 9789;
+    public long fillRefBitmap9790 = 9790;
+    public long fillRefBitmap9791 = 9791;
+    public long fillRefBitmap9792 = 9792;
+    public long fillRefBitmap9793 = 9793;
+    public long fillRefBitmap9794 = 9794;
+    public long fillRefBitmap9795 = 9795;
+    public long fillRefBitmap9796 = 9796;
+    public long fillRefBitmap9797 = 9797;
+    public long fillRefBitmap9798 = 9798;
+    public long fillRefBitmap9799 = 9799;
+    public long fillRefBitmap9800 = 9800;
+    public long fillRefBitmap9801 = 9801;
+    public long fillRefBitmap9802 = 9802;
+    public long fillRefBitmap9803 = 9803;
+    public long fillRefBitmap9804 = 9804;
+    public long fillRefBitmap9805 = 9805;
+    public long fillRefBitmap9806 = 9806;
+    public long fillRefBitmap9807 = 9807;
+    public long fillRefBitmap9808 = 9808;
+    public long fillRefBitmap9809 = 9809;
+    public long fillRefBitmap9810 = 9810;
+    public long fillRefBitmap9811 = 9811;
+    public long fillRefBitmap9812 = 9812;
+    public long fillRefBitmap9813 = 9813;
+    public long fillRefBitmap9814 = 9814;
+    public long fillRefBitmap9815 = 9815;
+    public long fillRefBitmap9816 = 9816;
+    public long fillRefBitmap9817 = 9817;
+    public long fillRefBitmap9818 = 9818;
+    public long fillRefBitmap9819 = 9819;
+    public long fillRefBitmap9820 = 9820;
+    public long fillRefBitmap9821 = 9821;
+    public long fillRefBitmap9822 = 9822;
+    public long fillRefBitmap9823 = 9823;
+    public long fillRefBitmap9824 = 9824;
+    public long fillRefBitmap9825 = 9825;
+    public long fillRefBitmap9826 = 9826;
+    public long fillRefBitmap9827 = 9827;
+    public long fillRefBitmap9828 = 9828;
+    public long fillRefBitmap9829 = 9829;
+    public long fillRefBitmap9830 = 9830;
+    public long fillRefBitmap9831 = 9831;
+    public long fillRefBitmap9832 = 9832;
+    public long fillRefBitmap9833 = 9833;
+    public long fillRefBitmap9834 = 9834;
+    public long fillRefBitmap9835 = 9835;
+    public long fillRefBitmap9836 = 9836;
+    public long fillRefBitmap9837 = 9837;
+    public long fillRefBitmap9838 = 9838;
+    public long fillRefBitmap9839 = 9839;
+    public long fillRefBitmap9840 = 9840;
+    public long fillRefBitmap9841 = 9841;
+    public long fillRefBitmap9842 = 9842;
+    public long fillRefBitmap9843 = 9843;
+    public long fillRefBitmap9844 = 9844;
+    public long fillRefBitmap9845 = 9845;
+    public long fillRefBitmap9846 = 9846;
+    public long fillRefBitmap9847 = 9847;
+    public long fillRefBitmap9848 = 9848;
+    public long fillRefBitmap9849 = 9849;
+    public long fillRefBitmap9850 = 9850;
+    public long fillRefBitmap9851 = 9851;
+    public long fillRefBitmap9852 = 9852;
+    public long fillRefBitmap9853 = 9853;
+    public long fillRefBitmap9854 = 9854;
+    public long fillRefBitmap9855 = 9855;
+    public long fillRefBitmap9856 = 9856;
+    public long fillRefBitmap9857 = 9857;
+    public long fillRefBitmap9858 = 9858;
+    public long fillRefBitmap9859 = 9859;
+    public long fillRefBitmap9860 = 9860;
+    public long fillRefBitmap9861 = 9861;
+    public long fillRefBitmap9862 = 9862;
+    public long fillRefBitmap9863 = 9863;
+    public long fillRefBitmap9864 = 9864;
+    public long fillRefBitmap9865 = 9865;
+    public long fillRefBitmap9866 = 9866;
+    public long fillRefBitmap9867 = 9867;
+    public long fillRefBitmap9868 = 9868;
+    public long fillRefBitmap9869 = 9869;
+    public long fillRefBitmap9870 = 9870;
+    public long fillRefBitmap9871 = 9871;
+    public long fillRefBitmap9872 = 9872;
+    public long fillRefBitmap9873 = 9873;
+    public long fillRefBitmap9874 = 9874;
+    public long fillRefBitmap9875 = 9875;
+    public long fillRefBitmap9876 = 9876;
+    public long fillRefBitmap9877 = 9877;
+    public long fillRefBitmap9878 = 9878;
+    public long fillRefBitmap9879 = 9879;
+    public long fillRefBitmap9880 = 9880;
+    public long fillRefBitmap9881 = 9881;
+    public long fillRefBitmap9882 = 9882;
+    public long fillRefBitmap9883 = 9883;
+    public long fillRefBitmap9884 = 9884;
+    public long fillRefBitmap9885 = 9885;
+    public long fillRefBitmap9886 = 9886;
+    public long fillRefBitmap9887 = 9887;
+    public long fillRefBitmap9888 = 9888;
+    public long fillRefBitmap9889 = 9889;
+    public long fillRefBitmap9890 = 9890;
+    public long fillRefBitmap9891 = 9891;
+    public long fillRefBitmap9892 = 9892;
+    public long fillRefBitmap9893 = 9893;
+    public long fillRefBitmap9894 = 9894;
+    public long fillRefBitmap9895 = 9895;
+    public long fillRefBitmap9896 = 9896;
+    public long fillRefBitmap9897 = 9897;
+    public long fillRefBitmap9898 = 9898;
+    public long fillRefBitmap9899 = 9899;
+    public long fillRefBitmap9900 = 9900;
+    public long fillRefBitmap9901 = 9901;
+    public long fillRefBitmap9902 = 9902;
+    public long fillRefBitmap9903 = 9903;
+    public long fillRefBitmap9904 = 9904;
+    public long fillRefBitmap9905 = 9905;
+    public long fillRefBitmap9906 = 9906;
+    public long fillRefBitmap9907 = 9907;
+    public long fillRefBitmap9908 = 9908;
+    public long fillRefBitmap9909 = 9909;
+    public long fillRefBitmap9910 = 9910;
+    public long fillRefBitmap9911 = 9911;
+    public long fillRefBitmap9912 = 9912;
+    public long fillRefBitmap9913 = 9913;
+    public long fillRefBitmap9914 = 9914;
+    public long fillRefBitmap9915 = 9915;
+    public long fillRefBitmap9916 = 9916;
+    public long fillRefBitmap9917 = 9917;
+    public long fillRefBitmap9918 = 9918;
+    public long fillRefBitmap9919 = 9919;
+    public long fillRefBitmap9920 = 9920;
+    public long fillRefBitmap9921 = 9921;
+    public long fillRefBitmap9922 = 9922;
+    public long fillRefBitmap9923 = 9923;
+    public long fillRefBitmap9924 = 9924;
+    public long fillRefBitmap9925 = 9925;
+    public long fillRefBitmap9926 = 9926;
+    public long fillRefBitmap9927 = 9927;
+    public long fillRefBitmap9928 = 9928;
+    public long fillRefBitmap9929 = 9929;
+    public long fillRefBitmap9930 = 9930;
+    public long fillRefBitmap9931 = 9931;
+    public long fillRefBitmap9932 = 9932;
+    public long fillRefBitmap9933 = 9933;
+    public long fillRefBitmap9934 = 9934;
+    public long fillRefBitmap9935 = 9935;
+    public long fillRefBitmap9936 = 9936;
+    public long fillRefBitmap9937 = 9937;
+    public long fillRefBitmap9938 = 9938;
+    public long fillRefBitmap9939 = 9939;
+    public long fillRefBitmap9940 = 9940;
+    public long fillRefBitmap9941 = 9941;
+    public long fillRefBitmap9942 = 9942;
+    public long fillRefBitmap9943 = 9943;
+    public long fillRefBitmap9944 = 9944;
+    public long fillRefBitmap9945 = 9945;
+    public long fillRefBitmap9946 = 9946;
+    public long fillRefBitmap9947 = 9947;
+    public long fillRefBitmap9948 = 9948;
+    public long fillRefBitmap9949 = 9949;
+    public long fillRefBitmap9950 = 9950;
+    public long fillRefBitmap9951 = 9951;
+    public long fillRefBitmap9952 = 9952;
+    public long fillRefBitmap9953 = 9953;
+    public long fillRefBitmap9954 = 9954;
+    public long fillRefBitmap9955 = 9955;
+    public long fillRefBitmap9956 = 9956;
+    public long fillRefBitmap9957 = 9957;
+    public long fillRefBitmap9958 = 9958;
+    public long fillRefBitmap9959 = 9959;
+    public long fillRefBitmap9960 = 9960;
+    public long fillRefBitmap9961 = 9961;
+    public long fillRefBitmap9962 = 9962;
+    public long fillRefBitmap9963 = 9963;
+    public long fillRefBitmap9964 = 9964;
+    public long fillRefBitmap9965 = 9965;
+    public long fillRefBitmap9966 = 9966;
+    public long fillRefBitmap9967 = 9967;
+    public long fillRefBitmap9968 = 9968;
+    public long fillRefBitmap9969 = 9969;
+    public long fillRefBitmap9970 = 9970;
+    public long fillRefBitmap9971 = 9971;
+    public long fillRefBitmap9972 = 9972;
+    public long fillRefBitmap9973 = 9973;
+    public long fillRefBitmap9974 = 9974;
+    public long fillRefBitmap9975 = 9975;
+    public long fillRefBitmap9976 = 9976;
+    public long fillRefBitmap9977 = 9977;
+    public long fillRefBitmap9978 = 9978;
+    public long fillRefBitmap9979 = 9979;
+    public long fillRefBitmap9980 = 9980;
+    public long fillRefBitmap9981 = 9981;
+    public long fillRefBitmap9982 = 9982;
+    public long fillRefBitmap9983 = 9983;
+    public long fillRefBitmap9984 = 9984;
+    public long fillRefBitmap9985 = 9985;
+    public long fillRefBitmap9986 = 9986;
+    public long fillRefBitmap9987 = 9987;
+    public long fillRefBitmap9988 = 9988;
+    public long fillRefBitmap9989 = 9989;
+    public long fillRefBitmap9990 = 9990;
+    public long fillRefBitmap9991 = 9991;
+    public long fillRefBitmap9992 = 9992;
+    public long fillRefBitmap9993 = 9993;
+    public long fillRefBitmap9994 = 9994;
+    public long fillRefBitmap9995 = 9995;
+    public long fillRefBitmap9996 = 9996;
+    public long fillRefBitmap9997 = 9997;
+    public long fillRefBitmap9998 = 9998;
+    public long fillRefBitmap9999 = 9999;
 }
diff --git a/test/160-read-barrier-stress/src/ManyFieldsBase2.java b/test/160-read-barrier-stress/src/ManyFieldsBase2.java
index 54bbe99f73..fc40328537 100644
--- a/test/160-read-barrier-stress/src/ManyFieldsBase2.java
+++ b/test/160-read-barrier-stress/src/ManyFieldsBase2.java
@@ -1015,4 +1015,5005 @@ class ManyFieldsBase2 extends ManyFieldsBase1 {
     public Object testField2997 = new Integer(2997);
     public Object testField2998 = new Integer(2998);
     public Object testField2999 = new Integer(2999);
+
+    public long fillRefBitmap10000 = 10000;
+    public long fillRefBitmap10001 = 10001;
+    public long fillRefBitmap10002 = 10002;
+    public long fillRefBitmap10003 = 10003;
+    public long fillRefBitmap10004 = 10004;
+    public long fillRefBitmap10005 = 10005;
+    public long fillRefBitmap10006 = 10006;
+    public long fillRefBitmap10007 = 10007;
+    public long fillRefBitmap10008 = 10008;
+    public long fillRefBitmap10009 = 10009;
+    public long fillRefBitmap10010 = 10010;
+    public long fillRefBitmap10011 = 10011;
+    public long fillRefBitmap10012 = 10012;
+    public long fillRefBitmap10013 = 10013;
+    public long fillRefBitmap10014 = 10014;
+    public long fillRefBitmap10015 = 10015;
+    public long fillRefBitmap10016 = 10016;
+    public long fillRefBitmap10017 = 10017;
+    public long fillRefBitmap10018 = 10018;
+    public long fillRefBitmap10019 = 10019;
+    public long fillRefBitmap10020 = 10020;
+    public long fillRefBitmap10021 = 10021;
+    public long fillRefBitmap10022 = 10022;
+    public long fillRefBitmap10023 = 10023;
+    public long fillRefBitmap10024 = 10024;
+    public long fillRefBitmap10025 = 10025;
+    public long fillRefBitmap10026 = 10026;
+    public long fillRefBitmap10027 = 10027;
+    public long fillRefBitmap10028 = 10028;
+    public long fillRefBitmap10029 = 10029;
+    public long fillRefBitmap10030 = 10030;
+    public long fillRefBitmap10031 = 10031;
+    public long fillRefBitmap10032 = 10032;
+    public long fillRefBitmap10033 = 10033;
+    public long fillRefBitmap10034 = 10034;
+    public long fillRefBitmap10035 = 10035;
+    public long fillRefBitmap10036 = 10036;
+    public long fillRefBitmap10037 = 10037;
+    public long fillRefBitmap10038 = 10038;
+    public long fillRefBitmap10039 = 10039;
+    public long fillRefBitmap10040 = 10040;
+    public long fillRefBitmap10041 = 10041;
+    public long fillRefBitmap10042 = 10042;
+    public long fillRefBitmap10043 = 10043;
+    public long fillRefBitmap10044 = 10044;
+    public long fillRefBitmap10045 = 10045;
+    public long fillRefBitmap10046 = 10046;
+    public long fillRefBitmap10047 = 10047;
+    public long fillRefBitmap10048 = 10048;
+    public long fillRefBitmap10049 = 10049;
+    public long fillRefBitmap10050 = 10050;
+    public long fillRefBitmap10051 = 10051;
+    public long fillRefBitmap10052 = 10052;
+    public long fillRefBitmap10053 = 10053;
+    public long fillRefBitmap10054 = 10054;
+    public long fillRefBitmap10055 = 10055;
+    public long fillRefBitmap10056 = 10056;
+    public long fillRefBitmap10057 = 10057;
+    public long fillRefBitmap10058 = 10058;
+    public long fillRefBitmap10059 = 10059;
+    public long fillRefBitmap10060 = 10060;
+    public long fillRefBitmap10061 = 10061;
+    public long fillRefBitmap10062 = 10062;
+    public long fillRefBitmap10063 = 10063;
+    public long fillRefBitmap10064 = 10064;
+    public long fillRefBitmap10065 = 10065;
+    public long fillRefBitmap10066 = 10066;
+    public long fillRefBitmap10067 = 10067;
+    public long fillRefBitmap10068 = 10068;
+    public long fillRefBitmap10069 = 10069;
+    public long fillRefBitmap10070 = 10070;
+    public long fillRefBitmap10071 = 10071;
+    public long fillRefBitmap10072 = 10072;
+    public long fillRefBitmap10073 = 10073;
+    public long fillRefBitmap10074 = 10074;
+    public long fillRefBitmap10075 = 10075;
+    public long fillRefBitmap10076 = 10076;
+    public long fillRefBitmap10077 = 10077;
+    public long fillRefBitmap10078 = 10078;
+    public long fillRefBitmap10079 = 10079;
+    public long fillRefBitmap10080 = 10080;
+    public long fillRefBitmap10081 = 10081;
+    public long fillRefBitmap10082 = 10082;
+    public long fillRefBitmap10083 = 10083;
+    public long fillRefBitmap10084 = 10084;
+    public long fillRefBitmap10085 = 10085;
+    public long fillRefBitmap10086 = 10086;
+    public long fillRefBitmap10087 = 10087;
+    public long fillRefBitmap10088 = 10088;
+    public long fillRefBitmap10089 = 10089;
+    public long fillRefBitmap10090 = 10090;
+    public long fillRefBitmap10091 = 10091;
+    public long fillRefBitmap10092 = 10092;
+    public long fillRefBitmap10093 = 10093;
+    public long fillRefBitmap10094 = 10094;
+    public long fillRefBitmap10095 = 10095;
+    public long fillRefBitmap10096 = 10096;
+    public long fillRefBitmap10097 = 10097;
+    public long fillRefBitmap10098 = 10098;
+    public long fillRefBitmap10099 = 10099;
+    public long fillRefBitmap10100 = 10100;
+    public long fillRefBitmap10101 = 10101;
+    public long fillRefBitmap10102 = 10102;
+    public long fillRefBitmap10103 = 10103;
+    public long fillRefBitmap10104 = 10104;
+    public long fillRefBitmap10105 = 10105;
+    public long fillRefBitmap10106 = 10106;
+    public long fillRefBitmap10107 = 10107;
+    public long fillRefBitmap10108 = 10108;
+    public long fillRefBitmap10109 = 10109;
+    public long fillRefBitmap10110 = 10110;
+    public long fillRefBitmap10111 = 10111;
+    public long fillRefBitmap10112 = 10112;
+    public long fillRefBitmap10113 = 10113;
+    public long fillRefBitmap10114 = 10114;
+    public long fillRefBitmap10115 = 10115;
+    public long fillRefBitmap10116 = 10116;
+    public long fillRefBitmap10117 = 10117;
+    public long fillRefBitmap10118 = 10118;
+    public long fillRefBitmap10119 = 10119;
+    public long fillRefBitmap10120 = 10120;
+    public long fillRefBitmap10121 = 10121;
+    public long fillRefBitmap10122 = 10122;
+    public long fillRefBitmap10123 = 10123;
+    public long fillRefBitmap10124 = 10124;
+    public long fillRefBitmap10125 = 10125;
+    public long fillRefBitmap10126 = 10126;
+    public long fillRefBitmap10127 = 10127;
+    public long fillRefBitmap10128 = 10128;
+    public long fillRefBitmap10129 = 10129;
+    public long fillRefBitmap10130 = 10130;
+    public long fillRefBitmap10131 = 10131;
+    public long fillRefBitmap10132 = 10132;
+    public long fillRefBitmap10133 = 10133;
+    public long fillRefBitmap10134 = 10134;
+    public long fillRefBitmap10135 = 10135;
+    public long fillRefBitmap10136 = 10136;
+    public long fillRefBitmap10137 = 10137;
+    public long fillRefBitmap10138 = 10138;
+    public long fillRefBitmap10139 = 10139;
+    public long fillRefBitmap10140 = 10140;
+    public long fillRefBitmap10141 = 10141;
+    public long fillRefBitmap10142 = 10142;
+    public long fillRefBitmap10143 = 10143;
+    public long fillRefBitmap10144 = 10144;
+    public long fillRefBitmap10145 = 10145;
+    public long fillRefBitmap10146 = 10146;
+    public long fillRefBitmap10147 = 10147;
+    public long fillRefBitmap10148 = 10148;
+    public long fillRefBitmap10149 = 10149;
+    public long fillRefBitmap10150 = 10150;
+    public long fillRefBitmap10151 = 10151;
+    public long fillRefBitmap10152 = 10152;
+    public long fillRefBitmap10153 = 10153;
+    public long fillRefBitmap10154 = 10154;
+    public long fillRefBitmap10155 = 10155;
+    public long fillRefBitmap10156 = 10156;
+    public long fillRefBitmap10157 = 10157;
+    public long fillRefBitmap10158 = 10158;
+    public long fillRefBitmap10159 = 10159;
+    public long fillRefBitmap10160 = 10160;
+    public long fillRefBitmap10161 = 10161;
+    public long fillRefBitmap10162 = 10162;
+    public long fillRefBitmap10163 = 10163;
+    public long fillRefBitmap10164 = 10164;
+    public long fillRefBitmap10165 = 10165;
+    public long fillRefBitmap10166 = 10166;
+    public long fillRefBitmap10167 = 10167;
+    public long fillRefBitmap10168 = 10168;
+    public long fillRefBitmap10169 = 10169;
+    public long fillRefBitmap10170 = 10170;
+    public long fillRefBitmap10171 = 10171;
+    public long fillRefBitmap10172 = 10172;
+    public long fillRefBitmap10173 = 10173;
+    public long fillRefBitmap10174 = 10174;
+    public long fillRefBitmap10175 = 10175;
+    public long fillRefBitmap10176 = 10176;
+    public long fillRefBitmap10177 = 10177;
+    public long fillRefBitmap10178 = 10178;
+    public long fillRefBitmap10179 = 10179;
+    public long fillRefBitmap10180 = 10180;
+    public long fillRefBitmap10181 = 10181;
+    public long fillRefBitmap10182 = 10182;
+    public long fillRefBitmap10183 = 10183;
+    public long fillRefBitmap10184 = 10184;
+    public long fillRefBitmap10185 = 10185;
+    public long fillRefBitmap10186 = 10186;
+    public long fillRefBitmap10187 = 10187;
+    public long fillRefBitmap10188 = 10188;
+    public long fillRefBitmap10189 = 10189;
+    public long fillRefBitmap10190 = 10190;
+    public long fillRefBitmap10191 = 10191;
+    public long fillRefBitmap10192 = 10192;
+    public long fillRefBitmap10193 = 10193;
+    public long fillRefBitmap10194 = 10194;
+    public long fillRefBitmap10195 = 10195;
+    public long fillRefBitmap10196 = 10196;
+    public long fillRefBitmap10197 = 10197;
+    public long fillRefBitmap10198 = 10198;
+    public long fillRefBitmap10199 = 10199;
+    public long fillRefBitmap10200 = 10200;
+    public long fillRefBitmap10201 = 10201;
+    public long fillRefBitmap10202 = 10202;
+    public long fillRefBitmap10203 = 10203;
+    public long fillRefBitmap10204 = 10204;
+    public long fillRefBitmap10205 = 10205;
+    public long fillRefBitmap10206 = 10206;
+    public long fillRefBitmap10207 = 10207;
+    public long fillRefBitmap10208 = 10208;
+    public long fillRefBitmap10209 = 10209;
+    public long fillRefBitmap10210 = 10210;
+    public long fillRefBitmap10211 = 10211;
+    public long fillRefBitmap10212 = 10212;
+    public long fillRefBitmap10213 = 10213;
+    public long fillRefBitmap10214 = 10214;
+    public long fillRefBitmap10215 = 10215;
+    public long fillRefBitmap10216 = 10216;
+    public long fillRefBitmap10217 = 10217;
+    public long fillRefBitmap10218 = 10218;
+    public long fillRefBitmap10219 = 10219;
+    public long fillRefBitmap10220 = 10220;
+    public long fillRefBitmap10221 = 10221;
+    public long fillRefBitmap10222 = 10222;
+    public long fillRefBitmap10223 = 10223;
+    public long fillRefBitmap10224 = 10224;
+    public long fillRefBitmap10225 = 10225;
+    public long fillRefBitmap10226 = 10226;
+    public long fillRefBitmap10227 = 10227;
+    public long fillRefBitmap10228 = 10228;
+    public long fillRefBitmap10229 = 10229;
+    public long fillRefBitmap10230 = 10230;
+    public long fillRefBitmap10231 = 10231;
+    public long fillRefBitmap10232 = 10232;
+    public long fillRefBitmap10233 = 10233;
+    public long fillRefBitmap10234 = 10234;
+    public long fillRefBitmap10235 = 10235;
+    public long fillRefBitmap10236 = 10236;
+    public long fillRefBitmap10237 = 10237;
+    public long fillRefBitmap10238 = 10238;
+    public long fillRefBitmap10239 = 10239;
+    public long fillRefBitmap10240 = 10240;
+    public long fillRefBitmap10241 = 10241;
+    public long fillRefBitmap10242 = 10242;
+    public long fillRefBitmap10243 = 10243;
+    public long fillRefBitmap10244 = 10244;
+    public long fillRefBitmap10245 = 10245;
+    public long fillRefBitmap10246 = 10246;
+    public long fillRefBitmap10247 = 10247;
+    public long fillRefBitmap10248 = 10248;
+    public long fillRefBitmap10249 = 10249;
+    public long fillRefBitmap10250 = 10250;
+    public long fillRefBitmap10251 = 10251;
+    public long fillRefBitmap10252 = 10252;
+    public long fillRefBitmap10253 = 10253;
+    public long fillRefBitmap10254 = 10254;
+    public long fillRefBitmap10255 = 10255;
+    public long fillRefBitmap10256 = 10256;
+    public long fillRefBitmap10257 = 10257;
+    public long fillRefBitmap10258 = 10258;
+    public long fillRefBitmap10259 = 10259;
+    public long fillRefBitmap10260 = 10260;
+    public long fillRefBitmap10261 = 10261;
+    public long fillRefBitmap10262 = 10262;
+    public long fillRefBitmap10263 = 10263;
+    public long fillRefBitmap10264 = 10264;
+    public long fillRefBitmap10265 = 10265;
+    public long fillRefBitmap10266 = 10266;
+    public long fillRefBitmap10267 = 10267;
+    public long fillRefBitmap10268 = 10268;
+    public long fillRefBitmap10269 = 10269;
+    public long fillRefBitmap10270 = 10270;
+    public long fillRefBitmap10271 = 10271;
+    public long fillRefBitmap10272 = 10272;
+    public long fillRefBitmap10273 = 10273;
+    public long fillRefBitmap10274 = 10274;
+    public long fillRefBitmap10275 = 10275;
+    public long fillRefBitmap10276 = 10276;
+    public long fillRefBitmap10277 = 10277;
+    public long fillRefBitmap10278 = 10278;
+    public long fillRefBitmap10279 = 10279;
+    public long fillRefBitmap10280 = 10280;
+    public long fillRefBitmap10281 = 10281;
+    public long fillRefBitmap10282 = 10282;
+    public long fillRefBitmap10283 = 10283;
+    public long fillRefBitmap10284 = 10284;
+    public long fillRefBitmap10285 = 10285;
+    public long fillRefBitmap10286 = 10286;
+    public long fillRefBitmap10287 = 10287;
+    public long fillRefBitmap10288 = 10288;
+    public long fillRefBitmap10289 = 10289;
+    public long fillRefBitmap10290 = 10290;
+    public long fillRefBitmap10291 = 10291;
+    public long fillRefBitmap10292 = 10292;
+    public long fillRefBitmap10293 = 10293;
+    public long fillRefBitmap10294 = 10294;
+    public long fillRefBitmap10295 = 10295;
+    public long fillRefBitmap10296 = 10296;
+    public long fillRefBitmap10297 = 10297;
+    public long fillRefBitmap10298 = 10298;
+    public long fillRefBitmap10299 = 10299;
+    public long fillRefBitmap10300 = 10300;
+    public long fillRefBitmap10301 = 10301;
+    public long fillRefBitmap10302 = 10302;
+    public long fillRefBitmap10303 = 10303;
+    public long fillRefBitmap10304 = 10304;
+    public long fillRefBitmap10305 = 10305;
+    public long fillRefBitmap10306 = 10306;
+    public long fillRefBitmap10307 = 10307;
+    public long fillRefBitmap10308 = 10308;
+    public long fillRefBitmap10309 = 10309;
+    public long fillRefBitmap10310 = 10310;
+    public long fillRefBitmap10311 = 10311;
+    public long fillRefBitmap10312 = 10312;
+    public long fillRefBitmap10313 = 10313;
+    public long fillRefBitmap10314 = 10314;
+    public long fillRefBitmap10315 = 10315;
+    public long fillRefBitmap10316 = 10316;
+    public long fillRefBitmap10317 = 10317;
+    public long fillRefBitmap10318 = 10318;
+    public long fillRefBitmap10319 = 10319;
+    public long fillRefBitmap10320 = 10320;
+    public long fillRefBitmap10321 = 10321;
+    public long fillRefBitmap10322 = 10322;
+    public long fillRefBitmap10323 = 10323;
+    public long fillRefBitmap10324 = 10324;
+    public long fillRefBitmap10325 = 10325;
+    public long fillRefBitmap10326 = 10326;
+    public long fillRefBitmap10327 = 10327;
+    public long fillRefBitmap10328 = 10328;
+    public long fillRefBitmap10329 = 10329;
+    public long fillRefBitmap10330 = 10330;
+    public long fillRefBitmap10331 = 10331;
+    public long fillRefBitmap10332 = 10332;
+    public long fillRefBitmap10333 = 10333;
+    public long fillRefBitmap10334 = 10334;
+    public long fillRefBitmap10335 = 10335;
+    public long fillRefBitmap10336 = 10336;
+    public long fillRefBitmap10337 = 10337;
+    public long fillRefBitmap10338 = 10338;
+    public long fillRefBitmap10339 = 10339;
+    public long fillRefBitmap10340 = 10340;
+    public long fillRefBitmap10341 = 10341;
+    public long fillRefBitmap10342 = 10342;
+    public long fillRefBitmap10343 = 10343;
+    public long fillRefBitmap10344 = 10344;
+    public long fillRefBitmap10345 = 10345;
+    public long fillRefBitmap10346 = 10346;
+    public long fillRefBitmap10347 = 10347;
+    public long fillRefBitmap10348 = 10348;
+    public long fillRefBitmap10349 = 10349;
+    public long fillRefBitmap10350 = 10350;
+    public long fillRefBitmap10351 = 10351;
+    public long fillRefBitmap10352 = 10352;
+    public long fillRefBitmap10353 = 10353;
+    public long fillRefBitmap10354 = 10354;
+    public long fillRefBitmap10355 = 10355;
+    public long fillRefBitmap10356 = 10356;
+    public long fillRefBitmap10357 = 10357;
+    public long fillRefBitmap10358 = 10358;
+    public long fillRefBitmap10359 = 10359;
+    public long fillRefBitmap10360 = 10360;
+    public long fillRefBitmap10361 = 10361;
+    public long fillRefBitmap10362 = 10362;
+    public long fillRefBitmap10363 = 10363;
+    public long fillRefBitmap10364 = 10364;
+    public long fillRefBitmap10365 = 10365;
+    public long fillRefBitmap10366 = 10366;
+    public long fillRefBitmap10367 = 10367;
+    public long fillRefBitmap10368 = 10368;
+    public long fillRefBitmap10369 = 10369;
+    public long fillRefBitmap10370 = 10370;
+    public long fillRefBitmap10371 = 10371;
+    public long fillRefBitmap10372 = 10372;
+    public long fillRefBitmap10373 = 10373;
+    public long fillRefBitmap10374 = 10374;
+    public long fillRefBitmap10375 = 10375;
+    public long fillRefBitmap10376 = 10376;
+    public long fillRefBitmap10377 = 10377;
+    public long fillRefBitmap10378 = 10378;
+    public long fillRefBitmap10379 = 10379;
+    public long fillRefBitmap10380 = 10380;
+    public long fillRefBitmap10381 = 10381;
+    public long fillRefBitmap10382 = 10382;
+    public long fillRefBitmap10383 = 10383;
+    public long fillRefBitmap10384 = 10384;
+    public long fillRefBitmap10385 = 10385;
+    public long fillRefBitmap10386 = 10386;
+    public long fillRefBitmap10387 = 10387;
+    public long fillRefBitmap10388 = 10388;
+    public long fillRefBitmap10389 = 10389;
+    public long fillRefBitmap10390 = 10390;
+    public long fillRefBitmap10391 = 10391;
+    public long fillRefBitmap10392 = 10392;
+    public long fillRefBitmap10393 = 10393;
+    public long fillRefBitmap10394 = 10394;
+    public long fillRefBitmap10395 = 10395;
+    public long fillRefBitmap10396 = 10396;
+    public long fillRefBitmap10397 = 10397;
+    public long fillRefBitmap10398 = 10398;
+    public long fillRefBitmap10399 = 10399;
+    public long fillRefBitmap10400 = 10400;
+    public long fillRefBitmap10401 = 10401;
+    public long fillRefBitmap10402 = 10402;
+    public long fillRefBitmap10403 = 10403;
+    public long fillRefBitmap10404 = 10404;
+    public long fillRefBitmap10405 = 10405;
+    public long fillRefBitmap10406 = 10406;
+    public long fillRefBitmap10407 = 10407;
+    public long fillRefBitmap10408 = 10408;
+    public long fillRefBitmap10409 = 10409;
+    public long fillRefBitmap10410 = 10410;
+    public long fillRefBitmap10411 = 10411;
+    public long fillRefBitmap10412 = 10412;
+    public long fillRefBitmap10413 = 10413;
+    public long fillRefBitmap10414 = 10414;
+    public long fillRefBitmap10415 = 10415;
+    public long fillRefBitmap10416 = 10416;
+    public long fillRefBitmap10417 = 10417;
+    public long fillRefBitmap10418 = 10418;
+    public long fillRefBitmap10419 = 10419;
+    public long fillRefBitmap10420 = 10420;
+    public long fillRefBitmap10421 = 10421;
+    public long fillRefBitmap10422 = 10422;
+    public long fillRefBitmap10423 = 10423;
+    public long fillRefBitmap10424 = 10424;
+    public long fillRefBitmap10425 = 10425;
+    public long fillRefBitmap10426 = 10426;
+    public long fillRefBitmap10427 = 10427;
+    public long fillRefBitmap10428 = 10428;
+    public long fillRefBitmap10429 = 10429;
+    public long fillRefBitmap10430 = 10430;
+    public long fillRefBitmap10431 = 10431;
+    public long fillRefBitmap10432 = 10432;
+    public long fillRefBitmap10433 = 10433;
+    public long fillRefBitmap10434 = 10434;
+    public long fillRefBitmap10435 = 10435;
+    public long fillRefBitmap10436 = 10436;
+    public long fillRefBitmap10437 = 10437;
+    public long fillRefBitmap10438 = 10438;
+    public long fillRefBitmap10439 = 10439;
+    public long fillRefBitmap10440 = 10440;
+    public long fillRefBitmap10441 = 10441;
+    public long fillRefBitmap10442 = 10442;
+    public long fillRefBitmap10443 = 10443;
+    public long fillRefBitmap10444 = 10444;
+    public long fillRefBitmap10445 = 10445;
+    public long fillRefBitmap10446 = 10446;
+    public long fillRefBitmap10447 = 10447;
+    public long fillRefBitmap10448 = 10448;
+    public long fillRefBitmap10449 = 10449;
+    public long fillRefBitmap10450 = 10450;
+    public long fillRefBitmap10451 = 10451;
+    public long fillRefBitmap10452 = 10452;
+    public long fillRefBitmap10453 = 10453;
+    public long fillRefBitmap10454 = 10454;
+    public long fillRefBitmap10455 = 10455;
+    public long fillRefBitmap10456 = 10456;
+    public long fillRefBitmap10457 = 10457;
+    public long fillRefBitmap10458 = 10458;
+    public long fillRefBitmap10459 = 10459;
+    public long fillRefBitmap10460 = 10460;
+    public long fillRefBitmap10461 = 10461;
+    public long fillRefBitmap10462 = 10462;
+    public long fillRefBitmap10463 = 10463;
+    public long fillRefBitmap10464 = 10464;
+    public long fillRefBitmap10465 = 10465;
+    public long fillRefBitmap10466 = 10466;
+    public long fillRefBitmap10467 = 10467;
+    public long fillRefBitmap10468 = 10468;
+    public long fillRefBitmap10469 = 10469;
+    public long fillRefBitmap10470 = 10470;
+    public long fillRefBitmap10471 = 10471;
+    public long fillRefBitmap10472 = 10472;
+    public long fillRefBitmap10473 = 10473;
+    public long fillRefBitmap10474 = 10474;
+    public long fillRefBitmap10475 = 10475;
+    public long fillRefBitmap10476 = 10476;
+    public long fillRefBitmap10477 = 10477;
+    public long fillRefBitmap10478 = 10478;
+    public long fillRefBitmap10479 = 10479;
+    public long fillRefBitmap10480 = 10480;
+    public long fillRefBitmap10481 = 10481;
+    public long fillRefBitmap10482 = 10482;
+    public long fillRefBitmap10483 = 10483;
+    public long fillRefBitmap10484 = 10484;
+    public long fillRefBitmap10485 = 10485;
+    public long fillRefBitmap10486 = 10486;
+    public long fillRefBitmap10487 = 10487;
+    public long fillRefBitmap10488 = 10488;
+    public long fillRefBitmap10489 = 10489;
+    public long fillRefBitmap10490 = 10490;
+    public long fillRefBitmap10491 = 10491;
+    public long fillRefBitmap10492 = 10492;
+    public long fillRefBitmap10493 = 10493;
+    public long fillRefBitmap10494 = 10494;
+    public long fillRefBitmap10495 = 10495;
+    public long fillRefBitmap10496 = 10496;
+    public long fillRefBitmap10497 = 10497;
+    public long fillRefBitmap10498 = 10498;
+    public long fillRefBitmap10499 = 10499;
+    public long fillRefBitmap10500 = 10500;
+    public long fillRefBitmap10501 = 10501;
+    public long fillRefBitmap10502 = 10502;
+    public long fillRefBitmap10503 = 10503;
+    public long fillRefBitmap10504 = 10504;
+    public long fillRefBitmap10505 = 10505;
+    public long fillRefBitmap10506 = 10506;
+    public long fillRefBitmap10507 = 10507;
+    public long fillRefBitmap10508 = 10508;
+    public long fillRefBitmap10509 = 10509;
+    public long fillRefBitmap10510 = 10510;
+    public long fillRefBitmap10511 = 10511;
+    public long fillRefBitmap10512 = 10512;
+    public long fillRefBitmap10513 = 10513;
+    public long fillRefBitmap10514 = 10514;
+    public long fillRefBitmap10515 = 10515;
+    public long fillRefBitmap10516 = 10516;
+    public long fillRefBitmap10517 = 10517;
+    public long fillRefBitmap10518 = 10518;
+    public long fillRefBitmap10519 = 10519;
+    public long fillRefBitmap10520 = 10520;
+    public long fillRefBitmap10521 = 10521;
+    public long fillRefBitmap10522 = 10522;
+    public long fillRefBitmap10523 = 10523;
+    public long fillRefBitmap10524 = 10524;
+    public long fillRefBitmap10525 = 10525;
+    public long fillRefBitmap10526 = 10526;
+    public long fillRefBitmap10527 = 10527;
+    public long fillRefBitmap10528 = 10528;
+    public long fillRefBitmap10529 = 10529;
+    public long fillRefBitmap10530 = 10530;
+    public long fillRefBitmap10531 = 10531;
+    public long fillRefBitmap10532 = 10532;
+    public long fillRefBitmap10533 = 10533;
+    public long fillRefBitmap10534 = 10534;
+    public long fillRefBitmap10535 = 10535;
+    public long fillRefBitmap10536 = 10536;
+    public long fillRefBitmap10537 = 10537;
+    public long fillRefBitmap10538 = 10538;
+    public long fillRefBitmap10539 = 10539;
+    public long fillRefBitmap10540 = 10540;
+    public long fillRefBitmap10541 = 10541;
+    public long fillRefBitmap10542 = 10542;
+    public long fillRefBitmap10543 = 10543;
+    public long fillRefBitmap10544 = 10544;
+    public long fillRefBitmap10545 = 10545;
+    public long fillRefBitmap10546 = 10546;
+    public long fillRefBitmap10547 = 10547;
+    public long fillRefBitmap10548 = 10548;
+    public long fillRefBitmap10549 = 10549;
+    public long fillRefBitmap10550 = 10550;
+    public long fillRefBitmap10551 = 10551;
+    public long fillRefBitmap10552 = 10552;
+    public long fillRefBitmap10553 = 10553;
+    public long fillRefBitmap10554 = 10554;
+    public long fillRefBitmap10555 = 10555;
+    public long fillRefBitmap10556 = 10556;
+    public long fillRefBitmap10557 = 10557;
+    public long fillRefBitmap10558 = 10558;
+    public long fillRefBitmap10559 = 10559;
+    public long fillRefBitmap10560 = 10560;
+    public long fillRefBitmap10561 = 10561;
+    public long fillRefBitmap10562 = 10562;
+    public long fillRefBitmap10563 = 10563;
+    public long fillRefBitmap10564 = 10564;
+    public long fillRefBitmap10565 = 10565;
+    public long fillRefBitmap10566 = 10566;
+    public long fillRefBitmap10567 = 10567;
+    public long fillRefBitmap10568 = 10568;
+    public long fillRefBitmap10569 = 10569;
+    public long fillRefBitmap10570 = 10570;
+    public long fillRefBitmap10571 = 10571;
+    public long fillRefBitmap10572 = 10572;
+    public long fillRefBitmap10573 = 10573;
+    public long fillRefBitmap10574 = 10574;
+    public long fillRefBitmap10575 = 10575;
+    public long fillRefBitmap10576 = 10576;
+    public long fillRefBitmap10577 = 10577;
+    public long fillRefBitmap10578 = 10578;
+    public long fillRefBitmap10579 = 10579;
+    public long fillRefBitmap10580 = 10580;
+    public long fillRefBitmap10581 = 10581;
+    public long fillRefBitmap10582 = 10582;
+    public long fillRefBitmap10583 = 10583;
+    public long fillRefBitmap10584 = 10584;
+    public long fillRefBitmap10585 = 10585;
+    public long fillRefBitmap10586 = 10586;
+    public long fillRefBitmap10587 = 10587;
+    public long fillRefBitmap10588 = 10588;
+    public long fillRefBitmap10589 = 10589;
+    public long fillRefBitmap10590 = 10590;
+    public long fillRefBitmap10591 = 10591;
+    public long fillRefBitmap10592 = 10592;
+    public long fillRefBitmap10593 = 10593;
+    public long fillRefBitmap10594 = 10594;
+    public long fillRefBitmap10595 = 10595;
+    public long fillRefBitmap10596 = 10596;
+    public long fillRefBitmap10597 = 10597;
+    public long fillRefBitmap10598 = 10598;
+    public long fillRefBitmap10599 = 10599;
+    public long fillRefBitmap10600 = 10600;
+    public long fillRefBitmap10601 = 10601;
+    public long fillRefBitmap10602 = 10602;
+    public long fillRefBitmap10603 = 10603;
+    public long fillRefBitmap10604 = 10604;
+    public long fillRefBitmap10605 = 10605;
+    public long fillRefBitmap10606 = 10606;
+    public long fillRefBitmap10607 = 10607;
+    public long fillRefBitmap10608 = 10608;
+    public long fillRefBitmap10609 = 10609;
+    public long fillRefBitmap10610 = 10610;
+    public long fillRefBitmap10611 = 10611;
+    public long fillRefBitmap10612 = 10612;
+    public long fillRefBitmap10613 = 10613;
+    public long fillRefBitmap10614 = 10614;
+    public long fillRefBitmap10615 = 10615;
+    public long fillRefBitmap10616 = 10616;
+    public long fillRefBitmap10617 = 10617;
+    public long fillRefBitmap10618 = 10618;
+    public long fillRefBitmap10619 = 10619;
+    public long fillRefBitmap10620 = 10620;
+    public long fillRefBitmap10621 = 10621;
+    public long fillRefBitmap10622 = 10622;
+    public long fillRefBitmap10623 = 10623;
+    public long fillRefBitmap10624 = 10624;
+    public long fillRefBitmap10625 = 10625;
+    public long fillRefBitmap10626 = 10626;
+    public long fillRefBitmap10627 = 10627;
+    public long fillRefBitmap10628 = 10628;
+    public long fillRefBitmap10629 = 10629;
+    public long fillRefBitmap10630 = 10630;
+    public long fillRefBitmap10631 = 10631;
+    public long fillRefBitmap10632 = 10632;
+    public long fillRefBitmap10633 = 10633;
+    public long fillRefBitmap10634 = 10634;
+    public long fillRefBitmap10635 = 10635;
+    public long fillRefBitmap10636 = 10636;
+    public long fillRefBitmap10637 = 10637;
+    public long fillRefBitmap10638 = 10638;
+    public long fillRefBitmap10639 = 10639;
+    public long fillRefBitmap10640 = 10640;
+    public long fillRefBitmap10641 = 10641;
+    public long fillRefBitmap10642 = 10642;
+    public long fillRefBitmap10643 = 10643;
+    public long fillRefBitmap10644 = 10644;
+    public long fillRefBitmap10645 = 10645;
+    public long fillRefBitmap10646 = 10646;
+    public long fillRefBitmap10647 = 10647;
+    public long fillRefBitmap10648 = 10648;
+    public long fillRefBitmap10649 = 10649;
+    public long fillRefBitmap10650 = 10650;
+    public long fillRefBitmap10651 = 10651;
+    public long fillRefBitmap10652 = 10652;
+    public long fillRefBitmap10653 = 10653;
+    public long fillRefBitmap10654 = 10654;
+    public long fillRefBitmap10655 = 10655;
+    public long fillRefBitmap10656 = 10656;
+    public long fillRefBitmap10657 = 10657;
+    public long fillRefBitmap10658 = 10658;
+    public long fillRefBitmap10659 = 10659;
+    public long fillRefBitmap10660 = 10660;
+    public long fillRefBitmap10661 = 10661;
+    public long fillRefBitmap10662 = 10662;
+    public long fillRefBitmap10663 = 10663;
+    public long fillRefBitmap10664 = 10664;
+    public long fillRefBitmap10665 = 10665;
+    public long fillRefBitmap10666 = 10666;
+    public long fillRefBitmap10667 = 10667;
+    public long fillRefBitmap10668 = 10668;
+    public long fillRefBitmap10669 = 10669;
+    public long fillRefBitmap10670 = 10670;
+    public long fillRefBitmap10671 = 10671;
+    public long fillRefBitmap10672 = 10672;
+    public long fillRefBitmap10673 = 10673;
+    public long fillRefBitmap10674 = 10674;
+    public long fillRefBitmap10675 = 10675;
+    public long fillRefBitmap10676 = 10676;
+    public long fillRefBitmap10677 = 10677;
+    public long fillRefBitmap10678 = 10678;
+    public long fillRefBitmap10679 = 10679;
+    public long fillRefBitmap10680 = 10680;
+    public long fillRefBitmap10681 = 10681;
+    public long fillRefBitmap10682 = 10682;
+    public long fillRefBitmap10683 = 10683;
+    public long fillRefBitmap10684 = 10684;
+    public long fillRefBitmap10685 = 10685;
+    public long fillRefBitmap10686 = 10686;
+    public long fillRefBitmap10687 = 10687;
+    public long fillRefBitmap10688 = 10688;
+    public long fillRefBitmap10689 = 10689;
+    public long fillRefBitmap10690 = 10690;
+    public long fillRefBitmap10691 = 10691;
+    public long fillRefBitmap10692 = 10692;
+    public long fillRefBitmap10693 = 10693;
+    public long fillRefBitmap10694 = 10694;
+    public long fillRefBitmap10695 = 10695;
+    public long fillRefBitmap10696 = 10696;
+    public long fillRefBitmap10697 = 10697;
+    public long fillRefBitmap10698 = 10698;
+    public long fillRefBitmap10699 = 10699;
+    public long fillRefBitmap10700 = 10700;
+    public long fillRefBitmap10701 = 10701;
+    public long fillRefBitmap10702 = 10702;
+    public long fillRefBitmap10703 = 10703;
+    public long fillRefBitmap10704 = 10704;
+    public long fillRefBitmap10705 = 10705;
+    public long fillRefBitmap10706 = 10706;
+    public long fillRefBitmap10707 = 10707;
+    public long fillRefBitmap10708 = 10708;
+    public long fillRefBitmap10709 = 10709;
+    public long fillRefBitmap10710 = 10710;
+    public long fillRefBitmap10711 = 10711;
+    public long fillRefBitmap10712 = 10712;
+    public long fillRefBitmap10713 = 10713;
+    public long fillRefBitmap10714 = 10714;
+    public long fillRefBitmap10715 = 10715;
+    public long fillRefBitmap10716 = 10716;
+    public long fillRefBitmap10717 = 10717;
+    public long fillRefBitmap10718 = 10718;
+    public long fillRefBitmap10719 = 10719;
+    public long fillRefBitmap10720 = 10720;
+    public long fillRefBitmap10721 = 10721;
+    public long fillRefBitmap10722 = 10722;
+    public long fillRefBitmap10723 = 10723;
+    public long fillRefBitmap10724 = 10724;
+    public long fillRefBitmap10725 = 10725;
+    public long fillRefBitmap10726 = 10726;
+    public long fillRefBitmap10727 = 10727;
+    public long fillRefBitmap10728 = 10728;
+    public long fillRefBitmap10729 = 10729;
+    public long fillRefBitmap10730 = 10730;
+    public long fillRefBitmap10731 = 10731;
+    public long fillRefBitmap10732 = 10732;
+    public long fillRefBitmap10733 = 10733;
+    public long fillRefBitmap10734 = 10734;
+    public long fillRefBitmap10735 = 10735;
+    public long fillRefBitmap10736 = 10736;
+    public long fillRefBitmap10737 = 10737;
+    public long fillRefBitmap10738 = 10738;
+    public long fillRefBitmap10739 = 10739;
+    public long fillRefBitmap10740 = 10740;
+    public long fillRefBitmap10741 = 10741;
+    public long fillRefBitmap10742 = 10742;
+    public long fillRefBitmap10743 = 10743;
+    public long fillRefBitmap10744 = 10744;
+    public long fillRefBitmap10745 = 10745;
+    public long fillRefBitmap10746 = 10746;
+    public long fillRefBitmap10747 = 10747;
+    public long fillRefBitmap10748 = 10748;
+    public long fillRefBitmap10749 = 10749;
+    public long fillRefBitmap10750 = 10750;
+    public long fillRefBitmap10751 = 10751;
+    public long fillRefBitmap10752 = 10752;
+    public long fillRefBitmap10753 = 10753;
+    public long fillRefBitmap10754 = 10754;
+    public long fillRefBitmap10755 = 10755;
+    public long fillRefBitmap10756 = 10756;
+    public long fillRefBitmap10757 = 10757;
+    public long fillRefBitmap10758 = 10758;
+    public long fillRefBitmap10759 = 10759;
+    public long fillRefBitmap10760 = 10760;
+    public long fillRefBitmap10761 = 10761;
+    public long fillRefBitmap10762 = 10762;
+    public long fillRefBitmap10763 = 10763;
+    public long fillRefBitmap10764 = 10764;
+    public long fillRefBitmap10765 = 10765;
+    public long fillRefBitmap10766 = 10766;
+    public long fillRefBitmap10767 = 10767;
+    public long fillRefBitmap10768 = 10768;
+    public long fillRefBitmap10769 = 10769;
+    public long fillRefBitmap10770 = 10770;
+    public long fillRefBitmap10771 = 10771;
+    public long fillRefBitmap10772 = 10772;
+    public long fillRefBitmap10773 = 10773;
+    public long fillRefBitmap10774 = 10774;
+    public long fillRefBitmap10775 = 10775;
+    public long fillRefBitmap10776 = 10776;
+    public long fillRefBitmap10777 = 10777;
+    public long fillRefBitmap10778 = 10778;
+    public long fillRefBitmap10779 = 10779;
+    public long fillRefBitmap10780 = 10780;
+    public long fillRefBitmap10781 = 10781;
+    public long fillRefBitmap10782 = 10782;
+    public long fillRefBitmap10783 = 10783;
+    public long fillRefBitmap10784 = 10784;
+    public long fillRefBitmap10785 = 10785;
+    public long fillRefBitmap10786 = 10786;
+    public long fillRefBitmap10787 = 10787;
+    public long fillRefBitmap10788 = 10788;
+    public long fillRefBitmap10789 = 10789;
+    public long fillRefBitmap10790 = 10790;
+    public long fillRefBitmap10791 = 10791;
+    public long fillRefBitmap10792 = 10792;
+    public long fillRefBitmap10793 = 10793;
+    public long fillRefBitmap10794 = 10794;
+    public long fillRefBitmap10795 = 10795;
+    public long fillRefBitmap10796 = 10796;
+    public long fillRefBitmap10797 = 10797;
+    public long fillRefBitmap10798 = 10798;
+    public long fillRefBitmap10799 = 10799;
+    public long fillRefBitmap10800 = 10800;
+    public long fillRefBitmap10801 = 10801;
+    public long fillRefBitmap10802 = 10802;
+    public long fillRefBitmap10803 = 10803;
+    public long fillRefBitmap10804 = 10804;
+    public long fillRefBitmap10805 = 10805;
+    public long fillRefBitmap10806 = 10806;
+    public long fillRefBitmap10807 = 10807;
+    public long fillRefBitmap10808 = 10808;
+    public long fillRefBitmap10809 = 10809;
+    public long fillRefBitmap10810 = 10810;
+    public long fillRefBitmap10811 = 10811;
+    public long fillRefBitmap10812 = 10812;
+    public long fillRefBitmap10813 = 10813;
+    public long fillRefBitmap10814 = 10814;
+    public long fillRefBitmap10815 = 10815;
+    public long fillRefBitmap10816 = 10816;
+    public long fillRefBitmap10817 = 10817;
+    public long fillRefBitmap10818 = 10818;
+    public long fillRefBitmap10819 = 10819;
+    public long fillRefBitmap10820 = 10820;
+    public long fillRefBitmap10821 = 10821;
+    public long fillRefBitmap10822 = 10822;
+    public long fillRefBitmap10823 = 10823;
+    public long fillRefBitmap10824 = 10824;
+    public long fillRefBitmap10825 = 10825;
+    public long fillRefBitmap10826 = 10826;
+    public long fillRefBitmap10827 = 10827;
+    public long fillRefBitmap10828 = 10828;
+    public long fillRefBitmap10829 = 10829;
+    public long fillRefBitmap10830 = 10830;
+    public long fillRefBitmap10831 = 10831;
+    public long fillRefBitmap10832 = 10832;
+    public long fillRefBitmap10833 = 10833;
+    public long fillRefBitmap10834 = 10834;
+    public long fillRefBitmap10835 = 10835;
+    public long fillRefBitmap10836 = 10836;
+    public long fillRefBitmap10837 = 10837;
+    public long fillRefBitmap10838 = 10838;
+    public long fillRefBitmap10839 = 10839;
+    public long fillRefBitmap10840 = 10840;
+    public long fillRefBitmap10841 = 10841;
+    public long fillRefBitmap10842 = 10842;
+    public long fillRefBitmap10843 = 10843;
+    public long fillRefBitmap10844 = 10844;
+    public long fillRefBitmap10845 = 10845;
+    public long fillRefBitmap10846 = 10846;
+    public long fillRefBitmap10847 = 10847;
+    public long fillRefBitmap10848 = 10848;
+    public long fillRefBitmap10849 = 10849;
+    public long fillRefBitmap10850 = 10850;
+    public long fillRefBitmap10851 = 10851;
+    public long fillRefBitmap10852 = 10852;
+    public long fillRefBitmap10853 = 10853;
+    public long fillRefBitmap10854 = 10854;
+    public long fillRefBitmap10855 = 10855;
+    public long fillRefBitmap10856 = 10856;
+    public long fillRefBitmap10857 = 10857;
+    public long fillRefBitmap10858 = 10858;
+    public long fillRefBitmap10859 = 10859;
+    public long fillRefBitmap10860 = 10860;
+    public long fillRefBitmap10861 = 10861;
+    public long fillRefBitmap10862 = 10862;
+    public long fillRefBitmap10863 = 10863;
+    public long fillRefBitmap10864 = 10864;
+    public long fillRefBitmap10865 = 10865;
+    public long fillRefBitmap10866 = 10866;
+    public long fillRefBitmap10867 = 10867;
+    public long fillRefBitmap10868 = 10868;
+    public long fillRefBitmap10869 = 10869;
+    public long fillRefBitmap10870 = 10870;
+    public long fillRefBitmap10871 = 10871;
+    public long fillRefBitmap10872 = 10872;
+    public long fillRefBitmap10873 = 10873;
+    public long fillRefBitmap10874 = 10874;
+    public long fillRefBitmap10875 = 10875;
+    public long fillRefBitmap10876 = 10876;
+    public long fillRefBitmap10877 = 10877;
+    public long fillRefBitmap10878 = 10878;
+    public long fillRefBitmap10879 = 10879;
+    public long fillRefBitmap10880 = 10880;
+    public long fillRefBitmap10881 = 10881;
+    public long fillRefBitmap10882 = 10882;
+    public long fillRefBitmap10883 = 10883;
+    public long fillRefBitmap10884 = 10884;
+    public long fillRefBitmap10885 = 10885;
+    public long fillRefBitmap10886 = 10886;
+    public long fillRefBitmap10887 = 10887;
+    public long fillRefBitmap10888 = 10888;
+    public long fillRefBitmap10889 = 10889;
+    public long fillRefBitmap10890 = 10890;
+    public long fillRefBitmap10891 = 10891;
+    public long fillRefBitmap10892 = 10892;
+    public long fillRefBitmap10893 = 10893;
+    public long fillRefBitmap10894 = 10894;
+    public long fillRefBitmap10895 = 10895;
+    public long fillRefBitmap10896 = 10896;
+    public long fillRefBitmap10897 = 10897;
+    public long fillRefBitmap10898 = 10898;
+    public long fillRefBitmap10899 = 10899;
+    public long fillRefBitmap10900 = 10900;
+    public long fillRefBitmap10901 = 10901;
+    public long fillRefBitmap10902 = 10902;
+    public long fillRefBitmap10903 = 10903;
+    public long fillRefBitmap10904 = 10904;
+    public long fillRefBitmap10905 = 10905;
+    public long fillRefBitmap10906 = 10906;
+    public long fillRefBitmap10907 = 10907;
+    public long fillRefBitmap10908 = 10908;
+    public long fillRefBitmap10909 = 10909;
+    public long fillRefBitmap10910 = 10910;
+    public long fillRefBitmap10911 = 10911;
+    public long fillRefBitmap10912 = 10912;
+    public long fillRefBitmap10913 = 10913;
+    public long fillRefBitmap10914 = 10914;
+    public long fillRefBitmap10915 = 10915;
+    public long fillRefBitmap10916 = 10916;
+    public long fillRefBitmap10917 = 10917;
+    public long fillRefBitmap10918 = 10918;
+    public long fillRefBitmap10919 = 10919;
+    public long fillRefBitmap10920 = 10920;
+    public long fillRefBitmap10921 = 10921;
+    public long fillRefBitmap10922 = 10922;
+    public long fillRefBitmap10923 = 10923;
+    public long fillRefBitmap10924 = 10924;
+    public long fillRefBitmap10925 = 10925;
+    public long fillRefBitmap10926 = 10926;
+    public long fillRefBitmap10927 = 10927;
+    public long fillRefBitmap10928 = 10928;
+    public long fillRefBitmap10929 = 10929;
+    public long fillRefBitmap10930 = 10930;
+    public long fillRefBitmap10931 = 10931;
+    public long fillRefBitmap10932 = 10932;
+    public long fillRefBitmap10933 = 10933;
+    public long fillRefBitmap10934 = 10934;
+    public long fillRefBitmap10935 = 10935;
+    public long fillRefBitmap10936 = 10936;
+    public long fillRefBitmap10937 = 10937;
+    public long fillRefBitmap10938 = 10938;
+    public long fillRefBitmap10939 = 10939;
+    public long fillRefBitmap10940 = 10940;
+    public long fillRefBitmap10941 = 10941;
+    public long fillRefBitmap10942 = 10942;
+    public long fillRefBitmap10943 = 10943;
+    public long fillRefBitmap10944 = 10944;
+    public long fillRefBitmap10945 = 10945;
+    public long fillRefBitmap10946 = 10946;
+    public long fillRefBitmap10947 = 10947;
+    public long fillRefBitmap10948 = 10948;
+    public long fillRefBitmap10949 = 10949;
+    public long fillRefBitmap10950 = 10950;
+    public long fillRefBitmap10951 = 10951;
+    public long fillRefBitmap10952 = 10952;
+    public long fillRefBitmap10953 = 10953;
+    public long fillRefBitmap10954 = 10954;
+    public long fillRefBitmap10955 = 10955;
+    public long fillRefBitmap10956 = 10956;
+    public long fillRefBitmap10957 = 10957;
+    public long fillRefBitmap10958 = 10958;
+    public long fillRefBitmap10959 = 10959;
+    public long fillRefBitmap10960 = 10960;
+    public long fillRefBitmap10961 = 10961;
+    public long fillRefBitmap10962 = 10962;
+    public long fillRefBitmap10963 = 10963;
+    public long fillRefBitmap10964 = 10964;
+    public long fillRefBitmap10965 = 10965;
+    public long fillRefBitmap10966 = 10966;
+    public long fillRefBitmap10967 = 10967;
+    public long fillRefBitmap10968 = 10968;
+    public long fillRefBitmap10969 = 10969;
+    public long fillRefBitmap10970 = 10970;
+    public long fillRefBitmap10971 = 10971;
+    public long fillRefBitmap10972 = 10972;
+    public long fillRefBitmap10973 = 10973;
+    public long fillRefBitmap10974 = 10974;
+    public long fillRefBitmap10975 = 10975;
+    public long fillRefBitmap10976 = 10976;
+    public long fillRefBitmap10977 = 10977;
+    public long fillRefBitmap10978 = 10978;
+    public long fillRefBitmap10979 = 10979;
+    public long fillRefBitmap10980 = 10980;
+    public long fillRefBitmap10981 = 10981;
+    public long fillRefBitmap10982 = 10982;
+    public long fillRefBitmap10983 = 10983;
+    public long fillRefBitmap10984 = 10984;
+    public long fillRefBitmap10985 = 10985;
+    public long fillRefBitmap10986 = 10986;
+    public long fillRefBitmap10987 = 10987;
+    public long fillRefBitmap10988 = 10988;
+    public long fillRefBitmap10989 = 10989;
+    public long fillRefBitmap10990 = 10990;
+    public long fillRefBitmap10991 = 10991;
+    public long fillRefBitmap10992 = 10992;
+    public long fillRefBitmap10993 = 10993;
+    public long fillRefBitmap10994 = 10994;
+    public long fillRefBitmap10995 = 10995;
+    public long fillRefBitmap10996 = 10996;
+    public long fillRefBitmap10997 = 10997;
+    public long fillRefBitmap10998 = 10998;
+    public long fillRefBitmap10999 = 10999;
+    public long fillRefBitmap11000 = 11000;
+    public long fillRefBitmap11001 = 11001;
+    public long fillRefBitmap11002 = 11002;
+    public long fillRefBitmap11003 = 11003;
+    public long fillRefBitmap11004 = 11004;
+    public long fillRefBitmap11005 = 11005;
+    public long fillRefBitmap11006 = 11006;
+    public long fillRefBitmap11007 = 11007;
+    public long fillRefBitmap11008 = 11008;
+    public long fillRefBitmap11009 = 11009;
+    public long fillRefBitmap11010 = 11010;
+    public long fillRefBitmap11011 = 11011;
+    public long fillRefBitmap11012 = 11012;
+    public long fillRefBitmap11013 = 11013;
+    public long fillRefBitmap11014 = 11014;
+    public long fillRefBitmap11015 = 11015;
+    public long fillRefBitmap11016 = 11016;
+    public long fillRefBitmap11017 = 11017;
+    public long fillRefBitmap11018 = 11018;
+    public long fillRefBitmap11019 = 11019;
+    public long fillRefBitmap11020 = 11020;
+    public long fillRefBitmap11021 = 11021;
+    public long fillRefBitmap11022 = 11022;
+    public long fillRefBitmap11023 = 11023;
+    public long fillRefBitmap11024 = 11024;
+    public long fillRefBitmap11025 = 11025;
+    public long fillRefBitmap11026 = 11026;
+    public long fillRefBitmap11027 = 11027;
+    public long fillRefBitmap11028 = 11028;
+    public long fillRefBitmap11029 = 11029;
+    public long fillRefBitmap11030 = 11030;
+    public long fillRefBitmap11031 = 11031;
+    public long fillRefBitmap11032 = 11032;
+    public long fillRefBitmap11033 = 11033;
+    public long fillRefBitmap11034 = 11034;
+    public long fillRefBitmap11035 = 11035;
+    public long fillRefBitmap11036 = 11036;
+    public long fillRefBitmap11037 = 11037;
+    public long fillRefBitmap11038 = 11038;
+    public long fillRefBitmap11039 = 11039;
+    public long fillRefBitmap11040 = 11040;
+    public long fillRefBitmap11041 = 11041;
+    public long fillRefBitmap11042 = 11042;
+    public long fillRefBitmap11043 = 11043;
+    public long fillRefBitmap11044 = 11044;
+    public long fillRefBitmap11045 = 11045;
+    public long fillRefBitmap11046 = 11046;
+    public long fillRefBitmap11047 = 11047;
+    public long fillRefBitmap11048 = 11048;
+    public long fillRefBitmap11049 = 11049;
+    public long fillRefBitmap11050 = 11050;
+    public long fillRefBitmap11051 = 11051;
+    public long fillRefBitmap11052 = 11052;
+    public long fillRefBitmap11053 = 11053;
+    public long fillRefBitmap11054 = 11054;
+    public long fillRefBitmap11055 = 11055;
+    public long fillRefBitmap11056 = 11056;
+    public long fillRefBitmap11057 = 11057;
+    public long fillRefBitmap11058 = 11058;
+    public long fillRefBitmap11059 = 11059;
+    public long fillRefBitmap11060 = 11060;
+    public long fillRefBitmap11061 = 11061;
+    public long fillRefBitmap11062 = 11062;
+    public long fillRefBitmap11063 = 11063;
+    public long fillRefBitmap11064 = 11064;
+    public long fillRefBitmap11065 = 11065;
+    public long fillRefBitmap11066 = 11066;
+    public long fillRefBitmap11067 = 11067;
+    public long fillRefBitmap11068 = 11068;
+    public long fillRefBitmap11069 = 11069;
+    public long fillRefBitmap11070 = 11070;
+    public long fillRefBitmap11071 = 11071;
+    public long fillRefBitmap11072 = 11072;
+    public long fillRefBitmap11073 = 11073;
+    public long fillRefBitmap11074 = 11074;
+    public long fillRefBitmap11075 = 11075;
+    public long fillRefBitmap11076 = 11076;
+    public long fillRefBitmap11077 = 11077;
+    public long fillRefBitmap11078 = 11078;
+    public long fillRefBitmap11079 = 11079;
+    public long fillRefBitmap11080 = 11080;
+    public long fillRefBitmap11081 = 11081;
+    public long fillRefBitmap11082 = 11082;
+    public long fillRefBitmap11083 = 11083;
+    public long fillRefBitmap11084 = 11084;
+    public long fillRefBitmap11085 = 11085;
+    public long fillRefBitmap11086 = 11086;
+    public long fillRefBitmap11087 = 11087;
+    public long fillRefBitmap11088 = 11088;
+    public long fillRefBitmap11089 = 11089;
+    public long fillRefBitmap11090 = 11090;
+    public long fillRefBitmap11091 = 11091;
+    public long fillRefBitmap11092 = 11092;
+    public long fillRefBitmap11093 = 11093;
+    public long fillRefBitmap11094 = 11094;
+    public long fillRefBitmap11095 = 11095;
+    public long fillRefBitmap11096 = 11096;
+    public long fillRefBitmap11097 = 11097;
+    public long fillRefBitmap11098 = 11098;
+    public long fillRefBitmap11099 = 11099;
+    public long fillRefBitmap11100 = 11100;
+    public long fillRefBitmap11101 = 11101;
+    public long fillRefBitmap11102 = 11102;
+    public long fillRefBitmap11103 = 11103;
+    public long fillRefBitmap11104 = 11104;
+    public long fillRefBitmap11105 = 11105;
+    public long fillRefBitmap11106 = 11106;
+    public long fillRefBitmap11107 = 11107;
+    public long fillRefBitmap11108 = 11108;
+    public long fillRefBitmap11109 = 11109;
+    public long fillRefBitmap11110 = 11110;
+    public long fillRefBitmap11111 = 11111;
+    public long fillRefBitmap11112 = 11112;
+    public long fillRefBitmap11113 = 11113;
+    public long fillRefBitmap11114 = 11114;
+    public long fillRefBitmap11115 = 11115;
+    public long fillRefBitmap11116 = 11116;
+    public long fillRefBitmap11117 = 11117;
+    public long fillRefBitmap11118 = 11118;
+    public long fillRefBitmap11119 = 11119;
+    public long fillRefBitmap11120 = 11120;
+    public long fillRefBitmap11121 = 11121;
+    public long fillRefBitmap11122 = 11122;
+    public long fillRefBitmap11123 = 11123;
+    public long fillRefBitmap11124 = 11124;
+    public long fillRefBitmap11125 = 11125;
+    public long fillRefBitmap11126 = 11126;
+    public long fillRefBitmap11127 = 11127;
+    public long fillRefBitmap11128 = 11128;
+    public long fillRefBitmap11129 = 11129;
+    public long fillRefBitmap11130 = 11130;
+    public long fillRefBitmap11131 = 11131;
+    public long fillRefBitmap11132 = 11132;
+    public long fillRefBitmap11133 = 11133;
+    public long fillRefBitmap11134 = 11134;
+    public long fillRefBitmap11135 = 11135;
+    public long fillRefBitmap11136 = 11136;
+    public long fillRefBitmap11137 = 11137;
+    public long fillRefBitmap11138 = 11138;
+    public long fillRefBitmap11139 = 11139;
+    public long fillRefBitmap11140 = 11140;
+    public long fillRefBitmap11141 = 11141;
+    public long fillRefBitmap11142 = 11142;
+    public long fillRefBitmap11143 = 11143;
+    public long fillRefBitmap11144 = 11144;
+    public long fillRefBitmap11145 = 11145;
+    public long fillRefBitmap11146 = 11146;
+    public long fillRefBitmap11147 = 11147;
+    public long fillRefBitmap11148 = 11148;
+    public long fillRefBitmap11149 = 11149;
+    public long fillRefBitmap11150 = 11150;
+    public long fillRefBitmap11151 = 11151;
+    public long fillRefBitmap11152 = 11152;
+    public long fillRefBitmap11153 = 11153;
+    public long fillRefBitmap11154 = 11154;
+    public long fillRefBitmap11155 = 11155;
+    public long fillRefBitmap11156 = 11156;
+    public long fillRefBitmap11157 = 11157;
+    public long fillRefBitmap11158 = 11158;
+    public long fillRefBitmap11159 = 11159;
+    public long fillRefBitmap11160 = 11160;
+    public long fillRefBitmap11161 = 11161;
+    public long fillRefBitmap11162 = 11162;
+    public long fillRefBitmap11163 = 11163;
+    public long fillRefBitmap11164 = 11164;
+    public long fillRefBitmap11165 = 11165;
+    public long fillRefBitmap11166 = 11166;
+    public long fillRefBitmap11167 = 11167;
+    public long fillRefBitmap11168 = 11168;
+    public long fillRefBitmap11169 = 11169;
+    public long fillRefBitmap11170 = 11170;
+    public long fillRefBitmap11171 = 11171;
+    public long fillRefBitmap11172 = 11172;
+    public long fillRefBitmap11173 = 11173;
+    public long fillRefBitmap11174 = 11174;
+    public long fillRefBitmap11175 = 11175;
+    public long fillRefBitmap11176 = 11176;
+    public long fillRefBitmap11177 = 11177;
+    public long fillRefBitmap11178 = 11178;
+    public long fillRefBitmap11179 = 11179;
+    public long fillRefBitmap11180 = 11180;
+    public long fillRefBitmap11181 = 11181;
+    public long fillRefBitmap11182 = 11182;
+    public long fillRefBitmap11183 = 11183;
+    public long fillRefBitmap11184 = 11184;
+    public long fillRefBitmap11185 = 11185;
+    public long fillRefBitmap11186 = 11186;
+    public long fillRefBitmap11187 = 11187;
+    public long fillRefBitmap11188 = 11188;
+    public long fillRefBitmap11189 = 11189;
+    public long fillRefBitmap11190 = 11190;
+    public long fillRefBitmap11191 = 11191;
+    public long fillRefBitmap11192 = 11192;
+    public long fillRefBitmap11193 = 11193;
+    public long fillRefBitmap11194 = 11194;
+    public long fillRefBitmap11195 = 11195;
+    public long fillRefBitmap11196 = 11196;
+    public long fillRefBitmap11197 = 11197;
+    public long fillRefBitmap11198 = 11198;
+    public long fillRefBitmap11199 = 11199;
+    public long fillRefBitmap11200 = 11200;
+    public long fillRefBitmap11201 = 11201;
+    public long fillRefBitmap11202 = 11202;
+    public long fillRefBitmap11203 = 11203;
+    public long fillRefBitmap11204 = 11204;
+    public long fillRefBitmap11205 = 11205;
+    public long fillRefBitmap11206 = 11206;
+    public long fillRefBitmap11207 = 11207;
+    public long fillRefBitmap11208 = 11208;
+    public long fillRefBitmap11209 = 11209;
+    public long fillRefBitmap11210 = 11210;
+    public long fillRefBitmap11211 = 11211;
+    public long fillRefBitmap11212 = 11212;
+    public long fillRefBitmap11213 = 11213;
+    public long fillRefBitmap11214 = 11214;
+    public long fillRefBitmap11215 = 11215;
+    public long fillRefBitmap11216 = 11216;
+    public long fillRefBitmap11217 = 11217;
+    public long fillRefBitmap11218 = 11218;
+    public long fillRefBitmap11219 = 11219;
+    public long fillRefBitmap11220 = 11220;
+    public long fillRefBitmap11221 = 11221;
+    public long fillRefBitmap11222 = 11222;
+    public long fillRefBitmap11223 = 11223;
+    public long fillRefBitmap11224 = 11224;
+    public long fillRefBitmap11225 = 11225;
+    public long fillRefBitmap11226 = 11226;
+    public long fillRefBitmap11227 = 11227;
+    public long fillRefBitmap11228 = 11228;
+    public long fillRefBitmap11229 = 11229;
+    public long fillRefBitmap11230 = 11230;
+    public long fillRefBitmap11231 = 11231;
+    public long fillRefBitmap11232 = 11232;
+    public long fillRefBitmap11233 = 11233;
+    public long fillRefBitmap11234 = 11234;
+    public long fillRefBitmap11235 = 11235;
+    public long fillRefBitmap11236 = 11236;
+    public long fillRefBitmap11237 = 11237;
+    public long fillRefBitmap11238 = 11238;
+    public long fillRefBitmap11239 = 11239;
+    public long fillRefBitmap11240 = 11240;
+    public long fillRefBitmap11241 = 11241;
+    public long fillRefBitmap11242 = 11242;
+    public long fillRefBitmap11243 = 11243;
+    public long fillRefBitmap11244 = 11244;
+    public long fillRefBitmap11245 = 11245;
+    public long fillRefBitmap11246 = 11246;
+    public long fillRefBitmap11247 = 11247;
+    public long fillRefBitmap11248 = 11248;
+    public long fillRefBitmap11249 = 11249;
+    public long fillRefBitmap11250 = 11250;
+    public long fillRefBitmap11251 = 11251;
+    public long fillRefBitmap11252 = 11252;
+    public long fillRefBitmap11253 = 11253;
+    public long fillRefBitmap11254 = 11254;
+    public long fillRefBitmap11255 = 11255;
+    public long fillRefBitmap11256 = 11256;
+    public long fillRefBitmap11257 = 11257;
+    public long fillRefBitmap11258 = 11258;
+    public long fillRefBitmap11259 = 11259;
+    public long fillRefBitmap11260 = 11260;
+    public long fillRefBitmap11261 = 11261;
+    public long fillRefBitmap11262 = 11262;
+    public long fillRefBitmap11263 = 11263;
+    public long fillRefBitmap11264 = 11264;
+    public long fillRefBitmap11265 = 11265;
+    public long fillRefBitmap11266 = 11266;
+    public long fillRefBitmap11267 = 11267;
+    public long fillRefBitmap11268 = 11268;
+    public long fillRefBitmap11269 = 11269;
+    public long fillRefBitmap11270 = 11270;
+    public long fillRefBitmap11271 = 11271;
+    public long fillRefBitmap11272 = 11272;
+    public long fillRefBitmap11273 = 11273;
+    public long fillRefBitmap11274 = 11274;
+    public long fillRefBitmap11275 = 11275;
+    public long fillRefBitmap11276 = 11276;
+    public long fillRefBitmap11277 = 11277;
+    public long fillRefBitmap11278 = 11278;
+    public long fillRefBitmap11279 = 11279;
+    public long fillRefBitmap11280 = 11280;
+    public long fillRefBitmap11281 = 11281;
+    public long fillRefBitmap11282 = 11282;
+    public long fillRefBitmap11283 = 11283;
+    public long fillRefBitmap11284 = 11284;
+    public long fillRefBitmap11285 = 11285;
+    public long fillRefBitmap11286 = 11286;
+    public long fillRefBitmap11287 = 11287;
+    public long fillRefBitmap11288 = 11288;
+    public long fillRefBitmap11289 = 11289;
+    public long fillRefBitmap11290 = 11290;
+    public long fillRefBitmap11291 = 11291;
+    public long fillRefBitmap11292 = 11292;
+    public long fillRefBitmap11293 = 11293;
+    public long fillRefBitmap11294 = 11294;
+    public long fillRefBitmap11295 = 11295;
+    public long fillRefBitmap11296 = 11296;
+    public long fillRefBitmap11297 = 11297;
+    public long fillRefBitmap11298 = 11298;
+    public long fillRefBitmap11299 = 11299;
+    public long fillRefBitmap11300 = 11300;
+    public long fillRefBitmap11301 = 11301;
+    public long fillRefBitmap11302 = 11302;
+    public long fillRefBitmap11303 = 11303;
+    public long fillRefBitmap11304 = 11304;
+    public long fillRefBitmap11305 = 11305;
+    public long fillRefBitmap11306 = 11306;
+    public long fillRefBitmap11307 = 11307;
+    public long fillRefBitmap11308 = 11308;
+    public long fillRefBitmap11309 = 11309;
+    public long fillRefBitmap11310 = 11310;
+    public long fillRefBitmap11311 = 11311;
+    public long fillRefBitmap11312 = 11312;
+    public long fillRefBitmap11313 = 11313;
+    public long fillRefBitmap11314 = 11314;
+    public long fillRefBitmap11315 = 11315;
+    public long fillRefBitmap11316 = 11316;
+    public long fillRefBitmap11317 = 11317;
+    public long fillRefBitmap11318 = 11318;
+    public long fillRefBitmap11319 = 11319;
+    public long fillRefBitmap11320 = 11320;
+    public long fillRefBitmap11321 = 11321;
+    public long fillRefBitmap11322 = 11322;
+    public long fillRefBitmap11323 = 11323;
+    public long fillRefBitmap11324 = 11324;
+    public long fillRefBitmap11325 = 11325;
+    public long fillRefBitmap11326 = 11326;
+    public long fillRefBitmap11327 = 11327;
+    public long fillRefBitmap11328 = 11328;
+    public long fillRefBitmap11329 = 11329;
+    public long fillRefBitmap11330 = 11330;
+    public long fillRefBitmap11331 = 11331;
+    public long fillRefBitmap11332 = 11332;
+    public long fillRefBitmap11333 = 11333;
+    public long fillRefBitmap11334 = 11334;
+    public long fillRefBitmap11335 = 11335;
+    public long fillRefBitmap11336 = 11336;
+    public long fillRefBitmap11337 = 11337;
+    public long fillRefBitmap11338 = 11338;
+    public long fillRefBitmap11339 = 11339;
+    public long fillRefBitmap11340 = 11340;
+    public long fillRefBitmap11341 = 11341;
+    public long fillRefBitmap11342 = 11342;
+    public long fillRefBitmap11343 = 11343;
+    public long fillRefBitmap11344 = 11344;
+    public long fillRefBitmap11345 = 11345;
+    public long fillRefBitmap11346 = 11346;
+    public long fillRefBitmap11347 = 11347;
+    public long fillRefBitmap11348 = 11348;
+    public long fillRefBitmap11349 = 11349;
+    public long fillRefBitmap11350 = 11350;
+    public long fillRefBitmap11351 = 11351;
+    public long fillRefBitmap11352 = 11352;
+    public long fillRefBitmap11353 = 11353;
+    public long fillRefBitmap11354 = 11354;
+    public long fillRefBitmap11355 = 11355;
+    public long fillRefBitmap11356 = 11356;
+    public long fillRefBitmap11357 = 11357;
+    public long fillRefBitmap11358 = 11358;
+    public long fillRefBitmap11359 = 11359;
+    public long fillRefBitmap11360 = 11360;
+    public long fillRefBitmap11361 = 11361;
+    public long fillRefBitmap11362 = 11362;
+    public long fillRefBitmap11363 = 11363;
+    public long fillRefBitmap11364 = 11364;
+    public long fillRefBitmap11365 = 11365;
+    public long fillRefBitmap11366 = 11366;
+    public long fillRefBitmap11367 = 11367;
+    public long fillRefBitmap11368 = 11368;
+    public long fillRefBitmap11369 = 11369;
+    public long fillRefBitmap11370 = 11370;
+    public long fillRefBitmap11371 = 11371;
+    public long fillRefBitmap11372 = 11372;
+    public long fillRefBitmap11373 = 11373;
+    public long fillRefBitmap11374 = 11374;
+    public long fillRefBitmap11375 = 11375;
+    public long fillRefBitmap11376 = 11376;
+    public long fillRefBitmap11377 = 11377;
+    public long fillRefBitmap11378 = 11378;
+    public long fillRefBitmap11379 = 11379;
+    public long fillRefBitmap11380 = 11380;
+    public long fillRefBitmap11381 = 11381;
+    public long fillRefBitmap11382 = 11382;
+    public long fillRefBitmap11383 = 11383;
+    public long fillRefBitmap11384 = 11384;
+    public long fillRefBitmap11385 = 11385;
+    public long fillRefBitmap11386 = 11386;
+    public long fillRefBitmap11387 = 11387;
+    public long fillRefBitmap11388 = 11388;
+    public long fillRefBitmap11389 = 11389;
+    public long fillRefBitmap11390 = 11390;
+    public long fillRefBitmap11391 = 11391;
+    public long fillRefBitmap11392 = 11392;
+    public long fillRefBitmap11393 = 11393;
+    public long fillRefBitmap11394 = 11394;
+    public long fillRefBitmap11395 = 11395;
+    public long fillRefBitmap11396 = 11396;
+    public long fillRefBitmap11397 = 11397;
+    public long fillRefBitmap11398 = 11398;
+    public long fillRefBitmap11399 = 11399;
+    public long fillRefBitmap11400 = 11400;
+    public long fillRefBitmap11401 = 11401;
+    public long fillRefBitmap11402 = 11402;
+    public long fillRefBitmap11403 = 11403;
+    public long fillRefBitmap11404 = 11404;
+    public long fillRefBitmap11405 = 11405;
+    public long fillRefBitmap11406 = 11406;
+    public long fillRefBitmap11407 = 11407;
+    public long fillRefBitmap11408 = 11408;
+    public long fillRefBitmap11409 = 11409;
+    public long fillRefBitmap11410 = 11410;
+    public long fillRefBitmap11411 = 11411;
+    public long fillRefBitmap11412 = 11412;
+    public long fillRefBitmap11413 = 11413;
+    public long fillRefBitmap11414 = 11414;
+    public long fillRefBitmap11415 = 11415;
+    public long fillRefBitmap11416 = 11416;
+    public long fillRefBitmap11417 = 11417;
+    public long fillRefBitmap11418 = 11418;
+    public long fillRefBitmap11419 = 11419;
+    public long fillRefBitmap11420 = 11420;
+    public long fillRefBitmap11421 = 11421;
+    public long fillRefBitmap11422 = 11422;
+    public long fillRefBitmap11423 = 11423;
+    public long fillRefBitmap11424 = 11424;
+    public long fillRefBitmap11425 = 11425;
+    public long fillRefBitmap11426 = 11426;
+    public long fillRefBitmap11427 = 11427;
+    public long fillRefBitmap11428 = 11428;
+    public long fillRefBitmap11429 = 11429;
+    public long fillRefBitmap11430 = 11430;
+    public long fillRefBitmap11431 = 11431;
+    public long fillRefBitmap11432 = 11432;
+    public long fillRefBitmap11433 = 11433;
+    public long fillRefBitmap11434 = 11434;
+    public long fillRefBitmap11435 = 11435;
+    public long fillRefBitmap11436 = 11436;
+    public long fillRefBitmap11437 = 11437;
+    public long fillRefBitmap11438 = 11438;
+    public long fillRefBitmap11439 = 11439;
+    public long fillRefBitmap11440 = 11440;
+    public long fillRefBitmap11441 = 11441;
+    public long fillRefBitmap11442 = 11442;
+    public long fillRefBitmap11443 = 11443;
+    public long fillRefBitmap11444 = 11444;
+    public long fillRefBitmap11445 = 11445;
+    public long fillRefBitmap11446 = 11446;
+    public long fillRefBitmap11447 = 11447;
+    public long fillRefBitmap11448 = 11448;
+    public long fillRefBitmap11449 = 11449;
+    public long fillRefBitmap11450 = 11450;
+    public long fillRefBitmap11451 = 11451;
+    public long fillRefBitmap11452 = 11452;
+    public long fillRefBitmap11453 = 11453;
+    public long fillRefBitmap11454 = 11454;
+    public long fillRefBitmap11455 = 11455;
+    public long fillRefBitmap11456 = 11456;
+    public long fillRefBitmap11457 = 11457;
+    public long fillRefBitmap11458 = 11458;
+    public long fillRefBitmap11459 = 11459;
+    public long fillRefBitmap11460 = 11460;
+    public long fillRefBitmap11461 = 11461;
+    public long fillRefBitmap11462 = 11462;
+    public long fillRefBitmap11463 = 11463;
+    public long fillRefBitmap11464 = 11464;
+    public long fillRefBitmap11465 = 11465;
+    public long fillRefBitmap11466 = 11466;
+    public long fillRefBitmap11467 = 11467;
+    public long fillRefBitmap11468 = 11468;
+    public long fillRefBitmap11469 = 11469;
+    public long fillRefBitmap11470 = 11470;
+    public long fillRefBitmap11471 = 11471;
+    public long fillRefBitmap11472 = 11472;
+    public long fillRefBitmap11473 = 11473;
+    public long fillRefBitmap11474 = 11474;
+    public long fillRefBitmap11475 = 11475;
+    public long fillRefBitmap11476 = 11476;
+    public long fillRefBitmap11477 = 11477;
+    public long fillRefBitmap11478 = 11478;
+    public long fillRefBitmap11479 = 11479;
+    public long fillRefBitmap11480 = 11480;
+    public long fillRefBitmap11481 = 11481;
+    public long fillRefBitmap11482 = 11482;
+    public long fillRefBitmap11483 = 11483;
+    public long fillRefBitmap11484 = 11484;
+    public long fillRefBitmap11485 = 11485;
+    public long fillRefBitmap11486 = 11486;
+    public long fillRefBitmap11487 = 11487;
+    public long fillRefBitmap11488 = 11488;
+    public long fillRefBitmap11489 = 11489;
+    public long fillRefBitmap11490 = 11490;
+    public long fillRefBitmap11491 = 11491;
+    public long fillRefBitmap11492 = 11492;
+    public long fillRefBitmap11493 = 11493;
+    public long fillRefBitmap11494 = 11494;
+    public long fillRefBitmap11495 = 11495;
+    public long fillRefBitmap11496 = 11496;
+    public long fillRefBitmap11497 = 11497;
+    public long fillRefBitmap11498 = 11498;
+    public long fillRefBitmap11499 = 11499;
+    public long fillRefBitmap11500 = 11500;
+    public long fillRefBitmap11501 = 11501;
+    public long fillRefBitmap11502 = 11502;
+    public long fillRefBitmap11503 = 11503;
+    public long fillRefBitmap11504 = 11504;
+    public long fillRefBitmap11505 = 11505;
+    public long fillRefBitmap11506 = 11506;
+    public long fillRefBitmap11507 = 11507;
+    public long fillRefBitmap11508 = 11508;
+    public long fillRefBitmap11509 = 11509;
+    public long fillRefBitmap11510 = 11510;
+    public long fillRefBitmap11511 = 11511;
+    public long fillRefBitmap11512 = 11512;
+    public long fillRefBitmap11513 = 11513;
+    public long fillRefBitmap11514 = 11514;
+    public long fillRefBitmap11515 = 11515;
+    public long fillRefBitmap11516 = 11516;
+    public long fillRefBitmap11517 = 11517;
+    public long fillRefBitmap11518 = 11518;
+    public long fillRefBitmap11519 = 11519;
+    public long fillRefBitmap11520 = 11520;
+    public long fillRefBitmap11521 = 11521;
+    public long fillRefBitmap11522 = 11522;
+    public long fillRefBitmap11523 = 11523;
+    public long fillRefBitmap11524 = 11524;
+    public long fillRefBitmap11525 = 11525;
+    public long fillRefBitmap11526 = 11526;
+    public long fillRefBitmap11527 = 11527;
+    public long fillRefBitmap11528 = 11528;
+    public long fillRefBitmap11529 = 11529;
+    public long fillRefBitmap11530 = 11530;
+    public long fillRefBitmap11531 = 11531;
+    public long fillRefBitmap11532 = 11532;
+    public long fillRefBitmap11533 = 11533;
+    public long fillRefBitmap11534 = 11534;
+    public long fillRefBitmap11535 = 11535;
+    public long fillRefBitmap11536 = 11536;
+    public long fillRefBitmap11537 = 11537;
+    public long fillRefBitmap11538 = 11538;
+    public long fillRefBitmap11539 = 11539;
+    public long fillRefBitmap11540 = 11540;
+    public long fillRefBitmap11541 = 11541;
+    public long fillRefBitmap11542 = 11542;
+    public long fillRefBitmap11543 = 11543;
+    public long fillRefBitmap11544 = 11544;
+    public long fillRefBitmap11545 = 11545;
+    public long fillRefBitmap11546 = 11546;
+    public long fillRefBitmap11547 = 11547;
+    public long fillRefBitmap11548 = 11548;
+    public long fillRefBitmap11549 = 11549;
+    public long fillRefBitmap11550 = 11550;
+    public long fillRefBitmap11551 = 11551;
+    public long fillRefBitmap11552 = 11552;
+    public long fillRefBitmap11553 = 11553;
+    public long fillRefBitmap11554 = 11554;
+    public long fillRefBitmap11555 = 11555;
+    public long fillRefBitmap11556 = 11556;
+    public long fillRefBitmap11557 = 11557;
+    public long fillRefBitmap11558 = 11558;
+    public long fillRefBitmap11559 = 11559;
+    public long fillRefBitmap11560 = 11560;
+    public long fillRefBitmap11561 = 11561;
+    public long fillRefBitmap11562 = 11562;
+    public long fillRefBitmap11563 = 11563;
+    public long fillRefBitmap11564 = 11564;
+    public long fillRefBitmap11565 = 11565;
+    public long fillRefBitmap11566 = 11566;
+    public long fillRefBitmap11567 = 11567;
+    public long fillRefBitmap11568 = 11568;
+    public long fillRefBitmap11569 = 11569;
+    public long fillRefBitmap11570 = 11570;
+    public long fillRefBitmap11571 = 11571;
+    public long fillRefBitmap11572 = 11572;
+    public long fillRefBitmap11573 = 11573;
+    public long fillRefBitmap11574 = 11574;
+    public long fillRefBitmap11575 = 11575;
+    public long fillRefBitmap11576 = 11576;
+    public long fillRefBitmap11577 = 11577;
+    public long fillRefBitmap11578 = 11578;
+    public long fillRefBitmap11579 = 11579;
+    public long fillRefBitmap11580 = 11580;
+    public long fillRefBitmap11581 = 11581;
+    public long fillRefBitmap11582 = 11582;
+    public long fillRefBitmap11583 = 11583;
+    public long fillRefBitmap11584 = 11584;
+    public long fillRefBitmap11585 = 11585;
+    public long fillRefBitmap11586 = 11586;
+    public long fillRefBitmap11587 = 11587;
+    public long fillRefBitmap11588 = 11588;
+    public long fillRefBitmap11589 = 11589;
+    public long fillRefBitmap11590 = 11590;
+    public long fillRefBitmap11591 = 11591;
+    public long fillRefBitmap11592 = 11592;
+    public long fillRefBitmap11593 = 11593;
+    public long fillRefBitmap11594 = 11594;
+    public long fillRefBitmap11595 = 11595;
+    public long fillRefBitmap11596 = 11596;
+    public long fillRefBitmap11597 = 11597;
+    public long fillRefBitmap11598 = 11598;
+    public long fillRefBitmap11599 = 11599;
+    public long fillRefBitmap11600 = 11600;
+    public long fillRefBitmap11601 = 11601;
+    public long fillRefBitmap11602 = 11602;
+    public long fillRefBitmap11603 = 11603;
+    public long fillRefBitmap11604 = 11604;
+    public long fillRefBitmap11605 = 11605;
+    public long fillRefBitmap11606 = 11606;
+    public long fillRefBitmap11607 = 11607;
+    public long fillRefBitmap11608 = 11608;
+    public long fillRefBitmap11609 = 11609;
+    public long fillRefBitmap11610 = 11610;
+    public long fillRefBitmap11611 = 11611;
+    public long fillRefBitmap11612 = 11612;
+    public long fillRefBitmap11613 = 11613;
+    public long fillRefBitmap11614 = 11614;
+    public long fillRefBitmap11615 = 11615;
+    public long fillRefBitmap11616 = 11616;
+    public long fillRefBitmap11617 = 11617;
+    public long fillRefBitmap11618 = 11618;
+    public long fillRefBitmap11619 = 11619;
+    public long fillRefBitmap11620 = 11620;
+    public long fillRefBitmap11621 = 11621;
+    public long fillRefBitmap11622 = 11622;
+    public long fillRefBitmap11623 = 11623;
+    public long fillRefBitmap11624 = 11624;
+    public long fillRefBitmap11625 = 11625;
+    public long fillRefBitmap11626 = 11626;
+    public long fillRefBitmap11627 = 11627;
+    public long fillRefBitmap11628 = 11628;
+    public long fillRefBitmap11629 = 11629;
+    public long fillRefBitmap11630 = 11630;
+    public long fillRefBitmap11631 = 11631;
+    public long fillRefBitmap11632 = 11632;
+    public long fillRefBitmap11633 = 11633;
+    public long fillRefBitmap11634 = 11634;
+    public long fillRefBitmap11635 = 11635;
+    public long fillRefBitmap11636 = 11636;
+    public long fillRefBitmap11637 = 11637;
+    public long fillRefBitmap11638 = 11638;
+    public long fillRefBitmap11639 = 11639;
+    public long fillRefBitmap11640 = 11640;
+    public long fillRefBitmap11641 = 11641;
+    public long fillRefBitmap11642 = 11642;
+    public long fillRefBitmap11643 = 11643;
+    public long fillRefBitmap11644 = 11644;
+    public long fillRefBitmap11645 = 11645;
+    public long fillRefBitmap11646 = 11646;
+    public long fillRefBitmap11647 = 11647;
+    public long fillRefBitmap11648 = 11648;
+    public long fillRefBitmap11649 = 11649;
+    public long fillRefBitmap11650 = 11650;
+    public long fillRefBitmap11651 = 11651;
+    public long fillRefBitmap11652 = 11652;
+    public long fillRefBitmap11653 = 11653;
+    public long fillRefBitmap11654 = 11654;
+    public long fillRefBitmap11655 = 11655;
+    public long fillRefBitmap11656 = 11656;
+    public long fillRefBitmap11657 = 11657;
+    public long fillRefBitmap11658 = 11658;
+    public long fillRefBitmap11659 = 11659;
+    public long fillRefBitmap11660 = 11660;
+    public long fillRefBitmap11661 = 11661;
+    public long fillRefBitmap11662 = 11662;
+    public long fillRefBitmap11663 = 11663;
+    public long fillRefBitmap11664 = 11664;
+    public long fillRefBitmap11665 = 11665;
+    public long fillRefBitmap11666 = 11666;
+    public long fillRefBitmap11667 = 11667;
+    public long fillRefBitmap11668 = 11668;
+    public long fillRefBitmap11669 = 11669;
+    public long fillRefBitmap11670 = 11670;
+    public long fillRefBitmap11671 = 11671;
+    public long fillRefBitmap11672 = 11672;
+    public long fillRefBitmap11673 = 11673;
+    public long fillRefBitmap11674 = 11674;
+    public long fillRefBitmap11675 = 11675;
+    public long fillRefBitmap11676 = 11676;
+    public long fillRefBitmap11677 = 11677;
+    public long fillRefBitmap11678 = 11678;
+    public long fillRefBitmap11679 = 11679;
+    public long fillRefBitmap11680 = 11680;
+    public long fillRefBitmap11681 = 11681;
+    public long fillRefBitmap11682 = 11682;
+    public long fillRefBitmap11683 = 11683;
+    public long fillRefBitmap11684 = 11684;
+    public long fillRefBitmap11685 = 11685;
+    public long fillRefBitmap11686 = 11686;
+    public long fillRefBitmap11687 = 11687;
+    public long fillRefBitmap11688 = 11688;
+    public long fillRefBitmap11689 = 11689;
+    public long fillRefBitmap11690 = 11690;
+    public long fillRefBitmap11691 = 11691;
+    public long fillRefBitmap11692 = 11692;
+    public long fillRefBitmap11693 = 11693;
+    public long fillRefBitmap11694 = 11694;
+    public long fillRefBitmap11695 = 11695;
+    public long fillRefBitmap11696 = 11696;
+    public long fillRefBitmap11697 = 11697;
+    public long fillRefBitmap11698 = 11698;
+    public long fillRefBitmap11699 = 11699;
+    public long fillRefBitmap11700 = 11700;
+    public long fillRefBitmap11701 = 11701;
+    public long fillRefBitmap11702 = 11702;
+    public long fillRefBitmap11703 = 11703;
+    public long fillRefBitmap11704 = 11704;
+    public long fillRefBitmap11705 = 11705;
+    public long fillRefBitmap11706 = 11706;
+    public long fillRefBitmap11707 = 11707;
+    public long fillRefBitmap11708 = 11708;
+    public long fillRefBitmap11709 = 11709;
+    public long fillRefBitmap11710 = 11710;
+    public long fillRefBitmap11711 = 11711;
+    public long fillRefBitmap11712 = 11712;
+    public long fillRefBitmap11713 = 11713;
+    public long fillRefBitmap11714 = 11714;
+    public long fillRefBitmap11715 = 11715;
+    public long fillRefBitmap11716 = 11716;
+    public long fillRefBitmap11717 = 11717;
+    public long fillRefBitmap11718 = 11718;
+    public long fillRefBitmap11719 = 11719;
+    public long fillRefBitmap11720 = 11720;
+    public long fillRefBitmap11721 = 11721;
+    public long fillRefBitmap11722 = 11722;
+    public long fillRefBitmap11723 = 11723;
+    public long fillRefBitmap11724 = 11724;
+    public long fillRefBitmap11725 = 11725;
+    public long fillRefBitmap11726 = 11726;
+    public long fillRefBitmap11727 = 11727;
+    public long fillRefBitmap11728 = 11728;
+    public long fillRefBitmap11729 = 11729;
+    public long fillRefBitmap11730 = 11730;
+    public long fillRefBitmap11731 = 11731;
+    public long fillRefBitmap11732 = 11732;
+    public long fillRefBitmap11733 = 11733;
+    public long fillRefBitmap11734 = 11734;
+    public long fillRefBitmap11735 = 11735;
+    public long fillRefBitmap11736 = 11736;
+    public long fillRefBitmap11737 = 11737;
+    public long fillRefBitmap11738 = 11738;
+    public long fillRefBitmap11739 = 11739;
+    public long fillRefBitmap11740 = 11740;
+    public long fillRefBitmap11741 = 11741;
+    public long fillRefBitmap11742 = 11742;
+    public long fillRefBitmap11743 = 11743;
+    public long fillRefBitmap11744 = 11744;
+    public long fillRefBitmap11745 = 11745;
+    public long fillRefBitmap11746 = 11746;
+    public long fillRefBitmap11747 = 11747;
+    public long fillRefBitmap11748 = 11748;
+    public long fillRefBitmap11749 = 11749;
+    public long fillRefBitmap11750 = 11750;
+    public long fillRefBitmap11751 = 11751;
+    public long fillRefBitmap11752 = 11752;
+    public long fillRefBitmap11753 = 11753;
+    public long fillRefBitmap11754 = 11754;
+    public long fillRefBitmap11755 = 11755;
+    public long fillRefBitmap11756 = 11756;
+    public long fillRefBitmap11757 = 11757;
+    public long fillRefBitmap11758 = 11758;
+    public long fillRefBitmap11759 = 11759;
+    public long fillRefBitmap11760 = 11760;
+    public long fillRefBitmap11761 = 11761;
+    public long fillRefBitmap11762 = 11762;
+    public long fillRefBitmap11763 = 11763;
+    public long fillRefBitmap11764 = 11764;
+    public long fillRefBitmap11765 = 11765;
+    public long fillRefBitmap11766 = 11766;
+    public long fillRefBitmap11767 = 11767;
+    public long fillRefBitmap11768 = 11768;
+    public long fillRefBitmap11769 = 11769;
+    public long fillRefBitmap11770 = 11770;
+    public long fillRefBitmap11771 = 11771;
+    public long fillRefBitmap11772 = 11772;
+    public long fillRefBitmap11773 = 11773;
+    public long fillRefBitmap11774 = 11774;
+    public long fillRefBitmap11775 = 11775;
+    public long fillRefBitmap11776 = 11776;
+    public long fillRefBitmap11777 = 11777;
+    public long fillRefBitmap11778 = 11778;
+    public long fillRefBitmap11779 = 11779;
+    public long fillRefBitmap11780 = 11780;
+    public long fillRefBitmap11781 = 11781;
+    public long fillRefBitmap11782 = 11782;
+    public long fillRefBitmap11783 = 11783;
+    public long fillRefBitmap11784 = 11784;
+    public long fillRefBitmap11785 = 11785;
+    public long fillRefBitmap11786 = 11786;
+    public long fillRefBitmap11787 = 11787;
+    public long fillRefBitmap11788 = 11788;
+    public long fillRefBitmap11789 = 11789;
+    public long fillRefBitmap11790 = 11790;
+    public long fillRefBitmap11791 = 11791;
+    public long fillRefBitmap11792 = 11792;
+    public long fillRefBitmap11793 = 11793;
+    public long fillRefBitmap11794 = 11794;
+    public long fillRefBitmap11795 = 11795;
+    public long fillRefBitmap11796 = 11796;
+    public long fillRefBitmap11797 = 11797;
+    public long fillRefBitmap11798 = 11798;
+    public long fillRefBitmap11799 = 11799;
+    public long fillRefBitmap11800 = 11800;
+    public long fillRefBitmap11801 = 11801;
+    public long fillRefBitmap11802 = 11802;
+    public long fillRefBitmap11803 = 11803;
+    public long fillRefBitmap11804 = 11804;
+    public long fillRefBitmap11805 = 11805;
+    public long fillRefBitmap11806 = 11806;
+    public long fillRefBitmap11807 = 11807;
+    public long fillRefBitmap11808 = 11808;
+    public long fillRefBitmap11809 = 11809;
+    public long fillRefBitmap11810 = 11810;
+    public long fillRefBitmap11811 = 11811;
+    public long fillRefBitmap11812 = 11812;
+    public long fillRefBitmap11813 = 11813;
+    public long fillRefBitmap11814 = 11814;
+    public long fillRefBitmap11815 = 11815;
+    public long fillRefBitmap11816 = 11816;
+    public long fillRefBitmap11817 = 11817;
+    public long fillRefBitmap11818 = 11818;
+    public long fillRefBitmap11819 = 11819;
+    public long fillRefBitmap11820 = 11820;
+    public long fillRefBitmap11821 = 11821;
+    public long fillRefBitmap11822 = 11822;
+    public long fillRefBitmap11823 = 11823;
+    public long fillRefBitmap11824 = 11824;
+    public long fillRefBitmap11825 = 11825;
+    public long fillRefBitmap11826 = 11826;
+    public long fillRefBitmap11827 = 11827;
+    public long fillRefBitmap11828 = 11828;
+    public long fillRefBitmap11829 = 11829;
+    public long fillRefBitmap11830 = 11830;
+    public long fillRefBitmap11831 = 11831;
+    public long fillRefBitmap11832 = 11832;
+    public long fillRefBitmap11833 = 11833;
+    public long fillRefBitmap11834 = 11834;
+    public long fillRefBitmap11835 = 11835;
+    public long fillRefBitmap11836 = 11836;
+    public long fillRefBitmap11837 = 11837;
+    public long fillRefBitmap11838 = 11838;
+    public long fillRefBitmap11839 = 11839;
+    public long fillRefBitmap11840 = 11840;
+    public long fillRefBitmap11841 = 11841;
+    public long fillRefBitmap11842 = 11842;
+    public long fillRefBitmap11843 = 11843;
+    public long fillRefBitmap11844 = 11844;
+    public long fillRefBitmap11845 = 11845;
+    public long fillRefBitmap11846 = 11846;
+    public long fillRefBitmap11847 = 11847;
+    public long fillRefBitmap11848 = 11848;
+    public long fillRefBitmap11849 = 11849;
+    public long fillRefBitmap11850 = 11850;
+    public long fillRefBitmap11851 = 11851;
+    public long fillRefBitmap11852 = 11852;
+    public long fillRefBitmap11853 = 11853;
+    public long fillRefBitmap11854 = 11854;
+    public long fillRefBitmap11855 = 11855;
+    public long fillRefBitmap11856 = 11856;
+    public long fillRefBitmap11857 = 11857;
+    public long fillRefBitmap11858 = 11858;
+    public long fillRefBitmap11859 = 11859;
+    public long fillRefBitmap11860 = 11860;
+    public long fillRefBitmap11861 = 11861;
+    public long fillRefBitmap11862 = 11862;
+    public long fillRefBitmap11863 = 11863;
+    public long fillRefBitmap11864 = 11864;
+    public long fillRefBitmap11865 = 11865;
+    public long fillRefBitmap11866 = 11866;
+    public long fillRefBitmap11867 = 11867;
+    public long fillRefBitmap11868 = 11868;
+    public long fillRefBitmap11869 = 11869;
+    public long fillRefBitmap11870 = 11870;
+    public long fillRefBitmap11871 = 11871;
+    public long fillRefBitmap11872 = 11872;
+    public long fillRefBitmap11873 = 11873;
+    public long fillRefBitmap11874 = 11874;
+    public long fillRefBitmap11875 = 11875;
+    public long fillRefBitmap11876 = 11876;
+    public long fillRefBitmap11877 = 11877;
+    public long fillRefBitmap11878 = 11878;
+    public long fillRefBitmap11879 = 11879;
+    public long fillRefBitmap11880 = 11880;
+    public long fillRefBitmap11881 = 11881;
+    public long fillRefBitmap11882 = 11882;
+    public long fillRefBitmap11883 = 11883;
+    public long fillRefBitmap11884 = 11884;
+    public long fillRefBitmap11885 = 11885;
+    public long fillRefBitmap11886 = 11886;
+    public long fillRefBitmap11887 = 11887;
+    public long fillRefBitmap11888 = 11888;
+    public long fillRefBitmap11889 = 11889;
+    public long fillRefBitmap11890 = 11890;
+    public long fillRefBitmap11891 = 11891;
+    public long fillRefBitmap11892 = 11892;
+    public long fillRefBitmap11893 = 11893;
+    public long fillRefBitmap11894 = 11894;
+    public long fillRefBitmap11895 = 11895;
+    public long fillRefBitmap11896 = 11896;
+    public long fillRefBitmap11897 = 11897;
+    public long fillRefBitmap11898 = 11898;
+    public long fillRefBitmap11899 = 11899;
+    public long fillRefBitmap11900 = 11900;
+    public long fillRefBitmap11901 = 11901;
+    public long fillRefBitmap11902 = 11902;
+    public long fillRefBitmap11903 = 11903;
+    public long fillRefBitmap11904 = 11904;
+    public long fillRefBitmap11905 = 11905;
+    public long fillRefBitmap11906 = 11906;
+    public long fillRefBitmap11907 = 11907;
+    public long fillRefBitmap11908 = 11908;
+    public long fillRefBitmap11909 = 11909;
+    public long fillRefBitmap11910 = 11910;
+    public long fillRefBitmap11911 = 11911;
+    public long fillRefBitmap11912 = 11912;
+    public long fillRefBitmap11913 = 11913;
+    public long fillRefBitmap11914 = 11914;
+    public long fillRefBitmap11915 = 11915;
+    public long fillRefBitmap11916 = 11916;
+    public long fillRefBitmap11917 = 11917;
+    public long fillRefBitmap11918 = 11918;
+    public long fillRefBitmap11919 = 11919;
+    public long fillRefBitmap11920 = 11920;
+    public long fillRefBitmap11921 = 11921;
+    public long fillRefBitmap11922 = 11922;
+    public long fillRefBitmap11923 = 11923;
+    public long fillRefBitmap11924 = 11924;
+    public long fillRefBitmap11925 = 11925;
+    public long fillRefBitmap11926 = 11926;
+    public long fillRefBitmap11927 = 11927;
+    public long fillRefBitmap11928 = 11928;
+    public long fillRefBitmap11929 = 11929;
+    public long fillRefBitmap11930 = 11930;
+    public long fillRefBitmap11931 = 11931;
+    public long fillRefBitmap11932 = 11932;
+    public long fillRefBitmap11933 = 11933;
+    public long fillRefBitmap11934 = 11934;
+    public long fillRefBitmap11935 = 11935;
+    public long fillRefBitmap11936 = 11936;
+    public long fillRefBitmap11937 = 11937;
+    public long fillRefBitmap11938 = 11938;
+    public long fillRefBitmap11939 = 11939;
+    public long fillRefBitmap11940 = 11940;
+    public long fillRefBitmap11941 = 11941;
+    public long fillRefBitmap11942 = 11942;
+    public long fillRefBitmap11943 = 11943;
+    public long fillRefBitmap11944 = 11944;
+    public long fillRefBitmap11945 = 11945;
+    public long fillRefBitmap11946 = 11946;
+    public long fillRefBitmap11947 = 11947;
+    public long fillRefBitmap11948 = 11948;
+    public long fillRefBitmap11949 = 11949;
+    public long fillRefBitmap11950 = 11950;
+    public long fillRefBitmap11951 = 11951;
+    public long fillRefBitmap11952 = 11952;
+    public long fillRefBitmap11953 = 11953;
+    public long fillRefBitmap11954 = 11954;
+    public long fillRefBitmap11955 = 11955;
+    public long fillRefBitmap11956 = 11956;
+    public long fillRefBitmap11957 = 11957;
+    public long fillRefBitmap11958 = 11958;
+    public long fillRefBitmap11959 = 11959;
+    public long fillRefBitmap11960 = 11960;
+    public long fillRefBitmap11961 = 11961;
+    public long fillRefBitmap11962 = 11962;
+    public long fillRefBitmap11963 = 11963;
+    public long fillRefBitmap11964 = 11964;
+    public long fillRefBitmap11965 = 11965;
+    public long fillRefBitmap11966 = 11966;
+    public long fillRefBitmap11967 = 11967;
+    public long fillRefBitmap11968 = 11968;
+    public long fillRefBitmap11969 = 11969;
+    public long fillRefBitmap11970 = 11970;
+    public long fillRefBitmap11971 = 11971;
+    public long fillRefBitmap11972 = 11972;
+    public long fillRefBitmap11973 = 11973;
+    public long fillRefBitmap11974 = 11974;
+    public long fillRefBitmap11975 = 11975;
+    public long fillRefBitmap11976 = 11976;
+    public long fillRefBitmap11977 = 11977;
+    public long fillRefBitmap11978 = 11978;
+    public long fillRefBitmap11979 = 11979;
+    public long fillRefBitmap11980 = 11980;
+    public long fillRefBitmap11981 = 11981;
+    public long fillRefBitmap11982 = 11982;
+    public long fillRefBitmap11983 = 11983;
+    public long fillRefBitmap11984 = 11984;
+    public long fillRefBitmap11985 = 11985;
+    public long fillRefBitmap11986 = 11986;
+    public long fillRefBitmap11987 = 11987;
+    public long fillRefBitmap11988 = 11988;
+    public long fillRefBitmap11989 = 11989;
+    public long fillRefBitmap11990 = 11990;
+    public long fillRefBitmap11991 = 11991;
+    public long fillRefBitmap11992 = 11992;
+    public long fillRefBitmap11993 = 11993;
+    public long fillRefBitmap11994 = 11994;
+    public long fillRefBitmap11995 = 11995;
+    public long fillRefBitmap11996 = 11996;
+    public long fillRefBitmap11997 = 11997;
+    public long fillRefBitmap11998 = 11998;
+    public long fillRefBitmap11999 = 11999;
+    public long fillRefBitmap12000 = 12000;
+    public long fillRefBitmap12001 = 12001;
+    public long fillRefBitmap12002 = 12002;
+    public long fillRefBitmap12003 = 12003;
+    public long fillRefBitmap12004 = 12004;
+    public long fillRefBitmap12005 = 12005;
+    public long fillRefBitmap12006 = 12006;
+    public long fillRefBitmap12007 = 12007;
+    public long fillRefBitmap12008 = 12008;
+    public long fillRefBitmap12009 = 12009;
+    public long fillRefBitmap12010 = 12010;
+    public long fillRefBitmap12011 = 12011;
+    public long fillRefBitmap12012 = 12012;
+    public long fillRefBitmap12013 = 12013;
+    public long fillRefBitmap12014 = 12014;
+    public long fillRefBitmap12015 = 12015;
+    public long fillRefBitmap12016 = 12016;
+    public long fillRefBitmap12017 = 12017;
+    public long fillRefBitmap12018 = 12018;
+    public long fillRefBitmap12019 = 12019;
+    public long fillRefBitmap12020 = 12020;
+    public long fillRefBitmap12021 = 12021;
+    public long fillRefBitmap12022 = 12022;
+    public long fillRefBitmap12023 = 12023;
+    public long fillRefBitmap12024 = 12024;
+    public long fillRefBitmap12025 = 12025;
+    public long fillRefBitmap12026 = 12026;
+    public long fillRefBitmap12027 = 12027;
+    public long fillRefBitmap12028 = 12028;
+    public long fillRefBitmap12029 = 12029;
+    public long fillRefBitmap12030 = 12030;
+    public long fillRefBitmap12031 = 12031;
+    public long fillRefBitmap12032 = 12032;
+    public long fillRefBitmap12033 = 12033;
+    public long fillRefBitmap12034 = 12034;
+    public long fillRefBitmap12035 = 12035;
+    public long fillRefBitmap12036 = 12036;
+    public long fillRefBitmap12037 = 12037;
+    public long fillRefBitmap12038 = 12038;
+    public long fillRefBitmap12039 = 12039;
+    public long fillRefBitmap12040 = 12040;
+    public long fillRefBitmap12041 = 12041;
+    public long fillRefBitmap12042 = 12042;
+    public long fillRefBitmap12043 = 12043;
+    public long fillRefBitmap12044 = 12044;
+    public long fillRefBitmap12045 = 12045;
+    public long fillRefBitmap12046 = 12046;
+    public long fillRefBitmap12047 = 12047;
+    public long fillRefBitmap12048 = 12048;
+    public long fillRefBitmap12049 = 12049;
+    public long fillRefBitmap12050 = 12050;
+    public long fillRefBitmap12051 = 12051;
+    public long fillRefBitmap12052 = 12052;
+    public long fillRefBitmap12053 = 12053;
+    public long fillRefBitmap12054 = 12054;
+    public long fillRefBitmap12055 = 12055;
+    public long fillRefBitmap12056 = 12056;
+    public long fillRefBitmap12057 = 12057;
+    public long fillRefBitmap12058 = 12058;
+    public long fillRefBitmap12059 = 12059;
+    public long fillRefBitmap12060 = 12060;
+    public long fillRefBitmap12061 = 12061;
+    public long fillRefBitmap12062 = 12062;
+    public long fillRefBitmap12063 = 12063;
+    public long fillRefBitmap12064 = 12064;
+    public long fillRefBitmap12065 = 12065;
+    public long fillRefBitmap12066 = 12066;
+    public long fillRefBitmap12067 = 12067;
+    public long fillRefBitmap12068 = 12068;
+    public long fillRefBitmap12069 = 12069;
+    public long fillRefBitmap12070 = 12070;
+    public long fillRefBitmap12071 = 12071;
+    public long fillRefBitmap12072 = 12072;
+    public long fillRefBitmap12073 = 12073;
+    public long fillRefBitmap12074 = 12074;
+    public long fillRefBitmap12075 = 12075;
+    public long fillRefBitmap12076 = 12076;
+    public long fillRefBitmap12077 = 12077;
+    public long fillRefBitmap12078 = 12078;
+    public long fillRefBitmap12079 = 12079;
+    public long fillRefBitmap12080 = 12080;
+    public long fillRefBitmap12081 = 12081;
+    public long fillRefBitmap12082 = 12082;
+    public long fillRefBitmap12083 = 12083;
+    public long fillRefBitmap12084 = 12084;
+    public long fillRefBitmap12085 = 12085;
+    public long fillRefBitmap12086 = 12086;
+    public long fillRefBitmap12087 = 12087;
+    public long fillRefBitmap12088 = 12088;
+    public long fillRefBitmap12089 = 12089;
+    public long fillRefBitmap12090 = 12090;
+    public long fillRefBitmap12091 = 12091;
+    public long fillRefBitmap12092 = 12092;
+    public long fillRefBitmap12093 = 12093;
+    public long fillRefBitmap12094 = 12094;
+    public long fillRefBitmap12095 = 12095;
+    public long fillRefBitmap12096 = 12096;
+    public long fillRefBitmap12097 = 12097;
+    public long fillRefBitmap12098 = 12098;
+    public long fillRefBitmap12099 = 12099;
+    public long fillRefBitmap12100 = 12100;
+    public long fillRefBitmap12101 = 12101;
+    public long fillRefBitmap12102 = 12102;
+    public long fillRefBitmap12103 = 12103;
+    public long fillRefBitmap12104 = 12104;
+    public long fillRefBitmap12105 = 12105;
+    public long fillRefBitmap12106 = 12106;
+    public long fillRefBitmap12107 = 12107;
+    public long fillRefBitmap12108 = 12108;
+    public long fillRefBitmap12109 = 12109;
+    public long fillRefBitmap12110 = 12110;
+    public long fillRefBitmap12111 = 12111;
+    public long fillRefBitmap12112 = 12112;
+    public long fillRefBitmap12113 = 12113;
+    public long fillRefBitmap12114 = 12114;
+    public long fillRefBitmap12115 = 12115;
+    public long fillRefBitmap12116 = 12116;
+    public long fillRefBitmap12117 = 12117;
+    public long fillRefBitmap12118 = 12118;
+    public long fillRefBitmap12119 = 12119;
+    public long fillRefBitmap12120 = 12120;
+    public long fillRefBitmap12121 = 12121;
+    public long fillRefBitmap12122 = 12122;
+    public long fillRefBitmap12123 = 12123;
+    public long fillRefBitmap12124 = 12124;
+    public long fillRefBitmap12125 = 12125;
+    public long fillRefBitmap12126 = 12126;
+    public long fillRefBitmap12127 = 12127;
+    public long fillRefBitmap12128 = 12128;
+    public long fillRefBitmap12129 = 12129;
+    public long fillRefBitmap12130 = 12130;
+    public long fillRefBitmap12131 = 12131;
+    public long fillRefBitmap12132 = 12132;
+    public long fillRefBitmap12133 = 12133;
+    public long fillRefBitmap12134 = 12134;
+    public long fillRefBitmap12135 = 12135;
+    public long fillRefBitmap12136 = 12136;
+    public long fillRefBitmap12137 = 12137;
+    public long fillRefBitmap12138 = 12138;
+    public long fillRefBitmap12139 = 12139;
+    public long fillRefBitmap12140 = 12140;
+    public long fillRefBitmap12141 = 12141;
+    public long fillRefBitmap12142 = 12142;
+    public long fillRefBitmap12143 = 12143;
+    public long fillRefBitmap12144 = 12144;
+    public long fillRefBitmap12145 = 12145;
+    public long fillRefBitmap12146 = 12146;
+    public long fillRefBitmap12147 = 12147;
+    public long fillRefBitmap12148 = 12148;
+    public long fillRefBitmap12149 = 12149;
+    public long fillRefBitmap12150 = 12150;
+    public long fillRefBitmap12151 = 12151;
+    public long fillRefBitmap12152 = 12152;
+    public long fillRefBitmap12153 = 12153;
+    public long fillRefBitmap12154 = 12154;
+    public long fillRefBitmap12155 = 12155;
+    public long fillRefBitmap12156 = 12156;
+    public long fillRefBitmap12157 = 12157;
+    public long fillRefBitmap12158 = 12158;
+    public long fillRefBitmap12159 = 12159;
+    public long fillRefBitmap12160 = 12160;
+    public long fillRefBitmap12161 = 12161;
+    public long fillRefBitmap12162 = 12162;
+    public long fillRefBitmap12163 = 12163;
+    public long fillRefBitmap12164 = 12164;
+    public long fillRefBitmap12165 = 12165;
+    public long fillRefBitmap12166 = 12166;
+    public long fillRefBitmap12167 = 12167;
+    public long fillRefBitmap12168 = 12168;
+    public long fillRefBitmap12169 = 12169;
+    public long fillRefBitmap12170 = 12170;
+    public long fillRefBitmap12171 = 12171;
+    public long fillRefBitmap12172 = 12172;
+    public long fillRefBitmap12173 = 12173;
+    public long fillRefBitmap12174 = 12174;
+    public long fillRefBitmap12175 = 12175;
+    public long fillRefBitmap12176 = 12176;
+    public long fillRefBitmap12177 = 12177;
+    public long fillRefBitmap12178 = 12178;
+    public long fillRefBitmap12179 = 12179;
+    public long fillRefBitmap12180 = 12180;
+    public long fillRefBitmap12181 = 12181;
+    public long fillRefBitmap12182 = 12182;
+    public long fillRefBitmap12183 = 12183;
+    public long fillRefBitmap12184 = 12184;
+    public long fillRefBitmap12185 = 12185;
+    public long fillRefBitmap12186 = 12186;
+    public long fillRefBitmap12187 = 12187;
+    public long fillRefBitmap12188 = 12188;
+    public long fillRefBitmap12189 = 12189;
+    public long fillRefBitmap12190 = 12190;
+    public long fillRefBitmap12191 = 12191;
+    public long fillRefBitmap12192 = 12192;
+    public long fillRefBitmap12193 = 12193;
+    public long fillRefBitmap12194 = 12194;
+    public long fillRefBitmap12195 = 12195;
+    public long fillRefBitmap12196 = 12196;
+    public long fillRefBitmap12197 = 12197;
+    public long fillRefBitmap12198 = 12198;
+    public long fillRefBitmap12199 = 12199;
+    public long fillRefBitmap12200 = 12200;
+    public long fillRefBitmap12201 = 12201;
+    public long fillRefBitmap12202 = 12202;
+    public long fillRefBitmap12203 = 12203;
+    public long fillRefBitmap12204 = 12204;
+    public long fillRefBitmap12205 = 12205;
+    public long fillRefBitmap12206 = 12206;
+    public long fillRefBitmap12207 = 12207;
+    public long fillRefBitmap12208 = 12208;
+    public long fillRefBitmap12209 = 12209;
+    public long fillRefBitmap12210 = 12210;
+    public long fillRefBitmap12211 = 12211;
+    public long fillRefBitmap12212 = 12212;
+    public long fillRefBitmap12213 = 12213;
+    public long fillRefBitmap12214 = 12214;
+    public long fillRefBitmap12215 = 12215;
+    public long fillRefBitmap12216 = 12216;
+    public long fillRefBitmap12217 = 12217;
+    public long fillRefBitmap12218 = 12218;
+    public long fillRefBitmap12219 = 12219;
+    public long fillRefBitmap12220 = 12220;
+    public long fillRefBitmap12221 = 12221;
+    public long fillRefBitmap12222 = 12222;
+    public long fillRefBitmap12223 = 12223;
+    public long fillRefBitmap12224 = 12224;
+    public long fillRefBitmap12225 = 12225;
+    public long fillRefBitmap12226 = 12226;
+    public long fillRefBitmap12227 = 12227;
+    public long fillRefBitmap12228 = 12228;
+    public long fillRefBitmap12229 = 12229;
+    public long fillRefBitmap12230 = 12230;
+    public long fillRefBitmap12231 = 12231;
+    public long fillRefBitmap12232 = 12232;
+    public long fillRefBitmap12233 = 12233;
+    public long fillRefBitmap12234 = 12234;
+    public long fillRefBitmap12235 = 12235;
+    public long fillRefBitmap12236 = 12236;
+    public long fillRefBitmap12237 = 12237;
+    public long fillRefBitmap12238 = 12238;
+    public long fillRefBitmap12239 = 12239;
+    public long fillRefBitmap12240 = 12240;
+    public long fillRefBitmap12241 = 12241;
+    public long fillRefBitmap12242 = 12242;
+    public long fillRefBitmap12243 = 12243;
+    public long fillRefBitmap12244 = 12244;
+    public long fillRefBitmap12245 = 12245;
+    public long fillRefBitmap12246 = 12246;
+    public long fillRefBitmap12247 = 12247;
+    public long fillRefBitmap12248 = 12248;
+    public long fillRefBitmap12249 = 12249;
+    public long fillRefBitmap12250 = 12250;
+    public long fillRefBitmap12251 = 12251;
+    public long fillRefBitmap12252 = 12252;
+    public long fillRefBitmap12253 = 12253;
+    public long fillRefBitmap12254 = 12254;
+    public long fillRefBitmap12255 = 12255;
+    public long fillRefBitmap12256 = 12256;
+    public long fillRefBitmap12257 = 12257;
+    public long fillRefBitmap12258 = 12258;
+    public long fillRefBitmap12259 = 12259;
+    public long fillRefBitmap12260 = 12260;
+    public long fillRefBitmap12261 = 12261;
+    public long fillRefBitmap12262 = 12262;
+    public long fillRefBitmap12263 = 12263;
+    public long fillRefBitmap12264 = 12264;
+    public long fillRefBitmap12265 = 12265;
+    public long fillRefBitmap12266 = 12266;
+    public long fillRefBitmap12267 = 12267;
+    public long fillRefBitmap12268 = 12268;
+    public long fillRefBitmap12269 = 12269;
+    public long fillRefBitmap12270 = 12270;
+    public long fillRefBitmap12271 = 12271;
+    public long fillRefBitmap12272 = 12272;
+    public long fillRefBitmap12273 = 12273;
+    public long fillRefBitmap12274 = 12274;
+    public long fillRefBitmap12275 = 12275;
+    public long fillRefBitmap12276 = 12276;
+    public long fillRefBitmap12277 = 12277;
+    public long fillRefBitmap12278 = 12278;
+    public long fillRefBitmap12279 = 12279;
+    public long fillRefBitmap12280 = 12280;
+    public long fillRefBitmap12281 = 12281;
+    public long fillRefBitmap12282 = 12282;
+    public long fillRefBitmap12283 = 12283;
+    public long fillRefBitmap12284 = 12284;
+    public long fillRefBitmap12285 = 12285;
+    public long fillRefBitmap12286 = 12286;
+    public long fillRefBitmap12287 = 12287;
+    public long fillRefBitmap12288 = 12288;
+    public long fillRefBitmap12289 = 12289;
+    public long fillRefBitmap12290 = 12290;
+    public long fillRefBitmap12291 = 12291;
+    public long fillRefBitmap12292 = 12292;
+    public long fillRefBitmap12293 = 12293;
+    public long fillRefBitmap12294 = 12294;
+    public long fillRefBitmap12295 = 12295;
+    public long fillRefBitmap12296 = 12296;
+    public long fillRefBitmap12297 = 12297;
+    public long fillRefBitmap12298 = 12298;
+    public long fillRefBitmap12299 = 12299;
+    public long fillRefBitmap12300 = 12300;
+    public long fillRefBitmap12301 = 12301;
+    public long fillRefBitmap12302 = 12302;
+    public long fillRefBitmap12303 = 12303;
+    public long fillRefBitmap12304 = 12304;
+    public long fillRefBitmap12305 = 12305;
+    public long fillRefBitmap12306 = 12306;
+    public long fillRefBitmap12307 = 12307;
+    public long fillRefBitmap12308 = 12308;
+    public long fillRefBitmap12309 = 12309;
+    public long fillRefBitmap12310 = 12310;
+    public long fillRefBitmap12311 = 12311;
+    public long fillRefBitmap12312 = 12312;
+    public long fillRefBitmap12313 = 12313;
+    public long fillRefBitmap12314 = 12314;
+    public long fillRefBitmap12315 = 12315;
+    public long fillRefBitmap12316 = 12316;
+    public long fillRefBitmap12317 = 12317;
+    public long fillRefBitmap12318 = 12318;
+    public long fillRefBitmap12319 = 12319;
+    public long fillRefBitmap12320 = 12320;
+    public long fillRefBitmap12321 = 12321;
+    public long fillRefBitmap12322 = 12322;
+    public long fillRefBitmap12323 = 12323;
+    public long fillRefBitmap12324 = 12324;
+    public long fillRefBitmap12325 = 12325;
+    public long fillRefBitmap12326 = 12326;
+    public long fillRefBitmap12327 = 12327;
+    public long fillRefBitmap12328 = 12328;
+    public long fillRefBitmap12329 = 12329;
+    public long fillRefBitmap12330 = 12330;
+    public long fillRefBitmap12331 = 12331;
+    public long fillRefBitmap12332 = 12332;
+    public long fillRefBitmap12333 = 12333;
+    public long fillRefBitmap12334 = 12334;
+    public long fillRefBitmap12335 = 12335;
+    public long fillRefBitmap12336 = 12336;
+    public long fillRefBitmap12337 = 12337;
+    public long fillRefBitmap12338 = 12338;
+    public long fillRefBitmap12339 = 12339;
+    public long fillRefBitmap12340 = 12340;
+    public long fillRefBitmap12341 = 12341;
+    public long fillRefBitmap12342 = 12342;
+    public long fillRefBitmap12343 = 12343;
+    public long fillRefBitmap12344 = 12344;
+    public long fillRefBitmap12345 = 12345;
+    public long fillRefBitmap12346 = 12346;
+    public long fillRefBitmap12347 = 12347;
+    public long fillRefBitmap12348 = 12348;
+    public long fillRefBitmap12349 = 12349;
+    public long fillRefBitmap12350 = 12350;
+    public long fillRefBitmap12351 = 12351;
+    public long fillRefBitmap12352 = 12352;
+    public long fillRefBitmap12353 = 12353;
+    public long fillRefBitmap12354 = 12354;
+    public long fillRefBitmap12355 = 12355;
+    public long fillRefBitmap12356 = 12356;
+    public long fillRefBitmap12357 = 12357;
+    public long fillRefBitmap12358 = 12358;
+    public long fillRefBitmap12359 = 12359;
+    public long fillRefBitmap12360 = 12360;
+    public long fillRefBitmap12361 = 12361;
+    public long fillRefBitmap12362 = 12362;
+    public long fillRefBitmap12363 = 12363;
+    public long fillRefBitmap12364 = 12364;
+    public long fillRefBitmap12365 = 12365;
+    public long fillRefBitmap12366 = 12366;
+    public long fillRefBitmap12367 = 12367;
+    public long fillRefBitmap12368 = 12368;
+    public long fillRefBitmap12369 = 12369;
+    public long fillRefBitmap12370 = 12370;
+    public long fillRefBitmap12371 = 12371;
+    public long fillRefBitmap12372 = 12372;
+    public long fillRefBitmap12373 = 12373;
+    public long fillRefBitmap12374 = 12374;
+    public long fillRefBitmap12375 = 12375;
+    public long fillRefBitmap12376 = 12376;
+    public long fillRefBitmap12377 = 12377;
+    public long fillRefBitmap12378 = 12378;
+    public long fillRefBitmap12379 = 12379;
+    public long fillRefBitmap12380 = 12380;
+    public long fillRefBitmap12381 = 12381;
+    public long fillRefBitmap12382 = 12382;
+    public long fillRefBitmap12383 = 12383;
+    public long fillRefBitmap12384 = 12384;
+    public long fillRefBitmap12385 = 12385;
+    public long fillRefBitmap12386 = 12386;
+    public long fillRefBitmap12387 = 12387;
+    public long fillRefBitmap12388 = 12388;
+    public long fillRefBitmap12389 = 12389;
+    public long fillRefBitmap12390 = 12390;
+    public long fillRefBitmap12391 = 12391;
+    public long fillRefBitmap12392 = 12392;
+    public long fillRefBitmap12393 = 12393;
+    public long fillRefBitmap12394 = 12394;
+    public long fillRefBitmap12395 = 12395;
+    public long fillRefBitmap12396 = 12396;
+    public long fillRefBitmap12397 = 12397;
+    public long fillRefBitmap12398 = 12398;
+    public long fillRefBitmap12399 = 12399;
+    public long fillRefBitmap12400 = 12400;
+    public long fillRefBitmap12401 = 12401;
+    public long fillRefBitmap12402 = 12402;
+    public long fillRefBitmap12403 = 12403;
+    public long fillRefBitmap12404 = 12404;
+    public long fillRefBitmap12405 = 12405;
+    public long fillRefBitmap12406 = 12406;
+    public long fillRefBitmap12407 = 12407;
+    public long fillRefBitmap12408 = 12408;
+    public long fillRefBitmap12409 = 12409;
+    public long fillRefBitmap12410 = 12410;
+    public long fillRefBitmap12411 = 12411;
+    public long fillRefBitmap12412 = 12412;
+    public long fillRefBitmap12413 = 12413;
+    public long fillRefBitmap12414 = 12414;
+    public long fillRefBitmap12415 = 12415;
+    public long fillRefBitmap12416 = 12416;
+    public long fillRefBitmap12417 = 12417;
+    public long fillRefBitmap12418 = 12418;
+    public long fillRefBitmap12419 = 12419;
+    public long fillRefBitmap12420 = 12420;
+    public long fillRefBitmap12421 = 12421;
+    public long fillRefBitmap12422 = 12422;
+    public long fillRefBitmap12423 = 12423;
+    public long fillRefBitmap12424 = 12424;
+    public long fillRefBitmap12425 = 12425;
+    public long fillRefBitmap12426 = 12426;
+    public long fillRefBitmap12427 = 12427;
+    public long fillRefBitmap12428 = 12428;
+    public long fillRefBitmap12429 = 12429;
+    public long fillRefBitmap12430 = 12430;
+    public long fillRefBitmap12431 = 12431;
+    public long fillRefBitmap12432 = 12432;
+    public long fillRefBitmap12433 = 12433;
+    public long fillRefBitmap12434 = 12434;
+    public long fillRefBitmap12435 = 12435;
+    public long fillRefBitmap12436 = 12436;
+    public long fillRefBitmap12437 = 12437;
+    public long fillRefBitmap12438 = 12438;
+    public long fillRefBitmap12439 = 12439;
+    public long fillRefBitmap12440 = 12440;
+    public long fillRefBitmap12441 = 12441;
+    public long fillRefBitmap12442 = 12442;
+    public long fillRefBitmap12443 = 12443;
+    public long fillRefBitmap12444 = 12444;
+    public long fillRefBitmap12445 = 12445;
+    public long fillRefBitmap12446 = 12446;
+    public long fillRefBitmap12447 = 12447;
+    public long fillRefBitmap12448 = 12448;
+    public long fillRefBitmap12449 = 12449;
+    public long fillRefBitmap12450 = 12450;
+    public long fillRefBitmap12451 = 12451;
+    public long fillRefBitmap12452 = 12452;
+    public long fillRefBitmap12453 = 12453;
+    public long fillRefBitmap12454 = 12454;
+    public long fillRefBitmap12455 = 12455;
+    public long fillRefBitmap12456 = 12456;
+    public long fillRefBitmap12457 = 12457;
+    public long fillRefBitmap12458 = 12458;
+    public long fillRefBitmap12459 = 12459;
+    public long fillRefBitmap12460 = 12460;
+    public long fillRefBitmap12461 = 12461;
+    public long fillRefBitmap12462 = 12462;
+    public long fillRefBitmap12463 = 12463;
+    public long fillRefBitmap12464 = 12464;
+    public long fillRefBitmap12465 = 12465;
+    public long fillRefBitmap12466 = 12466;
+    public long fillRefBitmap12467 = 12467;
+    public long fillRefBitmap12468 = 12468;
+    public long fillRefBitmap12469 = 12469;
+    public long fillRefBitmap12470 = 12470;
+    public long fillRefBitmap12471 = 12471;
+    public long fillRefBitmap12472 = 12472;
+    public long fillRefBitmap12473 = 12473;
+    public long fillRefBitmap12474 = 12474;
+    public long fillRefBitmap12475 = 12475;
+    public long fillRefBitmap12476 = 12476;
+    public long fillRefBitmap12477 = 12477;
+    public long fillRefBitmap12478 = 12478;
+    public long fillRefBitmap12479 = 12479;
+    public long fillRefBitmap12480 = 12480;
+    public long fillRefBitmap12481 = 12481;
+    public long fillRefBitmap12482 = 12482;
+    public long fillRefBitmap12483 = 12483;
+    public long fillRefBitmap12484 = 12484;
+    public long fillRefBitmap12485 = 12485;
+    public long fillRefBitmap12486 = 12486;
+    public long fillRefBitmap12487 = 12487;
+    public long fillRefBitmap12488 = 12488;
+    public long fillRefBitmap12489 = 12489;
+    public long fillRefBitmap12490 = 12490;
+    public long fillRefBitmap12491 = 12491;
+    public long fillRefBitmap12492 = 12492;
+    public long fillRefBitmap12493 = 12493;
+    public long fillRefBitmap12494 = 12494;
+    public long fillRefBitmap12495 = 12495;
+    public long fillRefBitmap12496 = 12496;
+    public long fillRefBitmap12497 = 12497;
+    public long fillRefBitmap12498 = 12498;
+    public long fillRefBitmap12499 = 12499;
+    public long fillRefBitmap12500 = 12500;
+    public long fillRefBitmap12501 = 12501;
+    public long fillRefBitmap12502 = 12502;
+    public long fillRefBitmap12503 = 12503;
+    public long fillRefBitmap12504 = 12504;
+    public long fillRefBitmap12505 = 12505;
+    public long fillRefBitmap12506 = 12506;
+    public long fillRefBitmap12507 = 12507;
+    public long fillRefBitmap12508 = 12508;
+    public long fillRefBitmap12509 = 12509;
+    public long fillRefBitmap12510 = 12510;
+    public long fillRefBitmap12511 = 12511;
+    public long fillRefBitmap12512 = 12512;
+    public long fillRefBitmap12513 = 12513;
+    public long fillRefBitmap12514 = 12514;
+    public long fillRefBitmap12515 = 12515;
+    public long fillRefBitmap12516 = 12516;
+    public long fillRefBitmap12517 = 12517;
+    public long fillRefBitmap12518 = 12518;
+    public long fillRefBitmap12519 = 12519;
+    public long fillRefBitmap12520 = 12520;
+    public long fillRefBitmap12521 = 12521;
+    public long fillRefBitmap12522 = 12522;
+    public long fillRefBitmap12523 = 12523;
+    public long fillRefBitmap12524 = 12524;
+    public long fillRefBitmap12525 = 12525;
+    public long fillRefBitmap12526 = 12526;
+    public long fillRefBitmap12527 = 12527;
+    public long fillRefBitmap12528 = 12528;
+    public long fillRefBitmap12529 = 12529;
+    public long fillRefBitmap12530 = 12530;
+    public long fillRefBitmap12531 = 12531;
+    public long fillRefBitmap12532 = 12532;
+    public long fillRefBitmap12533 = 12533;
+    public long fillRefBitmap12534 = 12534;
+    public long fillRefBitmap12535 = 12535;
+    public long fillRefBitmap12536 = 12536;
+    public long fillRefBitmap12537 = 12537;
+    public long fillRefBitmap12538 = 12538;
+    public long fillRefBitmap12539 = 12539;
+    public long fillRefBitmap12540 = 12540;
+    public long fillRefBitmap12541 = 12541;
+    public long fillRefBitmap12542 = 12542;
+    public long fillRefBitmap12543 = 12543;
+    public long fillRefBitmap12544 = 12544;
+    public long fillRefBitmap12545 = 12545;
+    public long fillRefBitmap12546 = 12546;
+    public long fillRefBitmap12547 = 12547;
+    public long fillRefBitmap12548 = 12548;
+    public long fillRefBitmap12549 = 12549;
+    public long fillRefBitmap12550 = 12550;
+    public long fillRefBitmap12551 = 12551;
+    public long fillRefBitmap12552 = 12552;
+    public long fillRefBitmap12553 = 12553;
+    public long fillRefBitmap12554 = 12554;
+    public long fillRefBitmap12555 = 12555;
+    public long fillRefBitmap12556 = 12556;
+    public long fillRefBitmap12557 = 12557;
+    public long fillRefBitmap12558 = 12558;
+    public long fillRefBitmap12559 = 12559;
+    public long fillRefBitmap12560 = 12560;
+    public long fillRefBitmap12561 = 12561;
+    public long fillRefBitmap12562 = 12562;
+    public long fillRefBitmap12563 = 12563;
+    public long fillRefBitmap12564 = 12564;
+    public long fillRefBitmap12565 = 12565;
+    public long fillRefBitmap12566 = 12566;
+    public long fillRefBitmap12567 = 12567;
+    public long fillRefBitmap12568 = 12568;
+    public long fillRefBitmap12569 = 12569;
+    public long fillRefBitmap12570 = 12570;
+    public long fillRefBitmap12571 = 12571;
+    public long fillRefBitmap12572 = 12572;
+    public long fillRefBitmap12573 = 12573;
+    public long fillRefBitmap12574 = 12574;
+    public long fillRefBitmap12575 = 12575;
+    public long fillRefBitmap12576 = 12576;
+    public long fillRefBitmap12577 = 12577;
+    public long fillRefBitmap12578 = 12578;
+    public long fillRefBitmap12579 = 12579;
+    public long fillRefBitmap12580 = 12580;
+    public long fillRefBitmap12581 = 12581;
+    public long fillRefBitmap12582 = 12582;
+    public long fillRefBitmap12583 = 12583;
+    public long fillRefBitmap12584 = 12584;
+    public long fillRefBitmap12585 = 12585;
+    public long fillRefBitmap12586 = 12586;
+    public long fillRefBitmap12587 = 12587;
+    public long fillRefBitmap12588 = 12588;
+    public long fillRefBitmap12589 = 12589;
+    public long fillRefBitmap12590 = 12590;
+    public long fillRefBitmap12591 = 12591;
+    public long fillRefBitmap12592 = 12592;
+    public long fillRefBitmap12593 = 12593;
+    public long fillRefBitmap12594 = 12594;
+    public long fillRefBitmap12595 = 12595;
+    public long fillRefBitmap12596 = 12596;
+    public long fillRefBitmap12597 = 12597;
+    public long fillRefBitmap12598 = 12598;
+    public long fillRefBitmap12599 = 12599;
+    public long fillRefBitmap12600 = 12600;
+    public long fillRefBitmap12601 = 12601;
+    public long fillRefBitmap12602 = 12602;
+    public long fillRefBitmap12603 = 12603;
+    public long fillRefBitmap12604 = 12604;
+    public long fillRefBitmap12605 = 12605;
+    public long fillRefBitmap12606 = 12606;
+    public long fillRefBitmap12607 = 12607;
+    public long fillRefBitmap12608 = 12608;
+    public long fillRefBitmap12609 = 12609;
+    public long fillRefBitmap12610 = 12610;
+    public long fillRefBitmap12611 = 12611;
+    public long fillRefBitmap12612 = 12612;
+    public long fillRefBitmap12613 = 12613;
+    public long fillRefBitmap12614 = 12614;
+    public long fillRefBitmap12615 = 12615;
+    public long fillRefBitmap12616 = 12616;
+    public long fillRefBitmap12617 = 12617;
+    public long fillRefBitmap12618 = 12618;
+    public long fillRefBitmap12619 = 12619;
+    public long fillRefBitmap12620 = 12620;
+    public long fillRefBitmap12621 = 12621;
+    public long fillRefBitmap12622 = 12622;
+    public long fillRefBitmap12623 = 12623;
+    public long fillRefBitmap12624 = 12624;
+    public long fillRefBitmap12625 = 12625;
+    public long fillRefBitmap12626 = 12626;
+    public long fillRefBitmap12627 = 12627;
+    public long fillRefBitmap12628 = 12628;
+    public long fillRefBitmap12629 = 12629;
+    public long fillRefBitmap12630 = 12630;
+    public long fillRefBitmap12631 = 12631;
+    public long fillRefBitmap12632 = 12632;
+    public long fillRefBitmap12633 = 12633;
+    public long fillRefBitmap12634 = 12634;
+    public long fillRefBitmap12635 = 12635;
+    public long fillRefBitmap12636 = 12636;
+    public long fillRefBitmap12637 = 12637;
+    public long fillRefBitmap12638 = 12638;
+    public long fillRefBitmap12639 = 12639;
+    public long fillRefBitmap12640 = 12640;
+    public long fillRefBitmap12641 = 12641;
+    public long fillRefBitmap12642 = 12642;
+    public long fillRefBitmap12643 = 12643;
+    public long fillRefBitmap12644 = 12644;
+    public long fillRefBitmap12645 = 12645;
+    public long fillRefBitmap12646 = 12646;
+    public long fillRefBitmap12647 = 12647;
+    public long fillRefBitmap12648 = 12648;
+    public long fillRefBitmap12649 = 12649;
+    public long fillRefBitmap12650 = 12650;
+    public long fillRefBitmap12651 = 12651;
+    public long fillRefBitmap12652 = 12652;
+    public long fillRefBitmap12653 = 12653;
+    public long fillRefBitmap12654 = 12654;
+    public long fillRefBitmap12655 = 12655;
+    public long fillRefBitmap12656 = 12656;
+    public long fillRefBitmap12657 = 12657;
+    public long fillRefBitmap12658 = 12658;
+    public long fillRefBitmap12659 = 12659;
+    public long fillRefBitmap12660 = 12660;
+    public long fillRefBitmap12661 = 12661;
+    public long fillRefBitmap12662 = 12662;
+    public long fillRefBitmap12663 = 12663;
+    public long fillRefBitmap12664 = 12664;
+    public long fillRefBitmap12665 = 12665;
+    public long fillRefBitmap12666 = 12666;
+    public long fillRefBitmap12667 = 12667;
+    public long fillRefBitmap12668 = 12668;
+    public long fillRefBitmap12669 = 12669;
+    public long fillRefBitmap12670 = 12670;
+    public long fillRefBitmap12671 = 12671;
+    public long fillRefBitmap12672 = 12672;
+    public long fillRefBitmap12673 = 12673;
+    public long fillRefBitmap12674 = 12674;
+    public long fillRefBitmap12675 = 12675;
+    public long fillRefBitmap12676 = 12676;
+    public long fillRefBitmap12677 = 12677;
+    public long fillRefBitmap12678 = 12678;
+    public long fillRefBitmap12679 = 12679;
+    public long fillRefBitmap12680 = 12680;
+    public long fillRefBitmap12681 = 12681;
+    public long fillRefBitmap12682 = 12682;
+    public long fillRefBitmap12683 = 12683;
+    public long fillRefBitmap12684 = 12684;
+    public long fillRefBitmap12685 = 12685;
+    public long fillRefBitmap12686 = 12686;
+    public long fillRefBitmap12687 = 12687;
+    public long fillRefBitmap12688 = 12688;
+    public long fillRefBitmap12689 = 12689;
+    public long fillRefBitmap12690 = 12690;
+    public long fillRefBitmap12691 = 12691;
+    public long fillRefBitmap12692 = 12692;
+    public long fillRefBitmap12693 = 12693;
+    public long fillRefBitmap12694 = 12694;
+    public long fillRefBitmap12695 = 12695;
+    public long fillRefBitmap12696 = 12696;
+    public long fillRefBitmap12697 = 12697;
+    public long fillRefBitmap12698 = 12698;
+    public long fillRefBitmap12699 = 12699;
+    public long fillRefBitmap12700 = 12700;
+    public long fillRefBitmap12701 = 12701;
+    public long fillRefBitmap12702 = 12702;
+    public long fillRefBitmap12703 = 12703;
+    public long fillRefBitmap12704 = 12704;
+    public long fillRefBitmap12705 = 12705;
+    public long fillRefBitmap12706 = 12706;
+    public long fillRefBitmap12707 = 12707;
+    public long fillRefBitmap12708 = 12708;
+    public long fillRefBitmap12709 = 12709;
+    public long fillRefBitmap12710 = 12710;
+    public long fillRefBitmap12711 = 12711;
+    public long fillRefBitmap12712 = 12712;
+    public long fillRefBitmap12713 = 12713;
+    public long fillRefBitmap12714 = 12714;
+    public long fillRefBitmap12715 = 12715;
+    public long fillRefBitmap12716 = 12716;
+    public long fillRefBitmap12717 = 12717;
+    public long fillRefBitmap12718 = 12718;
+    public long fillRefBitmap12719 = 12719;
+    public long fillRefBitmap12720 = 12720;
+    public long fillRefBitmap12721 = 12721;
+    public long fillRefBitmap12722 = 12722;
+    public long fillRefBitmap12723 = 12723;
+    public long fillRefBitmap12724 = 12724;
+    public long fillRefBitmap12725 = 12725;
+    public long fillRefBitmap12726 = 12726;
+    public long fillRefBitmap12727 = 12727;
+    public long fillRefBitmap12728 = 12728;
+    public long fillRefBitmap12729 = 12729;
+    public long fillRefBitmap12730 = 12730;
+    public long fillRefBitmap12731 = 12731;
+    public long fillRefBitmap12732 = 12732;
+    public long fillRefBitmap12733 = 12733;
+    public long fillRefBitmap12734 = 12734;
+    public long fillRefBitmap12735 = 12735;
+    public long fillRefBitmap12736 = 12736;
+    public long fillRefBitmap12737 = 12737;
+    public long fillRefBitmap12738 = 12738;
+    public long fillRefBitmap12739 = 12739;
+    public long fillRefBitmap12740 = 12740;
+    public long fillRefBitmap12741 = 12741;
+    public long fillRefBitmap12742 = 12742;
+    public long fillRefBitmap12743 = 12743;
+    public long fillRefBitmap12744 = 12744;
+    public long fillRefBitmap12745 = 12745;
+    public long fillRefBitmap12746 = 12746;
+    public long fillRefBitmap12747 = 12747;
+    public long fillRefBitmap12748 = 12748;
+    public long fillRefBitmap12749 = 12749;
+    public long fillRefBitmap12750 = 12750;
+    public long fillRefBitmap12751 = 12751;
+    public long fillRefBitmap12752 = 12752;
+    public long fillRefBitmap12753 = 12753;
+    public long fillRefBitmap12754 = 12754;
+    public long fillRefBitmap12755 = 12755;
+    public long fillRefBitmap12756 = 12756;
+    public long fillRefBitmap12757 = 12757;
+    public long fillRefBitmap12758 = 12758;
+    public long fillRefBitmap12759 = 12759;
+    public long fillRefBitmap12760 = 12760;
+    public long fillRefBitmap12761 = 12761;
+    public long fillRefBitmap12762 = 12762;
+    public long fillRefBitmap12763 = 12763;
+    public long fillRefBitmap12764 = 12764;
+    public long fillRefBitmap12765 = 12765;
+    public long fillRefBitmap12766 = 12766;
+    public long fillRefBitmap12767 = 12767;
+    public long fillRefBitmap12768 = 12768;
+    public long fillRefBitmap12769 = 12769;
+    public long fillRefBitmap12770 = 12770;
+    public long fillRefBitmap12771 = 12771;
+    public long fillRefBitmap12772 = 12772;
+    public long fillRefBitmap12773 = 12773;
+    public long fillRefBitmap12774 = 12774;
+    public long fillRefBitmap12775 = 12775;
+    public long fillRefBitmap12776 = 12776;
+    public long fillRefBitmap12777 = 12777;
+    public long fillRefBitmap12778 = 12778;
+    public long fillRefBitmap12779 = 12779;
+    public long fillRefBitmap12780 = 12780;
+    public long fillRefBitmap12781 = 12781;
+    public long fillRefBitmap12782 = 12782;
+    public long fillRefBitmap12783 = 12783;
+    public long fillRefBitmap12784 = 12784;
+    public long fillRefBitmap12785 = 12785;
+    public long fillRefBitmap12786 = 12786;
+    public long fillRefBitmap12787 = 12787;
+    public long fillRefBitmap12788 = 12788;
+    public long fillRefBitmap12789 = 12789;
+    public long fillRefBitmap12790 = 12790;
+    public long fillRefBitmap12791 = 12791;
+    public long fillRefBitmap12792 = 12792;
+    public long fillRefBitmap12793 = 12793;
+    public long fillRefBitmap12794 = 12794;
+    public long fillRefBitmap12795 = 12795;
+    public long fillRefBitmap12796 = 12796;
+    public long fillRefBitmap12797 = 12797;
+    public long fillRefBitmap12798 = 12798;
+    public long fillRefBitmap12799 = 12799;
+    public long fillRefBitmap12800 = 12800;
+    public long fillRefBitmap12801 = 12801;
+    public long fillRefBitmap12802 = 12802;
+    public long fillRefBitmap12803 = 12803;
+    public long fillRefBitmap12804 = 12804;
+    public long fillRefBitmap12805 = 12805;
+    public long fillRefBitmap12806 = 12806;
+    public long fillRefBitmap12807 = 12807;
+    public long fillRefBitmap12808 = 12808;
+    public long fillRefBitmap12809 = 12809;
+    public long fillRefBitmap12810 = 12810;
+    public long fillRefBitmap12811 = 12811;
+    public long fillRefBitmap12812 = 12812;
+    public long fillRefBitmap12813 = 12813;
+    public long fillRefBitmap12814 = 12814;
+    public long fillRefBitmap12815 = 12815;
+    public long fillRefBitmap12816 = 12816;
+    public long fillRefBitmap12817 = 12817;
+    public long fillRefBitmap12818 = 12818;
+    public long fillRefBitmap12819 = 12819;
+    public long fillRefBitmap12820 = 12820;
+    public long fillRefBitmap12821 = 12821;
+    public long fillRefBitmap12822 = 12822;
+    public long fillRefBitmap12823 = 12823;
+    public long fillRefBitmap12824 = 12824;
+    public long fillRefBitmap12825 = 12825;
+    public long fillRefBitmap12826 = 12826;
+    public long fillRefBitmap12827 = 12827;
+    public long fillRefBitmap12828 = 12828;
+    public long fillRefBitmap12829 = 12829;
+    public long fillRefBitmap12830 = 12830;
+    public long fillRefBitmap12831 = 12831;
+    public long fillRefBitmap12832 = 12832;
+    public long fillRefBitmap12833 = 12833;
+    public long fillRefBitmap12834 = 12834;
+    public long fillRefBitmap12835 = 12835;
+    public long fillRefBitmap12836 = 12836;
+    public long fillRefBitmap12837 = 12837;
+    public long fillRefBitmap12838 = 12838;
+    public long fillRefBitmap12839 = 12839;
+    public long fillRefBitmap12840 = 12840;
+    public long fillRefBitmap12841 = 12841;
+    public long fillRefBitmap12842 = 12842;
+    public long fillRefBitmap12843 = 12843;
+    public long fillRefBitmap12844 = 12844;
+    public long fillRefBitmap12845 = 12845;
+    public long fillRefBitmap12846 = 12846;
+    public long fillRefBitmap12847 = 12847;
+    public long fillRefBitmap12848 = 12848;
+    public long fillRefBitmap12849 = 12849;
+    public long fillRefBitmap12850 = 12850;
+    public long fillRefBitmap12851 = 12851;
+    public long fillRefBitmap12852 = 12852;
+    public long fillRefBitmap12853 = 12853;
+    public long fillRefBitmap12854 = 12854;
+    public long fillRefBitmap12855 = 12855;
+    public long fillRefBitmap12856 = 12856;
+    public long fillRefBitmap12857 = 12857;
+    public long fillRefBitmap12858 = 12858;
+    public long fillRefBitmap12859 = 12859;
+    public long fillRefBitmap12860 = 12860;
+    public long fillRefBitmap12861 = 12861;
+    public long fillRefBitmap12862 = 12862;
+    public long fillRefBitmap12863 = 12863;
+    public long fillRefBitmap12864 = 12864;
+    public long fillRefBitmap12865 = 12865;
+    public long fillRefBitmap12866 = 12866;
+    public long fillRefBitmap12867 = 12867;
+    public long fillRefBitmap12868 = 12868;
+    public long fillRefBitmap12869 = 12869;
+    public long fillRefBitmap12870 = 12870;
+    public long fillRefBitmap12871 = 12871;
+    public long fillRefBitmap12872 = 12872;
+    public long fillRefBitmap12873 = 12873;
+    public long fillRefBitmap12874 = 12874;
+    public long fillRefBitmap12875 = 12875;
+    public long fillRefBitmap12876 = 12876;
+    public long fillRefBitmap12877 = 12877;
+    public long fillRefBitmap12878 = 12878;
+    public long fillRefBitmap12879 = 12879;
+    public long fillRefBitmap12880 = 12880;
+    public long fillRefBitmap12881 = 12881;
+    public long fillRefBitmap12882 = 12882;
+    public long fillRefBitmap12883 = 12883;
+    public long fillRefBitmap12884 = 12884;
+    public long fillRefBitmap12885 = 12885;
+    public long fillRefBitmap12886 = 12886;
+    public long fillRefBitmap12887 = 12887;
+    public long fillRefBitmap12888 = 12888;
+    public long fillRefBitmap12889 = 12889;
+    public long fillRefBitmap12890 = 12890;
+    public long fillRefBitmap12891 = 12891;
+    public long fillRefBitmap12892 = 12892;
+    public long fillRefBitmap12893 = 12893;
+    public long fillRefBitmap12894 = 12894;
+    public long fillRefBitmap12895 = 12895;
+    public long fillRefBitmap12896 = 12896;
+    public long fillRefBitmap12897 = 12897;
+    public long fillRefBitmap12898 = 12898;
+    public long fillRefBitmap12899 = 12899;
+    public long fillRefBitmap12900 = 12900;
+    public long fillRefBitmap12901 = 12901;
+    public long fillRefBitmap12902 = 12902;
+    public long fillRefBitmap12903 = 12903;
+    public long fillRefBitmap12904 = 12904;
+    public long fillRefBitmap12905 = 12905;
+    public long fillRefBitmap12906 = 12906;
+    public long fillRefBitmap12907 = 12907;
+    public long fillRefBitmap12908 = 12908;
+    public long fillRefBitmap12909 = 12909;
+    public long fillRefBitmap12910 = 12910;
+    public long fillRefBitmap12911 = 12911;
+    public long fillRefBitmap12912 = 12912;
+    public long fillRefBitmap12913 = 12913;
+    public long fillRefBitmap12914 = 12914;
+    public long fillRefBitmap12915 = 12915;
+    public long fillRefBitmap12916 = 12916;
+    public long fillRefBitmap12917 = 12917;
+    public long fillRefBitmap12918 = 12918;
+    public long fillRefBitmap12919 = 12919;
+    public long fillRefBitmap12920 = 12920;
+    public long fillRefBitmap12921 = 12921;
+    public long fillRefBitmap12922 = 12922;
+    public long fillRefBitmap12923 = 12923;
+    public long fillRefBitmap12924 = 12924;
+    public long fillRefBitmap12925 = 12925;
+    public long fillRefBitmap12926 = 12926;
+    public long fillRefBitmap12927 = 12927;
+    public long fillRefBitmap12928 = 12928;
+    public long fillRefBitmap12929 = 12929;
+    public long fillRefBitmap12930 = 12930;
+    public long fillRefBitmap12931 = 12931;
+    public long fillRefBitmap12932 = 12932;
+    public long fillRefBitmap12933 = 12933;
+    public long fillRefBitmap12934 = 12934;
+    public long fillRefBitmap12935 = 12935;
+    public long fillRefBitmap12936 = 12936;
+    public long fillRefBitmap12937 = 12937;
+    public long fillRefBitmap12938 = 12938;
+    public long fillRefBitmap12939 = 12939;
+    public long fillRefBitmap12940 = 12940;
+    public long fillRefBitmap12941 = 12941;
+    public long fillRefBitmap12942 = 12942;
+    public long fillRefBitmap12943 = 12943;
+    public long fillRefBitmap12944 = 12944;
+    public long fillRefBitmap12945 = 12945;
+    public long fillRefBitmap12946 = 12946;
+    public long fillRefBitmap12947 = 12947;
+    public long fillRefBitmap12948 = 12948;
+    public long fillRefBitmap12949 = 12949;
+    public long fillRefBitmap12950 = 12950;
+    public long fillRefBitmap12951 = 12951;
+    public long fillRefBitmap12952 = 12952;
+    public long fillRefBitmap12953 = 12953;
+    public long fillRefBitmap12954 = 12954;
+    public long fillRefBitmap12955 = 12955;
+    public long fillRefBitmap12956 = 12956;
+    public long fillRefBitmap12957 = 12957;
+    public long fillRefBitmap12958 = 12958;
+    public long fillRefBitmap12959 = 12959;
+    public long fillRefBitmap12960 = 12960;
+    public long fillRefBitmap12961 = 12961;
+    public long fillRefBitmap12962 = 12962;
+    public long fillRefBitmap12963 = 12963;
+    public long fillRefBitmap12964 = 12964;
+    public long fillRefBitmap12965 = 12965;
+    public long fillRefBitmap12966 = 12966;
+    public long fillRefBitmap12967 = 12967;
+    public long fillRefBitmap12968 = 12968;
+    public long fillRefBitmap12969 = 12969;
+    public long fillRefBitmap12970 = 12970;
+    public long fillRefBitmap12971 = 12971;
+    public long fillRefBitmap12972 = 12972;
+    public long fillRefBitmap12973 = 12973;
+    public long fillRefBitmap12974 = 12974;
+    public long fillRefBitmap12975 = 12975;
+    public long fillRefBitmap12976 = 12976;
+    public long fillRefBitmap12977 = 12977;
+    public long fillRefBitmap12978 = 12978;
+    public long fillRefBitmap12979 = 12979;
+    public long fillRefBitmap12980 = 12980;
+    public long fillRefBitmap12981 = 12981;
+    public long fillRefBitmap12982 = 12982;
+    public long fillRefBitmap12983 = 12983;
+    public long fillRefBitmap12984 = 12984;
+    public long fillRefBitmap12985 = 12985;
+    public long fillRefBitmap12986 = 12986;
+    public long fillRefBitmap12987 = 12987;
+    public long fillRefBitmap12988 = 12988;
+    public long fillRefBitmap12989 = 12989;
+    public long fillRefBitmap12990 = 12990;
+    public long fillRefBitmap12991 = 12991;
+    public long fillRefBitmap12992 = 12992;
+    public long fillRefBitmap12993 = 12993;
+    public long fillRefBitmap12994 = 12994;
+    public long fillRefBitmap12995 = 12995;
+    public long fillRefBitmap12996 = 12996;
+    public long fillRefBitmap12997 = 12997;
+    public long fillRefBitmap12998 = 12998;
+    public long fillRefBitmap12999 = 12999;
+    public long fillRefBitmap13000 = 13000;
+    public long fillRefBitmap13001 = 13001;
+    public long fillRefBitmap13002 = 13002;
+    public long fillRefBitmap13003 = 13003;
+    public long fillRefBitmap13004 = 13004;
+    public long fillRefBitmap13005 = 13005;
+    public long fillRefBitmap13006 = 13006;
+    public long fillRefBitmap13007 = 13007;
+    public long fillRefBitmap13008 = 13008;
+    public long fillRefBitmap13009 = 13009;
+    public long fillRefBitmap13010 = 13010;
+    public long fillRefBitmap13011 = 13011;
+    public long fillRefBitmap13012 = 13012;
+    public long fillRefBitmap13013 = 13013;
+    public long fillRefBitmap13014 = 13014;
+    public long fillRefBitmap13015 = 13015;
+    public long fillRefBitmap13016 = 13016;
+    public long fillRefBitmap13017 = 13017;
+    public long fillRefBitmap13018 = 13018;
+    public long fillRefBitmap13019 = 13019;
+    public long fillRefBitmap13020 = 13020;
+    public long fillRefBitmap13021 = 13021;
+    public long fillRefBitmap13022 = 13022;
+    public long fillRefBitmap13023 = 13023;
+    public long fillRefBitmap13024 = 13024;
+    public long fillRefBitmap13025 = 13025;
+    public long fillRefBitmap13026 = 13026;
+    public long fillRefBitmap13027 = 13027;
+    public long fillRefBitmap13028 = 13028;
+    public long fillRefBitmap13029 = 13029;
+    public long fillRefBitmap13030 = 13030;
+    public long fillRefBitmap13031 = 13031;
+    public long fillRefBitmap13032 = 13032;
+    public long fillRefBitmap13033 = 13033;
+    public long fillRefBitmap13034 = 13034;
+    public long fillRefBitmap13035 = 13035;
+    public long fillRefBitmap13036 = 13036;
+    public long fillRefBitmap13037 = 13037;
+    public long fillRefBitmap13038 = 13038;
+    public long fillRefBitmap13039 = 13039;
+    public long fillRefBitmap13040 = 13040;
+    public long fillRefBitmap13041 = 13041;
+    public long fillRefBitmap13042 = 13042;
+    public long fillRefBitmap13043 = 13043;
+    public long fillRefBitmap13044 = 13044;
+    public long fillRefBitmap13045 = 13045;
+    public long fillRefBitmap13046 = 13046;
+    public long fillRefBitmap13047 = 13047;
+    public long fillRefBitmap13048 = 13048;
+    public long fillRefBitmap13049 = 13049;
+    public long fillRefBitmap13050 = 13050;
+    public long fillRefBitmap13051 = 13051;
+    public long fillRefBitmap13052 = 13052;
+    public long fillRefBitmap13053 = 13053;
+    public long fillRefBitmap13054 = 13054;
+    public long fillRefBitmap13055 = 13055;
+    public long fillRefBitmap13056 = 13056;
+    public long fillRefBitmap13057 = 13057;
+    public long fillRefBitmap13058 = 13058;
+    public long fillRefBitmap13059 = 13059;
+    public long fillRefBitmap13060 = 13060;
+    public long fillRefBitmap13061 = 13061;
+    public long fillRefBitmap13062 = 13062;
+    public long fillRefBitmap13063 = 13063;
+    public long fillRefBitmap13064 = 13064;
+    public long fillRefBitmap13065 = 13065;
+    public long fillRefBitmap13066 = 13066;
+    public long fillRefBitmap13067 = 13067;
+    public long fillRefBitmap13068 = 13068;
+    public long fillRefBitmap13069 = 13069;
+    public long fillRefBitmap13070 = 13070;
+    public long fillRefBitmap13071 = 13071;
+    public long fillRefBitmap13072 = 13072;
+    public long fillRefBitmap13073 = 13073;
+    public long fillRefBitmap13074 = 13074;
+    public long fillRefBitmap13075 = 13075;
+    public long fillRefBitmap13076 = 13076;
+    public long fillRefBitmap13077 = 13077;
+    public long fillRefBitmap13078 = 13078;
+    public long fillRefBitmap13079 = 13079;
+    public long fillRefBitmap13080 = 13080;
+    public long fillRefBitmap13081 = 13081;
+    public long fillRefBitmap13082 = 13082;
+    public long fillRefBitmap13083 = 13083;
+    public long fillRefBitmap13084 = 13084;
+    public long fillRefBitmap13085 = 13085;
+    public long fillRefBitmap13086 = 13086;
+    public long fillRefBitmap13087 = 13087;
+    public long fillRefBitmap13088 = 13088;
+    public long fillRefBitmap13089 = 13089;
+    public long fillRefBitmap13090 = 13090;
+    public long fillRefBitmap13091 = 13091;
+    public long fillRefBitmap13092 = 13092;
+    public long fillRefBitmap13093 = 13093;
+    public long fillRefBitmap13094 = 13094;
+    public long fillRefBitmap13095 = 13095;
+    public long fillRefBitmap13096 = 13096;
+    public long fillRefBitmap13097 = 13097;
+    public long fillRefBitmap13098 = 13098;
+    public long fillRefBitmap13099 = 13099;
+    public long fillRefBitmap13100 = 13100;
+    public long fillRefBitmap13101 = 13101;
+    public long fillRefBitmap13102 = 13102;
+    public long fillRefBitmap13103 = 13103;
+    public long fillRefBitmap13104 = 13104;
+    public long fillRefBitmap13105 = 13105;
+    public long fillRefBitmap13106 = 13106;
+    public long fillRefBitmap13107 = 13107;
+    public long fillRefBitmap13108 = 13108;
+    public long fillRefBitmap13109 = 13109;
+    public long fillRefBitmap13110 = 13110;
+    public long fillRefBitmap13111 = 13111;
+    public long fillRefBitmap13112 = 13112;
+    public long fillRefBitmap13113 = 13113;
+    public long fillRefBitmap13114 = 13114;
+    public long fillRefBitmap13115 = 13115;
+    public long fillRefBitmap13116 = 13116;
+    public long fillRefBitmap13117 = 13117;
+    public long fillRefBitmap13118 = 13118;
+    public long fillRefBitmap13119 = 13119;
+    public long fillRefBitmap13120 = 13120;
+    public long fillRefBitmap13121 = 13121;
+    public long fillRefBitmap13122 = 13122;
+    public long fillRefBitmap13123 = 13123;
+    public long fillRefBitmap13124 = 13124;
+    public long fillRefBitmap13125 = 13125;
+    public long fillRefBitmap13126 = 13126;
+    public long fillRefBitmap13127 = 13127;
+    public long fillRefBitmap13128 = 13128;
+    public long fillRefBitmap13129 = 13129;
+    public long fillRefBitmap13130 = 13130;
+    public long fillRefBitmap13131 = 13131;
+    public long fillRefBitmap13132 = 13132;
+    public long fillRefBitmap13133 = 13133;
+    public long fillRefBitmap13134 = 13134;
+    public long fillRefBitmap13135 = 13135;
+    public long fillRefBitmap13136 = 13136;
+    public long fillRefBitmap13137 = 13137;
+    public long fillRefBitmap13138 = 13138;
+    public long fillRefBitmap13139 = 13139;
+    public long fillRefBitmap13140 = 13140;
+    public long fillRefBitmap13141 = 13141;
+    public long fillRefBitmap13142 = 13142;
+    public long fillRefBitmap13143 = 13143;
+    public long fillRefBitmap13144 = 13144;
+    public long fillRefBitmap13145 = 13145;
+    public long fillRefBitmap13146 = 13146;
+    public long fillRefBitmap13147 = 13147;
+    public long fillRefBitmap13148 = 13148;
+    public long fillRefBitmap13149 = 13149;
+    public long fillRefBitmap13150 = 13150;
+    public long fillRefBitmap13151 = 13151;
+    public long fillRefBitmap13152 = 13152;
+    public long fillRefBitmap13153 = 13153;
+    public long fillRefBitmap13154 = 13154;
+    public long fillRefBitmap13155 = 13155;
+    public long fillRefBitmap13156 = 13156;
+    public long fillRefBitmap13157 = 13157;
+    public long fillRefBitmap13158 = 13158;
+    public long fillRefBitmap13159 = 13159;
+    public long fillRefBitmap13160 = 13160;
+    public long fillRefBitmap13161 = 13161;
+    public long fillRefBitmap13162 = 13162;
+    public long fillRefBitmap13163 = 13163;
+    public long fillRefBitmap13164 = 13164;
+    public long fillRefBitmap13165 = 13165;
+    public long fillRefBitmap13166 = 13166;
+    public long fillRefBitmap13167 = 13167;
+    public long fillRefBitmap13168 = 13168;
+    public long fillRefBitmap13169 = 13169;
+    public long fillRefBitmap13170 = 13170;
+    public long fillRefBitmap13171 = 13171;
+    public long fillRefBitmap13172 = 13172;
+    public long fillRefBitmap13173 = 13173;
+    public long fillRefBitmap13174 = 13174;
+    public long fillRefBitmap13175 = 13175;
+    public long fillRefBitmap13176 = 13176;
+    public long fillRefBitmap13177 = 13177;
+    public long fillRefBitmap13178 = 13178;
+    public long fillRefBitmap13179 = 13179;
+    public long fillRefBitmap13180 = 13180;
+    public long fillRefBitmap13181 = 13181;
+    public long fillRefBitmap13182 = 13182;
+    public long fillRefBitmap13183 = 13183;
+    public long fillRefBitmap13184 = 13184;
+    public long fillRefBitmap13185 = 13185;
+    public long fillRefBitmap13186 = 13186;
+    public long fillRefBitmap13187 = 13187;
+    public long fillRefBitmap13188 = 13188;
+    public long fillRefBitmap13189 = 13189;
+    public long fillRefBitmap13190 = 13190;
+    public long fillRefBitmap13191 = 13191;
+    public long fillRefBitmap13192 = 13192;
+    public long fillRefBitmap13193 = 13193;
+    public long fillRefBitmap13194 = 13194;
+    public long fillRefBitmap13195 = 13195;
+    public long fillRefBitmap13196 = 13196;
+    public long fillRefBitmap13197 = 13197;
+    public long fillRefBitmap13198 = 13198;
+    public long fillRefBitmap13199 = 13199;
+    public long fillRefBitmap13200 = 13200;
+    public long fillRefBitmap13201 = 13201;
+    public long fillRefBitmap13202 = 13202;
+    public long fillRefBitmap13203 = 13203;
+    public long fillRefBitmap13204 = 13204;
+    public long fillRefBitmap13205 = 13205;
+    public long fillRefBitmap13206 = 13206;
+    public long fillRefBitmap13207 = 13207;
+    public long fillRefBitmap13208 = 13208;
+    public long fillRefBitmap13209 = 13209;
+    public long fillRefBitmap13210 = 13210;
+    public long fillRefBitmap13211 = 13211;
+    public long fillRefBitmap13212 = 13212;
+    public long fillRefBitmap13213 = 13213;
+    public long fillRefBitmap13214 = 13214;
+    public long fillRefBitmap13215 = 13215;
+    public long fillRefBitmap13216 = 13216;
+    public long fillRefBitmap13217 = 13217;
+    public long fillRefBitmap13218 = 13218;
+    public long fillRefBitmap13219 = 13219;
+    public long fillRefBitmap13220 = 13220;
+    public long fillRefBitmap13221 = 13221;
+    public long fillRefBitmap13222 = 13222;
+    public long fillRefBitmap13223 = 13223;
+    public long fillRefBitmap13224 = 13224;
+    public long fillRefBitmap13225 = 13225;
+    public long fillRefBitmap13226 = 13226;
+    public long fillRefBitmap13227 = 13227;
+    public long fillRefBitmap13228 = 13228;
+    public long fillRefBitmap13229 = 13229;
+    public long fillRefBitmap13230 = 13230;
+    public long fillRefBitmap13231 = 13231;
+    public long fillRefBitmap13232 = 13232;
+    public long fillRefBitmap13233 = 13233;
+    public long fillRefBitmap13234 = 13234;
+    public long fillRefBitmap13235 = 13235;
+    public long fillRefBitmap13236 = 13236;
+    public long fillRefBitmap13237 = 13237;
+    public long fillRefBitmap13238 = 13238;
+    public long fillRefBitmap13239 = 13239;
+    public long fillRefBitmap13240 = 13240;
+    public long fillRefBitmap13241 = 13241;
+    public long fillRefBitmap13242 = 13242;
+    public long fillRefBitmap13243 = 13243;
+    public long fillRefBitmap13244 = 13244;
+    public long fillRefBitmap13245 = 13245;
+    public long fillRefBitmap13246 = 13246;
+    public long fillRefBitmap13247 = 13247;
+    public long fillRefBitmap13248 = 13248;
+    public long fillRefBitmap13249 = 13249;
+    public long fillRefBitmap13250 = 13250;
+    public long fillRefBitmap13251 = 13251;
+    public long fillRefBitmap13252 = 13252;
+    public long fillRefBitmap13253 = 13253;
+    public long fillRefBitmap13254 = 13254;
+    public long fillRefBitmap13255 = 13255;
+    public long fillRefBitmap13256 = 13256;
+    public long fillRefBitmap13257 = 13257;
+    public long fillRefBitmap13258 = 13258;
+    public long fillRefBitmap13259 = 13259;
+    public long fillRefBitmap13260 = 13260;
+    public long fillRefBitmap13261 = 13261;
+    public long fillRefBitmap13262 = 13262;
+    public long fillRefBitmap13263 = 13263;
+    public long fillRefBitmap13264 = 13264;
+    public long fillRefBitmap13265 = 13265;
+    public long fillRefBitmap13266 = 13266;
+    public long fillRefBitmap13267 = 13267;
+    public long fillRefBitmap13268 = 13268;
+    public long fillRefBitmap13269 = 13269;
+    public long fillRefBitmap13270 = 13270;
+    public long fillRefBitmap13271 = 13271;
+    public long fillRefBitmap13272 = 13272;
+    public long fillRefBitmap13273 = 13273;
+    public long fillRefBitmap13274 = 13274;
+    public long fillRefBitmap13275 = 13275;
+    public long fillRefBitmap13276 = 13276;
+    public long fillRefBitmap13277 = 13277;
+    public long fillRefBitmap13278 = 13278;
+    public long fillRefBitmap13279 = 13279;
+    public long fillRefBitmap13280 = 13280;
+    public long fillRefBitmap13281 = 13281;
+    public long fillRefBitmap13282 = 13282;
+    public long fillRefBitmap13283 = 13283;
+    public long fillRefBitmap13284 = 13284;
+    public long fillRefBitmap13285 = 13285;
+    public long fillRefBitmap13286 = 13286;
+    public long fillRefBitmap13287 = 13287;
+    public long fillRefBitmap13288 = 13288;
+    public long fillRefBitmap13289 = 13289;
+    public long fillRefBitmap13290 = 13290;
+    public long fillRefBitmap13291 = 13291;
+    public long fillRefBitmap13292 = 13292;
+    public long fillRefBitmap13293 = 13293;
+    public long fillRefBitmap13294 = 13294;
+    public long fillRefBitmap13295 = 13295;
+    public long fillRefBitmap13296 = 13296;
+    public long fillRefBitmap13297 = 13297;
+    public long fillRefBitmap13298 = 13298;
+    public long fillRefBitmap13299 = 13299;
+    public long fillRefBitmap13300 = 13300;
+    public long fillRefBitmap13301 = 13301;
+    public long fillRefBitmap13302 = 13302;
+    public long fillRefBitmap13303 = 13303;
+    public long fillRefBitmap13304 = 13304;
+    public long fillRefBitmap13305 = 13305;
+    public long fillRefBitmap13306 = 13306;
+    public long fillRefBitmap13307 = 13307;
+    public long fillRefBitmap13308 = 13308;
+    public long fillRefBitmap13309 = 13309;
+    public long fillRefBitmap13310 = 13310;
+    public long fillRefBitmap13311 = 13311;
+    public long fillRefBitmap13312 = 13312;
+    public long fillRefBitmap13313 = 13313;
+    public long fillRefBitmap13314 = 13314;
+    public long fillRefBitmap13315 = 13315;
+    public long fillRefBitmap13316 = 13316;
+    public long fillRefBitmap13317 = 13317;
+    public long fillRefBitmap13318 = 13318;
+    public long fillRefBitmap13319 = 13319;
+    public long fillRefBitmap13320 = 13320;
+    public long fillRefBitmap13321 = 13321;
+    public long fillRefBitmap13322 = 13322;
+    public long fillRefBitmap13323 = 13323;
+    public long fillRefBitmap13324 = 13324;
+    public long fillRefBitmap13325 = 13325;
+    public long fillRefBitmap13326 = 13326;
+    public long fillRefBitmap13327 = 13327;
+    public long fillRefBitmap13328 = 13328;
+    public long fillRefBitmap13329 = 13329;
+    public long fillRefBitmap13330 = 13330;
+    public long fillRefBitmap13331 = 13331;
+    public long fillRefBitmap13332 = 13332;
+    public long fillRefBitmap13333 = 13333;
+    public long fillRefBitmap13334 = 13334;
+    public long fillRefBitmap13335 = 13335;
+    public long fillRefBitmap13336 = 13336;
+    public long fillRefBitmap13337 = 13337;
+    public long fillRefBitmap13338 = 13338;
+    public long fillRefBitmap13339 = 13339;
+    public long fillRefBitmap13340 = 13340;
+    public long fillRefBitmap13341 = 13341;
+    public long fillRefBitmap13342 = 13342;
+    public long fillRefBitmap13343 = 13343;
+    public long fillRefBitmap13344 = 13344;
+    public long fillRefBitmap13345 = 13345;
+    public long fillRefBitmap13346 = 13346;
+    public long fillRefBitmap13347 = 13347;
+    public long fillRefBitmap13348 = 13348;
+    public long fillRefBitmap13349 = 13349;
+    public long fillRefBitmap13350 = 13350;
+    public long fillRefBitmap13351 = 13351;
+    public long fillRefBitmap13352 = 13352;
+    public long fillRefBitmap13353 = 13353;
+    public long fillRefBitmap13354 = 13354;
+    public long fillRefBitmap13355 = 13355;
+    public long fillRefBitmap13356 = 13356;
+    public long fillRefBitmap13357 = 13357;
+    public long fillRefBitmap13358 = 13358;
+    public long fillRefBitmap13359 = 13359;
+    public long fillRefBitmap13360 = 13360;
+    public long fillRefBitmap13361 = 13361;
+    public long fillRefBitmap13362 = 13362;
+    public long fillRefBitmap13363 = 13363;
+    public long fillRefBitmap13364 = 13364;
+    public long fillRefBitmap13365 = 13365;
+    public long fillRefBitmap13366 = 13366;
+    public long fillRefBitmap13367 = 13367;
+    public long fillRefBitmap13368 = 13368;
+    public long fillRefBitmap13369 = 13369;
+    public long fillRefBitmap13370 = 13370;
+    public long fillRefBitmap13371 = 13371;
+    public long fillRefBitmap13372 = 13372;
+    public long fillRefBitmap13373 = 13373;
+    public long fillRefBitmap13374 = 13374;
+    public long fillRefBitmap13375 = 13375;
+    public long fillRefBitmap13376 = 13376;
+    public long fillRefBitmap13377 = 13377;
+    public long fillRefBitmap13378 = 13378;
+    public long fillRefBitmap13379 = 13379;
+    public long fillRefBitmap13380 = 13380;
+    public long fillRefBitmap13381 = 13381;
+    public long fillRefBitmap13382 = 13382;
+    public long fillRefBitmap13383 = 13383;
+    public long fillRefBitmap13384 = 13384;
+    public long fillRefBitmap13385 = 13385;
+    public long fillRefBitmap13386 = 13386;
+    public long fillRefBitmap13387 = 13387;
+    public long fillRefBitmap13388 = 13388;
+    public long fillRefBitmap13389 = 13389;
+    public long fillRefBitmap13390 = 13390;
+    public long fillRefBitmap13391 = 13391;
+    public long fillRefBitmap13392 = 13392;
+    public long fillRefBitmap13393 = 13393;
+    public long fillRefBitmap13394 = 13394;
+    public long fillRefBitmap13395 = 13395;
+    public long fillRefBitmap13396 = 13396;
+    public long fillRefBitmap13397 = 13397;
+    public long fillRefBitmap13398 = 13398;
+    public long fillRefBitmap13399 = 13399;
+    public long fillRefBitmap13400 = 13400;
+    public long fillRefBitmap13401 = 13401;
+    public long fillRefBitmap13402 = 13402;
+    public long fillRefBitmap13403 = 13403;
+    public long fillRefBitmap13404 = 13404;
+    public long fillRefBitmap13405 = 13405;
+    public long fillRefBitmap13406 = 13406;
+    public long fillRefBitmap13407 = 13407;
+    public long fillRefBitmap13408 = 13408;
+    public long fillRefBitmap13409 = 13409;
+    public long fillRefBitmap13410 = 13410;
+    public long fillRefBitmap13411 = 13411;
+    public long fillRefBitmap13412 = 13412;
+    public long fillRefBitmap13413 = 13413;
+    public long fillRefBitmap13414 = 13414;
+    public long fillRefBitmap13415 = 13415;
+    public long fillRefBitmap13416 = 13416;
+    public long fillRefBitmap13417 = 13417;
+    public long fillRefBitmap13418 = 13418;
+    public long fillRefBitmap13419 = 13419;
+    public long fillRefBitmap13420 = 13420;
+    public long fillRefBitmap13421 = 13421;
+    public long fillRefBitmap13422 = 13422;
+    public long fillRefBitmap13423 = 13423;
+    public long fillRefBitmap13424 = 13424;
+    public long fillRefBitmap13425 = 13425;
+    public long fillRefBitmap13426 = 13426;
+    public long fillRefBitmap13427 = 13427;
+    public long fillRefBitmap13428 = 13428;
+    public long fillRefBitmap13429 = 13429;
+    public long fillRefBitmap13430 = 13430;
+    public long fillRefBitmap13431 = 13431;
+    public long fillRefBitmap13432 = 13432;
+    public long fillRefBitmap13433 = 13433;
+    public long fillRefBitmap13434 = 13434;
+    public long fillRefBitmap13435 = 13435;
+    public long fillRefBitmap13436 = 13436;
+    public long fillRefBitmap13437 = 13437;
+    public long fillRefBitmap13438 = 13438;
+    public long fillRefBitmap13439 = 13439;
+    public long fillRefBitmap13440 = 13440;
+    public long fillRefBitmap13441 = 13441;
+    public long fillRefBitmap13442 = 13442;
+    public long fillRefBitmap13443 = 13443;
+    public long fillRefBitmap13444 = 13444;
+    public long fillRefBitmap13445 = 13445;
+    public long fillRefBitmap13446 = 13446;
+    public long fillRefBitmap13447 = 13447;
+    public long fillRefBitmap13448 = 13448;
+    public long fillRefBitmap13449 = 13449;
+    public long fillRefBitmap13450 = 13450;
+    public long fillRefBitmap13451 = 13451;
+    public long fillRefBitmap13452 = 13452;
+    public long fillRefBitmap13453 = 13453;
+    public long fillRefBitmap13454 = 13454;
+    public long fillRefBitmap13455 = 13455;
+    public long fillRefBitmap13456 = 13456;
+    public long fillRefBitmap13457 = 13457;
+    public long fillRefBitmap13458 = 13458;
+    public long fillRefBitmap13459 = 13459;
+    public long fillRefBitmap13460 = 13460;
+    public long fillRefBitmap13461 = 13461;
+    public long fillRefBitmap13462 = 13462;
+    public long fillRefBitmap13463 = 13463;
+    public long fillRefBitmap13464 = 13464;
+    public long fillRefBitmap13465 = 13465;
+    public long fillRefBitmap13466 = 13466;
+    public long fillRefBitmap13467 = 13467;
+    public long fillRefBitmap13468 = 13468;
+    public long fillRefBitmap13469 = 13469;
+    public long fillRefBitmap13470 = 13470;
+    public long fillRefBitmap13471 = 13471;
+    public long fillRefBitmap13472 = 13472;
+    public long fillRefBitmap13473 = 13473;
+    public long fillRefBitmap13474 = 13474;
+    public long fillRefBitmap13475 = 13475;
+    public long fillRefBitmap13476 = 13476;
+    public long fillRefBitmap13477 = 13477;
+    public long fillRefBitmap13478 = 13478;
+    public long fillRefBitmap13479 = 13479;
+    public long fillRefBitmap13480 = 13480;
+    public long fillRefBitmap13481 = 13481;
+    public long fillRefBitmap13482 = 13482;
+    public long fillRefBitmap13483 = 13483;
+    public long fillRefBitmap13484 = 13484;
+    public long fillRefBitmap13485 = 13485;
+    public long fillRefBitmap13486 = 13486;
+    public long fillRefBitmap13487 = 13487;
+    public long fillRefBitmap13488 = 13488;
+    public long fillRefBitmap13489 = 13489;
+    public long fillRefBitmap13490 = 13490;
+    public long fillRefBitmap13491 = 13491;
+    public long fillRefBitmap13492 = 13492;
+    public long fillRefBitmap13493 = 13493;
+    public long fillRefBitmap13494 = 13494;
+    public long fillRefBitmap13495 = 13495;
+    public long fillRefBitmap13496 = 13496;
+    public long fillRefBitmap13497 = 13497;
+    public long fillRefBitmap13498 = 13498;
+    public long fillRefBitmap13499 = 13499;
+    public long fillRefBitmap13500 = 13500;
+    public long fillRefBitmap13501 = 13501;
+    public long fillRefBitmap13502 = 13502;
+    public long fillRefBitmap13503 = 13503;
+    public long fillRefBitmap13504 = 13504;
+    public long fillRefBitmap13505 = 13505;
+    public long fillRefBitmap13506 = 13506;
+    public long fillRefBitmap13507 = 13507;
+    public long fillRefBitmap13508 = 13508;
+    public long fillRefBitmap13509 = 13509;
+    public long fillRefBitmap13510 = 13510;
+    public long fillRefBitmap13511 = 13511;
+    public long fillRefBitmap13512 = 13512;
+    public long fillRefBitmap13513 = 13513;
+    public long fillRefBitmap13514 = 13514;
+    public long fillRefBitmap13515 = 13515;
+    public long fillRefBitmap13516 = 13516;
+    public long fillRefBitmap13517 = 13517;
+    public long fillRefBitmap13518 = 13518;
+    public long fillRefBitmap13519 = 13519;
+    public long fillRefBitmap13520 = 13520;
+    public long fillRefBitmap13521 = 13521;
+    public long fillRefBitmap13522 = 13522;
+    public long fillRefBitmap13523 = 13523;
+    public long fillRefBitmap13524 = 13524;
+    public long fillRefBitmap13525 = 13525;
+    public long fillRefBitmap13526 = 13526;
+    public long fillRefBitmap13527 = 13527;
+    public long fillRefBitmap13528 = 13528;
+    public long fillRefBitmap13529 = 13529;
+    public long fillRefBitmap13530 = 13530;
+    public long fillRefBitmap13531 = 13531;
+    public long fillRefBitmap13532 = 13532;
+    public long fillRefBitmap13533 = 13533;
+    public long fillRefBitmap13534 = 13534;
+    public long fillRefBitmap13535 = 13535;
+    public long fillRefBitmap13536 = 13536;
+    public long fillRefBitmap13537 = 13537;
+    public long fillRefBitmap13538 = 13538;
+    public long fillRefBitmap13539 = 13539;
+    public long fillRefBitmap13540 = 13540;
+    public long fillRefBitmap13541 = 13541;
+    public long fillRefBitmap13542 = 13542;
+    public long fillRefBitmap13543 = 13543;
+    public long fillRefBitmap13544 = 13544;
+    public long fillRefBitmap13545 = 13545;
+    public long fillRefBitmap13546 = 13546;
+    public long fillRefBitmap13547 = 13547;
+    public long fillRefBitmap13548 = 13548;
+    public long fillRefBitmap13549 = 13549;
+    public long fillRefBitmap13550 = 13550;
+    public long fillRefBitmap13551 = 13551;
+    public long fillRefBitmap13552 = 13552;
+    public long fillRefBitmap13553 = 13553;
+    public long fillRefBitmap13554 = 13554;
+    public long fillRefBitmap13555 = 13555;
+    public long fillRefBitmap13556 = 13556;
+    public long fillRefBitmap13557 = 13557;
+    public long fillRefBitmap13558 = 13558;
+    public long fillRefBitmap13559 = 13559;
+    public long fillRefBitmap13560 = 13560;
+    public long fillRefBitmap13561 = 13561;
+    public long fillRefBitmap13562 = 13562;
+    public long fillRefBitmap13563 = 13563;
+    public long fillRefBitmap13564 = 13564;
+    public long fillRefBitmap13565 = 13565;
+    public long fillRefBitmap13566 = 13566;
+    public long fillRefBitmap13567 = 13567;
+    public long fillRefBitmap13568 = 13568;
+    public long fillRefBitmap13569 = 13569;
+    public long fillRefBitmap13570 = 13570;
+    public long fillRefBitmap13571 = 13571;
+    public long fillRefBitmap13572 = 13572;
+    public long fillRefBitmap13573 = 13573;
+    public long fillRefBitmap13574 = 13574;
+    public long fillRefBitmap13575 = 13575;
+    public long fillRefBitmap13576 = 13576;
+    public long fillRefBitmap13577 = 13577;
+    public long fillRefBitmap13578 = 13578;
+    public long fillRefBitmap13579 = 13579;
+    public long fillRefBitmap13580 = 13580;
+    public long fillRefBitmap13581 = 13581;
+    public long fillRefBitmap13582 = 13582;
+    public long fillRefBitmap13583 = 13583;
+    public long fillRefBitmap13584 = 13584;
+    public long fillRefBitmap13585 = 13585;
+    public long fillRefBitmap13586 = 13586;
+    public long fillRefBitmap13587 = 13587;
+    public long fillRefBitmap13588 = 13588;
+    public long fillRefBitmap13589 = 13589;
+    public long fillRefBitmap13590 = 13590;
+    public long fillRefBitmap13591 = 13591;
+    public long fillRefBitmap13592 = 13592;
+    public long fillRefBitmap13593 = 13593;
+    public long fillRefBitmap13594 = 13594;
+    public long fillRefBitmap13595 = 13595;
+    public long fillRefBitmap13596 = 13596;
+    public long fillRefBitmap13597 = 13597;
+    public long fillRefBitmap13598 = 13598;
+    public long fillRefBitmap13599 = 13599;
+    public long fillRefBitmap13600 = 13600;
+    public long fillRefBitmap13601 = 13601;
+    public long fillRefBitmap13602 = 13602;
+    public long fillRefBitmap13603 = 13603;
+    public long fillRefBitmap13604 = 13604;
+    public long fillRefBitmap13605 = 13605;
+    public long fillRefBitmap13606 = 13606;
+    public long fillRefBitmap13607 = 13607;
+    public long fillRefBitmap13608 = 13608;
+    public long fillRefBitmap13609 = 13609;
+    public long fillRefBitmap13610 = 13610;
+    public long fillRefBitmap13611 = 13611;
+    public long fillRefBitmap13612 = 13612;
+    public long fillRefBitmap13613 = 13613;
+    public long fillRefBitmap13614 = 13614;
+    public long fillRefBitmap13615 = 13615;
+    public long fillRefBitmap13616 = 13616;
+    public long fillRefBitmap13617 = 13617;
+    public long fillRefBitmap13618 = 13618;
+    public long fillRefBitmap13619 = 13619;
+    public long fillRefBitmap13620 = 13620;
+    public long fillRefBitmap13621 = 13621;
+    public long fillRefBitmap13622 = 13622;
+    public long fillRefBitmap13623 = 13623;
+    public long fillRefBitmap13624 = 13624;
+    public long fillRefBitmap13625 = 13625;
+    public long fillRefBitmap13626 = 13626;
+    public long fillRefBitmap13627 = 13627;
+    public long fillRefBitmap13628 = 13628;
+    public long fillRefBitmap13629 = 13629;
+    public long fillRefBitmap13630 = 13630;
+    public long fillRefBitmap13631 = 13631;
+    public long fillRefBitmap13632 = 13632;
+    public long fillRefBitmap13633 = 13633;
+    public long fillRefBitmap13634 = 13634;
+    public long fillRefBitmap13635 = 13635;
+    public long fillRefBitmap13636 = 13636;
+    public long fillRefBitmap13637 = 13637;
+    public long fillRefBitmap13638 = 13638;
+    public long fillRefBitmap13639 = 13639;
+    public long fillRefBitmap13640 = 13640;
+    public long fillRefBitmap13641 = 13641;
+    public long fillRefBitmap13642 = 13642;
+    public long fillRefBitmap13643 = 13643;
+    public long fillRefBitmap13644 = 13644;
+    public long fillRefBitmap13645 = 13645;
+    public long fillRefBitmap13646 = 13646;
+    public long fillRefBitmap13647 = 13647;
+    public long fillRefBitmap13648 = 13648;
+    public long fillRefBitmap13649 = 13649;
+    public long fillRefBitmap13650 = 13650;
+    public long fillRefBitmap13651 = 13651;
+    public long fillRefBitmap13652 = 13652;
+    public long fillRefBitmap13653 = 13653;
+    public long fillRefBitmap13654 = 13654;
+    public long fillRefBitmap13655 = 13655;
+    public long fillRefBitmap13656 = 13656;
+    public long fillRefBitmap13657 = 13657;
+    public long fillRefBitmap13658 = 13658;
+    public long fillRefBitmap13659 = 13659;
+    public long fillRefBitmap13660 = 13660;
+    public long fillRefBitmap13661 = 13661;
+    public long fillRefBitmap13662 = 13662;
+    public long fillRefBitmap13663 = 13663;
+    public long fillRefBitmap13664 = 13664;
+    public long fillRefBitmap13665 = 13665;
+    public long fillRefBitmap13666 = 13666;
+    public long fillRefBitmap13667 = 13667;
+    public long fillRefBitmap13668 = 13668;
+    public long fillRefBitmap13669 = 13669;
+    public long fillRefBitmap13670 = 13670;
+    public long fillRefBitmap13671 = 13671;
+    public long fillRefBitmap13672 = 13672;
+    public long fillRefBitmap13673 = 13673;
+    public long fillRefBitmap13674 = 13674;
+    public long fillRefBitmap13675 = 13675;
+    public long fillRefBitmap13676 = 13676;
+    public long fillRefBitmap13677 = 13677;
+    public long fillRefBitmap13678 = 13678;
+    public long fillRefBitmap13679 = 13679;
+    public long fillRefBitmap13680 = 13680;
+    public long fillRefBitmap13681 = 13681;
+    public long fillRefBitmap13682 = 13682;
+    public long fillRefBitmap13683 = 13683;
+    public long fillRefBitmap13684 = 13684;
+    public long fillRefBitmap13685 = 13685;
+    public long fillRefBitmap13686 = 13686;
+    public long fillRefBitmap13687 = 13687;
+    public long fillRefBitmap13688 = 13688;
+    public long fillRefBitmap13689 = 13689;
+    public long fillRefBitmap13690 = 13690;
+    public long fillRefBitmap13691 = 13691;
+    public long fillRefBitmap13692 = 13692;
+    public long fillRefBitmap13693 = 13693;
+    public long fillRefBitmap13694 = 13694;
+    public long fillRefBitmap13695 = 13695;
+    public long fillRefBitmap13696 = 13696;
+    public long fillRefBitmap13697 = 13697;
+    public long fillRefBitmap13698 = 13698;
+    public long fillRefBitmap13699 = 13699;
+    public long fillRefBitmap13700 = 13700;
+    public long fillRefBitmap13701 = 13701;
+    public long fillRefBitmap13702 = 13702;
+    public long fillRefBitmap13703 = 13703;
+    public long fillRefBitmap13704 = 13704;
+    public long fillRefBitmap13705 = 13705;
+    public long fillRefBitmap13706 = 13706;
+    public long fillRefBitmap13707 = 13707;
+    public long fillRefBitmap13708 = 13708;
+    public long fillRefBitmap13709 = 13709;
+    public long fillRefBitmap13710 = 13710;
+    public long fillRefBitmap13711 = 13711;
+    public long fillRefBitmap13712 = 13712;
+    public long fillRefBitmap13713 = 13713;
+    public long fillRefBitmap13714 = 13714;
+    public long fillRefBitmap13715 = 13715;
+    public long fillRefBitmap13716 = 13716;
+    public long fillRefBitmap13717 = 13717;
+    public long fillRefBitmap13718 = 13718;
+    public long fillRefBitmap13719 = 13719;
+    public long fillRefBitmap13720 = 13720;
+    public long fillRefBitmap13721 = 13721;
+    public long fillRefBitmap13722 = 13722;
+    public long fillRefBitmap13723 = 13723;
+    public long fillRefBitmap13724 = 13724;
+    public long fillRefBitmap13725 = 13725;
+    public long fillRefBitmap13726 = 13726;
+    public long fillRefBitmap13727 = 13727;
+    public long fillRefBitmap13728 = 13728;
+    public long fillRefBitmap13729 = 13729;
+    public long fillRefBitmap13730 = 13730;
+    public long fillRefBitmap13731 = 13731;
+    public long fillRefBitmap13732 = 13732;
+    public long fillRefBitmap13733 = 13733;
+    public long fillRefBitmap13734 = 13734;
+    public long fillRefBitmap13735 = 13735;
+    public long fillRefBitmap13736 = 13736;
+    public long fillRefBitmap13737 = 13737;
+    public long fillRefBitmap13738 = 13738;
+    public long fillRefBitmap13739 = 13739;
+    public long fillRefBitmap13740 = 13740;
+    public long fillRefBitmap13741 = 13741;
+    public long fillRefBitmap13742 = 13742;
+    public long fillRefBitmap13743 = 13743;
+    public long fillRefBitmap13744 = 13744;
+    public long fillRefBitmap13745 = 13745;
+    public long fillRefBitmap13746 = 13746;
+    public long fillRefBitmap13747 = 13747;
+    public long fillRefBitmap13748 = 13748;
+    public long fillRefBitmap13749 = 13749;
+    public long fillRefBitmap13750 = 13750;
+    public long fillRefBitmap13751 = 13751;
+    public long fillRefBitmap13752 = 13752;
+    public long fillRefBitmap13753 = 13753;
+    public long fillRefBitmap13754 = 13754;
+    public long fillRefBitmap13755 = 13755;
+    public long fillRefBitmap13756 = 13756;
+    public long fillRefBitmap13757 = 13757;
+    public long fillRefBitmap13758 = 13758;
+    public long fillRefBitmap13759 = 13759;
+    public long fillRefBitmap13760 = 13760;
+    public long fillRefBitmap13761 = 13761;
+    public long fillRefBitmap13762 = 13762;
+    public long fillRefBitmap13763 = 13763;
+    public long fillRefBitmap13764 = 13764;
+    public long fillRefBitmap13765 = 13765;
+    public long fillRefBitmap13766 = 13766;
+    public long fillRefBitmap13767 = 13767;
+    public long fillRefBitmap13768 = 13768;
+    public long fillRefBitmap13769 = 13769;
+    public long fillRefBitmap13770 = 13770;
+    public long fillRefBitmap13771 = 13771;
+    public long fillRefBitmap13772 = 13772;
+    public long fillRefBitmap13773 = 13773;
+    public long fillRefBitmap13774 = 13774;
+    public long fillRefBitmap13775 = 13775;
+    public long fillRefBitmap13776 = 13776;
+    public long fillRefBitmap13777 = 13777;
+    public long fillRefBitmap13778 = 13778;
+    public long fillRefBitmap13779 = 13779;
+    public long fillRefBitmap13780 = 13780;
+    public long fillRefBitmap13781 = 13781;
+    public long fillRefBitmap13782 = 13782;
+    public long fillRefBitmap13783 = 13783;
+    public long fillRefBitmap13784 = 13784;
+    public long fillRefBitmap13785 = 13785;
+    public long fillRefBitmap13786 = 13786;
+    public long fillRefBitmap13787 = 13787;
+    public long fillRefBitmap13788 = 13788;
+    public long fillRefBitmap13789 = 13789;
+    public long fillRefBitmap13790 = 13790;
+    public long fillRefBitmap13791 = 13791;
+    public long fillRefBitmap13792 = 13792;
+    public long fillRefBitmap13793 = 13793;
+    public long fillRefBitmap13794 = 13794;
+    public long fillRefBitmap13795 = 13795;
+    public long fillRefBitmap13796 = 13796;
+    public long fillRefBitmap13797 = 13797;
+    public long fillRefBitmap13798 = 13798;
+    public long fillRefBitmap13799 = 13799;
+    public long fillRefBitmap13800 = 13800;
+    public long fillRefBitmap13801 = 13801;
+    public long fillRefBitmap13802 = 13802;
+    public long fillRefBitmap13803 = 13803;
+    public long fillRefBitmap13804 = 13804;
+    public long fillRefBitmap13805 = 13805;
+    public long fillRefBitmap13806 = 13806;
+    public long fillRefBitmap13807 = 13807;
+    public long fillRefBitmap13808 = 13808;
+    public long fillRefBitmap13809 = 13809;
+    public long fillRefBitmap13810 = 13810;
+    public long fillRefBitmap13811 = 13811;
+    public long fillRefBitmap13812 = 13812;
+    public long fillRefBitmap13813 = 13813;
+    public long fillRefBitmap13814 = 13814;
+    public long fillRefBitmap13815 = 13815;
+    public long fillRefBitmap13816 = 13816;
+    public long fillRefBitmap13817 = 13817;
+    public long fillRefBitmap13818 = 13818;
+    public long fillRefBitmap13819 = 13819;
+    public long fillRefBitmap13820 = 13820;
+    public long fillRefBitmap13821 = 13821;
+    public long fillRefBitmap13822 = 13822;
+    public long fillRefBitmap13823 = 13823;
+    public long fillRefBitmap13824 = 13824;
+    public long fillRefBitmap13825 = 13825;
+    public long fillRefBitmap13826 = 13826;
+    public long fillRefBitmap13827 = 13827;
+    public long fillRefBitmap13828 = 13828;
+    public long fillRefBitmap13829 = 13829;
+    public long fillRefBitmap13830 = 13830;
+    public long fillRefBitmap13831 = 13831;
+    public long fillRefBitmap13832 = 13832;
+    public long fillRefBitmap13833 = 13833;
+    public long fillRefBitmap13834 = 13834;
+    public long fillRefBitmap13835 = 13835;
+    public long fillRefBitmap13836 = 13836;
+    public long fillRefBitmap13837 = 13837;
+    public long fillRefBitmap13838 = 13838;
+    public long fillRefBitmap13839 = 13839;
+    public long fillRefBitmap13840 = 13840;
+    public long fillRefBitmap13841 = 13841;
+    public long fillRefBitmap13842 = 13842;
+    public long fillRefBitmap13843 = 13843;
+    public long fillRefBitmap13844 = 13844;
+    public long fillRefBitmap13845 = 13845;
+    public long fillRefBitmap13846 = 13846;
+    public long fillRefBitmap13847 = 13847;
+    public long fillRefBitmap13848 = 13848;
+    public long fillRefBitmap13849 = 13849;
+    public long fillRefBitmap13850 = 13850;
+    public long fillRefBitmap13851 = 13851;
+    public long fillRefBitmap13852 = 13852;
+    public long fillRefBitmap13853 = 13853;
+    public long fillRefBitmap13854 = 13854;
+    public long fillRefBitmap13855 = 13855;
+    public long fillRefBitmap13856 = 13856;
+    public long fillRefBitmap13857 = 13857;
+    public long fillRefBitmap13858 = 13858;
+    public long fillRefBitmap13859 = 13859;
+    public long fillRefBitmap13860 = 13860;
+    public long fillRefBitmap13861 = 13861;
+    public long fillRefBitmap13862 = 13862;
+    public long fillRefBitmap13863 = 13863;
+    public long fillRefBitmap13864 = 13864;
+    public long fillRefBitmap13865 = 13865;
+    public long fillRefBitmap13866 = 13866;
+    public long fillRefBitmap13867 = 13867;
+    public long fillRefBitmap13868 = 13868;
+    public long fillRefBitmap13869 = 13869;
+    public long fillRefBitmap13870 = 13870;
+    public long fillRefBitmap13871 = 13871;
+    public long fillRefBitmap13872 = 13872;
+    public long fillRefBitmap13873 = 13873;
+    public long fillRefBitmap13874 = 13874;
+    public long fillRefBitmap13875 = 13875;
+    public long fillRefBitmap13876 = 13876;
+    public long fillRefBitmap13877 = 13877;
+    public long fillRefBitmap13878 = 13878;
+    public long fillRefBitmap13879 = 13879;
+    public long fillRefBitmap13880 = 13880;
+    public long fillRefBitmap13881 = 13881;
+    public long fillRefBitmap13882 = 13882;
+    public long fillRefBitmap13883 = 13883;
+    public long fillRefBitmap13884 = 13884;
+    public long fillRefBitmap13885 = 13885;
+    public long fillRefBitmap13886 = 13886;
+    public long fillRefBitmap13887 = 13887;
+    public long fillRefBitmap13888 = 13888;
+    public long fillRefBitmap13889 = 13889;
+    public long fillRefBitmap13890 = 13890;
+    public long fillRefBitmap13891 = 13891;
+    public long fillRefBitmap13892 = 13892;
+    public long fillRefBitmap13893 = 13893;
+    public long fillRefBitmap13894 = 13894;
+    public long fillRefBitmap13895 = 13895;
+    public long fillRefBitmap13896 = 13896;
+    public long fillRefBitmap13897 = 13897;
+    public long fillRefBitmap13898 = 13898;
+    public long fillRefBitmap13899 = 13899;
+    public long fillRefBitmap13900 = 13900;
+    public long fillRefBitmap13901 = 13901;
+    public long fillRefBitmap13902 = 13902;
+    public long fillRefBitmap13903 = 13903;
+    public long fillRefBitmap13904 = 13904;
+    public long fillRefBitmap13905 = 13905;
+    public long fillRefBitmap13906 = 13906;
+    public long fillRefBitmap13907 = 13907;
+    public long fillRefBitmap13908 = 13908;
+    public long fillRefBitmap13909 = 13909;
+    public long fillRefBitmap13910 = 13910;
+    public long fillRefBitmap13911 = 13911;
+    public long fillRefBitmap13912 = 13912;
+    public long fillRefBitmap13913 = 13913;
+    public long fillRefBitmap13914 = 13914;
+    public long fillRefBitmap13915 = 13915;
+    public long fillRefBitmap13916 = 13916;
+    public long fillRefBitmap13917 = 13917;
+    public long fillRefBitmap13918 = 13918;
+    public long fillRefBitmap13919 = 13919;
+    public long fillRefBitmap13920 = 13920;
+    public long fillRefBitmap13921 = 13921;
+    public long fillRefBitmap13922 = 13922;
+    public long fillRefBitmap13923 = 13923;
+    public long fillRefBitmap13924 = 13924;
+    public long fillRefBitmap13925 = 13925;
+    public long fillRefBitmap13926 = 13926;
+    public long fillRefBitmap13927 = 13927;
+    public long fillRefBitmap13928 = 13928;
+    public long fillRefBitmap13929 = 13929;
+    public long fillRefBitmap13930 = 13930;
+    public long fillRefBitmap13931 = 13931;
+    public long fillRefBitmap13932 = 13932;
+    public long fillRefBitmap13933 = 13933;
+    public long fillRefBitmap13934 = 13934;
+    public long fillRefBitmap13935 = 13935;
+    public long fillRefBitmap13936 = 13936;
+    public long fillRefBitmap13937 = 13937;
+    public long fillRefBitmap13938 = 13938;
+    public long fillRefBitmap13939 = 13939;
+    public long fillRefBitmap13940 = 13940;
+    public long fillRefBitmap13941 = 13941;
+    public long fillRefBitmap13942 = 13942;
+    public long fillRefBitmap13943 = 13943;
+    public long fillRefBitmap13944 = 13944;
+    public long fillRefBitmap13945 = 13945;
+    public long fillRefBitmap13946 = 13946;
+    public long fillRefBitmap13947 = 13947;
+    public long fillRefBitmap13948 = 13948;
+    public long fillRefBitmap13949 = 13949;
+    public long fillRefBitmap13950 = 13950;
+    public long fillRefBitmap13951 = 13951;
+    public long fillRefBitmap13952 = 13952;
+    public long fillRefBitmap13953 = 13953;
+    public long fillRefBitmap13954 = 13954;
+    public long fillRefBitmap13955 = 13955;
+    public long fillRefBitmap13956 = 13956;
+    public long fillRefBitmap13957 = 13957;
+    public long fillRefBitmap13958 = 13958;
+    public long fillRefBitmap13959 = 13959;
+    public long fillRefBitmap13960 = 13960;
+    public long fillRefBitmap13961 = 13961;
+    public long fillRefBitmap13962 = 13962;
+    public long fillRefBitmap13963 = 13963;
+    public long fillRefBitmap13964 = 13964;
+    public long fillRefBitmap13965 = 13965;
+    public long fillRefBitmap13966 = 13966;
+    public long fillRefBitmap13967 = 13967;
+    public long fillRefBitmap13968 = 13968;
+    public long fillRefBitmap13969 = 13969;
+    public long fillRefBitmap13970 = 13970;
+    public long fillRefBitmap13971 = 13971;
+    public long fillRefBitmap13972 = 13972;
+    public long fillRefBitmap13973 = 13973;
+    public long fillRefBitmap13974 = 13974;
+    public long fillRefBitmap13975 = 13975;
+    public long fillRefBitmap13976 = 13976;
+    public long fillRefBitmap13977 = 13977;
+    public long fillRefBitmap13978 = 13978;
+    public long fillRefBitmap13979 = 13979;
+    public long fillRefBitmap13980 = 13980;
+    public long fillRefBitmap13981 = 13981;
+    public long fillRefBitmap13982 = 13982;
+    public long fillRefBitmap13983 = 13983;
+    public long fillRefBitmap13984 = 13984;
+    public long fillRefBitmap13985 = 13985;
+    public long fillRefBitmap13986 = 13986;
+    public long fillRefBitmap13987 = 13987;
+    public long fillRefBitmap13988 = 13988;
+    public long fillRefBitmap13989 = 13989;
+    public long fillRefBitmap13990 = 13990;
+    public long fillRefBitmap13991 = 13991;
+    public long fillRefBitmap13992 = 13992;
+    public long fillRefBitmap13993 = 13993;
+    public long fillRefBitmap13994 = 13994;
+    public long fillRefBitmap13995 = 13995;
+    public long fillRefBitmap13996 = 13996;
+    public long fillRefBitmap13997 = 13997;
+    public long fillRefBitmap13998 = 13998;
+    public long fillRefBitmap13999 = 13999;
+    public long fillRefBitmap14000 = 14000;
+    public long fillRefBitmap14001 = 14001;
+    public long fillRefBitmap14002 = 14002;
+    public long fillRefBitmap14003 = 14003;
+    public long fillRefBitmap14004 = 14004;
+    public long fillRefBitmap14005 = 14005;
+    public long fillRefBitmap14006 = 14006;
+    public long fillRefBitmap14007 = 14007;
+    public long fillRefBitmap14008 = 14008;
+    public long fillRefBitmap14009 = 14009;
+    public long fillRefBitmap14010 = 14010;
+    public long fillRefBitmap14011 = 14011;
+    public long fillRefBitmap14012 = 14012;
+    public long fillRefBitmap14013 = 14013;
+    public long fillRefBitmap14014 = 14014;
+    public long fillRefBitmap14015 = 14015;
+    public long fillRefBitmap14016 = 14016;
+    public long fillRefBitmap14017 = 14017;
+    public long fillRefBitmap14018 = 14018;
+    public long fillRefBitmap14019 = 14019;
+    public long fillRefBitmap14020 = 14020;
+    public long fillRefBitmap14021 = 14021;
+    public long fillRefBitmap14022 = 14022;
+    public long fillRefBitmap14023 = 14023;
+    public long fillRefBitmap14024 = 14024;
+    public long fillRefBitmap14025 = 14025;
+    public long fillRefBitmap14026 = 14026;
+    public long fillRefBitmap14027 = 14027;
+    public long fillRefBitmap14028 = 14028;
+    public long fillRefBitmap14029 = 14029;
+    public long fillRefBitmap14030 = 14030;
+    public long fillRefBitmap14031 = 14031;
+    public long fillRefBitmap14032 = 14032;
+    public long fillRefBitmap14033 = 14033;
+    public long fillRefBitmap14034 = 14034;
+    public long fillRefBitmap14035 = 14035;
+    public long fillRefBitmap14036 = 14036;
+    public long fillRefBitmap14037 = 14037;
+    public long fillRefBitmap14038 = 14038;
+    public long fillRefBitmap14039 = 14039;
+    public long fillRefBitmap14040 = 14040;
+    public long fillRefBitmap14041 = 14041;
+    public long fillRefBitmap14042 = 14042;
+    public long fillRefBitmap14043 = 14043;
+    public long fillRefBitmap14044 = 14044;
+    public long fillRefBitmap14045 = 14045;
+    public long fillRefBitmap14046 = 14046;
+    public long fillRefBitmap14047 = 14047;
+    public long fillRefBitmap14048 = 14048;
+    public long fillRefBitmap14049 = 14049;
+    public long fillRefBitmap14050 = 14050;
+    public long fillRefBitmap14051 = 14051;
+    public long fillRefBitmap14052 = 14052;
+    public long fillRefBitmap14053 = 14053;
+    public long fillRefBitmap14054 = 14054;
+    public long fillRefBitmap14055 = 14055;
+    public long fillRefBitmap14056 = 14056;
+    public long fillRefBitmap14057 = 14057;
+    public long fillRefBitmap14058 = 14058;
+    public long fillRefBitmap14059 = 14059;
+    public long fillRefBitmap14060 = 14060;
+    public long fillRefBitmap14061 = 14061;
+    public long fillRefBitmap14062 = 14062;
+    public long fillRefBitmap14063 = 14063;
+    public long fillRefBitmap14064 = 14064;
+    public long fillRefBitmap14065 = 14065;
+    public long fillRefBitmap14066 = 14066;
+    public long fillRefBitmap14067 = 14067;
+    public long fillRefBitmap14068 = 14068;
+    public long fillRefBitmap14069 = 14069;
+    public long fillRefBitmap14070 = 14070;
+    public long fillRefBitmap14071 = 14071;
+    public long fillRefBitmap14072 = 14072;
+    public long fillRefBitmap14073 = 14073;
+    public long fillRefBitmap14074 = 14074;
+    public long fillRefBitmap14075 = 14075;
+    public long fillRefBitmap14076 = 14076;
+    public long fillRefBitmap14077 = 14077;
+    public long fillRefBitmap14078 = 14078;
+    public long fillRefBitmap14079 = 14079;
+    public long fillRefBitmap14080 = 14080;
+    public long fillRefBitmap14081 = 14081;
+    public long fillRefBitmap14082 = 14082;
+    public long fillRefBitmap14083 = 14083;
+    public long fillRefBitmap14084 = 14084;
+    public long fillRefBitmap14085 = 14085;
+    public long fillRefBitmap14086 = 14086;
+    public long fillRefBitmap14087 = 14087;
+    public long fillRefBitmap14088 = 14088;
+    public long fillRefBitmap14089 = 14089;
+    public long fillRefBitmap14090 = 14090;
+    public long fillRefBitmap14091 = 14091;
+    public long fillRefBitmap14092 = 14092;
+    public long fillRefBitmap14093 = 14093;
+    public long fillRefBitmap14094 = 14094;
+    public long fillRefBitmap14095 = 14095;
+    public long fillRefBitmap14096 = 14096;
+    public long fillRefBitmap14097 = 14097;
+    public long fillRefBitmap14098 = 14098;
+    public long fillRefBitmap14099 = 14099;
+    public long fillRefBitmap14100 = 14100;
+    public long fillRefBitmap14101 = 14101;
+    public long fillRefBitmap14102 = 14102;
+    public long fillRefBitmap14103 = 14103;
+    public long fillRefBitmap14104 = 14104;
+    public long fillRefBitmap14105 = 14105;
+    public long fillRefBitmap14106 = 14106;
+    public long fillRefBitmap14107 = 14107;
+    public long fillRefBitmap14108 = 14108;
+    public long fillRefBitmap14109 = 14109;
+    public long fillRefBitmap14110 = 14110;
+    public long fillRefBitmap14111 = 14111;
+    public long fillRefBitmap14112 = 14112;
+    public long fillRefBitmap14113 = 14113;
+    public long fillRefBitmap14114 = 14114;
+    public long fillRefBitmap14115 = 14115;
+    public long fillRefBitmap14116 = 14116;
+    public long fillRefBitmap14117 = 14117;
+    public long fillRefBitmap14118 = 14118;
+    public long fillRefBitmap14119 = 14119;
+    public long fillRefBitmap14120 = 14120;
+    public long fillRefBitmap14121 = 14121;
+    public long fillRefBitmap14122 = 14122;
+    public long fillRefBitmap14123 = 14123;
+    public long fillRefBitmap14124 = 14124;
+    public long fillRefBitmap14125 = 14125;
+    public long fillRefBitmap14126 = 14126;
+    public long fillRefBitmap14127 = 14127;
+    public long fillRefBitmap14128 = 14128;
+    public long fillRefBitmap14129 = 14129;
+    public long fillRefBitmap14130 = 14130;
+    public long fillRefBitmap14131 = 14131;
+    public long fillRefBitmap14132 = 14132;
+    public long fillRefBitmap14133 = 14133;
+    public long fillRefBitmap14134 = 14134;
+    public long fillRefBitmap14135 = 14135;
+    public long fillRefBitmap14136 = 14136;
+    public long fillRefBitmap14137 = 14137;
+    public long fillRefBitmap14138 = 14138;
+    public long fillRefBitmap14139 = 14139;
+    public long fillRefBitmap14140 = 14140;
+    public long fillRefBitmap14141 = 14141;
+    public long fillRefBitmap14142 = 14142;
+    public long fillRefBitmap14143 = 14143;
+    public long fillRefBitmap14144 = 14144;
+    public long fillRefBitmap14145 = 14145;
+    public long fillRefBitmap14146 = 14146;
+    public long fillRefBitmap14147 = 14147;
+    public long fillRefBitmap14148 = 14148;
+    public long fillRefBitmap14149 = 14149;
+    public long fillRefBitmap14150 = 14150;
+    public long fillRefBitmap14151 = 14151;
+    public long fillRefBitmap14152 = 14152;
+    public long fillRefBitmap14153 = 14153;
+    public long fillRefBitmap14154 = 14154;
+    public long fillRefBitmap14155 = 14155;
+    public long fillRefBitmap14156 = 14156;
+    public long fillRefBitmap14157 = 14157;
+    public long fillRefBitmap14158 = 14158;
+    public long fillRefBitmap14159 = 14159;
+    public long fillRefBitmap14160 = 14160;
+    public long fillRefBitmap14161 = 14161;
+    public long fillRefBitmap14162 = 14162;
+    public long fillRefBitmap14163 = 14163;
+    public long fillRefBitmap14164 = 14164;
+    public long fillRefBitmap14165 = 14165;
+    public long fillRefBitmap14166 = 14166;
+    public long fillRefBitmap14167 = 14167;
+    public long fillRefBitmap14168 = 14168;
+    public long fillRefBitmap14169 = 14169;
+    public long fillRefBitmap14170 = 14170;
+    public long fillRefBitmap14171 = 14171;
+    public long fillRefBitmap14172 = 14172;
+    public long fillRefBitmap14173 = 14173;
+    public long fillRefBitmap14174 = 14174;
+    public long fillRefBitmap14175 = 14175;
+    public long fillRefBitmap14176 = 14176;
+    public long fillRefBitmap14177 = 14177;
+    public long fillRefBitmap14178 = 14178;
+    public long fillRefBitmap14179 = 14179;
+    public long fillRefBitmap14180 = 14180;
+    public long fillRefBitmap14181 = 14181;
+    public long fillRefBitmap14182 = 14182;
+    public long fillRefBitmap14183 = 14183;
+    public long fillRefBitmap14184 = 14184;
+    public long fillRefBitmap14185 = 14185;
+    public long fillRefBitmap14186 = 14186;
+    public long fillRefBitmap14187 = 14187;
+    public long fillRefBitmap14188 = 14188;
+    public long fillRefBitmap14189 = 14189;
+    public long fillRefBitmap14190 = 14190;
+    public long fillRefBitmap14191 = 14191;
+    public long fillRefBitmap14192 = 14192;
+    public long fillRefBitmap14193 = 14193;
+    public long fillRefBitmap14194 = 14194;
+    public long fillRefBitmap14195 = 14195;
+    public long fillRefBitmap14196 = 14196;
+    public long fillRefBitmap14197 = 14197;
+    public long fillRefBitmap14198 = 14198;
+    public long fillRefBitmap14199 = 14199;
+    public long fillRefBitmap14200 = 14200;
+    public long fillRefBitmap14201 = 14201;
+    public long fillRefBitmap14202 = 14202;
+    public long fillRefBitmap14203 = 14203;
+    public long fillRefBitmap14204 = 14204;
+    public long fillRefBitmap14205 = 14205;
+    public long fillRefBitmap14206 = 14206;
+    public long fillRefBitmap14207 = 14207;
+    public long fillRefBitmap14208 = 14208;
+    public long fillRefBitmap14209 = 14209;
+    public long fillRefBitmap14210 = 14210;
+    public long fillRefBitmap14211 = 14211;
+    public long fillRefBitmap14212 = 14212;
+    public long fillRefBitmap14213 = 14213;
+    public long fillRefBitmap14214 = 14214;
+    public long fillRefBitmap14215 = 14215;
+    public long fillRefBitmap14216 = 14216;
+    public long fillRefBitmap14217 = 14217;
+    public long fillRefBitmap14218 = 14218;
+    public long fillRefBitmap14219 = 14219;
+    public long fillRefBitmap14220 = 14220;
+    public long fillRefBitmap14221 = 14221;
+    public long fillRefBitmap14222 = 14222;
+    public long fillRefBitmap14223 = 14223;
+    public long fillRefBitmap14224 = 14224;
+    public long fillRefBitmap14225 = 14225;
+    public long fillRefBitmap14226 = 14226;
+    public long fillRefBitmap14227 = 14227;
+    public long fillRefBitmap14228 = 14228;
+    public long fillRefBitmap14229 = 14229;
+    public long fillRefBitmap14230 = 14230;
+    public long fillRefBitmap14231 = 14231;
+    public long fillRefBitmap14232 = 14232;
+    public long fillRefBitmap14233 = 14233;
+    public long fillRefBitmap14234 = 14234;
+    public long fillRefBitmap14235 = 14235;
+    public long fillRefBitmap14236 = 14236;
+    public long fillRefBitmap14237 = 14237;
+    public long fillRefBitmap14238 = 14238;
+    public long fillRefBitmap14239 = 14239;
+    public long fillRefBitmap14240 = 14240;
+    public long fillRefBitmap14241 = 14241;
+    public long fillRefBitmap14242 = 14242;
+    public long fillRefBitmap14243 = 14243;
+    public long fillRefBitmap14244 = 14244;
+    public long fillRefBitmap14245 = 14245;
+    public long fillRefBitmap14246 = 14246;
+    public long fillRefBitmap14247 = 14247;
+    public long fillRefBitmap14248 = 14248;
+    public long fillRefBitmap14249 = 14249;
+    public long fillRefBitmap14250 = 14250;
+    public long fillRefBitmap14251 = 14251;
+    public long fillRefBitmap14252 = 14252;
+    public long fillRefBitmap14253 = 14253;
+    public long fillRefBitmap14254 = 14254;
+    public long fillRefBitmap14255 = 14255;
+    public long fillRefBitmap14256 = 14256;
+    public long fillRefBitmap14257 = 14257;
+    public long fillRefBitmap14258 = 14258;
+    public long fillRefBitmap14259 = 14259;
+    public long fillRefBitmap14260 = 14260;
+    public long fillRefBitmap14261 = 14261;
+    public long fillRefBitmap14262 = 14262;
+    public long fillRefBitmap14263 = 14263;
+    public long fillRefBitmap14264 = 14264;
+    public long fillRefBitmap14265 = 14265;
+    public long fillRefBitmap14266 = 14266;
+    public long fillRefBitmap14267 = 14267;
+    public long fillRefBitmap14268 = 14268;
+    public long fillRefBitmap14269 = 14269;
+    public long fillRefBitmap14270 = 14270;
+    public long fillRefBitmap14271 = 14271;
+    public long fillRefBitmap14272 = 14272;
+    public long fillRefBitmap14273 = 14273;
+    public long fillRefBitmap14274 = 14274;
+    public long fillRefBitmap14275 = 14275;
+    public long fillRefBitmap14276 = 14276;
+    public long fillRefBitmap14277 = 14277;
+    public long fillRefBitmap14278 = 14278;
+    public long fillRefBitmap14279 = 14279;
+    public long fillRefBitmap14280 = 14280;
+    public long fillRefBitmap14281 = 14281;
+    public long fillRefBitmap14282 = 14282;
+    public long fillRefBitmap14283 = 14283;
+    public long fillRefBitmap14284 = 14284;
+    public long fillRefBitmap14285 = 14285;
+    public long fillRefBitmap14286 = 14286;
+    public long fillRefBitmap14287 = 14287;
+    public long fillRefBitmap14288 = 14288;
+    public long fillRefBitmap14289 = 14289;
+    public long fillRefBitmap14290 = 14290;
+    public long fillRefBitmap14291 = 14291;
+    public long fillRefBitmap14292 = 14292;
+    public long fillRefBitmap14293 = 14293;
+    public long fillRefBitmap14294 = 14294;
+    public long fillRefBitmap14295 = 14295;
+    public long fillRefBitmap14296 = 14296;
+    public long fillRefBitmap14297 = 14297;
+    public long fillRefBitmap14298 = 14298;
+    public long fillRefBitmap14299 = 14299;
+    public long fillRefBitmap14300 = 14300;
+    public long fillRefBitmap14301 = 14301;
+    public long fillRefBitmap14302 = 14302;
+    public long fillRefBitmap14303 = 14303;
+    public long fillRefBitmap14304 = 14304;
+    public long fillRefBitmap14305 = 14305;
+    public long fillRefBitmap14306 = 14306;
+    public long fillRefBitmap14307 = 14307;
+    public long fillRefBitmap14308 = 14308;
+    public long fillRefBitmap14309 = 14309;
+    public long fillRefBitmap14310 = 14310;
+    public long fillRefBitmap14311 = 14311;
+    public long fillRefBitmap14312 = 14312;
+    public long fillRefBitmap14313 = 14313;
+    public long fillRefBitmap14314 = 14314;
+    public long fillRefBitmap14315 = 14315;
+    public long fillRefBitmap14316 = 14316;
+    public long fillRefBitmap14317 = 14317;
+    public long fillRefBitmap14318 = 14318;
+    public long fillRefBitmap14319 = 14319;
+    public long fillRefBitmap14320 = 14320;
+    public long fillRefBitmap14321 = 14321;
+    public long fillRefBitmap14322 = 14322;
+    public long fillRefBitmap14323 = 14323;
+    public long fillRefBitmap14324 = 14324;
+    public long fillRefBitmap14325 = 14325;
+    public long fillRefBitmap14326 = 14326;
+    public long fillRefBitmap14327 = 14327;
+    public long fillRefBitmap14328 = 14328;
+    public long fillRefBitmap14329 = 14329;
+    public long fillRefBitmap14330 = 14330;
+    public long fillRefBitmap14331 = 14331;
+    public long fillRefBitmap14332 = 14332;
+    public long fillRefBitmap14333 = 14333;
+    public long fillRefBitmap14334 = 14334;
+    public long fillRefBitmap14335 = 14335;
+    public long fillRefBitmap14336 = 14336;
+    public long fillRefBitmap14337 = 14337;
+    public long fillRefBitmap14338 = 14338;
+    public long fillRefBitmap14339 = 14339;
+    public long fillRefBitmap14340 = 14340;
+    public long fillRefBitmap14341 = 14341;
+    public long fillRefBitmap14342 = 14342;
+    public long fillRefBitmap14343 = 14343;
+    public long fillRefBitmap14344 = 14344;
+    public long fillRefBitmap14345 = 14345;
+    public long fillRefBitmap14346 = 14346;
+    public long fillRefBitmap14347 = 14347;
+    public long fillRefBitmap14348 = 14348;
+    public long fillRefBitmap14349 = 14349;
+    public long fillRefBitmap14350 = 14350;
+    public long fillRefBitmap14351 = 14351;
+    public long fillRefBitmap14352 = 14352;
+    public long fillRefBitmap14353 = 14353;
+    public long fillRefBitmap14354 = 14354;
+    public long fillRefBitmap14355 = 14355;
+    public long fillRefBitmap14356 = 14356;
+    public long fillRefBitmap14357 = 14357;
+    public long fillRefBitmap14358 = 14358;
+    public long fillRefBitmap14359 = 14359;
+    public long fillRefBitmap14360 = 14360;
+    public long fillRefBitmap14361 = 14361;
+    public long fillRefBitmap14362 = 14362;
+    public long fillRefBitmap14363 = 14363;
+    public long fillRefBitmap14364 = 14364;
+    public long fillRefBitmap14365 = 14365;
+    public long fillRefBitmap14366 = 14366;
+    public long fillRefBitmap14367 = 14367;
+    public long fillRefBitmap14368 = 14368;
+    public long fillRefBitmap14369 = 14369;
+    public long fillRefBitmap14370 = 14370;
+    public long fillRefBitmap14371 = 14371;
+    public long fillRefBitmap14372 = 14372;
+    public long fillRefBitmap14373 = 14373;
+    public long fillRefBitmap14374 = 14374;
+    public long fillRefBitmap14375 = 14375;
+    public long fillRefBitmap14376 = 14376;
+    public long fillRefBitmap14377 = 14377;
+    public long fillRefBitmap14378 = 14378;
+    public long fillRefBitmap14379 = 14379;
+    public long fillRefBitmap14380 = 14380;
+    public long fillRefBitmap14381 = 14381;
+    public long fillRefBitmap14382 = 14382;
+    public long fillRefBitmap14383 = 14383;
+    public long fillRefBitmap14384 = 14384;
+    public long fillRefBitmap14385 = 14385;
+    public long fillRefBitmap14386 = 14386;
+    public long fillRefBitmap14387 = 14387;
+    public long fillRefBitmap14388 = 14388;
+    public long fillRefBitmap14389 = 14389;
+    public long fillRefBitmap14390 = 14390;
+    public long fillRefBitmap14391 = 14391;
+    public long fillRefBitmap14392 = 14392;
+    public long fillRefBitmap14393 = 14393;
+    public long fillRefBitmap14394 = 14394;
+    public long fillRefBitmap14395 = 14395;
+    public long fillRefBitmap14396 = 14396;
+    public long fillRefBitmap14397 = 14397;
+    public long fillRefBitmap14398 = 14398;
+    public long fillRefBitmap14399 = 14399;
+    public long fillRefBitmap14400 = 14400;
+    public long fillRefBitmap14401 = 14401;
+    public long fillRefBitmap14402 = 14402;
+    public long fillRefBitmap14403 = 14403;
+    public long fillRefBitmap14404 = 14404;
+    public long fillRefBitmap14405 = 14405;
+    public long fillRefBitmap14406 = 14406;
+    public long fillRefBitmap14407 = 14407;
+    public long fillRefBitmap14408 = 14408;
+    public long fillRefBitmap14409 = 14409;
+    public long fillRefBitmap14410 = 14410;
+    public long fillRefBitmap14411 = 14411;
+    public long fillRefBitmap14412 = 14412;
+    public long fillRefBitmap14413 = 14413;
+    public long fillRefBitmap14414 = 14414;
+    public long fillRefBitmap14415 = 14415;
+    public long fillRefBitmap14416 = 14416;
+    public long fillRefBitmap14417 = 14417;
+    public long fillRefBitmap14418 = 14418;
+    public long fillRefBitmap14419 = 14419;
+    public long fillRefBitmap14420 = 14420;
+    public long fillRefBitmap14421 = 14421;
+    public long fillRefBitmap14422 = 14422;
+    public long fillRefBitmap14423 = 14423;
+    public long fillRefBitmap14424 = 14424;
+    public long fillRefBitmap14425 = 14425;
+    public long fillRefBitmap14426 = 14426;
+    public long fillRefBitmap14427 = 14427;
+    public long fillRefBitmap14428 = 14428;
+    public long fillRefBitmap14429 = 14429;
+    public long fillRefBitmap14430 = 14430;
+    public long fillRefBitmap14431 = 14431;
+    public long fillRefBitmap14432 = 14432;
+    public long fillRefBitmap14433 = 14433;
+    public long fillRefBitmap14434 = 14434;
+    public long fillRefBitmap14435 = 14435;
+    public long fillRefBitmap14436 = 14436;
+    public long fillRefBitmap14437 = 14437;
+    public long fillRefBitmap14438 = 14438;
+    public long fillRefBitmap14439 = 14439;
+    public long fillRefBitmap14440 = 14440;
+    public long fillRefBitmap14441 = 14441;
+    public long fillRefBitmap14442 = 14442;
+    public long fillRefBitmap14443 = 14443;
+    public long fillRefBitmap14444 = 14444;
+    public long fillRefBitmap14445 = 14445;
+    public long fillRefBitmap14446 = 14446;
+    public long fillRefBitmap14447 = 14447;
+    public long fillRefBitmap14448 = 14448;
+    public long fillRefBitmap14449 = 14449;
+    public long fillRefBitmap14450 = 14450;
+    public long fillRefBitmap14451 = 14451;
+    public long fillRefBitmap14452 = 14452;
+    public long fillRefBitmap14453 = 14453;
+    public long fillRefBitmap14454 = 14454;
+    public long fillRefBitmap14455 = 14455;
+    public long fillRefBitmap14456 = 14456;
+    public long fillRefBitmap14457 = 14457;
+    public long fillRefBitmap14458 = 14458;
+    public long fillRefBitmap14459 = 14459;
+    public long fillRefBitmap14460 = 14460;
+    public long fillRefBitmap14461 = 14461;
+    public long fillRefBitmap14462 = 14462;
+    public long fillRefBitmap14463 = 14463;
+    public long fillRefBitmap14464 = 14464;
+    public long fillRefBitmap14465 = 14465;
+    public long fillRefBitmap14466 = 14466;
+    public long fillRefBitmap14467 = 14467;
+    public long fillRefBitmap14468 = 14468;
+    public long fillRefBitmap14469 = 14469;
+    public long fillRefBitmap14470 = 14470;
+    public long fillRefBitmap14471 = 14471;
+    public long fillRefBitmap14472 = 14472;
+    public long fillRefBitmap14473 = 14473;
+    public long fillRefBitmap14474 = 14474;
+    public long fillRefBitmap14475 = 14475;
+    public long fillRefBitmap14476 = 14476;
+    public long fillRefBitmap14477 = 14477;
+    public long fillRefBitmap14478 = 14478;
+    public long fillRefBitmap14479 = 14479;
+    public long fillRefBitmap14480 = 14480;
+    public long fillRefBitmap14481 = 14481;
+    public long fillRefBitmap14482 = 14482;
+    public long fillRefBitmap14483 = 14483;
+    public long fillRefBitmap14484 = 14484;
+    public long fillRefBitmap14485 = 14485;
+    public long fillRefBitmap14486 = 14486;
+    public long fillRefBitmap14487 = 14487;
+    public long fillRefBitmap14488 = 14488;
+    public long fillRefBitmap14489 = 14489;
+    public long fillRefBitmap14490 = 14490;
+    public long fillRefBitmap14491 = 14491;
+    public long fillRefBitmap14492 = 14492;
+    public long fillRefBitmap14493 = 14493;
+    public long fillRefBitmap14494 = 14494;
+    public long fillRefBitmap14495 = 14495;
+    public long fillRefBitmap14496 = 14496;
+    public long fillRefBitmap14497 = 14497;
+    public long fillRefBitmap14498 = 14498;
+    public long fillRefBitmap14499 = 14499;
+    public long fillRefBitmap14500 = 14500;
+    public long fillRefBitmap14501 = 14501;
+    public long fillRefBitmap14502 = 14502;
+    public long fillRefBitmap14503 = 14503;
+    public long fillRefBitmap14504 = 14504;
+    public long fillRefBitmap14505 = 14505;
+    public long fillRefBitmap14506 = 14506;
+    public long fillRefBitmap14507 = 14507;
+    public long fillRefBitmap14508 = 14508;
+    public long fillRefBitmap14509 = 14509;
+    public long fillRefBitmap14510 = 14510;
+    public long fillRefBitmap14511 = 14511;
+    public long fillRefBitmap14512 = 14512;
+    public long fillRefBitmap14513 = 14513;
+    public long fillRefBitmap14514 = 14514;
+    public long fillRefBitmap14515 = 14515;
+    public long fillRefBitmap14516 = 14516;
+    public long fillRefBitmap14517 = 14517;
+    public long fillRefBitmap14518 = 14518;
+    public long fillRefBitmap14519 = 14519;
+    public long fillRefBitmap14520 = 14520;
+    public long fillRefBitmap14521 = 14521;
+    public long fillRefBitmap14522 = 14522;
+    public long fillRefBitmap14523 = 14523;
+    public long fillRefBitmap14524 = 14524;
+    public long fillRefBitmap14525 = 14525;
+    public long fillRefBitmap14526 = 14526;
+    public long fillRefBitmap14527 = 14527;
+    public long fillRefBitmap14528 = 14528;
+    public long fillRefBitmap14529 = 14529;
+    public long fillRefBitmap14530 = 14530;
+    public long fillRefBitmap14531 = 14531;
+    public long fillRefBitmap14532 = 14532;
+    public long fillRefBitmap14533 = 14533;
+    public long fillRefBitmap14534 = 14534;
+    public long fillRefBitmap14535 = 14535;
+    public long fillRefBitmap14536 = 14536;
+    public long fillRefBitmap14537 = 14537;
+    public long fillRefBitmap14538 = 14538;
+    public long fillRefBitmap14539 = 14539;
+    public long fillRefBitmap14540 = 14540;
+    public long fillRefBitmap14541 = 14541;
+    public long fillRefBitmap14542 = 14542;
+    public long fillRefBitmap14543 = 14543;
+    public long fillRefBitmap14544 = 14544;
+    public long fillRefBitmap14545 = 14545;
+    public long fillRefBitmap14546 = 14546;
+    public long fillRefBitmap14547 = 14547;
+    public long fillRefBitmap14548 = 14548;
+    public long fillRefBitmap14549 = 14549;
+    public long fillRefBitmap14550 = 14550;
+    public long fillRefBitmap14551 = 14551;
+    public long fillRefBitmap14552 = 14552;
+    public long fillRefBitmap14553 = 14553;
+    public long fillRefBitmap14554 = 14554;
+    public long fillRefBitmap14555 = 14555;
+    public long fillRefBitmap14556 = 14556;
+    public long fillRefBitmap14557 = 14557;
+    public long fillRefBitmap14558 = 14558;
+    public long fillRefBitmap14559 = 14559;
+    public long fillRefBitmap14560 = 14560;
+    public long fillRefBitmap14561 = 14561;
+    public long fillRefBitmap14562 = 14562;
+    public long fillRefBitmap14563 = 14563;
+    public long fillRefBitmap14564 = 14564;
+    public long fillRefBitmap14565 = 14565;
+    public long fillRefBitmap14566 = 14566;
+    public long fillRefBitmap14567 = 14567;
+    public long fillRefBitmap14568 = 14568;
+    public long fillRefBitmap14569 = 14569;
+    public long fillRefBitmap14570 = 14570;
+    public long fillRefBitmap14571 = 14571;
+    public long fillRefBitmap14572 = 14572;
+    public long fillRefBitmap14573 = 14573;
+    public long fillRefBitmap14574 = 14574;
+    public long fillRefBitmap14575 = 14575;
+    public long fillRefBitmap14576 = 14576;
+    public long fillRefBitmap14577 = 14577;
+    public long fillRefBitmap14578 = 14578;
+    public long fillRefBitmap14579 = 14579;
+    public long fillRefBitmap14580 = 14580;
+    public long fillRefBitmap14581 = 14581;
+    public long fillRefBitmap14582 = 14582;
+    public long fillRefBitmap14583 = 14583;
+    public long fillRefBitmap14584 = 14584;
+    public long fillRefBitmap14585 = 14585;
+    public long fillRefBitmap14586 = 14586;
+    public long fillRefBitmap14587 = 14587;
+    public long fillRefBitmap14588 = 14588;
+    public long fillRefBitmap14589 = 14589;
+    public long fillRefBitmap14590 = 14590;
+    public long fillRefBitmap14591 = 14591;
+    public long fillRefBitmap14592 = 14592;
+    public long fillRefBitmap14593 = 14593;
+    public long fillRefBitmap14594 = 14594;
+    public long fillRefBitmap14595 = 14595;
+    public long fillRefBitmap14596 = 14596;
+    public long fillRefBitmap14597 = 14597;
+    public long fillRefBitmap14598 = 14598;
+    public long fillRefBitmap14599 = 14599;
+    public long fillRefBitmap14600 = 14600;
+    public long fillRefBitmap14601 = 14601;
+    public long fillRefBitmap14602 = 14602;
+    public long fillRefBitmap14603 = 14603;
+    public long fillRefBitmap14604 = 14604;
+    public long fillRefBitmap14605 = 14605;
+    public long fillRefBitmap14606 = 14606;
+    public long fillRefBitmap14607 = 14607;
+    public long fillRefBitmap14608 = 14608;
+    public long fillRefBitmap14609 = 14609;
+    public long fillRefBitmap14610 = 14610;
+    public long fillRefBitmap14611 = 14611;
+    public long fillRefBitmap14612 = 14612;
+    public long fillRefBitmap14613 = 14613;
+    public long fillRefBitmap14614 = 14614;
+    public long fillRefBitmap14615 = 14615;
+    public long fillRefBitmap14616 = 14616;
+    public long fillRefBitmap14617 = 14617;
+    public long fillRefBitmap14618 = 14618;
+    public long fillRefBitmap14619 = 14619;
+    public long fillRefBitmap14620 = 14620;
+    public long fillRefBitmap14621 = 14621;
+    public long fillRefBitmap14622 = 14622;
+    public long fillRefBitmap14623 = 14623;
+    public long fillRefBitmap14624 = 14624;
+    public long fillRefBitmap14625 = 14625;
+    public long fillRefBitmap14626 = 14626;
+    public long fillRefBitmap14627 = 14627;
+    public long fillRefBitmap14628 = 14628;
+    public long fillRefBitmap14629 = 14629;
+    public long fillRefBitmap14630 = 14630;
+    public long fillRefBitmap14631 = 14631;
+    public long fillRefBitmap14632 = 14632;
+    public long fillRefBitmap14633 = 14633;
+    public long fillRefBitmap14634 = 14634;
+    public long fillRefBitmap14635 = 14635;
+    public long fillRefBitmap14636 = 14636;
+    public long fillRefBitmap14637 = 14637;
+    public long fillRefBitmap14638 = 14638;
+    public long fillRefBitmap14639 = 14639;
+    public long fillRefBitmap14640 = 14640;
+    public long fillRefBitmap14641 = 14641;
+    public long fillRefBitmap14642 = 14642;
+    public long fillRefBitmap14643 = 14643;
+    public long fillRefBitmap14644 = 14644;
+    public long fillRefBitmap14645 = 14645;
+    public long fillRefBitmap14646 = 14646;
+    public long fillRefBitmap14647 = 14647;
+    public long fillRefBitmap14648 = 14648;
+    public long fillRefBitmap14649 = 14649;
+    public long fillRefBitmap14650 = 14650;
+    public long fillRefBitmap14651 = 14651;
+    public long fillRefBitmap14652 = 14652;
+    public long fillRefBitmap14653 = 14653;
+    public long fillRefBitmap14654 = 14654;
+    public long fillRefBitmap14655 = 14655;
+    public long fillRefBitmap14656 = 14656;
+    public long fillRefBitmap14657 = 14657;
+    public long fillRefBitmap14658 = 14658;
+    public long fillRefBitmap14659 = 14659;
+    public long fillRefBitmap14660 = 14660;
+    public long fillRefBitmap14661 = 14661;
+    public long fillRefBitmap14662 = 14662;
+    public long fillRefBitmap14663 = 14663;
+    public long fillRefBitmap14664 = 14664;
+    public long fillRefBitmap14665 = 14665;
+    public long fillRefBitmap14666 = 14666;
+    public long fillRefBitmap14667 = 14667;
+    public long fillRefBitmap14668 = 14668;
+    public long fillRefBitmap14669 = 14669;
+    public long fillRefBitmap14670 = 14670;
+    public long fillRefBitmap14671 = 14671;
+    public long fillRefBitmap14672 = 14672;
+    public long fillRefBitmap14673 = 14673;
+    public long fillRefBitmap14674 = 14674;
+    public long fillRefBitmap14675 = 14675;
+    public long fillRefBitmap14676 = 14676;
+    public long fillRefBitmap14677 = 14677;
+    public long fillRefBitmap14678 = 14678;
+    public long fillRefBitmap14679 = 14679;
+    public long fillRefBitmap14680 = 14680;
+    public long fillRefBitmap14681 = 14681;
+    public long fillRefBitmap14682 = 14682;
+    public long fillRefBitmap14683 = 14683;
+    public long fillRefBitmap14684 = 14684;
+    public long fillRefBitmap14685 = 14685;
+    public long fillRefBitmap14686 = 14686;
+    public long fillRefBitmap14687 = 14687;
+    public long fillRefBitmap14688 = 14688;
+    public long fillRefBitmap14689 = 14689;
+    public long fillRefBitmap14690 = 14690;
+    public long fillRefBitmap14691 = 14691;
+    public long fillRefBitmap14692 = 14692;
+    public long fillRefBitmap14693 = 14693;
+    public long fillRefBitmap14694 = 14694;
+    public long fillRefBitmap14695 = 14695;
+    public long fillRefBitmap14696 = 14696;
+    public long fillRefBitmap14697 = 14697;
+    public long fillRefBitmap14698 = 14698;
+    public long fillRefBitmap14699 = 14699;
+    public long fillRefBitmap14700 = 14700;
+    public long fillRefBitmap14701 = 14701;
+    public long fillRefBitmap14702 = 14702;
+    public long fillRefBitmap14703 = 14703;
+    public long fillRefBitmap14704 = 14704;
+    public long fillRefBitmap14705 = 14705;
+    public long fillRefBitmap14706 = 14706;
+    public long fillRefBitmap14707 = 14707;
+    public long fillRefBitmap14708 = 14708;
+    public long fillRefBitmap14709 = 14709;
+    public long fillRefBitmap14710 = 14710;
+    public long fillRefBitmap14711 = 14711;
+    public long fillRefBitmap14712 = 14712;
+    public long fillRefBitmap14713 = 14713;
+    public long fillRefBitmap14714 = 14714;
+    public long fillRefBitmap14715 = 14715;
+    public long fillRefBitmap14716 = 14716;
+    public long fillRefBitmap14717 = 14717;
+    public long fillRefBitmap14718 = 14718;
+    public long fillRefBitmap14719 = 14719;
+    public long fillRefBitmap14720 = 14720;
+    public long fillRefBitmap14721 = 14721;
+    public long fillRefBitmap14722 = 14722;
+    public long fillRefBitmap14723 = 14723;
+    public long fillRefBitmap14724 = 14724;
+    public long fillRefBitmap14725 = 14725;
+    public long fillRefBitmap14726 = 14726;
+    public long fillRefBitmap14727 = 14727;
+    public long fillRefBitmap14728 = 14728;
+    public long fillRefBitmap14729 = 14729;
+    public long fillRefBitmap14730 = 14730;
+    public long fillRefBitmap14731 = 14731;
+    public long fillRefBitmap14732 = 14732;
+    public long fillRefBitmap14733 = 14733;
+    public long fillRefBitmap14734 = 14734;
+    public long fillRefBitmap14735 = 14735;
+    public long fillRefBitmap14736 = 14736;
+    public long fillRefBitmap14737 = 14737;
+    public long fillRefBitmap14738 = 14738;
+    public long fillRefBitmap14739 = 14739;
+    public long fillRefBitmap14740 = 14740;
+    public long fillRefBitmap14741 = 14741;
+    public long fillRefBitmap14742 = 14742;
+    public long fillRefBitmap14743 = 14743;
+    public long fillRefBitmap14744 = 14744;
+    public long fillRefBitmap14745 = 14745;
+    public long fillRefBitmap14746 = 14746;
+    public long fillRefBitmap14747 = 14747;
+    public long fillRefBitmap14748 = 14748;
+    public long fillRefBitmap14749 = 14749;
+    public long fillRefBitmap14750 = 14750;
+    public long fillRefBitmap14751 = 14751;
+    public long fillRefBitmap14752 = 14752;
+    public long fillRefBitmap14753 = 14753;
+    public long fillRefBitmap14754 = 14754;
+    public long fillRefBitmap14755 = 14755;
+    public long fillRefBitmap14756 = 14756;
+    public long fillRefBitmap14757 = 14757;
+    public long fillRefBitmap14758 = 14758;
+    public long fillRefBitmap14759 = 14759;
+    public long fillRefBitmap14760 = 14760;
+    public long fillRefBitmap14761 = 14761;
+    public long fillRefBitmap14762 = 14762;
+    public long fillRefBitmap14763 = 14763;
+    public long fillRefBitmap14764 = 14764;
+    public long fillRefBitmap14765 = 14765;
+    public long fillRefBitmap14766 = 14766;
+    public long fillRefBitmap14767 = 14767;
+    public long fillRefBitmap14768 = 14768;
+    public long fillRefBitmap14769 = 14769;
+    public long fillRefBitmap14770 = 14770;
+    public long fillRefBitmap14771 = 14771;
+    public long fillRefBitmap14772 = 14772;
+    public long fillRefBitmap14773 = 14773;
+    public long fillRefBitmap14774 = 14774;
+    public long fillRefBitmap14775 = 14775;
+    public long fillRefBitmap14776 = 14776;
+    public long fillRefBitmap14777 = 14777;
+    public long fillRefBitmap14778 = 14778;
+    public long fillRefBitmap14779 = 14779;
+    public long fillRefBitmap14780 = 14780;
+    public long fillRefBitmap14781 = 14781;
+    public long fillRefBitmap14782 = 14782;
+    public long fillRefBitmap14783 = 14783;
+    public long fillRefBitmap14784 = 14784;
+    public long fillRefBitmap14785 = 14785;
+    public long fillRefBitmap14786 = 14786;
+    public long fillRefBitmap14787 = 14787;
+    public long fillRefBitmap14788 = 14788;
+    public long fillRefBitmap14789 = 14789;
+    public long fillRefBitmap14790 = 14790;
+    public long fillRefBitmap14791 = 14791;
+    public long fillRefBitmap14792 = 14792;
+    public long fillRefBitmap14793 = 14793;
+    public long fillRefBitmap14794 = 14794;
+    public long fillRefBitmap14795 = 14795;
+    public long fillRefBitmap14796 = 14796;
+    public long fillRefBitmap14797 = 14797;
+    public long fillRefBitmap14798 = 14798;
+    public long fillRefBitmap14799 = 14799;
+    public long fillRefBitmap14800 = 14800;
+    public long fillRefBitmap14801 = 14801;
+    public long fillRefBitmap14802 = 14802;
+    public long fillRefBitmap14803 = 14803;
+    public long fillRefBitmap14804 = 14804;
+    public long fillRefBitmap14805 = 14805;
+    public long fillRefBitmap14806 = 14806;
+    public long fillRefBitmap14807 = 14807;
+    public long fillRefBitmap14808 = 14808;
+    public long fillRefBitmap14809 = 14809;
+    public long fillRefBitmap14810 = 14810;
+    public long fillRefBitmap14811 = 14811;
+    public long fillRefBitmap14812 = 14812;
+    public long fillRefBitmap14813 = 14813;
+    public long fillRefBitmap14814 = 14814;
+    public long fillRefBitmap14815 = 14815;
+    public long fillRefBitmap14816 = 14816;
+    public long fillRefBitmap14817 = 14817;
+    public long fillRefBitmap14818 = 14818;
+    public long fillRefBitmap14819 = 14819;
+    public long fillRefBitmap14820 = 14820;
+    public long fillRefBitmap14821 = 14821;
+    public long fillRefBitmap14822 = 14822;
+    public long fillRefBitmap14823 = 14823;
+    public long fillRefBitmap14824 = 14824;
+    public long fillRefBitmap14825 = 14825;
+    public long fillRefBitmap14826 = 14826;
+    public long fillRefBitmap14827 = 14827;
+    public long fillRefBitmap14828 = 14828;
+    public long fillRefBitmap14829 = 14829;
+    public long fillRefBitmap14830 = 14830;
+    public long fillRefBitmap14831 = 14831;
+    public long fillRefBitmap14832 = 14832;
+    public long fillRefBitmap14833 = 14833;
+    public long fillRefBitmap14834 = 14834;
+    public long fillRefBitmap14835 = 14835;
+    public long fillRefBitmap14836 = 14836;
+    public long fillRefBitmap14837 = 14837;
+    public long fillRefBitmap14838 = 14838;
+    public long fillRefBitmap14839 = 14839;
+    public long fillRefBitmap14840 = 14840;
+    public long fillRefBitmap14841 = 14841;
+    public long fillRefBitmap14842 = 14842;
+    public long fillRefBitmap14843 = 14843;
+    public long fillRefBitmap14844 = 14844;
+    public long fillRefBitmap14845 = 14845;
+    public long fillRefBitmap14846 = 14846;
+    public long fillRefBitmap14847 = 14847;
+    public long fillRefBitmap14848 = 14848;
+    public long fillRefBitmap14849 = 14849;
+    public long fillRefBitmap14850 = 14850;
+    public long fillRefBitmap14851 = 14851;
+    public long fillRefBitmap14852 = 14852;
+    public long fillRefBitmap14853 = 14853;
+    public long fillRefBitmap14854 = 14854;
+    public long fillRefBitmap14855 = 14855;
+    public long fillRefBitmap14856 = 14856;
+    public long fillRefBitmap14857 = 14857;
+    public long fillRefBitmap14858 = 14858;
+    public long fillRefBitmap14859 = 14859;
+    public long fillRefBitmap14860 = 14860;
+    public long fillRefBitmap14861 = 14861;
+    public long fillRefBitmap14862 = 14862;
+    public long fillRefBitmap14863 = 14863;
+    public long fillRefBitmap14864 = 14864;
+    public long fillRefBitmap14865 = 14865;
+    public long fillRefBitmap14866 = 14866;
+    public long fillRefBitmap14867 = 14867;
+    public long fillRefBitmap14868 = 14868;
+    public long fillRefBitmap14869 = 14869;
+    public long fillRefBitmap14870 = 14870;
+    public long fillRefBitmap14871 = 14871;
+    public long fillRefBitmap14872 = 14872;
+    public long fillRefBitmap14873 = 14873;
+    public long fillRefBitmap14874 = 14874;
+    public long fillRefBitmap14875 = 14875;
+    public long fillRefBitmap14876 = 14876;
+    public long fillRefBitmap14877 = 14877;
+    public long fillRefBitmap14878 = 14878;
+    public long fillRefBitmap14879 = 14879;
+    public long fillRefBitmap14880 = 14880;
+    public long fillRefBitmap14881 = 14881;
+    public long fillRefBitmap14882 = 14882;
+    public long fillRefBitmap14883 = 14883;
+    public long fillRefBitmap14884 = 14884;
+    public long fillRefBitmap14885 = 14885;
+    public long fillRefBitmap14886 = 14886;
+    public long fillRefBitmap14887 = 14887;
+    public long fillRefBitmap14888 = 14888;
+    public long fillRefBitmap14889 = 14889;
+    public long fillRefBitmap14890 = 14890;
+    public long fillRefBitmap14891 = 14891;
+    public long fillRefBitmap14892 = 14892;
+    public long fillRefBitmap14893 = 14893;
+    public long fillRefBitmap14894 = 14894;
+    public long fillRefBitmap14895 = 14895;
+    public long fillRefBitmap14896 = 14896;
+    public long fillRefBitmap14897 = 14897;
+    public long fillRefBitmap14898 = 14898;
+    public long fillRefBitmap14899 = 14899;
+    public long fillRefBitmap14900 = 14900;
+    public long fillRefBitmap14901 = 14901;
+    public long fillRefBitmap14902 = 14902;
+    public long fillRefBitmap14903 = 14903;
+    public long fillRefBitmap14904 = 14904;
+    public long fillRefBitmap14905 = 14905;
+    public long fillRefBitmap14906 = 14906;
+    public long fillRefBitmap14907 = 14907;
+    public long fillRefBitmap14908 = 14908;
+    public long fillRefBitmap14909 = 14909;
+    public long fillRefBitmap14910 = 14910;
+    public long fillRefBitmap14911 = 14911;
+    public long fillRefBitmap14912 = 14912;
+    public long fillRefBitmap14913 = 14913;
+    public long fillRefBitmap14914 = 14914;
+    public long fillRefBitmap14915 = 14915;
+    public long fillRefBitmap14916 = 14916;
+    public long fillRefBitmap14917 = 14917;
+    public long fillRefBitmap14918 = 14918;
+    public long fillRefBitmap14919 = 14919;
+    public long fillRefBitmap14920 = 14920;
+    public long fillRefBitmap14921 = 14921;
+    public long fillRefBitmap14922 = 14922;
+    public long fillRefBitmap14923 = 14923;
+    public long fillRefBitmap14924 = 14924;
+    public long fillRefBitmap14925 = 14925;
+    public long fillRefBitmap14926 = 14926;
+    public long fillRefBitmap14927 = 14927;
+    public long fillRefBitmap14928 = 14928;
+    public long fillRefBitmap14929 = 14929;
+    public long fillRefBitmap14930 = 14930;
+    public long fillRefBitmap14931 = 14931;
+    public long fillRefBitmap14932 = 14932;
+    public long fillRefBitmap14933 = 14933;
+    public long fillRefBitmap14934 = 14934;
+    public long fillRefBitmap14935 = 14935;
+    public long fillRefBitmap14936 = 14936;
+    public long fillRefBitmap14937 = 14937;
+    public long fillRefBitmap14938 = 14938;
+    public long fillRefBitmap14939 = 14939;
+    public long fillRefBitmap14940 = 14940;
+    public long fillRefBitmap14941 = 14941;
+    public long fillRefBitmap14942 = 14942;
+    public long fillRefBitmap14943 = 14943;
+    public long fillRefBitmap14944 = 14944;
+    public long fillRefBitmap14945 = 14945;
+    public long fillRefBitmap14946 = 14946;
+    public long fillRefBitmap14947 = 14947;
+    public long fillRefBitmap14948 = 14948;
+    public long fillRefBitmap14949 = 14949;
+    public long fillRefBitmap14950 = 14950;
+    public long fillRefBitmap14951 = 14951;
+    public long fillRefBitmap14952 = 14952;
+    public long fillRefBitmap14953 = 14953;
+    public long fillRefBitmap14954 = 14954;
+    public long fillRefBitmap14955 = 14955;
+    public long fillRefBitmap14956 = 14956;
+    public long fillRefBitmap14957 = 14957;
+    public long fillRefBitmap14958 = 14958;
+    public long fillRefBitmap14959 = 14959;
+    public long fillRefBitmap14960 = 14960;
+    public long fillRefBitmap14961 = 14961;
+    public long fillRefBitmap14962 = 14962;
+    public long fillRefBitmap14963 = 14963;
+    public long fillRefBitmap14964 = 14964;
+    public long fillRefBitmap14965 = 14965;
+    public long fillRefBitmap14966 = 14966;
+    public long fillRefBitmap14967 = 14967;
+    public long fillRefBitmap14968 = 14968;
+    public long fillRefBitmap14969 = 14969;
+    public long fillRefBitmap14970 = 14970;
+    public long fillRefBitmap14971 = 14971;
+    public long fillRefBitmap14972 = 14972;
+    public long fillRefBitmap14973 = 14973;
+    public long fillRefBitmap14974 = 14974;
+    public long fillRefBitmap14975 = 14975;
+    public long fillRefBitmap14976 = 14976;
+    public long fillRefBitmap14977 = 14977;
+    public long fillRefBitmap14978 = 14978;
+    public long fillRefBitmap14979 = 14979;
+    public long fillRefBitmap14980 = 14980;
+    public long fillRefBitmap14981 = 14981;
+    public long fillRefBitmap14982 = 14982;
+    public long fillRefBitmap14983 = 14983;
+    public long fillRefBitmap14984 = 14984;
+    public long fillRefBitmap14985 = 14985;
+    public long fillRefBitmap14986 = 14986;
+    public long fillRefBitmap14987 = 14987;
+    public long fillRefBitmap14988 = 14988;
+    public long fillRefBitmap14989 = 14989;
+    public long fillRefBitmap14990 = 14990;
+    public long fillRefBitmap14991 = 14991;
+    public long fillRefBitmap14992 = 14992;
+    public long fillRefBitmap14993 = 14993;
+    public long fillRefBitmap14994 = 14994;
+    public long fillRefBitmap14995 = 14995;
+    public long fillRefBitmap14996 = 14996;
+    public long fillRefBitmap14997 = 14997;
+    public long fillRefBitmap14998 = 14998;
+    public long fillRefBitmap14999 = 14999;
 }
diff --git a/test/2048-bad-native-registry/expected-stdout.txt b/test/2048-bad-native-registry/expected-stdout.txt
index 48a6058836..394fe09aee 100644
--- a/test/2048-bad-native-registry/expected-stdout.txt
+++ b/test/2048-bad-native-registry/expected-stdout.txt
@@ -3,4 +3,4 @@ Returning bad finalizer: 0xN
 Returning placeholder object: 0xN
 About to null reference.
 Native finalizer looping
-TimeoutException on FinalizerWatchdogDaemon:ReferenceQueueDaemon timed out while targeting libcore.util.NativeAllocationRegistry$CleanerThunk@N(freeFunction = 0xN, nativePtr = 0xN, size = 666)
+TimeoutException on FinalizerWatchdogDaemon:ReferenceQueueDaemon timed out while targeting libcore.util.NativeAllocationRegistry$CleanerThunk@N(CTD)
diff --git a/test/2048-bad-native-registry/run.py b/test/2048-bad-native-registry/run.py
index f5e6e21bd4..574c95f6b0 100644
--- a/test/2048-bad-native-registry/run.py
+++ b/test/2048-bad-native-registry/run.py
@@ -17,6 +17,8 @@
 
 def run(ctx, args):
   ctx.default_run(args, android_log_tags="*:f", expected_exit_code=2)
+  # Do not compare CleanerThink detail information; it can vary.
+  ctx.run(fr"sed -i 's/freeFunction =.*size = 666/CTD/g' '{args.stdout_file}'")
   # Do not compare addresses, so replace hex numbers with 'N'.
   ctx.run(fr"sed -i 's/0x[0-9a-f][0-9a-f]*/0xN/g' '{args.stdout_file}'")
   ctx.run(fr"sed -i 's/@[0-9a-f][0-9a-f]*/@N/g' '{args.stdout_file}'")
diff --git a/test/2279-aconfig-flags/src/Main.java b/test/2279-aconfig-flags/src/Main.java
index 6d600b8136..606bc5aa45 100644
--- a/test/2279-aconfig-flags/src/Main.java
+++ b/test/2279-aconfig-flags/src/Main.java
@@ -14,6 +14,8 @@
  * limitations under the License.
  */
 
+import dalvik.system.VMRuntime;
+
 public class Main {
     public static void main(String[] args) {
         // Test a flag in libcore/libcore.aconfig.
@@ -27,6 +29,9 @@ public class Main {
             throw new AssertionError(
                     "The value of com.android.art.flags.test flag is expected to be true.");
         }
+
+        // TODO: Ramp the flag fully and assert the value.
+        isArtTestRwFlagEnabled();
     }
 
     private static boolean isVTrunkStableFlagEnabled() {
@@ -38,4 +43,9 @@ public class Main {
         // The Flags class definition is expected to be in core-libart.jar.
         return com.android.art.flags.Flags.test();
     }
+
+
+    private static boolean isArtTestRwFlagEnabled() {
+        return VMRuntime.isArtTestRwFlagEnabled();
+    }
 }
diff --git a/test/2390-virtual-thread-carrier-leak/build.py b/test/2390-virtual-thread-carrier-leak/build.py
new file mode 100644
index 0000000000..d531de7a5c
--- /dev/null
+++ b/test/2390-virtual-thread-carrier-leak/build.py
@@ -0,0 +1,20 @@
+#
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+def build(ctx):
+  if ctx.jvm:
+    return  # The test does not build on JVM
+  ctx.default_build()
diff --git a/test/2390-virtual-thread-carrier-leak/expected-stderr.txt b/test/2390-virtual-thread-carrier-leak/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/2390-virtual-thread-carrier-leak/expected-stdout.txt b/test/2390-virtual-thread-carrier-leak/expected-stdout.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/2390-virtual-thread-carrier-leak/info.txt b/test/2390-virtual-thread-carrier-leak/info.txt
new file mode 100644
index 0000000000..7afe06a0a5
--- /dev/null
+++ b/test/2390-virtual-thread-carrier-leak/info.txt
@@ -0,0 +1 @@
+Tests for Virtual Thread
diff --git a/test/2390-virtual-thread-carrier-leak/src/Main.java b/test/2390-virtual-thread-carrier-leak/src/Main.java
new file mode 100644
index 0000000000..ae54892902
--- /dev/null
+++ b/test/2390-virtual-thread-carrier-leak/src/Main.java
@@ -0,0 +1,100 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import android.system.Os;
+
+import dalvik.system.VirtualThreadContext;
+
+import java.lang.ref.WeakReference;
+
+/**
+ * Check that the carrier of virtual thread isn't leaked.
+ */
+public class Main {
+
+    private static WeakReference<Thread> WEAK_REF = null;
+
+    private static final long NANOS_PER_SECOND = 1_000_000_000L;
+
+    public static void main(String[] args) throws InterruptedException {
+        if (!com.android.art.flags.Flags.virtualThreadImplV1()) {
+            return;
+        }
+        // Exit if the thread throws any exception.
+        Thread.setDefaultUncaughtExceptionHandler((t, e) -> {
+            System.err.println("thread: " + t.getName());
+            e.printStackTrace(System.err);
+            System.exit(1);
+        });
+
+        // Verify that a carrier thread isn't reachable and leaked by the underlying
+        // Virtual Thread implementation after parking the Virtual Thread.
+        VirtualThreadContext context = $noinline$startVirtualThreadAndGetParkedContext();
+        long startTime = System.nanoTime();
+        while (!WEAK_REF.refersTo(null)) {
+            if (System.nanoTime() - startTime > 20 * NANOS_PER_SECOND) {
+                throw new AssertionError("20s time out");
+            }
+            runGcAndFinalization();
+        }
+
+        Thread carrier2 = Thread.unparkVirtual(context);
+        carrier2.join();
+    }
+
+    private static void runGcAndFinalization() {
+        for (int i = 0; i < 3; ++i) {
+            // Both GC and finalization are needed. Otherwise, the test could fail in the gcstress.
+            Runtime.getRuntime().gc();
+            System.runFinalization();
+        }
+    }
+
+    private static VirtualThreadContext $noinline$startVirtualThreadAndGetParkedContext() {
+        Thread carrier1 = Thread.startVirtual(Main::task);
+        WEAK_REF = new WeakReference<>(carrier1);
+
+        VirtualThreadContext context = null;
+        while (context == null || context.parkedStates == null) {
+            context = carrier1.getVirtualThreadContext();
+        }
+        return context;
+    }
+
+    private static void task() {
+        int tid1 = Os.gettid();
+        long threadId1 = $noinline$getCarrierThreadId();
+        Thread.parkVirtual();
+        int tid2 = Os.gettid();
+        long threadId2 = $noinline$getCarrierThreadId();
+        // Verify that the 2 carrier threads are not identical.
+        if (tid1 == tid2) {
+            throw new RuntimeException("tid shouldn't normally be the same: "
+                    + tid1 + " != " + tid2);
+        }
+        if (threadId1 == threadId2) {
+            throw new RuntimeException("tid shouldn't normally be the same: "
+                    + threadId1 + " != " + threadId2);
+        }
+    }
+
+    /**
+     * This method is extracted to avoid holding a reference to the carrier thread.
+     */
+    private static long $noinline$getCarrierThreadId() {
+        return Thread.currentThread().threadId();
+    }
+}
diff --git a/test/2390-virtual-thread-carrier-leak/test-metadata.json b/test/2390-virtual-thread-carrier-leak/test-metadata.json
new file mode 100644
index 0000000000..75f6c0270f
--- /dev/null
+++ b/test/2390-virtual-thread-carrier-leak/test-metadata.json
@@ -0,0 +1,5 @@
+{
+  "build-param": {
+    "jvm-supported": "false"
+  }
+}
diff --git a/test/2390-virtual-thread-context-leak/build.py b/test/2390-virtual-thread-context-leak/build.py
new file mode 100644
index 0000000000..d531de7a5c
--- /dev/null
+++ b/test/2390-virtual-thread-context-leak/build.py
@@ -0,0 +1,20 @@
+#
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+def build(ctx):
+  if ctx.jvm:
+    return  # The test does not build on JVM
+  ctx.default_build()
diff --git a/test/2390-virtual-thread-context-leak/expected-stderr.txt b/test/2390-virtual-thread-context-leak/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/2390-virtual-thread-context-leak/expected-stdout.txt b/test/2390-virtual-thread-context-leak/expected-stdout.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/2390-virtual-thread-context-leak/info.txt b/test/2390-virtual-thread-context-leak/info.txt
new file mode 100644
index 0000000000..7afe06a0a5
--- /dev/null
+++ b/test/2390-virtual-thread-context-leak/info.txt
@@ -0,0 +1 @@
+Tests for Virtual Thread
diff --git a/test/2390-virtual-thread-context-leak/src/Main.java b/test/2390-virtual-thread-context-leak/src/Main.java
new file mode 100644
index 0000000000..154725a17f
--- /dev/null
+++ b/test/2390-virtual-thread-context-leak/src/Main.java
@@ -0,0 +1,91 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import android.system.Os;
+
+import dalvik.system.VirtualThreadContext;
+
+import java.lang.ref.ReferenceQueue;
+import java.lang.ref.WeakReference;
+import java.text.DateFormat;
+import java.util.Date;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.concurrent.ConcurrentLinkedQueue;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.atomic.AtomicInteger;
+
+
+/**
+ *  Virtual Thread isn't GC root according to the spec. Verify that a virtual thread context and
+ *  associated parked states aren't reachable if no application code references it.
+ *  The spec may change in the future.
+ */
+public class Main {
+    private static final long NANOS_PER_SECOND = 1_000_000_000L;
+
+    public static void main(String[] args) throws InterruptedException {
+        if (!com.android.art.flags.Flags.virtualThreadImplV1()) {
+            return;
+        }
+        // Exit if the thread throws any exception.
+        Thread.setDefaultUncaughtExceptionHandler((t, e) -> {
+            System.err.println("thread: " + t.getName());
+            e.printStackTrace(System.err);
+            System.exit(1);
+        });
+
+        WeakReference<VirtualThreadContext> ref = $noinline$startVirtualThreadAndGetParkedContext();
+        long startTime = System.nanoTime();
+        while (!ref.refersTo(null)) {
+            if (System.nanoTime() - startTime > 20 * NANOS_PER_SECOND) {
+                throw new AssertionError("20s time out");
+            }
+
+            runGcAndFinalization();
+        }
+    }
+
+    private static void runGcAndFinalization() {
+        for (int i = 0; i < 3; ++i) {
+            // Both GC and finalization are needed. Otherwise, the test could fail in the gcstress.
+            Runtime.getRuntime().gc();
+            System.runFinalization();
+        }
+    }
+
+    private static WeakReference<VirtualThreadContext> $noinline$startVirtualThreadAndGetParkedContext() {
+        Thread carrier1 = Thread.startVirtual(Main::task);
+        return new WeakReference<>(carrier1.getVirtualThreadContext());
+    }
+
+    private static void task() {
+        long threadId1 = $noinline$getCarrierThreadId();
+        Thread.parkVirtual();
+        long threadId2 = $noinline$getCarrierThreadId();
+        if (threadId1 == threadId2) {
+            throw new RuntimeException("tid shouldn't normally be the same: "
+                    + threadId1 + " != " + threadId2);
+        }
+    }
+
+    /**
+     * This method is extracted to avoid holding a reference to the carrier thread.
+     */
+    private static long $noinline$getCarrierThreadId() {
+        return Thread.currentThread().threadId();
+    }
+}
diff --git a/test/2390-virtual-thread-context-leak/test-metadata.json b/test/2390-virtual-thread-context-leak/test-metadata.json
new file mode 100644
index 0000000000..75f6c0270f
--- /dev/null
+++ b/test/2390-virtual-thread-context-leak/test-metadata.json
@@ -0,0 +1,5 @@
+{
+  "build-param": {
+    "jvm-supported": "false"
+  }
+}
diff --git a/test/2390-virtual-thread-parking-error-leak/build.py b/test/2390-virtual-thread-parking-error-leak/build.py
new file mode 100644
index 0000000000..d531de7a5c
--- /dev/null
+++ b/test/2390-virtual-thread-parking-error-leak/build.py
@@ -0,0 +1,20 @@
+#
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+def build(ctx):
+  if ctx.jvm:
+    return  # The test does not build on JVM
+  ctx.default_build()
diff --git a/test/2390-virtual-thread-parking-error-leak/expected-stderr.txt b/test/2390-virtual-thread-parking-error-leak/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/2390-virtual-thread-parking-error-leak/expected-stdout.txt b/test/2390-virtual-thread-parking-error-leak/expected-stdout.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/2390-virtual-thread-parking-error-leak/info.txt b/test/2390-virtual-thread-parking-error-leak/info.txt
new file mode 100644
index 0000000000..7afe06a0a5
--- /dev/null
+++ b/test/2390-virtual-thread-parking-error-leak/info.txt
@@ -0,0 +1 @@
+Tests for Virtual Thread
diff --git a/test/2390-virtual-thread-parking-error-leak/src/Main.java b/test/2390-virtual-thread-parking-error-leak/src/Main.java
new file mode 100644
index 0000000000..84fef1487d
--- /dev/null
+++ b/test/2390-virtual-thread-parking-error-leak/src/Main.java
@@ -0,0 +1,70 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import android.system.Os;
+
+import dalvik.system.VirtualThreadContext;
+import dalvik.system.VirtualThreadParkingError;
+
+import java.lang.ref.ReferenceQueue;
+import java.lang.ref.WeakReference;
+import java.text.DateFormat;
+import java.util.Date;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.concurrent.ConcurrentLinkedQueue;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * Verify that app code can't catch VirtualThreadParkingError which should only used
+ * by ART internally.
+ */
+public class Main {
+
+    public static void main(String[] args) throws InterruptedException {
+        if (!com.android.art.flags.Flags.virtualThreadImplV1()) {
+            return;
+        }
+        // Exit if the thread throws any exception.
+        Thread.setDefaultUncaughtExceptionHandler((t, e) -> {
+            System.err.println("thread: " + t.getName());
+            e.printStackTrace(System.err);
+            System.exit(1);
+        });
+
+        Thread.startVirtual(Main::task1);
+        Thread.startVirtual(Main::task2);
+    }
+
+    private static void task1() {
+        try {
+            Thread.parkVirtual();
+        } catch (VirtualThreadParkingError e) {
+            e.printStackTrace(System.err);
+            throw new AssertionError("parkVirtual() shouldn't throw.");
+        }
+    }
+
+    private static void task2() {
+        try {
+            Thread.parkVirtual();
+        } catch (Throwable e) {
+            e.printStackTrace(System.err);
+            throw new AssertionError("parkVirtual() shouldn't throw.");
+        }
+    }
+}
diff --git a/test/2390-virtual-thread-parking-error-leak/test-metadata.json b/test/2390-virtual-thread-parking-error-leak/test-metadata.json
new file mode 100644
index 0000000000..75f6c0270f
--- /dev/null
+++ b/test/2390-virtual-thread-parking-error-leak/test-metadata.json
@@ -0,0 +1,5 @@
+{
+  "build-param": {
+    "jvm-supported": "false"
+  }
+}
diff --git a/test/2391-virtual-thread-sleeps/build.py b/test/2391-virtual-thread-sleeps/build.py
new file mode 100644
index 0000000000..d531de7a5c
--- /dev/null
+++ b/test/2391-virtual-thread-sleeps/build.py
@@ -0,0 +1,20 @@
+#
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+def build(ctx):
+  if ctx.jvm:
+    return  # The test does not build on JVM
+  ctx.default_build()
diff --git a/test/2391-virtual-thread-sleeps/expected-stderr.txt b/test/2391-virtual-thread-sleeps/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/2391-virtual-thread-sleeps/expected-stdout.txt b/test/2391-virtual-thread-sleeps/expected-stdout.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/2391-virtual-thread-sleeps/info.txt b/test/2391-virtual-thread-sleeps/info.txt
new file mode 100644
index 0000000000..7afe06a0a5
--- /dev/null
+++ b/test/2391-virtual-thread-sleeps/info.txt
@@ -0,0 +1 @@
+Tests for Virtual Thread
diff --git a/test/2391-virtual-thread-sleeps/src/Main.java b/test/2391-virtual-thread-sleeps/src/Main.java
new file mode 100644
index 0000000000..1031e4ea31
--- /dev/null
+++ b/test/2391-virtual-thread-sleeps/src/Main.java
@@ -0,0 +1,239 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import android.system.Os;
+
+import dalvik.system.VirtualThreadContext;
+
+import java.text.DateFormat;
+import java.util.Date;
+import java.util.HashSet;
+import java.util.Set;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.concurrent.ConcurrentLinkedQueue;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * Implement a thread sleeping for virtual thread on top of a single-threaded {@link Timer}.
+ * Each sleeping virtual thread should take ~1 KB or less memory. It should be more scalable than
+ * platform threads.
+ */
+public class Main {
+
+    private static final boolean DEBUG = false;
+
+    public static void main(String[] args) throws InterruptedException {
+        if (!com.android.art.flags.Flags.virtualThreadImplV1()) {
+            return;
+        }
+        // Exit if the thread throws any exception.
+        Thread.setDefaultUncaughtExceptionHandler((t, e) -> {
+            System.err.println("thread: " + t.getName());
+            e.printStackTrace(System.err);
+            System.exit(1);
+        });
+
+        testNSleepingThreads(2);
+        testNSleepingThreads(10);
+        testNSleepingThreads(100);
+        testNSleepingThreads(1000);
+        testNSleepingThreads(10000);
+    }
+
+    /**
+     * Start {@code numOfThreads} virtual threads sleeping for the given duration and wait until
+     * all threads join or time out.
+     *
+     * For the heap size limit, one virtual thread in the parked state takes ~1kB heap. Too many
+     * virtual threads can cause {@link OutOfMemoryError}.
+     * @param numOfThreads number of concurrent virtual threads sleeping
+     */
+    private static void testNSleepingThreads(int numOfThreads) {
+        long sleepDurationMs = Math.max(numOfThreads / 10, 100);
+        try (SleepingVirtualThreadTestCase test =
+                     new SleepingVirtualThreadTestCase(numOfThreads, sleepDurationMs)) {
+            test.start();
+            test.waitUntilAllThreadsWakeUp();
+        }
+    }
+
+    private static void debugPrintln(String msg) {
+        if (DEBUG) {
+             System.out.println(msg);
+        }
+    }
+
+    /**
+     * Starts the given number of virtual threads which sleeps for the given duration concurrently.
+     */
+    private static class SleepingVirtualThreadTestCase implements AutoCloseable {
+        private static final int CARRIER_THREADS_LIMIT = 128;
+        private final DateFormat df = DateFormat.getTimeInstance();
+        private final Timer mTimer = new Timer();
+        private final int mNumOfThreads;
+        private final long mSleepDurationMs;
+        private final Set<Long> virtualThreadIds;
+        private final AtomicInteger mJoinedCounter = new AtomicInteger(0);
+        private final LinkedBlockingQueue<Object> mJoinedNotifier
+                = new LinkedBlockingQueue<Object>();
+
+        SleepingVirtualThreadTestCase(int numOfThreads, long sleepDurationMs) {
+            mNumOfThreads = numOfThreads;
+            mSleepDurationMs = sleepDurationMs;
+            virtualThreadIds = new HashSet<>(mNumOfThreads);
+        }
+
+        void start() {
+            long startTime = System.currentTimeMillis();
+            debugPrintln("Started at " + df.format(new Date(startTime)));
+
+            startAndParkAllThreads();
+        }
+
+        private void startAndParkAllThreads() {
+            ConcurrentLinkedQueue<ParkedSleepingThreadHolder> parkingThreads =
+                    new ConcurrentLinkedQueue<>();
+            AtomicInteger runningSize = new AtomicInteger(0);
+            AtomicInteger startedCounter = new AtomicInteger(0);
+            AtomicInteger parkedCounter = new AtomicInteger(0);
+
+            // Without the proper support from the ForkJoinPool and a new implementation of
+            // Thread.sleep(ms) for Virtual Thread, we simulate a simple fixed sized
+            // pool and Thread.sleep(ms) here. It starts the given number of virtual thread and
+            // schedule all virtual threads to unpark after sleeping for the given durations, i.e.
+            // parkedCounter == mNumOfThreads.
+            while (parkedCounter.get() < mNumOfThreads) {
+                while (startedCounter.get() < mNumOfThreads &&
+                        runningSize.get() < CARRIER_THREADS_LIMIT) {
+                    runningSize.incrementAndGet();
+                    startSleepingThread(parkingThreads, mNumOfThreads, mSleepDurationMs,
+                            mJoinedCounter, mJoinedNotifier);
+                    startedCounter.incrementAndGet();
+                }
+                while (!parkingThreads.isEmpty()) {
+                    ParkedSleepingThreadHolder head = parkingThreads.peek();
+                    if (head == null) {
+                        break;
+                    }
+                    VirtualThreadContext vt_context = head.thread.getVirtualThreadContext();
+                    virtualThreadIds.add(vt_context.id);
+                    if (vt_context.parkedStates == null) {
+                        // Stop iterating
+                        break;
+                    }
+                    long delay = head.startTime - System.currentTimeMillis() + head.millis;
+                    if (delay > 0) {
+                        debugPrintln("Thread delayed for " + delay  + "ms " + "started.");
+                        mTimer.schedule(new TimerTask() {
+                            @Override
+                            public void run() {
+                                Thread th = Thread.unparkVirtual(vt_context);
+                                debugPrintln("Thread " + th.getName() + " delayed for " + delay  + "ms " + "started.");
+                            }
+                        }, delay);
+                    } else {
+                        Thread.unparkVirtual(vt_context);
+                    }
+                    parkingThreads.poll();
+                    parkedCounter.incrementAndGet();
+                    runningSize.decrementAndGet();
+                }
+            }
+            debugPrintln("Started " + mNumOfThreads + " threads!");
+            debugPrintln("Approx. " + (mNumOfThreads - mJoinedCounter.get()) + " threads are sleeping");
+        }
+
+        void waitUntilAllThreadsWakeUp() {
+            // The constant multiplier needs to be significantly larger than 2 because the timer
+            // is single-threaded, and is slower in the interpreter mode.
+            long timeoutThresholdMs = mSleepDurationMs * 5;
+            Object joined_signal;
+            try {
+                joined_signal = mJoinedNotifier.poll(timeoutThresholdMs, TimeUnit.MILLISECONDS);
+            } catch (InterruptedException e) {
+                throw new RuntimeException(e);
+            }
+
+            long endTime = System.currentTimeMillis();
+            debugPrintln("all " + mJoinedCounter.get() + " Threads joined at "
+                    + df.format(new Date(endTime)));
+            if (joined_signal == null) {
+                throw new AssertionError("Expected " + mNumOfThreads + " threads to "
+                        + "join, but only " + mJoinedCounter.get() + " threads joined within " +
+                        timeoutThresholdMs + " ms.");
+            }
+            if (virtualThreadIds.size() != mNumOfThreads) {
+                throw new AssertionError("Expected " + mNumOfThreads + " threads, but only " +
+                        virtualThreadIds.size() + " unique virtual thread ids.");
+            }
+        }
+
+        private static void startSleepingThread(
+                ConcurrentLinkedQueue<ParkedSleepingThreadHolder> queue,
+                long numOfThreads, long sleepDurationMs,
+                AtomicInteger joinedCounter, LinkedBlockingQueue<Object> allJoinedNotifier) {
+            Thread.startVirtual(() -> sleepingTask(queue, numOfThreads, sleepDurationMs,
+                    joinedCounter, allJoinedNotifier));
+        }
+
+        private static void sleepingTask(ConcurrentLinkedQueue<ParkedSleepingThreadHolder> queue,
+                long numOfThreads, long sleepDurationMs,
+                AtomicInteger joinedCounter, LinkedBlockingQueue<Object> allJoinedNotifier) {
+            int tid1 = Os.gettid();
+            parkVirtual(queue,  sleepDurationMs);
+            int tid2 = Os.gettid();
+            if (tid1 == tid2) {
+                // It may actually happen when tid is re-used for a separate Thread object.
+                throw new RuntimeException("tid shouldn't normally be the same: "
+                        + tid1 + " != " + tid2);
+            }
+
+            int c = joinedCounter.incrementAndGet();
+
+            if (c >= numOfThreads) {
+                allJoinedNotifier.add(new Object());
+            }
+        }
+
+        private static void parkVirtual(ConcurrentLinkedQueue<ParkedSleepingThreadHolder> queue,
+                long millis) {
+            queue.add(new ParkedSleepingThreadHolder(Thread.currentThread(),
+                    System.currentTimeMillis(), millis));
+            Thread.parkVirtual();
+        }
+
+        @Override
+        public void close() {
+            // Timer thread isn't a daemon. Canceling the timer allows process termination.
+            mTimer.cancel();
+        }
+    }
+
+    private static class ParkedSleepingThreadHolder {
+        private final Thread thread;
+        private final long startTime;
+        private final long millis;
+
+        ParkedSleepingThreadHolder(Thread thread, long startTime, long millis) {
+            this.thread = thread;
+            this.startTime = startTime;
+            this.millis = millis;
+        }
+    }
+}
diff --git a/test/2391-virtual-thread-sleeps/test-metadata.json b/test/2391-virtual-thread-sleeps/test-metadata.json
new file mode 100644
index 0000000000..75f6c0270f
--- /dev/null
+++ b/test/2391-virtual-thread-sleeps/test-metadata.json
@@ -0,0 +1,5 @@
+{
+  "build-param": {
+    "jvm-supported": "false"
+  }
+}
diff --git a/test/2392-virtual-thread-pinning-jni/build.py b/test/2392-virtual-thread-pinning-jni/build.py
new file mode 100644
index 0000000000..abe80d8bbb
--- /dev/null
+++ b/test/2392-virtual-thread-pinning-jni/build.py
@@ -0,0 +1,20 @@
+#
+# Copyright (C) 2024 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+def build(ctx):
+  if ctx.jvm:
+    return  # The test does not build on JVM
+  ctx.default_build()
diff --git a/test/2392-virtual-thread-pinning-jni/expected-stderr.txt b/test/2392-virtual-thread-pinning-jni/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/2392-virtual-thread-pinning-jni/expected-stdout.txt b/test/2392-virtual-thread-pinning-jni/expected-stdout.txt
new file mode 100644
index 0000000000..6a5618ebc6
--- /dev/null
+++ b/test/2392-virtual-thread-pinning-jni/expected-stdout.txt
@@ -0,0 +1 @@
+JNI_OnLoad called
diff --git a/test/2392-virtual-thread-pinning-jni/info.txt b/test/2392-virtual-thread-pinning-jni/info.txt
new file mode 100644
index 0000000000..7afe06a0a5
--- /dev/null
+++ b/test/2392-virtual-thread-pinning-jni/info.txt
@@ -0,0 +1 @@
+Tests for Virtual Thread
diff --git a/libdexfile/dex/compact_dex_utils.h b/test/2392-virtual-thread-pinning-jni/jni.cc
similarity index 50%
rename from libdexfile/dex/compact_dex_utils.h
rename to test/2392-virtual-thread-pinning-jni/jni.cc
index c88b799e1f..3ffade385c 100644
--- a/libdexfile/dex/compact_dex_utils.h
+++ b/test/2392-virtual-thread-pinning-jni/jni.cc
@@ -1,5 +1,5 @@
 /*
- * Copyright (C) 2018 The Android Open Source Project
+ * Copyright (C) 2013 The Android Open Source Project
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
@@ -14,24 +14,23 @@
  * limitations under the License.
  */
 
-#ifndef ART_LIBDEXFILE_DEX_COMPACT_DEX_UTILS_H_
-#define ART_LIBDEXFILE_DEX_COMPACT_DEX_UTILS_H_
+#include <stdio.h>
 
-#include <vector>
-
-#include "base/bit_utils.h"
+#include "jni.h"
 
 namespace art {
+namespace Test2932 {
+
 
-// Add padding to the end of the array until the size is aligned.
-template <typename T, template<typename> class Allocator>
-static inline void AlignmentPadVector(std::vector<T, Allocator<T>>* dest,
-                                      size_t alignment) {
-  while (!IsAlignedParam(dest->size(), alignment)) {
-    dest->push_back(T());
-  }
+extern "C" JNIEXPORT void JNICALL Java_art_Test2392_parkNative(JNIEnv* env, jclass klass) {
+    jmethodID park_virtual_method_id = env->GetStaticMethodID(klass, "parkVirtual", "()V");
+    if (park_virtual_method_id == nullptr) {
+      env->ThrowNew(env->FindClass("java/lang/RuntimeError"),
+                    "Failed to find parkVirtual method");
+      return;
+    }
+    env->CallStaticVoidMethod(klass, park_virtual_method_id);
 }
 
+}  // namespace Test2932
 }  // namespace art
-
-#endif  // ART_LIBDEXFILE_DEX_COMPACT_DEX_UTILS_H_
diff --git a/benchmark/jni_loader.cc b/test/2392-virtual-thread-pinning-jni/src/Main.java
similarity index 53%
rename from benchmark/jni_loader.cc
rename to test/2392-virtual-thread-pinning-jni/src/Main.java
index 2c9f86e3b3..52bd070658 100644
--- a/benchmark/jni_loader.cc
+++ b/test/2392-virtual-thread-pinning-jni/src/Main.java
@@ -1,5 +1,5 @@
 /*
- * Copyright (C) 2016 The Android Open Source Project
+ * Copyright (C) 2025 The Android Open Source Project
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
@@ -14,19 +14,19 @@
  * limitations under the License.
  */
 
-#include <jni.h>
+import art.Test2392;
 
-extern void register_micro_native_methods(JNIEnv* env);
-
-jint JNI_OnLoad(JavaVM* vm, void* /*reserved*/) {
-  JNIEnv* env;
-  if (vm->GetEnv(reinterpret_cast<void**>(&env), JNI_VERSION_1_6) != JNI_OK) {
-    return -1;
-  }
-
-  // List of functions to call to register methods explicitly.
-  // Otherwise we use the regular JNI naming conventions to register implicitly.
-  register_micro_native_methods(env);
+/**
+ * Verify that a virtual thread is pinned on a carrier thread when parking in
+ * synchronized block.
+ */
+public class Main {
 
-  return JNI_VERSION_1_6;
+    public static void main(String[] args) throws InterruptedException {
+        if (!com.android.art.flags.Flags.virtualThreadImplV1()) {
+            return;
+        }
+        System.loadLibrary(args[0]);
+        Test2392.main(args);
+    }
 }
diff --git a/test/2392-virtual-thread-pinning-jni/src/art/Test2392.java b/test/2392-virtual-thread-pinning-jni/src/art/Test2392.java
new file mode 100644
index 0000000000..54cc2d5dd7
--- /dev/null
+++ b/test/2392-virtual-thread-pinning-jni/src/art/Test2392.java
@@ -0,0 +1,139 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package art;
+
+import android.system.Os;
+
+import dalvik.system.VirtualThreadContext;
+
+import java.lang.ref.WeakReference;
+
+/**
+ * Verify that a virtual thread is pinned on a carrier thread when
+
+ */
+public class Test2392 {
+
+    private static WeakReference<Thread> WEAK_REF = null;
+    private static volatile WeakReference<Thread> WEAK_REF2 = null;
+
+    public static void main(String[] args) throws InterruptedException {
+        // Exit if the thread throws any exception.
+        Thread.setDefaultUncaughtExceptionHandler((t, e) -> {
+            System.err.println("thread: " + t.getName());
+            e.printStackTrace(System.err);
+            System.exit(1);
+        });
+
+        VirtualThreadContext context = startVirtualThreadAndVerifyPinning();
+        while (context.parkedStates == null) {}
+
+        long startTime = System.currentTimeMillis();
+        while (!WEAK_REF.refersTo(null)) {
+            if (System.currentTimeMillis() - startTime > 10 * 1000) {
+                throw new AssertionError("10s time out. The carrier thread should be GC-ed");
+            }
+            System.gc();
+        }
+
+        Thread carrier2 = Thread.unparkVirtual(context);
+        carrier2.join();
+    }
+
+    private static VirtualThreadContext startVirtualThreadAndVerifyPinning() {
+        VirtualThreadContext context = startVirtualThreadAndGetParkedContext();
+        Thread carrier = context.pinnedCarrierThread;
+        if (carrier == null) {
+            throw new AssertionError("carrier shouldn't be null");
+        }
+        System.gc();
+        if (!WEAK_REF.refersTo(carrier)) {
+            throw new AssertionError("The carrier thread should be the same");
+        }
+        if (!WEAK_REF2.refersTo(carrier)) {
+            throw new AssertionError("The carrier thread should be the same");
+        }
+
+        Thread carrier2 = Thread.unparkVirtual(context);
+        if (carrier != carrier2) {
+            throw new AssertionError("The carrier thread should be the same");
+        }
+        return context;
+    }
+
+    private static VirtualThreadContext startVirtualThreadAndGetParkedContext() {
+        Thread carrier1 = Thread.startVirtual(Test2392::task);
+        WEAK_REF = new WeakReference<>(carrier1);
+
+        VirtualThreadContext context = null;
+        while (context == null || context.pinnedCarrierThread == null || WEAK_REF2 == null) {
+            context = carrier1.getVirtualThreadContext();
+        }
+        return context;
+    }
+
+    private static void task() {
+        int tid1 = Os.gettid();
+        long threadId1 = getCarrierThreadId();
+        WEAK_REF2 = new WeakReference<>(Thread.currentThread());
+        // synchronized (MONITOR) {
+        //     Thread.parkVirtual();
+        // }
+        parkNative();
+
+        int tid2 = Os.gettid();
+        long threadId2 = getCarrierThreadId();
+        // Verify that the 2 carrier threads are identical.
+        if (tid1 != tid2) {
+            throw new AssertionError("tid should be the same: "
+                    + tid1 + " != " + tid2);
+        }
+        if (threadId1 != threadId2) {
+            throw new AssertionError("tid should be the same: "
+                    + threadId1 + " != " + threadId2);
+        }
+
+        parkVirtual();
+
+        // Verify that the 2 carrier threads are not identical.
+        tid2 = Os.gettid();
+        threadId2 = getCarrierThreadId();
+        if (tid1 == tid2) {
+            throw new AssertionError("tid shouldn't be the same: "
+                    + tid1 + " != " + tid2);
+        }
+        if (threadId1 == threadId2) {
+            throw new AssertionError("tid shouldn't be the same: "
+                    + threadId1 + " != " + threadId2);
+        }
+    }
+
+    /**
+     * This method is extracted to avoid holding a reference to the carrier thread.
+     */
+    private static long getCarrierThreadId() {
+        return Thread.currentThread().threadId();
+    }
+
+    private static native void parkNative();
+
+    /**
+     * Called by the JNI code.
+     */
+    public static void parkVirtual() {
+        Thread.parkVirtual();
+    }
+}
diff --git a/test/2392-virtual-thread-pinning-jni/test-metadata.json b/test/2392-virtual-thread-pinning-jni/test-metadata.json
new file mode 100644
index 0000000000..75f6c0270f
--- /dev/null
+++ b/test/2392-virtual-thread-pinning-jni/test-metadata.json
@@ -0,0 +1,5 @@
+{
+  "build-param": {
+    "jvm-supported": "false"
+  }
+}
diff --git a/test/2393-virtual-thread-pinning-monitor/build.py b/test/2393-virtual-thread-pinning-monitor/build.py
new file mode 100644
index 0000000000..d531de7a5c
--- /dev/null
+++ b/test/2393-virtual-thread-pinning-monitor/build.py
@@ -0,0 +1,20 @@
+#
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+def build(ctx):
+  if ctx.jvm:
+    return  # The test does not build on JVM
+  ctx.default_build()
diff --git a/test/2393-virtual-thread-pinning-monitor/expected-stderr.txt b/test/2393-virtual-thread-pinning-monitor/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/2393-virtual-thread-pinning-monitor/expected-stdout.txt b/test/2393-virtual-thread-pinning-monitor/expected-stdout.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/2393-virtual-thread-pinning-monitor/info.txt b/test/2393-virtual-thread-pinning-monitor/info.txt
new file mode 100644
index 0000000000..7afe06a0a5
--- /dev/null
+++ b/test/2393-virtual-thread-pinning-monitor/info.txt
@@ -0,0 +1 @@
+Tests for Virtual Thread
diff --git a/test/2393-virtual-thread-pinning-monitor/src/Main.java b/test/2393-virtual-thread-pinning-monitor/src/Main.java
new file mode 100644
index 0000000000..78cbf0c417
--- /dev/null
+++ b/test/2393-virtual-thread-pinning-monitor/src/Main.java
@@ -0,0 +1,133 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import android.system.Os;
+
+import dalvik.system.VirtualThreadContext;
+
+import java.lang.ref.WeakReference;
+
+/**
+ * Verify that a virtual thread is pinned on a carrier thread when parking in
+ * synchronized block.
+ */
+public class Main {
+
+    private static final Object MONITOR = new Object();
+    private static WeakReference<Thread> WEAK_REF = null;
+    private static volatile WeakReference<Thread> WEAK_REF2 = null;
+
+    public static void main(String[] args) throws InterruptedException {
+        if (!com.android.art.flags.Flags.virtualThreadImplV1()) {
+            return;
+        }
+        // Exit if the thread throws any exception.
+        Thread.setDefaultUncaughtExceptionHandler((t, e) -> {
+            System.err.println("thread: " + t.getName());
+            e.printStackTrace(System.err);
+            System.exit(1);
+        });
+
+        VirtualThreadContext context = startVirtualThreadAndVerifyPinning();
+        while (context.parkedStates == null) {}
+
+        long startTime = System.currentTimeMillis();
+        while (!WEAK_REF.refersTo(null)) {
+            if (System.currentTimeMillis() - startTime > 10 * 1000) {
+                throw new AssertionError("10s time out. The carrier thread should be GC-ed");
+            }
+            System.gc();
+        }
+
+        Thread carrier2 = Thread.unparkVirtual(context);
+        carrier2.join();
+    }
+
+    private static VirtualThreadContext startVirtualThreadAndVerifyPinning() {
+        VirtualThreadContext context = startVirtualThreadAndGetParkedContext();
+        Thread carrier = context.pinnedCarrierThread;
+        if (carrier == null) {
+            throw new AssertionError("carrier shouldn't be null");
+        }
+        System.gc();
+        if (!WEAK_REF.refersTo(carrier)) {
+            throw new AssertionError("The carrier thread should be the same");
+        }
+        if (!WEAK_REF2.refersTo(carrier)) {
+            throw new AssertionError("The carrier thread should be the same");
+        }
+
+        Thread carrier2 = Thread.unparkVirtual(context);
+        if (carrier != carrier2) {
+            throw new AssertionError("The carrier thread should be the same");
+        }
+        return context;
+    }
+
+    private static VirtualThreadContext startVirtualThreadAndGetParkedContext() {
+        Thread carrier1 = Thread.startVirtual(Main::task);
+        WEAK_REF = new WeakReference<>(carrier1);
+
+        VirtualThreadContext context = null;
+        while (context == null || context.pinnedCarrierThread == null || WEAK_REF2 == null) {
+            context = carrier1.getVirtualThreadContext();
+        }
+        return context;
+    }
+
+    private static void task() {
+        int tid1 = Os.gettid();
+        long threadId1 = getCarrierThreadId();
+        WEAK_REF2 = new WeakReference<>(Thread.currentThread());
+        synchronized (MONITOR) {
+            Thread.parkVirtual();
+        }
+
+        int tid2 = Os.gettid();
+        long threadId2 = getCarrierThreadId();
+        // Verify that the 2 carrier threads are identical.
+        if (tid1 != tid2) {
+            throw new AssertionError("tid should be the same: "
+                    + tid1 + " != " + tid2);
+        }
+        if (threadId1 != threadId2) {
+            throw new AssertionError("tid should be the same: "
+                    + threadId1 + " != " + threadId2);
+        }
+
+        Thread.parkVirtual();
+
+        // Verify that the 2 carrier threads are not identical.
+        tid2 = Os.gettid();
+        threadId2 = getCarrierThreadId();
+        if (tid1 == tid2) {
+            throw new AssertionError("tid shouldn't be the same: "
+                    + tid1 + " != " + tid2);
+        }
+        if (threadId1 == threadId2) {
+            throw new AssertionError("tid shouldn't be the same: "
+                    + threadId1 + " != " + threadId2);
+        }
+    }
+
+    /**
+     * This method is extracted to avoid holding a reference to the carrier thread.
+     */
+    private static long getCarrierThreadId() {
+        return Thread.currentThread().threadId();
+    }
+
+}
diff --git a/test/2393-virtual-thread-pinning-monitor/test-metadata.json b/test/2393-virtual-thread-pinning-monitor/test-metadata.json
new file mode 100644
index 0000000000..75f6c0270f
--- /dev/null
+++ b/test/2393-virtual-thread-pinning-monitor/test-metadata.json
@@ -0,0 +1,5 @@
+{
+  "build-param": {
+    "jvm-supported": "false"
+  }
+}
diff --git a/test/401-optimizing-compiler/expected-stdout.txt b/test/401-optimizing-compiler/expected-stdout.txt
index d6ef64b96b..c06458f690 100644
--- a/test/401-optimizing-compiler/expected-stdout.txt
+++ b/test/401-optimizing-compiler/expected-stdout.txt
@@ -12,3 +12,4 @@ Forced GC
 Forced GC
 Forced GC
 Forced GC
+Forced GC
diff --git a/test/401-optimizing-compiler/src/Main.java b/test/401-optimizing-compiler/src/Main.java
index f2e451854b..ecdb61eb06 100644
--- a/test/401-optimizing-compiler/src/Main.java
+++ b/test/401-optimizing-compiler/src/Main.java
@@ -115,6 +115,12 @@ public class Main {
     if (!s.equals("hello world")) {
       throw new Error("Unexpected string: " + s);
     }
+
+    Object[] array = new Object[1];
+    Object o = testBranchWithConstZero(array, false);
+    if (o != array) {
+      throw new Error("Unexpected result: " + o);
+    }
   }
 
   public static void invokePrivate() {
@@ -183,6 +189,14 @@ public class Main {
     forceGCStaticMethod();
   }
 
+  public static Object testBranchWithConstZero(Object[] o, boolean branch) {
+    if (branch) {
+      o = null;
+    }
+    forceGCStaticMethod();
+    return o;
+  }
+
   public static void printStaticMethod() {
     System.out.println("In static method");
   }
diff --git a/test/482-checker-loop-back-edge-use/src/Main.java b/test/482-checker-loop-back-edge-use/src/Main.java
index 8311d8cc4f..cbb9be857d 100644
--- a/test/482-checker-loop-back-edge-use/src/Main.java
+++ b/test/482-checker-loop-back-edge-use/src/Main.java
@@ -37,7 +37,7 @@ public class Main {
   /// CHECK-DAG:                   Goto            liveness:<<GotoLiv2:\d+>> loop:<<Loop2:B\d+>>
   /// CHECK-EVAL:    <<IfLiv>> + 1 == <<ArgUse>>
   /// CHECK-EVAL:    <<GotoLiv1>> < <<GotoLiv2>>
-  /// CHECK-EVAL:    <<GotoLiv1>> + 2 == <<ArgLoopUse>>
+  /// CHECK-EVAL:    <<GotoLiv1>> + 4 == <<ArgLoopUse>>
   //
   // Loop invariant exit check is hoisted from the loop by peeling.
 
@@ -58,7 +58,7 @@ public class Main {
   /// CHECK:                       Goto            liveness:<<GotoLiv2:\d+>>
   /// CHECK-EVAL:    <<InvokeLiv>> == <<ArgUse>>
   /// CHECK-EVAL:    <<GotoLiv1>> < <<GotoLiv2>>
-  /// CHECK-EVAL:    <<GotoLiv2>> + 2 == <<ArgLoopUse>>
+  /// CHECK-EVAL:    <<GotoLiv2>> + 4 == <<ArgLoopUse>>
 
   public static void loop3(boolean incoming) {
     // 'incoming' only needs a use at the outer loop's back edge.
@@ -89,8 +89,8 @@ public class Main {
   /// CHECK:                       Exit
   /// CHECK-EVAL:    <<InvokeLiv>> == <<ArgUse>>
   /// CHECK-EVAL:    <<GotoLiv1>> < <<GotoLiv2>>
-  /// CHECK-EVAL:    <<GotoLiv1>> + 2 == <<ArgLoopUse1>>
-  /// CHECK-EVAL:    <<GotoLiv2>> + 2 == <<ArgLoopUse2>>
+  /// CHECK-EVAL:    <<GotoLiv1>> + 4 == <<ArgLoopUse1>>
+  /// CHECK-EVAL:    <<GotoLiv2>> + 4 == <<ArgLoopUse2>>
 
   public static void loop5(boolean incoming) {
     // 'incoming' must have a use at both back edges.
@@ -111,7 +111,7 @@ public class Main {
   /// CHECK:                       Exit
   /// CHECK-EVAL:    <<InvokeLiv>> == <<ArgUse>>
   /// CHECK-EVAL:    <<GotoLiv1>> < <<GotoLiv2>>
-  /// CHECK-EVAL:    <<GotoLiv2>> + 2 == <<ArgLoopUse>>
+  /// CHECK-EVAL:    <<GotoLiv2>> + 4 == <<ArgLoopUse>>
 
   public static void loop6(boolean incoming) {
     // 'incoming' must have a use only at the first loop's back edge.
@@ -133,7 +133,7 @@ public class Main {
   /// CHECK-EVAL:    <<InvokeLiv>> == <<ArgUse1>>
   /// CHECK-EVAL:    <<IfLiv>> + 1 == <<ArgUse2>>
   /// CHECK-EVAL:    <<GotoLiv1>> < <<GotoLiv2>>
-  /// CHECK-EVAL:    <<GotoLiv1>> + 2 == <<ArgLoopUse>>
+  /// CHECK-EVAL:    <<GotoLiv1>> + 4 == <<ArgLoopUse>>
   //
   // Loop invariant exit check is hoisted from the loop by peeling.
 
@@ -154,7 +154,7 @@ public class Main {
   /// CHECK-DAG:                   Exit
   /// CHECK-EVAL:    <<IfLiv>> + 1 == <<ArgUse>>
   /// CHECK-EVAL:    <<GotoLiv1>> < <<GotoLiv2>>
-  /// CHECK-EVAL:    <<GotoLiv1>> + 2 == <<ArgLoopUse>>
+  /// CHECK-EVAL:    <<GotoLiv1>> + 4 == <<ArgLoopUse>>
   //
   // Loop invariant exit check is hoisted from the loop by peeling.
 
diff --git a/test/559-checker-irreducible-loop/smali/IrreducibleLoop.smali b/test/559-checker-irreducible-loop/smali/IrreducibleLoop.smali
index a88422fa0d..c644d77cf5 100644
--- a/test/559-checker-irreducible-loop/smali/IrreducibleLoop.smali
+++ b/test/559-checker-irreducible-loop/smali/IrreducibleLoop.smali
@@ -145,7 +145,7 @@
 ## CHECK-DAG: <<LoopPhi:i\d+>>  Phi [<<Arg>>,<<PhiInLoop:i\d+>>] liveness:<<ArgLoopPhiUse>> ranges:{[<<ArgLoopPhiUse>>,<<PhiInLoopUse:\d+>>)}
 ## CHECK-DAG: <<PhiInLoop>>     Phi [<<Arg>>,<<LoopPhi>>] liveness:<<PhiInLoopUse>> ranges:{[<<PhiInLoopUse>>,<<BackEdgeLifetimeEnd:\d+>>)}
 ## CHECK:                       Return liveness:<<ReturnLiveness:\d+>>
-## CHECK-EVAL:    <<ReturnLiveness>> == <<BackEdgeLifetimeEnd>> + 2
+## CHECK-EVAL:    <<ReturnLiveness>> == <<BackEdgeLifetimeEnd>> + 4
 .method public static liveness(II)I
    .registers 2
    if-eq p0, p1, :other_loop_entry
diff --git a/test/565-checker-condition-liveness/src/Main.java b/test/565-checker-condition-liveness/src/Main.java
index 4abf66d8ce..6663a9772b 100644
--- a/test/565-checker-condition-liveness/src/Main.java
+++ b/test/565-checker-condition-liveness/src/Main.java
@@ -32,48 +32,48 @@ public class Main {
   }
   
   /// CHECK-START-{ARM,ARM64}: void Main.testThrowIntoCatchBlock(int, java.lang.Object, int[]) liveness (after)
-  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[23,25]
-  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[11,23,25,33]
-  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[11,23,25,33]
-  /// CHECK-DAG:  <<Const1:i\d+>>   IntConstant 1         env_uses:[23,25]
-  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:10
-  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:20
-  /// CHECK-DAG:                    ArrayLength                                                               liveness:22
-  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:24
+  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[45,49]
+  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[21,45,49,65]
+  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[21,45,49,65]
+  /// CHECK-DAG:  <<Const1:i\d+>>   IntConstant 1         env_uses:[45,49]
+  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:20
+  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:40
+  /// CHECK-DAG:                    ArrayLength                                                               liveness:44
+  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:48
   /// CHECK-DAG:                    TryBoundary
   
   /// CHECK-START-{ARM,ARM64}-DEBUGGABLE: void Main.testThrowIntoCatchBlock(int, java.lang.Object, int[]) liveness (after)
-  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[11,23,25,33]
-  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[11,23,25,33]
-  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[11,23,25,33]
-  /// CHECK-DAG:  <<Const1:i\d+>>   IntConstant 1         env_uses:[23,25,33]
-  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:10
-  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:20
-  /// CHECK-DAG:                    ArrayLength                                                               liveness:22
-  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:24
+  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[21,45,49,65]
+  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[21,45,49,65]
+  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[21,45,49,65]
+  /// CHECK-DAG:  <<Const1:i\d+>>   IntConstant 1         env_uses:[45,49,65]
+  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:20
+  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:40
+  /// CHECK-DAG:                    ArrayLength                                                               liveness:44
+  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:48
   /// CHECK-DAG:                    TryBoundary
 
   // X86 and X86_64 generate at use site the ArrayLength, meaning only the BoundsCheck will have environment uses.
   /// CHECK-START-{X86,X86_64}: void Main.testThrowIntoCatchBlock(int, java.lang.Object, int[]) liveness (after)
-  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[25,25]
-  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[11,25,25,33]
-  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[11,25,25,33]
-  /// CHECK-DAG:  <<Const1:i\d+>>   IntConstant 1         env_uses:[25,25]
-  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:10
-  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:20
-  /// CHECK-DAG:                    ArrayLength                                                               liveness:22
-  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:24
+  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[49,49]
+  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[21,49,49,65]
+  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[21,49,49,65]
+  /// CHECK-DAG:  <<Const1:i\d+>>   IntConstant 1         env_uses:[49,49]
+  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:20
+  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:40
+  /// CHECK-DAG:                    ArrayLength                                                               liveness:44
+  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:48
   /// CHECK-DAG:                    TryBoundary
 
   /// CHECK-START-{X86,X86_64}-DEBUGGABLE: void Main.testThrowIntoCatchBlock(int, java.lang.Object, int[]) liveness (after)
-  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[11,25,25,33]
-  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[11,25,25,33]
-  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[11,25,25,33]
-  /// CHECK-DAG:  <<Const1:i\d+>>   IntConstant 1         env_uses:[25,25,33]
-  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:10
-  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:20
-  /// CHECK-DAG:                    ArrayLength                                                               liveness:22
-  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:24
+  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[21,49,49,65]
+  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[21,49,49,65]
+  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[21,49,49,65]
+  /// CHECK-DAG:  <<Const1:i\d+>>   IntConstant 1         env_uses:[49,49,65]
+  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:20
+  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:40
+  /// CHECK-DAG:                    ArrayLength                                                               liveness:44
+  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:48
   /// CHECK-DAG:                    TryBoundary
 
   //
@@ -88,76 +88,76 @@ public class Main {
 
   /// CHECK-START-{ARM,ARM64}: void Main.testBoundsCheck(int, java.lang.Object, int[]) liveness (after)
   /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[]
-  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[11,19,21]
-  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[11,19,21]
+  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[21,37,41]
+  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[21,37,41]
   /// CHECK-DAG:  <<Const1:i\d+>>   IntConstant 1         env_uses:[]
-  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:10
-  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:16
-  /// CHECK-DAG:                    ArrayLength                                                               liveness:18
-  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:20
+  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:20
+  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:32
+  /// CHECK-DAG:                    ArrayLength                                                               liveness:36
+  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:40
 
   /// CHECK-START-{ARM,ARM64}-DEBUGGABLE: void Main.testBoundsCheck(int, java.lang.Object, int[]) liveness (after)
-  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[11,19,21]
-  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[11,19,21]
-  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[11,19,21]
-  /// CHECK-DAG:  <<Const1:i\d+>>   IntConstant 1         env_uses:[19,21]
-  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:10
-  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:16
-  /// CHECK-DAG:                    ArrayLength                                                               liveness:18
-  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:20
+  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[21,37,41]
+  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[21,37,41]
+  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[21,37,41]
+  /// CHECK-DAG:  <<Const1:i\d+>>   IntConstant 1         env_uses:[37,41]
+  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:20
+  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:32
+  /// CHECK-DAG:                    ArrayLength                                                               liveness:36
+  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:40
 
   /// CHECK-START-{X86,X86_64}: void Main.testBoundsCheck(int, java.lang.Object, int[]) liveness (after)
   /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[]
-  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[11,21,21]
-  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[11,21,21]
+  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[21,41,41]
+  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[21,41,41]
   /// CHECK-DAG:  <<Const1:i\d+>>   IntConstant 1         env_uses:[]
-  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:10
-  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:16
-  /// CHECK-DAG:                    ArrayLength                                                               liveness:18
-  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:20
+  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:20
+  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:32
+  /// CHECK-DAG:                    ArrayLength                                                               liveness:36
+  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:40
 
   /// CHECK-START-{X86,X86_64}-DEBUGGABLE: void Main.testBoundsCheck(int, java.lang.Object, int[]) liveness (after)
-  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[11,21,21]
-  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[11,21,21]
-  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[11,21,21]
-  /// CHECK-DAG:  <<Const1:i\d+>>   IntConstant 1         env_uses:[21,21]
-  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:10
-  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:16
-  /// CHECK-DAG:                    ArrayLength                                                               liveness:18
-  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:20
+  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[21,41,41]
+  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[21,41,41]
+  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[21,41,41]
+  /// CHECK-DAG:  <<Const1:i\d+>>   IntConstant 1         env_uses:[41,41]
+  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:20
+  /// CHECK-DAG:                    NullCheck             env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:32
+  /// CHECK-DAG:                    ArrayLength                                                               liveness:36
+  /// CHECK-DAG:                    BoundsCheck           env:[[<<Const1>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:40
   public static void testBoundsCheck(int x, Object y, int[] a) {
     a[1] = x;
   }
 
   /// CHECK-START: void Main.testDeoptimize(int, java.lang.Object, int[]) liveness (after)
-  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[25]
-  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[13,21,25]
-  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[13,21,25]
-  /// CHECK-DAG:  <<Const0:i\d+>>   IntConstant 0         env_uses:[25]
-  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:12
-  /// CHECK-DAG:                    NullCheck             env:[[<<Const0>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:18
-  /// CHECK-DAG:                    ArrayLength                                                               liveness:20
-  /// CHECK-DAG:                    Deoptimize            env:[[<<Const0>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:24
+  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[49]
+  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[25,41,49]
+  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[25,41,49]
+  /// CHECK-DAG:  <<Const0:i\d+>>   IntConstant 0         env_uses:[49]
+  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:24
+  /// CHECK-DAG:                    NullCheck             env:[[<<Const0>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:36
+  /// CHECK-DAG:                    ArrayLength                                                               liveness:40
+  /// CHECK-DAG:                    Deoptimize            env:[[<<Const0>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:48
   
   /// CHECK-START-{ARM,ARM64}-DEBUGGABLE: void Main.testDeoptimize(int, java.lang.Object, int[]) liveness (after)
-  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[13,21,25]
-  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[13,21,25]
-  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[13,21,25]
-  /// CHECK-DAG:  <<Const0:i\d+>>   IntConstant 0         env_uses:[21,25]
-  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:12
-  /// CHECK-DAG:                    NullCheck             env:[[<<Const0>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:18
-  /// CHECK-DAG:                    ArrayLength                                                               liveness:20
-  /// CHECK-DAG:                    Deoptimize            env:[[<<Const0>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:24
+  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[25,41,49]
+  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[25,41,49]
+  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[25,41,49]
+  /// CHECK-DAG:  <<Const0:i\d+>>   IntConstant 0         env_uses:[41,49]
+  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:24
+  /// CHECK-DAG:                    NullCheck             env:[[<<Const0>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:36
+  /// CHECK-DAG:                    ArrayLength                                                               liveness:40
+  /// CHECK-DAG:                    Deoptimize            env:[[<<Const0>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:48
 
   /// CHECK-START-{X86,X86_64}-DEBUGGABLE: void Main.testDeoptimize(int, java.lang.Object, int[]) liveness (after)
-  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[13,21,25]
-  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[13,21,25]
-  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[13,21,25]
-  /// CHECK-DAG:  <<Const0:i\d+>>   IntConstant 0         env_uses:[21,25]
-  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:12
-  /// CHECK-DAG:                    NullCheck             env:[[<<Const0>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:18
-  /// CHECK-DAG:                    ArrayLength                                                               liveness:20
-  /// CHECK-DAG:                    Deoptimize            env:[[<<Const0>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:24
+  /// CHECK-DAG:  <<IntArg:i\d+>>   ParameterValue        env_uses:[25,41,49]
+  /// CHECK-DAG:  <<RefArg:l\d+>>   ParameterValue        env_uses:[25,41,49]
+  /// CHECK-DAG:  <<Array:l\d+>>    ParameterValue        env_uses:[25,41,49]
+  /// CHECK-DAG:  <<Const0:i\d+>>   IntConstant 0         env_uses:[41,49]
+  /// CHECK-DAG:                    SuspendCheck          env:[[_,<<IntArg>>,<<RefArg>>,<<Array>>]]           liveness:24
+  /// CHECK-DAG:                    NullCheck             env:[[<<Const0>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:36
+  /// CHECK-DAG:                    ArrayLength                                                               liveness:40
+  /// CHECK-DAG:                    Deoptimize            env:[[<<Const0>>,<<IntArg>>,<<RefArg>>,<<Array>>]]  liveness:48
   //
   // A value that's not live in compiled code may still be needed in interpreter,
   // due to code motion, etc.
diff --git a/test/674-hiddenapi/hiddenapi.cc b/test/674-hiddenapi/hiddenapi.cc
index a851e3f425..43ab7a7fff 100644
--- a/test/674-hiddenapi/hiddenapi.cc
+++ b/test/674-hiddenapi/hiddenapi.cc
@@ -24,7 +24,7 @@
 #include "ti-agent/scoped_utf_chars.h"
 
 #ifdef ART_TARGET_ANDROID
-#include "nativeloader/dlext_namespaces.h"
+#include <bionic/dlext_namespaces.h>
 #endif
 
 namespace art {
diff --git a/test/674-hiddenapi/src-ex/ChildClass.java b/test/674-hiddenapi/src-ex/ChildClass.java
index 5ffc9be271..35a08c71ba 100644
--- a/test/674-hiddenapi/src-ex/ChildClass.java
+++ b/test/674-hiddenapi/src-ex/ChildClass.java
@@ -17,6 +17,8 @@
 import dalvik.system.VMRuntime;
 import java.lang.invoke.MethodHandles;
 import java.lang.invoke.MethodType;
+import java.lang.reflect.Field;
+import java.lang.reflect.Modifier;
 import java.util.function.Consumer;
 
 public class ChildClass {
@@ -316,13 +318,13 @@ public class ChildClass {
       if (!Reflection.canGetField(klass, name)) {
         throwAccessException(klass, name, true, "Field.getInt()");
       }
-      if (!Reflection.canSetField(klass, name)) {
+      if (!isUnmodifiable(klass, name) && !Reflection.canSetField(klass, name)) {
         throwAccessException(klass, name, true, "Field.setInt()");
       }
       if (!JNI.canGetField(klass, name, isStatic)) {
         throwAccessException(klass, name, true, "getIntField");
       }
-      if (!JNI.canSetField(klass, name, isStatic)) {
+      if (!isUnmodifiable(klass, name) && !JNI.canSetField(klass, name, isStatic)) {
         throwAccessException(klass, name, true, "setIntField");
       }
     }
@@ -331,6 +333,12 @@ public class ChildClass {
     checkMemberCallback(klass, name, isPublic, true /* isField */, invokesMemberCallback);
   }
 
+  private static final boolean isUnmodifiable(Class<?> klass, String fieldName) throws Exception {
+    Field field = klass.getDeclaredField(fieldName);
+
+    return Modifier.isFinal(field.getModifiers()) && Modifier.isStatic(field.getModifiers());
+  }
+
   private static void checkMethod(Class<?> klass, String name, boolean isStatic,
       Visibility visibility, Behaviour behaviour, boolean invokesMemberCallback,
       boolean testHiddenApiCheckHardeningDisabled) throws Exception {
diff --git a/test/717-integer-value-of/src/Main.java b/test/717-integer-value-of/src/Main.java
index 557b65c1c7..afbea109f6 100644
--- a/test/717-integer-value-of/src/Main.java
+++ b/test/717-integer-value-of/src/Main.java
@@ -42,16 +42,17 @@ public class Main {
                     cacheField.setAccessible(true);
 
                     Integer[] cache = (Integer[]) cacheField.get(integerCacheClass);
-                    Integer[] alt_cache = new Integer[cache.length];
-                    System.arraycopy(cache, 0, alt_cache, 0, cache.length);
+                    Integer originalZero = cache[0];
+                    Integer alternativeZero = new Integer(0);
 
                     // Let the main thread know that everything is set up.
                     synchronized (start_end) {
                         start_end.notify();
                     }
+
                     while (!start_end.flag) {
-                        cacheField.set(integerCacheClass, alt_cache);
-                        cacheField.set(integerCacheClass, cache);
+                        cache[0] = alternativeZero;
+                        cache[0] = originalZero;
                     }
                 } catch (Throwable t) {
                     throw new Error(t);
@@ -63,14 +64,14 @@ public class Main {
             start_end.wait();  // Wait for the thread to start.
         }
         // Previously, this may have used an invalid IntegerValueOfInfo (because of seeing
-        // the `alt_cache` which is not in the boot image) when asked to emit code after
-        // using a valid info (using `cache`) when requesting locations.
+        // the `alternativeZero` which is not in the boot image) when asked to emit code after
+        // using a valid info (using `originalZero`) when requesting locations.
         ensureJitCompiled(Main.class, "getAsInteger");
 
         start_end.flag = true;
         t.join();
 
-        Runtime.getRuntime().gc();  // Collect the `alt_cache`.
+        Runtime.getRuntime().gc();  // Collect the `alternativeZero`.
 
         // If `getAsInteger()` was miscompiled, it shall try to retrieve an Integer reference
         // from a collected array (low = 0, high = 0 means that this happens only for value 0),
diff --git a/test/733-priority-mappings/build.py b/test/733-priority-mappings/build.py
new file mode 100644
index 0000000000..7025b8150a
--- /dev/null
+++ b/test/733-priority-mappings/build.py
@@ -0,0 +1,20 @@
+#
+# Copyright (C) 2022 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+def build(ctx):
+  if ctx.jvm:
+    return  # The test does not build on JVM
+  ctx.default_build()
diff --git a/test/733-priority-mappings/expected-stderr.txt b/test/733-priority-mappings/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/733-priority-mappings/expected-stdout.txt b/test/733-priority-mappings/expected-stdout.txt
new file mode 100644
index 0000000000..a965a70ed4
--- /dev/null
+++ b/test/733-priority-mappings/expected-stdout.txt
@@ -0,0 +1 @@
+Done
diff --git a/test/733-priority-mappings/info.txt b/test/733-priority-mappings/info.txt
new file mode 100644
index 0000000000..b4ab8e096e
--- /dev/null
+++ b/test/733-priority-mappings/info.txt
@@ -0,0 +1,2 @@
+Basic check for our priority <--> niceness mapping infrastructure, in support
+of Thread.setPriority() and friends.
diff --git a/test/733-priority-mappings/src/Main.java b/test/733-priority-mappings/src/Main.java
new file mode 100644
index 0000000000..bf02cc01c1
--- /dev/null
+++ b/test/733-priority-mappings/src/Main.java
@@ -0,0 +1,57 @@
+/*
+ * Copyright (C) 2024 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import dalvik.system.VMRuntime;
+
+public class Main {
+    public static void main(String[] args) throws Exception {
+        for (int p = Thread.MIN_PRIORITY; p <= Thread.MAX_PRIORITY; ++p) {
+            int niceness = Thread.nicenessForPriority(p);
+            if (p > Thread.MIN_PRIORITY &&  Thread.nicenessForPriority(p - 1) <= niceness) {
+                throw new Error("Niceness not monotonic at " + p);
+            }
+            int mapped_p = Thread.priorityForNiceness(Thread.nicenessForPriority(p));
+            if (mapped_p != p) {
+                throw new Error(p + " mapped to: " + Thread.nicenessForPriority(p) +
+                        " which mapped back to " + mapped_p);
+            }
+        }
+        Thread self = Thread.currentThread();
+        VMRuntime vmrt = VMRuntime.getRuntime();
+        for (int p = Thread.MIN_PRIORITY; p <= Thread.MAX_PRIORITY; ++p) {
+            self.setPriority(p);
+            int result = self.getPriority();
+            if (result != p) {
+                System.out.println("Set priority to " + p + " but got " + result);
+            }
+            int niceness_result = vmrt.getThreadNiceness(self);
+            if (niceness_result != Thread.nicenessForPriority(p)) {
+                System.out.println("Set priority to " + p
+                    + " but got niceness " + niceness_result);
+            }
+        }
+        for (int n = -20; n <= 19; ++n) {
+            vmrt.setThreadNiceness(self, n);
+            int result = vmrt.getThreadNiceness(self);
+            if (result != n) {
+                System.out.println("Set niceness to " + n + " but got " + result);
+            }
+        }
+        System.out.println("Done");
+    }
+
+    private static native int getThreadPlatformPriority();
+}
diff --git a/test/861-crc/expected-stderr.txt b/test/861-crc/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/861-crc/expected-stdout.txt b/test/861-crc/expected-stdout.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/861-crc/info.txt b/test/861-crc/info.txt
new file mode 100644
index 0000000000..dc9eb733ec
--- /dev/null
+++ b/test/861-crc/info.txt
@@ -0,0 +1,3 @@
+Regression test for the load-store elimination pass, which used to treat inputs
+of invokes that only read as unused, and therefore stores in that input were
+wrongly eliminated.
diff --git a/test/861-crc/src/Main.java b/test/861-crc/src/Main.java
new file mode 100644
index 0000000000..bbb93ee300
--- /dev/null
+++ b/test/861-crc/src/Main.java
@@ -0,0 +1,35 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.zip.CRC32;
+
+public class Main {
+
+  public static void main(String[] args) {
+      byte byteArray[] = { 1, 2 };
+      CRC32 crc = new CRC32();
+      // This is an intrinsic that only reads, and our load-store elimination pass forgot to take
+      // that into account.
+      crc.update(byteArray);
+      assertEquals(3066839698L, crc.getValue());
+  }
+
+  public static void assertEquals(long expected, long value) {
+    if (expected != value) {
+      throw new Error("Expected: " + expected + ", got: " + value);
+    }
+  }
+}
diff --git a/test/862-range/expected-stderr.txt b/test/862-range/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/862-range/expected-stdout.txt b/test/862-range/expected-stdout.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/862-range/info.txt b/test/862-range/info.txt
new file mode 100644
index 0000000000..f3a08e7cb9
--- /dev/null
+++ b/test/862-range/info.txt
@@ -0,0 +1,2 @@
+Regression test for the fast compiler, which used to not adjust stack offsets of
+input parameters after creating a frame.
diff --git a/test/862-range/src/Main.java b/test/862-range/src/Main.java
new file mode 100644
index 0000000000..53a2515ce9
--- /dev/null
+++ b/test/862-range/src/Main.java
@@ -0,0 +1,39 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class Main {
+
+  public static void main(String[] args) {
+    invoke(1, 2, 3, 4, 5, 6, 7, 8);
+    assertEquals(8, field);
+  }
+
+  public static void assertEquals(int expected, int actual) {
+    if (expected != actual) {
+      throw new Error("Expected " + expected + ", got " + actual);
+    }
+  }
+
+  public static void invoke(int a, int b, int c, int d, int e, int f, int g, int h) {
+    forward(a, b, c, d, e, f, g, h);
+  }
+
+  public static void forward(int a, int b, int c, int d, int e, int f, int g, int h) {
+    field = h;
+  }
+
+  static int field;
+}
diff --git a/test/863-serialization/expected-stderr.txt b/test/863-serialization/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/863-serialization/expected-stdout.txt b/test/863-serialization/expected-stdout.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/863-serialization/info.txt b/test/863-serialization/info.txt
new file mode 100644
index 0000000000..a9693a9f64
--- /dev/null
+++ b/test/863-serialization/info.txt
@@ -0,0 +1,2 @@
+Regression test for JNI::NewObject where we forgot to check if a class is
+instantiable.
diff --git a/test/863-serialization/src/Main.java b/test/863-serialization/src/Main.java
new file mode 100644
index 0000000000..72cc01f896
--- /dev/null
+++ b/test/863-serialization/src/Main.java
@@ -0,0 +1,62 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.ByteArrayInputStream;
+import java.io.InvalidClassException;
+import java.io.ObjectInputStream;
+
+public class Main {
+
+  public static void main(String[] args) throws Exception {
+    deserializeHexToConcurrentHashMap();
+  }
+
+  public static byte[] hexStringToByteArray(String hexString) {
+    if (hexString == null || hexString.isEmpty()) {
+      return new byte[0];
+    }
+    if (hexString.length() % 2 != 0) {
+      throw new IllegalArgumentException("Hex string must have an even number of characters.");
+    }
+    int len = hexString.length();
+    byte[] data = new byte[len / 2];
+    for (int i = 0; i < len; i += 2) {
+      int highNibble = Character.digit(hexString.charAt(i), 16);
+      int lowNibble = Character.digit(hexString.charAt(i + 1), 16);
+      if (highNibble == -1 || lowNibble == -1) {
+        throw new IllegalArgumentException(
+            "Invalid hex character in string: " + hexString.charAt(i) + hexString.charAt(i + 1));
+      }
+      data[i / 2] = (byte) ((highNibble << 4) + lowNibble);
+    }
+    return data;
+  }
+
+  public static void deserializeHexToConcurrentHashMap() throws Exception {
+    byte[] bytes = hexStringToByteArray("ACED0005737200266A6176612E7574696C2E636F6E63757272656E742E436F6E63757272656E74486173684D61706499DE129D87293D0300007870737200146A6176612E746578742E44617465466F726D6174642CA1E4C22615FC0200007870737200146A6176612E746578742E44617465466F726D6174642CA1E4C22615FC020000787070707878000000");
+    ByteArrayInputStream bis = new ByteArrayInputStream(bytes);
+    ObjectInputStream ois = new ObjectInputStream(bis);
+    try {
+      Object deserializedObject = ois.readObject();
+      throw new Error("Expected InvalidClassException");
+    } catch (InvalidClassException e) {
+      // expected
+      if (!(e.getCause() instanceof InstantiationException)) {
+        throw new Error("Expected InstantiationException");
+      }
+    }
+  }
+}
diff --git a/test/910-methods/expected-stdout.txt b/test/910-methods/expected-stdout.txt
index 633a3c974e..a83b3e326f 100644
--- a/test/910-methods/expected-stdout.txt
+++ b/test/910-methods/expected-stdout.txt
@@ -4,7 +4,7 @@ class java.lang.Object
 Max locals: 3
 Argument size: 1
 Location start: 0
-Location end: 39
+Location end: ok
 Is native: false
 Is obsolete: false
 Is synthetic: false
@@ -52,7 +52,7 @@ class art.Test910$NestedSynthetic
 Max locals: 1
 Argument size: 0
 Location start: 0
-Location end: 2
+Location end: ok
 Is native: false
 Is obsolete: false
 Is synthetic: true
diff --git a/test/910-methods/src/art/Test910.java b/test/910-methods/src/art/Test910.java
index 5d54a45c12..93ed9b70b4 100644
--- a/test/910-methods/src/art/Test910.java
+++ b/test/910-methods/src/art/Test910.java
@@ -91,15 +91,22 @@ public class Test910 {
     }
 
     System.out.print("Location start: ");
+    long methodStart = 0;
     try {
-      System.out.println(getMethodLocationStart(m));
+      methodStart = getMethodLocationStart(m);
+      System.out.println(methodStart);
     } catch (RuntimeException e) {
       System.out.println(e.getMessage());
     }
 
     System.out.print("Location end: ");
     try {
-      System.out.println(getMethodLocationEnd(m));
+      long methodEnd = getMethodLocationEnd(m);
+      if (methodEnd > 0 && methodEnd > methodStart) {
+        System.out.println("ok");
+      } else {
+        System.out.println(methodEnd);
+      }
     } catch (RuntimeException e) {
       System.out.println(e.getMessage());
     }
diff --git a/test/912-classes/expected-stdout.txt b/test/912-classes/expected-stdout.txt
index 5ce834eff7..d8380f91c0 100644
--- a/test/912-classes/expected-stdout.txt
+++ b/test/912-classes/expected-stdout.txt
@@ -23,7 +23,7 @@ java.util.ArrayList interface=false array=false modifiable=true
 [public static final int java.lang.Integer.BYTES, static final byte[] java.lang.Integer.DigitOnes, static final byte[] java.lang.Integer.DigitTens, public static final int java.lang.Integer.MAX_VALUE, public static final int java.lang.Integer.MIN_VALUE, public static final int java.lang.Integer.SIZE, private static final java.lang.String[] java.lang.Integer.SMALL_NEG_VALUES, private static final java.lang.String[] java.lang.Integer.SMALL_NONNEG_VALUES, public static final java.lang.Class java.lang.Integer.TYPE, static final char[] java.lang.Integer.digits, private static final long java.lang.Integer.serialVersionUID, private final int java.lang.Integer.value]
 []
 []
-[java.lang.Integer(), public java.lang.Integer(int), public java.lang.Integer(java.lang.String) throws java.lang.NumberFormatException, public static int java.lang.Integer.bitCount(int), public static int java.lang.Integer.compare(int,int), public static int java.lang.Integer.compareUnsigned(int,int), public static int java.lang.Integer.compress(int,int), public static java.lang.Integer java.lang.Integer.decode(java.lang.String) throws java.lang.NumberFormatException, public static int java.lang.Integer.divideUnsigned(int,int), public static int java.lang.Integer.expand(int,int), private static void java.lang.Integer.formatUnsignedInt(int,int,byte[],int), static void java.lang.Integer.formatUnsignedInt(int,int,byte[],int,int), static void java.lang.Integer.formatUnsignedInt(int,int,char[],int,int), static int java.lang.Integer.getChars(int,int,byte[]), static int java.lang.Integer.getChars(int,int,char[]), public static java.lang.Integer java.lang.Integer.getInteger(java.lang.String), public static java.lang.Integer java.lang.Integer.getInteger(java.lang.String,int), public static java.lang.Integer java.lang.Integer.getInteger(java.lang.String,java.lang.Integer), public static int java.lang.Integer.hashCode(int), public static int java.lang.Integer.highestOneBit(int), public static int java.lang.Integer.lowestOneBit(int), public static int java.lang.Integer.max(int,int), public static int java.lang.Integer.min(int,int), public static int java.lang.Integer.numberOfLeadingZeros(int), public static int java.lang.Integer.numberOfTrailingZeros(int), private static int java.lang.Integer.parallelSuffix(int), public static int java.lang.Integer.parseInt(java.lang.CharSequence,int,int,int) throws java.lang.NumberFormatException, public static int java.lang.Integer.parseInt(java.lang.String) throws java.lang.NumberFormatException, public static int java.lang.Integer.parseInt(java.lang.String,int) throws java.lang.NumberFormatException, public static int java.lang.Integer.parseUnsignedInt(java.lang.CharSequence,int,int,int) throws java.lang.NumberFormatException, public static int java.lang.Integer.parseUnsignedInt(java.lang.String) throws java.lang.NumberFormatException, public static int java.lang.Integer.parseUnsignedInt(java.lang.String,int) throws java.lang.NumberFormatException, public static int java.lang.Integer.remainderUnsigned(int,int), public static int java.lang.Integer.reverse(int), public static int java.lang.Integer.reverseBytes(int), public static int java.lang.Integer.rotateLeft(int,int), public static int java.lang.Integer.rotateRight(int,int), public static int java.lang.Integer.signum(int), static int java.lang.Integer.stringSize(int), public static int java.lang.Integer.sum(int,int), public static java.lang.String java.lang.Integer.toBinaryString(int), public static java.lang.String java.lang.Integer.toHexString(int), public static java.lang.String java.lang.Integer.toOctalString(int), public static java.lang.String java.lang.Integer.toString(int), public static java.lang.String java.lang.Integer.toString(int,int), public static long java.lang.Integer.toUnsignedLong(int), public static java.lang.String java.lang.Integer.toUnsignedString(int), public static java.lang.String java.lang.Integer.toUnsignedString(int,int), private static java.lang.String java.lang.Integer.toUnsignedString0(int,int), public static java.lang.Integer java.lang.Integer.valueOf(int), public static java.lang.Integer java.lang.Integer.valueOf(java.lang.String) throws java.lang.NumberFormatException, public static java.lang.Integer java.lang.Integer.valueOf(java.lang.String,int) throws java.lang.NumberFormatException, public byte java.lang.Integer.byteValue(), public int java.lang.Integer.compareTo(java.lang.Integer), public int java.lang.Integer.compareTo(java.lang.Object), public java.util.Optional java.lang.Integer.describeConstable(), public double java.lang.Integer.doubleValue(), public boolean java.lang.Integer.equals(java.lang.Object), public float java.lang.Integer.floatValue(), public int java.lang.Integer.hashCode(), public int java.lang.Integer.intValue(), public long java.lang.Integer.longValue(), public java.lang.Integer java.lang.Integer.resolveConstantDesc(java.lang.invoke.MethodHandles$Lookup), public java.lang.Object java.lang.Integer.resolveConstantDesc(java.lang.invoke.MethodHandles$Lookup) throws java.lang.ReflectiveOperationException, public short java.lang.Integer.shortValue(), public java.lang.String java.lang.Integer.toString()]
+[java.lang.Integer(), public java.lang.Integer(int), public java.lang.Integer(java.lang.String) throws java.lang.NumberFormatException, public static int java.lang.Integer.bitCount(int), public byte java.lang.Integer.byteValue(), public static int java.lang.Integer.compare(int,int), public int java.lang.Integer.compareTo(java.lang.Integer), public int java.lang.Integer.compareTo(java.lang.Object), public static int java.lang.Integer.compareUnsigned(int,int), public static int java.lang.Integer.compress(int,int), public static java.lang.Integer java.lang.Integer.decode(java.lang.String) throws java.lang.NumberFormatException, public java.util.Optional java.lang.Integer.describeConstable(), public static int java.lang.Integer.divideUnsigned(int,int), public double java.lang.Integer.doubleValue(), public boolean java.lang.Integer.equals(java.lang.Object), public static int java.lang.Integer.expand(int,int), public float java.lang.Integer.floatValue(), private static void java.lang.Integer.formatUnsignedInt(int,int,byte[],int), static void java.lang.Integer.formatUnsignedInt(int,int,byte[],int,int), static void java.lang.Integer.formatUnsignedInt(int,int,char[],int,int), static int java.lang.Integer.getChars(int,int,byte[]), static int java.lang.Integer.getChars(int,int,char[]), public static java.lang.Integer java.lang.Integer.getInteger(java.lang.String), public static java.lang.Integer java.lang.Integer.getInteger(java.lang.String,int), public static java.lang.Integer java.lang.Integer.getInteger(java.lang.String,java.lang.Integer), public int java.lang.Integer.hashCode(), public static int java.lang.Integer.hashCode(int), public static int java.lang.Integer.highestOneBit(int), public int java.lang.Integer.intValue(), public long java.lang.Integer.longValue(), public static int java.lang.Integer.lowestOneBit(int), public static int java.lang.Integer.max(int,int), public static int java.lang.Integer.min(int,int), public static int java.lang.Integer.numberOfLeadingZeros(int), public static int java.lang.Integer.numberOfTrailingZeros(int), private static int java.lang.Integer.parallelSuffix(int), public static int java.lang.Integer.parseInt(java.lang.CharSequence,int,int,int) throws java.lang.NumberFormatException, public static int java.lang.Integer.parseInt(java.lang.String) throws java.lang.NumberFormatException, public static int java.lang.Integer.parseInt(java.lang.String,int) throws java.lang.NumberFormatException, public static int java.lang.Integer.parseUnsignedInt(java.lang.CharSequence,int,int,int) throws java.lang.NumberFormatException, public static int java.lang.Integer.parseUnsignedInt(java.lang.String) throws java.lang.NumberFormatException, public static int java.lang.Integer.parseUnsignedInt(java.lang.String,int) throws java.lang.NumberFormatException, public static int java.lang.Integer.remainderUnsigned(int,int), public java.lang.Integer java.lang.Integer.resolveConstantDesc(java.lang.invoke.MethodHandles$Lookup), public java.lang.Object java.lang.Integer.resolveConstantDesc(java.lang.invoke.MethodHandles$Lookup) throws java.lang.ReflectiveOperationException, public static int java.lang.Integer.reverse(int), public static int java.lang.Integer.reverseBytes(int), public static int java.lang.Integer.rotateLeft(int,int), public static int java.lang.Integer.rotateRight(int,int), public short java.lang.Integer.shortValue(), public static int java.lang.Integer.signum(int), static int java.lang.Integer.stringSize(int), public static int java.lang.Integer.sum(int,int), public static java.lang.String java.lang.Integer.toBinaryString(int), public static java.lang.String java.lang.Integer.toHexString(int), public static java.lang.String java.lang.Integer.toOctalString(int), public java.lang.String java.lang.Integer.toString(), public static java.lang.String java.lang.Integer.toString(int), public static java.lang.String java.lang.Integer.toString(int,int), public static long java.lang.Integer.toUnsignedLong(int), public static java.lang.String java.lang.Integer.toUnsignedString(int), public static java.lang.String java.lang.Integer.toUnsignedString(int,int), private static java.lang.String java.lang.Integer.toUnsignedString0(int,int), public static java.lang.Integer java.lang.Integer.valueOf(int), public static java.lang.Integer java.lang.Integer.valueOf(java.lang.String) throws java.lang.NumberFormatException, public static java.lang.Integer java.lang.Integer.valueOf(java.lang.String,int) throws java.lang.NumberFormatException]
 []
 []
 int 100000
diff --git a/test/913-heaps/expected-stdout.txt b/test/913-heaps/expected-stdout.txt
index 87ad2d669d..f54fe80108 100644
--- a/test/913-heaps/expected-stdout.txt
+++ b/test/913-heaps/expected-stdout.txt
@@ -1,8 +1,8 @@
 ---
 true true
 root@root --(stack-local[id=1,tag=3000,depth=2,method=doFollowReferencesTestNonRoot,vreg=0,location= 31])--> 1@1000 [size=16, length=-1]
-root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=120, length=-1]
-root@root --(thread)--> 3000@0 [size=120, length=-1]
+root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=132, length=-1]
+root@root --(thread)--> 3000@0 [size=132, length=-1]
 1001@0 --(superclass)--> 1000@0 [size=123456780000, length=-1]
 1002@0 --(interface)--> 2001@0 [size=123456780004, length=-1]
 1002@0 --(superclass)--> 1001@0 [size=123456780001, length=-1]
@@ -46,8 +46,8 @@ root@root --(jni-local[id=1,tag=3000,depth=0,method=followReferences])--> 1@1000
 root@root --(stack-local[id=1,tag=3000,depth=1,method=doFollowReferencesTestImpl,vreg=10,location= 8])--> 1@1000 [size=16, length=-1]
 root@root --(stack-local[id=1,tag=3000,depth=1,method=doFollowReferencesTestImpl,vreg=5,location= 8])--> 1@1000 [size=16, length=-1]
 root@root --(stack-local[id=1,tag=3000,depth=2,method=doFollowReferencesTestRoot,vreg=5,location= 20])--> 1@1000 [size=16, length=-1]
-root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=120, length=-1]
-root@root --(thread)--> 3000@0 [size=120, length=-1]
+root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=132, length=-1]
+root@root --(thread)--> 3000@0 [size=132, length=-1]
 1001@0 --(superclass)--> 1000@0 [size=123456780005, length=-1]
 1002@0 --(interface)--> 2001@0 [size=123456780009, length=-1]
 1002@0 --(superclass)--> 1001@0 [size=123456780006, length=-1]
@@ -86,17 +86,17 @@ root@root --(thread)--> 3000@0 [size=120, length=-1]
 5@1002 --(field@9)--> 6@1000 [size=16, length=-1]
 6@1000 --(class)--> 1000@0 [size=123456780005, length=-1]
 ---
-root@root --(thread)--> 3000@0 [size=120, length=-1]
+root@root --(thread)--> 3000@0 [size=132, length=-1]
 ---
 3@1001 --(class)--> 1001@0 [size=123456780011, length=-1]
 ---
-root@root --(thread)--> 3000@0 [size=120, length=-1]
+root@root --(thread)--> 3000@0 [size=132, length=-1]
 ---
 3@1001 --(class)--> 1001@0 [size=123456780016, length=-1]
 ---
 root@root --(stack-local[id=1,tag=3000,depth=2,method=doFollowReferencesTestNonRoot,vreg=0,location= 31])--> 1@1000 [size=16, length=-1]
-root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=120, length=-1]
-root@root --(thread)--> 3000@0 [size=120, length=-1]
+root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=132, length=-1]
+root@root --(thread)--> 3000@0 [size=132, length=-1]
 ---
 1001@0 --(superclass)--> 1000@0 [size=123456780020, length=-1]
 3@1001 --(class)--> 1001@0 [size=123456780021, length=-1]
@@ -108,8 +108,8 @@ root@root --(jni-local[id=1,tag=3000,depth=0,method=followReferences])--> 1@1000
 root@root --(stack-local[id=1,tag=3000,depth=1,method=doFollowReferencesTestImpl,vreg=10,location= 8])--> 1@1000 [size=16, length=-1]
 root@root --(stack-local[id=1,tag=3000,depth=1,method=doFollowReferencesTestImpl,vreg=5,location= 8])--> 1@1000 [size=16, length=-1]
 root@root --(stack-local[id=1,tag=3000,depth=2,method=doFollowReferencesTestRoot,vreg=5,location= 20])--> 1@1000 [size=16, length=-1]
-root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=120, length=-1]
-root@root --(thread)--> 3000@0 [size=120, length=-1]
+root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=132, length=-1]
+root@root --(thread)--> 3000@0 [size=132, length=-1]
 ---
 1001@0 --(superclass)--> 1000@0 [size=123456780025, length=-1]
 3@1001 --(class)--> 1001@0 [size=123456780026, length=-1]
@@ -189,8 +189,8 @@ root@root --(stack-local[id=1,tag=3000,depth=2,method=doFollowReferencesTestRoot
 ---
 ---- untagged objects
 root@root --(stack-local[id=1,tag=3000,depth=2,method=doFollowReferencesTestNonRoot,vreg=0,location= 31])--> 1@1000 [size=16, length=-1]
-root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=120, length=-1]
-root@root --(thread)--> 3000@0 [size=120, length=-1]
+root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=132, length=-1]
+root@root --(thread)--> 3000@0 [size=132, length=-1]
 1001@0 --(superclass)--> 1000@0 [size=123456780050, length=-1]
 1002@0 --(interface)--> 2001@0 [size=123456780054, length=-1]
 1002@0 --(superclass)--> 1001@0 [size=123456780051, length=-1]
@@ -234,8 +234,8 @@ root@root --(jni-local[id=1,tag=3000,depth=0,method=followReferences])--> 1@1000
 root@root --(stack-local[id=1,tag=3000,depth=1,method=doFollowReferencesTestImpl,vreg=10,location= 8])--> 1@1000 [size=16, length=-1]
 root@root --(stack-local[id=1,tag=3000,depth=1,method=doFollowReferencesTestImpl,vreg=5,location= 8])--> 1@1000 [size=16, length=-1]
 root@root --(stack-local[id=1,tag=3000,depth=2,method=doFollowReferencesTestRoot,vreg=5,location= 20])--> 1@1000 [size=16, length=-1]
-root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=120, length=-1]
-root@root --(thread)--> 3000@0 [size=120, length=-1]
+root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=132, length=-1]
+root@root --(thread)--> 3000@0 [size=132, length=-1]
 1001@0 --(superclass)--> 1000@0 [size=123456780055, length=-1]
 1002@0 --(interface)--> 2001@0 [size=123456780059, length=-1]
 1002@0 --(superclass)--> 1001@0 [size=123456780056, length=-1]
@@ -275,8 +275,8 @@ root@root --(thread)--> 3000@0 [size=120, length=-1]
 6@1000 --(class)--> 1000@0 [size=123456780055, length=-1]
 ---
 ---- tagged classes
-root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=120, length=-1]
-root@root --(thread)--> 3000@0 [size=120, length=-1]
+root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=132, length=-1]
+root@root --(thread)--> 3000@0 [size=132, length=-1]
 1001@0 --(superclass)--> 1000@0 [size=123456780060, length=-1]
 1002@0 --(interface)--> 2001@0 [size=123456780064, length=-1]
 1002@0 --(superclass)--> 1001@0 [size=123456780061, length=-1]
@@ -301,8 +301,8 @@ root@root --(thread)--> 3000@0 [size=120, length=-1]
 5@1002 --(field@8)--> 500@0 [size=20, length=2]
 6@1000 --(class)--> 1000@0 [size=123456780060, length=-1]
 ---
-root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=120, length=-1]
-root@root --(thread)--> 3000@0 [size=120, length=-1]
+root@root --(stack-local[id=1,tag=3000,depth=5,method=run,vreg=2,location= 0])--> 3000@0 [size=132, length=-1]
+root@root --(thread)--> 3000@0 [size=132, length=-1]
 1001@0 --(superclass)--> 1000@0 [size=123456780065, length=-1]
 1002@0 --(interface)--> 2001@0 [size=123456780069, length=-1]
 1002@0 --(superclass)--> 1001@0 [size=123456780066, length=-1]
diff --git a/test/913-heaps/info.txt b/test/913-heaps/info.txt
index 875a5f6ec1..5dd3f9ecb5 100644
--- a/test/913-heaps/info.txt
+++ b/test/913-heaps/info.txt
@@ -1 +1,2 @@
 Tests basic functions in the jvmti plugin.
+Output expectation depends on size of Java Thread objects.
diff --git a/test/956-methodhandles/src/Main.java b/test/956-methodhandles/src/Main.java
index 51f7a17779..d9dc84cc18 100644
--- a/test/956-methodhandles/src/Main.java
+++ b/test/956-methodhandles/src/Main.java
@@ -1915,15 +1915,13 @@ public class Main {
     c5.accept("hello there");
 
     // Failures
-    MethodHandle abstract_target =
-        MethodHandles.lookup()
-                    .findSpecial(Consumer.class,
-                                 acceptMethod.getName(),
-                                 MethodType.methodType(acceptMethod.getReturnType(),
-                                                       acceptMethod.getParameterTypes()),
-                                 c3.getClass());
-    try {
-      abstract_target.invoke(c3, "hello");
+    try {
+      MethodHandles.lookup()
+                  .findSpecial(Consumer.class,
+                               acceptMethod.getName(),
+                               MethodType.methodType(acceptMethod.getReturnType(),
+                                                     acceptMethod.getParameterTypes()),
+                               c3.getClass());
     } catch (IllegalAccessException e) {
       System.out.println("Got expected IAE when invoke-special on an abstract interface method");
     }
diff --git a/test/959-invoke-polymorphic-accessors/src/Main.java b/test/959-invoke-polymorphic-accessors/src/Main.java
index 73b7746832..3bdc7b3d9a 100644
--- a/test/959-invoke-polymorphic-accessors/src/Main.java
+++ b/test/959-invoke-polymorphic-accessors/src/Main.java
@@ -20,7 +20,20 @@ import java.lang.reflect.Field;
 
 public class Main {
 
-    private static final boolean DALVIK_RUN = "Dalvik".equals(System.getProperty("java.vm.name"));
+    private static final boolean STATIC_FINAL_CAN_BE_MODIFIED;
+
+    static {
+        boolean canBeModified = false;
+        try {
+            Field f = Main.class.getDeclaredField("STATIC_FINAL_CAN_BE_MODIFIED");
+            f.setAccessible(true);
+            f.setBoolean(null, true);
+            canBeModified = true;
+        } catch (Exception e) {
+            canBeModified = false;
+        }
+        STATIC_FINAL_CAN_BE_MODIFIED = canBeModified;
+    }
 
     public static class ValueHolder {
         public boolean m_z = false;
@@ -961,31 +974,28 @@ public class Main {
                 MethodHandles.lookup().unreflectSetter(f).invokeExact(v, 'A');
                 assertEquals('A', (char) MethodHandles.lookup().unreflectGetter(f).invokeExact(v));
             }
-            if (DALVIK_RUN) {
+            {
                 // public static final field test
-                // for JVM it is not possible to get the unreflected setter for a static final
-                // field, see b/242985782
                 Field f = ValueHolder.class.getDeclaredField("s_fi");
                 try {
                     MethodHandles.lookup().unreflectSetter(f);
                     fail();
                 } catch (IllegalAccessException expected) {}
-                MethodHandles.lookup().unreflectGetter(f);
-                f.setAccessible(true);
-                int savedValue = (int) MethodHandles.lookup().unreflectGetter(f).invokeExact();
-                int newValue = savedValue + 1;
-                MethodHandles.lookup().unreflectSetter(f).invokeExact(newValue);
-                assertEquals(newValue, (int) MethodHandles.lookup().unreflectGetter(f).invokeExact()
-                );
-                MethodHandles.lookup().unreflectSetter(f).invokeExact(savedValue);
-                assertEquals(savedValue, (int) MethodHandles.lookup().unreflectGetter(f).invokeExact()
-                );
-                f.setAccessible(false);
-                try {
-                    MethodHandles.lookup().unreflectSetter(f);
-                    fail();
-                } catch (IllegalAccessException expected) {}
-                MethodHandles.lookup().unreflectGetter(f);
+                int actual = (int) MethodHandles.lookup().unreflectGetter(f).invokeExact();
+                assertEquals(ValueHolder.s_fi, actual);
+                if (!STATIC_FINAL_CAN_BE_MODIFIED) {
+                    f.setAccessible(true);
+                    try {
+                        MethodHandles.lookup().unreflectSetter(f);
+                        fail();
+                    } catch (IllegalAccessException expected) {}
+                    f.setAccessible(false);
+                    try {
+                        MethodHandles.lookup().unreflectSetter(f);
+                        fail();
+                    } catch (IllegalAccessException expected) {}
+                    MethodHandles.lookup().unreflectGetter(f);
+                }
             }
             {
                 // private field test
@@ -1015,10 +1025,7 @@ public class Main {
                     fail();
                 } catch (IllegalAccessException expected) {}
             }
-            if (DALVIK_RUN) {
-                // private static final field test
-                // for JVM it is not possible to get the unreflected setter for a static final
-                // field, see b/242985782
+            {
                 Field f = ValueHolder.class.getDeclaredField("s_fz");  // private static final field
                 try {
                     MethodHandles.lookup().unreflectSetter(f);
@@ -1029,13 +1036,14 @@ public class Main {
                     fail();
                 } catch (IllegalAccessException expected) {}
                 f.setAccessible(true);
-                // Setter is okay despite being final because field isAccessible().
-                MethodHandles.lookup().unreflectSetter(f).invokeExact(false);
+                if (!STATIC_FINAL_CAN_BE_MODIFIED) {
+                    try {
+                        MethodHandles.lookup().unreflectSetter(f);
+                        fail();
+                    } catch (IllegalAccessException expected) {}
+                }
                 assertEquals(false, (boolean) MethodHandles.lookup().unreflectGetter(f).invokeExact()
                 );
-                MethodHandles.lookup().unreflectSetter(f).invokeExact(true);
-                assertEquals(true, (boolean) MethodHandles.lookup().unreflectGetter(f).invokeExact()
-                );
                 f.setAccessible(false);
                 try {
                     MethodHandles.lookup().unreflectSetter(f);
diff --git a/test/Android.bp b/test/Android.bp
index a447d4aa37..c96ebccc09 100644
--- a/test/Android.bp
+++ b/test/Android.bp
@@ -318,17 +318,15 @@ art_cc_defaults {
 art_cc_defaults {
     name: "art_gtest_defaults",
     defaults: [
-        "art_test_defaults",
-        "art_gtest_common_defaults",
+        "art_test_defaults", // Must be first to make the -Wno-* overrides work.
         "art_debug_defaults",
+        "art_gtest_common_defaults",
+        "libartd-gtest_static_defaults",
     ],
     test_suites: ["art-host-tests"],
     test_options: {
         test_suite_tag: ["art-host-gtest"],
     },
-    static_libs: [
-        "libartd-gtest",
-    ],
     // Reduce test executable size by disabling automatic export of static lib symbols.
     // Don't use --exclude-libs=ALL, because it breaks tests under ASAN by hiding __asan* symbols.
     ldflags: [
@@ -338,6 +336,7 @@ art_cc_defaults {
 
 art_cc_defaults {
     name: "art_standalone_gtest_defaults",
+
     defaults: [
         // Note: We don't include "art_debug_defaults" here, as standalone ART
         // gtests link with the "non-d" versions of the libraries contained in
@@ -345,11 +344,19 @@ art_cc_defaults {
         // (including the Release ART APEX).
         "art_standalone_test_defaults",
         "art_gtest_common_defaults",
+
+        // This dependency links the whole runtime statically into the test. Note that the boot
+        // classpath is not (normally) bundled with the test, so if the runtime is used to actually
+        // start a VM it may load the boot classpath from the device. Depending on the test
+        // configuration, that may not be in sync with the statically linked runtime.
+        "libart-gtest_static_defaults",
     ],
+
     test_suites: [
         "general-tests",
         "mts-art",
     ],
+
     // Support multilib variants (using different suffix per sub-architecture), which is needed on
     // build targets with secondary architectures, as the MTS test suite packaging logic flattens
     // all test artifacts into a single `testcases` directory.
@@ -362,13 +369,7 @@ art_cc_defaults {
             suffix: "64",
         },
     },
-    static_libs: [
-        // This dependency links the whole runtime statically into the test. Note that the boot
-        // classpath is not (normally) bundled with the test, so if the runtime is used to actually
-        // start a VM it may load the boot classpath from the device. Depending on the test
-        // configuration, that may not be in sync with the statically linked runtime.
-        "libart-gtest",
-    ],
+
     version_script: ":art-standalone-gtest-version",
 }
 
@@ -412,7 +413,7 @@ art_cc_defaults {
 }
 
 // Properties common to `libart-gtest` and `libartd-gtest`.
-art_cc_defaults {
+cc_defaults {
     name: "libart-gtest-common",
     defaults: [
         "art_defaults",
@@ -421,10 +422,15 @@ art_cc_defaults {
     srcs: [
         "common/gtest_main.cc",
     ],
+    header_libs: [
+        "libart_headers",
+        "libnativehelper_header_only",
+    ],
     whole_static_libs: [
         "libgtest_isolated",
     ],
-    shared_libs: [
+    shared_libs: [ // Since we're building static libs we're only using the headers from these.
+        "libdexfile",
         "liblog",
     ],
     target: {
@@ -440,15 +446,23 @@ art_cc_defaults {
     },
 }
 
-art_cc_library_static {
+cc_library_static {
     name: "libart-gtest",
     defaults: [
         "libart-gtest-common",
     ],
+}
+
+cc_defaults {
+    name: "libart-gtest_static_defaults",
+    defaults: [
+        "libart-compiler-gtest_static_defaults",
+        "libart-runtime-gtest_static_defaults",
+        "libartbase-art-gtest_static_defaults",
+        "libdexfile_static_defaults",
+    ],
     whole_static_libs: [
-        "libart-compiler-gtest",
-        "libart-runtime-gtest",
-        "libartbase-art-gtest",
+        "libart-gtest",
     ],
 }
 
@@ -458,10 +472,18 @@ art_cc_library_static {
         "art_debug_defaults",
         "libart-gtest-common",
     ],
+}
+
+cc_defaults {
+    name: "libartd-gtest_static_defaults",
+    defaults: [
+        "libartbased-art-gtest_static_defaults",
+        "libartd-compiler-gtest_static_defaults",
+        "libartd-runtime-gtest_static_defaults",
+        "libdexfiled_static_defaults",
+    ],
     whole_static_libs: [
-        "libartd-compiler-gtest",
-        "libartd-runtime-gtest",
-        "libartbased-art-gtest",
+        "libartd-gtest",
     ],
 }
 
@@ -938,6 +960,7 @@ cc_defaults {
         "2262-miranda-methods/jni_invoke.cc",
         "2270-mh-internal-hiddenapi-use/mh-internal-hidden-api.cc",
         "2275-pthread-name/native_getname.cc",
+        "2392-virtual-thread-pinning-jni/jni.cc",
         "common/runtime_state.cc",
         "common/stack_inspect.cc",
     ],
diff --git a/test/ArtTest.xml b/test/ArtTest.xml
index 05d7e96d68..29ebde8df7 100644
--- a/test/ArtTest.xml
+++ b/test/ArtTest.xml
@@ -14,14 +14,12 @@
      limitations under the License.
 -->
 <configuration description="ART generic test runner">
-    <option name="test-suite-tag" value="ArtTest" />
+    <option name="test-suite-tag" value="art-test" />
 
     <target_preparer class="com.android.tradefed.targetprep.InstallApexModuleTargetPreparer" >
         <option name="test-file-name" value="com.android.art.testing.apex" />
     </target_preparer>
 
-    <!--
     <test class="com.android.tradefed.testtype.ArtTest">
     </test>
-    -->
 </configuration>
diff --git a/test/common/runtime_state.cc b/test/common/runtime_state.cc
index 465e2f6850..322db6c9e8 100644
--- a/test/common/runtime_state.cc
+++ b/test/common/runtime_state.cc
@@ -364,13 +364,17 @@ extern "C" JNIEXPORT jboolean JNICALL Java_Main_hasSingleImplementation(JNIEnv*
                                                                         jclass,
                                                                         jclass cls,
                                                                         jstring method_name) {
-  ArtMethod* method = nullptr;
   ScopedObjectAccess soa(Thread::Current());
   ScopedUtfChars chars(env, method_name);
   CHECK(chars.c_str() != nullptr);
-  method = soa.Decode<mirror::Class>(cls)->FindDeclaredVirtualMethodByName(
-      chars.c_str(), kRuntimePointerSize);
-  return method->HasSingleImplementation();
+  for (ArtMethod& method : soa.Decode<mirror::Class>(cls)->GetMethods(kRuntimePointerSize)) {
+    if (method.IsVirtual() && method.GetName() == std::string_view(chars.c_str())) {
+      ArtMethod* const np_method = method.GetInterfaceMethodIfProxy(kRuntimePointerSize);
+      return np_method->HasSingleImplementation();
+    }
+  }
+  LOG(FATAL) << "Should have found a method: " << std::string(chars.c_str());
+  return false;
 }
 
 extern "C" JNIEXPORT int JNICALL Java_Main_getHotnessCounter(JNIEnv* env,
diff --git a/test/default_run.py b/test/default_run.py
index 21673f79a3..a89a856cad 100755
--- a/test/default_run.py
+++ b/test/default_run.py
@@ -949,6 +949,8 @@ def default_run(ctx, args, **kwargs):
     skip_reg_exp = fr'#-# #:#:# # # ({skip_tag_set}) [^\n]*\n'
     skip_reg_exp = skip_reg_exp.replace('#', '[0-9.]+').replace(' ', ' +')
     ctx.run(fr"sed -i -z -E 's/{skip_reg_exp}//g' '{args.stderr_file}'")
+    ctx.run(fr"sed -i -E '/^.* E aconfig_cpp_codegen: error: failed to get package map file: failed to open/d' '{args.stderr_file}'")
+    ctx.run(fr"sed -i -E '/^.* E aconfig_cpp_codegen: error: package does not exist, returning flag default value./d' '{args.stderr_file}'")
     if not HAVE_IMAGE:
       message = "(Unable to open file|Could not create image space)"
       ctx.run(fr"sed -i -E '/^.* E dalvikvm(|32|64): .* {message}/d' '{args.stderr_file}'")
@@ -976,6 +978,9 @@ def default_run(ctx, args, **kwargs):
     # namespace, that gives libarttest(d).so full access to the internal ART
     # libraries.
     LD_LIBRARY_PATH = f"/data/{TEST_DIRECTORY}/com.android.art/lib{SUFFIX64}:{LD_LIBRARY_PATH}"
+    # TODO: Remove once testing apex is gone. The libs are copied into further subdirectory.
+    #       We intend to remove the testing apex, so this should be short lived work-around.
+    LD_LIBRARY_PATH = f"/apex/com.android.art/lib{SUFFIX64}/com.android.art/lib{SUFFIX64}:{LD_LIBRARY_PATH}"
     dlib = ("" if TEST_IS_NDEBUG else "d")
     art_test_internal_libraries = [
         f"libartagent{dlib}.so",
diff --git a/test/knownfailures.json b/test/knownfailures.json
index 4d2d8ae654..4b66f51b55 100644
--- a/test/knownfailures.json
+++ b/test/knownfailures.json
@@ -1141,6 +1141,7 @@
                   "692-vdex-inmem-loader",
                   "693-vdex-inmem-loader-evict",
                   "723-string-init-range",
+                  "733-priority-mappings",
                   "734-duplicate-fields",
                   "735-interface-clone",
                   "736-interface-super-Object",
@@ -1616,6 +1617,21 @@
       "description": ["Fails on QEMU"],
       "env_vars": {"ART_TEST_ON_VM": "true"}
     },
+    {
+      "tests": ["2042-reference-processing"],
+      "description": ["Timeout on QEMU"],
+      "env_vars": {"ART_TEST_ON_VM": "true"}
+    },
+    {
+      "tests": ["2390-virtual-thread-carrier-leak",
+                "2390-virtual-thread-context-leak",
+                "2390-virtual-thread-parking-error-leak",
+                "2391-virtual-thread-sleeps",
+                "2392-virtual-thread-pinning-jni",
+                "2393-virtual-thread-pinning-monitor"],
+      "variant": "jvm",
+      "description": ["Tests for ART-specific Virtual Thread APIs."]
+    },
     {
         "tests": ["169-threadgroup-jni"],
         "variant": "jvm",
diff --git a/test/run_test_build.py b/test/run_test_build.py
index 3b6ac73a33..6a694cc345 100755
--- a/test/run_test_build.py
+++ b/test/run_test_build.py
@@ -55,6 +55,19 @@ RBE_D8_DISABLED_FOR = {
   "979-const-method-handle",  # b/228312861: RBE uses wrong inputs.
 }
 
+TRADEFED_DISABLED = {
+  "2031-zygote-compiled-frame-deopt",
+  "2254-class-value-before-and-after-u",
+  "656-annotation-lookup-generic-jni",
+  "674-hiddenapi",
+  "677-fsi",
+  "677-fsi2",
+  "689-zygote-jit-deopt",
+  "728-imt-conflict-zygote",
+  "817-hiddenapi",
+  "900-hello-plugin",
+}
+
 # Debug option. Report commands that are taking a lot of user CPU time.
 REPORT_SLOW_COMMANDS = False
 
@@ -562,6 +575,9 @@ def create_ci_runner_scripts(out, mode, test_names):
   setup = out / "setup.sh"
   setup_script = create_setup_script(False) + create_setup_script(True)
   setup.write_text("\n".join(setup_script))
+  test_names = list(set(test_names) - TRADEFED_DISABLED)
+  if not test_names:
+    return {}
 
   python = sys.executable
   script = 'art/test/testrunner/testrunner.py'
diff --git a/test/testrunner/target_config.py b/test/testrunner/target_config.py
index c7583172ad..495199e2a3 100644
--- a/test/testrunner/target_config.py
+++ b/test/testrunner/target_config.py
@@ -232,8 +232,10 @@ target_config = {
 
     # ASAN (host) configurations.
 
-    # These configurations need detect_leaks=0 to work in non-setup environments like build bots,
-    # as our build tools leak. b/37751350
+    # These configurations need detect_leaks=0 to work in non-setup environments
+    # like build bots, as our build tools leak. It gets overridden in
+    # Android.gtest.mk to enable leak detection again during the test runs.
+    # b/37751350
 
     'art-gtest-asan': {
         'make' : 'test-art-host-gtest',
diff --git a/test/ti-agent/jni_binder.cc b/test/ti-agent/jni_binder.cc
index e85dc83e44..2192a16a20 100644
--- a/test/ti-agent/jni_binder.cc
+++ b/test/ti-agent/jni_binder.cc
@@ -16,6 +16,8 @@
 
 #include "jni_binder.h"
 
+#include <algorithm>
+
 #include <dlfcn.h>
 #include <inttypes.h>
 #include <stdio.h>
diff --git a/test/utils/regen-test-files b/test/utils/regen-test-files
index 4e5a9723c2..cb991ca50b 100755
--- a/test/utils/regen-test-files
+++ b/test/utils/regen-test-files
@@ -293,8 +293,6 @@ art_gtest_postsubmit_only_module_names = [
 
 # ART gtests not supported in MTS.
 art_gtest_modules_excluded_from_mts = [
-    # TODO(b/347717488): Consider adding this test to ART MTS.
-    "libnativebridge-tests",
 ]
 
 # ART gtests supported in MTS that do not need root access to the device.
@@ -363,23 +361,8 @@ failing_tests_excluded_from_test_mapping = {
   # Empty.
 }
 
-# Tests failing because of linking issues, currently exluded from MTS
-# and Mainline Presubmits to minimize noise in continuous runs while
-# we investigate.
-#
-# Example of admissible values in this dictionary: same as for
-# `failing_tests_excluded_from_test_mapping` (see above).
-#
-# TODO(b/247108425): Address the linking issues and re-enable these
-# tests.
-failing_tests_excluded_from_mts_and_mainline_presubmits = {
-    "art_standalone_compiler_tests": ["JniCompilerTest*"],
-    "art_standalone_libartpalette_tests": ["PaletteClientJniTest*"],
-}
-
 failing_tests_excluded_from_mainline_presubmits = (
-  failing_tests_excluded_from_test_mapping |
-  failing_tests_excluded_from_mts_and_mainline_presubmits
+  failing_tests_excluded_from_test_mapping
 )
 
 # Is `run_test` a Checker test (i.e. a test containing Checker
@@ -840,11 +823,6 @@ class Generator:
                        [],
                        "")
 
-    # Android Virtualization Framework presubmits
-    avf_presubmit_tests = ["ComposHostTestCases"]
-    avf_presubmit_tests_dict = gen_tests_dict(avf_presubmit_tests,
-                                              failing_tests_excluded_from_test_mapping)
-
     # Presubmits.
     other_presubmit_tests = [
         "ArtServiceTests",
@@ -877,7 +855,6 @@ class Generator:
             ("mainline-presubmit", mainline_presubmit_tests_dict),
             ("presubmit", presubmit_tests_dict),
             ("hwasan-presubmit", hwasan_presubmit_tests_dict),
-            ("avf-presubmit", avf_presubmit_tests_dict),
             ("postsubmit", postsubmit_tests_dict),
         ]
         if test_group_dict
@@ -1000,13 +977,6 @@ class Generator:
       for testcase in flaky_tests_excluded_from_mts[module]:
         append_test_exclusion(f"{module} {testcase}")
 
-    # Excluded failing tests.
-    xml_comment = root.createComment(" Excluded failing tests (b/247108425). ")
-    configuration.appendChild(xml_comment)
-    for module in failing_tests_excluded_from_mts_and_mainline_presubmits:
-      for testcase in failing_tests_excluded_from_mts_and_mainline_presubmits[module]:
-        append_test_exclusion(f"{module} {testcase}")
-
     xml_str = root.toprettyxml(indent = XML_INDENT, encoding = "utf-8")
 
     mts_art_tests_list_user_file = os.path.join(self.mts_config_dir, "mts-art-tests-list-user.xml")
diff --git a/tools/ahat/README.txt b/tools/ahat/README.txt
index 526a64666a..6512a506a2 100644
--- a/tools/ahat/README.txt
+++ b/tools/ahat/README.txt
@@ -20,13 +20,10 @@ Usage:
 TODO:
  * Add a user guide.
  * Dim 'image' and 'zygote' heap sizes slightly? Why do we even show these?
- * Let user re-sort sites objects info by clicking column headers.
- * Let user re-sort "Objects" list.
  * Show site context and heap and class filter in "Objects" view?
+ * Add option to sort on more table columns as desired.
  * Have a menu at the top of an object view with links to the sections?
  * Include ahat version and hprof file in the menu at the top of the page?
- * Heaped Table
-   - Make sortable by clicking on headers.
  * For HeapTable with single heap shown, the heap name isn't centered?
  * Consistently document functions.
  * Show version number with --version.
@@ -48,9 +45,6 @@ Things to Test:
    showing all the instances.
  * Instance.getDexCacheLocation
 
-Reported Issues:
- * Request to be able to sort tables by size.
-
 Known Issues:
  * Line number decoding for allocations in proguarded classes.
 
diff --git a/tools/ahat/src/main/com/android/ahat/DocString.java b/tools/ahat/src/main/com/android/ahat/DocString.java
index ca5dbf06ce..b4aee90b2e 100644
--- a/tools/ahat/src/main/com/android/ahat/DocString.java
+++ b/tools/ahat/src/main/com/android/ahat/DocString.java
@@ -226,4 +226,23 @@ class DocString {
   public String html() {
     return mStringBuilder.toString();
   }
+
+  @Override
+  public String toString() {
+    return html();
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (this == obj) {
+      return true;
+    }
+
+    if (obj == null || getClass() != obj.getClass()) {
+      return false;
+    }
+
+    DocString other = (DocString) obj;
+    return html().equals(other.html());
+  }
 }
diff --git a/tools/ahat/src/main/com/android/ahat/SiteHandler.java b/tools/ahat/src/main/com/android/ahat/SiteHandler.java
index 671784efca..f4f776fc12 100644
--- a/tools/ahat/src/main/com/android/ahat/SiteHandler.java
+++ b/tools/ahat/src/main/com/android/ahat/SiteHandler.java
@@ -81,18 +81,22 @@ class SiteHandler implements AhatHandler {
     }
 
     doc.section("Objects Allocated");
-    SizeTable.table(doc, mSnapshot.isDiffed(),
-        new Column("Instances", Column.Align.RIGHT),
-        new Column("", Column.Align.RIGHT, mSnapshot.isDiffed()),
-        new Column("Heap"),
-        new Column("Class"));
+    Comparator<Site.ObjectsInfo> defaultCompare = Sort.withPriority(
+        Sort.OBJECTS_INFO_BY_HEAP_NAME, Sort.OBJECTS_INFO_BY_SIZE, Sort.OBJECTS_INFO_BY_CLASS_NAME);
+    Sorter<Site.ObjectsInfo> sorter = new Sorter<>(query, defaultCompare);
+    sorter.addKey("Instances", Comparator.comparingLong(x -> x.numInstances));
+    sorter.addKey("Instances.delta",
+        Comparator.comparingLong(x -> x.numInstances - x.getBaseline().numInstances));
+
+    SizeTable.SortSpec<Site.ObjectsInfo> spec =
+        new SizeTable.SortSpec<>(sorter, "objects", x -> x.numBytes, x -> x.getBaseline().numBytes);
+    SizeTable.table(doc, mSnapshot.isDiffed(), spec,
+        new Column(sorter.link("Instances", "Instances"), Column.Align.RIGHT),
+        new Column(sorter.link("Instances.delta", ""), Column.Align.RIGHT, mSnapshot.isDiffed()),
+        new Column("Heap"), new Column("Class"));
 
     List<Site.ObjectsInfo> infos = site.getObjectsInfos();
-    Comparator<Site.ObjectsInfo> compare = Sort.withPriority(
-        Sort.OBJECTS_INFO_BY_HEAP_NAME,
-        Sort.OBJECTS_INFO_BY_SIZE,
-        Sort.OBJECTS_INFO_BY_CLASS_NAME);
-    Collections.sort(infos, compare);
+    sorter.sort(infos);
     SubsetSelector<Site.ObjectsInfo> selector
       = new SubsetSelector(query, OBJECTS_ALLOCATED_ID, infos);
     for (Site.ObjectsInfo info : selector.selected()) {
diff --git a/tools/ahat/src/main/com/android/ahat/SizeTable.java b/tools/ahat/src/main/com/android/ahat/SizeTable.java
index 46e395669f..4ca71bcb98 100644
--- a/tools/ahat/src/main/com/android/ahat/SizeTable.java
+++ b/tools/ahat/src/main/com/android/ahat/SizeTable.java
@@ -17,9 +17,12 @@
 package com.android.ahat;
 
 import com.android.ahat.heapdump.Size;
+
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Comparator;
 import java.util.List;
+import java.util.function.Function;
 
 /**
  * Class for rendering a table that includes all categories of Size.
@@ -31,6 +34,80 @@ import java.util.List;
  *    |Java Size|Native Size|...|Total Size|custom columns...|
  */
 class SizeTable {
+  /**
+   * Specification to make a sortable size table.
+   * To make the internal columns of a SizeTable sortable, you need to provide
+   * the sorter instance for rows of the table, a sort key prefix that
+   * SizeTable can use for generating its own sort keys, and functions to
+   * extract the Size and base Size of an object.
+   */
+  public static class SortSpec<T> {
+    public Sorter<T> sorter;
+    public String sortKeyPrefix;
+    public Function<T, Size> getSize;
+    public Function<T, Size> getBaseSize;
+
+    public SortSpec(Sorter<T> sorter, String sortKeyPrefix, Function<T, Size> getSize,
+        Function<T, Size> getBaseSize) {
+      this.sorter = sorter;
+      this.sortKeyPrefix = sortKeyPrefix;
+      this.getSize = getSize;
+      this.getBaseSize = getBaseSize;
+    }
+  }
+
+  /**
+   * Start a sortable size table with a custom left column.
+   *
+   * |left column|Java Size|Native Size|...|Total Size|custom columns...|
+   *
+   * This should be followed by calls to the 'row' method to fill in the table
+   * contents and the 'end' method to end the table.
+   *
+   * @param doc The output document to write to.
+   * @param left The left column spec.
+   * @param showDiff Set to true if size diffs should be shown.
+   * @param sortSpec The sort specification.
+   * @param columns Custom columns to add to the table.
+   */
+  static <T> void table(
+      Doc doc, Column left, boolean showDiff, SortSpec<T> sortSpec, Column... columns) {
+    String javaKey = sortSpec.sortKeyPrefix + ".java";
+    String javaDeltaKey = sortSpec.sortKeyPrefix + ".java.delta";
+    String nativeKey = sortSpec.sortKeyPrefix + ".native";
+    String nativeDeltaKey = sortSpec.sortKeyPrefix + ".native.delta";
+    String totalKey = sortSpec.sortKeyPrefix + ".total";
+    String totalDeltaKey = sortSpec.sortKeyPrefix + ".total.delta";
+
+    Sorter<T> sorter = sortSpec.sorter;
+    sorter.addKey(javaKey, Comparator.comparingLong(x -> sortSpec.getSize.apply(x).getJavaSize()));
+    sorter.addKey(javaDeltaKey,
+        Comparator.comparingLong(x
+            -> sortSpec.getSize.apply(x).getJavaSize()
+                - sortSpec.getBaseSize.apply(x).getJavaSize()));
+    sorter.addKey(nativeKey,
+        Comparator.comparingLong(x -> sortSpec.getSize.apply(x).getRegisteredNativeSize()));
+    sorter.addKey(nativeDeltaKey,
+        Comparator.comparingLong(x
+            -> sortSpec.getSize.apply(x).getRegisteredNativeSize()
+                - sortSpec.getBaseSize.apply(x).getRegisteredNativeSize()));
+    sorter.addKey(totalKey, Comparator.comparingLong(x -> sortSpec.getSize.apply(x).getSize()));
+    sorter.addKey(totalDeltaKey,
+        Comparator.comparingLong(
+            x -> sortSpec.getSize.apply(x).getSize() - sortSpec.getBaseSize.apply(x).getSize()));
+
+    List<Column> cols = new ArrayList<Column>();
+    cols.add(left);
+    cols.add(new Column(sorter.link(javaKey, "Java Size"), Column.Align.RIGHT));
+    cols.add(new Column(sorter.link(javaDeltaKey, ""), Column.Align.RIGHT, showDiff));
+    cols.add(new Column(sorter.link(nativeKey, "Registered Native Size"), Column.Align.RIGHT));
+    cols.add(new Column(sorter.link(nativeDeltaKey, ""), Column.Align.RIGHT, showDiff));
+    cols.add(new Column(sorter.link(totalKey, "Total Size"), Column.Align.RIGHT));
+    cols.add(new Column(sorter.link(totalDeltaKey, ""), Column.Align.RIGHT, showDiff));
+    cols.addAll(Arrays.asList(columns));
+    doc.table(cols.toArray(new Column[cols.size()]));
+  }
+
   /**
    * Start a size table with a custom left column.
    *
@@ -88,6 +165,21 @@ class SizeTable {
     table(doc, new Column("", Column.Align.LEFT, false), showDiff, columns);
   }
 
+  /**
+   * Start a sortable size table without a custom left column.
+   *
+   * |Java Size|Native Size|...|Total Size|custom columns...|
+   * This should be followed by calls to the 'row' method to fill in the table
+   * contents and the 'end' method to end the table.
+   *
+   * Set showDiff to true if size diffs should be shown.
+   */
+  static <T> void table(Doc doc, boolean showDiff, SortSpec<T> sortSpec, Column... columns) {
+    // Re-use the code for a size table with custom left column by having an
+    // invisible custom left column.
+    table(doc, new Column("", Column.Align.LEFT, false), showDiff, sortSpec, columns);
+  }
+
   /**
    * Add a row to the currently active size table without a custom left column.
    * The number of values must match the number of columns provided for the
diff --git a/tools/ahat/src/main/com/android/ahat/Sorter.java b/tools/ahat/src/main/com/android/ahat/Sorter.java
new file mode 100644
index 0000000000..69933b3a98
--- /dev/null
+++ b/tools/ahat/src/main/com/android/ahat/Sorter.java
@@ -0,0 +1,126 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.android.ahat;
+
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * UI support to allow a user to control how things are sorted.
+ * You can instantiate a Sorter for a particular list of objects you want
+ * sorted some way in the page output, such as rows of a table. The current
+ * sort order is controlled by selecting a specific key to sort the objects
+ * on.
+ *
+ * The Sorter class takes care of encoding the chosen sort keys and directions
+ * in the URL, generating links the user can click on to select the sort key
+ * and direction, and performing a sort based on the currently selected
+ * settings.
+ *
+ * Typical usage is to construct the Sorter for a specific page query, add one
+ * or more sort keys, get links associated for those keys to display on the
+ * page, and finally call the sort method to sort the objects in question.
+ *
+ * The Sorter will ignore any sort keys in the URL that it doesn't know about.
+ * That makes it possible to have multiple different sorters on the same page,
+ * as long as they don't have any sort keys in common.
+ */
+class Sorter<T> {
+  private Query mQuery;
+  private Map<String, Comparator<T>> mKeys;
+  private Comparator<T> mDefaultCompare;
+
+  /**
+   * Constructs a sorter object.
+   * @param query A query for the page containing the sort specification.
+   * @param defaultCompare Default comparison to use for sort.
+   */
+  Sorter(Query query, Comparator<T> defaultCompare) {
+    mQuery = query;
+    mKeys = new HashMap<>();
+    mDefaultCompare = defaultCompare;
+  }
+
+  /**
+   * Defines a sort key to use in this sorter.
+   * @param key A unique string used to identify the sort key.
+   * @param compare Comparator to use if the sort key is selected.
+   */
+  void addKey(String key, Comparator<T> compare) {
+    mKeys.put(key, compare);
+  }
+
+  /**
+   * Returns a sort link for the given sort key.
+   *
+   * @param key the sort key for the link to control.
+   * @param text the display contents for the link.
+   *
+   * @return A link that users can click to control the sort key.
+   */
+  DocString link(String key, String text) {
+    if (!mKeys.containsKey(key)) {
+      // We know nothing about this key. No need to link anything.
+      return DocString.text(text);
+    }
+
+    String sort = mQuery.get("sort", null);
+    boolean sorting = sort != null && key.equals(sort.substring(1));
+
+    if (sorting && sort.startsWith("-")) {
+      // Descending sort is currently enabled on this key. Add a link that
+      // enables ascending sort when you click on it.
+      return DocString.link(mQuery.with("sort", "+" + key), DocString.text(text + ""));
+    }
+
+    if (sorting && sort.startsWith("+")) {
+      // Ascending sort is currently enabled on this key. Add a link that
+      // reverts back to the default sort when you click on it.
+      return DocString.link(mQuery.with("sort", null), DocString.text(text + ""));
+    }
+
+    // No sort is currently enabled on this key. Add a link that enables
+    // descending sort when you click on it.
+    return DocString.link(mQuery.with("sort", "-" + key), DocString.text(text));
+  }
+
+  /**
+   * Sorts the given list based on the current sort options.
+   */
+  void sort(List<T> list) {
+    Comparator<T> compare = mDefaultCompare;
+
+    String sort = mQuery.get("sort", null);
+    if (sort != null) {
+      Comparator<T> selected = mKeys.get(sort.substring(1));
+      if (selected != null) {
+        if (sort.startsWith("+")) {
+          compare = selected.thenComparing(compare);
+        } else if (sort.startsWith("-")) {
+          compare = selected.reversed().thenComparing(compare);
+        } else {
+          // Ignore malformed sort value.
+        }
+      }
+    }
+
+    Collections.sort(list, compare);
+  }
+};
diff --git a/tools/ahat/src/main/com/android/ahat/heapdump/Parser.java b/tools/ahat/src/main/com/android/ahat/heapdump/Parser.java
index 3c16ce1ae6..fe6ad814aa 100644
--- a/tools/ahat/src/main/com/android/ahat/heapdump/Parser.java
+++ b/tools/ahat/src/main/com/android/ahat/heapdump/Parser.java
@@ -19,11 +19,13 @@ package com.android.ahat.heapdump;
 import com.android.ahat.progress.NullProgress;
 import com.android.ahat.progress.Progress;
 import com.android.ahat.proguard.ProguardMap;
+
 import java.io.File;
 import java.io.IOException;
 import java.nio.BufferUnderflowException;
 import java.nio.ByteBuffer;
 import java.nio.channels.FileChannel;
+import java.nio.channels.SeekableByteChannel;
 import java.nio.charset.StandardCharsets;
 import java.nio.file.StandardOpenOption;
 import java.util.ArrayList;
@@ -277,7 +279,7 @@ public class Parser {
 
           case 0x0C:   // HEAP DUMP
           case 0x1C: { // HEAP DUMP SEGMENT
-            int endOfRecord = hprof.tell() + recordLength;
+            long endOfRecord = hprof.tell() + recordLength;
             if (classById == null) {
               classById = new Instances<AhatClassObj>(classes);
             }
@@ -692,18 +694,18 @@ public class Parser {
 
   private static class ClassInstData {
     // The byte position in the hprof file where instance field data starts.
-    public int position;
+    public long position;
 
-    public ClassInstData(int position) {
+    public ClassInstData(long position) {
       this.position = position;
     }
   }
 
   private static class ObjArrayData {
     public int length;          // Number of array elements.
-    public int position;        // Position in hprof file containing element data.
+    public long position; // Position in hprof file containing element data.
 
-    public ObjArrayData(int length, int position) {
+    public ObjArrayData(int length, long position) {
       this.length = length;
       this.position = position;
     }
@@ -905,115 +907,199 @@ public class Parser {
     }
   }
 
+  /**
+   * SeekableByteChannel interface for ByteBuffer.
+   */
+  private static class ByteBufferChannel implements SeekableByteChannel {
+    private final ByteBuffer mBuffer;
+
+    public ByteBufferChannel(ByteBuffer buffer) {
+      mBuffer = buffer;
+    }
+
+    @Override
+    public long position() throws IOException {
+      return mBuffer.position();
+    }
+
+    @Override
+    public SeekableByteChannel position(long newPosition) throws IOException {
+      mBuffer.position((int) newPosition);
+      return this;
+    }
+
+    @Override
+    public int read(ByteBuffer dst) throws IOException {
+      int read = 0;
+      while (dst.hasRemaining() && mBuffer.hasRemaining()) {
+        dst.put(mBuffer.get());
+        read++;
+      }
+      return read;
+    }
+
+    @Override
+    public long size() throws IOException {
+      return mBuffer.capacity();
+    }
+
+    @Override
+    public SeekableByteChannel truncate(long size) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public int write(ByteBuffer src) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public boolean isOpen() {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public void close() throws IOException {
+      throw new UnsupportedOperationException();
+    }
+  }
+
   /**
    * Wrapper around a ByteBuffer that presents a uniform interface for
    * accessing data from an hprof file.
    */
   private static class HprofBuffer {
     private boolean mIdSize8;
-    private final ByteBuffer mBuffer;
+    private final SeekableByteChannel mChannel;
+    private final ByteBuffer mBuffer = ByteBuffer.allocate(1024);
+    private long mBufferStartPosition = 0;
 
     public HprofBuffer(File path) throws IOException {
-      FileChannel channel = FileChannel.open(path.toPath(), StandardOpenOption.READ);
-      mBuffer = channel.map(FileChannel.MapMode.READ_ONLY, 0, channel.size());
-      channel.close();
+      mChannel = FileChannel.open(path.toPath(), StandardOpenOption.READ);
+      mBuffer.flip();
     }
 
     public HprofBuffer(ByteBuffer buffer) {
-      mBuffer = buffer;
+      mChannel = new ByteBufferChannel(buffer);
+      mBuffer.flip();
+    }
+
+    private ByteBuffer read(int num_bytes) throws IOException {
+      while (num_bytes > mBuffer.remaining()) {
+        mBufferStartPosition = mChannel.position() - mBuffer.remaining();
+        mBuffer.compact();
+        mChannel.read(mBuffer);
+        mBuffer.flip();
+      }
+      return mBuffer;
     }
 
     public void setIdSize8() {
       mIdSize8 = true;
     }
 
-    public boolean hasRemaining() {
-      return mBuffer.hasRemaining();
+    public boolean hasRemaining() throws IOException {
+      return mBuffer.hasRemaining() || mChannel.position() < mChannel.size();
     }
 
     /**
      * Returns the size of the file in bytes.
      */
-    public int size() {
-      return mBuffer.capacity();
+    public long size() throws IOException {
+      return mChannel.size();
     }
 
     /**
      * Return the current absolution position in the file.
      */
-    public int tell() {
-      return mBuffer.position();
+    public long tell() throws IOException {
+      return mBufferStartPosition + mBuffer.position();
     }
 
     /**
      * Seek to the given absolution position in the file.
      */
-    public void seek(int position) {
-      mBuffer.position(position);
+    public void seek(long position) throws IOException {
+      mChannel.position(position);
+      mBuffer.clear();
+      mBuffer.flip();
+      mBufferStartPosition = position;
     }
 
     /**
      * Skip ahead in the file by the given delta bytes. Delta may be negative
      * to skip backwards in the file.
      */
-    public void skip(int delta) {
+    public void skip(long delta) throws IOException {
       seek(tell() + delta);
     }
 
-    public int getU1() {
-      return mBuffer.get() & 0xFF;
+    public int getU1() throws IOException {
+      return read(1).get() & 0xFF;
     }
 
-    public int getU2() {
-      return mBuffer.getShort() & 0xFFFF;
+    public int getU2() throws IOException {
+      return read(2).getShort() & 0xFFFF;
     }
 
-    public int getU4() {
-      return mBuffer.getInt();
+    public int getU4() throws IOException {
+      return read(4).getInt();
     }
 
-    public long getId() {
+    public long getId() throws IOException {
       if (mIdSize8) {
-        return mBuffer.getLong();
+        return read(8).getLong();
       } else {
-        return mBuffer.getInt() & 0xFFFFFFFFL;
+        return read(4).getInt() & 0xFFFFFFFFL;
       }
     }
 
-    public boolean getBool() {
-      return mBuffer.get() != 0;
+    public boolean getBool() throws IOException {
+      return read(1).get() != 0;
     }
 
-    public char getChar() {
-      return mBuffer.getChar();
+    public char getChar() throws IOException {
+      return read(2).getChar();
     }
 
-    public float getFloat() {
-      return mBuffer.getFloat();
+    public float getFloat() throws IOException {
+      return read(4).getFloat();
     }
 
-    public double getDouble() {
-      return mBuffer.getDouble();
+    public double getDouble() throws IOException {
+      return read(8).getDouble();
     }
 
-    public byte getByte() {
-      return mBuffer.get();
+    public byte getByte() throws IOException {
+      return read(1).get();
     }
 
-    public void getBytes(byte[] bytes) {
-      mBuffer.get(bytes);
+    public void getBytes(byte[] bytes) throws IOException {
+      if (mBuffer.remaining() >= bytes.length) {
+        mBuffer.get(bytes);
+        return;
+      }
+
+      ByteBuffer buf = ByteBuffer.wrap(bytes);
+      buf.put(mBuffer);
+      while (buf.hasRemaining()) {
+        mChannel.read(buf);
+      }
+      mBuffer.clear();
+      mBuffer.flip();
+      mBufferStartPosition = mChannel.position();
     }
 
-    public short getShort() {
-      return mBuffer.getShort();
+    public short getShort() throws IOException {
+      return read(2).getShort();
     }
 
-    public int getInt() {
-      return mBuffer.getInt();
+    public int getInt() throws IOException {
+      return read(4).getInt();
     }
 
-    public long getLong() {
-      return mBuffer.getLong();
+    public long getLong() throws IOException {
+      return read(8).getLong();
     }
 
     private static Type[] TYPES = new Type[] {
@@ -1022,7 +1108,7 @@ public class Parser {
         Type.BYTE, Type.SHORT, Type.INT, Type.LONG
     };
 
-    public Type getType() throws HprofFormatException {
+    public Type getType() throws HprofFormatException, IOException {
       int id = getU1();
       Type type = id < TYPES.length ? TYPES[id] : null;
       if (type == null) {
@@ -1031,7 +1117,7 @@ public class Parser {
       return type;
     }
 
-    public Type getPrimitiveType() throws HprofFormatException {
+    public Type getPrimitiveType() throws HprofFormatException, IOException {
       Type type = getType();
       if (type == Type.OBJECT) {
         throw new HprofFormatException("Expected primitive type, but found type 'Object'");
@@ -1043,7 +1129,7 @@ public class Parser {
      * Get a value from the hprof file, using the given instances map to
      * convert instance ids to their corresponding AhatInstance objects.
      */
-    public Value getValue(Type type, Instances instances) {
+    public Value getValue(Type type, Instances instances) throws IOException {
       switch (type) {
         case OBJECT:  return Value.pack(instances.get(getId()));
         case BOOLEAN: return Value.pack(getBool());
@@ -1063,7 +1149,7 @@ public class Parser {
      * DefferredInstanceValues rather than their corresponding AhatInstance
      * objects.
      */
-    public Value getDeferredValue(Type type) {
+    public Value getDeferredValue(Type type) throws IOException {
       switch (type) {
         case OBJECT: return new DeferredInstanceValue(getId());
         case BOOLEAN: return Value.pack(getBool());
diff --git a/tools/ahat/src/test/com/android/ahat/AhatTestSuite.java b/tools/ahat/src/test/com/android/ahat/AhatTestSuite.java
index abc3cc7561..3a3204d4e0 100644
--- a/tools/ahat/src/test/com/android/ahat/AhatTestSuite.java
+++ b/tools/ahat/src/test/com/android/ahat/AhatTestSuite.java
@@ -21,24 +21,11 @@ import org.junit.runner.RunWith;
 import org.junit.runners.Suite;
 
 @RunWith(Suite.class)
-@Suite.SuiteClasses({
-  DiffFieldsTest.class,
-  DiffTest.class,
-  DominatorsTest.class,
-  HtmlEscaperTest.class,
-  InstanceTest.class,
-  NativeAllocationTest.class,
-  ObjectHandlerTest.class,
-  ObjectsHandlerTest.class,
-  OverviewHandlerTest.class,
-  PerformanceTest.class,
-  ProguardMapTest.class,
-  RootedHandlerTest.class,
-  QueryTest.class,
-  RiTest.class,
-  SiteHandlerTest.class,
-  SiteTest.class
-})
+@Suite.SuiteClasses({DiffFieldsTest.class, DiffTest.class, DominatorsTest.class,
+    HtmlEscaperTest.class, InstanceTest.class, NativeAllocationTest.class, ObjectHandlerTest.class,
+    ObjectsHandlerTest.class, OverviewHandlerTest.class, PerformanceTest.class,
+    ProguardMapTest.class, RootedHandlerTest.class, QueryTest.class, RiTest.class,
+    SiteHandlerTest.class, SiteTest.class, SorterTest.class})
 
 public class AhatTestSuite {
   public static void main(String[] args) {
diff --git a/tools/ahat/src/test/com/android/ahat/SorterTest.java b/tools/ahat/src/test/com/android/ahat/SorterTest.java
new file mode 100644
index 0000000000..2050067a50
--- /dev/null
+++ b/tools/ahat/src/test/com/android/ahat/SorterTest.java
@@ -0,0 +1,179 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.android.ahat;
+
+import static org.junit.Assert.assertEquals;
+
+import org.junit.Test;
+
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Comparator;
+import java.util.List;
+
+public class SorterTest {
+  // Creates a Sorter for use in tests from the given uri.
+  // The default sort is ascending longs.
+  // The sort key "abs" can be used to sort by absolute value.
+  // The sort key "parity" can be used to sort evens before odds.
+  private Sorter<Long> testSorter(String uri) throws URISyntaxException {
+    Comparator<Long> normal = Comparator.comparingLong(x -> x);
+    Comparator<Long> abs = Comparator.comparingLong(x -> x < 0 ? -x : x);
+    Comparator<Long> parity = Comparator.comparingLong(x -> x % 2);
+    Sorter<Long> sorter = new Sorter(new Query(new URI(uri)), normal);
+    sorter.addKey("abs", abs);
+    sorter.addKey("parity", parity);
+    return sorter;
+  }
+
+  // Creates a list of longs for test purposes.
+  private List<Long> testList(long... values) {
+    List<Long> list = new ArrayList<Long>(values.length);
+    for (int i = 0; i < values.length; ++i) {
+      list.add(values[i]);
+    }
+    return list;
+  }
+
+  @Test
+  public void defaultSort() throws URISyntaxException {
+    // Default sort should be done if no key is selected.
+    String uri = "http://localhost:7100/foo";
+    Sorter<Long> sorter = testSorter(uri);
+    List<Long> list = testList(4, 2, 5, 6, 0);
+    sorter.sort(list);
+
+    List<Long> want = testList(0, 2, 4, 5, 6);
+    assertEquals(want, list);
+  }
+
+  @Test
+  public void malformedSort() throws URISyntaxException {
+    // Default sort should be done if selected sort is malformed.
+    String uri = "http://localhost:7100/foo?sort=Xabs";
+    Sorter<Long> sorter = testSorter(uri);
+    List<Long> list = testList(4, 2, 5, 6, 0);
+    sorter.sort(list);
+
+    List<Long> want = testList(0, 2, 4, 5, 6);
+    assertEquals(want, list);
+  }
+
+  @Test
+  public void ascendingSort() throws URISyntaxException {
+    String uri = "http://localhost:7100/foo?sort=+abs";
+    Sorter<Long> sorter = testSorter(uri);
+    List<Long> list = testList(4, -2, 5, -6, 0);
+    sorter.sort(list);
+
+    List<Long> want = testList(0, -2, 4, 5, -6);
+    assertEquals(want, list);
+  }
+
+  @Test
+  public void descendingSort() throws URISyntaxException {
+    String uri = "http://localhost:7100/foo?sort=-abs";
+    Sorter<Long> sorter = testSorter(uri);
+    List<Long> list = testList(4, -2, 5, -6, 0);
+    sorter.sort(list);
+
+    List<Long> want = testList(-6, 5, 4, -2, 0);
+    assertEquals(want, list);
+  }
+
+  @Test
+  public void secondarySort() throws URISyntaxException {
+    // The default sort should be used as a secondary sort for otherwise equal
+    // keys.
+    String uri = "http://localhost:7100/foo?sort=+parity";
+    Sorter<Long> sorter = testSorter(uri);
+    List<Long> list = testList(5, 2, 1, 6, 0);
+    sorter.sort(list);
+
+    List<Long> want = testList(0, 2, 6, 1, 5);
+    assertEquals(want, list);
+  }
+
+  @Test
+  public void unusedSort() throws URISyntaxException {
+    // Unknown sort keys are ignored, falling back to default sort.
+    String uri = "http://localhost:7100/foo?sort=whatever";
+    Sorter<Long> sorter = testSorter(uri);
+    List<Long> list = testList(4, 2, 5, 6, 0);
+    sorter.sort(list);
+
+    List<Long> want = testList(0, 2, 4, 5, 6);
+    assertEquals(want, list);
+  }
+
+  @Test
+  public void defaultLink() throws URISyntaxException {
+    String uri = "http://localhost:7100/foo";
+    Sorter<Long> sorter = testSorter(uri);
+    DocString link = sorter.link("abs", "Hello");
+    DocString want = DocString.link(new URI("/foo?sort=-abs"), DocString.text("Hello"));
+    assertEquals(want, link);
+  }
+
+  @Test
+  public void malformedLink() throws URISyntaxException {
+    // Treat this as if there was nothing set.
+    String uri = "http://localhost:7100/foo?sort=Xabs";
+    Sorter<Long> sorter = testSorter(uri);
+    DocString link = sorter.link("abs", "Hello");
+    DocString want = DocString.link(new URI("/foo?sort=-abs"), DocString.text("Hello"));
+    assertEquals(want, link);
+  }
+
+  @Test
+  public void descendingLink() throws URISyntaxException {
+    String uri = "http://localhost:7100/foo?sort=-abs";
+    Sorter<Long> sorter = testSorter(uri);
+    DocString link = sorter.link("abs", "Hello");
+    DocString want = DocString.link(new URI("/foo?sort=+abs"), DocString.text("Hello"));
+    assertEquals(want, link);
+  }
+
+  @Test
+  public void ascendingLink() throws URISyntaxException {
+    String uri = "http://localhost:7100/foo?sort=+abs";
+    Sorter<Long> sorter = testSorter(uri);
+    DocString link = sorter.link("abs", "Hello");
+    DocString want = DocString.link(new URI("/foo?"), DocString.text("Hello"));
+    assertEquals(want, link);
+  }
+
+  @Test
+  public void unselectedLink() throws URISyntaxException {
+    String uri = "http://localhost:7100/foo?sort=+parity";
+    Sorter<Long> sorter = testSorter(uri);
+    DocString link = sorter.link("abs", "Hello");
+    DocString want = DocString.link(new URI("/foo?sort=-abs"), DocString.text("Hello"));
+    assertEquals(want, link);
+  }
+
+  @Test
+  public void unusedLink() throws URISyntaxException {
+    String uri = "http://localhost:7100/foo?sort=+abs";
+    Sorter<Long> sorter = testSorter(uri);
+    DocString link = sorter.link("whatever", "Hello");
+    DocString want = DocString.text("Hello");
+    assertEquals(want, link);
+  }
+}
diff --git a/tools/art_build.py b/tools/art_build.py
new file mode 100755
index 0000000000..204499c4be
--- /dev/null
+++ b/tools/art_build.py
@@ -0,0 +1,750 @@
+#!/usr/bin/env python3
+#
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import argparse
+import os
+import shlex
+import shutil
+import subprocess
+import sys
+from typing import Callable, Dict, List, Optional
+
+
+I18N_APEX = "com.android.i18n"
+TZDATA_APEX = "com.android.tzdata"
+CORE_IMG_JARS: List[str] = [
+    'core-oj',
+    'core-libart',
+    'okhttp',
+    'bouncycastle',
+    'apache-xml',
+]
+HOSTDEX_JARS: List[tuple[str, str]] = [
+    ("com.android.conscrypt", "conscrypt"),
+    ("com.android.i18n", "core-icu4j"),
+]
+
+# Define a more specific type for build variables, which are strings.
+BuildVarsDict = Dict[str, str]
+
+
+def run_subprocess(
+    command: List[str],
+    cwd: Optional[str] = None,
+    env: Optional[Dict[str, str]] = None,
+    capture_stdout: bool = False,
+) -> subprocess.CompletedProcess:
+    """Runs a subprocess command (always with shell=False). Exits on failure.
+
+    Args:
+        command: The command to execute as a list of strings. The first
+                 element is the program, subsequent are arguments.
+        cwd: The current working directory for the subprocess.
+        env: Environment variables to set for the subprocess.
+        capture_stdout: If True, stdout will be captured. stderr passes through.
+                        Defaults to False (both stdout/stderr pass through).
+    """
+    current_env = os.environ.copy()
+    if env:
+        current_env.update(env)
+
+    stdout_setting = subprocess.PIPE if capture_stdout else None
+    stderr_setting = None
+
+    try:
+        # The command is constructed from trusted sources (hardcoded values
+        # or build system variables). With shell=False, this call is not
+        # vulnerable to command injection.
+        # nosemgrep: default-ruleset.python.lang.security.audit.dangerous-subprocess-use-audit
+        result = subprocess.run(
+            command,
+            cwd=cwd,
+            env=current_env,
+            shell=False,
+            stdout=stdout_setting,
+            stderr=stderr_setting,
+            text=True,
+            check=True,
+        )
+        return result
+    except subprocess.CalledProcessError as e:
+        print(f"Error: Command failed with return code {e.returncode}")
+        print(f"  Command: {shlex.join(e.cmd)}")
+        if capture_stdout:
+            if e.stdout:
+                print(f"  Stdout (captured): {e.stdout.strip()}")
+            print("  (Stderr from the subprocess should have already printed")
+            print("   to the console above.)")
+        else:
+            print("  (Stdout and stderr from the subprocess should have already")
+            print("   printed to the console above.)")
+        sys.exit(1)
+    except Exception as e:
+        print(f"An unexpected error occurred during subprocess execution: {e}")
+        sys.exit(1)
+
+
+def _build_dumpvars_command(
+    vars_to_get: List[str]
+) -> List[str]:
+    """Constructs the command list for soong_ui.bash --dumpvars-mode.
+    Assumes CWD is the Android root.
+    """
+    soong_ui_path: str = "build/soong/soong_ui.bash"
+
+    vars_string: str = " ".join(vars_to_get)
+
+    command_list: List[str] = [
+        soong_ui_path,
+        "--dumpvars-mode",
+        f"--vars={vars_string}"
+    ]
+    return command_list
+
+
+def _parse_dumpvars_output(output_string: str) -> BuildVarsDict:
+    """Parses the standard output from soong_ui.bash --dumpvars-mode.
+
+    Args:
+        output_string: The raw string output from soong_ui.bash --dumpvars-mode.
+
+    Returns:
+        A BuildVarsDict (a dictionary with a predefined structure) containing
+        the requested build variables as string key-value pairs. Refer to
+        the BuildVarsDict type definition for the specific expected keys.
+    """
+    parsed_vars: BuildVarsDict = {}
+    output_lines: List[str] = output_string.strip().splitlines()
+
+    for line in output_lines:
+        # Dumpvars output looks like KEY='value' or KEY="value" or KEY=value
+        # Simple split on first '=' is usually sufficient.
+        if "=" in line:
+            key: str
+            value_with_quotes: str
+            key, value_with_quotes = line.split("=", 1)
+
+            # Remove potential quotes around the value and strip whitespace
+            value = value_with_quotes.strip()
+            if value.startswith("'") and value.endswith("'"):
+                value = value[1:-1]
+            elif value.startswith('"') and value.endswith('"'):
+                value = value[1:-1]
+
+            parsed_vars[key.strip()] = value
+        else:
+            # Lines without '=' are unexpected for variable output, might be
+            # warnings/info from the build system itself.
+            print(f"Error: Unparseable line from dumpvars output: "
+                  f"'{line}'")
+            sys.exit(1)
+
+    return parsed_vars
+
+
+def get_android_build_vars(
+    vars_to_get: List[str]
+) -> BuildVarsDict:
+    """Dumps specified variables from the Android build system.
+    Assumes CWD is already the Android root and necessary TARGET_* env vars
+    are set.
+    """
+    command_list = _build_dumpvars_command(vars_to_get=vars_to_get)
+
+    print(f"Executing build variable retrieval command in Android root "
+          f"({os.getcwd()}):")
+    print(f"  $ {shlex.join(command_list)}")
+
+    process_result = run_subprocess(
+        command_list,
+        capture_stdout=True
+    )
+
+    parsed_vars: BuildVarsDict = _parse_dumpvars_output(
+        process_result.stdout)
+
+    for var_name in vars_to_get:
+        if var_name not in parsed_vars or not parsed_vars.get(var_name):
+            print(f"Error: Essential variable '{var_name}' not found or "
+                  "empty in retrieved build variables. This indicates a "
+                  "problem with the build setup or dumpvars output.")
+            print("\nRetrieved Variables:")
+            for k, v in parsed_vars.items():
+                print(f"  {k}='{v}'")
+            sys.exit(1)
+
+    return parsed_vars
+
+
+def extract_from_apex(apex_name: str, build_vars: BuildVarsDict):
+    """Extract files from an APEX file"""
+    host_out = build_vars.get("HOST_OUT")
+    target_out = build_vars.get("TARGET_OUT")
+
+    print(f"Extracting from apex: {apex_name}")
+    apex_root = os.path.join(target_out, "apex")
+    apex_input_file = os.path.join(apex_root, f"{apex_name}.apex")
+    apex_out_dir = os.path.join(apex_root, apex_name)
+
+    if not os.path.exists(apex_input_file):
+        apex_input_file = os.path.join(apex_root, f"{apex_name}.capex")
+
+    if not os.path.exists(apex_input_file):
+        print(f"Error: APEX file for '{apex_name}' not found in "
+              f"'{apex_root}'.")
+        sys.exit(1)
+
+    shutil.rmtree(apex_out_dir, ignore_errors=True)
+    os.makedirs(apex_out_dir, exist_ok=True)
+
+    deapexer_path = os.path.join(host_out, "bin", "deapexer")
+    debugfs_path = os.path.join(host_out, "bin", "debugfs")
+    fsckerofs_path = os.path.join(host_out, "bin", "fsck.erofs")
+
+    deapexer_command = [
+        deapexer_path,
+        "--debugfs_path", debugfs_path,
+        "--fsckerofs_path", fsckerofs_path,
+        "extract",
+        apex_input_file,
+        apex_out_dir,
+    ]
+    run_subprocess(deapexer_command)
+
+    host_apex_out_dir = os.path.join(host_out, apex_name)
+    shutil.rmtree(host_apex_out_dir, ignore_errors=True)
+    os.makedirs(host_apex_out_dir, exist_ok=True)
+
+    etc_src = os.path.join(apex_out_dir, "etc")
+    etc_dest = os.path.join(host_apex_out_dir, "etc")
+    if os.path.exists(etc_src):
+        shutil.copytree(etc_src, etc_dest, dirs_exist_ok=True)
+    else:
+        print(f"No 'etc' directory found in extracted {apex_name}.")
+
+
+def perform_copy(source_path: str, target_path: str) -> None:
+    """
+    Performs a single file copy operation. Overwrites target.
+    Exits the script with status 1 if any error occurs.
+
+    Args:
+        source_path (str): The path to the source file.
+        target_path (str): The path to the target file.
+
+    Returns:
+        None
+    """
+    try:
+        if not os.path.exists(source_path):
+            print(f"  ERROR: Source file not found! ({source_path})",
+                  file=sys.stderr)
+            sys.exit(1)
+
+        target_dir: str = os.path.dirname(target_path)
+        os.makedirs(target_dir, exist_ok=True)
+
+        shutil.copy(source_path, target_path)
+
+    except Exception as e:
+        err_msg = (f"  ERROR: An unexpected error occurred during copy "
+                   f"({source_path} -> {target_path}): {e}")
+        print(err_msg, file=sys.stderr)
+        sys.exit(1)
+
+
+def host_i18n_data_action(build_vars: BuildVarsDict):
+    """Custom action to process i18n data."""
+    extract_from_apex(I18N_APEX, build_vars)
+
+
+def host_tzdata_data_action(build_vars: BuildVarsDict):
+    """Custom action to process tzdata data."""
+    extract_from_apex(TZDATA_APEX, build_vars)
+
+
+def _get_core_img_jar_source_path(
+    build_vars: BuildVarsDict, jar_basename: str
+) -> str:
+    """
+    Returns the full source path for a given core image JAR.
+    This path is where the build system places JARs for dexpreopting.
+    """
+    out_dir = build_vars.get("OUT_DIR")
+    target_arch = build_vars.get("TARGET_ARCH")
+    # Path corresponds to art/build/Android.common_path.mk variable
+    # CORE_IMG_JAR_DIR.
+    core_img_jar_dir = os.path.join(
+        out_dir, 'soong', f'dexpreopt_{target_arch}', 'dex_artjars_input'
+    )
+    return os.path.join(core_img_jar_dir, f"{jar_basename}.jar")
+
+
+def _get_hostdex_jar_source_path(
+    build_vars: BuildVarsDict, jar_basename: str
+) -> str:
+    """Returns the source path for a given hostdex JAR."""
+    host_out_java_libs = build_vars.get("HOST_OUT_JAVA_LIBRARIES")
+    return os.path.join(host_out_java_libs, f"{jar_basename}-hostdex.jar")
+
+
+def copy_core_img_jars_action(build_vars: BuildVarsDict):
+    """Copies JARs listed in the global CORE_IMG_JARS."""
+    target_base_dir = os.path.join(
+        build_vars.get("HOST_OUT"), 'apex/com.android.art/javalib'
+    )
+
+    for jar_base_name in CORE_IMG_JARS:
+        source: str = _get_core_img_jar_source_path(
+            build_vars, jar_base_name
+        )
+        target: str = os.path.join(target_base_dir, f"{jar_base_name}.jar")
+        perform_copy(source, target)
+
+
+def copy_hostdex_jars_action(build_vars: BuildVarsDict):
+    """Copies hostdex JARs to their respective APEX javalib dirs."""
+    # We can still use the host variant of `conscrypt` and `core-icu4j`
+    # because they don't go into the primary boot image that is used in
+    # host gtests, and hence can't lead to checksum mismatches.
+    host_out = build_vars.get("HOST_OUT")
+    for apex_name, jar_basename in HOSTDEX_JARS:
+        source = _get_hostdex_jar_source_path(build_vars, jar_basename)
+        target = os.path.join(
+            host_out, f'apex/{apex_name}/javalib/{jar_basename}.jar')
+        perform_copy(source, target)
+
+
+def copy_all_host_boot_image_jars_action(build_vars: BuildVarsDict):
+    """
+    Composite action to copy all necessary JARs for the host boot image.
+    This includes CORE_IMG_JARS, conscrypt, and i18n JARs.
+    """
+    copy_core_img_jars_action(build_vars)
+    copy_hostdex_jars_action(build_vars)
+
+
+class Target:
+    """Represents an internal build target with potential actions/dependencies.
+
+    Attributes:
+        name: The unique identifier for this internal target.
+        make_targets: List of actual Soong/Make targets required by this target.
+        dependencies: List of names of other internal Targets this one depends on.
+        action: An optional function to execute after make_targets are built.
+    """
+
+    def __init__(self, name: str,
+                 make_targets: Optional[List[str]] = None,
+                 dependencies: Optional[List[str]] = None,
+                 action: Optional[Callable] = None):
+        """Initializes a Target."""
+        self.name = name
+        self.make_targets = make_targets or []
+        self.dependencies = dependencies or []
+        self.action = action
+
+    def execute_post_build_action(self, build_vars: BuildVarsDict):
+        """Executes the target's post-build action, if defined."""
+        if self.action:
+            self.action(build_vars)
+
+
+class Builder:
+    """Manages internal target definitions and orchestrates the build process."""
+
+    def __init__(self):
+        """Initializes the Builder."""
+        self.targets: dict[str, Target] = {}
+        self.enabled_internal_targets: List[str] = []
+        self.positional_make_targets: List[str] = []
+        self.build_vars: Optional[BuildVarsDict] = None
+
+    def _get_copy_core_img_make_targets(self) -> List[str]:
+        """
+        Generates the list of make_targets (file paths) for CORE_IMG_JARS.
+        These are the source JARs that the copy_core_img_jars_action will
+        copy. Relies on self.build_vars having been set.
+        """
+        make_targets = []
+        for jar_base_name in CORE_IMG_JARS:
+            source_file_path = _get_core_img_jar_source_path(
+                self.build_vars, jar_base_name
+            )
+            make_targets.append(source_file_path)
+        return make_targets
+
+    def setup_default_targets(self, build_vars: BuildVarsDict):
+        """Defines built-in targets using build_vars and stores build_vars."""
+        self.build_vars = build_vars
+
+        # These test_* lists are placeholders; actual dependencies might differ.
+        test_art_host_deps = ["dalvikvm", "dexlist"]
+        test_art_target_deps = [
+            "com.android.art.testing", "com.android.conscrypt",
+            "com.android.i18n"
+        ]
+        test_target_core_img_outs = ["core-oj", "core-libart"]
+
+        # HOST_CORE_IMG_OUTS
+        copy_core_img_make_targets = self._get_copy_core_img_make_targets()
+        hostdex_jar_make_targets = [
+            _get_hostdex_jar_source_path(self.build_vars, basename)
+            for _, basename in HOSTDEX_JARS
+        ]
+        all_boot_image_make_targets = (copy_core_img_make_targets +
+                                       hostdex_jar_make_targets)
+        self.add_target(Target(
+            name="host_core_img_outs",
+            action=copy_all_host_boot_image_jars_action,
+            make_targets=all_boot_image_make_targets,
+        ))
+        # HOST_I18N_DATA
+        self.add_target(Target(
+            name="extract-host-i18n-data",
+            action=host_i18n_data_action,
+            make_targets=[I18N_APEX, "deapexer", "debugfs", "fsck.erofs"],
+        ))
+        # HOST_TZDATA_DATA
+        self.add_target(Target(
+            name="extract-host-tzdata-data",
+            action=host_tzdata_data_action,
+            make_targets=[TZDATA_APEX, "deapexer", "debugfs", "fsck.erofs"],
+        ))
+        self.add_target(Target(
+            name="build-art-host",
+            make_targets=(
+                ["art-script"] + test_art_host_deps
+            ),
+            dependencies=[
+                "host_core_img_outs",
+                "extract-host-i18n-data",
+                "extract-host-tzdata-data",
+            ],
+        ))
+        # build-art-host-gtests depends on build-art-host  and
+        #    $(ART_TEST_HOST_GTEST_DEPENDENCIES)
+        # ART_TEST_HOST_GTEST_DEPENDENCIES := $(HOST_I18N_DATA)
+        self.add_target(Target(
+            name="build-art-host-gtests",
+            dependencies=["build-art-host", "extract-host-i18n-data"],
+        ))
+        self.add_target(Target(
+            name="build-art-target",
+            make_targets=(
+                ["art-script"] + test_art_target_deps
+                + test_target_core_img_outs
+            ),
+        ))
+        self.add_target(Target(
+            name="build-art",
+            dependencies=["build-art-host", "build-art-target"],
+        ))
+
+    def add_target(self, target: Target):
+        """Adds or updates an internal target definition.
+
+        Args:
+            target: The Target object to add.
+        """
+        if target.name in self.targets:
+            print(f"Error: Redefining internal target '{target.name}'.")
+            sys.exit(1)
+        self.targets[target.name] = target
+
+    def _collect_recursive(self,
+                           target_name: str,
+                           collected_make_targets: List[str],
+                           collected_actions: List[Target],
+                           recursion_stack: set[str],
+                           processed_nodes: set[str]):
+        """Recursively collects dependencies, detecting cycles.
+
+        Internal helper for collect_targets. Modifies lists in place.
+
+        Args:
+            target_name: The name of the internal target to collect.
+            collected_make_targets: List to store collected make targets.
+            collected_actions: List to store collected Target objects with actions.
+            recursion_stack: A set of targets in the current traversal path,
+                             used to detect actual cycles.
+            processed_nodes: A set of targets that have already been fully
+                             processed, used to handle shared dependencies
+                             (like diamond dependencies) efficiently.
+        """
+        if target_name in processed_nodes:
+            return
+
+        # If we encounter a node that is already in our current recursion
+        # stack, we have found a genuine cycle.
+        if target_name in recursion_stack:
+            print(f"Error: Cycle detected in internal target dependencies "
+                  f"involving '{target_name}'.")
+            sys.exit(1)
+
+        if target_name not in self.targets:
+            print(f"Error: Definition error - Internal target '{target_name}' "
+                  "(specified as a dependency for another internal target) "
+                  "was not found in the defined internal targets"
+                  f" {self.targets}.")
+            sys.exit(1)
+
+        # Add the current target to the recursion stack for this path.
+        recursion_stack.add(target_name)
+        target = self.targets[target_name]
+
+        for dependency_name in target.dependencies:
+            self._collect_recursive(
+                dependency_name, collected_make_targets, collected_actions,
+                recursion_stack, processed_nodes
+            )
+
+        # After traversing all children, remove from the recursion stack and
+        # mark as fully processed.
+        recursion_stack.remove(target_name)
+        processed_nodes.add(target_name)
+
+        collected_make_targets.extend(target.make_targets)
+
+        if target.action:
+            collected_actions.append(target)
+
+    def collect_targets(self,
+                        target_name: str,
+                        collected_make_targets: List[str],
+                        collected_actions: List[Target],
+                        processed_nodes: set[str]):
+        """Collects make targets and actions for an internal target.
+
+        Handles the top-level call for recursive collection.
+
+        Args:
+            target_name: The name of the internal target to collect.
+            collected_make_targets: List to store collected make targets.
+            collected_actions: List to store collected Target objects with actions.
+            processed_nodes: A set of targets already processed in this build.
+        """
+        # The recursion stack is specific to each top-level traversal.
+        recursion_stack = set()
+        self._collect_recursive(target_name, collected_make_targets,
+                                collected_actions, recursion_stack,
+                                processed_nodes)
+
+    def build(self):
+        """Builds targets based on enabled internal and positional targets."""
+        if self.build_vars is None:
+            print("Error: build_vars not set in Builder. "
+                  "setup_default_targets must be called first.",
+                  file=sys.stderr)
+            sys.exit(1)
+
+        all_make_targets: List[str] = []
+        all_actions: List[Target] = []
+        # This set will track all nodes processed during this build run
+        # to avoid redundant work. It's passed through the collectors.
+        processed_nodes_for_build = set()
+
+        for internal_target_name in self.enabled_internal_targets:
+            if internal_target_name in self.targets:
+                self.collect_targets(internal_target_name,
+                                     all_make_targets,
+                                     all_actions,
+                                     processed_nodes_for_build)
+            else:
+                print(f"Error: Enabled internal target "
+                      f"'{internal_target_name}' not found in definitions. "
+                      "Exiting.")
+                sys.exit(1)
+
+        if self.positional_make_targets:
+            all_make_targets.extend(self.positional_make_targets)
+
+        unique_make_targets = list(dict.fromkeys(all_make_targets))
+
+        if unique_make_targets:
+            env_for_make = os.environ.copy()
+            frameworks_base_dir_path = "frameworks/base"
+            if not os.path.isdir(frameworks_base_dir_path):
+                # This is often necessary for reduced manifest branches (e.g.,
+                # master-art) to allow them to build successfully when certain
+                # framework dependencies are not present in the source tree.
+                print("Info: 'frameworks/base' directory not found at "
+                      f"'{os.path.abspath(frameworks_base_dir_path)}'.")
+                print("      Setting SOONG_ALLOW_MISSING_DEPENDENCIES=true and "
+                      "TARGET_BUILD_UNBUNDLED=true.")
+                env_for_make["SOONG_ALLOW_MISSING_DEPENDENCIES"] = "true"
+                env_for_make["TARGET_BUILD_UNBUNDLED"] = "true"
+
+            make_command = ["./build/soong/soong_ui.bash", "--make-mode"]
+            make_command.extend(unique_make_targets)
+
+            print_env_parts = []
+            # Only show a few key env vars for the log to avoid clutter
+            for key in ["TARGET_PRODUCT", "TARGET_RELEASE",
+                        "TARGET_BUILD_VARIANT",
+                        "SOONG_ALLOW_MISSING_DEPENDENCIES",
+                        "TARGET_BUILD_UNBUNDLED"]:
+                if key in env_for_make:
+                    print_env_parts.append(
+                        f"{key}={shlex.quote(env_for_make[key])}"
+                    )
+
+            print_cmd_str = " ".join(print_env_parts)
+            if print_cmd_str:
+                 print_cmd_str += " "
+            print_cmd_str += ' '.join(shlex.quote(arg) for arg in make_command)
+
+            print(f"Running make command: {print_cmd_str}")
+
+            run_subprocess(make_command, env=env_for_make)
+        else:
+            print("No make targets specified or collected.")
+
+        # dict.fromkeys preserves the order, ensuring actions are executed
+        # in dependency order.
+        unique_actions = list(dict.fromkeys(all_actions))
+
+        if unique_actions:
+            print("Executing post-build actions...")
+            for target_obj in unique_actions:
+                target_obj.execute_post_build_action(self.build_vars)
+        else:
+            print("No post-build actions to execute.")
+
+
+def _setup_env_and_get_primary_build_vars(
+    args: argparse.Namespace,
+) -> BuildVarsDict:
+    """
+    Sets up the script's execution environment (CWD, ANDROID_BUILD_TOP env var)
+    and retrieves primary build variables from dumpvars.
+    """
+    actual_android_root = os.path.abspath(args.android_root)
+    print(f"Info: Effective Android build root: {actual_android_root}")
+    os.environ["ANDROID_BUILD_TOP"] = actual_android_root
+    try:
+        os.chdir(actual_android_root)
+        print(f"Info: Changed CWD to: {actual_android_root}")
+    except Exception as e:
+        print(f"Error: Could not change CWD to {actual_android_root}: {e}")
+        sys.exit(1)
+
+    required_env_vars = ["TARGET_PRODUCT", "TARGET_RELEASE",
+                         "TARGET_BUILD_VARIANT"]
+    missing_vars = [v for v in required_env_vars if not os.environ.get(v)]
+    if missing_vars:
+        error_msg1 = (
+            "Error: The following essential environment variables must be set:"
+        )
+        error_msg_part2 = f"        {', '.join(missing_vars)}."
+        print(error_msg1)
+        print(error_msg_part2)
+        sys.exit(1)
+
+    vars_to_get_via_dumpvars = [
+        "OUT_DIR", "HOST_OUT", "TARGET_OUT", "TARGET_ARCH",
+        "HOST_OUT_JAVA_LIBRARIES"
+    ]
+
+    build_vars_from_dumpvars: BuildVarsDict = get_android_build_vars(
+        vars_to_get=vars_to_get_via_dumpvars,
+    )
+    return build_vars_from_dumpvars
+
+
+def parse_command_line_arguments(builder: Builder) -> argparse.ArgumentParser:
+    """Parses args, populates builder lists, and returns the parser.
+
+    Args:
+        builder: The Builder instance to populate with parsed targets.
+
+    Returns:
+        The configured ArgumentParser object.
+    """
+    parser = argparse.ArgumentParser(
+        description="Builds ART targets with Soong, handling APEX extractions."
+    )
+    # Arguments for enabling internal targets
+    parser.add_argument(
+        "--build-art-host",
+        action="store_true",
+        help="Build build-art-host components (activates internal target)."
+    )
+    parser.add_argument(
+        "--build-art-host-gtests",
+        action="store_true",
+        help="Build build-art-host-gtests components (internal target)."
+    )
+    parser.add_argument(
+        "--build-art-target",
+        action="store_true",
+        help="Build build-art-target components (activates internal target)."
+    )
+    parser.add_argument(
+        "--build-art",
+        action="store_true",
+        help="Build build-art components (activates internal target)."
+    )
+    parser.add_argument(
+        "--android-root",
+        default=os.environ.get("ANDROID_BUILD_TOP", "."),
+        help="Path to the Android root directory. Overrides "
+             "ANDROID_BUILD_TOP environment variable if set. "
+             "Defaults to $ANDROID_BUILD_TOP or '.' if not set."
+    )
+    # Argument for positional real build targets
+    parser.add_argument(
+        "positional_targets",
+        metavar="MAKE_TARGET",
+        nargs="*",
+        help="Additional Soong/Make build targets to build."
+    )
+
+    args = parser.parse_args()
+
+    if args.build_art_host:
+        builder.enabled_internal_targets.append("build-art-host")
+    if args.build_art_host_gtests:
+        builder.enabled_internal_targets.append("build-art-host-gtests")
+    if args.build_art_target:
+        builder.enabled_internal_targets.append("build-art-target")
+    if args.build_art:
+        builder.enabled_internal_targets.append("build-art")
+
+    builder.positional_make_targets.extend(args.positional_targets)
+
+    return parser
+
+
+def main():
+    """Main execution function."""
+    builder = Builder()
+    parser = parse_command_line_arguments(builder)
+    args = parser.parse_args()
+
+    build_vars = _setup_env_and_get_primary_build_vars(args)
+    builder.setup_default_targets(build_vars)
+
+    if builder.enabled_internal_targets or builder.positional_make_targets:
+        builder.build()
+    else:
+        print("No build targets specified. Printing help:")
+        parser.print_help()
+        sys.exit(1)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/tools/boot-image-profile-aaos-sample-generate.py b/tools/boot-image-profile-aaos-sample-generate.py
new file mode 100644
index 0000000000..23e7ddfc64
--- /dev/null
+++ b/tools/boot-image-profile-aaos-sample-generate.py
@@ -0,0 +1,615 @@
+#!/usr/bin/env python3
+#
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Automates boot image profile generation for Android Automotive.
+
+This script orchestrates the process of generating a boot image profile
+by performing the following main stages:
+
+1.  Configuring a device using art/tools/boot-image-profile-configure-device.sh
+    to generate a `boot.zip` file.
+2.  Executing a simple set of activities (Maps, Play Store) on the Android
+    device via ADB shell commands to generate usage profiles.
+    Note: This "Starting Maps and Play activities" is a simple CUJ
+    implementation as a starting point. It may need to be expanded or customized
+    for better profiling by reflecting real usage.
+3.  Generating boot image profiles based on the generated `boot.zip` file
+    and the collected device profiles, using
+    art/tools/boot-image-profile-generate.sh.
+
+The script requires the ANDROID_BUILD_TOP environment variable to be set.
+
+Example Usage:
+    $ source build/envsetup.sh # Or equivalent setup
+    $ python3 art/tools/boot-image-profile-aaos-sample-generate.py
+
+Example Output Trace:
+    --- Stage 1: Configuring Device ---
+    --- Stage 2: Starting ADB/CUJ Sequence ---
+    --- Stage 3: Running Generation Script ---
+    --- All stages completed successfully! ---
+"""
+
+import os
+import shlex
+import subprocess
+import sys
+import time
+from typing import List, Optional
+
+
+# --- Helper Functions ---
+def print_message(message: str) -> None:
+  """Prints a message to stderr."""
+  print(f"{message}", file=sys.stderr)
+
+
+def print_separator() -> None:
+  """Prints a separator line."""
+  print_message("---------------------------------------\n")
+
+
+# --- End Helper Functions ---
+
+# --- Configuration ---
+# Pattern to look for in logcat to indicate the device is ready for profiling.
+# Set to None to skip the logcat check and only wait for ADB device.
+# LOGCAT_READY_PATTERN: Optional[str] = (
+#     "Displayed com.android.car.carlauncher/.CarLauncher"
+# )
+LOGCAT_READY_PATTERN = None  # Example: Disable logcat check
+
+# Logcat filters to apply when monitoring for the ready pattern.
+LOGCAT_FILTER_SPECS: List[str] = [
+    "ActivityTaskManager:I",
+    "*:S",
+]
+
+# Ensure ANDROID_BUILD_TOP is set and resolve its absolute path.
+ANDROID_BUILD_TOP: str = os.getenv("ANDROID_BUILD_TOP", "")
+if not ANDROID_BUILD_TOP:
+  print_message(
+      "Error: ANDROID_BUILD_TOP environment variable is not set. "
+      "Please run `source build/envsetup.sh` and select a target."
+  )
+  sys.exit(1)
+ANDROID_BUILD_TOP = os.path.abspath(ANDROID_BUILD_TOP)
+print(f"--- Using ANDROID_BUILD_TOP: {ANDROID_BUILD_TOP} ---")
+
+# Define paths to required scripts and generated files.
+_SCRIPT_DIR = "art/tools"  # Relative path to scripts within build top
+_FRAMEWORKS_CONFIG_DIR = "frameworks/base/config"  # Relative path to denylist
+
+SCRIPT1_PATH: str = os.path.join(
+    ANDROID_BUILD_TOP, _SCRIPT_DIR, "boot-image-profile-configure-device.sh"
+)
+SCRIPT1_ARG: str = "boot.zip"
+
+SCRIPT2_PATH: str = os.path.join(
+    ANDROID_BUILD_TOP, _SCRIPT_DIR, "boot-image-profile-generate.sh"
+)
+BOOT_ZIP_ARG: str = os.path.join(ANDROID_BUILD_TOP, "boot.zip")
+DENYLIST_ARG: str = os.path.join(
+    ANDROID_BUILD_TOP, _FRAMEWORKS_CONFIG_DIR, "preloaded-classes-denylist"
+)
+MAPS_PROF_ARG: str = os.path.join(ANDROID_BUILD_TOP, "maps.prof")
+VENDING_PROF_ARG: str = os.path.join(ANDROID_BUILD_TOP, "vending.prof")
+
+# Arguments for the generation script (SCRIPT2).
+SCRIPT2_ARGS_LIST: List[str] = [
+    ANDROID_BUILD_TOP,
+    BOOT_ZIP_ARG,
+    DENYLIST_ARG,
+    MAPS_PROF_ARG,
+    VENDING_PROF_ARG,
+    "--profman-arg",
+    "--upgrade-startup-to-hot=false",
+]
+
+# Package names and activities for the CUJ steps.
+MAPS_PACKAGE: str = (
+    "com.google.android.apps.maps/com.google.android.maps.MapsActivity"
+)
+VENDING_PACKAGE: str = (
+    "com.android.vending/com.google.android.finsky.activities.MainActivity"
+)
+
+# Duration to sleep after a CUJ activity is launched, to allow it to run
+# before taking a snapshot of the profile.
+CUJ_ACTIVITY_SLEEP_SECONDS: int = 15
+
+# Duration to sleep after starting logcat, to allow it to initialize.
+LOGCAT_START_SLEEP_SECONDS: float = 0.5
+
+# --- End Configuration ---
+
+
+def run_generic_command(
+    command_list: List[str],
+    check: bool = True,
+    capture: bool = True,
+    cmd_timeout: Optional[int | float] = None,
+    **kwargs,
+) -> Optional[subprocess.CompletedProcess]:
+  """Runs a command, handling errors and timeouts.
+
+  Args:
+      command_list: A list of strings representing the command and its
+        arguments.
+      check: If True, raise CalledProcessError on non-zero exit code. If False,
+        return the CompletedProcess object regardless of exit code.
+      capture: If True, capture stdout/stderr. If False, let them pass through.
+      cmd_timeout: Timeout in seconds for the command.
+      **kwargs: Additional arguments to pass to subprocess.run.
+
+  Returns:
+      A CompletedProcess object or None on error.
+
+  Raises:
+      subprocess.CalledProcessError: If check is True and the command fails.
+      subprocess.TimeoutExpired: If the command times out and check is True.
+  """
+  command_str = " ".join(shlex.quote(part) for part in command_list)
+  print_message(f"--- Running command: {command_str} ---")
+  try:
+    result = subprocess.run(
+        command_list,
+        check=check,
+        stdout=subprocess.PIPE if capture else None,
+        stderr=subprocess.PIPE if capture else None,
+        text=True,  # Use text mode for string I/O
+        encoding="utf-8",  # Explicitly set encoding
+        errors="replace",  # Replace uninterpretable characters
+        timeout=cmd_timeout,
+        **kwargs,
+    )
+    # If check=True, subprocess.run already raises on error.
+    # If check=False, we just return the result.
+    return result
+  except FileNotFoundError:
+    print_message(
+        f"Error: Command not found: '{command_list[0]}'. Is it in your PATH?"
+    )
+  except subprocess.TimeoutExpired:
+    print_message(
+        f"Error: Command '{command_str}' timed out after {cmd_timeout} seconds."
+    )
+    if check:
+      raise
+  except subprocess.CalledProcessError as e:
+    print_message(
+        f"Error: Command '{command_str}' failed with exit code {e.returncode}."
+    )
+    if e.stdout:
+      print_message(f"--- stdout ---\n{e.stdout.strip()}")
+    if e.stderr:
+      print_message(f"--- stderr ---\n{e.stderr.strip()}")
+    if check:
+      raise
+  except Exception as e:
+    print_message(f"Unexpected Python error running {command_str}: {e}")
+  return None  # Return None on any caught exception
+
+
+def run_shell_script(
+    script_path: str, *script_args: str, interpreter: str = "bash"
+) -> bool:
+  """Runs a shell script using the specified interpreter.
+
+  Args:
+      script_path: The path to the shell script.
+      *script_args: Positional arguments to pass to the script.
+      interpreter: The shell interpreter to use (e.g., "bash", "sh").
+
+  Returns:
+      True if the script exited with code 0, False otherwise or if an
+      error occurred running the command.
+  """
+  command = [interpreter, script_path] + list(script_args)
+  print_message(f"--- Running script: {script_path} ---")
+  # Run with check=False so we can inspect the return code manually
+  result = run_generic_command(command, check=False, capture=True)
+
+  if result is None:
+    # An exception occurred in run_generic_command
+    print_message(f"--- Script {script_path} command execution failed ---")
+    print_separator()
+    return False
+
+  # Output captured stdout/stderr for user information
+  if result.stdout:
+    print_message(f"--- stdout ---\n{result.stdout.strip()}")
+  if result.stderr:
+    print_message(f"--- stderr ---\n{result.stderr.strip()}")
+
+  if result.returncode == 0:
+    print_message(f"--- Script {script_path} SUCCESS ---")
+    print_separator()
+    return True
+  print_message(
+      f"--- Script {script_path} FAILED with exit code {result.returncode} ---"
+  )
+  print_separator()
+  return False
+
+
+def wait_for_device_ready(
+    timeout_seconds: int = 360,
+    check_interval: int = 1,
+    logcat_pattern: Optional[str] = LOGCAT_READY_PATTERN,
+    logcat_filters: List[str] = LOGCAT_FILTER_SPECS,
+) -> bool:
+  """Waits for ADB connection and optionally a specific logcat pattern.
+
+  Args:
+      timeout_seconds: Maximum time in seconds to wait.
+      check_interval: Time in seconds to wait between logcat reads.
+      logcat_pattern: The string or regex pattern to look for in logcat. If
+        None, only waits for ADB device connection.
+      logcat_filters: List of logcat filter specifications (e.g., "TAG:LEVEL").
+
+  Returns:
+      True if the device becomes ready within the timeout, False otherwise.
+  """
+  print_message(f"--- Waiting for ADB device (max {timeout_seconds}s) ---")
+  start_time = time.time()
+
+  print_message("[Wait Step 1/2] Waiting for ADB connection...")
+  adb_wait_cmd = ["adb", "wait-for-device"]
+  try:
+    # Using run here is simpler as wait-for-device holds until ready or timeout
+    subprocess.run(
+        adb_wait_cmd,
+        timeout=timeout_seconds,
+        capture_output=True,
+        text=True,
+        check=True,  # check=True is appropriate here
+        encoding="utf-8",
+        errors="replace",
+    )
+    elapsed_time = time.time() - start_time
+    print_message(
+        "[Wait Step 1/2] Complete: Device connected. (Elapsed:"
+        f" {elapsed_time:.1f}s)"
+    )
+  except (
+      subprocess.TimeoutExpired,
+      subprocess.CalledProcessError,
+      FileNotFoundError,
+  ) as e:
+    print_message(f"ERROR: Initial ADB connection failed: {e}")
+    print_message("Is device connected? Is adb in PATH?")
+    print_separator()
+    return False
+  except Exception as e:
+    print_message(f"ERROR during 'adb wait-for-device': {e}")
+    print_separator()
+    return False
+
+  if not logcat_pattern:
+    print_message(
+        "[Wait Step 2/2] Skipping logcat pattern check"
+        " (LOGCAT_READY_PATTERN is None)."
+    )
+    print_message("--- Device ready based on ADB connection ---")
+    print_separator()
+    return True
+
+  logcat_proc: Optional[subprocess.Popen] = None
+  logcat_found: bool = False
+  logcat_failed: bool = False
+
+  print_message(
+      "\n[Wait Step 2/2] Starting logcat check for pattern"
+      f" '{logcat_pattern}'..."
+  )
+
+  try:
+    print_message("  Clearing logcat buffer...")
+    run_generic_command(["adb", "logcat", "-c"], check=False)
+
+    logcat_cmd = ["adb", "logcat", "-v", "brief"] + logcat_filters
+    print_message(
+        f"  Starting logcat: {' '.join(shlex.quote(p) for p in logcat_cmd)}"
+    )
+    try:
+      # Use Popen to read line by line
+      logcat_proc = subprocess.Popen(
+          logcat_cmd,
+          stdout=subprocess.PIPE,
+          stderr=subprocess.PIPE,
+          text=True,
+          encoding="utf-8",
+          errors="replace",
+      )
+      time.sleep(LOGCAT_START_SLEEP_SECONDS)
+    except (FileNotFoundError, Exception) as proc_err:
+      print_message(f"ERROR: Failed to start logcat process: {proc_err}")
+      logcat_failed = True
+
+    if not logcat_failed:
+      while time.time() - start_time < timeout_seconds:
+        # Check if logcat process died unexpectedly
+        if logcat_proc and logcat_proc.poll() is not None:
+          print_message(
+              "  Error: logcat process terminated unexpectedly (Code:"
+              f" {logcat_proc.returncode})."
+              f" Stderr:\n{logcat_proc.stderr.read()}"
+          )
+          logcat_failed = True
+          break
+
+        # Attempt to read line. readline() can block, but the outer
+        # timeout loop will eventually catch it.
+        try:
+          # Check if stdout is not None before reading
+          if logcat_proc and logcat_proc.stdout:
+            line = logcat_proc.stdout.readline()
+            if not line:
+              # If readline returns empty string, process might be exiting
+              # or stream is just slow. Check poll() again or wait.
+              if logcat_proc.poll() is not None:
+                print_message("  logcat stream ended.")
+                logcat_failed = True
+                break
+              # Wait a bit if no line was read but process is still running
+              time.sleep(check_interval)
+              continue
+
+            line = line.strip()
+            if not line:
+              continue  # Skip empty lines
+
+            if logcat_pattern in line:
+              print_message(f'  Found target logcat line: "{line}"')
+              print_message(
+                  "[Wait Step 2/2] SUCCESS: Found logcat pattern. (Elapsed:"
+                  f" {time.time() - start_time:.1f}s)"
+              )
+              logcat_found = True
+              break
+
+        except Exception as read_err:
+          print_message(f"  Warning: Error reading logcat stream: {read_err}.")
+          # Continue the loop after a short pause
+          time.sleep(check_interval)
+
+  finally:
+    # Ensure the logcat subprocess is terminated
+    if logcat_proc and logcat_proc.poll() is None:
+      print_message("  Terminating logcat process...")
+      try:
+        # Give it a moment to terminate gracefully
+        logcat_proc.terminate()
+        logcat_proc.wait(timeout=2)
+      except subprocess.TimeoutExpired:
+        # If it doesn't terminate, kill it
+        print_message("  Logcat process did not terminate, killing...")
+        logcat_proc.kill()
+      except Exception as term_err:
+        # Catch any other errors during termination
+        print_message(f"  Error during logcat termination: {term_err}")
+
+  if logcat_found:
+    print_message("--- Device ready based on logcat pattern ---")
+    print_separator()
+    return True
+  else:
+    print_message(
+        f"--- Logcat pattern '{logcat_pattern}' not found within timeout or"
+        " logcat failed. ---"
+    )
+    print_separator()
+    return False
+
+
+def execute_adb_step(
+    cmd_list: List[str],
+    step_num: int,
+    description: str,
+    allow_fail: bool = False,
+) -> bool:
+  """Executes a single ADB shell command step.
+
+  Args:
+      cmd_list: The list representing the ADB command and its arguments.
+      step_num: The sequential number of this step.
+      description: A brief description of the step.
+      allow_fail: If True, the function returns True even if the command fails
+        (non-zero exit code). If False, it returns False on failure.
+
+  Returns:
+      True if the step completed successfully or allow_fail is True and
+      the command ran (even if it failed). False if the command execution
+      itself failed (e.g., command not found, timeout) or if allow_fail
+      is False and the command returned a non-zero exit code.
+  """
+  print_message(f"\n--- ADB Step {step_num}: {description} ---")
+  result = run_generic_command(cmd_list, check=False, capture=True)
+
+  if result is None:
+    print_message(
+        f"ERROR: Step {step_num} ({description}) command execution failed."
+    )
+    print_separator()
+    return False
+
+  # Decode stdout and stderr if they exist
+  cmd_stdout = result.stdout.strip() if result.stdout else ""
+  cmd_stderr = result.stderr.strip() if result.stderr else ""
+
+  if result.returncode != 0:
+    error_message_base = (
+        f"Step {step_num} ({description}) failed with exit code"
+        f" {result.returncode}."
+    )
+    if cmd_stdout:
+      print_message(f"Stdout from failed command:\n{cmd_stdout}")
+    if cmd_stderr:
+      print_message(f"Stderr from failed command:\n{cmd_stderr}")
+    if allow_fail:
+      print_message(
+          f"Warning: {error_message_base} Proceeding as allow_fail is True.",
+      )
+      print_separator()
+      return True  # Command ran, allowed to fail
+    else:
+      print_message(f"ERROR: {error_message_base} Stopping ADB sequence.")
+      print_separator()
+      return False  # Command ran, not allowed to fail
+
+  # Command succeeded (returncode == 0)
+  print_separator()
+  return True
+
+
+def execute_adb_step_or_exit(
+    cmd_list: List[str],
+    step_num: int,
+    description: str,
+    allow_fail: bool = False,
+) -> None:
+  """Executes an ADB step and exits if it fails (unless allow_fail is True).
+
+  Args:
+      cmd_list: The list representing the ADB command and its arguments.
+      step_num: The sequential number of this step.
+      description: A brief description of the step.
+      allow_fail: If True, the function does not exit on failure, just prints a
+        warning.
+  """
+  if not execute_adb_step(cmd_list, step_num, description, allow_fail):
+    print_message(
+        f"ERROR: ADB Step {step_num} ({description}) failed. Aborting."
+    )
+    sys.exit(1)
+
+
+def main() -> None:
+  """Main function to orchestrate the profile generation process."""
+  print_message("-" * 20)  # Separator before start
+  print_message(f"Starting script from directory: {os.getcwd()}")
+  print_message(f"Script 1: {SCRIPT1_PATH}")
+  print_message(f"Script 2: {SCRIPT2_PATH}")
+  print_message(
+      f"Wait method: Logcat pattern ('{LOGCAT_READY_PATTERN}')"
+      if LOGCAT_READY_PATTERN
+      else "Wait method: Only ADB connection (Logcat pattern disabled)"
+  )
+  print_message("-" * 20)  # Separator after info
+
+  print_message("\n--- Stage 1: Configuring Device ---")
+  print_message(f"\n--- Running Configuration Script ---")
+  script1_abs_path = os.path.join(ANDROID_BUILD_TOP, SCRIPT1_PATH)
+  if not os.path.exists(script1_abs_path):
+    print_message(
+        f"Error: Configuration script not found at {script1_abs_path}"
+    )
+    sys.exit(1)
+  if not run_shell_script(script1_abs_path, SCRIPT1_ARG):
+    print_message("Configuration Script failed. Aborting.")
+    sys.exit(1)
+  print_message("Configuration Script successful.")
+  print_message("\n--- Waiting for Device Readiness ---")
+  if not wait_for_device_ready():
+    print_message("Device not ready within timeout. Aborting.")
+    sys.exit(1)
+  print_message("Device reported ready.")
+
+  print_message("\n--- Stage 2: Starting ADB/CUJ Sequence ---")
+  execute_adb_step_or_exit(
+      [
+          "adb",
+          "shell",
+          "find /data/misc/profiles -name '*.prof' -exec truncate -s 0 {} \\;",
+      ],
+      1,
+      "Clear existing profiles",
+      allow_fail=True,  # Allowed to fail if dir doesn't exist etc.
+  )
+  execute_adb_step_or_exit(
+      ["adb", "shell", "am", "get-current-user"], 2, "Get current user"
+  )
+  execute_adb_step_or_exit(
+      [
+          "adb",
+          "shell",
+          "am",
+          "start",
+          "-S",
+          "-W",
+          "-a",
+          "android.intent.action.MAIN",
+          "-n",
+          MAPS_PACKAGE,
+      ],
+      3,
+      f"Start Maps activity ({MAPS_PACKAGE})",
+  )
+  print_message(
+      f"Pausing {CUJ_ACTIVITY_SLEEP_SECONDS} seconds after Maps activity"
+      " launch..."
+  )
+  time.sleep(CUJ_ACTIVITY_SLEEP_SECONDS)
+  execute_adb_step_or_exit(
+      ["adb", "shell", "cmd", "package", "snapshot-profile", "android"],
+      4,
+      "Snapshot profile after Maps",
+  )
+  execute_adb_step_or_exit(
+      ["adb", "pull", "/data/misc/profman/android.prof", MAPS_PROF_ARG],
+      5,
+      f"Pull profile to {MAPS_PROF_ARG}",
+  )
+  execute_adb_step_or_exit(
+      ["adb", "shell", "am", "start", "-S", "-W", "-n", VENDING_PACKAGE],
+      6,
+      f"Start Vending activity ({VENDING_PACKAGE})",
+  )
+  print_message(
+      f"Pausing {CUJ_ACTIVITY_SLEEP_SECONDS} seconds after Vending activity"
+      " launch..."
+  )
+  time.sleep(CUJ_ACTIVITY_SLEEP_SECONDS)
+  execute_adb_step_or_exit(
+      ["adb", "shell", "cmd", "package", "snapshot-profile", "android"],
+      7,
+      "Snapshot profile after Vending",
+  )
+  execute_adb_step_or_exit(
+      ["adb", "pull", "/data/misc/profman/android.prof", VENDING_PROF_ARG],
+      8,
+      f"Pull profile to {VENDING_PROF_ARG}",
+  )
+
+  print_message("\n--- Stage 3: Running Generation Script ---")
+  print_message(
+      "ADB/CUJ sequence finished (check logs for warnings). Proceeding."
+  )
+  script2_abs_path = os.path.join(ANDROID_BUILD_TOP, SCRIPT2_PATH)
+  if not os.path.exists(script2_abs_path):
+    print_message(f"Error: Generation script not found at {script2_abs_path}")
+    sys.exit(1)
+  if run_shell_script(script2_abs_path, *SCRIPT2_ARGS_LIST):
+    print_message("\n--- All stages completed successfully! ---")
+    sys.exit(0)
+  else:
+    print_message(f"\n--- Generation Script ({SCRIPT2_PATH}) failed. ---")
+    sys.exit(1)
+
+
+if __name__ == "__main__":
+  main()
diff --git a/tools/boot-image-profile-configure-device.sh b/tools/boot-image-profile-configure-device.sh
index 081f442457..abbfbd27f7 100755
--- a/tools/boot-image-profile-configure-device.sh
+++ b/tools/boot-image-profile-configure-device.sh
@@ -35,11 +35,12 @@ echo "Changing dirs to the build top"
 cd "$ANDROID_BUILD_TOP"
 
 # Make dist in order to easily get the boot and system server dex files
-# This will be stored in $ANDROID_PRODUCT_OUT/boot.zip
+# This will be stored in $DIST_DIR/boot.zip (usually out/dist/boot.zip)
 echo "Make dist"
 m dist
 echo "Copy boot.zip to $OUT_BOOT_ZIP"
-cp "$ANDROID_PRODUCT_OUT"/boot.zip $OUT_BOOT_ZIP
+DIST_DIR=${DIST_DIR:-out/dist}
+cp "$DIST_DIR"/boot.zip $OUT_BOOT_ZIP
 
 echo "Setting properties and clearing existing profiles"
 # If the device needs to be rebooted, it is better to set the properties
diff --git a/tools/build-linux-x86-host-tools.sh b/tools/build-linux-x86-host-tools.sh
index c0483fa97c..71f0262ebd 100755
--- a/tools/build-linux-x86-host-tools.sh
+++ b/tools/build-linux-x86-host-tools.sh
@@ -27,13 +27,25 @@ vars="$(build/soong/soong_ui.bash --dumpvars-mode --vars="OUT_DIR DIST_DIR")"
 eval $vars
 
 HOST_BINARIES=(
-  ${OUT_DIR}/host/linux-x86/bin/dex2oat64
-  ${OUT_DIR}/host/linux-x86/bin/dex2oatd64
-  ${OUT_DIR}/host/linux-x86/bin/dex2oat
-  ${OUT_DIR}/host/linux-x86/bin/dex2oatd
+  ${OUT_DIR}/host/linux-x86/bin/aapt2
+  ${OUT_DIR}/host/linux-x86/bin/apksigner
+  ${OUT_DIR}/host/linux-x86/bin/aprotoc
   ${OUT_DIR}/host/linux-x86/bin/deapexer
   ${OUT_DIR}/host/linux-x86/bin/debugfs_static
+  ${OUT_DIR}/host/linux-x86/bin/derive_classpath
+  ${OUT_DIR}/host/linux-x86/bin/dex2oat
+  ${OUT_DIR}/host/linux-x86/bin/dex2oat64
+  ${OUT_DIR}/host/linux-x86/bin/dex2oatd
+  ${OUT_DIR}/host/linux-x86/bin/dex2oatd64
   ${OUT_DIR}/host/linux-x86/bin/oatdump
+  ${OUT_DIR}/host/linux-x86/bin/profman
+  ${OUT_DIR}/host/linux-x86/bin/soong_zip
+  ${OUT_DIR}/host/linux-x86/bin/zipalign
+  ${OUT_DIR}/host/linux-x86/framework/apksigner.jar
+)
+
+SOURCE_FILES=(
+  system/apex/proto/apex_manifest.proto
 )
 
 # Build statically linked musl binaries for linux-x86 hosts without the
@@ -41,12 +53,12 @@ HOST_BINARIES=(
 build/soong/soong_ui.bash --make-mode USE_HOST_MUSL=true BUILD_HOST_static=true ${HOST_BINARIES[*]}
 # Zip these binaries in a temporary file
 prebuilts/build-tools/linux-x86/bin/soong_zip -o "${DIST_DIR}/temp-host-tools.zip" \
-  -j ${HOST_BINARIES[*]/#/-f }
+  -j ${HOST_BINARIES[*]/#/-f } ${SOURCE_FILES[*]/#/-f }
 
 # Build art_release.zip and copy only art jars in a temporary zip
 build/soong/soong_ui.bash --make-mode dist "${DIST_DIR}/art_release.zip"
 prebuilts/build-tools/linux-x86/bin/zip2zip -i "${DIST_DIR}/art_release.zip" \
-  -o "${DIST_DIR}/temp-art-jars.zip" "bootjars/*"
+  -o "${DIST_DIR}/temp-art-jars.zip" "bootjars/*" "licenses/*/*"
 
 # Merge both temporary zips into output zip
 prebuilts/build-tools/linux-x86/bin/merge_zips "${DIST_DIR}/art-host-tools-linux-x86.zip" \
diff --git a/tools/buildbot-build.sh b/tools/buildbot-build.sh
index 214780b741..f8b4143d1e 100755
--- a/tools/buildbot-build.sh
+++ b/tools/buildbot-build.sh
@@ -63,7 +63,17 @@ if [[ $TARGET_ARCH = "riscv64" && ! ( -d frameworks/base ) ]]; then
 fi
 
 java_libraries_dir=${out_dir}/target/common/obj/JAVA_LIBRARIES
-common_targets="vogar core-tests core-ojtests apache-harmony-jdwp-tests-hostdex jsr166-tests mockito-target"
+libcore_tests_classpath="core-tests core-ojtests jsr166-tests mockito-target"
+libjdwp_tests_classpath="apache-harmony-jdwp-tests-hostdex"
+common_targets="vogar ${libjdwp_tests_classpath} ${libcore_tests_classpath}"
+# Add classpath for libjdwp tests.
+for jar in ${libjdwp_tests_classpath} ; do
+  common_targets="$common_targets out/host/common/obj/JAVA_LIBRARIES/${jar}_intermediates/classes.jar"
+done
+# Add classpath for libcore tests.
+for jar in ${libcore_tests_classpath} ; do
+  common_targets="$common_targets out/target/common/obj/JAVA_LIBRARIES/${jar}_intermediates/classes.jar"
+done
 # These build targets have different names on device and host.
 specific_targets="libjavacoretests libwrapagentproperties libwrapagentpropertiesd"
 build_host="no"
@@ -360,7 +370,7 @@ EOF
   for apex in ${apexes[@]}; do
     apex=$(override_apex_name $apex)
     cat <<EOF >> $apex_xml_file
-    <apex-info moduleName="${apex}" modulePath="/system/apex/${apex}.apex" preinstalledModulePath="/system/apex/${apex}.apex" versionCode="1" versionName="" isFactory="true" isActive="true">
+    <apex-info moduleName="${apex}" modulePath="/system/apex/${apex}.apex" preinstalledModulePath="/system/apex/${apex}.apex" versionCode="1" versionName="" isFactory="true" isActive="true" partition="SYSTEM">
     </apex-info>
 EOF
   done
diff --git a/tools/cpp-define-generator/Android.bp b/tools/cpp-define-generator/Android.bp
index 44097bcdd0..cbbf30c80e 100644
--- a/tools/cpp-define-generator/Android.bp
+++ b/tools/cpp-define-generator/Android.bp
@@ -35,6 +35,9 @@ cc_object {
         "libart_headers",
         "libdexfile_all_headers", // For dex/modifiers.h
     ],
+    shared_libs: [
+        "art-aconfig-flags-lib", // For thread.h. Only the headers are needed.
+    ],
     cflags: [
         // Produce text file rather than binary.
         "-S",
diff --git a/tools/cpp-define-generator/art_method.def b/tools/cpp-define-generator/art_method.def
index 3c34247ec2..6ca60b1c9f 100644
--- a/tools/cpp-define-generator/art_method.def
+++ b/tools/cpp-define-generator/art_method.def
@@ -31,8 +31,12 @@ ASM_DEFINE(ART_METHOD_IS_MEMORY_SHARED_FLAG_BIT,
            art::MostSignificantBit(art::kAccMemorySharedMethod))
 ASM_DEFINE(ART_METHOD_IS_STATIC_FLAG,
            art::kAccStatic)
+ASM_DEFINE(ART_METHOD_IS_OBSOLETE_FLAG,
+           art::kAccObsoleteMethod)
 ASM_DEFINE(ART_METHOD_IS_STATIC_FLAG_BIT,
            art::MostSignificantBit(art::kAccStatic))
+ASM_DEFINE(ART_METHOD_IS_OBSOLETE_FLAG_BIT,
+           art::WhichPowerOf2(art::kAccObsoleteMethod))
 ASM_DEFINE(ART_METHOD_NTERP_INVOKE_FAST_PATH_FLAG,
            art::kAccNterpInvokeFastPathFlag)
 ASM_DEFINE(ART_METHOD_NTERP_INVOKE_FAST_PATH_FLAG_BIT,
diff --git a/tools/fuzzer/dex-verifier-corpus/b405851405.dex b/tools/fuzzer/dex-verifier-corpus/b405851405.dex
new file mode 100644
index 0000000000..711671e217
Binary files /dev/null and b/tools/fuzzer/dex-verifier-corpus/b405851405.dex differ
diff --git a/tools/fuzzer/dex-verifier-corpus/b405879857.dex b/tools/fuzzer/dex-verifier-corpus/b405879857.dex
new file mode 100644
index 0000000000..3f015303fa
Binary files /dev/null and b/tools/fuzzer/dex-verifier-corpus/b405879857.dex differ
diff --git a/tools/fuzzer/libart_verify_classes_fuzzer.cc b/tools/fuzzer/libart_verify_classes_fuzzer.cc
index de99b654cb..de501c58c3 100644
--- a/tools/fuzzer/libart_verify_classes_fuzzer.cc
+++ b/tools/fuzzer/libart_verify_classes_fuzzer.cc
@@ -41,6 +41,8 @@ int skipped_gc_iterations = 0;
 // TODO: These values were obtained from local experimenting. They can be changed after
 // further investigation.
 static constexpr int kMaxSkipGCIterations = 100;
+// Global variable to signal LSAN that we are not leaking memory.
+uint8_t* allocated_signal_stack = nullptr;
 
 namespace art {
 // A class to be friends with ClassLinker and access the internal FindDexCacheDataLocked method.
@@ -138,6 +140,15 @@ extern "C" int LLVMFuzzerInitialize([[maybe_unused]] int* argc, [[maybe_unused]]
   // how to start a Runtime.
   art::Thread::Current()->TransitionFromRunnableToSuspended(art::ThreadState::kNative);
 
+  // Query the current stack and add it to the global variable. Otherwise LSAN complains about a
+  // non-existing leak.
+  stack_t ss;
+  if (sigaltstack(nullptr, &ss) == -1) {
+    PLOG(FATAL) << "sigaltstack failed";
+  }
+  allocated_signal_stack = reinterpret_cast<uint8_t*>(ss.ss_sp);
+
+
   return 0;
 }
 
diff --git a/tools/hiddenapi/hiddenapi.cc b/tools/hiddenapi/hiddenapi.cc
index f5d59b9f39..478c90e980 100644
--- a/tools/hiddenapi/hiddenapi.cc
+++ b/tools/hiddenapi/hiddenapi.cc
@@ -652,8 +652,6 @@ class DexFileEditor final {
  public:
   // Add dex file to copy to output (possibly several files for multi-dex).
   void Add(const DexFile* dex, const std::vector<uint8_t>&& hiddenapi_data) {
-    // We do not support non-standard dex encodings, e.g. compact dex.
-    CHECK(dex->IsStandardDexFile());
     inputs_.emplace_back(dex, std::move(hiddenapi_data));
   }
 
diff --git a/tools/hiddenapi/hiddenapi_test.cc b/tools/hiddenapi/hiddenapi_test.cc
index 940a2630d9..daafc4863d 100644
--- a/tools/hiddenapi/hiddenapi_test.cc
+++ b/tools/hiddenapi/hiddenapi_test.cc
@@ -123,9 +123,6 @@ class HiddenApiTest : public CommonRuntimeTest {
     if (dex_file.get() == nullptr) {
       LOG(FATAL) << "Open failed for '" << file.GetFilename() << "' " << error_msg;
       UNREACHABLE();
-    } else if (!dex_file->IsStandardDexFile()) {
-      LOG(FATAL) << "Expected a standard dex file '" << file.GetFilename() << "'";
-      UNREACHABLE();
     }
 
     return dex_file;
diff --git a/tools/jvmti-agents/field-null-percent/fieldnull.cc b/tools/jvmti-agents/field-null-percent/fieldnull.cc
index b2cd13b686..a97a168178 100644
--- a/tools/jvmti-agents/field-null-percent/fieldnull.cc
+++ b/tools/jvmti-agents/field-null-percent/fieldnull.cc
@@ -184,8 +184,9 @@ static jint AgentStart(JavaVM* vm, char* options, bool is_onload) {
   CHECK_JVMTI(jvmti->SetEventCallbacks(&cb, sizeof(cb)));
   if (is_onload) {
     unsigned char* ptr = nullptr;
-    CHECK_JVMTI(jvmti->Allocate(strlen(options) + 1, &ptr));
-    strcpy(reinterpret_cast<char*>(ptr), options);
+    size_t opt_len = strlen(options);
+    CHECK_JVMTI(jvmti->Allocate(opt_len + 1, &ptr));
+    memcpy(reinterpret_cast<char*>(ptr), options, opt_len + 1);
     CHECK_JVMTI(jvmti->SetEnvironmentLocalStorage(ptr));
     CHECK_JVMTI(jvmti->SetEventNotificationMode(JVMTI_ENABLE, JVMTI_EVENT_VM_INIT, nullptr));
   } else {
@@ -215,4 +216,3 @@ extern "C" JNIEXPORT jint JNICALL Agent_OnLoad(JavaVM* jvm,
 }
 
 }  // namespace fieldnull
-
diff --git a/tools/jvmti-agents/wrapagentproperties/wrapagentproperties.cc b/tools/jvmti-agents/wrapagentproperties/wrapagentproperties.cc
index b2dcaabcfe..88a313057e 100644
--- a/tools/jvmti-agents/wrapagentproperties/wrapagentproperties.cc
+++ b/tools/jvmti-agents/wrapagentproperties/wrapagentproperties.cc
@@ -148,7 +148,7 @@ struct ExtraJvmtiInterface : public jvmtiInterface_1_ {
       if (res != JVMTI_ERROR_NONE) {
         return res;
       }
-      strcpy(*out, val.c_str());
+      memcpy(*out, val.c_str(), val.size() + 1);
       return JVMTI_ERROR_NONE;
     } else {
       return funcs->original_interface->GetSystemProperty(env, prop, out);
@@ -344,4 +344,3 @@ extern "C" JNIEXPORT void JNICALL Agent_OnUnload(JavaVM* jvm) {
 }
 
 }  // namespace wrapagentproperties
-
diff --git a/tools/luci/config/generated/cr-buildbucket.cfg b/tools/luci/config/generated/cr-buildbucket.cfg
index cef584cf10..058074de04 100644
--- a/tools/luci/config/generated/cr-buildbucket.cfg
+++ b/tools/luci/config/generated/cr-buildbucket.cfg
@@ -29,6 +29,7 @@ buckets {
         properties_j: "concurrent_collector:true"
         properties_j: "debug:true"
         properties_j: "generational_cc:true"
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -39,6 +40,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -59,6 +64,7 @@ buckets {
         properties_j: "concurrent_collector:true"
         properties_j: "debug:true"
         properties_j: "generational_cc:true"
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -69,6 +75,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -88,6 +98,7 @@ buckets {
         properties_j: "builder_group:\"client.art\""
         properties_j: "debug:true"
         properties_j: "generational_cc:true"
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -98,6 +109,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -117,6 +132,7 @@ buckets {
         properties_j: "builder_group:\"client.art\""
         properties_j: "debug:true"
         properties_j: "generational_cc:true"
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -127,6 +143,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -148,6 +168,7 @@ buckets {
         properties_j: "debug:true"
         properties_j: "gcstress:true"
         properties_j: "generational_cc:true"
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--debug\",\"--gcstress\"]"
       }
       execution_timeout_secs: 108000
@@ -158,6 +179,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -179,6 +204,7 @@ buckets {
         properties_j: "debug:true"
         properties_j: "gcstress:true"
         properties_j: "generational_cc:true"
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--debug\",\"--gcstress\"]"
       }
       execution_timeout_secs: 108000
@@ -189,6 +215,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -209,6 +239,7 @@ buckets {
         properties_j: "debug:true"
         properties_j: "gcstress:true"
         properties_j: "generational_cc:true"
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--debug\",\"--gcstress\"]"
       }
       execution_timeout_secs: 108000
@@ -219,6 +250,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -239,6 +274,7 @@ buckets {
         properties_j: "debug:true"
         properties_j: "gcstress:true"
         properties_j: "generational_cc:true"
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--debug\",\"--gcstress\"]"
       }
       execution_timeout_secs: 108000
@@ -249,6 +285,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -268,6 +308,7 @@ buckets {
         properties_j: "builder_group:\"client.art\""
         properties_j: "concurrent_collector:true"
         properties_j: "generational_cc:true"
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--ndebug\"]"
       }
       execution_timeout_secs: 108000
@@ -278,6 +319,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -297,6 +342,7 @@ buckets {
         properties_j: "builder_group:\"client.art\""
         properties_j: "concurrent_collector:true"
         properties_j: "generational_cc:true"
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--ndebug\"]"
       }
       execution_timeout_secs: 108000
@@ -307,6 +353,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -325,6 +375,7 @@ buckets {
         properties_j: "bitness:32"
         properties_j: "builder_group:\"client.art\""
         properties_j: "debug:true"
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -335,6 +386,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -353,6 +408,7 @@ buckets {
         properties_j: "bitness:64"
         properties_j: "builder_group:\"client.art\""
         properties_j: "debug:true"
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -363,6 +419,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -384,6 +444,7 @@ buckets {
         properties_j: "debug:true"
         properties_j: "generational_cc:true"
         properties_j: "heap_poisoning:true"
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -394,6 +455,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -415,6 +480,7 @@ buckets {
         properties_j: "debug:true"
         properties_j: "generational_cc:true"
         properties_j: "heap_poisoning:true"
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -425,6 +491,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -448,6 +518,7 @@ buckets {
         properties_j: "generational_cc:true"
         properties_j: "on_virtual_machine:true"
         properties_j: "product:\"armv8\""
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -485,6 +556,7 @@ buckets {
         properties_j: "generational_cc:true"
         properties_j: "on_virtual_machine:true"
         properties_j: "product:\"riscv64\""
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -521,6 +593,7 @@ buckets {
         properties_j: "device:\"target.arm.32\""
         properties_j: "generational_cc:true"
         properties_j: "product:\"arm_krait\""
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -531,6 +604,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -553,6 +630,7 @@ buckets {
         properties_j: "device:\"target.arm.64\""
         properties_j: "generational_cc:true"
         properties_j: "product:\"armv8\""
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -563,6 +641,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -584,6 +666,7 @@ buckets {
         properties_j: "device:\"target.arm.cmc.32\""
         properties_j: "generational_cc:true"
         properties_j: "product:\"arm_krait\""
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -594,6 +677,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -615,6 +702,7 @@ buckets {
         properties_j: "device:\"target.arm.cmc.64\""
         properties_j: "generational_cc:true"
         properties_j: "product:\"armv8\""
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -625,6 +713,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -648,6 +740,7 @@ buckets {
         properties_j: "gcstress:true"
         properties_j: "generational_cc:true"
         properties_j: "product:\"arm_krait\""
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--debug\",\"--gcstress\"]"
       }
       execution_timeout_secs: 108000
@@ -658,6 +751,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -681,6 +778,7 @@ buckets {
         properties_j: "gcstress:true"
         properties_j: "generational_cc:true"
         properties_j: "product:\"armv8\""
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--debug\",\"--gcstress\"]"
       }
       execution_timeout_secs: 108000
@@ -691,6 +789,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -713,6 +815,7 @@ buckets {
         properties_j: "gcstress:true"
         properties_j: "generational_cc:true"
         properties_j: "product:\"arm_krait\""
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--debug\",\"--gcstress\"]"
       }
       execution_timeout_secs: 108000
@@ -723,6 +826,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -745,6 +852,7 @@ buckets {
         properties_j: "gcstress:true"
         properties_j: "generational_cc:true"
         properties_j: "product:\"armv8\""
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--debug\",\"--gcstress\"]"
       }
       execution_timeout_secs: 108000
@@ -755,6 +863,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -776,6 +888,7 @@ buckets {
         properties_j: "device:\"target.arm.ndebug.32\""
         properties_j: "generational_cc:true"
         properties_j: "product:\"arm_krait\""
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--ndebug\"]"
       }
       execution_timeout_secs: 108000
@@ -786,6 +899,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -807,6 +924,7 @@ buckets {
         properties_j: "device:\"target.arm.ndebug.64\""
         properties_j: "generational_cc:true"
         properties_j: "product:\"armv8\""
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--ndebug\"]"
       }
       execution_timeout_secs: 108000
@@ -817,6 +935,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -840,6 +962,7 @@ buckets {
         properties_j: "generational_cc:true"
         properties_j: "heap_poisoning:true"
         properties_j: "product:\"arm_krait\""
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -850,6 +973,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
@@ -873,6 +1000,7 @@ buckets {
         properties_j: "generational_cc:true"
         properties_j: "heap_poisoning:true"
         properties_j: "product:\"armv8\""
+        properties_j: "repo_root:\"https://googleplex-android.googlesource.com\""
         properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--debug\"]"
       }
       execution_timeout_secs: 108000
@@ -883,6 +1011,10 @@ buckets {
       }
       build_numbers: YES
       service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "art.superproject"
+        value: 100
+      }
       experiments {
         key: "luci.recipes.use_python3"
         value: 100
diff --git a/tools/luci/config/generated/luci-milo.cfg b/tools/luci/config/generated/luci-milo.cfg
index 92f3addf78..678d08feaf 100644
--- a/tools/luci/config/generated/luci-milo.cfg
+++ b/tools/luci/config/generated/luci-milo.cfg
@@ -7,8 +7,8 @@
 consoles {
   id: "luci"
   name: "ART LUCI Console"
-  repo_url: "https://android.googlesource.com/platform/art"
-  refs: "regexp:refs/heads/master"
+  repo_url: "https://googleplex-android.googlesource.com/platform/art"
+  refs: "regexp:refs/heads/main"
   manifest_name: "REVISION"
   builders {
     name: "buildbucket/luci.art.ci/target.arm.32"
diff --git a/tools/luci/config/generated/luci-scheduler.cfg b/tools/luci/config/generated/luci-scheduler.cfg
index a2b426ee19..abb704ff7a 100644
--- a/tools/luci/config/generated/luci-scheduler.cfg
+++ b/tools/luci/config/generated/luci-scheduler.cfg
@@ -317,8 +317,8 @@ trigger {
   triggers: "target.arm.poison.32"
   triggers: "target.arm.poison.64"
   gitiles {
-    repo: "https://android.googlesource.com/platform/art"
-    refs: "regexp:refs/heads/master"
+    repo: "https://googleplex-android.googlesource.com/platform/art"
+    refs: "regexp:refs/heads/main"
   }
 }
 trigger {
@@ -354,8 +354,8 @@ trigger {
   triggers: "target.arm.poison.32"
   triggers: "target.arm.poison.64"
   gitiles {
-    repo: "https://android.googlesource.com/platform/libcore"
-    refs: "regexp:refs/heads/master"
+    repo: "https://googleplex-android.googlesource.com/platform/libcore"
+    refs: "regexp:refs/heads/main"
   }
 }
 trigger {
@@ -391,7 +391,7 @@ trigger {
   triggers: "target.arm.poison.32"
   triggers: "target.arm.poison.64"
   gitiles {
-    repo: "https://android.googlesource.com/platform/manifest"
+    repo: "https://googleplex-android.googlesource.com/platform/manifest"
     refs: "regexp:refs/heads/master-art"
   }
 }
@@ -428,8 +428,8 @@ trigger {
   triggers: "target.arm.poison.32"
   triggers: "target.arm.poison.64"
   gitiles {
-    repo: "https://android.googlesource.com/platform/external/vogar"
-    refs: "regexp:refs/heads/master"
+    repo: "https://googleplex-android.googlesource.com/platform/external/vogar"
+    refs: "regexp:refs/heads/main"
   }
 }
 acl_sets {
diff --git a/tools/luci/config/generated/project.cfg b/tools/luci/config/generated/project.cfg
index 5619b9ab83..1d7f2ddccb 100644
--- a/tools/luci/config/generated/project.cfg
+++ b/tools/luci/config/generated/project.cfg
@@ -7,7 +7,7 @@
 name: "art"
 access: "group:googlers"
 lucicfg {
-  version: "1.44.1"
+  version: "1.45.0"
   package_dir: ".."
   config_dir: "generated"
   entry_point: "main.star"
diff --git a/tools/luci/config/main.star b/tools/luci/config/main.star
index 42ee495bbc..a86ed0e3df 100755
--- a/tools/luci/config/main.star
+++ b/tools/luci/config/main.star
@@ -20,6 +20,8 @@
 After modifying this file execute it ('./main.star') to regenerate the configs.
 """
 
+REPO_ROOT = "https://googleplex-android.googlesource.com"
+
 lucicfg.check_version("1.30.9", "Please update depot_tools")
 
 luci.builder.defaults.experiments.set({
@@ -129,9 +131,9 @@ luci.notifier_template(
 
 luci.console_view(
     name = "luci",
-    repo = "https://android.googlesource.com/platform/art",
+    repo = REPO_ROOT + "/platform/art",
     title = "ART LUCI Console",
-    refs = ["refs/heads/master"],
+    refs = ["refs/heads/main"],
     include_experimental_builds = True,
 )
 
@@ -149,28 +151,28 @@ luci.notifier(
 luci.gitiles_poller(
     name = "art",
     bucket = "ci",
-    repo = "https://android.googlesource.com/platform/art",
-    refs = ["refs/heads/master"],
+    repo = REPO_ROOT + "/platform/art",
+    refs = ["refs/heads/main"],
 )
 
 luci.gitiles_poller(
     name = "libcore",
     bucket = "ci",
-    repo = "https://android.googlesource.com/platform/libcore",
-    refs = ["refs/heads/master"],
+    repo = REPO_ROOT + "/platform/libcore",
+    refs = ["refs/heads/main"],
 )
 
 luci.gitiles_poller(
     name = "vogar",
     bucket = "ci",
-    repo = "https://android.googlesource.com/platform/external/vogar",
-    refs = ["refs/heads/master"],
+    repo = REPO_ROOT + "/platform/external/vogar",
+    refs = ["refs/heads/main"],
 )
 
 luci.gitiles_poller(
     name = "manifest",
     bucket = "ci",
-    repo = "https://android.googlesource.com/platform/manifest",
+    repo = REPO_ROOT + "/platform/manifest",
     refs = ["refs/heads/master-art"],
 )
 
@@ -297,9 +299,10 @@ def add_builder(mode,
         "gcstress": gcstress,
         "heap_poisoning": poison,
         "testrunner_args": testrunner_args,
+        "repo_root": REPO_ROOT,
     }
 
-    experiments = {"art.superproject": 100} if mode == "qemu" else {}
+    experiments = {"art.superproject": 100}
 
     ci_builder(name=name,
                category="|".join(category.split("|")[:-1]),
diff --git a/tools/veridex/Android.bp b/tools/veridex/Android.bp
index f069a02ee5..25700f5fdc 100644
--- a/tools/veridex/Android.bp
+++ b/tools/veridex/Android.bp
@@ -76,13 +76,12 @@ genrule {
 }
 
 genrule {
-    name: "system_stub_dex",
+    name: "system-stubs.zip",
     visibility: ["//visibility:private"],
     srcs: [":system_stub_dex_d8_input_jar"],
     tools: ["d8"],
     out: [
-        "dex_dir/classes.dex",
-        "dex_dir/classes2.dex",
+        "system-stubs.zip",
     ],
     cmd: "mkdir -p $(genDir)/dex_dir &&" +
         "$(location d8) " +
@@ -91,7 +90,7 @@ genrule {
         "-JDcom.android.tools.r8.emitPermittedSubclassesAnnotationsInDex " +
         "-JXX:OnError=\"cat hs_err_pid%p.log\" " +
         "-JXX:CICompilerCount=6 -JXX:+UseDynamicNumberOfGCThreads " +
-        "--output $(genDir)/dex_dir " +
+        "--output $(out) " +
         "--min-api 1000 " +
         "$(in)",
 }
@@ -139,18 +138,6 @@ genrule {
         " -j -l $(genDir)/tmp/org.apache.http.legacy-stubs.zip.list",
 }
 
-genrule {
-    name: "system-stubs.zip",
-    visibility: ["//visibility:private"],
-    srcs: [":system_stub_dex"],
-    tools: ["soong_zip"],
-    out: ["system-stubs.zip"],
-    cmd: "mkdir -p $(genDir)/tmp &&" +
-        "ls -1 $(locations :system_stub_dex) | sort > $(genDir)/tmp/system-stubs.zip.list && " +
-        "$(locations soong_zip) -o $(genDir)/system-stubs.zip" +
-        " -j -l $(genDir)/tmp/system-stubs.zip.list",
-}
-
 python_binary_host {
     name: "appcompat",
     srcs: ["appcompat.py"],
```

