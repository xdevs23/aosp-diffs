```diff
diff --git a/.gitignore b/.gitignore
index 822882f7..ce56a639 100644
--- a/.gitignore
+++ b/.gitignore
@@ -4,4 +4,5 @@
 /rust-analyzer-chromiumos-wrapper/target
 compiler_wrapper/compiler_wrapper
 llvm-project-copy/
+.llvm-project-copy/
 logs
diff --git a/OWNERS.toolchain b/OWNERS.toolchain
index 967440e3..0cf8b9e9 100644
--- a/OWNERS.toolchain
+++ b/OWNERS.toolchain
@@ -1,7 +1,4 @@
 ajordanr@google.com
-cjdb@google.com
-denik@chromium.org
 gbiv@chromium.org
 inglorion@chromium.org
-manojgupta@chromium.org
-ryanbeltran@chromium.org
+spothire@google.com
diff --git a/PRESUBMIT.cfg b/PRESUBMIT.cfg
index 5fccf3ac..5e0b382f 100644
--- a/PRESUBMIT.cfg
+++ b/PRESUBMIT.cfg
@@ -1,5 +1,5 @@
 [Hook Scripts]
-toolchain_utils_presubmits = ./toolchain_utils_githooks/check-presubmit ${PRESUBMIT_FILES}
+toolchain_utils_presubmits = ./toolchain_utils_githooks/check-presubmit --infer_files
 
 [Hook Overrides Options]
 cros_license_check: --exclude_regex=\b(default_remotes)$
diff --git a/afdo_metadata/chrome_afdo.json b/afdo_metadata/chrome_afdo.json
deleted file mode 100644
index 5dbfa960..00000000
--- a/afdo_metadata/chrome_afdo.json
+++ /dev/null
@@ -1,14 +0,0 @@
-{
-    "silvermont": {
-        "name": "R79-3931.2-1571659204.afdo"
-    }, 
-    "benchmark": {
-        "name": "chromeos-chrome-amd64-79.0.3943.1_rc-r1.afdo"
-    }, 
-    "airmont": {
-        "name": "R79-3931.2-1571653097.afdo"
-    }, 
-    "broadwell": {
-        "name": "R79-3931.2-1571657055.afdo"
-    }
-}
\ No newline at end of file
diff --git a/afdo_metadata/kernel_afdo.json b/afdo_metadata/kernel_afdo.json
index 0180b302..edbfa300 100644
--- a/afdo_metadata/kernel_afdo.json
+++ b/afdo_metadata/kernel_afdo.json
@@ -1,14 +1,14 @@
 {
     "chromeos-kernel-5_10": {
-        "name": "R125-15823.16-1712568782"
+        "name": "R137-16238.12-1744592956"
     },
     "chromeos-kernel-5_15": {
-        "name": "R125-15832.0-1712569184"
+        "name": "R137-16250.0-1744592886"
     },
     "chromeos-kernel-5_4": {
-        "name": "R125-15823.16-1712569649"
+        "name": "R137-16249.0-1744593013"
     },
-    "chromeos-kernel-6_1": {
-        "name": "R125-15823.16-1712569561"
+    "chromeos-kernel-6_6": {
+        "name": "R137-16238.12-1744592681"
     }
 }
\ No newline at end of file
diff --git a/afdo_metadata/kernel_arm_afdo.json b/afdo_metadata/kernel_arm_afdo.json
index f2c02097..2d76fe99 100644
--- a/afdo_metadata/kernel_arm_afdo.json
+++ b/afdo_metadata/kernel_arm_afdo.json
@@ -1,5 +1,5 @@
 {
-    "chromeos-kernel-5_15": {
-        "name": "R124-15786.10-1709548825"
+    "chromeos-kernel-6_6": {
+        "name": "R137-16238.12-1744593121"
     }
 }
\ No newline at end of file
diff --git a/afdo_redaction/redact_profile.py b/afdo_redaction/redact_profile.py
old mode 100755
new mode 100644
index 0779d2ac..e7b0ba86
--- a/afdo_redaction/redact_profile.py
+++ b/afdo_redaction/redact_profile.py
@@ -1,5 +1,3 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
 # Copyright 2018 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -250,7 +248,3 @@ def _main():
         summary_output_file=sys.stderr,
         profile_output_file=sys.stdout,
     )
-
-
-if __name__ == "__main__":
-    _main()
diff --git a/afdo_redaction/redact_profile_test.py b/afdo_redaction/redact_profile_test.py
old mode 100755
new mode 100644
index 93c65510..71cf13af
--- a/afdo_redaction/redact_profile_test.py
+++ b/afdo_redaction/redact_profile_test.py
@@ -133,7 +133,3 @@ class Tests(unittest.TestCase):
         self.assertIn("Retained 1/101 functions", summary)
         self.assertIn("Retained 1,575/84,275 samples, total", summary)
         self.assertIn("Retained 1,083/34,583 top-level samples", summary)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/afdo_redaction/remove_cold_functions.py b/afdo_redaction/remove_cold_functions.py
old mode 100755
new mode 100644
index c6043bc0..d9258cab
--- a/afdo_redaction/remove_cold_functions.py
+++ b/afdo_redaction/remove_cold_functions.py
@@ -1,5 +1,3 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
 # Copyright 2020 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -208,7 +206,3 @@ def main():
                         run(stdin, stdout, args.number, cwp, benchmark)
             else:
                 run(stdin, stdout, args.number)
-
-
-if __name__ == "__main__":
-    main()
diff --git a/afdo_redaction/remove_cold_functions_test.py b/afdo_redaction/remove_cold_functions_test.py
old mode 100755
new mode 100644
index 89a87f82..87622803
--- a/afdo_redaction/remove_cold_functions_test.py
+++ b/afdo_redaction/remove_cold_functions_test.py
@@ -146,7 +146,3 @@ class Test(unittest.TestCase):
             " profiles",
             output,
         )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/afdo_redaction/remove_indirect_calls.py b/afdo_redaction/remove_indirect_calls.py
old mode 100755
new mode 100644
index 32dab3f4..453b9eff
--- a/afdo_redaction/remove_indirect_calls.py
+++ b/afdo_redaction/remove_indirect_calls.py
@@ -1,5 +1,3 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -79,7 +77,3 @@ def main():
     with open(args.input) as stdin:
         with open(args.output, "w") as stdout:
             run(stdin, stdout)
-
-
-if __name__ == "__main__":
-    main()
diff --git a/afdo_redaction/remove_indirect_calls_test.py b/afdo_redaction/remove_indirect_calls_test.py
old mode 100755
new mode 100644
index 640b747f..ece2205e
--- a/afdo_redaction/remove_indirect_calls_test.py
+++ b/afdo_redaction/remove_indirect_calls_test.py
@@ -60,7 +60,3 @@ class Test(unittest.TestCase):
   """.strip().splitlines()
 
         self.assertEqual(_run_test(profile_lines), expected_lines)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/afdo_tools/bisection/afdo_prof_analysis.py b/afdo_tools/bisection/afdo_prof_analysis.py
old mode 100755
new mode 100644
index c9ca9214..6f50dd6a
--- a/afdo_tools/bisection/afdo_prof_analysis.py
+++ b/afdo_tools/bisection/afdo_prof_analysis.py
@@ -1,5 +1,3 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -20,17 +18,16 @@ exit code. The codes known to this script are:
   - >127: quit immediately
 """
 
-
 import argparse
+from datetime import date
+from enum import IntEnum
 import json
 import logging
 import os
 import random
 import subprocess
-import time
-from datetime import date
-from enum import IntEnum
 from tempfile import mkstemp
+import time
 
 
 class StatusEnum(IntEnum):
@@ -325,7 +322,6 @@ def range_search(decider, good, bad, common_funcs, lo, hi):
     mid_hi_funcs = []
     min_range_funcs = []
     for _ in range(_NUM_RUNS_RANGE_SEARCH):
-
         if min_range_funcs:  # only examine range we've already narrowed to
             random.shuffle(lo_mid_funcs)
             random.shuffle(mid_hi_funcs)
@@ -435,7 +431,7 @@ def parse_args():
     return parser.parse_args()
 
 
-def main(flags):
+def main_impl(flags):
     logging.getLogger().setLevel(logging.INFO)
     if not flags.no_resume and flags.seed:  # conflicting seeds
         raise RuntimeError(
@@ -485,5 +481,5 @@ def main(flags):
     return results
 
 
-if __name__ == "__main__":
-    main(parse_args())
+def main():
+    main_impl(parse_args())
diff --git a/afdo_tools/bisection/afdo_prof_analysis_e2e_test.py b/afdo_tools/bisection/afdo_prof_analysis_e2e_test.py
old mode 100755
new mode 100644
index 8a0dae38..14121b9d
--- a/afdo_tools/bisection/afdo_prof_analysis_e2e_test.py
+++ b/afdo_tools/bisection/afdo_prof_analysis_e2e_test.py
@@ -1,29 +1,27 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
 """End-to-end test for afdo_prof_analysis."""
 
-
+import datetime
 import json
 import os
+from pathlib import Path
 import shutil
 import tempfile
-import unittest
-from datetime import date
 
 from afdo_tools.bisection import afdo_prof_analysis as analysis
+from llvm_tools import test_helpers
 
 
-class ObjectWithFields(object):
+class ObjectWithFields:
     """Turns kwargs given to the constructor into fields on an object.
 
     Examples:
-      x = ObjectWithFields(a=1, b=2)
-      assert x.a == 1
-      assert x.b == 2
+        x = ObjectWithFields(a=1, b=2)
+        assert x.a == 1
+        assert x.b == 2
     """
 
     def __init__(self, **kwargs):
@@ -31,7 +29,7 @@ class ObjectWithFields(object):
             setattr(self, key, val)
 
 
-class AfdoProfAnalysisE2ETest(unittest.TestCase):
+class AfdoProfAnalysisE2ETest(test_helpers.TempDirTestCase):
     """Class for end-to-end testing of AFDO Profile Analysis"""
 
     # nothing significant about the values, just easier to remember even vs odd
@@ -59,6 +57,26 @@ class AfdoProfAnalysisE2ETest(unittest.TestCase):
         "bisect_results": {"ranges": [], "individuals": ["func_a"]},
     }
 
+    def setUp(self):
+        super().setUp()
+
+        # Test scripts depend on AFDO_TEST_DIR pointing to a directory to run
+        # in. Set that up for them.
+        self.tempdir = self.make_tempdir()
+
+        saved_value = None
+        tmpdir_env_var = "AFDO_TEST_DIR"
+        saved_value = os.environ.get(tmpdir_env_var)
+        os.environ[tmpdir_env_var] = str(self.tempdir)
+
+        def restore_environ():
+            if saved_value is None:
+                del os.environ[tmpdir_env_var]
+            else:
+                os.environ[tmpdir_env_var] = saved_value
+
+        self.addCleanup(restore_environ)
+
     def test_afdo_prof_analysis(self):
         # Individual issues take precedence by nature of our algos
         # so first, that should be caught
@@ -66,7 +84,8 @@ class AfdoProfAnalysisE2ETest(unittest.TestCase):
         bad = self.bad_prof.copy()
         self.run_check(good, bad, self.expected)
 
-        # Now remove individuals and exclusively BAD, and check that range is caught
+        # Now remove individuals and exclusively BAD, and check that range is
+        # caught
         bad["func_a"] = good["func_a"]
         bad.pop("bad_func_a")
         bad.pop("bad_func_b")
@@ -108,7 +127,7 @@ class AfdoProfAnalysisE2ETest(unittest.TestCase):
         os.close(fd_second)
         completed_state_file = "%s.completed.%s" % (
             state_file,
-            str(date.today()),
+            str(datetime.date.today()),
         )
         self.run_check(
             self.good_prof,
@@ -119,9 +138,9 @@ class AfdoProfAnalysisE2ETest(unittest.TestCase):
             out_file=second_result,
         )
 
-        with open(first_result) as f:
+        with open(first_result, encoding="utf-8") as f:
             initial_run = json.load(f)
-        with open(second_result) as f:
+        with open(second_result, encoding="utf-8") as f:
             loaded_run = json.load(f)
         self.assertEqual(initial_run, loaded_run)
 
@@ -141,14 +160,14 @@ class AfdoProfAnalysisE2ETest(unittest.TestCase):
             )
 
     def test_state_assumption(self):
-        def compare_runs(tmp_dir, first_ctr, second_ctr):
-            """Compares given prof versions between first and second run in test."""
-            first_prof = "%s/.first_run_%d" % (tmp_dir, first_ctr)
-            second_prof = "%s/.second_run_%d" % (tmp_dir, second_ctr)
-            with open(first_prof) as f:
-                first_prof_text = f.read()
-            with open(second_prof) as f:
-                second_prof_text = f.read()
+        def compare_runs(
+            tmp_dir: Path, first_ctr: int, second_ctr: int
+        ) -> None:
+            """Compares given prof versions between 1st and 2nd run in test."""
+            first_prof = tmp_dir / f".first_run_{first_ctr}"
+            second_prof = tmp_dir / f".second_run_{second_ctr}"
+            first_prof_text = first_prof.read_text(encoding="utf-8")
+            second_prof_text = second_prof.read_text(encoding="utf-8")
             self.assertEqual(first_prof_text, second_prof_text)
 
         good_prof = {"func_a": ":1\n3: 3\n5: 7\n"}
@@ -164,37 +183,32 @@ class AfdoProfAnalysisE2ETest(unittest.TestCase):
             "bad_only_functions": False,
         }
 
-        # using a static temp dir rather than a dynamic one because these files are
-        # shared between the bash scripts and this Python test, and the arguments
-        # to the bash scripts are fixed by afdo_prof_analysis.py so it would be
-        # difficult to communicate dynamically generated directory to bash scripts
-        scripts_tmp_dir = "%s/afdo_test_tmp" % os.getcwd()
-        os.mkdir(scripts_tmp_dir)
-        self.addCleanup(shutil.rmtree, scripts_tmp_dir, ignore_errors=True)
+        my_dir = os.path.dirname(os.path.abspath(__file__))
+        scripts_tmp_dir = self.tempdir / "afdo_test_tmp"
+        scripts_tmp_dir.mkdir()
 
         # files used in the bash scripts used as external deciders below
-        # - count_file tracks the current number of calls to the script in total
+        # - count_file tracks the current number of calls to the script in
+        #   total
         # - local_count_file tracks the number of calls to the script without
-        # interruption
-        count_file = "%s/.count" % scripts_tmp_dir
-        local_count_file = "%s/.local_count" % scripts_tmp_dir
+        #   interruption
+        count_file = scripts_tmp_dir / ".count"
+        local_count_file = scripts_tmp_dir / ".local_count"
 
         # runs through whole thing at once
         initial_seed = self.run_check(
             good_prof,
             bad_prof,
             expected,
-            extern_decider="state_assumption_external.sh",
-        )
-        with open(count_file) as f:
-            num_calls = int(f.read())
-        os.remove(count_file)  # reset counts for second run
-        finished_state_file = "afdo_analysis_state.json.completed.%s" % str(
-            date.today()
+            extern_decider=os.path.join(my_dir, "state_assumption_external.sh"),
         )
-        self.addCleanup(os.remove, finished_state_file)
+        num_calls = int(count_file.read_text(encoding="utf-8"))
+        count_file.unlink()
 
         # runs the same analysis but interrupted each iteration
+        interrupt_decider = os.path.join(
+            my_dir, "state_assumption_interrupt.sh"
+        )
         for i in range(2 * num_calls + 1):
             no_resume_run = i == 0
             seed = initial_seed if no_resume_run else None
@@ -204,13 +218,13 @@ class AfdoProfAnalysisE2ETest(unittest.TestCase):
                     bad_prof,
                     expected,
                     no_resume=no_resume_run,
-                    extern_decider="state_assumption_interrupt.sh",
+                    extern_decider=interrupt_decider,
                     seed=seed,
                 )
                 break
             except RuntimeError:
                 # script was interrupted, so we restart local count
-                os.remove(local_count_file)
+                local_count_file.unlink()
         else:
             raise RuntimeError("Test failed -- took too many iterations")
 
@@ -219,7 +233,8 @@ class AfdoProfAnalysisE2ETest(unittest.TestCase):
 
         start = 3
         for ctr in range(start, num_calls):
-            # second run counter incremented by 4 for each one first run is because
+            # second run counter incremented by 4 for each one first run is
+            # because
             # +2 for performing initial checks on good and bad profs each time
             # +1 for PROBLEM_STATUS run which causes error and restart
             compare_runs(scripts_tmp_dir, ctr, 6 + (ctr - start) * 4)
@@ -235,41 +250,32 @@ class AfdoProfAnalysisE2ETest(unittest.TestCase):
         extern_decider=None,
         seed=None,
     ):
-
         temp_dir = tempfile.mkdtemp()
         self.addCleanup(shutil.rmtree, temp_dir, ignore_errors=True)
 
-        good_prof_file = "%s/%s" % (temp_dir, "good_prof.txt")
-        bad_prof_file = "%s/%s" % (temp_dir, "bad_prof.txt")
+        good_prof_file = os.path.join(temp_dir, "good_prof.txt")
+        bad_prof_file = os.path.join(temp_dir, "bad_prof.txt")
         good_prof_text = analysis.json_to_text(good_prof)
         bad_prof_text = analysis.json_to_text(bad_prof)
-        with open(good_prof_file, "w") as f:
+        with open(good_prof_file, "w", encoding="utf-8") as f:
             f.write(good_prof_text)
-        with open(bad_prof_file, "w") as f:
+        with open(bad_prof_file, "w", encoding="utf-8") as f:
             f.write(bad_prof_text)
 
         dir_path = os.path.dirname(
             os.path.realpath(__file__)
         )  # dir of this file
-        external_script = "%s/%s" % (
+        external_script = os.path.join(
             dir_path,
             extern_decider or "e2e_external.sh",
         )
 
-        # FIXME: This test ideally shouldn't be writing to $PWD
+        # FIXME: This test ideally shouldn't be writing to the directory of
+        # this file.
         if state_file is None:
-            state_file = "%s/afdo_analysis_state.json" % os.getcwd()
-
-            def rm_state():
-                try:
-                    os.unlink(state_file)
-                except OSError:
-                    # Probably because the file DNE. That's fine.
-                    pass
+            state_file = os.path.join(self.tempdir, "afdo_analysis_state.json")
 
-            self.addCleanup(rm_state)
-
-        actual = analysis.main(
+        actual = analysis.main_impl(
             ObjectWithFields(
                 good_prof=good_prof_file,
                 bad_prof=bad_prof_file,
@@ -284,7 +290,3 @@ class AfdoProfAnalysisE2ETest(unittest.TestCase):
         actual_seed = actual.pop("seed")  # nothing to check
         self.assertEqual(actual, expected)
         return actual_seed
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/afdo_tools/bisection/afdo_prof_analysis_test.py b/afdo_tools/bisection/afdo_prof_analysis_test.py
old mode 100755
new mode 100644
index babfc021..de63ec90
--- a/afdo_tools/bisection/afdo_prof_analysis_test.py
+++ b/afdo_tools/bisection/afdo_prof_analysis_test.py
@@ -6,9 +6,8 @@
 
 """Tests for afdo_prof_analysis."""
 
-
-import random
 import io
+import random
 import unittest
 
 from afdo_tools.bisection import afdo_prof_analysis as analysis
@@ -74,7 +73,6 @@ class AfdoProfAnalysisTest(unittest.TestCase):
         self.assertEqual(analysis.json_to_text(example_prof), expected_text)
 
     def test_bisect_profiles(self):
-
         # mock run of external script with arbitrarily-chosen bad profile vals
         # save_run specified and unused b/c afdo_prof_analysis.py
         # will call with argument explicitly specified
@@ -94,7 +92,6 @@ class AfdoProfAnalysisTest(unittest.TestCase):
         self.assertEqual(results["ranges"], [])
 
     def test_range_search(self):
-
         # arbitrarily chosen functions whose values in the bad profile constitute
         # a problematic pair
         # pylint: disable=unused-argument
@@ -162,7 +159,3 @@ class AfdoProfAnalysisTest(unittest.TestCase):
                 DeciderClass(), self.good_items, self.bad_items
             )
         )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/afdo_tools/bisection/state_assumption_external.sh b/afdo_tools/bisection/state_assumption_external.sh
index a2076b0d..47168f11 100755
--- a/afdo_tools/bisection/state_assumption_external.sh
+++ b/afdo_tools/bisection/state_assumption_external.sh
@@ -11,7 +11,8 @@ BAD_STATUS=1
 SKIP_STATUS=125
 PROBLEM_STATUS=127
 
-tmp_dir=$(pwd)/afdo_test_tmp
+my_dir="$(dirname "$(readlink -m "$0")")"
+tmp_dir="${AFDO_TEST_DIR:?}/afdo_test_tmp"
 count_file=${tmp_dir}/.count
 
 # keep count for purpose of filenames
diff --git a/afdo_tools/bisection/state_assumption_interrupt.sh b/afdo_tools/bisection/state_assumption_interrupt.sh
index d1599d0b..c96b7597 100755
--- a/afdo_tools/bisection/state_assumption_interrupt.sh
+++ b/afdo_tools/bisection/state_assumption_interrupt.sh
@@ -8,7 +8,8 @@
 
 PROBLEM_STATUS=127
 
-tmp_dir=$(pwd)/afdo_test_tmp
+my_dir="$(dirname "$(readlink -m "$0")")"
+tmp_dir="${AFDO_TEST_DIR:?}/afdo_test_tmp"
 
 count_file="${tmp_dir}/.count"
 if [[ -f "${count_file}" ]]; then
@@ -34,5 +35,5 @@ if [[ ${local_count} -ge 2 ]] && [[ $(( ${num_call}%2 )) -ne 0 ]]; then
 fi
 
 # script just needs any second argument to write profs to .second_run_*
-$(pwd)/state_assumption_external.sh "$1" 'second_run'
+"${my_dir}/state_assumption_external.sh" "$1" 'second_run'
 exit $?
diff --git a/afdo_tools/generate_afdo_from_tryjob.py b/afdo_tools/generate_afdo_from_tryjob.py
old mode 100755
new mode 100644
index e398f8a1..bb3a0619
--- a/afdo_tools/generate_afdo_from_tryjob.py
+++ b/afdo_tools/generate_afdo_from_tryjob.py
@@ -1,20 +1,17 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
 """Given a tryjob and perf profile, generates an AFDO profile."""
 
-
 import argparse
-import distutils.spawn
 import os
 import os.path
 import shutil
 import subprocess
 import sys
 import tempfile
+from typing import List
 
 
 _CREATE_LLVM_PROF = "create_llvm_prof"
@@ -64,9 +61,9 @@ def _generate_afdo(perf_profile_loc, tryjob_loc, output_name):
     # extracting the rest in _fetch_and_maybe_unpack.
     subprocess.check_call(["tar", "xaf", "debug.tgz", chrome_in_debug_loc])
 
-    # Note that the AFDO tool *requires* a binary named `chrome` to be present if
-    # we're generating a profile for chrome. It's OK for this to be split debug
-    # information.
+    # Note that the AFDO tool *requires* a binary named `chrome` to be present
+    # if we're generating a profile for chrome. It's OK for this to be split
+    # debug information.
     os.rename(chrome_in_debug_loc, "chrome")
 
     print("Generating AFDO profile.")
@@ -110,7 +107,7 @@ def _tryjob_arg(tryjob_arg):
     return _GS_PREFIX + chell_path + tryjob_arg
 
 
-def main():
+def main(argv: List[str]):
     parser = argparse.ArgumentParser(description=__doc__)
     parser.add_argument(
         "--perf_profile",
@@ -138,9 +135,9 @@ def main():
         action="store_true",
         help="Don't remove the tempdir on failure",
     )
-    args = parser.parse_args()
+    args = parser.parse_args(argv)
 
-    if not distutils.spawn.find_executable(_CREATE_LLVM_PROF):
+    if not shutil.which(_CREATE_LLVM_PROF):
         sys.exit(_CREATE_LLVM_PROF + " not found; are you in the chroot?")
 
     profile = _abspath_or_gs_link(args.perf_profile)
@@ -153,9 +150,9 @@ def main():
         os.chdir(temp_dir)
         _generate_afdo(profile, args.tryjob, afdo_output)
 
-        # The AFDO tooling is happy to generate essentially empty profiles for us.
-        # Chrome's profiles are often 8+ MB; if we only see a small fraction of
-        # that, something's off. 512KB was arbitrarily selected.
+        # The AFDO tooling is happy to generate essentially empty profiles for
+        # us. Chrome's profiles are often 8+ MB; if we only see a small
+        # fraction of that, something's off. 512KB was arbitrarily selected.
         if os.path.getsize(afdo_output) < 512 * 1024:
             raise ValueError(
                 "The AFDO profile is suspiciously small for Chrome. "
@@ -171,7 +168,3 @@ def main():
             shutil.rmtree(temp_dir, ignore_errors=True)
         else:
             print("Artifacts are available at", temp_dir)
-
-
-if __name__ == "__main__":
-    sys.exit(main())
diff --git a/afdo_tools/monitor_chrome_afdo.py b/afdo_tools/monitor_chrome_afdo.py
new file mode 100644
index 00000000..8e75d7a2
--- /dev/null
+++ b/afdo_tools/monitor_chrome_afdo.py
@@ -0,0 +1,741 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Complains if Chrome's AFDO profiles are too old on a branch."""
+
+import argparse
+import collections
+import dataclasses
+import datetime
+import enum
+import logging
+import os
+from pathlib import Path
+import re
+import sys
+import textwrap
+from typing import Dict, Iterable, List, Optional, Tuple
+
+from cros_utils import bugs
+from cros_utils import cros_paths
+from cros_utils import git_utils
+from cros_utils import gs
+
+
+# Profiles with benchmark versions less than this won't be fully parsed.
+# There're legacy profile formats/subtypes/arches/etc that we should no longer
+# care about.
+MIN_PROFILE_MAJOR_VERSION = 120
+
+# For synthesized cronjob logs, how many hours should pass without any reports
+# before the job is considered 'turned down'.
+# Default to around a week, since branches come and go 1x/mo.
+CRONJOB_TURNDOWN_TIME_HOURS = 7 * 24
+
+# Complaint is used below to make function signatures clearer. Semantically
+# each Complaint is a list of paragraphs that should be printed together as a
+# single diagnostic. Lines may be reflowed.
+Complaint = List[str]
+
+# Iterable of milestones to skip monitoring for. Once a milestone leaves
+# stable, it can be removed from this.
+SKIP_MILESTONES = (
+    # b/384142128#comment33
+    131,
+)
+
+
+class ProfileArch(enum.Enum):
+    """The arch of a Chrome AFDO profile."""
+
+    ARM = "arm"
+    AMD64 = "amd64"
+
+    @classmethod
+    def parse(cls, arch: str) -> "ProfileArch":
+        for x in cls:
+            if x.value == arch:
+                return x
+        raise ValueError(f"No corresponding ProfileArch for {arch!r}")
+
+
+class ProfileSubtype(enum.Enum):
+    """The subtype of a Chrome AFDO profile."""
+
+    ATOM = "atom"
+    ARM32 = "arm32"
+    BIGCORE = "bigcore"
+    EXP = "exp"
+    NONE = "none"
+
+    @classmethod
+    def parse(cls, subtype: str) -> "ProfileArch":
+        for x in cls:
+            if x.value == subtype:
+                return x
+        raise ValueError(f"No corresponding ProfileSubtype for {subtype!r}")
+
+
+# All of the (arch, subtype) combos we care to monitor at the moment, and where
+# they exist under a ${chromium}/src checkout.
+#
+# Note that `exp` and `arm32` profiles are ignored, as they aren't used in
+# production.
+CHROME_STAMP_FILE_LOCATIONS: Dict[Tuple[ProfileArch, ProfileSubtype], str] = {
+    (
+        ProfileArch.ARM,
+        ProfileSubtype.NONE,
+    ): "chromeos/profiles/arm.afdo.newest.txt",
+    (
+        ProfileArch.AMD64,
+        ProfileSubtype.ATOM,
+    ): "chromeos/profiles/atom.afdo.newest.txt",
+    (
+        ProfileArch.AMD64,
+        ProfileSubtype.BIGCORE,
+    ): "chromeos/profiles/bigcore.afdo.newest.txt",
+}
+
+# N.B., This is expected to return a subset of CHROME_STAMP_FILE_LOCATIONS. That
+# said, iterating over CHROME_STAMP_FILE_LOCATIONS.keys() can be confusing
+# in places, so have a thin wrapper.
+def monitored_profile_configs() -> Iterable[Tuple[ProfileArch, ProfileSubtype]]:
+    """Returns an iterable of all currently-monitored profile configurations."""
+    return CHROME_STAMP_FILE_LOCATIONS.keys()
+
+
+@dataclasses.dataclass(frozen=True, order=True, eq=True)
+class ChromeVersion:
+    """Chrome version."""
+
+    major: int
+    minor: int
+    build: int
+    patch: int
+    revision: int
+
+
+@dataclasses.dataclass(frozen=True, eq=True)
+class ChromeGsProfile:
+    """Represents a Chrome profile in gs://."""
+
+    # When the profile was last modified.
+    last_modified: datetime.datetime
+    # Arch of the profile.
+    arch: ProfileArch
+    # Subtype of the profile, e.g., atom, bigcore
+    subtype: ProfileSubtype
+    # The version of Chrome used to generate the benchmark part of this
+    # profile.
+    benchmark_part_version: ChromeVersion
+    # The version of Chrome CWP profiles are sourced from.
+    cwp_part_version: ChromeVersion
+    # The timestamp at which the CWP profile was generated.
+    cwp_timestamp: int
+
+    _FULL_NAME_RE = re.compile(
+        r"^"
+        r"chromeos-chrome-"
+        r"(?P<arch>[^-]+)-"
+        r"(?P<subtype>[^-]+)-"
+        r"(?P<cwp_major>\d+)-"
+        r"(?P<cwp_build>\d+)\."
+        r"(?P<cwp_patch>\d+)-"
+        r"(?P<cwp_timestamp>\d+)-"
+        r"benchmark-"
+        r"(?P<bench_major>\d+)\."
+        r"(?P<bench_minor>\d+)\."
+        r"(?P<bench_build>\d+)\."
+        r"(?P<bench_patch>\d+)-"
+        r"r(?P<bench_revision>\d+)"
+        r"-redacted\.afdo\.xz"
+        r"$"
+    )
+
+    def full_name(self) -> str:
+        return (
+            "chromeos-chrome-"
+            f"{self.arch.value}-"
+            f"{self.subtype.value}-"
+            f"{self.cwp_part_version.major}-"
+            f"{self.cwp_part_version.build}."
+            f"{self.cwp_part_version.patch}-"
+            f"{self.cwp_timestamp}-"
+            "benchmark-"
+            f"{self.benchmark_part_version.major}."
+            f"{self.benchmark_part_version.minor}."
+            f"{self.benchmark_part_version.build}."
+            f"{self.benchmark_part_version.patch}-"
+            f"r{self.benchmark_part_version.revision}"
+            "-redacted.afdo.xz"
+        )
+
+    @classmethod
+    def from_full_name_if_new_enough(
+        cls, last_modified: datetime.datetime, full_name: str
+    ) -> Optional["ChromeGsProfile"]:
+        m = cls._FULL_NAME_RE.match(full_name)
+        if not m:
+            raise ValueError(f"{full_name!r} is not parseable as a profile")
+        groups = m.groupdict()
+
+        bench_major = int(groups["bench_major"])
+        if bench_major < MIN_PROFILE_MAJOR_VERSION:
+            return None
+
+        arch = ProfileArch.parse(groups["arch"])
+        raw_subtype = groups["subtype"]
+        subtype = ProfileSubtype.parse(raw_subtype)
+
+        cwp_part_version = ChromeVersion(
+            major=int(groups["cwp_major"]),
+            minor=0,
+            build=int(groups["cwp_build"]),
+            patch=int(groups["cwp_patch"]),
+            revision=0,
+        )
+        benchmark_part_version = ChromeVersion(
+            major=int(groups["bench_major"]),
+            minor=int(groups["bench_minor"]),
+            build=int(groups["bench_build"]),
+            patch=int(groups["bench_patch"]),
+            revision=int(groups["bench_revision"]),
+        )
+        return cls(
+            last_modified=last_modified,
+            arch=arch,
+            subtype=subtype,
+            benchmark_part_version=benchmark_part_version,
+            cwp_part_version=cwp_part_version,
+            cwp_timestamp=int(groups["cwp_timestamp"]),
+        )
+
+
+def fetch_release_afdo_profiles() -> Dict[int, List[ChromeGsProfile]]:
+    """Fetches release Chrome AFDO profiles, grouped by major version.
+
+    The major version used is specifically the benchmark part. List ordering is
+    unspecified.
+    """
+    results = collections.defaultdict(list)
+    for gs_entry in gs.ls("gs://chromeos-prebuilt/afdo-job/vetted/release"):
+        profile_name = os.path.basename(gs_entry.gs_path)
+        # All directories end with `/`, so  their basenames are empty.
+        if not profile_name:
+            continue
+        assert gs_entry.last_modified is not None, (
+            "Non-directory unexpectedly has a None last-modified date: "
+            f"{gs_entry}"
+        )
+        profile = ChromeGsProfile.from_full_name_if_new_enough(
+            gs_entry.last_modified, profile_name
+        )
+        if profile:
+            results[profile.benchmark_part_version.major].append(profile)
+    return results
+
+
+def find_most_recent_branch_profile(
+    afdo_profiles: Dict[int, List[ChromeGsProfile]],
+    arch: ProfileArch,
+    subtype: ProfileSubtype,
+    branch_number: int,
+) -> ChromeGsProfile:
+    """Returns the most recent profile for a given branch number.
+
+    Falls back to prior branches if none could be found for the given branch.
+
+    Raises:
+        ValueError if no compatible profiles could be found.
+    """
+    logging.debug(
+        "Finding most recent profile for M%d, arch=%s, subtype=%s",
+        branch_number,
+        arch,
+        subtype,
+    )
+    for i in range(branch_number, 0, -1):
+        branch_profiles = afdo_profiles.get(i)
+        if not branch_profiles:
+            logging.debug("No profiles found for branch M%d", i)
+            continue
+
+        matching_profiles = (
+            x
+            for x in branch_profiles
+            if x.arch is arch and x.subtype is subtype
+        )
+        result = max(
+            matching_profiles, key=lambda x: x.last_modified, default=None
+        )
+        if result:
+            return result
+        logging.debug("No _matching_ profiles found for branch M%d", i)
+
+    raise ValueError(
+        f"Found no branch profiles for {arch}-{subtype} starting "
+        f"at M{branch_number}"
+    )
+
+
+def check_cwp_profiles_are_new(
+    branches: List[Tuple[git_utils.Channel, git_utils.ChannelBranch]],
+    afdo_profiles: Dict[int, List[ChromeGsProfile]],
+    now: datetime.datetime,
+    max_profile_age: datetime.timedelta,
+) -> Dict[int, List[Complaint]]:
+    """Checks to see if the CWP profile parts for the given channel look good.
+
+    Returns:
+        Complaints about profiles, per-milestone.
+    """
+    complaints = {}
+    for channel, branch in branches:
+        if branch.release_number in SKIP_MILESTONES:
+            logging.warning(
+                "Release M%d is marked as skipped; ignoring CWP profiles.",
+                branch.release_number,
+            )
+            continue
+
+        logging.info(
+            "Monitoring CWP profiles for M%s (%s)",
+            branch.release_number,
+            channel,
+        )
+        branch_complaints = []
+
+        for arch, subtype in monitored_profile_configs():
+            most_recent_profile = find_most_recent_branch_profile(
+                afdo_profiles=afdo_profiles,
+                arch=arch,
+                subtype=subtype,
+                branch_number=branch.release_number,
+            )
+            time_since_modification = now - most_recent_profile.last_modified
+            logging.info(
+                "Most recent profile for %s-%s is %s, which is %s old.",
+                arch,
+                subtype,
+                most_recent_profile.full_name(),
+                time_since_modification,
+            )
+            if time_since_modification < max_profile_age:
+                continue
+
+            branch_complaints.append(
+                [
+                    textwrap.dedent(
+                        f"""\
+                        CWP profile on M{branch.release_number} for
+                        {arch}-{subtype} is too old. Its age is
+                        {time_since_modification}, but the limit is
+                        {max_profile_age}.
+                        """
+                    ),
+                    textwrap.dedent(
+                        """\
+                        You might want to reach out to CWP and ensure profile
+                        generation is going smoothly.
+                        """
+                    ),
+                ]
+            )
+
+        if branch_complaints:
+            complaints[branch.release_number] = branch_complaints
+    return complaints
+
+
+def find_newest_chrome_version(chromeos_chrome_files: List[str]) -> str:
+    """Returns the newest Chrome version from the given ebuilds.
+
+    Returns:
+        The Chrome version, as a string that can be used in a Chromium
+        repository.
+    """
+    chrome_re = re.compile(
+        r"^chromeos-chrome-((\d+)\.(\d+)\.(\d+)\.(\d+))_rc-r(\d+).ebuild$"
+    )
+    candidates = []
+    for f in chromeos_chrome_files:
+        m = chrome_re.fullmatch(f)
+        if not m:
+            continue
+
+        full_version, major, minor, build, patch, revision = m.groups()
+        ver = ChromeVersion(
+            major=int(major),
+            minor=int(minor),
+            build=int(build),
+            patch=int(patch),
+            revision=int(revision),
+        )
+        candidates.append((ver, full_version))
+
+    if not candidates:
+        raise ValueError(
+            f"No stable Chrome ebuilds found in {chromeos_chrome_files}"
+        )
+    _, result = max(candidates)
+    return result
+
+
+def find_afdo_profile_by_version(
+    afdo_profiles: Dict[int, List[ChromeGsProfile]],
+    stamp_contents: str,
+) -> ChromeGsProfile:
+    for profile_listing in afdo_profiles.values():
+        for profile in profile_listing:
+            if profile.full_name() == stamp_contents:
+                return profile
+    raise ValueError(
+        f"No available profile is associated with stamp {stamp_contents}"
+    )
+
+
+def maybe_diagnose_current_chrome_afdo_profile(
+    *,
+    channel: git_utils.Channel,
+    branch: git_utils.ChannelBranch,
+    arch: ProfileArch,
+    subtype: ProfileSubtype,
+    now: datetime.datetime,
+    afdo_profiles: Dict[int, List[ChromeGsProfile]],
+    current_profile_stamp: str,
+    max_profile_age: datetime.timedelta,
+) -> Optional[Complaint]:
+    """Potentially complains about the age of the given profile.
+
+    Returns:
+        A complaint to issue about the given profile, if any.
+    """
+    current_profile = find_afdo_profile_by_version(
+        afdo_profiles, current_profile_stamp
+    )
+
+    age = now - current_profile.last_modified
+    logging.info(
+        "Profile on M%s (%s) for %s-%s is %s, which is %s old.",
+        branch.release_number,
+        channel,
+        arch,
+        subtype,
+        current_profile.full_name(),
+        age,
+    )
+    if age < max_profile_age:
+        return None
+
+    logging.error(
+        "Profile is too old; maximum allowable age is %s.", max_profile_age
+    )
+
+    complaint = [
+        textwrap.dedent(
+            f"""\
+            AFDO profile on M{branch.release_number} for {arch}-{subtype} is
+            too old. Its age is {age}, and the limit is {max_profile_age}.
+            """
+        ),
+    ]
+
+    # Opportunistically search to see if something newer could've landed, and
+    # log it if so.
+    most_recent_uploaded_profile = find_most_recent_branch_profile(
+        afdo_profiles, arch, subtype, branch.release_number
+    )
+    if most_recent_uploaded_profile == current_profile:
+        complaint.append(
+            textwrap.dedent(
+                """\
+                No newer verified profile of this type exists. Maybe the
+                benchmark generation pipeline is having issues?
+                """
+            ),
+        )
+    else:
+        new_age = now - most_recent_uploaded_profile.last_modified
+        complaint += (
+            textwrap.dedent(
+                f"""\
+                NOTE: A newer profile of this type, which is {new_age} old,
+                exists in gs://. It's
+                {most_recent_uploaded_profile.full_name()}.
+                """
+            ),
+            textwrap.dedent(
+                """\
+                Since a newer profile exists but has not yet been rolled, Skia
+                autorollers may be malfunctioning.
+                """
+            ),
+        )
+
+    return complaint
+
+
+def check_afdo_profiles_are_new(
+    *,
+    chrome_src: Path,
+    chromiumos_overlay: Path,
+    branches: List[Tuple[git_utils.Channel, git_utils.ChannelBranch]],
+    afdo_profiles: Dict[int, List[ChromeGsProfile]],
+    now: datetime.datetime,
+    max_profile_age: datetime.timedelta,
+) -> Dict[int, List[Complaint]]:
+    """Checks to see if the AFDO profiles for the given channel look good.
+
+    Returns:
+        Complaints about profiles, per-milestone.
+    """
+    complaints = {}
+    for channel, branch in branches:
+        if branch.release_number in SKIP_MILESTONES:
+            logging.warning(
+                "Release M%d is marked as skipped; ignoring AFDO profiles.",
+                branch.release_number,
+            )
+            continue
+
+        logging.info(
+            "Monitoring landed AFDO profiles for M%s (%s)",
+            branch.release_number,
+            channel,
+        )
+        chromeos_chrome_contents = git_utils.maybe_list_dir_contents_at_commit(
+            git_dir=chromiumos_overlay,
+            ref=f"{branch.remote}/{branch.branch_name}",
+            path_from_git_root="chromeos-base/chromeos-chrome",
+        )
+        if chromeos_chrome_contents is None:
+            raise ValueError(
+                f"No chromeos-base/chromeos-chrome directory at "
+                f"{branch.branch_name}"
+            )
+
+        newest_chrome_version = find_newest_chrome_version(
+            chromeos_chrome_contents
+        )
+        logging.info(
+            "Newest Chrome version on %s is %s", channel, newest_chrome_version
+        )
+
+        branch_complaints = []
+        for arch, subtype in monitored_profile_configs():
+            stamp_file = CHROME_STAMP_FILE_LOCATIONS[(arch, subtype)]
+            stamp_contents = git_utils.maybe_show_file_at_commit(
+                git_dir=chrome_src,
+                ref=newest_chrome_version,
+                path_from_git_root=stamp_file,
+            )
+            if not stamp_contents:
+                raise ValueError(
+                    f"No version file found at {stamp_file} in Chromium at "
+                    f"{newest_chrome_version}"
+                )
+
+            maybe_complaint = maybe_diagnose_current_chrome_afdo_profile(
+                channel=channel,
+                branch=branch,
+                arch=arch,
+                subtype=subtype,
+                now=now,
+                afdo_profiles=afdo_profiles,
+                current_profile_stamp=stamp_contents.strip(),
+                max_profile_age=max_profile_age,
+            )
+            if maybe_complaint:
+                branch_complaints.append(maybe_complaint)
+
+        if branch_complaints:
+            complaints[branch.release_number] = branch_complaints
+
+    return complaints
+
+
+def merge_milestone_complaints(
+    a: Dict[int, List[Complaint]], b: Dict[int, List[Complaint]]
+) -> Dict[int, List[Complaint]]:
+    """Merges two per-milestone Complaints dicts into one."""
+    return {k: sorted(a.get(k, []) + b.get(k, [])) for k in a.keys() | b.keys()}
+
+
+def format_complaint(complaint: Complaint, width: int) -> str:
+    """Formats a complaint for printing. May return multiple lines."""
+    result_paragraphs = (textwrap.fill(x, width) for x in complaint)
+    return "\n\n".join(result_paragraphs)
+
+
+def format_complaints(
+    milestone: int, complaints: List[Complaint], width: int
+) -> str:
+    lines = [f"Complaint(s) for M{milestone}:"]
+    for complaint in sorted(complaints):
+        # Set width to 70 because 80cols is standard, and we're adding
+        # indentation below.
+        formatted = format_complaint(complaint, width=width)
+        indented = formatted.replace("\n", "\n\t  ")
+        lines.append(f"\t- {indented}")
+    return "\n".join(lines)
+
+
+def upload_cronjob_reports(
+    branches: List[Tuple[git_utils.Channel, git_utils.ChannelBranch]],
+    milestone_complaints: Dict[int, List[Complaint]],
+) -> None:
+    """Uploads synthesized cronjob reports outlining this script's findings."""
+    for channel, branch in branches:
+        logging.info(
+            "Uploading cronjob report for M%d (%s)...",
+            branch.release_number,
+            channel,
+        )
+        complaints = milestone_complaints.get(branch.release_number)
+        if complaints:
+            failed = True
+            message = format_complaints(
+                branch.release_number,
+                complaints,
+                width=70,
+            )
+        else:
+            failed = False
+            message = "All profiles are sufficiently fresh."
+
+        bugs.SendCronjobLog(
+            cronjob_name=f"Chrome AFDO Monitor, M{branch.release_number}",
+            failed=failed,
+            message=message,
+            turndown_time_hours=CRONJOB_TURNDOWN_TIME_HOURS,
+        )
+
+
+def main(argv: List[str]) -> None:
+    parser = argparse.ArgumentParser(
+        description=__doc__,
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+    )
+    parser.add_argument(
+        "--debug",
+        action="store_true",
+        help="Enable debug logging.",
+    )
+    parser.add_argument(
+        "--chrome-tree",
+        type=Path,
+        required=True,
+        help="Path to a Chrome tree to inspect.",
+    )
+    parser.add_argument(
+        "--fetch",
+        action="store_true",
+        help="Run `git fetch` in relevant directories prior to running.",
+    )
+    parser.add_argument(
+        "--max-profile-age-days",
+        type=int,
+        default=10,
+        help="""
+        The maximum number of days old the newest Chrome profile can be before
+        this script starts erroring about it. Default: %(default)s
+        """,
+    )
+    parser.add_argument(
+        "--max-cwp-age-days",
+        type=int,
+        default=10,
+        help="""
+        The maximum number of days old the newest CWP profile can be before
+        this script starts erroring about it. Default: %(default)s
+        """,
+    )
+    parser.add_argument(
+        "--upload-cronjob-reports",
+        action="store_true",
+        help="""
+        If specified, this script will upload per-channel cronjob reports
+        (for synthesized, per-milestone cronjobs, e.g., `M125 Chrome AFDO
+        monitor`) rather than reporting its results through stdout and exit
+        codes. If this flag is passed, the exit code of this script will be 0
+        even if there are old AFDO profiles detected.
+        """,
+    )
+    parser.add_argument(
+        "channel",
+        nargs="*",
+        type=git_utils.Channel.parse,
+        default=list(git_utils.Channel),
+        help=f"""
+        Channel(s) to update. If none are passed, this will update all
+        channels. Choose from {[x.value for x in git_utils.Channel]}.
+        """,
+    )
+    opts = parser.parse_args(argv)
+
+    logging.basicConfig(
+        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
+        "%(message)s",
+        level=logging.DEBUG if opts.debug else logging.INFO,
+    )
+
+    chromeos_tree = cros_paths.script_chromiumos_checkout_or_exit()
+    chromiumos_overlay = chromeos_tree / cros_paths.CHROMIUMOS_OVERLAY
+    chrome_src = opts.chrome_tree / "src"
+
+    if opts.fetch:
+        logging.info("Fetching chromiumos-overlay...")
+        git_utils.fetch(chromiumos_overlay)
+        logging.info("Fetching chromium...")
+        git_utils.fetch(chrome_src)
+
+    channel_branches = git_utils.autodetect_cros_channels(
+        git_repo=chromiumos_overlay
+    )
+
+    logging.info("Fetching listing of released AFDO profiles...")
+    afdo_profiles = fetch_release_afdo_profiles()
+    logging.info(
+        "%d profiles fetched", sum(len(x) for x in afdo_profiles.values())
+    )
+
+    now = datetime.datetime.now(datetime.timezone.utc)
+    branch_tuples = [(x, channel_branches[x]) for x in opts.channel]
+    cwp_complaints = check_cwp_profiles_are_new(
+        branch_tuples,
+        afdo_profiles,
+        now,
+        max_profile_age=datetime.timedelta(days=opts.max_cwp_age_days),
+    )
+    afdo_complaints = check_afdo_profiles_are_new(
+        chrome_src=chrome_src,
+        chromiumos_overlay=chromiumos_overlay,
+        branches=branch_tuples,
+        afdo_profiles=afdo_profiles,
+        now=now,
+        max_profile_age=datetime.timedelta(days=opts.max_profile_age_days),
+    )
+
+    milestone_complaints = merge_milestone_complaints(
+        cwp_complaints, afdo_complaints
+    )
+
+    if opts.upload_cronjob_reports:
+        upload_cronjob_reports(branch_tuples, milestone_complaints)
+        logging.info("All cronjob reports published; my job here is done.")
+        return
+
+    if not milestone_complaints:
+        logging.info("All checks passed.")
+        return
+
+    for i, (milestone, complaints) in enumerate(
+        sorted(milestone_complaints.items())
+    ):
+        if i:
+            print()
+        print(format_complaints(milestone, complaints, width=70))
+
+    logging.error("Issues were found; exiting with an error.")
+    sys.exit(0 if opts.upload_cronjob_reports else 1)
diff --git a/afdo_tools/monitor_chrome_afdo_test.py b/afdo_tools/monitor_chrome_afdo_test.py
new file mode 100644
index 00000000..a8c2de4a
--- /dev/null
+++ b/afdo_tools/monitor_chrome_afdo_test.py
@@ -0,0 +1,233 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Tests for monitor_chrome_afdo."""
+
+import dataclasses
+import datetime
+from typing import List
+import unittest
+
+from afdo_tools import monitor_chrome_afdo
+
+
+def arbitrary_time() -> datetime.datetime:
+    """Returns an arbitrary datetime, in UTC."""
+    return datetime.datetime(2020, 1, 2, 3, 4, 5, 6, datetime.timezone.utc)
+
+
+def arbitrary_chrome_gs_profile_name() -> monitor_chrome_afdo.ChromeGsProfile:
+    """Returns an arbitrary profile name >= MIN_PROFILE_MAJOR_VERSION."""
+    major_cwp_version = monitor_chrome_afdo.MIN_PROFILE_MAJOR_VERSION
+    major_bench_version = major_cwp_version + 1
+    return (
+        f"chromeos-chrome-arm-none-{major_cwp_version}-6440.4-1716810247-"
+        f"benchmark-{major_bench_version}.1.6533.2-r3-redacted.afdo.xz"
+    )
+
+
+def arbitrary_chrome_gs_profile() -> monitor_chrome_afdo.ChromeGsProfile:
+    """Returns an arbitrary ChromeGsProfile."""
+    full_name = arbitrary_chrome_gs_profile_name()
+    x = monitor_chrome_afdo.ChromeGsProfile.from_full_name_if_new_enough(
+        last_modified=arbitrary_time(),
+        full_name=full_name,
+    )
+    assert x is not None, f"Profile name was too old? Name: {full_name}"
+    return x
+
+
+def increasing_chrome_gs_profile_sequence(
+    count: int,
+) -> List[monitor_chrome_afdo.ChromeGsProfile]:
+    """Returns an iterable of successive Chrome GS profiles.
+
+    They're all successive in that:
+    1. Their `last_modified` are yielded in increasing order.
+    2. Their benchmark versions are also yielded in increasing order.
+
+    Otherwise, all attributes (profile arch, subtype, etc) will remain
+    consistent across all profiles.
+
+    Returns:
+        A list of `count` profiles.
+    """
+    baseline = arbitrary_chrome_gs_profile()
+    return [
+        dataclasses.replace(
+            baseline,
+            last_modified=baseline.last_modified
+            + datetime.timedelta(seconds=x),
+            cwp_timestamp=baseline.cwp_timestamp + x,
+        )
+        for x in range(count)
+    ]
+
+
+class Test(unittest.TestCase):
+    """Tests for monitor_chrome_afdo."""
+
+    def test_all_profile_arch_parsing(self):
+        for arch in monitor_chrome_afdo.ProfileArch:
+            self.assertEqual(
+                arch, monitor_chrome_afdo.ProfileArch.parse(arch.value)
+            )
+        with self.assertRaises(ValueError):
+            monitor_chrome_afdo.ProfileArch.parse("not-a-profile-arch")
+
+    def test_all_profile_subtype_parsing(self):
+        for subtype in monitor_chrome_afdo.ProfileSubtype:
+            self.assertEqual(
+                subtype, monitor_chrome_afdo.ProfileSubtype.parse(subtype.value)
+            )
+        with self.assertRaises(ValueError):
+            monitor_chrome_afdo.ProfileSubtype.parse("not-a-profile-subtype")
+
+    def test_gs_profile_parsing(self):
+        last_modified = arbitrary_time()
+        profile_name = arbitrary_chrome_gs_profile_name()
+        self.assertEqual(
+            monitor_chrome_afdo.ChromeGsProfile.from_full_name_if_new_enough(
+                last_modified=last_modified,
+                full_name=profile_name,
+            ),
+            monitor_chrome_afdo.ChromeGsProfile(
+                last_modified=last_modified,
+                arch=monitor_chrome_afdo.ProfileArch.ARM,
+                subtype=monitor_chrome_afdo.ProfileSubtype.NONE,
+                benchmark_part_version=monitor_chrome_afdo.ChromeVersion(
+                    major=monitor_chrome_afdo.MIN_PROFILE_MAJOR_VERSION + 1,
+                    minor=1,
+                    build=6533,
+                    patch=2,
+                    revision=3,
+                ),
+                cwp_part_version=monitor_chrome_afdo.ChromeVersion(
+                    major=monitor_chrome_afdo.MIN_PROFILE_MAJOR_VERSION,
+                    minor=0,
+                    build=6440,
+                    patch=4,
+                    revision=0,
+                ),
+                cwp_timestamp=1716810247,
+            ),
+        )
+
+    def test_gs_profile_name_round_trips(self):
+        last_modified = arbitrary_time()
+        profile_name = arbitrary_chrome_gs_profile_name()
+        self.assertEqual(
+            monitor_chrome_afdo.ChromeGsProfile.from_full_name_if_new_enough(
+                last_modified=last_modified, full_name=profile_name
+            ).full_name(),
+            profile_name,
+        )
+
+    def test_gs_profile_parsing_on_old_profile(self):
+        major_bench_version = monitor_chrome_afdo.MIN_PROFILE_MAJOR_VERSION - 1
+
+        last_modified = arbitrary_time()
+        profile_name = (
+            f"chromeos-chrome-nonsense_arch-nonsense_type-0-6440.4-1716810247-"
+            f"benchmark-{major_bench_version}.1.6533.2-r3-redacted.afdo.xz"
+        )
+        self.assertIsNone(
+            monitor_chrome_afdo.ChromeGsProfile.from_full_name_if_new_enough(
+                last_modified=last_modified,
+                full_name=profile_name,
+            )
+        )
+
+    def test_finding_newest_chrome_version_on_no_stable_ebuilds(self):
+        with self.assertRaisesRegex(ValueError, "^No stable Chrome ebuilds.*"):
+            monitor_chrome_afdo.find_newest_chrome_version(
+                [
+                    "MANIFEST",
+                    "files/",
+                    "chromeos-chrome-9999.ebuild",
+                ]
+            )
+
+    def test_finding_newest_chrome_version_multiple_ebuilds(self):
+        self.assertEqual(
+            monitor_chrome_afdo.find_newest_chrome_version(
+                [
+                    "MANIFEST",
+                    "files/",
+                    "chromeos-chrome-9999.ebuild",
+                    "chromeos-chrome-127.0.6533.0_rc-r1.ebuild",
+                    "chromeos-chrome-126.0.6014.0_rc-r2.ebuild",
+                ]
+            ),
+            "127.0.6533.0",
+        )
+
+    def test_afdo_version_finding_works(self):
+        profile1, profile2, profile3 = increasing_chrome_gs_profile_sequence(3)
+        profiles = {
+            1: [profile1, profile2],
+            2: [profile3],
+        }
+
+        with self.assertRaisesRegex(ValueError, "^No available profile.*"):
+            monitor_chrome_afdo.find_afdo_profile_by_version(
+                profiles, "not a profile version"
+            )
+
+        self.assertIs(
+            monitor_chrome_afdo.find_afdo_profile_by_version(
+                profiles, profile2.full_name()
+            ),
+            profile2,
+        )
+
+    def test_branch_profile_finding_works_in_simple_cases(self):
+        profile1, profile2, profile3 = increasing_chrome_gs_profile_sequence(3)
+        arch = monitor_chrome_afdo.ProfileArch.AMD64
+        subtype = monitor_chrome_afdo.ProfileSubtype.BIGCORE
+        # Assert this since a having `(arch, subtype)` as the arch/subtype of
+        # _all_ profiles will cause nonsense results.
+        self.assertNotEqual((arch, subtype), (profile2.arch, profile2.subtype))
+
+        profile2 = dataclasses.replace(profile2, arch=arch, subtype=subtype)
+
+        profiles = {
+            1: [profile1],
+            2: [profile2, profile3],
+        }
+        result = monitor_chrome_afdo.find_most_recent_branch_profile(
+            afdo_profiles=profiles,
+            arch=arch,
+            subtype=subtype,
+            branch_number=2,
+        )
+        self.assertIs(result, profile2)
+
+        with self.assertRaisesRegex(ValueError, "^Found no branch profiles.*"):
+            monitor_chrome_afdo.find_most_recent_branch_profile(
+                afdo_profiles=profiles,
+                arch=arch,
+                subtype=subtype,
+                branch_number=1,
+            )
+
+    def test_branch_profile_finding_falls_back_to_prior_branches(self):
+        profile1, profile2, profile3 = increasing_chrome_gs_profile_sequence(3)
+        arch = monitor_chrome_afdo.ProfileArch.AMD64
+        subtype = monitor_chrome_afdo.ProfileSubtype.BIGCORE
+        self.assertNotEqual((arch, subtype), (profile1.arch, profile1.subtype))
+
+        profile1 = dataclasses.replace(profile2, arch=arch, subtype=subtype)
+        profiles = {
+            1: [profile1],
+            2: [profile2],
+            3: [profile3],
+        }
+        result = monitor_chrome_afdo.find_most_recent_branch_profile(
+            afdo_profiles=profiles,
+            arch=arch,
+            subtype=subtype,
+            branch_number=3,
+        )
+        self.assertIs(result, profile1)
diff --git a/afdo_tools/run_afdo_tryjob.py b/afdo_tools/run_afdo_tryjob.py
old mode 100755
new mode 100644
index 013e10c6..829464e7
--- a/afdo_tools/run_afdo_tryjob.py
+++ b/afdo_tools/run_afdo_tryjob.py
@@ -1,5 +1,3 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -49,7 +47,6 @@ If you provide neither --use_afdo_generation_stage nor
 since it's safer.
 """
 
-
 import argparse
 import collections
 import pipes
@@ -116,12 +113,13 @@ def main():
         tag_profiles_with_current_time = True
 
     patches = [
-        # Send profiles to localmirror instead of chromeos-prebuilt. This should
-        # always be done, since sending profiles into production is bad. :)
+        # Send profiles to localmirror instead of chromeos-prebuilt. This
+        # should always be done, since sending profiles into production is
+        # bad. :)
         # https://chromium-review.googlesource.com/c/chromiumos/third_party/autotest/+/1436158
         1436158,
-        # Force profile generation. Otherwise, we'll decide to not spawn off the
-        # perf hwtests.
+        # Force profile generation. Otherwise, we'll decide to not spawn off
+        # the perf hwtests.
         # https://chromium-review.googlesource.com/c/chromiumos/chromite/+/1313291
         1313291,
     ]
@@ -134,8 +132,8 @@ def main():
         patches.append(1436157)
 
     if use_afdo_generation_stage:
-        # Make the profile generation stage look in localmirror, instead of having
-        # it look in chromeos-prebuilt. Without this, we'll never upload
+        # Make the profile generation stage look in localmirror, instead of
+        # having it look in chromeos-prebuilt. Without this, we'll never upload
         # chrome.debug or try to generate an AFDO profile.
         # https://chromium-review.googlesource.com/c/chromiumos/chromite/+/1436583
         patches.append(1436583)
@@ -146,7 +144,8 @@ def main():
         )
 
     for patch in user_patches:
-        # We accept two formats. Either a URL that ends with a number, or a number.
+        # We accept two formats. Either a URL that ends with a number, or a
+        # number.
         if patch.startswith("http"):
             patch = patch.split("/")[-1]
         patches.append(int(patch))
@@ -176,7 +175,3 @@ def main():
     print(" ".join(pipes.quote(a) for a in args))
     if not dry_run:
         sys.exit(subprocess.call(args))
-
-
-if __name__ == "__main__":
-    main()
diff --git a/afdo_tools/update_kernel_afdo.cfg b/afdo_tools/update_kernel_afdo.cfg
index a69d6f84..35ca86be 100644
--- a/afdo_tools/update_kernel_afdo.cfg
+++ b/afdo_tools/update_kernel_afdo.cfg
@@ -2,7 +2,7 @@
 # All changes here won't affect kernel afdo update in branches.
 # WARNING: Changes must be submitted to have effect.
 
-AMD_KVERS="5.4 5.10 5.15 6.1"
-ARM_KVERS="5.15"
+AMD_KVERS="5.4 5.10 5.15 6.6"
+ARM_KVERS="6.6"
 AMD_METADATA_FILE="afdo_metadata/kernel_afdo.json"
 ARM_METADATA_FILE="afdo_metadata/kernel_arm_afdo.json"
diff --git a/afdo_tools/update_kernel_afdo.py b/afdo_tools/update_kernel_afdo.py
old mode 100755
new mode 100644
index 0a299bd2..c1bd2b67
--- a/afdo_tools/update_kernel_afdo.py
+++ b/afdo_tools/update_kernel_afdo.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -17,22 +16,17 @@ import logging
 import os
 from pathlib import Path
 import re
-import shlex
 import subprocess
 import sys
 from typing import Dict, Iterable, List, Optional, Tuple
 
+from cros_utils import cros_paths
 from cros_utils import git_utils
+from cros_utils import gs
 
 
-# Folks who should be on the R-line of any CLs that get uploaded.
-CL_REVIEWERS = (git_utils.REVIEWER_DETECTIVE,)
-
 # Folks who should be on the CC-line of any CLs that get uploaded.
-CL_CC = (
-    "denik@google.com",
-    "gbiv@google.com",
-)
+CL_CC = ("gbiv@chromium.org",)
 
 # Determine which gsutil to use.
 # 'gsutil.py' is provided by depot_tools, whereas 'gsutil'
@@ -76,32 +70,21 @@ class KernelVersion:
         return cls(major=int(m.group(1)), minor=int(m.group(2)))
 
 
+ARM_KERNEL_5_15 = (Arch.ARM, KernelVersion(5, 15))
+
 # Versions that rolling should be skipped on, for one reason or another.
 SKIPPED_VERSIONS: Dict[int, Iterable[Tuple[Arch, KernelVersion]]] = {
     # Kernel tracing was disabled on ARM in 114, b/275560674
-    114: ((Arch.ARM, KernelVersion(5, 15)),),
-    115: ((Arch.ARM, KernelVersion(5, 15)),),
+    114: (ARM_KERNEL_5_15,),
+    115: (ARM_KERNEL_5_15,),
+    # Kernel profiles are no longer generated as of M126. Don't complain about
+    # them.
+    124: (ARM_KERNEL_5_15,),
+    125: (ARM_KERNEL_5_15,),
+    126: (ARM_KERNEL_5_15,),
 }
 
 
-class Channel(enum.Enum):
-    """An enum that discusses channels."""
-
-    # Ordered from closest-to-ToT to farthest-from-ToT
-    CANARY = "canary"
-    BETA = "beta"
-    STABLE = "stable"
-
-    @classmethod
-    def parse(cls, val: str) -> "Channel":
-        for x in cls:
-            if val == x.value:
-                return x
-        raise ValueError(
-            f"No such channel: {val!r}; try one of {[x.value for x in cls]}"
-        )
-
-
 @dataclasses.dataclass(frozen=True)
 class ProfileSelectionInfo:
     """Preferences about profiles to select."""
@@ -155,26 +138,17 @@ def get_parser():
     parser.add_argument(
         "channel",
         nargs="*",
-        type=Channel.parse,
-        default=list(Channel),
+        type=git_utils.Channel.parse,
+        default=list(git_utils.Channel),
         help=f"""
         Channel(s) to update. If none are passed, this will update all
-        channels. Choose from {[x.value for x in Channel]}.
+        channels. Choose from {[x.value for x in git_utils.Channel]}.
         """,
     )
     return parser
 
 
-@dataclasses.dataclass(frozen=True, eq=True, order=True)
-class GitBranch:
-    """Represents a ChromeOS branch."""
-
-    remote: str
-    release_number: int
-    branch_name: str
-
-
-def git_checkout(git_dir: Path, branch: GitBranch) -> None:
+def git_checkout(git_dir: Path, branch: git_utils.ChannelBranch) -> None:
     subprocess.run(
         [
             "git",
@@ -197,61 +171,6 @@ def git_fetch(git_dir: Path) -> None:
     )
 
 
-def git_rev_parse(git_dir: Path, ref_or_sha: str) -> str:
-    return subprocess.run(
-        ["git", "rev-parse", ref_or_sha],
-        check=True,
-        cwd=git_dir,
-        stdin=subprocess.DEVNULL,
-        stdout=subprocess.PIPE,
-        encoding="utf-8",
-    ).stdout.strip()
-
-
-def autodetect_branches(toolchain_utils: Path) -> Dict[Channel, GitBranch]:
-    """Returns GitBranches for each branch type in toolchain_utils."""
-    stdout = subprocess.run(
-        [
-            "git",
-            "branch",
-            "-r",
-        ],
-        cwd=toolchain_utils,
-        check=True,
-        stdin=subprocess.DEVNULL,
-        stdout=subprocess.PIPE,
-        encoding="utf-8",
-    ).stdout
-
-    # Match "${remote}/release-R${branch_number}-${build}.B"
-    branch_re = re.compile(r"([^/]+)/(release-R(\d+)-\d+\.B)")
-    branches = []
-    for line in stdout.splitlines():
-        line = line.strip()
-        if m := branch_re.fullmatch(line):
-            remote, branch_name, branch_number = m.groups()
-            branches.append(GitBranch(remote, int(branch_number), branch_name))
-
-    branches.sort(key=lambda x: x.release_number)
-    if len(branches) < 2:
-        raise ValueError(
-            f"Expected at least two branches, but only found {len(branches)}"
-        )
-
-    stable = branches[-2]
-    beta = branches[-1]
-    canary = GitBranch(
-        remote=beta.remote,
-        release_number=beta.release_number + 1,
-        branch_name="main",
-    )
-    return {
-        Channel.CANARY: canary,
-        Channel.BETA: beta,
-        Channel.STABLE: stable,
-    }
-
-
 @dataclasses.dataclass(frozen=True, eq=True, order=True)
 class ArchUpdateConfig:
     """The AFDO update config for one architecture."""
@@ -338,65 +257,30 @@ class KernelGsProfile:
         )
 
 
-def datetime_from_gs_time(timestamp_str: str) -> datetime.datetime:
-    """Parses a datetime from gs."""
-    return datetime.datetime.strptime(
-        timestamp_str, "%Y-%m-%dT%H:%M:%SZ"
-    ).replace(tzinfo=datetime.timezone.utc)
-
-
 class KernelProfileFetcher:
     """Fetches kernel profiles from gs://. Caches results."""
 
     def __init__(self):
         self._cached_results: Dict[str, List[KernelGsProfile]] = {}
 
-    @staticmethod
-    def _parse_gs_stdout(stdout: str) -> List[KernelGsProfile]:
-        line_re = re.compile(r"\s*\d+\s+(\S+T\S+)\s+(gs://.+)")
-        results = []
-        # Ignore the last line, since that's "TOTAL:"
-        for line in stdout.splitlines()[:-1]:
-            line = line.strip()
-            if not line:
-                continue
-            m = line_re.fullmatch(line)
-            if m is None:
-                raise ValueError(f"Unexpected line from gs: {line!r}")
-            timestamp_str, gs_url = m.groups()
-            timestamp = datetime_from_gs_time(timestamp_str)
-            file_name = os.path.basename(gs_url)
-            results.append(KernelGsProfile.from_file_name(timestamp, file_name))
-        return results
-
     @classmethod
     def _fetch_impl(cls, gs_url: str) -> List[KernelGsProfile]:
-        cmd = [
-            GSUTIL,
-            "ls",
-            "-l",
-            gs_url,
-        ]
-        result = subprocess.run(
-            cmd,
-            check=False,
-            stdin=subprocess.DEVNULL,
-            stdout=subprocess.PIPE,
-            stderr=subprocess.PIPE,
-            encoding="utf-8",
-        )
-
-        if result.returncode:
-            # If nothing could be found, gsutil will exit after printing this.
-            if "One or more URLs matched no objects." in result.stderr:
-                return []
-            logging.error(
-                "%s failed; stderr:\n%s", shlex.join(cmd), result.stderr
+        results = []
+        for gs_entry in gs.ls(gs_url):
+            profile_name = os.path.basename(gs_entry.gs_path)
+            # All directories end with `/`, so  their basenames are empty.
+            if not profile_name:
+                continue
+            assert gs_entry.last_modified is not None, (
+                "Non-directory unexpectedly has a None last-modified date: "
+                f"{gs_entry}"
             )
-            result.check_returncode()
-            assert False, "unreachable"
-
-        return cls._parse_gs_stdout(result.stdout)
+            results.append(
+                KernelGsProfile.from_file_name(
+                    gs_entry.last_modified, profile_name
+                )
+            )
+        return results
 
     def fetch(self, gs_url: str) -> List[KernelGsProfile]:
         cached = self._cached_results.get(gs_url)
@@ -526,8 +410,8 @@ def fetch_and_validate_newest_afdo_artifact(
     selection_info: ProfileSelectionInfo,
     arch: Arch,
     kernel_version: KernelVersion,
-    branch: GitBranch,
-    channel: Channel,
+    branch: git_utils.ChannelBranch,
+    channel: git_utils.Channel,
 ) -> Optional[Tuple[str, bool]]:
     """Tries to update one AFDO profile on a branch.
 
@@ -541,7 +425,7 @@ def fetch_and_validate_newest_afdo_artifact(
     )
     # Try an older branch if we're not on stable. We should fail harder if we
     # only have old profiles on stable, though.
-    if newest_artifact is None and channel != Channel.STABLE:
+    if newest_artifact is None and channel != git_utils.Channel.STABLE:
         newest_artifact = find_newest_afdo_artifact(
             fetcher, arch, kernel_version, branch.release_number - 1
         )
@@ -575,12 +459,20 @@ def fetch_and_validate_newest_afdo_artifact(
     return newest_artifact.file_name_no_suffix, is_old
 
 
+def remove_untracked_mappings(
+    descriptors: Dict[KernelVersion, str],
+    versions_to_track: Iterable[KernelVersion],
+) -> Dict[KernelVersion, str]:
+    version_set = set(versions_to_track)
+    return {k: v for k, v in descriptors.items() if k in version_set}
+
+
 def update_afdo_for_channel(
     fetcher: KernelProfileFetcher,
     toolchain_utils: Path,
     selection_info: ProfileSelectionInfo,
-    channel: Channel,
-    branch: GitBranch,
+    channel: git_utils.Channel,
+    branch: git_utils.ChannelBranch,
     skipped_versions: Dict[int, Iterable[Tuple[Arch, KernelVersion]]],
 ) -> UpdateResult:
     """Updates AFDO on the given channel."""
@@ -594,7 +486,10 @@ def update_afdo_for_channel(
     made_changes = False
     had_failures = False
     for arch, cfg in update_cfgs.items():
-        afdo_mappings = read_afdo_descriptor_file(cfg.metadata_file)
+        afdo_mappings = remove_untracked_mappings(
+            read_afdo_descriptor_file(cfg.metadata_file),
+            cfg.versions_to_track,
+        )
         for kernel_version in cfg.versions_to_track:
             if to_skip and (arch, kernel_version) in to_skip:
                 logging.info(
@@ -646,7 +541,7 @@ def update_afdo_for_channel(
 
 
 def commit_new_profiles(
-    toolchain_utils: Path, channel: Channel, had_failures: bool
+    toolchain_utils: Path, channel: git_utils.Channel, had_failures: bool
 ):
     """Runs `git commit -a` with an appropriate message."""
     commit_message_lines = [
@@ -665,7 +560,7 @@ def commit_new_profiles(
             "This brings all profiles to their newest versions."
         )
 
-    if channel != Channel.CANARY:
+    if channel != git_utils.Channel.CANARY:
         commit_message_lines += (
             "",
             "Have PM pre-approval because this shouldn't break the release",
@@ -673,6 +568,10 @@ def commit_new_profiles(
         )
 
     commit_message_lines += (
+        "",
+        "Never rebase this CL! If there's a merge conflict, either abandon",
+        "this CL, or let Chrotomation do so after a few days. Rebasing could",
+        "cause a performance regression.",
         "",
         "BUG=None",
         "TEST=Verified in kernel-release-afdo-verify-orchestrator",
@@ -697,15 +596,14 @@ def commit_new_profiles(
 def upload_head_to_gerrit(
     toolchain_utils: Path,
     chromeos_tree: Optional[Path],
-    branch: GitBranch,
+    branch: git_utils.ChannelBranch,
 ):
     """Uploads HEAD to gerrit as a CL, and sets reviewers/CCs."""
     cl_ids = git_utils.upload_to_gerrit(
         toolchain_utils,
         branch.remote,
         branch.branch_name,
-        CL_REVIEWERS,
-        CL_CC,
+        cc=CL_CC,
     )
 
     if len(cl_ids) > 1:
@@ -721,19 +619,11 @@ def upload_head_to_gerrit(
         )
         return
 
-    git_utils.try_set_autosubmit_labels(chromeos_tree, cl_id)
-
-
-def find_chromeos_tree_root(a_dir: Path) -> Optional[Path]:
-    for parent in a_dir.parents:
-        if (parent / ".repo").is_dir():
-            return parent
-    return None
+    git_utils.set_autoreview_topic_and_labels(chromeos_tree, cl_id)
 
 
 def main(argv: List[str]) -> None:
-    my_dir = Path(__file__).resolve().parent
-    toolchain_utils = my_dir.parent
+    toolchain_utils = cros_paths.script_toolchain_utils_root()
 
     opts = get_parser().parse_args(argv)
     logging.basicConfig(
@@ -744,7 +634,7 @@ def main(argv: List[str]) -> None:
 
     chromeos_tree = opts.chromeos_tree
     if not chromeos_tree:
-        chromeos_tree = find_chromeos_tree_root(my_dir)
+        chromeos_tree = cros_paths.script_chromiumos_checkout()
         if chromeos_tree:
             logging.info("Autodetected ChromeOS tree root at %s", chromeos_tree)
 
@@ -757,10 +647,12 @@ def main(argv: List[str]) -> None:
         max_profile_age=datetime.timedelta(days=opts.max_age_days),
     )
 
-    branches = autodetect_branches(toolchain_utils)
+    branches = git_utils.autodetect_cros_channels(toolchain_utils)
     logging.debug("Current branches: %s", branches)
 
-    assert all(x in branches for x in Channel), "branches are missing channels?"
+    assert all(
+        x in branches for x in git_utils.Channel
+    ), "branches are missing channels?"
 
     fetcher = KernelProfileFetcher()
     had_failures = False
@@ -788,7 +680,7 @@ def main(argv: List[str]) -> None:
                 logging.info(
                     "--upload not specified. Leaving commit for %s at %s",
                     channel,
-                    git_rev_parse(worktree, "HEAD"),
+                    git_utils.resolve_ref(worktree, "HEAD"),
                 )
 
     if had_failures:
@@ -797,7 +689,3 @@ def main(argv: List[str]) -> None:
             "above logs. Most likely the things you're looking for are logged "
             "at the ERROR level."
         )
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/afdo_tools/update_kernel_afdo_test.py b/afdo_tools/update_kernel_afdo_test.py
old mode 100755
new mode 100644
index 1f365959..02ee6c34
--- a/afdo_tools/update_kernel_afdo_test.py
+++ b/afdo_tools/update_kernel_afdo_test.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -14,7 +13,7 @@ import textwrap
 import unittest
 from unittest import mock
 
-import update_kernel_afdo
+from afdo_tools import update_kernel_afdo
 
 
 class Test(unittest.TestCase):
@@ -39,58 +38,6 @@ class Test(unittest.TestCase):
             str(update_kernel_afdo.KernelVersion(major=5, minor=10)), "5.10"
         )
 
-    def test_channel_parsing(self):
-        with self.assertRaisesRegex(ValueError, "No such channel.*"):
-            update_kernel_afdo.Channel.parse("not a channel")
-
-        # Ensure these round-trip.
-        for channel in update_kernel_afdo.Channel:
-            self.assertEqual(
-                channel, update_kernel_afdo.Channel.parse(channel.value)
-            )
-
-    @mock.patch.object(subprocess, "run")
-    def test_branch_autodetection(self, subprocess_run):
-        subprocess_run.return_value = subprocess.CompletedProcess(
-            args=[],
-            returncode=0,
-            stdout=textwrap.dedent(
-                """
-                cros/not-a-release-branch
-                cros/release-R121-15699.B
-                cros/release-R122-15753.B
-                cros/release-R123-15786.B
-                cros/also-not-a-release-branch
-                m/main
-                """
-            ),
-        )
-
-        branch_dict = update_kernel_afdo.autodetect_branches(
-            toolchain_utils=self.make_tempdir()
-        )
-
-        self.assertEqual(
-            branch_dict,
-            {
-                update_kernel_afdo.Channel.CANARY: update_kernel_afdo.GitBranch(
-                    remote="cros",
-                    release_number=124,
-                    branch_name="main",
-                ),
-                update_kernel_afdo.Channel.BETA: update_kernel_afdo.GitBranch(
-                    remote="cros",
-                    release_number=123,
-                    branch_name="release-R123-15786.B",
-                ),
-                update_kernel_afdo.Channel.STABLE: update_kernel_afdo.GitBranch(
-                    remote="cros",
-                    release_number=122,
-                    branch_name="release-R122-15753.B",
-                ),
-            },
-        )
-
     def test_read_update_cfg_file(self):
         valid_contents = textwrap.dedent(
             """
@@ -132,7 +79,7 @@ class Test(unittest.TestCase):
         timestamp = datetime.datetime.fromtimestamp(1234, datetime.timezone.utc)
         profile = update_kernel_afdo.KernelGsProfile.from_file_name(
             timestamp,
-            "R124-15808.0-1710149961.gcov.xz",
+            "R124-15808.0-1710149961.afdo.xz",
         )
         self.assertEqual(
             profile,
@@ -140,7 +87,7 @@ class Test(unittest.TestCase):
                 release_number=124,
                 chrome_build="15808.0",
                 cwp_timestamp=1710149961,
-                suffix=".gcov.xz",
+                suffix=".afdo.xz",
                 gs_timestamp=timestamp,
             ),
         )
@@ -149,63 +96,24 @@ class Test(unittest.TestCase):
         timestamp = datetime.datetime.fromtimestamp(1234, datetime.timezone.utc)
         profile = update_kernel_afdo.KernelGsProfile.from_file_name(
             timestamp,
-            "R124-15808.0-1710149961.gcov.xz",
+            "R124-15808.0-1710149961.afdo.xz",
         )
         self.assertEqual(profile.file_name_no_suffix, "R124-15808.0-1710149961")
-        self.assertEqual(profile.file_name, "R124-15808.0-1710149961.gcov.xz")
+        self.assertEqual(profile.file_name, "R124-15808.0-1710149961.afdo.xz")
 
-    def test_gs_time_parsing(self):
-        self.assertEqual(
-            update_kernel_afdo.datetime_from_gs_time("2024-03-04T10:38:50Z"),
-            datetime.datetime(
-                year=2024,
-                month=3,
-                day=4,
-                hour=10,
-                minute=38,
-                second=50,
-                tzinfo=datetime.timezone.utc,
-            ),
-        )
+    def test_untracked_version_removal(self):
+        kernel_5_10 = update_kernel_afdo.KernelVersion(5, 10)
+        kernel_5_15 = update_kernel_afdo.KernelVersion(5, 15)
+        kernel_6_1 = update_kernel_afdo.KernelVersion(6, 1)
 
-    @mock.patch.object(subprocess, "run")
-    def test_kernel_profile_fetcher_works(self, subprocess_run):
-        subprocess_run.return_value = subprocess.CompletedProcess(
-            args=[],
-            returncode=0,
-            # Don't use textwrap.dedent; linter complains about the line being
-            # too long in that case.
-            stdout="""
-753112  2024-03-04T10:38:50Z gs://here/5.4/R124-15786.10-1709548729.gcov.xz
-TOTAL: 2 objects, 1234 bytes (1.1KiB)
-""",
-        )
-
-        fetcher = update_kernel_afdo.KernelProfileFetcher()
-        results = fetcher.fetch("gs://here/5.4")
-
-        expected_results = [
-            update_kernel_afdo.KernelGsProfile.from_file_name(
-                update_kernel_afdo.datetime_from_gs_time(
-                    "2024-03-04T10:38:50Z"
-                ),
-                "R124-15786.10-1709548729.gcov.xz",
+        self.assertEqual(
+            update_kernel_afdo.remove_untracked_mappings(
+                descriptors={kernel_5_10: "foo", kernel_5_15: "bar"},
+                versions_to_track=(kernel_5_15, kernel_6_1),
             ),
-        ]
-        self.assertEqual(results, expected_results)
-
-    @mock.patch.object(subprocess, "run")
-    def test_kernel_profile_fetcher_handles_no_profiles(self, subprocess_run):
-        subprocess_run.return_value = subprocess.CompletedProcess(
-            args=[],
-            returncode=1,
-            stderr="\nCommandException: One or more URLs matched no objects.\n",
+            {kernel_5_15: "bar"},
         )
 
-        fetcher = update_kernel_afdo.KernelProfileFetcher()
-        results = fetcher.fetch("gs://here/5.4")
-        self.assertEqual(results, [])
-
     @mock.patch.object(subprocess, "run")
     def test_kernel_profile_fetcher_caches_urls(self, subprocess_run):
         subprocess_run.return_value = subprocess.CompletedProcess(
@@ -214,7 +122,7 @@ TOTAL: 2 objects, 1234 bytes (1.1KiB)
             # Don't use textwrap.dedent; linter complains about the line being
             # too long in that case.
             stdout="""
-753112  2024-03-04T10:38:50Z gs://here/5.4/R124-15786.10-1709548729.gcov.xz
+753112  2024-03-04T10:38:50Z gs://here/5.4/R124-15786.10-1709548729.afdo.xz
 TOTAL: 2 objects, 1234 bytes (1.1KiB)
 """,
         )
@@ -234,11 +142,11 @@ TOTAL: 2 objects, 1234 bytes (1.1KiB)
     def test_newest_afdo_artifact_finding_works(self, fetch):
         late = update_kernel_afdo.KernelGsProfile.from_file_name(
             datetime.datetime.fromtimestamp(1236, datetime.timezone.utc),
-            "R124-15786.10-1709548729.gcov.xz",
+            "R124-15786.10-1709548729.afdo.xz",
         )
         early = update_kernel_afdo.KernelGsProfile.from_file_name(
             datetime.datetime.fromtimestamp(1234, datetime.timezone.utc),
-            "R124-99999.99-9999999999.gcov.xz",
+            "R124-99999.99-9999999999.afdo.xz",
         )
         fetch.return_value = [early, late]
 
@@ -282,23 +190,3 @@ TOTAL: 2 objects, 1234 bytes (1.1KiB)
         self.assertFalse(
             update_kernel_afdo.write_afdo_descriptor_file(file_path, contents)
         )
-
-    def test_repo_autodetects_nothing_if_no_repo_dir(self):
-        self.assertIsNone(
-            update_kernel_afdo.find_chromeos_tree_root(
-                Path("/does/not/exist/nor/is/under/a/repo")
-            )
-        )
-
-    def test_repo_autodetects_repo_dir_correctly(self):
-        tmpdir = self.make_tempdir()
-        test_subdir = tmpdir / "a/directory/and/another/one"
-        test_subdir.mkdir(parents=True)
-        (tmpdir / ".repo").mkdir()
-        self.assertEqual(
-            tmpdir, update_kernel_afdo.find_chromeos_tree_root(test_subdir)
-        )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/android_merge_from_upstream.sh b/android_merge_from_upstream.sh
index a391979d..15c6e044 100755
--- a/android_merge_from_upstream.sh
+++ b/android_merge_from_upstream.sh
@@ -11,8 +11,8 @@
 # https://android-review.googlesource.com/c/platform/external/toolchain-utils/+/1132504/1
 
 local_branch_name="merge_with_upstream"
-local_upstream="aosp/master"  # nocheck
-remote="aosp"
+local_upstream="goog/main"  # nocheck
+remote="goog"
 remote_branch="${remote}/upstream-main"  # nocheck
 
 my_dir="$(dirname "$(readlink -m "$0")")"
diff --git a/auto_abandon_cls.py b/auto_abandon_cls.py
old mode 100755
new mode 100644
index ae78bfa5..ec1af8bc
--- a/auto_abandon_cls.py
+++ b/auto_abandon_cls.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -10,12 +9,23 @@ Note that this needs to be run from inside a ChromeOS tree. Otherwise, the
 """
 
 import argparse
+import enum
 import logging
 import subprocess
-import sys
 from typing import List
 
 
+class GerritSearchType(enum.Enum):
+    """Specifies the kind of gerrit search for `enumerate_old_cls`."""
+
+    EXTERNAL_NO_LLVM = enum.auto()
+    LLVM_ONLY = enum.auto()
+    INTERNAL_ONLY = enum.auto()
+
+    def is_internal(self):
+        return self is self.INTERNAL_ONLY
+
+
 def gerrit_cmd(internal: bool) -> List[str]:
     cmd = ["gerrit"]
     if internal:
@@ -23,11 +33,24 @@ def gerrit_cmd(internal: bool) -> List[str]:
     return cmd
 
 
-def enumerate_old_cls(old_days: int, internal: bool) -> List[int]:
+def enumerate_old_cls(
+    old_days: int, search_type: GerritSearchType
+) -> List[int]:
     """Returns CL numbers that haven't been updated in `old_days` days."""
+    search_string = f"owner:me status:open age:{old_days}d"
+    llvm_repo = "project:external/github.com/llvm/llvm-project"
+    if search_type is GerritSearchType.EXTERNAL_NO_LLVM:
+        search_string += f" -{llvm_repo}"
+    elif search_type is GerritSearchType.LLVM_ONLY:
+        search_string += f" {llvm_repo}"
+    else:
+        assert (
+            search_type is GerritSearchType.INTERNAL_ONLY
+        ), f"Unhandled search type: {search_type}"
+
+    is_internal = search_type.is_internal()
     stdout = subprocess.run(
-        gerrit_cmd(internal)
-        + ["--raw", "search", f"owner:me status:open age:{old_days}d"],
+        gerrit_cmd(is_internal) + ["--raw", "search", search_string],
         check=True,
         stdin=subprocess.DEVNULL,
         stdout=subprocess.PIPE,
@@ -36,7 +59,7 @@ def enumerate_old_cls(old_days: int, internal: bool) -> List[int]:
     # Sort for prettier output; it's unclear if Gerrit always sorts, and it's
     # cheap.
     lines = stdout.splitlines()
-    if internal:
+    if is_internal:
         # These are printed as `chrome-internal:NNNN`, rather than `NNNN`.
         chrome_internal_prefix = "chrome-internal:"
         assert all(x.startswith(chrome_internal_prefix) for x in lines), lines
@@ -53,14 +76,17 @@ def abandon_cls(cls: List[int], internal: bool) -> None:
 
 
 def detect_and_abandon_cls(
-    old_days: int, dry_run: bool, internal: bool
+    old_days: int,
+    dry_run: bool,
+    search_type: GerritSearchType,
 ) -> None:
-    old_cls = enumerate_old_cls(old_days, internal)
+    old_cls = enumerate_old_cls(old_days, search_type)
     if not old_cls:
         logging.info("No CLs less than %d days old found; quit", old_days)
         return
 
-    cl_namespace = "i" if internal else "c"
+    is_internal = search_type.is_internal()
+    cl_namespace = "i" if is_internal else "c"
     logging.info(
         "Abandoning CLs: %s", [f"crrev.com/{cl_namespace}/{x}" for x in old_cls]
     )
@@ -68,7 +94,7 @@ def detect_and_abandon_cls(
         logging.info("--dry-run specified; skip the actual abandon part")
         return
 
-    abandon_cls(old_cls, internal)
+    abandon_cls(old_cls, is_internal)
 
 
 def main(argv: List[str]) -> None:
@@ -91,6 +117,15 @@ def main(argv: List[str]) -> None:
         'old'.
         """,
     )
+    parser.add_argument(
+        "--old-days-llvm",
+        default=60,
+        type=int,
+        help="""
+        How many days a CL needs to go without modification to be considered
+        'old', specifically for CLs to ChromeOS' LLVM project.
+        """,
+    )
     parser.add_argument(
         "--dry-run",
         action="store_true",
@@ -98,19 +133,21 @@ def main(argv: List[str]) -> None:
     )
     opts = parser.parse_args(argv)
 
-    logging.info("Checking for external CLs...")
+    logging.info("Checking for external, non-LLVM CLs...")
     detect_and_abandon_cls(
         old_days=opts.old_days,
         dry_run=opts.dry_run,
-        internal=False,
+        search_type=GerritSearchType.EXTERNAL_NO_LLVM,
+    )
+    logging.info("Checking for external LLVM CLs...")
+    detect_and_abandon_cls(
+        old_days=opts.old_days_llvm,
+        dry_run=opts.dry_run,
+        search_type=GerritSearchType.LLVM_ONLY,
     )
     logging.info("Checking for internal CLs...")
     detect_and_abandon_cls(
         old_days=opts.old_days,
         dry_run=opts.dry_run,
-        internal=True,
+        search_type=GerritSearchType.INTERNAL_ONLY,
     )
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/bestflags/README.md b/bestflags/README.md
deleted file mode 100644
index c9f4397d..00000000
--- a/bestflags/README.md
+++ /dev/null
@@ -1,23 +0,0 @@
-# bestflags
-
-There is a vast set of compiler flags that can be used to build Chrome for
-ChromeOS. This option space has not been explored before. This directory
-provides an infrastructure to build Chrome with certain flag combinations, test
-it, gather results and prepare a fresh batch of flags to repeat the process. The
-infrastructure supports plug-in modules that implement algorithms for searching
-in the N-Dimensional space of compiler flag combinations.
-
-Currently, three different algorithms are built, namely genetic algorithm, hill
-climbing and negative flag iterative elimination. The module `testing_batch.py`
-contains the testing of these algorithms.
-
-To run the script, type in `python testing_batch.py`.
-
-For further information about the project, please refer to the design document
-at:
-
-https://docs.google.com/a/google.com/document/d/19iE9rhszTWjISBpKJ3qK8uBCoUjs0o4etWDRkyEeUOw/
-
-There is also a presentation slide available at:
-
-https://docs.google.com/a/google.com/presentation/d/13rS9jALXffbP48YsF0-bsqovrVBfgzEud4e-XpavOdA/edit#slide=id.gf880fcd4_180
diff --git a/bestflags/example_algorithms.py b/bestflags/example_algorithms.py
deleted file mode 100644
index c39b2943..00000000
--- a/bestflags/example_algorithms.py
+++ /dev/null
@@ -1,216 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""An example main file running the algorithms.
-
-Part of the Chrome build flags optimization.
-
-An example use of the framework. It parses the input json configuration file.
-Then it initiates the variables of the generation. Finally, it sets up the
-processes for different modules and runs the experiment.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import json
-import multiprocessing
-from optparse import OptionParser
-import sys
-
-import flags
-from genetic_algorithm import GAGeneration
-from pipeline_process import PipelineProcess
-import pipeline_worker
-from steering import Steering
-from task import BUILD_STAGE
-from task import Task
-from task import TEST_STAGE
-import testing_batch
-
-
-parser = OptionParser()
-
-parser.add_option(
-    "-f",
-    "--file",
-    dest="filename",
-    help="configuration file FILE input",
-    metavar="FILE",
-)
-
-# The meta data for the genetic algorithm.
-BUILD_CMD = "BUILD_CMD"
-TEST_CMD = "TEST_CMD"
-OUTPUT = "OUTPUT"
-DEFAULT_OUTPUT = "output"
-CONF = "CONF"
-DEFAULT_CONF = "conf"
-NUM_BUILDER = "NUM_BUILDER"
-DEFAULT_NUM_BUILDER = 1
-NUM_TESTER = "NUM_TESTER"
-DEFAULT_NUM_TESTER = 1
-STOP_THRESHOLD = "STOP_THRESHOLD"
-DEFAULT_STOP_THRESHOLD = 1
-NUM_CHROMOSOMES = "NUM_CHROMOSOMES"
-DEFAULT_NUM_CHROMOSOMES = 20
-NUM_TRIALS = "NUM_TRIALS"
-DEFAULT_NUM_TRIALS = 20
-MUTATION_RATE = "MUTATION_RATE"
-DEFAULT_MUTATION_RATE = 0.01
-
-
-def _ProcessGA(meta_data):
-    """Set up the meta data for the genetic algorithm.
-
-    Args:
-      meta_data: the meta data for the genetic algorithm.
-    """
-    assert BUILD_CMD in meta_data
-    build_cmd = meta_data[BUILD_CMD]
-
-    assert TEST_CMD in meta_data
-    test_cmd = meta_data[TEST_CMD]
-
-    if OUTPUT not in meta_data:
-        output_file = DEFAULT_OUTPUT
-    else:
-        output_file = meta_data[OUTPUT]
-
-    if CONF not in meta_data:
-        conf_file = DEFAULT_CONF
-    else:
-        conf_file = meta_data[CONF]
-
-    if NUM_BUILDER not in meta_data:
-        num_builders = DEFAULT_NUM_BUILDER
-    else:
-        num_builders = meta_data[NUM_BUILDER]
-
-    if NUM_TESTER not in meta_data:
-        num_testers = DEFAULT_NUM_TESTER
-    else:
-        num_testers = meta_data[NUM_TESTER]
-
-    if STOP_THRESHOLD not in meta_data:
-        stop_threshold = DEFAULT_STOP_THRESHOLD
-    else:
-        stop_threshold = meta_data[STOP_THRESHOLD]
-
-    if NUM_CHROMOSOMES not in meta_data:
-        num_chromosomes = DEFAULT_NUM_CHROMOSOMES
-    else:
-        num_chromosomes = meta_data[NUM_CHROMOSOMES]
-
-    if NUM_TRIALS not in meta_data:
-        num_trials = DEFAULT_NUM_TRIALS
-    else:
-        num_trials = meta_data[NUM_TRIALS]
-
-    if MUTATION_RATE not in meta_data:
-        mutation_rate = DEFAULT_MUTATION_RATE
-    else:
-        mutation_rate = meta_data[MUTATION_RATE]
-
-    specs = flags.ReadConf(conf_file)
-
-    # Initiate the build/test command and the log directory.
-    Task.InitLogCommand(build_cmd, test_cmd, output_file)
-
-    # Initiate the build/test command and the log directory.
-    GAGeneration.InitMetaData(
-        stop_threshold, num_chromosomes, num_trials, specs, mutation_rate
-    )
-
-    # Generate the initial generations.
-    generation_tasks = testing_batch.GenerateRandomGATasks(
-        specs, num_chromosomes, num_trials
-    )
-    generations = [GAGeneration(generation_tasks, set([]), 0)]
-
-    # Execute the experiment.
-    _StartExperiment(num_builders, num_testers, generations)
-
-
-def _ParseJson(file_name):
-    """Parse the input json file.
-
-    Parse the input json file and call the proper function to perform the
-    algorithms.
-
-    Args:
-      file_name: the input json file name.
-    """
-
-    experiments = json.load(open(file_name))
-
-    for experiment in experiments:
-        if experiment == "GA":
-            # An GA experiment
-            _ProcessGA(experiments[experiment])
-
-
-def _StartExperiment(num_builders, num_testers, generations):
-    """Set up the experiment environment and execute the framework.
-
-    Args:
-      num_builders: number of concurrent builders.
-      num_testers: number of concurrent testers.
-      generations: the initial generation for the framework.
-    """
-
-    manager = multiprocessing.Manager()
-
-    # The queue between the steering algorithm and the builder.
-    steering_build = manager.Queue()
-    # The queue between the builder and the tester.
-    build_test = manager.Queue()
-    # The queue between the tester and the steering algorithm.
-    test_steering = manager.Queue()
-
-    # Set up the processes for the builder, tester and steering algorithm module.
-    build_process = PipelineProcess(
-        num_builders,
-        "builder",
-        {},
-        BUILD_STAGE,
-        steering_build,
-        pipeline_worker.Helper,
-        pipeline_worker.Worker,
-        build_test,
-    )
-
-    test_process = PipelineProcess(
-        num_testers,
-        "tester",
-        {},
-        TEST_STAGE,
-        build_test,
-        pipeline_worker.Helper,
-        pipeline_worker.Worker,
-        test_steering,
-    )
-
-    steer_process = multiprocessing.Process(
-        target=Steering,
-        args=(set([]), generations, test_steering, steering_build),
-    )
-
-    # Start the processes.
-    build_process.start()
-    test_process.start()
-    steer_process.start()
-
-    # Wait for the processes to finish.
-    build_process.join()
-    test_process.join()
-    steer_process.join()
-
-
-def main(argv):
-    (options, _) = parser.parse_args(argv)
-    assert options.filename
-    _ParseJson(options.filename)
-
-
-if __name__ == "__main__":
-    main(sys.argv)
diff --git a/bestflags/examples/omnetpp/README.md b/bestflags/examples/omnetpp/README.md
deleted file mode 100644
index b4582d0d..00000000
--- a/bestflags/examples/omnetpp/README.md
+++ /dev/null
@@ -1,34 +0,0 @@
-# `omnetpp`
-
-This directory contains the omnetpp example in SPEC2006 benchmark.
-
-It also contains the json configuration file which includes the meta data
-information to run the experiment.
-
-This directory contains a build file `build_omnetpp` which is used by the build
-module of the framework to compile the application.
-This directory contains a test file `test_omnetpp` which is used by the test
-module of the framework to benchmark the optimization compilation.
-This directory contains a conf file which includes the set of optimization flags
-the experiment will try.
-
-To use this direction, first gives the file the executable permission.
-
-```
-chmod a+x build_bikjmp
-chmod a+x test_bikjmp
-```
-
-Copy the SPEC2006 benchmark into this directory.
-
-To run, invoke the `example_algorithm.py` in the parent directory.
-
-```
-python example_algorithms.py --file=examples/omnetpp/example.json
-```
-
-For help,
-
-```
-python example_algorithms.py --help
-```
diff --git a/bestflags/examples/omnetpp/build_omnetpp b/bestflags/examples/omnetpp/build_omnetpp
deleted file mode 100755
index 35e9ec13..00000000
--- a/bestflags/examples/omnetpp/build_omnetpp
+++ /dev/null
@@ -1,69 +0,0 @@
-#!/bin/bash -x
-
-cd examples/omnetpp/cpu2006-redhat-ia32
-
-# Contains the optimization flags.
-flags=''
-
-# The index of the parameter.
-i=1
-
-# Indicate whether it is parsing the gcc param.
-in_gcc_param=false
-
-for parameter in "$@"
-  do
-    #  The last parameter is the file name.
-    if [ "$i" == "$#" ]; then
-      file=$parameter
-      break
-    fi
-
-    # The param is not a flag, it combines with the flag that comes right after.
-    # For example, --param max-inline-insns-single 
-    if [ "$parameter" == "-param" ]; then
-      in_gcc_param=true
-      flags+=-$parameter' '
-      let i++
-      continue
-    fi
-
-    # In in_gcc_param section, this flag follows the key word '--param'.
-    if [ $in_gcc_param == true ]; then
-      flags+=$parameter' '
-      let i++
-      in_gcc_param=false
-      continue
-    fi
-
-    # Normal flags.
-    flags+=-$parameter' '
-    let i++
-  done
-
-# Change the configuration file.
-content=$(sed s/amd64-m64-gcc41-kk/test$file/ config/linux64-amd64-pgi.cfg)
-echo "$content" | sed s/-O2/-O1\ "$flags"/ >config/linux64-amd64-pgi$file.cfg
-. ./shrc
-/usr/bin/time -o temp$file runspec --config linux64-amd64-pgi$file -D --action=build  471.omnetpp
-
-state=$?
-
-outfile="./benchspec/CPU2006/471.omnetpp/run/build_base_test$file.0000/omnetpp"
-
-if [ $state -eq 0 ];then
-  user_time=$(cat build_timer$file | grep "user" | cut -d "u" -f 1)
-  output_file="$file"
-
-  checksum=$(readelf -x .text $outfile | md5sum | cut -d " " -f 1)
-  file_size=$(ls -l $outfile | cut -d " " -f 5)
-  text_section=$(readelf -SW $outfile | grep ".text")
-  size_hex=$(echo $text_section | sed "s/\s\{1,\}/\ /g" | cut -d ' ' -f 6)
-  size=$(echo $size_hex | ( echo "ibase=16" ; tr '[:lower:]' '[:upper:]') | bc)
-
-  echo $checksum $user_time $output_file $file_size $size
-else
-  echo "error" "error" "error" "error" "error"
-fi
-
-return $state
\ No newline at end of file
diff --git a/bestflags/examples/omnetpp/conf b/bestflags/examples/omnetpp/conf
deleted file mode 100644
index c7059e12..00000000
--- a/bestflags/examples/omnetpp/conf
+++ /dev/null
@@ -1,2 +0,0 @@
-fgcse-after-reload
-ftree-vectorize
diff --git a/bestflags/examples/omnetpp/example.json b/bestflags/examples/omnetpp/example.json
deleted file mode 100644
index dda6dd11..00000000
--- a/bestflags/examples/omnetpp/example.json
+++ /dev/null
@@ -1,24 +0,0 @@
-{
-"GA":{
-
-"BUILD_CMD": "./examples/omnetpp/build_omnetpp",
-
-"TEST_CMD": "./examples/omnetpp/test_omnetpp",
-
-"OUTPUT": "output",
-
-"CONF": "examples/omnetpp/conf",
-
-"NUM_BUILDER": 3,
-
-"NUM_TESTER": 2,
-
-"STOP_THRESHOLD": 20,
-
-"NUM_CHROMOSOMES": 10,
-
-"NUM_TRIALS": 20,
-
-"MUTATION_RATE": 0.03
-}
-}
\ No newline at end of file
diff --git a/bestflags/examples/omnetpp/test_omnetpp b/bestflags/examples/omnetpp/test_omnetpp
deleted file mode 100755
index aeedb634..00000000
--- a/bestflags/examples/omnetpp/test_omnetpp
+++ /dev/null
@@ -1,24 +0,0 @@
-#!/bin/bash -ux
-
-cd /usr/local/google/home/yuhenglong/Desktop/spec2006/cpu2006-redhat-ia32/
-cd benchspec/CPU2006/471.omnetpp/run/build_base_test$1.0000
-
-(time ./omnetpp$1 ../../data/train/input/omnetpp.ini) 1>log-file 2>time.txt
-
-state=$?
-
-if [ $state -eq 0 ];then
-  diff ../../data/train/output/omnetpp.sca.result omnetpp.sca
-  state=$?
-  if [ $state -eq 0 ];then
-    time=$(cat time.txt | grep real | cut -f2 -s | cut -d 's' -f 1)
-    time=$(echo $time | awk -Fm '{ print ($1 * 60) + $2 }')
-    echo $time
-  else
-    echo "error"
-  fi
-else
-  echo "error"
-fi
-
-return $state
\ No newline at end of file
diff --git a/bestflags/flags.py b/bestflags/flags.py
deleted file mode 100644
index b1b79999..00000000
--- a/bestflags/flags.py
+++ /dev/null
@@ -1,202 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""Manage bundles of flags used for the optimizing of ChromeOS.
-
-Part of the Chrome build flags optimization.
-
-The content of this module is adapted from the Trakhelp JVM project. This module
-contains the basic Class Flag and the Class FlagSet. The core abstractions are:
-
-The class Flag, which takes a domain specific language describing how to fill
-the flags with values.
-
-The class FlagSet, which contains a number of flags and can create new FlagSets
-by mixing with other FlagSets.
-
-The Flag DSL works by replacing value ranges in [x-y] with numbers in the range
-x through y.
-
-Examples:
-  "foo[0-9]bar" will expand to e.g. "foo5bar".
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import random
-import re
-
-
-#
-# This matches a [...] group in the internal representation for a flag
-# specification, and is used in "filling out" flags - placing values inside
-# the flag_spec.  The internal flag_spec format is like "foo[0]", with
-# values filled out like 5; this would be transformed by
-# FormattedForUse() into "foo5".
-_FLAG_FILLOUT_VALUE_RE = re.compile(r"\[([^\]]*)\]")
-
-# This matches a numeric flag flag=[start-end].
-rx = re.compile(r"\[(?P<start>\d+)-(?P<end>\d+)\]")
-
-
-# Search the numeric flag pattern.
-def Search(spec):
-    return rx.search(spec)
-
-
-class NoSuchFileError(Exception):
-    """Define an Exception class for user providing invalid input file."""
-
-    pass
-
-
-def ReadConf(file_name):
-    """Parse the configuration file.
-
-    The configuration contains one flag specification in each line.
-
-    Args:
-      file_name: The name of the configuration file.
-
-    Returns:
-      A list of specs in the configuration file.
-
-    Raises:
-      NoSuchFileError: The caller should provide a valid configuration file.
-    """
-
-    with open(file_name, "r") as input_file:
-        lines = input_file.readlines()
-
-        return sorted([line.strip() for line in lines if line.strip()])
-
-    raise NoSuchFileError()
-
-
-class Flag(object):
-    """A class representing a particular command line flag argument.
-
-    The Flag consists of two parts: The spec and the value.
-    The spec is a definition of the following form: a string with escaped
-    sequences of the form [<start>-<end>] where start and end is an positive
-    integer for a fillable value.
-
-    An example of a spec is "foo[0-9]".
-    There are two kinds of flags, boolean flag and numeric flags. Boolean flags
-    can either be turned on or off, which numeric flags can have different
-    positive integer values. For example, -finline-limit=[1-1000] is a numeric
-    flag and -ftree-vectorize is a boolean flag.
-
-    A (boolean/numeric) flag is not turned on if it is not selected in the
-    FlagSet.
-    """
-
-    def __init__(self, spec, value=-1):
-        self._spec = spec
-
-        # If the value is not specified, generate a random value to use.
-        if value == -1:
-            # If creating a boolean flag, the value will be 0.
-            value = 0
-
-            # Parse the spec's expression for the flag value's numeric range.
-            numeric_flag_match = Search(spec)
-
-            # If this is a numeric flag, a value is chosen within start and end, start
-            # inclusive and end exclusive.
-            if numeric_flag_match:
-                start = int(numeric_flag_match.group("start"))
-                end = int(numeric_flag_match.group("end"))
-
-                assert start < end
-                value = random.randint(start, end)
-
-        self._value = value
-
-    def __eq__(self, other):
-        if isinstance(other, Flag):
-            return (
-                self._spec == other.GetSpec()
-                and self._value == other.GetValue()
-            )
-        return False
-
-    def __hash__(self):
-        return hash(self._spec) + self._value
-
-    def GetValue(self):
-        """Get the value for this flag.
-
-        Returns:
-         The value.
-        """
-
-        return self._value
-
-    def GetSpec(self):
-        """Get the spec for this flag.
-
-        Returns:
-         The spec.
-        """
-
-        return self._spec
-
-    def FormattedForUse(self):
-        """Calculate the combination of flag_spec and values.
-
-        For e.g. the flag_spec 'foo[0-9]' and the value equals to 5, this will
-        return 'foo5'. The filled out version of the flag is the text string you use
-        when you actually want to pass the flag to some binary.
-
-        Returns:
-          A string that represent the filled out flag, e.g. the flag with the
-          FlagSpec '-X[0-9]Y' and value equals to 5 would return '-X5Y'.
-        """
-
-        return _FLAG_FILLOUT_VALUE_RE.sub(str(self._value), self._spec)
-
-
-class FlagSet(object):
-    """A dictionary of Flag objects.
-
-    The flags dictionary stores the spec and flag pair.
-    """
-
-    def __init__(self, flag_array):
-        # Store the flags as a dictionary mapping of spec -> flag object
-        self._flags = dict([(flag.GetSpec(), flag) for flag in flag_array])
-
-    def __eq__(self, other):
-        return isinstance(other, FlagSet) and self._flags == other.GetFlags()
-
-    def __hash__(self):
-        return sum([hash(flag) for flag in self._flags.values()])
-
-    def __getitem__(self, flag_spec):
-        """Get flag with a particular flag_spec.
-
-        Args:
-          flag_spec: The flag_spec to find.
-
-        Returns:
-          A flag.
-        """
-
-        return self._flags[flag_spec]
-
-    def __contains__(self, flag_spec):
-        return self._flags.has_key(flag_spec)
-
-    def GetFlags(self):
-        return self._flags
-
-    def FormattedForUse(self):
-        """Format this for use in an application.
-
-        Returns:
-          A list of flags, sorted alphabetically and filled in with the values
-          for each flag.
-        """
-
-        return sorted([f.FormattedForUse() for f in self._flags.values()])
diff --git a/bestflags/flags_test.py b/bestflags/flags_test.py
deleted file mode 100644
index 231e569f..00000000
--- a/bestflags/flags_test.py
+++ /dev/null
@@ -1,193 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""Unit tests for the classes in module 'flags'.
-
-Part of the Chrome build flags optimization.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import random
-import sys
-import unittest
-
-from flags import Flag
-from flags import FlagSet
-
-
-# The number of tests to test.
-NUM_TESTS = 20
-
-
-class FlagTest(unittest.TestCase):
-    """This class tests the Flag class."""
-
-    def testInit(self):
-        """The value generated should fall within start and end of the spec.
-
-        If the value is not specified, the value generated should fall within start
-        and end of the spec.
-        """
-
-        for _ in range(NUM_TESTS):
-            start = random.randint(1, sys.maxint - 1)
-            end = random.randint(start + 1, sys.maxint)
-
-            spec = "flag=[%s-%s]" % (start, end)
-
-            test_flag = Flag(spec)
-
-            value = test_flag.GetValue()
-
-            # If the value is not specified when the flag is constructed, a random
-            # value is chosen. This value should fall within start and end of the
-            # spec.
-            assert start <= value and value < end
-
-    def testEqual(self):
-        """Test the equal operator (==) of the flag.
-
-        Two flags are equal if and only if their spec and value are equal.
-        """
-
-        tests = range(NUM_TESTS)
-
-        # Two tasks having the same spec and value should be equivalent.
-        for test in tests:
-            assert Flag(str(test), test) == Flag(str(test), test)
-
-        # Two tasks having different flag set should be different.
-        for test in tests:
-            flag = Flag(str(test), test)
-            other_flag_sets = [other for other in tests if test != other]
-            for other_test in other_flag_sets:
-                assert flag != Flag(str(other_test), other_test)
-
-    def testFormattedForUse(self):
-        """Test the FormattedForUse method of the flag.
-
-        The FormattedForUse replaces the string within the [] with the actual value.
-        """
-
-        for _ in range(NUM_TESTS):
-            start = random.randint(1, sys.maxint - 1)
-            end = random.randint(start + 1, sys.maxint)
-            value = random.randint(start, end - 1)
-
-            spec = "flag=[%s-%s]" % (start, end)
-
-            test_flag = Flag(spec, value)
-
-            # For numeric flag, the FormattedForUse replaces the string within the []
-            # with the actual value.
-            test_value = test_flag.FormattedForUse()
-            actual_value = "flag=%s" % value
-
-            assert test_value == actual_value
-
-        for _ in range(NUM_TESTS):
-            value = random.randint(1, sys.maxint - 1)
-
-            test_flag = Flag("flag", value)
-
-            # For boolean flag, the FormattedForUse returns the spec.
-            test_value = test_flag.FormattedForUse()
-            actual_value = "flag"
-            assert test_value == actual_value
-
-
-class FlagSetTest(unittest.TestCase):
-    """This class test the FlagSet class."""
-
-    def testEqual(self):
-        """Test the equal method of the Class FlagSet.
-
-        Two FlagSet instances are equal if all their flags are equal.
-        """
-
-        flag_names = range(NUM_TESTS)
-
-        # Two flag sets having the same flags should be equivalent.
-        for flag_name in flag_names:
-            spec = "%s" % flag_name
-
-            assert FlagSet([Flag(spec)]) == FlagSet([Flag(spec)])
-
-        # Two flag sets having different flags should be different.
-        for flag_name in flag_names:
-            spec = "%s" % flag_name
-            flag_set = FlagSet([Flag(spec)])
-            other_flag_sets = [
-                other for other in flag_names if flag_name != other
-            ]
-            for other_name in other_flag_sets:
-                other_spec = "%s" % other_name
-                assert flag_set != FlagSet([Flag(other_spec)])
-
-    def testGetItem(self):
-        """Test the get item method of the Class FlagSet.
-
-        The flag set is also indexed by the specs. The flag set should return the
-        appropriate flag given the spec.
-        """
-
-        tests = range(NUM_TESTS)
-
-        specs = [str(spec) for spec in tests]
-        flag_array = [Flag(spec) for spec in specs]
-
-        flag_set = FlagSet(flag_array)
-
-        # Created a dictionary of spec and flag, the flag set should return the flag
-        # the same as this dictionary.
-        spec_flag = dict(zip(specs, flag_array))
-
-        for spec in spec_flag:
-            assert flag_set[spec] == spec_flag[spec]
-
-    def testContain(self):
-        """Test the contain method of the Class FlagSet.
-
-        The flag set is also indexed by the specs. The flag set should return true
-        for spec if it contains a flag containing spec.
-        """
-
-        true_tests = range(NUM_TESTS)
-        false_tests = range(NUM_TESTS, NUM_TESTS * 2)
-
-        specs = [str(spec) for spec in true_tests]
-
-        flag_set = FlagSet([Flag(spec) for spec in specs])
-
-        for spec in specs:
-            assert spec in flag_set
-
-        for spec in false_tests:
-            assert spec not in flag_set
-
-    def testFormattedForUse(self):
-        """Test the FormattedForUse method of the Class FlagSet.
-
-        The output should be a sorted list of strings.
-        """
-
-        flag_names = range(NUM_TESTS)
-        flag_names.reverse()
-        flags = []
-        result = []
-
-        # Construct the flag set.
-        for flag_name in flag_names:
-            spec = "%s" % flag_name
-            flags.append(Flag(spec))
-            result.append(spec)
-
-        flag_set = FlagSet(flags)
-
-        # The results string should be sorted.
-        assert sorted(result) == flag_set.FormattedForUse()
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/bestflags/flags_util.py b/bestflags/flags_util.py
deleted file mode 100644
index c4a490e2..00000000
--- a/bestflags/flags_util.py
+++ /dev/null
@@ -1,96 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""Utility functions to explore the neighbor flags.
-
-Part of the Chrome build flags optimization.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import flags
-from flags import Flag
-
-
-def ClimbNext(flags_dict, climb_spec):
-    """Get the flags that are different from |flags_dict| by |climb_spec|.
-
-    Given a set of flags, |flags_dict|, return a new set of flags that are
-    adjacent along the flag spec |climb_spec|.
-
-    An example flags_dict is {foo=[1-9]:foo=5, bar=[1-5]:bar=2} and climb_spec is
-    bar=[1-5]. This method changes the flag that contains the spec bar=[1-5]. The
-    results are its neighbors dictionaries, i.e., {foo=[1-9]:foo=5,
-    bar=[1-5]:bar=1} and {foo=[1-9]:foo=5, bar=[1-5]:bar=3}.
-
-    Args:
-      flags_dict: The dictionary containing the original flags whose neighbors are
-        to be explored.
-      climb_spec: The spec in the flags_dict is to be changed. The spec is a
-        definition in the little language, a string with escaped sequences of the
-        form [<start>-<end>] where start and end is an positive integer for a
-        fillable value. An example of a spec is "foo[0-9]".
-
-    Returns:
-      List of dictionaries of neighbor flags.
-    """
-
-    # This method searches for a pattern [start-end] in the spec. If the spec
-    # contains this pattern, it is a numeric flag. Otherwise it is a boolean flag.
-    # For example, -finline-limit=[1-1000] is a numeric flag and -falign-jumps is
-    # a boolean flag.
-    numeric_flag_match = flags.Search(climb_spec)
-
-    # If the flags do not contain the spec.
-    if climb_spec not in flags_dict:
-        results = flags_dict.copy()
-
-        if numeric_flag_match:
-            # Numeric flags.
-            results[climb_spec] = Flag(
-                climb_spec, int(numeric_flag_match.group("start"))
-            )
-        else:
-            # Boolean flags.
-            results[climb_spec] = Flag(climb_spec)
-
-        return [results]
-
-    # The flags contain the spec.
-    if not numeric_flag_match:
-        # Boolean flags.
-        results = flags_dict.copy()
-
-        # Turn off the flag. A flag is turned off if it is not presented in the
-        # flags_dict.
-        del results[climb_spec]
-        return [results]
-
-    # Numeric flags.
-    flag = flags_dict[climb_spec]
-
-    # The value of the flag having spec.
-    value = flag.GetValue()
-    results = []
-
-    if value + 1 < int(numeric_flag_match.group("end")):
-        # If the value is not the end value, explore the value that is 1 larger than
-        # the current value.
-        neighbor = flags_dict.copy()
-        neighbor[climb_spec] = Flag(climb_spec, value + 1)
-        results.append(neighbor)
-
-    if value > int(numeric_flag_match.group("start")):
-        # If the value is not the start value, explore the value that is 1 lesser
-        # than the current value.
-        neighbor = flags_dict.copy()
-        neighbor[climb_spec] = Flag(climb_spec, value - 1)
-        results.append(neighbor)
-    else:
-        # Delete the value, i.e., turn off the flag. A flag is turned off if it is
-        # not presented in the flags_dict.
-        neighbor = flags_dict.copy()
-        del neighbor[climb_spec]
-        results.append(neighbor)
-
-    return results
diff --git a/bestflags/generation.py b/bestflags/generation.py
deleted file mode 100644
index 69622de5..00000000
--- a/bestflags/generation.py
+++ /dev/null
@@ -1,140 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""A generation of a set of tasks.
-
-Part of the Chrome build flags optimization.
-
-This module contains the base generation class. This class contains the tasks of
-this current generation. The generation will not evolve to the next generation
-until all the tasks in this generation are done executing. It also contains a
-set of tasks that could potentially be used to generate the next generation,
-e.g., in the genetic algorithm, a set of good species will be kept to evolve to
-the successive generations. For the hill climbing algorithm example, the
-candidate_pool will contain a current task t being evaluated and the exe_set
-will contains all the task t's neighbor.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-
-class NoneOverridingError(Exception):
-    """Define an Exception class for subclasses not overriding certain methods."""
-
-    pass
-
-
-class Generation(object):
-    """A generation of a framework run.
-
-    The base class of generation. Concrete subclasses, e.g., GAGeneration should
-    override the Next and IsImproved method to implement algorithm specific
-    applications.
-    """
-
-    def __init__(self, exe_set, candidate_pool):
-        """Set up the tasks set of this generation.
-
-        Args:
-          exe_set: A set of tasks to be run.
-          candidate_pool: A set of tasks to be considered to be used to generate the
-            next generation.
-        """
-
-        self._exe_set = exe_set
-        self._candidate_pool = candidate_pool
-
-        # Keeping the record of how many tasks are pending. Pending tasks are the
-        # ones that have been sent out to the next stage for execution but have not
-        # finished. A generation is not ready for the reproduction of the new
-        # generations until all its pending tasks have been executed.
-        self._pending = len(exe_set)
-
-    def CandidatePool(self):
-        """Return the candidate tasks of this generation."""
-
-        return self._candidate_pool
-
-    def Pool(self):
-        """Return the task set of this generation."""
-
-        return self._exe_set
-
-    def Done(self):
-        """All the tasks in this generation are done.
-
-        Returns:
-          True if all the tasks have been executed. That is the number of pending
-          task is 0.
-        """
-
-        return self._pending == 0
-
-    def UpdateTask(self, task):
-        """Match a task t in this generation that is equal to the input task.
-
-        This method is called when the input task has just finished execution. This
-        method finds out whether there is a pending task t in the current generation
-        that is the same as the input task. Two tasks are the same if their flag
-        options are the same. A task is pending if it has not been performed.
-        If there is a pending task t that matches the input task, task t will be
-        substituted with the input task in this generation. In that case, the input
-        task, as well as its build and test results encapsulated in the task, will
-        be stored in the current generation. These results could be used to produce
-        the next generation.
-        If there is a match, the current generation will have one less pending task.
-        When there is no pending task, the generation can be used to produce the
-        next generation.
-        The caller of this function is responsible for not calling this method on
-        the same task more than once.
-
-        Args:
-          task: A task that has its results ready.
-
-        Returns:
-          Whether the input task belongs to this generation.
-        """
-
-        # If there is a match, the input task belongs to this generation.
-        if task not in self._exe_set:
-            return False
-
-        # Remove the place holder task in this generation and store the new input
-        # task and its result.
-        self._exe_set.remove(task)
-        self._exe_set.add(task)
-
-        # The current generation will have one less task to wait on.
-        self._pending -= 1
-
-        assert self._pending >= 0
-
-        return True
-
-    def IsImproved(self):
-        """True if this generation has improvement upon its parent generation.
-
-        Raises:
-          NoneOverridingError: The subclass should override this method.
-        """
-        raise NoneOverridingError("Must be implemented by child class")
-
-    def Next(self, _):
-        """Calculate the next generation.
-
-        This is the core of the framework implementation. It must be overridden by
-        the concrete subclass to implement algorithm specific generations.
-
-        Args:
-          _: A set of tasks that have been generated before. The overridden method
-            in the subclasses can use this so as not to generate task that has been
-            generated before.
-
-        Returns:
-          A set of new generations.
-
-        Raises:
-          NoneOverridingError: The subclass should override this method.
-        """
-
-        raise NoneOverridingError("Must be implemented by child class")
diff --git a/bestflags/generation_test.py b/bestflags/generation_test.py
deleted file mode 100644
index 0928edcc..00000000
--- a/bestflags/generation_test.py
+++ /dev/null
@@ -1,73 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""Generation unittest.
-
-Part of the Chrome build flags optimization.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import random
-import unittest
-
-from generation import Generation
-from mock_task import IdentifierMockTask
-
-
-# Pick an integer at random.
-TEST_STAGE = -125
-
-# The number of tasks to be put in a generation to be tested.
-NUM_TASKS = 20
-
-# The stride of permutation used to shuffle the input list of tasks. Should be
-# relatively prime with NUM_TASKS.
-STRIDE = 7
-
-
-class GenerationTest(unittest.TestCase):
-    """This class test the Generation class.
-
-    Given a set of tasks in the generation, if there is any task that is pending,
-    then the Done method will return false, and true otherwise.
-    """
-
-    def testDone(self):
-        """ "Test the Done method.
-
-        Produce a generation with a set of tasks. Set the cost of the task one by
-        one and verify that the Done method returns false before setting the cost
-        for all the tasks. After the costs of all the tasks are set, the Done method
-        should return true.
-        """
-
-        random.seed(0)
-
-        testing_tasks = range(NUM_TASKS)
-
-        # The tasks for the generation to be tested.
-        tasks = [IdentifierMockTask(TEST_STAGE, t) for t in testing_tasks]
-
-        gen = Generation(set(tasks), None)
-
-        # Permute the list.
-        permutation = [(t * STRIDE) % NUM_TASKS for t in range(NUM_TASKS)]
-        permuted_tasks = [testing_tasks[index] for index in permutation]
-
-        # The Done method of the Generation should return false before all the tasks
-        # in the permuted list are set.
-        for testing_task in permuted_tasks:
-            assert not gen.Done()
-
-            # Mark a task as done by calling the UpdateTask method of the generation.
-            # Send the generation the task as well as its results.
-            gen.UpdateTask(IdentifierMockTask(TEST_STAGE, testing_task))
-
-        # The Done method should return true after all the tasks in the permuted
-        # list is set.
-        assert gen.Done()
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/bestflags/genetic_algorithm.py b/bestflags/genetic_algorithm.py
deleted file mode 100644
index c2bd5574..00000000
--- a/bestflags/genetic_algorithm.py
+++ /dev/null
@@ -1,304 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""The hill genetic algorithm.
-
-Part of the Chrome build flags optimization.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import random
-
-import flags
-from flags import Flag
-from flags import FlagSet
-from generation import Generation
-from task import Task
-
-
-def CrossoverWith(first_flag, second_flag):
-    """Get a crossed over gene.
-
-    At present, this just picks either/or of these values.  However, it could be
-    implemented as an integer maskover effort, if required.
-
-    Args:
-      first_flag: The first gene (Flag) to cross over with.
-      second_flag: The second gene (Flag) to cross over with.
-
-    Returns:
-      A Flag that can be considered appropriately randomly blended between the
-      first and second input flag.
-    """
-
-    return first_flag if random.randint(0, 1) else second_flag
-
-
-def RandomMutate(specs, flag_set, mutation_rate):
-    """Randomly mutate the content of a task.
-
-    Args:
-      specs: A list of spec from which the flag set is created.
-      flag_set: The current flag set being mutated
-      mutation_rate: What fraction of genes to mutate.
-
-    Returns:
-      A Genetic Task constructed by randomly mutating the input flag set.
-    """
-
-    results_flags = []
-
-    for spec in specs:
-        # Randomly choose whether this flag should be mutated.
-        if random.randint(0, int(1 / mutation_rate)):
-            continue
-
-        # If the flag is not already in the flag set, it is added.
-        if spec not in flag_set:
-            results_flags.append(Flag(spec))
-            continue
-
-        # If the flag is already in the flag set, it is mutated.
-        numeric_flag_match = flags.Search(spec)
-
-        # The value of a numeric flag will be changed, and a boolean flag will be
-        # dropped.
-        if not numeric_flag_match:
-            continue
-
-        value = flag_set[spec].GetValue()
-
-        # Randomly select a nearby value of the current value of the flag.
-        rand_arr = [value]
-        if value + 1 < int(numeric_flag_match.group("end")):
-            rand_arr.append(value + 1)
-
-        rand_arr.append(value - 1)
-        value = random.sample(rand_arr, 1)[0]
-
-        # If the value is smaller than the start of the spec, this flag will be
-        # dropped.
-        if value != int(numeric_flag_match.group("start")) - 1:
-            results_flags.append(Flag(spec, value))
-
-    return GATask(FlagSet(results_flags))
-
-
-class GATask(Task):
-    def __init__(self, flag_set):
-        Task.__init__(self, flag_set)
-
-    def ReproduceWith(self, other, specs, mutation_rate):
-        """Reproduce with other FlagSet.
-
-        Args:
-          other: A FlagSet to reproduce with.
-          specs: A list of spec from which the flag set is created.
-          mutation_rate: one in mutation_rate flags will be mutated (replaced by a
-            random version of the same flag, instead of one from either of the
-            parents).  Set to 0 to disable mutation.
-
-        Returns:
-          A GA task made by mixing self with other.
-        """
-
-        # Get the flag dictionary.
-        father_flags = self.GetFlags().GetFlags()
-        mother_flags = other.GetFlags().GetFlags()
-
-        # Flags that are common in both parents and flags that belong to only one
-        # parent.
-        self_flags = []
-        other_flags = []
-        common_flags = []
-
-        # Find out flags that are common to both parent and flags that belong soly
-        # to one parent.
-        for self_flag in father_flags:
-            if self_flag in mother_flags:
-                common_flags.append(self_flag)
-            else:
-                self_flags.append(self_flag)
-
-        for other_flag in mother_flags:
-            if other_flag not in father_flags:
-                other_flags.append(other_flag)
-
-        # Randomly select flags that belong to only one parent.
-        output_flags = [
-            father_flags[f] for f in self_flags if random.randint(0, 1)
-        ]
-        others = [mother_flags[f] for f in other_flags if random.randint(0, 1)]
-        output_flags.extend(others)
-        # Turn on flags that belong to both parent. Randomly choose the value of the
-        # flag from either parent.
-        for flag in common_flags:
-            output_flags.append(
-                CrossoverWith(father_flags[flag], mother_flags[flag])
-            )
-
-        # Mutate flags
-        if mutation_rate:
-            return RandomMutate(specs, FlagSet(output_flags), mutation_rate)
-
-        return GATask(FlagSet(output_flags))
-
-
-class GAGeneration(Generation):
-    """The Genetic Algorithm."""
-
-    # The value checks whether the algorithm has converged and arrives at a fixed
-    # point. If STOP_THRESHOLD of generations have not seen any performance
-    # improvement, the Genetic Algorithm stops.
-    STOP_THRESHOLD = None
-
-    # Number of tasks in each generation.
-    NUM_CHROMOSOMES = None
-
-    # The value checks whether the algorithm has converged and arrives at a fixed
-    # point. If NUM_TRIALS of trials have been attempted to generate a new task
-    # without a success, the Genetic Algorithm stops.
-    NUM_TRIALS = None
-
-    # The flags that can be used to generate new tasks.
-    SPECS = None
-
-    # What fraction of genes to mutate.
-    MUTATION_RATE = 0
-
-    @staticmethod
-    def InitMetaData(
-        stop_threshold, num_chromosomes, num_trials, specs, mutation_rate
-    ):
-        """Set up the meta data for the Genetic Algorithm.
-
-        Args:
-          stop_threshold: The number of generations, upon which no performance has
-            seen, the Genetic Algorithm stops.
-          num_chromosomes: Number of tasks in each generation.
-          num_trials: The number of trials, upon which new task has been tried to
-            generated without success, the Genetic Algorithm stops.
-          specs: The flags that can be used to generate new tasks.
-          mutation_rate: What fraction of genes to mutate.
-        """
-
-        GAGeneration.STOP_THRESHOLD = stop_threshold
-        GAGeneration.NUM_CHROMOSOMES = num_chromosomes
-        GAGeneration.NUM_TRIALS = num_trials
-        GAGeneration.SPECS = specs
-        GAGeneration.MUTATION_RATE = mutation_rate
-
-    def __init__(self, tasks, parents, total_stucks):
-        """Set up the meta data for the Genetic Algorithm.
-
-        Args:
-          tasks: A set of tasks to be run.
-          parents: A set of tasks from which this new generation is produced. This
-            set also contains the best tasks generated so far.
-          total_stucks: The number of generations that have not seen improvement.
-            The Genetic Algorithm will stop once the total_stucks equals to
-            NUM_TRIALS defined in the GAGeneration class.
-        """
-
-        Generation.__init__(self, tasks, parents)
-        self._total_stucks = total_stucks
-
-    def IsImproved(self):
-        """True if this generation has improvement upon its parent generation."""
-
-        tasks = self.Pool()
-        parents = self.CandidatePool()
-
-        # The first generate does not have parents.
-        if not parents:
-            return True
-
-        # Found out whether a task has improvement upon the best task in the
-        # parent generation.
-        best_parent = sorted(parents, key=lambda task: task.GetTestResult())[0]
-        best_current = sorted(tasks, key=lambda task: task.GetTestResult())[0]
-
-        # At least one task has improvement.
-        if best_current.IsImproved(best_parent):
-            self._total_stucks = 0
-            return True
-
-        # If STOP_THRESHOLD of generations have no improvement, the algorithm stops.
-        if self._total_stucks >= GAGeneration.STOP_THRESHOLD:
-            return False
-
-        self._total_stucks += 1
-        return True
-
-    def Next(self, cache):
-        """Calculate the next generation.
-
-        Generate a new generation from the a set of tasks. This set contains the
-          best set seen so far and the tasks executed in the parent generation.
-
-        Args:
-          cache: A set of tasks that have been generated before.
-
-        Returns:
-          A set of new generations.
-        """
-
-        target_len = GAGeneration.NUM_CHROMOSOMES
-        specs = GAGeneration.SPECS
-        mutation_rate = GAGeneration.MUTATION_RATE
-
-        # Collect a set of size target_len of tasks. This set will be used to
-        # produce a new generation of tasks.
-        gen_tasks = [task for task in self.Pool()]
-
-        parents = self.CandidatePool()
-        if parents:
-            gen_tasks.extend(parents)
-
-        # A set of tasks that are the best. This set will be used as the parent
-        # generation to produce the next generation.
-        sort_func = lambda task: task.GetTestResult()
-        retained_tasks = sorted(gen_tasks, key=sort_func)[:target_len]
-
-        child_pool = set()
-        for father in retained_tasks:
-            num_trials = 0
-            # Try num_trials times to produce a new child.
-            while num_trials < GAGeneration.NUM_TRIALS:
-                # Randomly select another parent.
-                mother = random.choice(retained_tasks)
-                # Cross over.
-                child = mother.ReproduceWith(father, specs, mutation_rate)
-                if child not in child_pool and child not in cache:
-                    child_pool.add(child)
-                    break
-                else:
-                    num_trials += 1
-
-        num_trials = 0
-
-        while (
-            len(child_pool) < target_len
-            and num_trials < GAGeneration.NUM_TRIALS
-        ):
-            for keep_task in retained_tasks:
-                # Mutation.
-                child = RandomMutate(specs, keep_task.GetFlags(), mutation_rate)
-                if child not in child_pool and child not in cache:
-                    child_pool.add(child)
-                    if len(child_pool) >= target_len:
-                        break
-                else:
-                    num_trials += 1
-
-        # If NUM_TRIALS of tries have been attempted without generating a set of new
-        # tasks, the algorithm stops.
-        if num_trials >= GAGeneration.NUM_TRIALS:
-            return []
-
-        assert len(child_pool) == target_len
-
-        return [
-            GAGeneration(child_pool, set(retained_tasks), self._total_stucks)
-        ]
diff --git a/bestflags/hill_climb_best_neighbor.py b/bestflags/hill_climb_best_neighbor.py
deleted file mode 100644
index 2455dd94..00000000
--- a/bestflags/hill_climb_best_neighbor.py
+++ /dev/null
@@ -1,110 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""A variation of the hill climbing algorithm.
-
-Part of the Chrome build flags optimization.
-
-This algorithm explores all the neighbors of the current task. If at least one
-neighbor gives better performance than the current task, it explores the best
-neighbor.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-from flags import FlagSet
-import flags_util
-from generation import Generation
-from task import Task
-
-
-class HillClimbingBestBranch(Generation):
-    """A variation of the hill climbing algorithm.
-
-    Given a task, it explores all its neighbors. Pick the best neighbor for the
-    next iteration.
-    """
-
-    def __init__(self, exe_set, parents, specs):
-        """Set up the tasks set of this generation.
-
-        Args:
-          exe_set: A set of tasks to be run.
-          parents: A set of tasks to be used to check whether their neighbors have
-            improved upon them.
-          specs: A list of specs to explore. The spec specifies the flags that can
-            be changed to find neighbors of a task.
-        """
-
-        Generation.__init__(self, exe_set, parents)
-        self._specs = specs
-
-        # This variable will be used, by the Next method, to generate the tasks for
-        # the next iteration. This self._next_task contains the best task in the
-        # current iteration and it will be set by the IsImproved method. The tasks
-        # of the next iteration are the neighbor of self._next_task.
-        self._next_task = None
-
-    def IsImproved(self):
-        """True if this generation has improvement over its parent generation.
-
-        If this generation improves upon the previous generation, this method finds
-        out the best task in this generation and sets it to _next_task for the
-        method Next to use.
-
-        Returns:
-          True if the best neighbor improves upon the parent task.
-        """
-
-        # Find the best neighbor.
-        best_task = None
-        for task in self._exe_set:
-            if not best_task or task.IsImproved(best_task):
-                best_task = task
-
-        if not best_task:
-            return False
-
-        # The first generation may not have parent generation.
-        parents = list(self._candidate_pool)
-        if parents:
-            assert len(parents) == 1
-            self._next_task = best_task
-            # If the best neighbor improves upon the parent task.
-            return best_task.IsImproved(parents[0])
-
-        self._next_task = best_task
-        return True
-
-    def Next(self, cache):
-        """Calculate the next generation.
-
-        The best neighbor b of the current task is the parent of the next
-        generation. The neighbors of b will be the set of tasks to be evaluated
-        next.
-
-        Args:
-          cache: A set of tasks that have been generated before.
-
-        Returns:
-          A set of new generations.
-        """
-
-        # The best neighbor.
-        current_task = self._next_task
-        flag_set = current_task.GetFlags()
-
-        # The neighbors of the best neighbor.
-        children_tasks = set([])
-        for spec in self._specs:
-            for next_flag in flags_util.ClimbNext(flag_set.GetFlags(), spec):
-                new_task = Task(FlagSet(next_flag.values()))
-
-                if new_task not in cache:
-                    children_tasks.add(new_task)
-
-        return [
-            HillClimbingBestBranch(
-                children_tasks, set([current_task]), self._specs
-            )
-        ]
diff --git a/bestflags/iterative_elimination.py b/bestflags/iterative_elimination.py
deleted file mode 100644
index 8d548606..00000000
--- a/bestflags/iterative_elimination.py
+++ /dev/null
@@ -1,179 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""Iterative flags elimination.
-
-Part of the Chrome build flags optimization.
-
-This module implements the flag iterative elimination algorithm (IE) adopted
-from the paper
-Z. Pan et al. Fast and Effective Orchestration of Compiler Optimizations for
-Automatic Performance Tuning.
-
-IE begins with the base line that turns on all the optimizations flags and
-setting the numeric flags to their highest values. IE turns off the one boolean
-flag or lower the value of a numeric flag with the most negative effect from the
-baseline. This process repeats with all remaining flags, until none of them
-causes performance degradation. The complexity of IE is O(n^2).
-
-For example, -fstrict-aliasing and -ftree-vectorize. The base line is
-b=[-fstrict-aliasing, -ftree-vectorize]. The two tasks in the first iteration
-are t0=[-fstrict-aliasing] and t1=[-ftree-vectorize]. The algorithm compares b
-with t0 and t1, respectively, and see whether setting the numeric flag with a
-lower value or removing the boolean flag -fstrict-aliasing produce a better
-fitness value.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import flags
-from generation import Generation
-import task
-
-
-def _DecreaseFlag(flags_dict, spec):
-    """Decrease the value of the flag that has the specification spec.
-
-    If the flag that contains the spec is a boolean flag, it is eliminated.
-    Otherwise the flag is a numeric flag, its value will be reduced by one.
-
-    Args:
-      flags_dict: The dictionary containing the original flags whose neighbors are
-        to be explored.
-      spec: The spec in the flags_dict is to be changed.
-
-    Returns:
-      Dictionary of neighbor flag that is only different from the original
-      dictionary by the spec.
-    """
-
-    # The specification must be held by one of the flags.
-    assert spec in flags_dict
-
-    # The results this method returns.
-    results = flags_dict.copy()
-
-    # This method searches for a pattern [start-end] in the spec. If the spec
-    # contains this pattern, it is a numeric flag. Otherwise it is a boolean flag.
-    # For example, -finline-limit=[1-1000] is a numeric flag and -falign-jumps is
-    # a boolean flag.
-    numeric_flag_match = flags.Search(spec)
-
-    if numeric_flag_match:
-        # numeric flag
-        val = results[spec].GetValue()
-
-        # If the value of the flag is the lower boundary of the specification, this
-        # flag will be turned off. Because it already contains the lowest value and
-        # can not be decreased any more.
-        if val == int(numeric_flag_match.group("start")):
-            # Turn off the flag. A flag is turned off if it is not presented in the
-            # flags_dict.
-            del results[spec]
-        else:
-            results[spec] = flags.Flag(spec, val - 1)
-    else:
-        # Turn off the flag. A flag is turned off if it is not presented in the
-        # flags_dict.
-        del results[spec]
-
-    return results
-
-
-class IterativeEliminationGeneration(Generation):
-    """The negative flag iterative elimination algorithm."""
-
-    def __init__(self, exe_set, parent_task):
-        """Set up the base line parent task.
-
-        The parent task is the base line against which the new tasks are compared.
-        The new tasks are only different from the base line from one flag f by
-        either turning this flag f off, or lower the flag value by 1.
-        If a new task is better than the base line, one flag is identified that
-        gives degradation. The flag that give the worst degradation will be removed
-        or lower the value by 1 in the base in each iteration.
-
-        Args:
-          exe_set: A set of tasks to be run. Each one only differs from the
-            parent_task by one flag.
-          parent_task: The base line task, against which the new tasks in exe_set
-            are compared.
-        """
-
-        Generation.__init__(self, exe_set, None)
-        self._parent_task = parent_task
-
-    def IsImproved(self):
-        """Whether any new task has improvement upon the parent task."""
-
-        parent = self._parent_task
-        # Whether there is any new task that has improvement over the parent base
-        # line task.
-        for curr in [curr for curr in self.Pool() if curr != parent]:
-            if curr.IsImproved(parent):
-                return True
-
-        return False
-
-    def Next(self, cache):
-        """Find out the flag that gives the worst degradation.
-
-        Found out the flag that gives the worst degradation. Turn that flag off from
-        the base line and use the new base line for the new generation.
-
-        Args:
-          cache: A set of tasks that have been generated before.
-
-        Returns:
-          A set of new generations.
-        """
-        parent_task = self._parent_task
-
-        # Find out the task that gives the worst degradation.
-        worst_task = parent_task
-
-        for curr in [curr for curr in self.Pool() if curr != parent_task]:
-            # The method IsImproved, which is supposed to be called before, ensures
-            # that there is at least a task that improves upon the parent_task.
-            if curr.IsImproved(worst_task):
-                worst_task = curr
-
-        assert worst_task != parent_task
-
-        # The flags_set of the worst task.
-        work_flags_set = worst_task.GetFlags().GetFlags()
-
-        results = set([])
-
-        # If the flags_set contains no flag, i.e., all the flags have been
-        # eliminated, the algorithm stops.
-        if not work_flags_set:
-            return []
-
-        # Turn of the remaining flags one by one for the next generation.
-        for spec in work_flags_set:
-            flag_set = flags.FlagSet(
-                _DecreaseFlag(work_flags_set, spec).values()
-            )
-            new_task = task.Task(flag_set)
-            if new_task not in cache:
-                results.add(new_task)
-
-        return [IterativeEliminationGeneration(results, worst_task)]
-
-
-class IterativeEliminationFirstGeneration(IterativeEliminationGeneration):
-    """The first iteration of the iterative elimination algorithm.
-
-    The first iteration also evaluates the base line task. The base line tasks in
-    the subsequent iterations have been evaluated. Therefore,
-    IterativeEliminationGeneration does not include the base line task in the
-    execution set.
-    """
-
-    def IsImproved(self):
-        # Find out the base line task in the execution set.
-        parent = next(task for task in self.Pool() if task == self._parent_task)
-        self._parent_task = parent
-
-        return IterativeEliminationGeneration.IsImproved(self)
diff --git a/bestflags/mock_task.py b/bestflags/mock_task.py
deleted file mode 100644
index e25daeba..00000000
--- a/bestflags/mock_task.py
+++ /dev/null
@@ -1,93 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""This module defines the common mock tasks used by various unit tests.
-
-Part of the Chrome build flags optimization.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-# Pick an integer at random.
-POISONPILL = 975
-
-
-class MockTask(object):
-    """This class emulates an actual task.
-
-    It does not do the actual work, but simply returns the result as given when
-    this task is constructed.
-    """
-
-    def __init__(self, stage, identifier, cost=0):
-        """Set up the results for this task.
-
-        Args:
-          stage: the stage of this test is in.
-          identifier: the identifier of this task.
-          cost: the mock cost of this task.
-
-          The _cost field stored the cost. Once this task is performed, i.e., by
-          calling the work method or by setting the result from other task, the
-          _cost field will have this cost. The stage field verifies that the module
-          being tested and the unitest are in the same stage. If the unitest does
-          not care about cost of this task, the cost parameter should be leaved
-          blank.
-        """
-
-        self._identifier = identifier
-        self._cost = cost
-        self._stage = stage
-
-        # Indicate that this method has not been performed yet.
-        self._performed = False
-
-    def __eq__(self, other):
-        if isinstance(other, MockTask):
-            return self._identifier == other.GetIdentifier(
-                self._stage
-            ) and self._cost == other.GetResult(self._stage)
-        return False
-
-    def GetIdentifier(self, stage):
-        assert stage == self._stage
-        return self._identifier
-
-    def SetResult(self, stage, cost):
-        assert stage == self._stage
-        self._cost = cost
-        self._performed = True
-
-    def Work(self, stage):
-        assert stage == self._stage
-        self._performed = True
-
-    def GetResult(self, stage):
-        assert stage == self._stage
-        return self._cost
-
-    def Done(self, stage):
-        """Indicates whether the task has been performed."""
-
-        assert stage == self._stage
-        return self._performed
-
-    def LogSteeringCost(self):
-        pass
-
-
-class IdentifierMockTask(MockTask):
-    """This class defines the mock task that does not consider the cost.
-
-    The task instances will be inserted into a set. Therefore the hash and the
-    equal methods are overridden. The unittests that compares identities of the
-    tasks for equality can use this mock task instead of the base mock tack.
-    """
-
-    def __hash__(self):
-        return self._identifier
-
-    def __eq__(self, other):
-        if isinstance(other, MockTask):
-            return self._identifier == other.GetIdentifier(self._stage)
-        return False
diff --git a/bestflags/pipeline_process.py b/bestflags/pipeline_process.py
deleted file mode 100644
index 3aab96fe..00000000
--- a/bestflags/pipeline_process.py
+++ /dev/null
@@ -1,145 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""Pipeline process that encapsulates the actual content.
-
-Part of the Chrome build flags optimization.
-
-The actual stages include the builder and the executor.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import multiprocessing
-
-
-# Pick an integer at random.
-POISONPILL = 975
-
-
-class PipelineProcess(multiprocessing.Process):
-    """A process that encapsulates the actual content pipeline stage.
-
-    The actual pipeline stage can be the builder or the tester.  This process
-    continuously pull tasks from the queue until a poison pill is received.
-    Once a job is received, it will hand it to the actual stage for processing.
-
-    Each pipeline stage contains three modules.
-    The first module continuously pulls task from the input queue. It searches the
-    cache to check whether the task has encountered before. If so, duplicate
-    computation can be avoided.
-    The second module consists of a pool of workers that do the actual work, e.g.,
-    the worker will compile the source code and get the image in the builder
-    pipeline stage.
-    The third module is a helper that put the result cost to the cost field of the
-    duplicate tasks. For example, if two tasks are equivalent, only one task, say
-    t1 will be executed and the other task, say t2 will not be executed. The third
-    mode gets the result from t1, when it is available and set the cost of t2 to
-    be the same as that of t1.
-    """
-
-    def __init__(
-        self,
-        num_processes,
-        name,
-        cache,
-        stage,
-        task_queue,
-        helper,
-        worker,
-        result_queue,
-    ):
-        """Set up input/output queue and the actual method to be called.
-
-        Args:
-          num_processes: Number of helpers subprocessors this stage has.
-          name: The name of this stage.
-          cache: The computed tasks encountered before.
-          stage: An int value that specifies the stage for this pipeline stage, for
-            example, build stage or test stage. This value will be used to retrieve
-            the keys in different stage. I.e., the flags set is the key in build
-            stage and the checksum is the key in the test stage. The key is used to
-            detect duplicates.
-          task_queue: The input task queue for this pipeline stage.
-          helper: The method hosted by the helper module to fill up the cost of the
-            duplicate tasks.
-          worker: The method hosted by the worker pools to do the actual work, e.g.,
-            compile the image.
-          result_queue: The output task queue for this pipeline stage.
-        """
-
-        multiprocessing.Process.__init__(self)
-
-        self._name = name
-        self._task_queue = task_queue
-        self._result_queue = result_queue
-
-        self._helper = helper
-        self._worker = worker
-
-        self._cache = cache
-        self._stage = stage
-        self._num_processes = num_processes
-
-        # the queues used by the modules for communication
-        manager = multiprocessing.Manager()
-        self._helper_queue = manager.Queue()
-        self._work_queue = manager.Queue()
-
-    def run(self):
-        """Busy pulling the next task from the queue for execution.
-
-        Once a job is pulled, this stage invokes the actual stage method and submits
-        the result to the next pipeline stage.
-
-        The process will terminate on receiving the poison pill from previous stage.
-        """
-
-        # the worker pool
-        work_pool = multiprocessing.Pool(self._num_processes)
-
-        # the helper process
-        helper_process = multiprocessing.Process(
-            target=self._helper,
-            args=(
-                self._stage,
-                self._cache,
-                self._helper_queue,
-                self._work_queue,
-                self._result_queue,
-            ),
-        )
-        helper_process.start()
-        mycache = self._cache.keys()
-
-        while True:
-            task = self._task_queue.get()
-            if task == POISONPILL:
-                # Poison pill means shutdown
-                self._result_queue.put(POISONPILL)
-                break
-
-            task_key = task.GetIdentifier(self._stage)
-            if task_key in mycache:
-                # The task has been encountered before. It will be sent to the helper
-                # module for further processing.
-                self._helper_queue.put(task)
-            else:
-                # Let the workers do the actual work.
-                work_pool.apply_async(
-                    self._worker,
-                    args=(
-                        self._stage,
-                        task,
-                        self._work_queue,
-                        self._result_queue,
-                    ),
-                )
-                mycache.append(task_key)
-
-        # Shutdown the workers pool and the helper process.
-        work_pool.close()
-        work_pool.join()
-
-        self._helper_queue.put(POISONPILL)
-        helper_process.join()
diff --git a/bestflags/pipeline_process_test.py b/bestflags/pipeline_process_test.py
deleted file mode 100644
index 04e641ec..00000000
--- a/bestflags/pipeline_process_test.py
+++ /dev/null
@@ -1,95 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""Pipeline Process unittest.
-
-Part of the Chrome build flags optimization.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import multiprocessing
-import unittest
-
-from mock_task import MockTask
-import pipeline_process
-
-
-# Pick an integer at random.
-ERROR = -334
-# Pick an integer at random.
-TEST_STAGE = -8
-
-
-def MockHelper(stage, done_dict, helper_queue, _, result_queue):
-    """This method echos input to the output."""
-
-    assert stage == TEST_STAGE
-    while True:
-        if not helper_queue.empty():
-            task = helper_queue.get()
-            if task == pipeline_process.POISONPILL:
-                # Poison pill means shutdown
-                break
-
-            if task in done_dict:
-                # verify that it does not get duplicate "1"s in the test.
-                result_queue.put(ERROR)
-            else:
-                result_queue.put(("helper", task.GetIdentifier(TEST_STAGE)))
-
-
-def MockWorker(stage, task, _, result_queue):
-    assert stage == TEST_STAGE
-    result_queue.put(("worker", task.GetIdentifier(TEST_STAGE)))
-
-
-class PipelineProcessTest(unittest.TestCase):
-    """This class test the PipelineProcess.
-
-    All the task inserted into the input queue should be taken out and hand to the
-    actual pipeline handler, except for the POISON_PILL.  All these task should
-    also be passed to the next pipeline stage via the output queue.
-    """
-
-    def testRun(self):
-        """Test the run method.
-
-        Ensure that all the tasks inserted into the queue are properly handled.
-        """
-
-        manager = multiprocessing.Manager()
-        inp = manager.Queue()
-        output = manager.Queue()
-
-        process = pipeline_process.PipelineProcess(
-            2, "testing", {}, TEST_STAGE, inp, MockHelper, MockWorker, output
-        )
-
-        process.start()
-        inp.put(MockTask(TEST_STAGE, 1))
-        inp.put(MockTask(TEST_STAGE, 1))
-        inp.put(MockTask(TEST_STAGE, 2))
-        inp.put(pipeline_process.POISONPILL)
-        process.join()
-
-        # All tasks are processed once and only once.
-        result = [
-            ("worker", 1),
-            ("helper", 1),
-            ("worker", 2),
-            pipeline_process.POISONPILL,
-        ]
-        while result:
-            task = output.get()
-
-            # One "1"s is passed to the worker and one to the helper.
-            self.assertNotEqual(task, ERROR)
-
-            # The messages received should be exactly the same as the result.
-            self.assertTrue(task in result)
-            result.remove(task)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/bestflags/pipeline_worker.py b/bestflags/pipeline_worker.py
deleted file mode 100644
index f18be66b..00000000
--- a/bestflags/pipeline_worker.py
+++ /dev/null
@@ -1,147 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""The pipeline_worker functions of the build and test stage of the framework.
-
-Part of the Chrome build flags optimization.
-
-This module defines the helper and the worker. If there are duplicate tasks, for
-example t1 and t2, needs to be built/tested, one of them, for example t1, will
-be built/tested and the helper waits for the result of t1 and set the results of
-the other task, t2 here, to be the same as that of t1. Setting the result of t2
-to be the same as t1 is referred to as resolving the result of t2.
-The worker invokes the work method of the tasks that are not duplicate.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import pipeline_process
-
-
-def Helper(stage, done_dict, helper_queue, completed_queue, result_queue):
-    """Helper that filters duplicate tasks.
-
-    This method Continuously pulls duplicate tasks from the helper_queue. The
-    duplicate tasks need not be compiled/tested. This method also pulls completed
-    tasks from the worker queue and let the results of the duplicate tasks be the
-    same as their corresponding finished task.
-
-    Args:
-      stage: The current stage of the pipeline, for example, build stage or test
-        stage.
-      done_dict: A dictionary of tasks that are done. The key of the dictionary is
-        the identifier of the task. The value of the dictionary is the results of
-        performing the corresponding task.
-      helper_queue: A queue of duplicate tasks whose results need to be resolved.
-        This is a communication channel between the pipeline_process and this
-        helper process.
-      completed_queue: A queue of tasks that have been built/tested. The results
-        of these tasks are needed to resolve the results of the duplicate tasks.
-        This is the communication channel between the workers and this helper
-        process.
-      result_queue: After the results of the duplicate tasks have been resolved,
-        the duplicate tasks will be sent to the next stage via this queue.
-    """
-
-    # The list of duplicate tasks, the results of which need to be resolved.
-    waiting_list = []
-
-    while True:
-        # Pull duplicate task from the helper queue.
-        if not helper_queue.empty():
-            task = helper_queue.get()
-
-            if task == pipeline_process.POISONPILL:
-                # Poison pill means no more duplicate task from the helper queue.
-                break
-
-            # The task has not been performed before.
-            assert not task.Done(stage)
-
-            # The identifier of this task.
-            identifier = task.GetIdentifier(stage)
-
-            # If a duplicate task comes before the corresponding resolved results from
-            # the completed_queue, it will be put in the waiting list. If the result
-            # arrives before the duplicate task, the duplicate task will be resolved
-            # right away.
-            if identifier in done_dict:
-                # This task has been encountered before and the result is available. The
-                # result can be resolved right away.
-                task.SetResult(stage, done_dict[identifier])
-                result_queue.put(task)
-            else:
-                waiting_list.append(task)
-
-        # Check and get completed tasks from completed_queue.
-        GetResultFromCompletedQueue(
-            stage, completed_queue, done_dict, waiting_list, result_queue
-        )
-
-    # Wait to resolve the results of the remaining duplicate tasks.
-    while waiting_list:
-        GetResultFromCompletedQueue(
-            stage, completed_queue, done_dict, waiting_list, result_queue
-        )
-
-
-def GetResultFromCompletedQueue(
-    stage, completed_queue, done_dict, waiting_list, result_queue
-):
-    """Pull results from the completed queue and resolves duplicate tasks.
-
-    Args:
-      stage: The current stage of the pipeline, for example, build stage or test
-        stage.
-      completed_queue: A queue of tasks that have been performed. The results of
-        these tasks are needed to resolve the results of the duplicate tasks. This
-        is the communication channel between the workers and this method.
-      done_dict: A dictionary of tasks that are done. The key of the dictionary is
-        the optimization flags of the task. The value of the dictionary is the
-        compilation results of the corresponding task.
-      waiting_list: The list of duplicate tasks, the results of which need to be
-        resolved.
-      result_queue: After the results of the duplicate tasks have been resolved,
-        the duplicate tasks will be sent to the next stage via this queue.
-
-    This helper method tries to pull a completed task from the completed queue.
-    If it gets a task from the queue, it resolves the results of all the relevant
-    duplicate tasks in the waiting list. Relevant tasks are the tasks that have
-    the same flags as the currently received results from the completed_queue.
-    """
-    # Pull completed task from the worker queue.
-    if not completed_queue.empty():
-        (identifier, result) = completed_queue.get()
-        done_dict[identifier] = result
-
-        tasks = [
-            t for t in waiting_list if t.GetIdentifier(stage) == identifier
-        ]
-        for duplicate_task in tasks:
-            duplicate_task.SetResult(stage, result)
-            result_queue.put(duplicate_task)
-            waiting_list.remove(duplicate_task)
-
-
-def Worker(stage, task, helper_queue, result_queue):
-    """Worker that performs the task.
-
-    This method calls the work method of the input task and distribute the result
-    to the helper and the next stage.
-
-    Args:
-      stage: The current stage of the pipeline, for example, build stage or test
-        stage.
-      task: Input task that needs to be performed.
-      helper_queue: Queue that holds the completed tasks and the results. This is
-        the communication channel between the worker and the helper.
-      result_queue: Queue that holds the completed tasks and the results. This is
-        the communication channel between the worker and the next stage.
-    """
-
-    # The task has not been completed before.
-    assert not task.Done(stage)
-
-    task.Work(stage)
-    helper_queue.put((task.GetIdentifier(stage), task.GetResult(stage)))
-    result_queue.put(task)
diff --git a/bestflags/pipeline_worker_test.py b/bestflags/pipeline_worker_test.py
deleted file mode 100644
index 15c51ec1..00000000
--- a/bestflags/pipeline_worker_test.py
+++ /dev/null
@@ -1,135 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""Unittest for the pipeline_worker functions in the build/test stage.
-
-Part of the Chrome build flags optimization.
-
-This module tests the helper method and the worker method.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import multiprocessing
-import random
-import sys
-import unittest
-
-from mock_task import MockTask
-import pipeline_process
-import pipeline_worker
-
-
-# Pick an integer at random.
-TEST_STAGE = -3
-
-
-def MockTaskCostGenerator():
-    """Calls a random number generator and returns a negative number."""
-    return random.randint(-sys.maxint - 1, -1)
-
-
-class PipelineWorkerTest(unittest.TestCase):
-    """This class tests the pipeline_worker functions.
-
-    Given the same identifier, the cost should result the same from the
-    pipeline_worker functions.
-    """
-
-    def testHelper(self):
-        """ "Test the helper.
-
-        Call the helper method twice, and test the results. The results should be
-        the same, i.e., the cost should be the same.
-        """
-
-        # Set up the input, helper and output queue for the helper method.
-        manager = multiprocessing.Manager()
-        helper_queue = manager.Queue()
-        result_queue = manager.Queue()
-        completed_queue = manager.Queue()
-
-        # Set up the helper process that holds the helper method.
-        helper_process = multiprocessing.Process(
-            target=pipeline_worker.Helper,
-            args=(TEST_STAGE, {}, helper_queue, completed_queue, result_queue),
-        )
-        helper_process.start()
-
-        # A dictionary defines the mock result to the helper.
-        mock_result = {1: 1995, 2: 59, 9: 1027}
-
-        # Test if there is a task that is done before, whether the duplicate task
-        # will have the same result. Here, two different scenarios are tested. That
-        # is the mock results are added to the completed_queue before and after the
-        # corresponding mock tasks being added to the input queue.
-        completed_queue.put((9, mock_result[9]))
-
-        # The output of the helper should contain all the following tasks.
-        results = [1, 1, 2, 9]
-
-        # Testing the correctness of having tasks having the same identifier, here
-        # 1.
-        for result in results:
-            helper_queue.put(
-                MockTask(TEST_STAGE, result, MockTaskCostGenerator())
-            )
-
-        completed_queue.put((2, mock_result[2]))
-        completed_queue.put((1, mock_result[1]))
-
-        # Signal there is no more duplicate task.
-        helper_queue.put(pipeline_process.POISONPILL)
-        helper_process.join()
-
-        while results:
-            task = result_queue.get()
-            identifier = task.GetIdentifier(TEST_STAGE)
-            self.assertTrue(identifier in results)
-            if identifier in mock_result:
-                self.assertTrue(
-                    task.GetResult(TEST_STAGE), mock_result[identifier]
-                )
-            results.remove(identifier)
-
-    def testWorker(self):
-        """ "Test the worker method.
-
-        The worker should process all the input tasks and output the tasks to the
-        helper and result queue.
-        """
-
-        manager = multiprocessing.Manager()
-        result_queue = manager.Queue()
-        completed_queue = manager.Queue()
-
-        # A dictionary defines the mock tasks and their corresponding results.
-        mock_work_tasks = {1: 86, 2: 788}
-
-        mock_tasks = []
-
-        for flag, cost in mock_work_tasks.iteritems():
-            mock_tasks.append(MockTask(TEST_STAGE, flag, cost))
-
-        # Submit the mock tasks to the worker.
-        for mock_task in mock_tasks:
-            pipeline_worker.Worker(
-                TEST_STAGE, mock_task, completed_queue, result_queue
-            )
-
-        # The tasks, from the output queue, should be the same as the input and
-        # should be performed.
-        for task in mock_tasks:
-            output = result_queue.get()
-            self.assertEqual(output, task)
-            self.assertTrue(output.Done(TEST_STAGE))
-
-        # The tasks, from the completed_queue, should be defined in the
-        # mock_work_tasks dictionary.
-        for flag, cost in mock_work_tasks.iteritems():
-            helper_input = completed_queue.get()
-            self.assertEqual(helper_input, (flag, cost))
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/bestflags/steering.py b/bestflags/steering.py
deleted file mode 100644
index ead2516b..00000000
--- a/bestflags/steering.py
+++ /dev/null
@@ -1,116 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""The framework stage that produces the next generation of tasks to run.
-
-Part of the Chrome build flags optimization.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import pipeline_process
-
-
-def Steering(cache, generations, input_queue, result_queue):
-    """The core method template that produces the next generation of tasks to run.
-
-    This method waits for the results of the tasks from the previous generation.
-    Upon the arrival of all these results, the method uses them to generate the
-    next generation of tasks.
-
-    The main logic of producing the next generation from previous generation is
-    application specific. For example, in the genetic algorithm, a task is
-    produced by combining two parents tasks, while in the hill climbing algorithm,
-    a task is generated by its immediate neighbor. The method 'Next' is overridden
-    in the concrete subclasses of the class Generation to produce the next
-    application-specific generation. The steering method invokes the 'Next'
-    method, produces the next generation and submits the tasks in this generation
-    to the next stage, e.g., the build/compilation stage.
-
-    Args:
-      cache: It stores the experiments that have been conducted before. Used to
-        avoid duplicate works.
-      generations: The initial generations of tasks to be run.
-      input_queue: The input results from the last stage of the framework. These
-        results will trigger new iteration of the algorithm.
-      result_queue: The output task queue for this pipeline stage. The new tasks
-        generated by the steering algorithm will be sent to the next stage via
-        this queue.
-    """
-
-    # Generations that have pending tasks to be executed. Pending tasks are those
-    # whose results are not ready. The tasks that have their results ready are
-    # referenced to as ready tasks. Once there is no pending generation, the
-    # algorithm terminates.
-    waiting = generations
-
-    # Record how many initial tasks there are. If there is no task at all, the
-    # algorithm can terminate right away.
-    num_tasks = 0
-
-    # Submit all the tasks in the initial generations to the next stage of the
-    # framework. The next stage can be the build/compilation stage.
-    for generation in generations:
-        # Only send the task that has not been performed before to the next stage.
-        for task in [task for task in generation.Pool() if task not in cache]:
-            result_queue.put(task)
-            cache.add(task)
-            num_tasks += 1
-
-    # If there is no task to be executed at all, the algorithm returns right away.
-    if not num_tasks:
-        # Inform the next stage that there will be no more task.
-        result_queue.put(pipeline_process.POISONPILL)
-        return
-
-    # The algorithm is done if there is no pending generation. A generation is
-    # pending if it has pending task.
-    while waiting:
-        # Busy-waiting for the next task.
-        if input_queue.empty():
-            continue
-
-        # If there is a task whose result is ready from the last stage of the
-        # feedback loop, there will be one less pending task.
-
-        task = input_queue.get()
-
-        # Store the result of this ready task. Intermediate results can be used to
-        # generate report for final result or be used to reboot from a crash from
-        # the failure of any module of the framework.
-        task.LogSteeringCost()
-
-        # Find out which pending generation this ready task belongs to. This pending
-        # generation will have one less pending task. The "next" expression iterates
-        # the generations in waiting until the first generation whose UpdateTask
-        # method returns true.
-        generation = next(gen for gen in waiting if gen.UpdateTask(task))
-
-        # If there is still any pending task, do nothing.
-        if not generation.Done():
-            continue
-
-        # All the tasks in the generation are finished. The generation is ready to
-        # produce the next generation.
-        waiting.remove(generation)
-
-        # Check whether a generation should generate the next generation.
-        # A generation may not generate the next generation, e.g., because a
-        # fixpoint has been reached, there has not been any improvement for a few
-        # generations or a local maxima is reached.
-        if not generation.IsImproved():
-            continue
-
-        for new_generation in generation.Next(cache):
-            # Make sure that each generation should contain at least one task.
-            assert new_generation.Pool()
-            waiting.append(new_generation)
-
-            # Send the tasks of the new generations to the next stage for execution.
-            for new_task in new_generation.Pool():
-                result_queue.put(new_task)
-                cache.add(new_task)
-
-    # Steering algorithm is finished and it informs the next stage that there will
-    # be no more task.
-    result_queue.put(pipeline_process.POISONPILL)
diff --git a/bestflags/steering_test.py b/bestflags/steering_test.py
deleted file mode 100644
index 28a2f108..00000000
--- a/bestflags/steering_test.py
+++ /dev/null
@@ -1,184 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""Steering stage unittest.
-
-Part of the Chrome build flags optimization.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import multiprocessing
-import unittest
-
-from generation import Generation
-from mock_task import IdentifierMockTask
-import pipeline_process
-import steering
-
-
-# Pick an integer at random.
-STEERING_TEST_STAGE = -8
-
-# The number of generations to be used to do the testing.
-NUMBER_OF_GENERATIONS = 20
-
-# The number of tasks to be put in each generation to be tested.
-NUMBER_OF_TASKS = 20
-
-# The stride of permutation used to shuffle the input list of tasks. Should be
-# relatively prime with NUMBER_OF_TASKS.
-STRIDE = 7
-
-
-class MockGeneration(Generation):
-    """This class emulates an actual generation.
-
-    It will output the next_generations when the method Next is called. The
-    next_generations is initiated when the MockGeneration instance is constructed.
-    """
-
-    def __init__(self, tasks, next_generations):
-        """Set up the next generations for this task.
-
-        Args:
-          tasks: A set of tasks to be run.
-          next_generations: A list of generations as the next generation of the
-            current generation.
-        """
-        Generation.__init__(self, tasks, None)
-        self._next_generations = next_generations
-
-    def Next(self, _):
-        return self._next_generations
-
-    def IsImproved(self):
-        if self._next_generations:
-            return True
-        return False
-
-
-class SteeringTest(unittest.TestCase):
-    """This class test the steering method.
-
-    The steering algorithm should return if there is no new task in the initial
-    generation. The steering algorithm should send all the tasks to the next stage
-    and should terminate once there is no pending generation. A generation is
-    pending if it contains pending task. A task is pending if its (test) result
-    is not ready.
-    """
-
-    def testSteering(self):
-        """Test that the steering algorithm processes all the tasks properly.
-
-        Test that the steering algorithm sends all the tasks to the next stage. Test
-        that the steering algorithm terminates once all the tasks have been
-        processed, i.e., the results for the tasks are all ready.
-        """
-
-        # A list of generations used to test the steering stage.
-        generations = []
-
-        task_index = 0
-        previous_generations = None
-
-        # Generate a sequence of generations to be tested. Each generation will
-        # output the next generation in reverse order of the list when the "Next"
-        # method is called.
-        for _ in range(NUMBER_OF_GENERATIONS):
-            # Use a consecutive sequence of numbers as identifiers for the set of
-            # tasks put into a generation.
-            test_ranges = range(task_index, task_index + NUMBER_OF_TASKS)
-            tasks = [
-                IdentifierMockTask(STEERING_TEST_STAGE, t) for t in test_ranges
-            ]
-            steering_tasks = set(tasks)
-
-            # Let the previous generation as the offspring generation of the current
-            # generation.
-            current_generation = MockGeneration(
-                steering_tasks, previous_generations
-            )
-            generations.insert(0, current_generation)
-            previous_generations = [current_generation]
-
-            task_index += NUMBER_OF_TASKS
-
-        # If there is no generation at all, the unittest returns right away.
-        if not current_generation:
-            return
-
-        # Set up the input and result queue for the steering method.
-        manager = multiprocessing.Manager()
-        input_queue = manager.Queue()
-        result_queue = manager.Queue()
-
-        steering_process = multiprocessing.Process(
-            target=steering.Steering,
-            args=(set(), [current_generation], input_queue, result_queue),
-        )
-        steering_process.start()
-
-        # Test that each generation is processed properly. I.e., the generations are
-        # processed in order.
-        while generations:
-            generation = generations.pop(0)
-            tasks = [task for task in generation.Pool()]
-
-            # Test that all the tasks are processed once and only once.
-            while tasks:
-                task = result_queue.get()
-
-                assert task in tasks
-                tasks.remove(task)
-
-                input_queue.put(task)
-
-        task = result_queue.get()
-
-        # Test that the steering algorithm returns properly after processing all
-        # the generations.
-        assert task == pipeline_process.POISONPILL
-
-        steering_process.join()
-
-    def testCache(self):
-        """The steering algorithm returns immediately if there is no new tasks.
-
-        If all the new tasks have been cached before, the steering algorithm does
-        not have to execute these tasks again and thus can terminate right away.
-        """
-
-        # Put a set of tasks in the cache and add this set to initial generation.
-        test_ranges = range(NUMBER_OF_TASKS)
-        tasks = [
-            IdentifierMockTask(STEERING_TEST_STAGE, t) for t in test_ranges
-        ]
-        steering_tasks = set(tasks)
-
-        current_generation = MockGeneration(steering_tasks, None)
-
-        # Set up the input and result queue for the steering method.
-        manager = multiprocessing.Manager()
-        input_queue = manager.Queue()
-        result_queue = manager.Queue()
-
-        steering_process = multiprocessing.Process(
-            target=steering.Steering,
-            args=(
-                steering_tasks,
-                [current_generation],
-                input_queue,
-                result_queue,
-            ),
-        )
-
-        steering_process.start()
-
-        # Test that the steering method returns right away.
-        assert result_queue.get() == pipeline_process.POISONPILL
-        steering_process.join()
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/bestflags/task.py b/bestflags/task.py
deleted file mode 100644
index a7822061..00000000
--- a/bestflags/task.py
+++ /dev/null
@@ -1,494 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""A reproducing entity.
-
-Part of the Chrome build flags optimization.
-
-The Task class is used by different modules. Each module fills in the
-corresponding information into a Task instance. Class Task contains the bit set
-representing the flags selection. The builder module is responsible for filling
-the image and the checksum field of a Task. The executor module will put the
-execution output to the execution field.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import os
-import subprocess
-import sys
-from uuid import uuid4
-
-
-BUILD_STAGE = 1
-TEST_STAGE = 2
-
-# Message indicating that the build or test failed.
-ERROR_STRING = "error"
-
-# The maximum number of tries a build can have. Some compilations may fail due
-# to unexpected environment circumstance. This variable defines how many tries
-# the build should attempt before giving up.
-BUILD_TRIES = 3
-
-# The maximum number of tries a test can have. Some tests may fail due to
-# unexpected environment circumstance. This variable defines how many tries the
-# test should attempt before giving up.
-TEST_TRIES = 3
-
-
-# Create the file/directory if it does not already exist.
-def _CreateDirectory(file_name):
-    directory = os.path.dirname(file_name)
-    if not os.path.exists(directory):
-        os.makedirs(directory)
-
-
-class Task(object):
-    """A single reproducing entity.
-
-    A single test of performance with a particular set of flags. It records the
-    flag set, the image, the check sum of the image and the cost.
-    """
-
-    # The command that will be used in the build stage to compile the tasks.
-    BUILD_COMMAND = None
-    # The command that will be used in the test stage to test the tasks.
-    TEST_COMMAND = None
-    # The directory to log the compilation and test results.
-    LOG_DIRECTORY = None
-
-    @staticmethod
-    def InitLogCommand(build_command, test_command, log_directory):
-        """Set up the build and test command for the task and the log directory.
-
-        This framework is generic. It lets the client specify application specific
-        compile and test methods by passing different build_command and
-        test_command.
-
-        Args:
-          build_command: The command that will be used in the build stage to compile
-            this task.
-          test_command: The command that will be used in the test stage to test this
-            task.
-          log_directory: The directory to log the compilation and test results.
-        """
-
-        Task.BUILD_COMMAND = build_command
-        Task.TEST_COMMAND = test_command
-        Task.LOG_DIRECTORY = log_directory
-
-    def __init__(self, flag_set):
-        """Set up the optimization flag selection for this task.
-
-        Args:
-          flag_set: The optimization flag set that is encapsulated by this task.
-        """
-
-        self._flag_set = flag_set
-
-        # A unique identifier that distinguishes this task from other tasks.
-        self._task_identifier = uuid4()
-
-        self._log_path = (Task.LOG_DIRECTORY, self._task_identifier)
-
-        # Initiate the hash value. The hash value is used so as not to recompute it
-        # every time the hash method is called.
-        self._hash_value = None
-
-        # Indicate that the task has not been compiled/tested.
-        self._build_cost = None
-        self._exe_cost = None
-        self._checksum = None
-        self._image = None
-        self._file_length = None
-        self._text_length = None
-
-    def __eq__(self, other):
-        """Test whether two tasks are equal.
-
-        Two tasks are equal if their flag_set are equal.
-
-        Args:
-          other: The other task with which this task is tested equality.
-        Returns:
-          True if the encapsulated flag sets are equal.
-        """
-        if isinstance(other, Task):
-            return self.GetFlags() == other.GetFlags()
-        return False
-
-    def __hash__(self):
-        if self._hash_value is None:
-            # Cache the hash value of the flags, so as not to recompute them.
-            self._hash_value = hash(self._flag_set)
-        return self._hash_value
-
-    def GetIdentifier(self, stage):
-        """Get the identifier of the task in the stage.
-
-        The flag set uniquely identifies a task in the build stage. The checksum of
-        the image of the task uniquely identifies the task in the test stage.
-
-        Args:
-          stage: The stage (build/test) in which this method is called.
-        Returns:
-          Return the flag set in build stage and return the checksum in test stage.
-        """
-
-        # Define the dictionary for different stage function lookup.
-        get_identifier_functions = {
-            BUILD_STAGE: self.FormattedFlags,
-            TEST_STAGE: self.__GetCheckSum,
-        }
-
-        assert stage in get_identifier_functions
-        return get_identifier_functions[stage]()
-
-    def GetResult(self, stage):
-        """Get the performance results of the task in the stage.
-
-        Args:
-          stage: The stage (build/test) in which this method is called.
-        Returns:
-          Performance results.
-        """
-
-        # Define the dictionary for different stage function lookup.
-        get_result_functions = {
-            BUILD_STAGE: self.__GetBuildResult,
-            TEST_STAGE: self.GetTestResult,
-        }
-
-        assert stage in get_result_functions
-
-        return get_result_functions[stage]()
-
-    def SetResult(self, stage, result):
-        """Set the performance results of the task in the stage.
-
-        This method is called by the pipeling_worker to set the results for
-        duplicated tasks.
-
-        Args:
-          stage: The stage (build/test) in which this method is called.
-          result: The performance results of the stage.
-        """
-
-        # Define the dictionary for different stage function lookup.
-        set_result_functions = {
-            BUILD_STAGE: self.__SetBuildResult,
-            TEST_STAGE: self.__SetTestResult,
-        }
-
-        assert stage in set_result_functions
-
-        set_result_functions[stage](result)
-
-    def Done(self, stage):
-        """Check whether the stage is done.
-
-        Args:
-          stage: The stage to be checked, build or test.
-        Returns:
-          True if the stage is done.
-        """
-
-        # Define the dictionary for different result string lookup.
-        done_string = {
-            BUILD_STAGE: self._build_cost,
-            TEST_STAGE: self._exe_cost,
-        }
-
-        assert stage in done_string
-
-        return done_string[stage] is not None
-
-    def Work(self, stage):
-        """Perform the task.
-
-        Args:
-          stage: The stage in which the task is performed, compile or test.
-        """
-
-        # Define the dictionary for different stage function lookup.
-        work_functions = {BUILD_STAGE: self.__Compile, TEST_STAGE: self.__Test}
-
-        assert stage in work_functions
-
-        work_functions[stage]()
-
-    def FormattedFlags(self):
-        """Format the optimization flag set of this task.
-
-        Returns:
-          The formatted optimization flag set that is encapsulated by this task.
-        """
-        return str(self._flag_set.FormattedForUse())
-
-    def GetFlags(self):
-        """Get the optimization flag set of this task.
-
-        Returns:
-          The optimization flag set that is encapsulated by this task.
-        """
-
-        return self._flag_set
-
-    def __GetCheckSum(self):
-        """Get the compilation image checksum of this task.
-
-        Returns:
-          The compilation image checksum of this task.
-        """
-
-        # The checksum should be computed before this method is called.
-        assert self._checksum is not None
-        return self._checksum
-
-    def __Compile(self):
-        """Run a compile.
-
-        This method compile an image using the present flags, get the image,
-        test the existent of the image and gathers monitoring information, and sets
-        the internal cost (fitness) for this set of flags.
-        """
-
-        # Format the flags as a string as input to compile command. The unique
-        # identifier is passed to the compile command. If concurrent processes are
-        # used to compile different tasks, these processes can use the identifier to
-        # write to different file.
-        flags = self._flag_set.FormattedForUse()
-        command = "%s %s %s" % (
-            Task.BUILD_COMMAND,
-            " ".join(flags),
-            self._task_identifier,
-        )
-
-        # Try BUILD_TRIES number of times before confirming that the build fails.
-        for _ in range(BUILD_TRIES):
-            try:
-                # Execute the command and get the execution status/results.
-                p = subprocess.Popen(
-                    command.split(),
-                    stdout=subprocess.PIPE,
-                    stderr=subprocess.PIPE,
-                )
-                (out, err) = p.communicate()
-
-                if out:
-                    out = out.strip()
-                    if out != ERROR_STRING:
-                        # Each build results contains the checksum of the result image, the
-                        # performance cost of the build, the compilation image, the length
-                        # of the build, and the length of the text section of the build.
-                        (
-                            checksum,
-                            cost,
-                            image,
-                            file_length,
-                            text_length,
-                        ) = out.split()
-                        # Build successfully.
-                        break
-
-                # Build failed.
-                cost = ERROR_STRING
-            except _:
-                # If there is exception getting the cost information of the build, the
-                # build failed.
-                cost = ERROR_STRING
-
-        # Convert the build cost from String to integer. The build cost is used to
-        # compare a task with another task. Set the build cost of the failing task
-        # to the max integer. The for loop will keep trying until either there is a
-        # success or BUILD_TRIES number of tries have been conducted.
-        self._build_cost = sys.maxint if cost == ERROR_STRING else float(cost)
-
-        self._checksum = checksum
-        self._file_length = file_length
-        self._text_length = text_length
-        self._image = image
-
-        self.__LogBuildCost(err)
-
-    def __Test(self):
-        """__Test the task against benchmark(s) using the input test command."""
-
-        # Ensure that the task is compiled before being tested.
-        assert self._image is not None
-
-        # If the task does not compile, no need to test.
-        if self._image == ERROR_STRING:
-            self._exe_cost = ERROR_STRING
-            return
-
-        # The unique identifier is passed to the test command. If concurrent
-        # processes are used to compile different tasks, these processes can use the
-        # identifier to write to different file.
-        command = "%s %s %s" % (
-            Task.TEST_COMMAND,
-            self._image,
-            self._task_identifier,
-        )
-
-        # Try TEST_TRIES number of times before confirming that the build fails.
-        for _ in range(TEST_TRIES):
-            try:
-                p = subprocess.Popen(
-                    command.split(),
-                    stdout=subprocess.PIPE,
-                    stderr=subprocess.PIPE,
-                )
-                (out, err) = p.communicate()
-
-                if out:
-                    out = out.strip()
-                    if out != ERROR_STRING:
-                        # The test results contains the performance cost of the test.
-                        cost = out
-                        # Test successfully.
-                        break
-
-                # Test failed.
-                cost = ERROR_STRING
-            except _:
-                # If there is exception getting the cost information of the test, the
-                # test failed. The for loop will keep trying until either there is a
-                # success or TEST_TRIES number of tries have been conducted.
-                cost = ERROR_STRING
-
-        self._exe_cost = sys.maxint if (cost == ERROR_STRING) else float(cost)
-
-        self.__LogTestCost(err)
-
-    def __SetBuildResult(
-        self, (checksum, build_cost, image, file_length, text_length)
-    ):
-        self._checksum = checksum
-        self._build_cost = build_cost
-        self._image = image
-        self._file_length = file_length
-        self._text_length = text_length
-
-    def __GetBuildResult(self):
-        return (
-            self._checksum,
-            self._build_cost,
-            self._image,
-            self._file_length,
-            self._text_length,
-        )
-
-    def GetTestResult(self):
-        return self._exe_cost
-
-    def __SetTestResult(self, exe_cost):
-        self._exe_cost = exe_cost
-
-    def LogSteeringCost(self):
-        """Log the performance results for the task.
-
-        This method is called by the steering stage and this method writes the
-        results out to a file. The results include the build and the test results.
-        """
-
-        steering_log = "%s/%s/steering.txt" % self._log_path
-
-        _CreateDirectory(steering_log)
-
-        with open(steering_log, "w") as out_file:
-            # Include the build and the test results.
-            steering_result = (
-                self._flag_set,
-                self._checksum,
-                self._build_cost,
-                self._image,
-                self._file_length,
-                self._text_length,
-                self._exe_cost,
-            )
-
-            # Write out the result in the comma-separated format (CSV).
-            out_file.write("%s,%s,%s,%s,%s,%s,%s\n" % steering_result)
-
-    def __LogBuildCost(self, log):
-        """Log the build results for the task.
-
-        The build results include the compilation time of the build, the result
-        image, the checksum, the file length and the text length of the image.
-        The file length of the image includes the length of the file of the image.
-        The text length only includes the length of the text section of the image.
-
-        Args:
-          log: The build log of this task.
-        """
-
-        build_result_log = "%s/%s/build.txt" % self._log_path
-
-        _CreateDirectory(build_result_log)
-
-        with open(build_result_log, "w") as out_file:
-            build_result = (
-                self._flag_set,
-                self._build_cost,
-                self._image,
-                self._checksum,
-                self._file_length,
-                self._text_length,
-            )
-
-            # Write out the result in the comma-separated format (CSV).
-            out_file.write("%s,%s,%s,%s,%s,%s\n" % build_result)
-
-        # The build information about running the build.
-        build_run_log = "%s/%s/build_log.txt" % self._log_path
-        _CreateDirectory(build_run_log)
-
-        with open(build_run_log, "w") as out_log_file:
-            # Write out the execution information.
-            out_log_file.write("%s" % log)
-
-    def __LogTestCost(self, log):
-        """Log the test results for the task.
-
-        The test results include the runtime execution time of the test.
-
-        Args:
-          log: The test log of this task.
-        """
-
-        test_log = "%s/%s/test.txt" % self._log_path
-
-        _CreateDirectory(test_log)
-
-        with open(test_log, "w") as out_file:
-            test_result = (self._flag_set, self._checksum, self._exe_cost)
-
-            # Write out the result in the comma-separated format (CSV).
-            out_file.write("%s,%s,%s\n" % test_result)
-
-        # The execution information about running the test.
-        test_run_log = "%s/%s/test_log.txt" % self._log_path
-
-        _CreateDirectory(test_run_log)
-
-        with open(test_run_log, "w") as out_log_file:
-            # Append the test log information.
-            out_log_file.write("%s" % log)
-
-    def IsImproved(self, other):
-        """Compare the current task with another task.
-
-        Args:
-          other: The other task against which the current task is compared.
-
-        Returns:
-          True if this task has improvement upon the other task.
-        """
-
-        # The execution costs must have been initiated.
-        assert self._exe_cost is not None
-        assert other.GetTestResult() is not None
-
-        return self._exe_cost < other.GetTestResult()
diff --git a/bestflags/task_test.py b/bestflags/task_test.py
deleted file mode 100644
index f151bc78..00000000
--- a/bestflags/task_test.py
+++ /dev/null
@@ -1,185 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""Task unittest.
-
-Part of the Chrome build flags optimization.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import random
-import sys
-import unittest
-
-import task
-from task import Task
-
-
-# The number of flags be tested.
-NUM_FLAGS = 20
-
-# The random build result values used to test get set result method.
-RANDOM_BUILD_RESULT = 100
-
-# The random test result values used to test get set result method.
-RANDOM_TESTRESULT = 100
-
-
-class MockFlagSet(object):
-    """This class emulates a set of flags.
-
-    It returns the flags and hash value, when the FormattedForUse method and the
-    __hash__ method is called, respectively. These values are initialized when the
-    MockFlagSet instance is constructed.
-    """
-
-    def __init__(self, flags=0, hash_value=-1):
-        self._flags = flags
-        self._hash_value = hash_value
-
-    def __eq__(self, other):
-        assert isinstance(other, MockFlagSet)
-        return self._flags == other.FormattedForUse()
-
-    def FormattedForUse(self):
-        return self._flags
-
-    def __hash__(self):
-        return self._hash_value
-
-    def GetHash(self):
-        return self._hash_value
-
-
-class TaskTest(unittest.TestCase):
-    """This class test the Task class."""
-
-    def testEqual(self):
-        """Test the equal method of the task.
-
-        Two tasks are equal if and only if their encapsulated flag_sets are equal.
-        """
-
-        flags = range(NUM_FLAGS)
-
-        # Two tasks having the same flag set should be equivalent.
-        flag_sets = [MockFlagSet(flag) for flag in flags]
-        for flag_set in flag_sets:
-            assert Task(flag_set) == Task(flag_set)
-
-        # Two tasks having different flag set should be different.
-        for flag_set in flag_sets:
-            test_task = Task(flag_set)
-            other_flag_sets = [
-                flags for flags in flag_sets if flags != flag_set
-            ]
-            for flag_set1 in other_flag_sets:
-                assert test_task != Task(flag_set1)
-
-    def testHash(self):
-        """Test the hash method of the task.
-
-        Two tasks are equal if and only if their encapsulated flag_sets are equal.
-        """
-
-        # Random identifier that is not relevant in this test.
-        identifier = random.randint(-sys.maxint - 1, -1)
-
-        flag_sets = [
-            MockFlagSet(identifier, value) for value in range(NUM_FLAGS)
-        ]
-        for flag_set in flag_sets:
-            # The hash of a task is the same as the hash of its flag set.
-            hash_task = Task(flag_set)
-            hash_value = hash(hash_task)
-            assert hash_value == flag_set.GetHash()
-
-            # The hash of a task does not change.
-            assert hash_value == hash(hash_task)
-
-    def testGetIdentifier(self):
-        """Test the get identifier method of the task.
-
-        The get identifier method should returns the flag set in the build stage.
-        """
-
-        flag_sets = [MockFlagSet(flag) for flag in range(NUM_FLAGS)]
-        for flag_set in flag_sets:
-            identifier_task = Task(flag_set)
-
-            identifier = identifier_task.GetIdentifier(task.BUILD_STAGE)
-
-            # The task formats the flag set into a string.
-            assert identifier == str(flag_set.FormattedForUse())
-
-    def testGetSetResult(self):
-        """Test the get and set result methods of the task.
-
-        The get result method should return the same results as were set.
-        """
-
-        flag_sets = [MockFlagSet(flag) for flag in range(NUM_FLAGS)]
-        for flag_set in flag_sets:
-            result_task = Task(flag_set)
-
-            # The get result method should return the same results as were set, in
-            # build stage. Currently, the build result is a 5-element tuple containing
-            # the checksum of the result image, the performance cost of the build, the
-            # compilation image, the length of the build, and the length of the text
-            # section of the build.
-            result = tuple(
-                [random.randint(0, RANDOM_BUILD_RESULT) for _ in range(5)]
-            )
-            result_task.SetResult(task.BUILD_STAGE, result)
-            assert result == result_task.GetResult(task.BUILD_STAGE)
-
-            # The checksum is the identifier of the test stage.
-            identifier = result_task.GetIdentifier(task.TEST_STAGE)
-            # The first element of the result tuple is the checksum.
-            assert identifier == result[0]
-
-            # The get result method should return the same results as were set, in
-            # test stage.
-            random_test_result = random.randint(0, RANDOM_TESTRESULT)
-            result_task.SetResult(task.TEST_STAGE, random_test_result)
-            test_result = result_task.GetResult(task.TEST_STAGE)
-            assert test_result == random_test_result
-
-    def testDone(self):
-        """Test the done methods of the task.
-
-        The done method should return false is the task has not perform and return
-        true after the task is finished.
-        """
-
-        flags = range(NUM_FLAGS)
-
-        flag_sets = [MockFlagSet(flag) for flag in flags]
-        for flag_set in flag_sets:
-            work_task = Task(flag_set)
-
-            # The task has not been compiled nor tested.
-            assert not work_task.Done(task.TEST_STAGE)
-            assert not work_task.Done(task.BUILD_STAGE)
-
-            # After the task has been compiled, it should indicate finished in BUILD
-            # stage.
-            result = tuple(
-                [random.randint(0, RANDOM_BUILD_RESULT) for _ in range(5)]
-            )
-            work_task.SetResult(task.BUILD_STAGE, result)
-            assert not work_task.Done(task.TEST_STAGE)
-            assert work_task.Done(task.BUILD_STAGE)
-
-            # After the task has been tested, it should indicate finished in TEST
-            # stage.
-            work_task.SetResult(
-                task.TEST_STAGE, random.randint(0, RANDOM_TESTRESULT)
-            )
-            assert work_task.Done(task.TEST_STAGE)
-            assert work_task.Done(task.BUILD_STAGE)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/bestflags/testing_batch.py b/bestflags/testing_batch.py
deleted file mode 100644
index 783d95bf..00000000
--- a/bestflags/testing_batch.py
+++ /dev/null
@@ -1,456 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-"""Hill climbing unitest.
-
-Part of the Chrome build flags optimization.
-
-Test the best branching hill climbing algorithms, genetic algorithm and
-iterative elimination algorithm.
-"""
-
-__author__ = "yuhenglong@google.com (Yuheng Long)"
-
-import multiprocessing
-import random
-import sys
-import unittest
-
-import flags
-from flags import Flag
-from flags import FlagSet
-from genetic_algorithm import GAGeneration
-from genetic_algorithm import GATask
-from hill_climb_best_neighbor import HillClimbingBestBranch
-from iterative_elimination import IterativeEliminationFirstGeneration
-import pipeline_process
-from steering import Steering
-from task import BUILD_STAGE
-from task import Task
-from task import TEST_STAGE
-
-
-# The number of flags be tested.
-NUM_FLAGS = 5
-
-# The value range of the flags.
-FLAG_RANGES = 10
-
-# The following variables are meta data for the Genetic Algorithm.
-STOP_THRESHOLD = 20
-NUM_CHROMOSOMES = 10
-NUM_TRIALS = 20
-MUTATION_RATE = 0.03
-
-
-def _GenerateRandomRasks(specs):
-    """Generate a task that has random values.
-
-    Args:
-      specs: A list of spec from which the flag set is created.
-
-    Returns:
-      A set containing a task that has random values.
-    """
-
-    flag_set = []
-
-    for spec in specs:
-        numeric_flag_match = flags.Search(spec)
-        if numeric_flag_match:
-            # Numeric flags.
-            start = int(numeric_flag_match.group("start"))
-            end = int(numeric_flag_match.group("end"))
-
-            value = random.randint(start - 1, end - 1)
-            if value != start - 1:
-                # If the value falls in the range, this flag is enabled.
-                flag_set.append(Flag(spec, value))
-        else:
-            # Boolean flags.
-            if random.randint(0, 1):
-                flag_set.append(Flag(spec))
-
-    return set([Task(FlagSet(flag_set))])
-
-
-def _GenerateAllFlagsTasks(specs):
-    """Generate a task that all the flags are enable.
-
-    All the boolean flags in the specs will be enabled and all the numeric flag
-    with have the largest legal value.
-
-    Args:
-      specs: A list of spec from which the flag set is created.
-
-    Returns:
-      A set containing a task that has all flags enabled.
-    """
-
-    flag_set = []
-
-    for spec in specs:
-        numeric_flag_match = flags.Search(spec)
-
-        if numeric_flag_match:
-            value = int(numeric_flag_match.group("end")) - 1
-        else:
-            value = -1
-        flag_set.append(Flag(spec, value))
-
-    return set([Task(FlagSet(flag_set))])
-
-
-def _GenerateNoFlagTask():
-    return set([Task(FlagSet([]))])
-
-
-def GenerateRandomGATasks(specs, num_tasks, num_trials):
-    """Generate a set of tasks for the Genetic Algorithm.
-
-    Args:
-      specs: A list of spec from which the flag set is created.
-      num_tasks: number of tasks that should be generated.
-      num_trials: the maximum number of tries should be attempted to generate the
-        set of tasks.
-
-    Returns:
-      A set of randomly generated tasks.
-    """
-
-    tasks = set([])
-
-    total_trials = 0
-    while len(tasks) < num_tasks and total_trials < num_trials:
-        new_flag = FlagSet(
-            [Flag(spec) for spec in specs if random.randint(0, 1)]
-        )
-        new_task = GATask(new_flag)
-
-        if new_task in tasks:
-            total_trials += 1
-        else:
-            tasks.add(new_task)
-            total_trials = 0
-
-    return tasks
-
-
-def _GenerateInitialFlags(specs, spec):
-    """Generate the flag_set of a task in the flag elimination algorithm.
-
-    Set the value of all the flags to the largest value, except for the flag that
-    contains spec.
-
-    For example, if the specs are [-finline-limit=[1-1000], -fstrict-aliasing] and
-    the spec is -finline-limit=[1-1000], then the result is
-    [-finline-limit=[1-1000]:-finline-limit=998,
-     -fstrict-aliasing:-fstrict-aliasing]
-
-    Args:
-      specs: an array of specifications from which the result flag_set is created.
-        The flag_set contains one and only one flag that contain the specification
-        spec.
-      spec: The flag containing this spec should have a value that is smaller than
-        the highest value the flag can have.
-
-    Returns:
-      An array of flags, each of which contains one spec in specs. All the values
-      of the flags are the largest values in specs, expect the one that contains
-      spec.
-    """
-
-    flag_set = []
-    for other_spec in specs:
-        numeric_flag_match = flags.Search(other_spec)
-        # Found the spec in the array specs.
-        if other_spec == spec:
-            # Numeric flag will have a value that is smaller than the largest value
-            # and Boolean flag will be deleted.
-            if numeric_flag_match:
-                end = int(numeric_flag_match.group("end"))
-                flag_set.append(flags.Flag(other_spec, end - 2))
-
-            continue
-
-        # other_spec != spec
-        if numeric_flag_match:
-            # numeric flag
-            end = int(numeric_flag_match.group("end"))
-            flag_set.append(flags.Flag(other_spec, end - 1))
-            continue
-
-        # boolean flag
-        flag_set.append(flags.Flag(other_spec))
-
-    return flag_set
-
-
-def _GenerateAllIterativeEliminationTasks(specs):
-    """Generate the initial tasks for the negative flag elimination algorithm.
-
-    Generate the base line task that turns on all the boolean flags and sets the
-    value to be the largest value for the numeric flag.
-
-    For example, if the specs are [-finline-limit=[1-1000], -fstrict-aliasing],
-    the base line is [-finline-limit=[1-1000]:-finline-limit=999,
-    -fstrict-aliasing:-fstrict-aliasing]
-
-    Generate a set of task, each turns off one of the flag or sets a value that is
-    smaller than the largest value for the flag.
-
-    Args:
-      specs: an array of specifications from which the result flag_set is created.
-
-    Returns:
-      An array containing one generation of the initial tasks for the negative
-      flag elimination algorithm.
-    """
-
-    # The set of tasks to be generated.
-    results = set([])
-    flag_set = []
-
-    for spec in specs:
-        numeric_flag_match = flags.Search(spec)
-        if numeric_flag_match:
-            # Numeric flag.
-            end_value = int(numeric_flag_match.group("end"))
-            flag_set.append(flags.Flag(spec, end_value - 1))
-            continue
-
-        # Boolean flag.
-        flag_set.append(flags.Flag(spec))
-
-    # The base line task that set all the flags to their largest values.
-    parent_task = Task(flags.FlagSet(flag_set))
-    results.add(parent_task)
-
-    for spec in specs:
-        results.add(Task(flags.FlagSet(_GenerateInitialFlags(specs, spec))))
-
-    return [IterativeEliminationFirstGeneration(results, parent_task)]
-
-
-def _ComputeCost(cost_func, specs, flag_set):
-    """Compute the mock cost of the flag_set using the input cost function.
-
-    All the boolean flags in the specs will be enabled and all the numeric flag
-    with have the largest legal value.
-
-    Args:
-      cost_func: The cost function which is used to compute the mock cost of a
-        dictionary of flags.
-      specs: All the specs that are used in the algorithm. This is used to check
-        whether certain flag is disabled in the flag_set dictionary.
-      flag_set: a dictionary of the spec and flag pairs.
-
-    Returns:
-      The mock cost of the input dictionary of the flags.
-    """
-
-    values = []
-
-    for spec in specs:
-        # If a flag is enabled, its value is added. Otherwise a padding 0 is added.
-        values.append(flag_set[spec].GetValue() if spec in flag_set else 0)
-
-    # The cost function string can use the values array.
-    return eval(cost_func)
-
-
-def _GenerateTestFlags(num_flags, upper_bound, file_name):
-    """Generate a set of mock flags and write it to a configuration file.
-
-    Generate a set of mock flags
-
-    Args:
-      num_flags: Number of numeric flags to be generated.
-      upper_bound: The value of the upper bound of the range.
-      file_name: The configuration file name into which the mock flags are put.
-    """
-
-    with open(file_name, "w") as output_file:
-        num_flags = int(num_flags)
-        upper_bound = int(upper_bound)
-        for i in range(num_flags):
-            output_file.write("%s=[1-%d]\n" % (i, upper_bound))
-
-
-def _TestAlgorithm(cost_func, specs, generations, best_result):
-    """Test the best result the algorithm should return.
-
-    Set up the framework, run the input algorithm and verify the result.
-
-    Args:
-      cost_func: The cost function which is used to compute the mock cost of a
-        dictionary of flags.
-      specs: All the specs that are used in the algorithm. This is used to check
-        whether certain flag is disabled in the flag_set dictionary.
-      generations: The initial generations to be evaluated.
-      best_result: The expected best result of the algorithm. If best_result is
-        -1, the algorithm may or may not return the best value. Therefore, no
-        assertion will be inserted.
-    """
-
-    # Set up the utilities to test the framework.
-    manager = multiprocessing.Manager()
-    input_queue = manager.Queue()
-    output_queue = manager.Queue()
-    pp_steer = multiprocessing.Process(
-        target=Steering, args=(set(), generations, output_queue, input_queue)
-    )
-    pp_steer.start()
-
-    # The best result of the algorithm so far.
-    result = sys.maxint
-
-    while True:
-        task = input_queue.get()
-
-        # POISONPILL signal the ends of the algorithm.
-        if task == pipeline_process.POISONPILL:
-            break
-
-        task.SetResult(BUILD_STAGE, (0, 0, 0, 0, 0))
-
-        # Compute the mock cost for the task.
-        task_result = _ComputeCost(cost_func, specs, task.GetFlags())
-        task.SetResult(TEST_STAGE, task_result)
-
-        # If the mock result of the current task is the best so far, set this
-        # result to be the best result.
-        if task_result < result:
-            result = task_result
-
-        output_queue.put(task)
-
-    pp_steer.join()
-
-    # Only do this test when best_result is not -1.
-    if best_result != -1:
-        assert best_result == result
-
-
-class MockAlgorithmsTest(unittest.TestCase):
-    """This class mock tests different steering algorithms.
-
-    The steering algorithms are responsible for generating the next set of tasks
-    to run in each iteration. This class does a functional testing on the
-    algorithms. It mocks out the computation of the fitness function from the
-    build and test phases by letting the user define the fitness function.
-    """
-
-    def _GenerateFlagSpecifications(self):
-        """Generate the testing specifications."""
-
-        mock_test_file = "scale_mock_test"
-        _GenerateTestFlags(NUM_FLAGS, FLAG_RANGES, mock_test_file)
-        return flags.ReadConf(mock_test_file)
-
-    def testBestHillClimb(self):
-        """Test the best hill climb algorithm.
-
-        Test whether it finds the best results as expected.
-        """
-
-        # Initiate the build/test command and the log directory.
-        Task.InitLogCommand(None, None, "output")
-
-        # Generate the testing specs.
-        specs = self._GenerateFlagSpecifications()
-
-        # Generate the initial generations for a test whose cost function is the
-        # summation of the values of all the flags.
-        generation_tasks = _GenerateAllFlagsTasks(specs)
-        generations = [HillClimbingBestBranch(generation_tasks, set([]), specs)]
-
-        # Test the algorithm. The cost function is the summation of all the values
-        # of all the flags. Therefore, the best value is supposed to be 0, i.e.,
-        # when all the flags are disabled.
-        _TestAlgorithm("sum(values[0:len(values)])", specs, generations, 0)
-
-        # This test uses a cost function that is the negative of the previous cost
-        # function. Therefore, the best result should be found in task with all the
-        # flags enabled.
-        cost_function = "sys.maxint - sum(values[0:len(values)])"
-        all_flags = list(generation_tasks)[0].GetFlags()
-        cost = _ComputeCost(cost_function, specs, all_flags)
-
-        # Generate the initial generations.
-        generation_tasks = _GenerateNoFlagTask()
-        generations = [HillClimbingBestBranch(generation_tasks, set([]), specs)]
-
-        # Test the algorithm. The cost function is negative of the summation of all
-        # the values of all the flags. Therefore, the best value is supposed to be
-        # 0, i.e., when all the flags are disabled.
-        _TestAlgorithm(cost_function, specs, generations, cost)
-
-    def testGeneticAlgorithm(self):
-        """Test the Genetic Algorithm.
-
-        Do a functional testing here and see how well it scales.
-        """
-
-        # Initiate the build/test command and the log directory.
-        Task.InitLogCommand(None, None, "output")
-
-        # Generate the testing specs.
-        specs = self._GenerateFlagSpecifications()
-        # Initiate the build/test command and the log directory.
-        GAGeneration.InitMetaData(
-            STOP_THRESHOLD, NUM_CHROMOSOMES, NUM_TRIALS, specs, MUTATION_RATE
-        )
-
-        # Generate the initial generations.
-        generation_tasks = GenerateRandomGATasks(
-            specs, NUM_CHROMOSOMES, NUM_TRIALS
-        )
-        generations = [GAGeneration(generation_tasks, set([]), 0)]
-
-        # Test the algorithm.
-        _TestAlgorithm("sum(values[0:len(values)])", specs, generations, -1)
-        cost_func = "sys.maxint - sum(values[0:len(values)])"
-        _TestAlgorithm(cost_func, specs, generations, -1)
-
-    def testIterativeElimination(self):
-        """Test the iterative elimination algorithm.
-
-        Test whether it finds the best results as expected.
-        """
-
-        # Initiate the build/test command and the log directory.
-        Task.InitLogCommand(None, None, "output")
-
-        # Generate the testing specs.
-        specs = self._GenerateFlagSpecifications()
-
-        # Generate the initial generations. The generation contains the base line
-        # task that turns on all the flags and tasks that each turn off one of the
-        # flags.
-        generations = _GenerateAllIterativeEliminationTasks(specs)
-
-        # Test the algorithm. The cost function is the summation of all the values
-        # of all the flags. Therefore, the best value is supposed to be 0, i.e.,
-        # when all the flags are disabled.
-        _TestAlgorithm("sum(values[0:len(values)])", specs, generations, 0)
-
-        # This test uses a cost function that is the negative of the previous cost
-        # function. Therefore, the best result should be found in task with all the
-        # flags enabled.
-        all_flags_tasks = _GenerateAllFlagsTasks(specs)
-        cost_function = "sys.maxint - sum(values[0:len(values)])"
-        # Compute the cost of the task that turns on all the flags.
-        all_flags = list(all_flags_tasks)[0].GetFlags()
-        cost = _ComputeCost(cost_function, specs, all_flags)
-
-        # Test the algorithm. The cost function is negative of the summation of all
-        # the values of all the flags. Therefore, the best value is supposed to be
-        # 0, i.e., when all the flags are disabled.
-        # The concrete type of the generation decides how the next generation will
-        # be generated.
-        _TestAlgorithm(cost_function, specs, generations, cost)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/binary_search_tool/.gitignore b/binary_search_tool/.gitignore
deleted file mode 100644
index c4977a33..00000000
--- a/binary_search_tool/.gitignore
+++ /dev/null
@@ -1,7 +0,0 @@
-log
-*.pyc
-working_set.txt
-objects.txt
-.binary_search_state.py.state*
-binary_search_state.py.state
-
diff --git a/binary_search_tool/MAINTENANCE b/binary_search_tool/MAINTENANCE
deleted file mode 100644
index 90ac582d..00000000
--- a/binary_search_tool/MAINTENANCE
+++ /dev/null
@@ -1,122 +0,0 @@
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-This document is for future maintainers of the binary search/bisection tools.
-
-Authors:
-  * Original Tool: asharif@, llozano@, cmtice@
-  * Updates after May 2016: cburden@
-  * chromeos-toolchain@
-
-The following are good reference materials on how the tool works:
-  * Ahmad's original presentation:
-    https://goto.google.com/zxdfyi
-
-  * Bisection tool update design doc:
-    https://goto.google.com/zcwei
-
-  * Bisection tool webpage:
-    https://goto.google.com/ruwpyi
-
-  * Compiler wrapper webpage:
-    https://goto.google.com/xossn
-
-
-TESTING:
-All unit tests live under the ./test directory. However, these tests
-specifically test binary_search_state.py, binary_search_perforce.py,
-run_bisect.py.
-
-These unit tests will not test the specific logic for ChromeOS/Android
-bisection. To test the ChromeOS/Android bisectors, use the common/hash_test.sh
-test. This is a simple test case that just checks the hashes of files on your
-file system. This means you won't have to find a specific compiler error for
-the bisector to triage in order to test each bisector.
-
-TODO:
-The bisection tool (I believe) is in a fairly good state. So these are mostly
-wishlist items and things that could use some improvement.
-
-  1. Get rid of binary_search_perforce.py. This file is mostly legacy code and
-     the majority of it isn't even used to bisect object files. The file was
-     originally intended to bisect CLs, and binary_search_state.py just reused
-     the binary searching logic from it. Maybe just extract the binary searching
-     logic from binary_search_perforce.py and put it in its own module in
-     cros_utils?
-
-  2. Cleanup unit tests in ./test. These tests are a little hacked together,
-     and are all under one test suite. Maybe consider organizing them across
-     multiple directories.
-
-  3. Create a "checkout setup" system for bisection. Currently if you want to
-     bisect, you have to run scripts/edit sources in this repo. Ideally these
-     scripts would be static, and if you wanted to bisect/make changes you would
-     "checkout" or copy all the scripts to a working directory and have a unique
-     working directory for each bisection. Credits to Luis for this idea =)
-
-  4. Make all scripts relative to each other. Currently all scripts enforce the
-     idea that their cwd will be ./binary_search_tool/. But it would be less
-     confusing to have each script relative to each other. There's quite a few
-     stackoverflow topics on how to do this best, but each one has some sort of
-     downside or flaw.
-
-  5. Overall modularize code more, especially in binary_search_state.py
-
-DESIGN EXPLANATIONS:
-Some of the design decisions are a bit difficult to understand from just reading
-the code unfortunately. I will attempt to clear up the major offenders of this:
-
-  1. common.py's argument dictionary:
-     binary_search_state.py and run_bisect.py both have to have near identical
-     arguments in order to support argument overriding in run_bisect.py. However
-     they do have to be slightly different. Mainly, run_bisect.py needs to have
-     no default values for arguments (so it can determine what's being
-     overriden).
-
-     In order to reduce huge amounts of code duplication for the argument
-     building, we put argument building in common.py. That way both modules
-     can reference the arguments, and they can have different configurations
-     across both.
-
-  2. Compiler wrapper:
-     The compiler wrapper is called before all compiler calls. It exists to
-     trick whatever build system (make, emerge, etc.) into thinking our
-     bisection is just a normal build, when really we're doing some tricks.
-
-     The biggest benefit the compiler wrapper gives is: knowing for sure which
-     files are actually generated by the compiler during bisection setup, and
-     potentially being able to skip compilations while triaging (speeding up the
-     triaging process significantly).
-
-  3. The weird options for the --verify, --verbose, --file_args, etc. arguments:
-     Some of the arguments for the bisection tool have a weird set of options
-     for the AddArgument method (nargs, const, default, StrToBool). This is so
-     we can make argument overriding workable. These options allow the following
-     functionality for a boolean argument (using --prune as an example):
-       * --prune (prune set to True)
-       * <not given> (prune set to False)
-       * --prune=True (prune set to True)
-       * --prune=False (prune set to False)
-
-     The first two are easy to implement (action='store_true'), but the last two
-     are why the extra weird arguments are required. Now, why would we want the
-     last two? Imagine if the Android bisector set --prune=True as a default
-     argument. With just the first two options above it would be impossible for
-     the user to override prune and set it to False. So the user needs the
-     --prune=False option. See the argparse documentation for more details.
-
-  4. General binary searching logic/pruning logic:
-     binary_search_state.py will enumerate all items into a list. The binary
-     search will find the *first* bad item (starting with lowest index).
-     Everything to the left of the "current" index is switched to good,
-     everything to right of the "current" index is switched to bad. Once a bad
-     item is found, it's put at the very end of the list.
-
-     If prune is set, the tool will continuing searching until all bad items are
-     found (instead of stopping after the first one). If the tool finds the same
-     item twice, that means no more bad items exist. This is because the item
-     was found, said item was put at the end of the list, and it was found
-     again. Because the binary search logic finds the bad item with the lowest
-     index, this means nothing in between the start of the list and the end of
-     the list is bad (thus no more bad items remain).
diff --git a/binary_search_tool/README.bisect.md b/binary_search_tool/README.bisect.md
deleted file mode 100644
index 32f4cba6..00000000
--- a/binary_search_tool/README.bisect.md
+++ /dev/null
@@ -1,251 +0,0 @@
-# `run_bisect.py`
-
-`run_bisect.py` is a wrapper around the general purpose
-`binary_search_state.py`. It provides a user friendly interface for
-bisecting various compilation errors.  The 2 currently provided
-methods of bisecting are ChromeOS package and object bisection. Each
-method defines a default set of options to pass to
-`binary_search_state.py` and allow the user to override these defaults
-(see the "Overriding" section).
-
-Please note that all commands, examples, scripts, etc. are to be run from your
-chroot unless stated otherwise.
-
-## Bisection Methods
-
-### ChromeOS Package
-
-This method will bisect across all packages in a ChromeOS repository and find
-the offending packages (according to your test script). This method takes the
-following arguments:
-
-* board: The board to bisect on. For example: daisy, falco, etc.
-* remote: The IP address of the physical machine you're using to test with.
-
-By default the ChromeOS package method will do a simple interactive test that
-pings the machine and prompts the user if the machine is good.
-
-1.  Setup: The ChromeOS package method requires that you have three build trees:
-
-    ```
-    /build/${board}.bad  - The build tree for your "bad" build
-    /build/${board}.good - The build tree for your "good" build
-    /build/${board}.work - A full copy of /build/${board}.bad
-    ```
-
-1.  Cleanup: run_bisect.py does most cleanup for you, the only thing required by
-    the user is to cleanup all built images and the three build trees made in
-    `/build/`
-
-1.  Default Arguments:
-
-    ```
-    --get_initial_items='cros_pkg/get_initial_items.sh'
-    --switch_to_good='cros_pkg/switch_to_good.sh'
-    --switch_to_bad='cros_pkg/switch_to_bad.sh'
-    --test_setup_script='cros_pkg/test_setup.sh'
-    --test_script='cros_pkg/interactive_test.sh'
-    --incremental
-    --prune
-    --file_args
-    ```
-
-1.  Additional Documentation: See `./cros_pkg/README.cros_pkg_triage` for full
-    documentation of ChromeOS package bisection.
-
-1.  Examples:
-
-    1.  Basic interactive test package bisection, on daisy board:
-
-        ```
-        ./run_bisect.py package daisy 172.17.211.184
-        ```
-
-    2.  Basic boot test package bisection, on daisy board:
-
-        ```
-        ./run_bisect.py package daisy 172.17.211.184 -t cros_pkg/boot_test.sh
-        ```
-
-### ChromeOS Object
-
-This method will bisect across all objects in a ChromeOS package and find
-the offending objects (according to your test script). This method takes the
-following arguments:
-
-* board: The board to bisect on. For example: daisy, falco, etc.
-* remote: The IP address of the physical machine you're using to test with.
-* package: The package to bisect with. For example: chromeos-chrome.
-* use_flags: (Optional) Use flags for emerge. For example: "-thinlto -cfi".
-* noreboot: (Optional) Do not reboot after updating the package.
-* dir: (Optional) the directory for your good/bad build trees. Defaults to
-       $BISECT_DIR or /tmp/sysroot_bisect. This value will set $BISECT_DIR
-       for all bisecting scripts.
-
-By default the ChromeOS object method will do a simple interactive test that
-pings the machine and prompts the user if the machine is good.
-
-1.  Setup: The ChromeOS package method requires that you populate your good and
-    bad set of objects. `sysroot_wrapper` will automatically detect the
-    `BISECT_STAGE` variable and use this to populate emerged objects. Here is an
-    example:
-
-    ```
-    # Defaults to /tmp/sysroot_bisect
-    export BISECT_DIR="/path/to/where/you/want/to/store/builds/"
-
-    export BISECT_STAGE="POPULATE_GOOD"
-    ./switch_to_good_compiler.sh
-    emerge-${board} -C ${package_to_bisect}
-    emerge-${board} ${package_to_bisect}
-
-    export BISECT_STAGE="POPULATE_BAD"
-    ./switch_to_bad_compiler.sh
-    emerge-${board} -C {package_to_bisect}
-    emerge-${board} ${package_to_bisect}
-    ```
-
-1.  Cleanup: The user must clean up all built images and the populated object
-    files.
-
-1.  Default Arguments:
-
-    ```
-    --get_initial_items='sysroot_wrapper/get_initial_items.sh'
-    --switch_to_good='sysroot_wrapper/switch_to_good.sh'
-    --switch_to_bad='sysroot_wrapper/switch_to_bad.sh'
-    --test_setup_script='sysroot_wrapper/test_setup.sh'
-    --test_script='sysroot_wrapper/interactive_test.sh'
-    --noincremental
-    --prune
-    --file_args
-    ```
-
-1.  Additional Documentation: See `./sysroot_wrapper/README` for full
-    documentation of ChromeOS object file bisecting.
-
-1.  Examples:
-
-    1.  Basic interactive test object bisection, on daisy board for cryptohome
-        package: `./run_bisect.py object daisy 172.17.211.184 cryptohome`
-
-    2.  Basic boot test package bisection, on daisy board for cryptohome
-        package: `./run_bisect.py object daisy 172.17.211.184 cryptohome
-        --test_script=sysroot_wrapper/boot_test.sh`
-
-### Android object
-
-NOTE: Because this isn't a ChromeOS bisection tool, the concept of a
-      chroot doesn't exist. Just run this tool from a normal shell.
-
-This method will bisect across all objects in the Android source tree and
-find the offending objects (according to your test script). This method takes
-the following arguments:
-
-*   `android_src`: The location of your android source tree
-
-*   `num_jobs`: (Optional) The number of jobs to pass to make. This is dependent
-    on how many cores your machine has. A good number is probably somewhere
-    around 5 to 10.
-
-*   `device_id`: (Optional) The serial code for the device you are testing on.
-    This is used to determine which device should be used in case multiple
-    devices are plugged into your computer. You can get serial code for your
-    device by running "adb devices".
-
-*   `dir`: (Optional) the directory for your good/bad build trees. Defaults to
-    `$BISECT_DIR` or `~/ANDROID_BISECT/`. This value will set `$BISECT_DIR` for
-    all bisecting scripts.
-
-  By default the Android object method will do a simple interactive test that
-  pings the machine and prompts the user if the machine is good.
-
-1.  Setup: The Android object method requires that you populate your good and
-    bad set of objects. The Android compiler wrapper will automatically detect
-    the `BISECT_STAGE` variable and use this to populate emerged objects. Here
-    is an example:
-
-    ```
-    # Defaults to ~/ANDROID_BISECT/
-    export BISECT_DIR="/path/to/where/you/want/to/store/builds/"
-
-    export BISECT_STAGE="POPULATE_GOOD"
-    # Install the "good" compiler
-    ./switch_to_good_compiler.sh
-    make clean
-    make -j <your_preferred_number_of_jobs>
-
-    export BISECT_STAGE="POPULATE_BAD"
-    # Install the "bad" compiler
-    ./switch_to_bad_compiler.sh
-    make clean
-    make -j <your_preferred_number_of_jobs>
-    ```
-
-1.  Cleanup: The user must clean up all built images and the populated object
-    files.
-
-1.  Default Arguments:
-
-    ```
-    --get_initial_items='android/get_initial_items.sh'
-    --switch_to_good='android/switch_to_good.sh'
-    --switch_to_bad='android/switch_to_bad.sh'
-    --test_setup_script='android/test_setup.sh'
-    --test_script='android/interactive_test.sh'
-    --incremental
-    --prune
-    --file_args
-    ```
-
-1.  Additional Documentation: See `./android/README.android` for full
-    documentation of Android object file bisecting.
-
-1.  Examples:
-
-    1.  Basic interactive test android bisection, where the android source is at
-        ~/android_src: `./run_bisect.py android ~/android_src`
-
-    2. Basic boot test android bisection, where the android source is at
-       `~/android_src`, and 10 jobs will be used to build android:
-       `./run_bisect.py
-       android ~/android_src --num_jobs=10
-       --test_script=sysroot_wrapper/boot_test.sh`
-
-### Resuming
-
-`run_bisect.py` and `binary_search_state.py` offer the
-ability to resume a bisection in case it was interrupted by a
-SIGINT, power failure, etc. Every time the tool completes a
-bisection iteration its state is saved to disk (usually to the file
-`./bisect_driver.py.state`). If passed the --resume option, the tool
-it will automatically detect the state file and resume from the last
-completed iteration.
-
-### Overriding
-
-You can run `./run_bisect.py --help` or `./binary_search_state.py
---help` for a full list of arguments that can be overriden. Here are
-a couple of examples:
-
-Example 1 (do boot test instead of interactive test):
-
-```
-./run_bisect.py package daisy 172.17.211.182 --test_script=cros_pkg/boot_test.sh
-```
-
-Example 2 (do package bisector system test instead of interactive test, this
-           is used to test the bisecting tool itself -- see comments in
-           hash_test.sh for more details):
-
-```
-./run_bisect.py package daisy 172.17.211.182 \
-    --test_script=common/hash_test.sh --test_setup_script=""
-```
-
-Example 3 (enable verbose mode, disable pruning, and disable verification):
-
-```
-./run_bisect.py package daisy 172.17.211.182
-      --verbose --prune=False --verify=False
-```
diff --git a/binary_search_tool/README.pass_bisect.md b/binary_search_tool/README.pass_bisect.md
deleted file mode 100644
index eb0f12a6..00000000
--- a/binary_search_tool/README.pass_bisect.md
+++ /dev/null
@@ -1,83 +0,0 @@
-# Pass bisection
-
-This document describes a feature for the bisection tool, which provides
-pass and transformation level bisection for a bad object file.
-
-Before reading this document, please refer to README.bisect for general usage
-of the bisection tool.
-
-The benefit of using pass level bisection is:
-When building a bad object file, it can tell you which pass and transformation
-in the compiler caused the error.
-
-*Notice:* This tool will only work for LLVM/clang, since it is using options
-`-opt-bisect-limit` and `print-debug-counter` that only exist in LLVM.
-
-## Arguments
-
-All the required arguments in object-file-level bisection tool are still
-to be provided. In addition, you will need to add the following arguments:
-
-1. `--pass_bisect`: enables pass level bisection
-2. `--ir_diff`: enables output of IR differences
-
-Please refer to `--help` or the examples below for details about how to use
-them.
-
-## HOW TO USE: ChromeOS
-
-*TODO* - Future work: Currently this only works for Android.
-
-## HOW TO USE: Android
-
-1.  Prerequisites: A general setup is still needed for Android, which means that
-    you need to populate good and bad set of objects with two versions of
-    compilers.
-
-    See the documentation in `README.bisect.md` for more detailed instructions.
-
-1.  Pass/Transformation Bisection: If you do not wish to override the other
-    arguments, this command should be sufficient to do pass/transformation level
-    bisection:
-
-    ```
-    ./run_bisect.py android PATH_TO_ANDROID_HOME_DIR
-                --pass_bisect=android/generate_cmd.sh
-                --prune=False
-                --ir_diff
-                --verbose
-    ```
-
-    Where:
-
-    ```
-    --pass_bisect:
-        Enables pass/transformation level bisection and with default
-        script to generate the command as android/generate_cmd.sh.
-    --prune:
-        For now, prune must be set to False to return only the first
-        bad item.
-    --ir_diff:
-        Optional argument to print out IR differences.
-    --verbose:
-        To show IR diff, verbose needs to be on.
-    ```
-
-    Other default arguments:
-
-    ```
-    --get_initial_items='android/get_initial_items.sh'
-    --switch_to_good='android/switch_to_good.sh'
-    --switch_to_bad='android/switch_to_bad.sh'
-    --test_setup_script='android/test_setup.sh'
-    --test_script='android/interactive_test.sh'
-    --incremental
-    --prune
-    --file_args
-    ```
-
-    You can always override them if needed. See README.bisect for more
-    details.
-
-1.  Other features: Features such as resuming, number of jobs, and device id
-    remain the same as before. See README.bisect for more details.
diff --git a/binary_search_tool/README.testing.md b/binary_search_tool/README.testing.md
deleted file mode 100644
index 0f06679b..00000000
--- a/binary_search_tool/README.testing.md
+++ /dev/null
@@ -1,94 +0,0 @@
-# Testing the binary search tool
-
-This file explains how to set up and run the various kinds of bisection tests.
-
-The bisection tool comes with several sets of tests which you should
-run after updating any of the bisection tool scripts OR after updating
-the Android compiler wrapper (to make sure the wrapper will still work
-correctly with bisection).
-
-## Before you start.
-
-Before you can run the tests, your PYTHONPATH environment variable
-must be correct.  This means that it must include both the
-toolchain-utils directory and the `binary_search_tool` directory.  The
-easiest way to set it is:
-
-```
-$ cd toolchain-utils
-$ export PYTHONPATH=`pwd`:${PYTHONPATH}
-$ cd binary_search_tool
-$ export PYTHONPATH=`pwd`:${PYTHONPATH}
-```
-
-
-## Running the unittests.
-
-To run the basic unit tests:
-
-```
-$ cd toolchain-utils/binary_search_tool/test
-$ ./binary_search_tool_test.py
-```
-
-# Running the bisection tests, testing the compiler wrapper.
-
-If you want to run the bisection tests, and test the compiler wrapper
-(to make sure the `POPULATE_GOOD` and `POPULATE_BAD` stages are still
-working properly) you can do the following.
-
-If you are testing with the ANDROID COMPILER WRAPPER, you need to to some
-preliminary setup:
-
-Set up the compiler wrapper to replace GCC:
-
-```
-$ cd <android-root/prebuilts/clang/host/linux-x86/clang-368880/bin
-$ cp clang gcc
-$ whereis gcc
-gcc: /usr/bin/gcc /usr/lib/gcc /usr/bin/X11/gcc /usr/share/man/man1/gcc.1.gz
-$ cd /usr/bin
-$ ls -l gcc
-lrwxrwxrwx 1 root root 7 Feb  3 17:00 gcc -> gcc-4.8*
-$ sudo mv gcc gcc.real
-$ sudo ln -s \
-    <android-root>/prebuilts/clang/host/linux-x86/clang-3688880/bin/gcc gcc
-```
-
-Move to the correct directory, then run the test script:
-
-```
-$ cd toolchain-utils/binary_search_tool
-$ ./run_bisect_tests.py
-```
-
-If you are testing with the CHROMEOS COMPILER WRAPPER, you MUST run the
-tests from INSIDE your CHROOT (but you don't need to do any special setup):
-
-```
-$ cd <path-to-chromeos-root>
-$ cros_sdk
-$ cd ~/trunk/src/third_party/toolchain-utils
-```
-
-Set up your `PYTHONPATH`:
-
-```
-$ export PYTHONPATH=`pwd`:${PYTHONPATH}
-$ cd binary_search_tool
-$ export PYTHONPATH=`pwd`:${PYTHONPATH}
-```
-
-Run the test script:
-
-```
-$ ./run_bisect_tests.py
-```
-
-
-## Running the bisection tests, without testing the compiler wrapper.
-
-```
-$ cd toolchain-utils/binary_search_tool
-$ ./full_bisect_test/run-test-nowrapper.sh
-```
diff --git a/binary_search_tool/__init__.py b/binary_search_tool/__init__.py
deleted file mode 100644
index 6e3ade4a..00000000
--- a/binary_search_tool/__init__.py
+++ /dev/null
@@ -1,4 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
diff --git a/binary_search_tool/android/README.android.md b/binary_search_tool/android/README.android.md
deleted file mode 100644
index 9445dcbb..00000000
--- a/binary_search_tool/android/README.android.md
+++ /dev/null
@@ -1,209 +0,0 @@
-# Android's binary search tool
-
-`binary_search_state.py` is a general binary search triage tool that
-performs a binary search on a set of things to try to identify which
-thing or thing(s) in the set is 'bad'.  `binary_search_state.py` assumes
-that the user has two sets, one where everything is known to be good,
-and one which contains at least one bad item.  `binary_search_state.py`
-then copies items from the good and bad sets into a working set and
-tests the result (good or bad).  `binary_search_state.py` requires that
-a set of scripts be supplied to it for any particular job.  For more
-information on `binary_search_state.py`, see
-
-https://sites.google.com/a/google.com/chromeos-toolchain-team-home2/home/team-tools-and-scripts/binary-searcher-tool-for-triage
-
-This particular set of scripts is designed to work with
-`binary_search_state.py` in order to find the bad object or set of
-bad objects in an Android build. Furthermore, it can also help find
-the bad compiler pass and transformation when building that bad object.
-
-
-## QUICKSTART
-
-After setting up your 2 build trees (see Prerequisites section), do the
-following:
-
--   Decide which test script to use (`boot_test.sh` or
-    `interactive_test.sh`)
--   Get the serial number for the Android device you will use for testing.
--   Run the following:
-
-    ```
-    $ cd <android_src>
-    $ source build/envsetup.sh
-    $ lunch <android_device_lunch_combo>
-    $ cd <path_to_toolchain_utils>/binary_search_tool/
-    $ NUM_JOBS=10 ANDROID_SERIAL=<device_serial> \
-          ./android/setup.sh <android_src>
-    ```
-
-    If you chose the boot test, then:
-
-    ```
-    TEST_SCRIPT=android/boot_test.sh
-    ```
-
-    If you chose the interactive test, then:
-
-    ```
-    TEST_SCRIPT=android/interactive_test.sh
-    ```
-
-    Finally, run the binary search tool:
-
-    ```
-    $ python ./binary_search_state.py \
-        --get_initial_items=android/get_initial_items.sh \
-        --switch_to_good=android/switch_to_good.sh \
-        --switch_to_bad=android/switch_to_bad.sh \
-        --test_setup_script=android/test_setup.sh \
-        --test_script=$TEST_SCRIPT \
-        --file_args \
-        --prune
-    ```
-
-    Once you have completely finished doing the binary search/triage,
-    run the cleanup script:
-
-    ```
-    $ android/cleanup.sh
-    ```
-
-
-## FILES AND SCRIPTS
-
-Check the header comments for each script for more in depth documentation.
-
-`boot_test.sh` - One of two possible test scripts used to determine
-                 if the Android image built from the objects is good
-                 or bad. This script tests to see if the image
-                 booted, and requires no user intervention.
-
-`cleanup.sh` - This is called after the binary search tool completes. This
-               script will clean up the common.sh file generated by setup.sh
-
-`get_initial_items.sh` - This script is used to determine all Android objects
-                         that will be bisected.
-
-`test_setup.sh` - This script will build and flash your image to the
-                  Android device. If the flash fails, this script will
-                  help the user troubleshoot by trying to flash again or
-                  by asking the user to manually flash it.
-
-`interactive_test.sh` - One of two possible scripts used to determine
-                        if the Android image built from the objects
-                        is good or bad.  This script requires user
-                        interaction to determine if the image is
-                        good or bad.
-
-`setup.sh` - This is the first script the user should call, after
-             taking care of the prerequisites.  It sets up the
-             environment appropriately for running the Android
-             object binary search triage, and it generates the
-             necessary common script (see below).
-
-`switch_to_bad.sh` - This script is used to link objects from the
-                     'bad' build tree into the work area.
-
-`switch_to_good.sh` - This script is used to link objects from the
-                      'good' build tree into the work area.
-
-`generate_cmd.sh` - This script will generate another temporary script, which
-                    contains the command line options to build the bad object
-                    file again with pass/transformation level limit.
-
-
-## GENERATED SCRIPTS
-
-`common.sh` - contains basic environment variable definitions for
-              this binary search triage session.
-
-## ASSUMPTIONS
-
--   There are two different Android builds, for the same board/lunch combo with
-    the same set of generated object files.  One build creates a good working
-    Android image and the other does not.
-
--   The toolchain bug you are tracking down is not related to the linker. If the
-    linker is broken or generates bad code, this tool is unlikely to help you.
-
-
-PREREQUISITES FOR USING THESE SCRIPTS:
-
-1.  Decide where to store each build tree
-    By default, each build tree is stored in `~/ANDROID_BISECT`. However you
-    can override this by exporting `BISECT_DIR` set to whatever directory you
-    please. Keep in mind these build trees take dozens of gigabytes each.
-
-2.  Setup your android build environment
-
-    ```
-    cd <android_src>
-    source build/envsetup.sh
-    lunch <android_device_lunch_combo>
-    ```
-
-3.  Populate the good build tree
-
-    1.  `make clean`
-    2.  `export BISECT_STAGE=POPULATE_GOOD`
-    3.  Install your "good" toolchain in Android, this will most likely be
-        the toolchain that comes preinstalled with the Android source.
-    4.  Build all of Android: `make -j10`. The "-j" parameter depends on how
-        many cores your machine has. See Android documentation for more details.
-
-4.  Populate the bad build tree
-
-    1.  `make clean`
-    2.  `export BISECT_STAGE=POPULATE_BAD`
-    3.  Install your "bad" toolchain in Android.
-    4.  Build all of Android again.
-
-5.  Run the android setup script
-
-    1.  `cd <path_to_toolchain_utils>/binary_search_tool/`
-    2.  `NUM_JOBS=<jobs> ANDROID_SERIAL=<android_serial_num>
-        android/setup.sh <android_src>`
-
-WARNING: It's important that you leave the full `out/` directory in your
-         Android source alone after Step 4. The binary search tool will
-         use this directory as a skeleton to build each test image while
-         triaging.
-
-## USING THESE SCRIPTS FOR BINARY TRIAGE OF OBJECTS
-
-To use these scripts, you must first run setup.sh, passing it the path to your
-Android source directory. setup.sh will do the following:
-
--   Verify that your build trees are set up correctly (with good, bad).
--   Verify that each build tree has the same contents.
--   Verify that the android build environment (lunch, etc.) are setup in your
-    current shell.
--   Create the common.sh file that the other scripts passed to the
-    binary triage tool will need.
-
-
-This set of scripts comes with two alternate test scripts.  One test
-script, `boot_test.sh`, just checks to make sure that the image
-booted (wait for device to boot to home screen) and assumes that is enough.
-The other test script, `interactive_test.sh`, is interactive and asks YOU
-to tell it whether the image on the android device is ok or not (it
-prompts you and waits for a response).
-
-
-Once you have run `setup.sh` (and decided which test script you
-want to use) run the binary triage tool using these scripts to
-isolate/identify the bad object:
-
-```
-./binary_search_state.py \
-   --get_initial_items=android/get_initial_items.sh \
-   --switch_to_good=android/switch_to_good.sh \
-   --switch_to_bad=android/switch_to_bad.sh \
-   --test_setup_script=android/test_setup.sh \
-   --test_script=android/boot_test.sh \  # could use interactive_test.sh instead
-   --prune
-```
-
-After you have finished running the tool and have identified the bad
-object(s), you will want to run the cleanup script (android/cleanup.sh).
diff --git a/binary_search_tool/android/boot_test.sh b/binary_search_tool/android/boot_test.sh
deleted file mode 100755
index 4c0c77e2..00000000
--- a/binary_search_tool/android/boot_test.sh
+++ /dev/null
@@ -1,61 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2016 Google LLC
-#
-# This script pings the android device to determine if it successfully booted.
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on the Android source tree. It
-# waits for the test setup script to build and install the image, then checks
-# if image boots or not. It should return '0' if the test succeeds
-# (the image is 'good'); '1' if the test fails (the image is 'bad'); and '125'
-# if it could not determine (does not apply in this case).
-#
-
-source android/common.sh
-
-# Check if boot animation has stopped and trim whitespace
-is_booted()
-{
-  # Wait for boot animation to stop and trim whitespace
-  status=`adb shell getprop init.svc.bootanim | tr -d '[:space:]'`
-  [[ "$status" == "stopped" ]]
-  return $?
-}
-
-# Wait for device to boot, retry every 1s
-# WARNING: Do not run without timeout command, could run forever
-wait_for_boot()
-{
-  while ! is_booted
-  do
-    sleep 1
-  done
-}
-
-echo "Waiting 60 seconds for device to come online..."
-timeout 60 adb wait-for-device
-retval=$?
-
-if [[ ${retval} -eq 0 ]]; then
-  echo "Android image has been built and installed."
-else
-  echo "Device failed to reboot within 60 seconds."
-  exit 1
-fi
-
-echo "Waiting 60 seconds for device to finish boot..."
-# Spawn subshell that will timeout in 60 seconds
-# Feed to cat so that timeout will recognize it as a command
-# (timeout only works for commands/programs, not functions)
-timeout 60 cat <(wait_for_boot)
-retval=$?
-
-if [[ ${retval} -eq 0 ]]; then
-  echo "Android device fully booted!"
-else
-  echo "Device failed to fully boot within 60 seconds."
-  exit 1
-fi
-
-exit ${retval}
diff --git a/binary_search_tool/android/cleanup.sh b/binary_search_tool/android/cleanup.sh
deleted file mode 100755
index 480b830b..00000000
--- a/binary_search_tool/android/cleanup.sh
+++ /dev/null
@@ -1,15 +0,0 @@
-#!/bin/bash
-#
-# Copyright 2016 Google LLC
-#
-# This script is part of the Android binary search triage process.
-# It should be the last script called by the user, after the user has
-# successfully run the bisection tool and found their bad items. This script
-# will perform all necessary cleanup for the bisection tool.
-#
-
-rm android/common.sh
-# Remove build command script if pass_bisect enabled
-rm -f android/cmd_script.sh
-# Remove tmp IR file used for ir_diff in pass beisction
-rm -f /tmp/bisection_bad_item.o
diff --git a/binary_search_tool/android/generate_cmd.sh b/binary_search_tool/android/generate_cmd.sh
deleted file mode 100755
index 6d0e5692..00000000
--- a/binary_search_tool/android/generate_cmd.sh
+++ /dev/null
@@ -1,60 +0,0 @@
-#!/bin/bash -eu
-
-# Copyright 2018 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-# This script extracts command line options to build bad item.
-# The generated script will be used by pass level bisection.
-#
-
-source android/common.sh
-
-abs_path=$1
-
-# The item will be `-o relative-path-to-object `, which will be used
-# for seeking command in populate log.
-# We care about the `-o` at the beginning and ` ` at the end are necessary,
-# so that we can get build command for exact this object file.
-# Example: prebuilt/../clang++ -O3 -MF obj1.o.d -o obj.o obj.cpp
-# We should count this command as one to build obj.o, not obj1.o.d.
-real_path=$(realpath --relative-to="${BISECT_WORK_BUILD}" "${abs_path}")
-item="-o $real_path "
-
-populate_log=${BISECT_BAD_BUILD}/_POPULATE_LOG
-
-output='#!/bin/bash -u\n'
-output+='source android/common.sh\n'
-
-result=$(egrep -m 1 -- "${item}" ${populate_log})
-
-# Re-generate bad item to tmp directory location
-tmp_ir='/tmp/bisection_bad_item.o'
-result=$(sed "s|$item|-o $tmp_ir |g" <<< ${result})
-
-# Remove `:` after cd command
-result=$(sed 's|cd:|cd|g' <<< ${result})
-
-# Add environment variable which helps pass level bisection
-result=$(sed 's| -o | $LIMIT_FLAGS -o |g' <<< ${result})
-
-output+=${result}
-
-# Symbolic link generated bad item to original object
-output+="\nln -f $tmp_ir $abs_path"
-output+="\ntouch $abs_path"
-
-echo -e "${output}" > android/cmd_script.sh
-
-chmod u+x android/cmd_script.sh
-
-echo 'Script created as android/cmd_script.sh'
-
-# Check if compiler is LLVM.
-if grep -q "clang" android/cmd_script.sh
-then
-    exit 0
-else
-    echo 'Pass/transformation level bisection only works for LLVM compiler.'
-    exit 1
-fi
\ No newline at end of file
diff --git a/binary_search_tool/android/get_initial_items.sh b/binary_search_tool/android/get_initial_items.sh
deleted file mode 100755
index 1ed30425..00000000
--- a/binary_search_tool/android/get_initial_items.sh
+++ /dev/null
@@ -1,13 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2016 Google LLC
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on the Android source tree.  This script
-# generates the list of current Android object files, that is then used
-# for doing the binary search.
-#
-
-source android/common.sh
-
-cat ${BISECT_GOOD_BUILD}/_LIST
diff --git a/binary_search_tool/android/interactive_test.sh b/binary_search_tool/android/interactive_test.sh
deleted file mode 100755
index 0a8a4b8c..00000000
--- a/binary_search_tool/android/interactive_test.sh
+++ /dev/null
@@ -1,39 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2016 Google LLC
-#
-# This script pings the android device to determine if it successfully booted.
-# It then asks the user if the image is good or not, allowing the user to
-# conduct whatever tests the user wishes, and waiting for a response.
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on the Android source tree. It
-# waits for the test setup script to build and install the image, then asks the
-# user if the image is good or not. It should return '0' if the test succeeds
-# (the image is 'good'); '1' if the test fails (the image is 'bad'); and '125'
-# if it could not determine (does not apply in this case).
-#
-
-source android/common.sh
-
-echo "Waiting 60 seconds for device to boot..."
-timeout 60 adb wait-for-device
-retval=$?
-
-if [[ ${retval} -eq 0 ]]; then
-  echo "Android image has been built and installed."
-else
-  echo "Device failed to reboot within 60 seconds."
-  exit 1
-fi
-
-while true; do
-  read -p "Is this a good Android image?" yn
-  case $yn in
-    [Yy]* ) exit 0;;
-    [Nn]* ) exit 1;;
-    * ) echo "Please answer yes or no.";;
-  esac
-done
-
-exit 125
diff --git a/binary_search_tool/android/setup.sh b/binary_search_tool/android/setup.sh
deleted file mode 100755
index 06918226..00000000
--- a/binary_search_tool/android/setup.sh
+++ /dev/null
@@ -1,147 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2016 Google LLC
-#
-# This script is part of the Android binary search triage process.
-# It should be the first script called by the user, after the user has set up
-# the two necessary build tree directories (see the prerequisites section of
-# README.android).
-#
-# WARNING:
-#   Before running this script make sure you have setup the Android build
-#   environment in this shell (i.e. successfully run 'lunch').
-#
-# This script takes three arguments.  The first argument must be the path of
-# the android source tree being tested. The second (optional) argument is the
-# device ID for fastboot/adb so the test device can be uniquely indentified in
-# case multiple phones are plugged in. The final (optional) argument is the
-# number of jobs that various programs can use for parallelism
-# (make, xargs, etc.). There is also the argument for bisection directory, but
-# this is not strictly an argument for just this script (as it should be set
-# during the POPULATE_GOOD and POPULATE_BAD steps, see README.android for
-# details).
-#
-# Example call:
-#   ANDROID_SERIAL=002ee16b1558a3d3 NUM_JOBS=10 android/setup.sh ~/android
-#
-#   This will setup the bisector for Nexus5X, using 10 jobs, where the android
-#   source lives at ~/android.
-#
-# NOTE: ANDROID_SERIAL is actually an option used by ADB. You can also simply
-# do 'export ANDROID_SERIAL=<device_id>' and the bisector will still work.
-# Furthermore, if your device is the only Android device plugged in you can
-# ignore ANDROID_SERIAL.
-#
-# This script sets all necessary environment variables, and ensures the
-# environment for the binary search triage process is setup properly. In
-# addition, this script generates common.sh, which generates enviroment
-# variables used by the other scripts in the package binary search triage process.
-#
-
-#
-# Positional arguments
-#
-
-ANDROID_SRC=$1
-
-#
-# Optional arguments
-#
-
-# If DEVICE_ID is not null export this as ANDROID_SERIAL for use by adb
-# If DEVICE_ID is null then leave null
-DEVICE_ID=${ANDROID_SERIAL:+"export ANDROID_SERIAL=${ANDROID_SERIAL} "}
-
-NUM_JOBS=${NUM_JOBS:-"1"}
-BISECT_ANDROID_DIR=${BISECT_DIR:-~/ANDROID_BISECT}
-
-#
-# Set up basic variables.
-#
-
-GOOD_BUILD=${BISECT_ANDROID_DIR}/good
-BAD_BUILD=${BISECT_ANDROID_DIR}/bad
-WORK_BUILD=${ANDROID_SRC}
-
-#
-# Verify that the necessary directories exist.
-#
-
-if [[ ! -d ${GOOD_BUILD} ]] ; then
-  echo "Error:  ${GOOD_BUILD} does not exist."
-  exit 1
-fi
-
-if [[ ! -d ${BAD_BUILD} ]] ; then
-  echo "Error:  ${BAD_BUILD} does not exist."
-  exit 1
-fi
-
-if [[ ! -d ${WORK_BUILD} ]] ; then
-  echo "Error:  ${WORK_BUILD} does not exist."
-  exit 1
-fi
-
-#
-# Verify that good/bad object lists are the same
-#
-
-good_list=`mktemp`
-bad_list=`mktemp`
-sort ${GOOD_BUILD}/_LIST > good_list
-sort ${BAD_BUILD}/_LIST > bad_list
-
-diff good_list bad_list
-diff_result=$?
-rm good_list bad_list
-
-if [ ${diff_result} -ne 0 ]; then
-  echo "Error: good and bad object lists differ."
-  echo "diff exited with non-zero status: ${diff_result}"
-  exit 1
-fi
-
-#
-# Ensure android build environment is setup
-#
-# ANDROID_PRODUCT_OUT is only set once lunch is successfully executed. Fail if
-# ANDROID_PRODUCT_OUT is unset.
-#
-
-if [ -z ${ANDROID_PRODUCT_OUT+0} ]; then
-  echo "Error: Android build environment is not setup."
-  echo "cd to ${ANDROID_SRC} and do the following:"
-  echo "  source build/envsetup.sh"
-  echo "  lunch <device_lunch_combo>"
-  exit 1
-fi
-
-#
-# Create common.sh file, containing appropriate environment variables.
-#
-
-COMMON_FILE="android/common.sh"
-
-cat <<-EOF > ${COMMON_FILE}
-
-BISECT_ANDROID_DIR=${BISECT_ANDROID_DIR}
-
-BISECT_ANDROID_SRC=${ANDROID_SRC}
-BISECT_NUM_JOBS=${NUM_JOBS}
-
-BISECT_GOOD_BUILD=${GOOD_BUILD}
-BISECT_BAD_BUILD=${BAD_BUILD}
-BISECT_WORK_BUILD=${WORK_BUILD}
-
-BISECT_GOOD_SET=${GOOD_BUILD}/_LIST
-BISECT_BAD_SET=${BAD_BUILD}/_LIST
-
-${DEVICE_ID}
-
-export BISECT_STAGE="TRIAGE"
-
-EOF
-
-chmod 755 ${COMMON_FILE}
-
-exit 0
diff --git a/binary_search_tool/android/switch_to_bad.sh b/binary_search_tool/android/switch_to_bad.sh
deleted file mode 100755
index 2100ed43..00000000
--- a/binary_search_tool/android/switch_to_bad.sh
+++ /dev/null
@@ -1,42 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2016 Google LLC
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on the Android source tree.  This script
-# symlinks a list of object files from the 'bad' build tree into the working
-# build tree, for testing.
-#
-# It is highly recommended to not use --noincremental with these scripts. If the
-# switch scripts are given non incremental sets of GOOD/BAD objects, make will
-# not be able to do an incremental build and will take much longer to build.
-#
-
-
-source android/common.sh
-
-OBJ_LIST_FILE=$1
-
-# Symlink from BAD obj to working tree.
-SWITCH_CMD="ln -sf ${BISECT_BAD_BUILD}/{} {}; touch {};"
-
-overall_status=0
-
-# Check that number of arguments == 1
-if [ $# -eq 1 ] ; then
-  # Run symlink once per input line, ignore empty lines.
-  # Have ${BISECT_NUM_JOBS} processes running concurrently.
-  # Pass to "sh" to allow multiple commands to be executed.
-  xargs -P ${BISECT_NUM_JOBS} -a ${OBJ_LIST_FILE} -r -l -I '{}' \
-    sh -c "${SWITCH_CMD}"
-else
-  echo "ERROR:"
-  echo "Please run the binary search tool with --file_args"
-  echo "Android has too many files to be passed as command line arguments"
-  echo "The binary search tool will now exit..."
-  exit 1
-fi
-overall_status=$?
-
-
-exit ${overall_status}
diff --git a/binary_search_tool/android/switch_to_good.sh b/binary_search_tool/android/switch_to_good.sh
deleted file mode 100755
index a5be3c3e..00000000
--- a/binary_search_tool/android/switch_to_good.sh
+++ /dev/null
@@ -1,41 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2016 Google LLC
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on the Android source tree.  This script
-# symlinks a list of object files from the 'good' build tree into the working
-# build tree, for testing.
-#
-# It is highly recommended to not use --noincremental with these scripts. If the
-# switch scripts are given non incremental sets of GOOD/BAD objects, make will
-# not be able to do an incremental build and will take much longer to build.
-#
-
-source android/common.sh
-
-OBJ_LIST_FILE=$1
-
-# Symlink from GOOD obj to working tree.
-SWITCH_CMD="ln -sf ${BISECT_GOOD_BUILD}/{} {}; touch {};"
-
-overall_status=0
-
-# Check that number of arguments == 1
-if [ $# -eq 1 ] ; then
-  # Run symlink once per input line, ignore empty lines.
-  # Have ${BISECT_NUM_JOBS} processes running concurrently.
-  # Pass to "sh" to allow multiple commands to be executed.
-  xargs -P ${BISECT_NUM_JOBS} -a ${OBJ_LIST_FILE} -r -l -I '{}' \
-    sh -c "${SWITCH_CMD}"
-else
-  echo "ERROR:"
-  echo "Please run the binary search tool with --file_args"
-  echo "Android has too many files to be passed as command line arguments"
-  echo "The binary search tool will now exit..."
-  exit 1
-fi
-overall_status=$?
-
-
-exit ${overall_status}
diff --git a/binary_search_tool/android/test_setup.sh b/binary_search_tool/android/test_setup.sh
deleted file mode 100755
index be4a0b76..00000000
--- a/binary_search_tool/android/test_setup.sh
+++ /dev/null
@@ -1,130 +0,0 @@
-#!/bin/bash
-#
-# Copyright 2016 Google LLC
-#
-# This is the test setup script for generating an Android image based off the
-# current working build tree. make is called to relink the object files and
-# generate the new Android image to be flashed. The device is then rebooted into
-# bootloader mode and fastboot is used to flash the new image. The device is
-# then rebooted so the user's test script can run.
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on the Android source tree. It should
-# return '0' if the setup succeeds; and '1' if the setup fails (the image
-# could not build or be flashed).
-#
-
-source android/common.sh
-
-manual_flash()
-{
-  echo
-  echo "Please manually flash the built image to your device."
-  echo "To do so follow these steps:"
-  echo "  1. Boot your device into fastboot mode."
-  echo "  2. cd to '${BISECT_ANDROID_SRC}'"
-  echo "  2. Run 'source build/envsetup.sh'"
-  echo "  3. Run 'lunch'"
-  echo "  4. Run '${ADB_DEVICE}fastboot flashall -w'"
-  echo "Or see the following link for more in depth steps:"
-  echo "https://source.android.com/source/running.html"
-  echo
-  while true; do
-    sleep 1
-    read -p "Was the flashing of the image successful? " choice
-    case $choice in
-      [Yy]*) return 0;;
-      [Nn]*) return 1;;
-      *) echo "Please answer y or n.";;
-    esac
-  done
-}
-
-auto_flash()
-{
-  echo
-  echo "Please ensure your Android device is on and in fastboot mode so"
-  echo "fastboot flash may run."
-  echo
-  sleep 1
-  read -p $'Press enter to continue and retry the flashing' notused
-
-  echo "  ${ADB_DEVICE}fastboot flashall -w"
-  fastboot flashall -w
-}
-
-flash()
-{
-  echo
-  echo "FLASHING"
-  echo "Rebooting device into fastboot mode."
-  echo "  ${ADB_DEVICE}adb reboot bootloader"
-  adb reboot bootloader
-
-  echo
-  echo "Waiting for device to reach fastboot mode."
-  echo "(will timeout after 60 seconds)"
-  # fastboot will block indefinitely until device comes online.
-  # Grab random variable to test if device is online.
-  # If takes >60s then we error out and ask the user for help.
-  timeout 60 fastboot getvar 0 2>/dev/null
-  fastboot_flash_status=$?
-
-  if [[ ${fastboot_flash_status} -eq 0 ]]; then
-    echo
-    echo "Flashing image."
-    echo "  ${ADB_DEVICE}fastboot flashall -w"
-    fastboot flashall -w
-    fastboot_flash_status=$?
-  fi
-
-  while [[ ${fastboot_flash_status} -ne 0 ]] ; do
-    echo
-    echo "fastboot flash has failed! From here you can:"
-    echo "1. Debug and/or flash manually"
-    echo "2. Retry flashing automatically"
-    echo "3. Abort this installation and skip this image"
-    echo "4. Abort this installation and mark test as failed"
-    sleep 1
-    read -p "Which method would you like to do? " choice
-    case $choice in
-      1) manual_flash && break;;
-      2) auto_flash && break;;
-      3) return 125;;
-      4) return 1;;
-      *) echo "Please answer 1, 2, 3, or 4.";;
-    esac
-  done
-}
-
-# Number of jobs make will use. Can be customized and played with.
-MAKE_JOBS=${BISECT_NUM_JOBS}
-
-# Set ADB_DEVICE to "ANDROID_SERIAL=${ANDROID_SERIAL}" or "" if device id not
-# set. This is used for debugging info so users can confirm which device
-# commands are being sent to.
-ADB_DEVICE=${ANDROID_SERIAL:+"ANDROID_SERIAL=${ANDROID_SERIAL} "}
-
-echo
-echo "INSTALLATION BEGIN"
-echo
-
-cd ${BISECT_ANDROID_SRC}
-
-echo "BUILDING IMAGE"
-
-make -j ${MAKE_JOBS}
-make_status=$?
-
-exit_val=0
-if [[ ${make_status} -eq 0 ]]; then
-  flash
-  exit_val=$?
-else
-  echo "ERROR:"
-  echo "make returned a non-zero status: ${make_status}. Skipping image..."
-  exit_val=1
-fi
-
-
-exit ${exit_val}
diff --git a/binary_search_tool/binary_search_perforce.py b/binary_search_tool/binary_search_perforce.py
deleted file mode 100755
index 01756b8e..00000000
--- a/binary_search_tool/binary_search_perforce.py
+++ /dev/null
@@ -1,577 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Module of binary serch for perforce."""
-
-import argparse
-import math
-import os
-import re
-import sys
-import tempfile
-
-from cros_utils import command_executer
-from cros_utils import logger
-
-
-verbose = True
-
-
-def _GetP4ClientSpec(client_name, p4_paths):
-    p4_string = ""
-    for p4_path in p4_paths:
-        if " " not in p4_path:
-            p4_string += " -a %s" % p4_path
-        else:
-            p4_string += (
-                ' -a "' + (" //" + client_name + "/").join(p4_path) + '"'
-            )
-
-    return p4_string
-
-
-def GetP4Command(client_name, p4_port, p4_paths, checkoutdir, p4_snapshot=""):
-    command = ""
-
-    if p4_snapshot:
-        command += "mkdir -p " + checkoutdir
-        for p4_path in p4_paths:
-            real_path = p4_path[1]
-            if real_path.endswith("..."):
-                real_path = real_path.replace("/...", "")
-                command += (
-                    "; mkdir -p "
-                    + checkoutdir
-                    + "/"
-                    + os.path.dirname(real_path)
-                )
-                command += (
-                    "&& rsync -lr "
-                    + p4_snapshot
-                    + "/"
-                    + real_path
-                    + " "
-                    + checkoutdir
-                    + "/"
-                    + os.path.dirname(real_path)
-                )
-        return command
-
-    command += " export P4CONFIG=.p4config"
-    command += " && mkdir -p " + checkoutdir
-    command += " && cd " + checkoutdir
-    command += " && cp ${HOME}/.p4config ."
-    command += " && chmod u+w .p4config"
-    command += ' && echo "P4PORT=' + p4_port + '" >> .p4config'
-    command += ' && echo "P4CLIENT=' + client_name + '" >> .p4config'
-    command += " && g4 client " + _GetP4ClientSpec(client_name, p4_paths)
-    command += " && g4 sync "
-    command += " && cd -"
-    return command
-
-
-class BinarySearchPoint(object):
-    """Class of binary search point."""
-
-    def __init__(self, revision, status, tag=None):
-        self.revision = revision
-        self.status = status
-        self.tag = tag
-
-
-class BinarySearcherForPass(object):
-    """Class of pass level binary searcher."""
-
-    def __init__(self, logger_to_set=None):
-        self.current = 0
-        self.lo = 0
-        self.hi = 0
-        self.total = 0
-        if logger_to_set is not None:
-            self.logger = logger_to_set
-        else:
-            self.logger = logger.GetLogger()
-
-    def GetNext(self):
-        # For the first run, update self.hi with total pass/transformation count
-        if self.hi == 0:
-            self.hi = self.total
-        self.current = (self.hi + self.lo) // 2
-        message = "Bisecting between: (%d, %d)" % (self.lo, self.hi)
-        self.logger.LogOutput(message, print_to_console=verbose)
-        message = "Current limit number: %d" % self.current
-        self.logger.LogOutput(message, print_to_console=verbose)
-        return self.current
-
-    def SetStatus(self, status):
-        """Set lo/hi status based on test script result
-
-        If status == 0, it means that runtime error is not introduced until current
-        pass/transformation, so we need to increase lower bound for binary search.
-
-        If status == 1, it means that runtime error still happens with current pass/
-        transformation, so we need to decrease upper bound for binary search.
-
-        Returns:
-          True if we find the bad pass/transformation, or cannot find bad one after
-          decreasing to the first pass/transformation. Otherwise False.
-        """
-        assert status in (0, 1, 125), status
-
-        if self.current == 0:
-            message = (
-                "Runtime error occurs before first pass/transformation. "
-                "Stop binary searching."
-            )
-            self.logger.LogOutput(message, print_to_console=verbose)
-            return True
-
-        if status == 0:
-            message = "Runtime error is not reproduced, increasing lower bound."
-            self.logger.LogOutput(message, print_to_console=verbose)
-            self.lo = self.current + 1
-        elif status == 1:
-            message = "Runtime error is reproduced, decreasing upper bound.."
-            self.logger.LogOutput(message, print_to_console=verbose)
-            self.hi = self.current
-
-        if self.lo >= self.hi:
-            return True
-
-        return False
-
-
-class BinarySearcher(object):
-    """Class of binary searcher."""
-
-    def __init__(self, logger_to_set=None):
-        self.sorted_list = []
-        self.index_log = []
-        self.status_log = []
-        self.skipped_indices = []
-        self.current = 0
-        self.points = {}
-        self.lo = 0
-        self.hi = 0
-        if logger_to_set is not None:
-            self.logger = logger_to_set
-        else:
-            self.logger = logger.GetLogger()
-
-    def SetSortedList(self, sorted_list):
-        assert sorted_list
-        self.sorted_list = sorted_list
-        self.index_log = []
-        self.hi = len(sorted_list) - 1
-        self.lo = 0
-        self.points = {}
-        for i in range(len(self.sorted_list)):
-            bsp = BinarySearchPoint(self.sorted_list[i], -1, "Not yet done.")
-            self.points[i] = bsp
-
-    def SetStatus(self, status, tag=None):
-        message = "Revision: %s index: %d returned: %d" % (
-            self.sorted_list[self.current],
-            self.current,
-            status,
-        )
-        self.logger.LogOutput(message, print_to_console=verbose)
-        assert status in (0, 1, 125), status
-        self.index_log.append(self.current)
-        self.status_log.append(status)
-        bsp = BinarySearchPoint(self.sorted_list[self.current], status, tag)
-        self.points[self.current] = bsp
-
-        if status == 125:
-            self.skipped_indices.append(self.current)
-
-        if status in (0, 1):
-            if status == 0:
-                self.lo = self.current + 1
-            elif status == 1:
-                self.hi = self.current
-            self.logger.LogOutput("lo: %d hi: %d\n" % (self.lo, self.hi))
-            self.current = (self.lo + self.hi) // 2
-
-        if self.lo == self.hi:
-            message = (
-                "Search complete. First bad version: %s"
-                " at index: %d"
-                % (
-                    self.sorted_list[self.current],
-                    self.lo,
-                )
-            )
-            self.logger.LogOutput(message)
-            return True
-
-        for index in range(self.lo, self.hi):
-            if index not in self.skipped_indices:
-                return False
-        self.logger.LogOutput(
-            "All skipped indices between: %d and %d\n" % (self.lo, self.hi),
-            print_to_console=verbose,
-        )
-        return True
-
-    # Does a better job with chromeos flakiness.
-    def GetNextFlakyBinary(self):
-        t = (self.lo, self.current, self.hi)
-        q = [t]
-        while q:
-            element = q.pop(0)
-            if element[1] in self.skipped_indices:
-                # Go top
-                to_add = (
-                    element[0],
-                    (element[0] + element[1]) // 2,
-                    element[1],
-                )
-                q.append(to_add)
-                # Go bottom
-                to_add = (
-                    element[1],
-                    (element[1] + element[2]) // 2,
-                    element[2],
-                )
-                q.append(to_add)
-            else:
-                self.current = element[1]
-                return
-        assert q, "Queue should never be 0-size!"
-
-    def GetNextFlakyLinear(self):
-        current_hi = self.current
-        current_lo = self.current
-        while True:
-            if current_hi < self.hi and current_hi not in self.skipped_indices:
-                self.current = current_hi
-                break
-            if current_lo >= self.lo and current_lo not in self.skipped_indices:
-                self.current = current_lo
-                break
-            if current_lo < self.lo and current_hi >= self.hi:
-                break
-
-            current_hi += 1
-            current_lo -= 1
-
-    def GetNext(self):
-        self.current = (self.hi + self.lo) // 2
-        # Try going forward if current is skipped.
-        if self.current in self.skipped_indices:
-            self.GetNextFlakyBinary()
-
-        # TODO: Add an estimated time remaining as well.
-        message = "Estimated tries: min: %d max: %d\n" % (
-            1 + math.log(self.hi - self.lo, 2),
-            self.hi - self.lo - len(self.skipped_indices),
-        )
-        self.logger.LogOutput(message, print_to_console=verbose)
-        message = "lo: %d hi: %d current: %d version: %s\n" % (
-            self.lo,
-            self.hi,
-            self.current,
-            self.sorted_list[self.current],
-        )
-        self.logger.LogOutput(message, print_to_console=verbose)
-        self.logger.LogOutput(str(self), print_to_console=verbose)
-        return self.sorted_list[self.current]
-
-    def SetLoRevision(self, lo_revision):
-        self.lo = self.sorted_list.index(lo_revision)
-
-    def SetHiRevision(self, hi_revision):
-        self.hi = self.sorted_list.index(hi_revision)
-
-    def GetAllPoints(self):
-        to_return = ""
-        for i in range(len(self.sorted_list)):
-            to_return += "%d %d %s\n" % (
-                self.points[i].status,
-                i,
-                self.points[i].revision,
-            )
-
-        return to_return
-
-    def __str__(self):
-        to_return = ""
-        to_return += "Current: %d\n" % self.current
-        to_return += str(self.index_log) + "\n"
-        revision_log = []
-        for index in self.index_log:
-            revision_log.append(self.sorted_list[index])
-        to_return += str(revision_log) + "\n"
-        to_return += str(self.status_log) + "\n"
-        to_return += "Skipped indices:\n"
-        to_return += str(self.skipped_indices) + "\n"
-        to_return += self.GetAllPoints()
-        return to_return
-
-
-class RevisionInfo(object):
-    """Class of reversion info."""
-
-    def __init__(self, date, client, description):
-        self.date = date
-        self.client = client
-        self.description = description
-        self.status = -1
-
-
-class VCSBinarySearcher(object):
-    """Class of VCS binary searcher."""
-
-    def __init__(self):
-        self.bs = BinarySearcher()
-        self.rim = {}
-        self.current_ce = None
-        self.checkout_dir = None
-        self.current_revision = None
-
-    def Initialize(self):
-        pass
-
-    def GetNextRevision(self):
-        pass
-
-    def CheckoutRevision(self, current_revision):
-        pass
-
-    def SetStatus(self, status):
-        pass
-
-    def Cleanup(self):
-        pass
-
-    def SetGoodRevision(self, revision):
-        if revision is None:
-            return
-        assert revision in self.bs.sorted_list
-        self.bs.SetLoRevision(revision)
-
-    def SetBadRevision(self, revision):
-        if revision is None:
-            return
-        assert revision in self.bs.sorted_list
-        self.bs.SetHiRevision(revision)
-
-
-class P4BinarySearcher(VCSBinarySearcher):
-    """Class of P4 binary searcher."""
-
-    def __init__(self, p4_port, p4_paths, test_command):
-        VCSBinarySearcher.__init__(self)
-        self.p4_port = p4_port
-        self.p4_paths = p4_paths
-        self.test_command = test_command
-        self.checkout_dir = tempfile.mkdtemp()
-        self.ce = command_executer.GetCommandExecuter()
-        self.client_name = "binary-searcher-$HOSTNAME-$USER"
-        self.job_log_root = "/home/asharif/www/coreboot_triage/"
-        self.changes = None
-
-    def Initialize(self):
-        self.Cleanup()
-        command = GetP4Command(
-            self.client_name, self.p4_port, self.p4_paths, 1, self.checkout_dir
-        )
-        self.ce.RunCommand(command)
-        command = "cd %s && g4 changes ..." % self.checkout_dir
-        _, out, _ = self.ce.RunCommandWOutput(command)
-        self.changes = re.findall(r"Change (\d+)", out)
-        change_infos = re.findall(
-            r"Change (\d+) on ([\d/]+) by " r"([^\s]+) ('[^']*')", out
-        )
-        for change_info in change_infos:
-            ri = RevisionInfo(change_info[1], change_info[2], change_info[3])
-            self.rim[change_info[0]] = ri
-        # g4 gives changes in reverse chronological order.
-        self.changes.reverse()
-        self.bs.SetSortedList(self.changes)
-
-    def SetStatus(self, status):
-        self.rim[self.current_revision].status = status
-        return self.bs.SetStatus(status)
-
-    def GetNextRevision(self):
-        next_revision = self.bs.GetNext()
-        self.current_revision = next_revision
-        return next_revision
-
-    def CleanupCLs(self):
-        if not os.path.isfile(self.checkout_dir + "/.p4config"):
-            command = "cd %s" % self.checkout_dir
-            command += " && cp ${HOME}/.p4config ."
-            command += ' && echo "P4PORT=' + self.p4_port + '" >> .p4config'
-            command += (
-                ' && echo "P4CLIENT=' + self.client_name + '" >> .p4config'
-            )
-            self.ce.RunCommand(command)
-        command = "cd %s" % self.checkout_dir
-        command += "; g4 changes -c %s" % self.client_name
-        _, out, _ = self.ce.RunCommandWOutput(command)
-        changes = re.findall(r"Change (\d+)", out)
-        if changes:
-            command = "cd %s" % self.checkout_dir
-            for change in changes:
-                command += "; g4 revert -c %s" % change
-            self.ce.RunCommand(command)
-
-    def CleanupClient(self):
-        command = "cd %s" % self.checkout_dir
-        command += "; g4 revert ..."
-        command += "; g4 client -d %s" % self.client_name
-        self.ce.RunCommand(command)
-
-    def Cleanup(self):
-        self.CleanupCLs()
-        self.CleanupClient()
-
-    def __str__(self):
-        to_return = ""
-        for change in self.changes:
-            ri = self.rim[change]
-            if ri.status == -1:
-                to_return = "%s\t%d\n" % (change, ri.status)
-            else:
-                to_return += "%s\t%d\t%s\t%s\t%s\t%s\t%s\t%s\n" % (
-                    change,
-                    ri.status,
-                    ri.date,
-                    ri.client,
-                    ri.description,
-                    self.job_log_root + change + ".cmd",
-                    self.job_log_root + change + ".out",
-                    self.job_log_root + change + ".err",
-                )
-        return to_return
-
-
-class P4GCCBinarySearcher(P4BinarySearcher):
-    """Class of P4 gcc binary searcher."""
-
-    # TODO: eventually get these patches from g4 instead of creating them manually
-    def HandleBrokenCLs(self, current_revision):
-        cr = int(current_revision)
-        problematic_ranges = []
-        problematic_ranges.append([44528, 44539])
-        problematic_ranges.append([44528, 44760])
-        problematic_ranges.append([44335, 44882])
-        command = "pwd"
-        for pr in problematic_ranges:
-            if cr in range(pr[0], pr[1]):
-                patch_file = "/home/asharif/triage_tool/%d-%d.patch" % (
-                    pr[0],
-                    pr[1],
-                )
-                with open(patch_file, encoding="utf-8") as f:
-                    patch = f.read()
-                files = re.findall("--- (//.*)", patch)
-                command += "; cd %s" % self.checkout_dir
-                for f in files:
-                    command += "; g4 open %s" % f
-                command += "; patch -p2 < %s" % patch_file
-        self.current_ce.RunCommand(command)
-
-    def CheckoutRevision(self, current_revision):
-        job_logger = logger.Logger(
-            self.job_log_root, current_revision, True, subdir=""
-        )
-        self.current_ce = command_executer.GetCommandExecuter(job_logger)
-
-        self.CleanupCLs()
-        # Change the revision of only the gcc part of the toolchain.
-        command = (
-            "cd %s/gcctools/google_vendor_src_branch/gcc "
-            "&& g4 revert ...; g4 sync @%s"
-            % (self.checkout_dir, current_revision)
-        )
-        self.current_ce.RunCommand(command)
-
-        self.HandleBrokenCLs(current_revision)
-
-
-def Main(argv):
-    """The main function."""
-    # Common initializations
-    ###  command_executer.InitCommandExecuter(True)
-    ce = command_executer.GetCommandExecuter()
-
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "-n",
-        "--num_tries",
-        dest="num_tries",
-        default="100",
-        help="Number of tries.",
-    )
-    parser.add_argument(
-        "-g",
-        "--good_revision",
-        dest="good_revision",
-        help="Last known good revision.",
-    )
-    parser.add_argument(
-        "-b",
-        "--bad_revision",
-        dest="bad_revision",
-        help="Last known bad revision.",
-    )
-    parser.add_argument(
-        "-s", "--script", dest="script", help="Script to run for every version."
-    )
-    options = parser.parse_args(argv)
-    # First get all revisions
-    p4_paths = [
-        "//depot2/gcctools/google_vendor_src_branch/gcc/gcc-4.4.3/...",
-        "//depot2/gcctools/google_vendor_src_branch/binutils/"
-        "binutils-2.20.1-mobile/...",
-        "//depot2/gcctools/google_vendor_src_branch/"
-        "binutils/binutils-20100303/...",
-    ]
-    p4gccbs = P4GCCBinarySearcher("perforce2:2666", p4_paths, "")
-
-    # Main loop:
-    terminated = False
-    num_tries = int(options.num_tries)
-    script = os.path.expanduser(options.script)
-
-    try:
-        p4gccbs.Initialize()
-        p4gccbs.SetGoodRevision(options.good_revision)
-        p4gccbs.SetBadRevision(options.bad_revision)
-        while not terminated and num_tries > 0:
-            current_revision = p4gccbs.GetNextRevision()
-
-            # Now run command to get the status
-            ce = command_executer.GetCommandExecuter()
-            command = "%s %s" % (script, p4gccbs.checkout_dir)
-            status = ce.RunCommand(command)
-            message = "Revision: %s produced: %d status\n" % (
-                current_revision,
-                status,
-            )
-            logger.GetLogger().LogOutput(message, print_to_console=verbose)
-            terminated = p4gccbs.SetStatus(status)
-            num_tries -= 1
-            logger.GetLogger().LogOutput(str(p4gccbs), print_to_console=verbose)
-
-        if not terminated:
-            logger.GetLogger().LogOutput(
-                "Tries: %d expired." % num_tries, print_to_console=verbose
-            )
-        logger.GetLogger().LogOutput(str(p4gccbs.bs), print_to_console=verbose)
-    except (KeyboardInterrupt, SystemExit):
-        logger.GetLogger().LogOutput("Cleaning up...")
-    finally:
-        logger.GetLogger().LogOutput(str(p4gccbs.bs), print_to_console=verbose)
-        p4gccbs.Cleanup()
-
-
-if __name__ == "__main__":
-    Main(sys.argv[1:])
diff --git a/binary_search_tool/binary_search_state.py b/binary_search_tool/binary_search_state.py
deleted file mode 100755
index 1b423b5c..00000000
--- a/binary_search_tool/binary_search_state.py
+++ /dev/null
@@ -1,1020 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""The binary search wrapper."""
-
-
-import argparse
-import contextlib
-import errno
-import math
-import os
-import pickle
-import re
-import shutil
-import sys
-import tempfile
-import time
-
-# Adds cros_utils to PYTHONPATH
-from binary_search_tool import binary_search_perforce
-from binary_search_tool import common
-from binary_search_tool import pass_mapping
-
-# Now we do import from cros_utils
-from cros_utils import command_executer
-from cros_utils import logger
-
-
-GOOD_SET_VAR = "BISECT_GOOD_SET"
-BAD_SET_VAR = "BISECT_BAD_SET"
-
-STATE_FILE = "%s.state" % sys.argv[0]
-HIDDEN_STATE_FILE = os.path.join(
-    os.path.dirname(STATE_FILE), ".%s" % os.path.basename(STATE_FILE)
-)
-
-
-@contextlib.contextmanager
-def SetFile(env_var, items):
-    """Generate set files that can be used by switch/test scripts.
-
-    Generate temporary set file (good/bad) holding contents of good/bad items for
-    the current binary search iteration. Store the name of each file as an
-    environment variable so all child processes can access it.
-
-    This function is a contextmanager, meaning it's meant to be used with the
-    "with" statement in Python. This is so cleanup and setup happens automatically
-    and cleanly. Execution of the outer "with" statement happens at the "yield"
-    statement.
-
-    Args:
-      env_var: What environment variable to store the file name in.
-      items: What items are in this set.
-    """
-    with tempfile.NamedTemporaryFile("w", encoding="utf-8") as f:
-        os.environ[env_var] = f.name
-        f.write("\n".join(items))
-        f.flush()
-        yield
-
-
-class BinarySearchState(object):
-    """The binary search state class."""
-
-    def __init__(
-        self,
-        get_initial_items,
-        switch_to_good,
-        switch_to_bad,
-        test_setup_script,
-        test_script,
-        incremental,
-        prune,
-        pass_bisect,
-        ir_diff,
-        iterations,
-        prune_iterations,
-        verify,
-        file_args,
-        verbose,
-    ):
-        """BinarySearchState constructor, see Run for full args documentation."""
-        self.get_initial_items = get_initial_items
-        self.switch_to_good = switch_to_good
-        self.switch_to_bad = switch_to_bad
-        self.test_setup_script = test_setup_script
-        self.test_script = test_script
-        self.incremental = incremental
-        self.prune = prune
-        self.pass_bisect = pass_bisect
-        self.ir_diff = ir_diff
-        self.iterations = iterations
-        self.prune_iterations = prune_iterations
-        self.verify = verify
-        self.file_args = file_args
-        self.verbose = verbose
-
-        self.l = logger.GetLogger()
-        self.ce = command_executer.GetCommandExecuter()
-
-        self.resumed = False
-        self.prune_cycles = 0
-        self.search_cycles = 0
-        self.binary_search = None
-        self.all_items = None
-        self.cmd_script = None
-        self.mode = None
-        self.PopulateItemsUsingCommand(self.get_initial_items)
-        self.currently_good_items = set()
-        self.currently_bad_items = set()
-        self.found_items = set()
-        self.known_good = set()
-
-        self.start_time = time.time()
-
-    def SwitchToGood(self, item_list):
-        """Switch given items to "good" set."""
-        if self.incremental:
-            self.l.LogOutput(
-                "Incremental set. Wanted to switch %s to good" % str(item_list),
-                print_to_console=self.verbose,
-            )
-            incremental_items = [
-                item
-                for item in item_list
-                if item not in self.currently_good_items
-            ]
-            item_list = incremental_items
-            self.l.LogOutput(
-                "Incremental set. Actually switching %s to good"
-                % str(item_list),
-                print_to_console=self.verbose,
-            )
-
-        if not item_list:
-            return
-
-        self.l.LogOutput(
-            "Switching %s to good" % str(item_list),
-            print_to_console=self.verbose,
-        )
-        self.RunSwitchScript(self.switch_to_good, item_list)
-        self.currently_good_items = self.currently_good_items.union(
-            set(item_list)
-        )
-        self.currently_bad_items.difference_update(set(item_list))
-
-    def SwitchToBad(self, item_list):
-        """Switch given items to "bad" set."""
-        if self.incremental:
-            self.l.LogOutput(
-                "Incremental set. Wanted to switch %s to bad" % str(item_list),
-                print_to_console=self.verbose,
-            )
-            incremental_items = [
-                item
-                for item in item_list
-                if item not in self.currently_bad_items
-            ]
-            item_list = incremental_items
-            self.l.LogOutput(
-                "Incremental set. Actually switching %s to bad"
-                % str(item_list),
-                print_to_console=self.verbose,
-            )
-
-        if not item_list:
-            return
-
-        self.l.LogOutput(
-            "Switching %s to bad" % str(item_list),
-            print_to_console=self.verbose,
-        )
-        self.RunSwitchScript(self.switch_to_bad, item_list)
-        self.currently_bad_items = self.currently_bad_items.union(
-            set(item_list)
-        )
-        self.currently_good_items.difference_update(set(item_list))
-
-    def RunSwitchScript(self, switch_script, item_list):
-        """Pass given items to switch script.
-
-        Args:
-          switch_script: path to switch script
-          item_list: list of all items to be switched
-        """
-        if self.file_args:
-            with tempfile.NamedTemporaryFile("w", encoding="utf-8") as f:
-                f.write("\n".join(item_list))
-                f.flush()
-                command = "%s %s" % (switch_script, f.name)
-                ret, _, _ = self.ce.RunCommandWExceptionCleanup(
-                    command, print_to_console=self.verbose
-                )
-        else:
-            command = "%s %s" % (switch_script, " ".join(item_list))
-            try:
-                ret, _, _ = self.ce.RunCommandWExceptionCleanup(
-                    command, print_to_console=self.verbose
-                )
-            except OSError as e:
-                if e.errno == errno.E2BIG:
-                    raise RuntimeError(
-                        "Too many arguments for switch script! Use "
-                        "--file_args"
-                    )
-        assert ret == 0, "Switch script %s returned %d" % (switch_script, ret)
-
-    def TestScript(self):
-        """Run test script and return exit code from script."""
-        command = self.test_script
-        ret, _, _ = self.ce.RunCommandWExceptionCleanup(command)
-        return ret
-
-    def TestSetupScript(self):
-        """Run test setup script and return exit code from script."""
-        if not self.test_setup_script:
-            return 0
-
-        command = self.test_setup_script
-        ret, _, _ = self.ce.RunCommandWExceptionCleanup(command)
-        return ret
-
-    def GenerateBadCommandScript(self, bad_items):
-        """Generate command line script for building bad item."""
-        assert not self.prune, "Prune must be false if pass_bisect is set."
-        assert len(bad_items) == 1, (
-            "Pruning is off, but number of bad " "items found was not 1."
-        )
-        item = list(bad_items)[0]
-        command = "%s %s" % (self.pass_bisect, item)
-        ret, _, _ = self.ce.RunCommandWExceptionCleanup(
-            command, print_to_console=self.verbose
-        )
-        return ret
-
-    def DoVerify(self):
-        """Verify correctness of test environment.
-
-        Verify that a "good" set of items produces a "good" result and that a "bad"
-        set of items produces a "bad" result. To be run directly before running
-        DoSearch. If verify is False this step is skipped.
-        """
-        if not self.verify:
-            return
-
-        self.l.LogOutput("VERIFICATION")
-        self.l.LogOutput("Beginning tests to verify good/bad sets\n")
-
-        self._OutputProgress("Verifying items from GOOD set\n")
-        with SetFile(GOOD_SET_VAR, self.all_items), SetFile(BAD_SET_VAR, []):
-            self.l.LogOutput("Resetting all items to good to verify.")
-            self.SwitchToGood(self.all_items)
-            status = self.TestSetupScript()
-            assert status == 0, "When reset_to_good, test setup should succeed."
-            status = self.TestScript()
-            assert status == 0, "When reset_to_good, status should be 0."
-
-        self._OutputProgress("Verifying items from BAD set\n")
-        with SetFile(GOOD_SET_VAR, []), SetFile(BAD_SET_VAR, self.all_items):
-            self.l.LogOutput("Resetting all items to bad to verify.")
-            self.SwitchToBad(self.all_items)
-            status = self.TestSetupScript()
-            # The following assumption is not true; a bad image might not
-            # successfully push onto a device.
-            # assert status == 0, 'When reset_to_bad, test setup should succeed.'
-            if status == 0:
-                status = self.TestScript()
-            assert status == 1, "When reset_to_bad, status should be 1."
-
-    def DoSearchBadItems(self):
-        """Perform full search for bad items.
-
-        Perform full search until prune_iterations number of bad items are found.
-        """
-        while (
-            True
-            and len(self.all_items) > 1
-            and self.prune_cycles < self.prune_iterations
-        ):
-            terminated = self.DoBinarySearchBadItems()
-            self.prune_cycles += 1
-            if not terminated:
-                break
-            # Prune is set.
-            prune_index = self.binary_search.current
-
-            # If found item is last item, no new items can be found
-            if prune_index == len(self.all_items) - 1:
-                self.l.LogOutput("First bad item is the last item. Breaking.")
-                self.l.LogOutput("Bad items are: %s" % self.all_items[-1])
-                self.found_items.add(self.all_items[-1])
-                break
-
-            # If already seen item we have no new bad items to find, finish up
-            if self.all_items[prune_index] in self.found_items:
-                self.l.LogOutput(
-                    "Found item already found before: %s."
-                    % self.all_items[prune_index],
-                    print_to_console=self.verbose,
-                )
-                self.l.LogOutput("No more bad items remaining. Done searching.")
-                self.l.LogOutput(
-                    "Bad items are: %s" % " ".join(self.found_items)
-                )
-                break
-
-            new_all_items = list(self.all_items)
-            # Move prune item to the end of the list.
-            new_all_items.append(new_all_items.pop(prune_index))
-            self.found_items.add(new_all_items[-1])
-
-            # Everything below newly found bad item is now known to be a good item.
-            # Take these good items out of the equation to save time on the next
-            # search. We save these known good items so they are still sent to the
-            # switch_to_good script.
-            if prune_index:
-                self.known_good.update(new_all_items[:prune_index])
-                new_all_items = new_all_items[prune_index:]
-
-            self.l.LogOutput(
-                "Old list: %s. New list: %s"
-                % (str(self.all_items), str(new_all_items)),
-                print_to_console=self.verbose,
-            )
-
-            if not self.prune:
-                self.l.LogOutput("Not continuning further, --prune is not set")
-                break
-            # FIXME: Do we need to Convert the currently good items to bad
-            self.PopulateItemsUsingList(new_all_items)
-
-        # If pass level bisecting is set, generate a script which contains command
-        # line options to rebuild bad item.
-        if self.pass_bisect:
-            status = self.GenerateBadCommandScript(self.found_items)
-            if status == 0:
-                self.cmd_script = os.path.join(
-                    os.path.dirname(self.pass_bisect), "cmd_script.sh"
-                )
-                self.l.LogOutput(
-                    "Command script generated at %s." % self.cmd_script
-                )
-            else:
-                raise RuntimeError("Error while generating command script.")
-
-    def DoBinarySearchBadItems(self):
-        """Perform single iteration of binary search."""
-        # If in resume mode don't reset search_cycles
-        if not self.resumed:
-            self.search_cycles = 0
-        else:
-            self.resumed = False
-
-        terminated = False
-        while self.search_cycles < self.iterations and not terminated:
-            self.SaveState()
-            self.OutputIterationProgressBadItem()
-
-            self.search_cycles += 1
-            [bad_items, good_items] = self.GetNextItems()
-
-            with SetFile(GOOD_SET_VAR, good_items), SetFile(
-                BAD_SET_VAR, bad_items
-            ):
-                # TODO: bad_items should come first.
-                self.SwitchToGood(good_items)
-                self.SwitchToBad(bad_items)
-                status = self.TestSetupScript()
-                if status == 0:
-                    status = self.TestScript()
-                terminated = self.binary_search.SetStatus(status)
-
-            if terminated:
-                self.l.LogOutput("Terminated!", print_to_console=self.verbose)
-        if not terminated:
-            self.l.LogOutput("Ran out of iterations searching...")
-        self.l.LogOutput(str(self), print_to_console=self.verbose)
-        return terminated
-
-    def CollectPassName(self, pass_info):
-        """Mapping opt-bisect output of pass info to debugcounter name."""
-        self.l.LogOutput(
-            "Pass info: %s" % pass_info, print_to_console=self.verbose
-        )
-
-        for desc in pass_mapping.pass_name:
-            if desc in pass_info:
-                return pass_mapping.pass_name[desc]
-
-        # If pass not found, return None
-        return None
-
-    def BuildWithPassLimit(self, limit, generate_ir=False):
-        """Rebuild bad item with pass level bisect limit
-
-        Run command line script generated by GenerateBadCommandScript(), with
-        pass level limit flags.
-
-        Returns:
-          pass_num: current number of the pass, or total number of passes if
-                    limit set to -1.
-          pass_name: The debugcounter name of current limit pass.
-        """
-        os.environ["LIMIT_FLAGS"] = "-mllvm -opt-bisect-limit=" + str(limit)
-        if generate_ir:
-            os.environ["LIMIT_FLAGS"] += " -S -emit-llvm"
-        self.l.LogOutput(
-            "Limit flags: %s" % os.environ["LIMIT_FLAGS"],
-            print_to_console=self.verbose,
-        )
-        command = self.cmd_script
-        _, _, msg = self.ce.RunCommandWOutput(command, print_to_console=False)
-
-        # Massages we get will be like this:
-        #   BISECT: running pass (9) <Pass Description> on <function> (<file>)
-        #   BISECT: running pass (10) <Pass Description> on <module> (<file>)
-        #   BISECT: NOT running pass (11) <Pass Description> on <SCG> (<file>)
-        #   BISECT: NOT running pass (12) <Pass Description> on <SCG> (<file>)
-        # We want to get the pass description of last running pass, to have
-        # transformation level bisect on it.
-        if "BISECT: " not in msg:
-            raise RuntimeError(
-                "No bisect info printed, OptBisect may not be "
-                "supported by the compiler."
-            )
-
-        lines = msg.split("\n")
-        pass_num = 0
-        last_pass = ""
-        for l in lines:
-            if "running pass" in l:
-                # For situation of limit==-1, we want the total number of passes
-                if limit != -1 and "BISECT: NOT " in l:
-                    break
-                pass_num += 1
-                last_pass = l
-        if limit not in (-1, pass_num):
-            raise ValueError(
-                "[Error] While building, limit number does not match."
-            )
-        return pass_num, self.CollectPassName(last_pass)
-
-    def BuildWithTransformLimit(
-        self, limit, pass_name=None, pass_limit=-1, generate_ir=False
-    ):
-        """Rebuild bad item with transformation level bisect limit
-
-        Run command line script generated by GenerateBadCommandScript(), with
-        pass level limit flags and transformation level limit flags.
-
-        Args:
-          limit: transformation level limit for bad item.
-          pass_name: name of bad pass debugcounter from pass level bisect result.
-          pass_limit: pass level limit from pass level bisect result.
-          generate_ir: Whether to generate IR comparison.
-
-        Returns:
-          Total number of transformations if limit set to -1, else return 0.
-        """
-        counter_name = pass_name
-
-        os.environ["LIMIT_FLAGS"] = (
-            "-mllvm -opt-bisect-limit="
-            + str(pass_limit)
-            + " -mllvm -debug-counter="
-            + counter_name
-            + "-count="
-            + str(limit)
-            + " -mllvm -print-debug-counter"
-        )
-        if generate_ir:
-            os.environ["LIMIT_FLAGS"] += " -S -emit-llvm"
-        self.l.LogOutput(
-            "Limit flags: %s" % os.environ["LIMIT_FLAGS"],
-            print_to_console=self.verbose,
-        )
-        command = self.cmd_script
-        _, _, msg = self.ce.RunCommandWOutput(command, print_to_console=False)
-
-        if "Counters and values:" not in msg:
-            # Print pass level IR diff only if transformation level bisection does
-            # not work.
-            if self.ir_diff:
-                self.PrintIRDiff(pass_limit)
-            raise RuntimeError(
-                "No bisect info printed, DebugCounter may not be "
-                "supported by the compiler."
-            )
-
-        # With debugcounter enabled, there will be DebugCounter counting info in
-        # the output.
-        lines = msg.split("\n")
-        for l in lines:
-            if pass_name in l:
-                # Output of debugcounter will be like:
-                #   instcombine-visit: {10, 0, 20}
-                #   dce-transform: {1, 0, -1}
-                # which indicates {Count, Skip, StopAfter}.
-                # The last number should be the limit we set.
-                # We want the first number as the total transformation count.
-                # Split each line by ,|{|} and we can get l_list as:
-                #   ['instcombine: ', '10', '0', '20', '']
-                # and we will need the second item in it.
-                l_list = re.split(",|{|}", l)
-                count = int(l_list[1])
-                if limit == -1:
-                    return count
-        # The returned value is only useful when limit == -1, which shows total
-        # transformation count.
-        return 0
-
-    def PrintIRDiff(self, pass_index, pass_name=None, trans_index=-1):
-        bad_item = list(self.found_items)[0]
-        self.l.LogOutput(
-            "IR difference before and after bad pass/transformation:",
-            print_to_console=self.verbose,
-        )
-
-        if trans_index == -1:
-            # Pass level IR diff
-            self.BuildWithPassLimit(pass_index, self.ir_diff)
-            good_ir = os.path.join(tempfile.tempdir, "good.s")
-            shutil.copyfile(bad_item, good_ir)
-            pass_index += 1
-            self.BuildWithPassLimit(pass_index, self.ir_diff)
-        else:
-            # Transformation level IR diff
-            self.BuildWithTransformLimit(
-                trans_index, pass_name, pass_index, self.ir_diff
-            )
-            good_ir = os.path.join(tempfile.tempdir, "good.s")
-            shutil.copyfile(bad_item, good_ir)
-            trans_index += 1
-            self.BuildWithTransformLimit(
-                trans_index, pass_name, pass_index, self.ir_diff
-            )
-
-        bad_ir = os.path.join(tempfile.tempdir, "bad.s")
-        shutil.copyfile(bad_item, bad_ir)
-
-        command = "diff %s %s" % (good_ir, bad_ir)
-        _, _, _ = self.ce.RunCommandWOutput(
-            command, print_to_console=self.verbose
-        )
-
-    def DoSearchBadPass(self):
-        """Perform full search for bad pass of bad item."""
-        logger.GetLogger().LogOutput(
-            "Starting to bisect bad pass for bad item."
-        )
-
-        # Pass level bisection
-        self.mode = "pass"
-        self.binary_search = binary_search_perforce.BinarySearcherForPass(
-            logger_to_set=self.l
-        )
-        self.binary_search.total, _ = self.BuildWithPassLimit(-1)
-        logger.GetLogger().LogOutput(
-            "Total %s number: %d" % (self.mode, self.binary_search.total)
-        )
-
-        pass_index, pass_name = self.DoBinarySearchBadPass()
-
-        if not pass_name and pass_index == 0:
-            raise ValueError("Bisecting passes cannot reproduce good result.")
-        logger.GetLogger().LogOutput("Bad pass found: %s." % pass_name)
-
-        # Transformation level bisection.
-        logger.GetLogger().LogOutput(
-            "Starting to bisect at transformation level."
-        )
-
-        self.mode = "transform"
-        self.binary_search = binary_search_perforce.BinarySearcherForPass(
-            logger_to_set=self.l
-        )
-        self.binary_search.total = self.BuildWithTransformLimit(
-            -1, pass_name, pass_index
-        )
-        logger.GetLogger().LogOutput(
-            "Total %s number: %d" % (self.mode, self.binary_search.total)
-        )
-
-        trans_index, _ = self.DoBinarySearchBadPass(pass_index, pass_name)
-        if trans_index == 0:
-            raise ValueError(
-                "Bisecting %s cannot reproduce good result." % pass_name
-            )
-
-        if self.ir_diff:
-            self.PrintIRDiff(pass_index, pass_name, trans_index)
-
-        logger.GetLogger().LogOutput(
-            "Bisection result for bad item %s:\n"
-            "Bad pass: %s at number %d\n"
-            "Bad transformation number: %d"
-            % (self.found_items, pass_name, pass_index, trans_index)
-        )
-
-    def DoBinarySearchBadPass(self, pass_index=-1, pass_name=None):
-        """Perform single iteration of binary search at pass level
-
-        Args:
-          pass_index: Works for transformation level bisection, indicates the limit
-                      number of pass from pass level bisecting result.
-          pass_name: Works for transformation level bisection, indicates
-                     DebugCounter name of the bad pass from pass level bisecting
-                     result.
-
-        Returns:
-          index: Index of problematic pass/transformation.
-          pass_name: Works for pass level bisection, returns DebugCounter name for
-                     bad pass.
-        """
-        # If in resume mode don't reset search_cycles
-        if not self.resumed:
-            self.search_cycles = 0
-        else:
-            self.resumed = False
-
-        terminated = False
-        index = 0
-        while self.search_cycles < self.iterations and not terminated:
-            self.SaveState()
-            self.OutputIterationProgressBadPass()
-
-            self.search_cycles += 1
-            current = self.binary_search.GetNext()
-
-            if self.mode == "pass":
-                index, pass_name = self.BuildWithPassLimit(current)
-            else:
-                self.BuildWithTransformLimit(current, pass_name, pass_index)
-                index = current
-
-            # TODO: Newly generated object should not directly replace original
-            # one, need to put it somewhere and symbol link original one to it.
-            # Will update cmd_script to do it.
-
-            status = self.TestSetupScript()
-            assert status == 0, "Test setup should succeed."
-            status = self.TestScript()
-            terminated = self.binary_search.SetStatus(status)
-
-            if terminated:
-                self.l.LogOutput("Terminated!", print_to_console=self.verbose)
-        if not terminated:
-            self.l.LogOutput("Ran out of iterations searching...")
-        self.l.LogOutput(str(self), print_to_console=self.verbose)
-        return index, pass_name
-
-    def PopulateItemsUsingCommand(self, command):
-        """Update all_items and binary search logic from executable.
-
-        This method is mainly required for enumerating the initial list of items
-        from the get_initial_items script.
-
-        Args:
-          command: path to executable that will enumerate items.
-        """
-        ce = command_executer.GetCommandExecuter()
-        _, out, _ = ce.RunCommandWExceptionCleanup(
-            command, return_output=True, print_to_console=self.verbose
-        )
-        all_items = out.split()
-        self.PopulateItemsUsingList(all_items)
-
-    def PopulateItemsUsingList(self, all_items):
-        """Update all_items and binary searching logic from list.
-
-        Args:
-          all_items: new list of all_items
-        """
-        self.all_items = all_items
-        self.binary_search = binary_search_perforce.BinarySearcher(
-            logger_to_set=self.l
-        )
-        self.binary_search.SetSortedList(self.all_items)
-
-    def SaveState(self):
-        """Save state to STATE_FILE.
-
-        SaveState will create a new unique, hidden state file to hold data from
-        object. Then atomically overwrite the STATE_FILE symlink to point to the
-        new data.
-
-        Raises:
-          OSError if STATE_FILE already exists but is not a symlink.
-        """
-        ce, l = self.ce, self.l
-        self.ce, self.l, self.binary_search.logger = None, None, None
-        old_state = None
-
-        _, path = tempfile.mkstemp(prefix=HIDDEN_STATE_FILE, dir=".")
-        with open(path, "wb") as f:
-            pickle.dump(self, f)
-
-        if os.path.exists(STATE_FILE):
-            if os.path.islink(STATE_FILE):
-                old_state = os.readlink(STATE_FILE)
-            else:
-                raise OSError(
-                    (
-                        "%s already exists and is not a symlink!\n"
-                        "State file saved to %s" % (STATE_FILE, path)
-                    )
-                )
-
-        # Create new link and atomically overwrite old link
-        temp_link = "%s.link" % HIDDEN_STATE_FILE
-        os.symlink(path, temp_link)
-        os.rename(temp_link, STATE_FILE)
-
-        if old_state:
-            os.remove(old_state)
-
-        self.ce, self.l, self.binary_search.logger = ce, l, l
-
-    @classmethod
-    def LoadState(cls):
-        """Create BinarySearchState object from STATE_FILE."""
-        if not os.path.isfile(STATE_FILE):
-            return None
-        try:
-            with open(STATE_FILE, "rb") as f:
-                bss = pickle.load(f)
-                bss.l = logger.GetLogger()
-                bss.ce = command_executer.GetCommandExecuter()
-                bss.binary_search.logger = bss.l
-                bss.start_time = time.time()
-
-                # Set resumed to be True so we can enter DoBinarySearch without the
-                # method resetting our current search_cycles to 0.
-                bss.resumed = True
-
-                # Set currently_good_items and currently_bad_items to empty so that the
-                # first iteration after resuming will always be non-incremental. This
-                # is just in case the environment changes, the user makes manual
-                # changes, or a previous switch_script corrupted the environment.
-                bss.currently_good_items = set()
-                bss.currently_bad_items = set()
-
-                binary_search_perforce.verbose = bss.verbose
-                return bss
-        except Exception:
-            return None
-
-    def RemoveState(self):
-        """Remove STATE_FILE and its symlinked data from file system."""
-        if os.path.exists(STATE_FILE):
-            if os.path.islink(STATE_FILE):
-                real_file = os.readlink(STATE_FILE)
-                os.remove(real_file)
-                os.remove(STATE_FILE)
-
-    def GetNextItems(self):
-        """Get next items for binary search based on result of the last test run."""
-        border_item = self.binary_search.GetNext()
-        index = self.all_items.index(border_item)
-
-        next_bad_items = self.all_items[: index + 1]
-        next_good_items = self.all_items[index + 1 :] + list(self.known_good)
-
-        return [next_bad_items, next_good_items]
-
-    def ElapsedTimeString(self):
-        """Return h m s format of elapsed time since execution has started."""
-        diff = int(time.time() - self.start_time)
-        seconds = diff % 60
-        minutes = (diff // 60) % 60
-        hours = diff // (60 * 60)
-
-        seconds = str(seconds).rjust(2)
-        minutes = str(minutes).rjust(2)
-        hours = str(hours).rjust(2)
-
-        return "%sh %sm %ss" % (hours, minutes, seconds)
-
-    def _OutputProgress(self, progress_text):
-        """Output current progress of binary search to console and logs.
-
-        Args:
-          progress_text: The progress to display to the user.
-        """
-        progress = (
-            "\n***** PROGRESS (elapsed time: %s) *****\n"
-            "%s"
-            "************************************************"
-        )
-        progress = progress % (self.ElapsedTimeString(), progress_text)
-        self.l.LogOutput(progress)
-
-    def OutputIterationProgressBadItem(self):
-        out = (
-            "Search %d of estimated %d.\n"
-            "Prune %d of max %d.\n"
-            "Current bad items found:\n"
-            "%s\n"
-        )
-        out = out % (
-            self.search_cycles + 1,
-            math.ceil(math.log(len(self.all_items), 2)),
-            self.prune_cycles + 1,
-            self.prune_iterations,
-            ", ".join(self.found_items),
-        )
-        self._OutputProgress(out)
-
-    def OutputIterationProgressBadPass(self):
-        out = "Search %d of estimated %d.\n" "Current limit: %s\n"
-        out = out % (
-            self.search_cycles + 1,
-            math.ceil(math.log(self.binary_search.total, 2)),
-            self.binary_search.current,
-        )
-        self._OutputProgress(out)
-
-    def __str__(self):
-        ret = ""
-        ret += "all: %s\n" % str(self.all_items)
-        ret += "currently_good: %s\n" % str(self.currently_good_items)
-        ret += "currently_bad: %s\n" % str(self.currently_bad_items)
-        ret += str(self.binary_search)
-        return ret
-
-
-class MockBinarySearchState(BinarySearchState):
-    """Mock class for BinarySearchState."""
-
-    def __init__(self, **kwargs):
-        # Initialize all arguments to None
-        default_kwargs = {
-            "get_initial_items": 'echo "1"',
-            "switch_to_good": None,
-            "switch_to_bad": None,
-            "test_setup_script": None,
-            "test_script": None,
-            "incremental": True,
-            "prune": False,
-            "pass_bisect": None,
-            "ir_diff": False,
-            "iterations": 50,
-            "prune_iterations": 100,
-            "verify": True,
-            "file_args": False,
-            "verbose": False,
-        }
-        default_kwargs.update(kwargs)
-        super(MockBinarySearchState, self).__init__(**default_kwargs)
-
-
-def _CanonicalizeScript(script_name):
-    """Return canonical path to script.
-
-    Args:
-      script_name: Relative or absolute path to script
-
-    Returns:
-      Canonicalized script path
-    """
-    script_name = os.path.expanduser(script_name)
-    if not script_name.startswith("/"):
-        return os.path.join(".", script_name)
-
-
-def Run(
-    get_initial_items,
-    switch_to_good,
-    switch_to_bad,
-    test_script,
-    test_setup_script=None,
-    iterations=50,
-    prune=False,
-    pass_bisect=None,
-    ir_diff=False,
-    noincremental=False,
-    file_args=False,
-    verify=True,
-    prune_iterations=100,
-    verbose=False,
-    resume=False,
-):
-    """Run binary search tool.
-
-    Equivalent to running through terminal.
-
-    Args:
-      get_initial_items: Script to enumerate all items being binary searched
-      switch_to_good: Script that will take items as input and switch them to good
-        set
-      switch_to_bad: Script that will take items as input and switch them to bad
-        set
-      test_script: Script that will determine if the current combination of good
-        and bad items make a "good" or "bad" result.
-      test_setup_script: Script to do necessary setup (building, compilation,
-        etc.) for test_script.
-      iterations: How many binary search iterations to run before exiting.
-      prune: If False the binary search tool will stop when the first bad item is
-        found. Otherwise then binary search tool will continue searching until all
-        bad items are found (or prune_iterations is reached).
-      pass_bisect: Script that takes single bad item from POPULATE_BAD and returns
-        the compiler command used to generate the bad item. This will turn on
-        pass/ transformation level bisection for the bad item. Requires that
-        'prune' be set to False, and needs support of `-opt-bisect-limit`(pass)
-        and `-print-debug-counter`(transformation) from LLVM.
-      ir_diff: Whether to print IR differences before and after bad
-        pass/transformation to verbose output. Defaults to False, only works when
-        pass_bisect is enabled.
-      noincremental: Whether to send "diffs" of good/bad items to switch scripts.
-      file_args: If True then arguments to switch scripts will be a file name
-        containing a newline separated list of the items to switch.
-      verify: If True, run tests to ensure initial good/bad sets actually produce
-        a good/bad result.
-      prune_iterations: Max number of bad items to search for.
-      verbose: If True will print extra debug information to user.
-      resume: If True will resume using STATE_FILE.
-
-    Returns:
-      0 for success, error otherwise
-    """
-    # Notice that all the argument checks are in the Run() function rather than
-    # in the Main() function. It is not common to do so but some wrappers are
-    # going to call Run() directly and bypass checks in Main() function.
-    if resume:
-        logger.GetLogger().LogOutput("Resuming from %s" % STATE_FILE)
-        bss = BinarySearchState.LoadState()
-        if not bss:
-            logger.GetLogger().LogOutput(
-                "%s is not a valid binary_search_tool state file, cannot resume!"
-                % STATE_FILE
-            )
-            return 1
-        logger.GetLogger().LogOutput(
-            "Note: resuming from previous state, "
-            "ignoring given options and loading saved "
-            "options instead."
-        )
-    else:
-        if not (
-            get_initial_items
-            and switch_to_good
-            and switch_to_bad
-            and test_script
-        ):
-            logger.GetLogger().LogOutput(
-                "The following options are required: " "[-i, -g, -b, -t] | [-r]"
-            )
-            return 1
-        if pass_bisect and prune:
-            logger.GetLogger().LogOutput(
-                '"--pass_bisect" only works when '
-                '"--prune" is set to be False.'
-            )
-            return 1
-        if not pass_bisect and ir_diff:
-            logger.GetLogger().LogOutput(
-                '"--ir_diff" only works when ' '"--pass_bisect" is enabled.'
-            )
-
-        switch_to_good = _CanonicalizeScript(switch_to_good)
-        switch_to_bad = _CanonicalizeScript(switch_to_bad)
-        if test_setup_script:
-            test_setup_script = _CanonicalizeScript(test_setup_script)
-        if pass_bisect:
-            pass_bisect = _CanonicalizeScript(pass_bisect)
-        test_script = _CanonicalizeScript(test_script)
-        get_initial_items = _CanonicalizeScript(get_initial_items)
-        incremental = not noincremental
-
-        binary_search_perforce.verbose = verbose
-
-        bss = BinarySearchState(
-            get_initial_items,
-            switch_to_good,
-            switch_to_bad,
-            test_setup_script,
-            test_script,
-            incremental,
-            prune,
-            pass_bisect,
-            ir_diff,
-            iterations,
-            prune_iterations,
-            verify,
-            file_args,
-            verbose,
-        )
-        bss.DoVerify()
-
-    bss.DoSearchBadItems()
-    if pass_bisect:
-        bss.DoSearchBadPass()
-    bss.RemoveState()
-    logger.GetLogger().LogOutput(
-        "Total execution time: %s" % bss.ElapsedTimeString()
-    )
-
-    return 0
-
-
-def Main(argv):
-    """The main function."""
-    # Common initializations
-
-    parser = argparse.ArgumentParser()
-    common.BuildArgParser(parser)
-    logger.GetLogger().LogOutput(" ".join(argv))
-    options = parser.parse_args(argv)
-
-    # Get dictionary of all options
-    args = vars(options)
-    return Run(**args)
-
-
-if __name__ == "__main__":
-    sys.exit(Main(sys.argv[1:]))
diff --git a/binary_search_tool/bisect_driver.py b/binary_search_tool/bisect_driver.py
deleted file mode 100644
index 8feb1a37..00000000
--- a/binary_search_tool/bisect_driver.py
+++ /dev/null
@@ -1,431 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-#
-# This script is used to help the compiler wrapper in the ChromeOS and
-# Android build systems bisect for bad object files.
-
-"""Utilities for bisection of ChromeOS and Android object files.
-
-This module contains a set of utilities to allow bisection between
-two sets (good and bad) of object files. Mostly used to find compiler
-bugs.
-
-Reference page:
-https://sites.google.com/a/google.com/chromeos-toolchain-team-home2/home/team-tools-and-scripts/bisecting-chromeos-compiler-problems/bisection-compiler-wrapper
-
-Design doc:
-https://docs.google.com/document/d/1yDgaUIa2O5w6dc3sSTe1ry-1ehKajTGJGQCbyn0fcEM
-"""
-
-
-import contextlib
-import fcntl
-import os
-import shutil
-import stat
-import subprocess
-import sys
-
-
-VALID_MODES = ("POPULATE_GOOD", "POPULATE_BAD", "TRIAGE")
-GOOD_CACHE = "good"
-BAD_CACHE = "bad"
-LIST_FILE = os.path.join(GOOD_CACHE, "_LIST")
-
-CONTINUE_ON_MISSING = os.environ.get("BISECT_CONTINUE_ON_MISSING", None) == "1"
-CONTINUE_ON_REDUNDANCY = (
-    os.environ.get("BISECT_CONTINUE_ON_REDUNDANCY", None) == "1"
-)
-WRAPPER_SAFE_MODE = os.environ.get("BISECT_WRAPPER_SAFE_MODE", None) == "1"
-
-
-class Error(Exception):
-    """The general compiler wrapper error class."""
-
-
-@contextlib.contextmanager
-def lock_file(path, mode):
-    """Lock file and block if other process has lock on file.
-
-    Acquire exclusive lock for file. Only blocks other processes if they attempt
-    to also acquire lock through this method. If only reading (modes 'r' and 'rb')
-    then the lock is shared (i.e. many reads can happen concurrently, but only one
-    process may write at a time).
-
-    This function is a contextmanager, meaning it's meant to be used with the
-    "with" statement in Python. This is so cleanup and setup happens automatically
-    and cleanly. Execution of the outer "with" statement happens at the "yield"
-    statement. Execution resumes after the yield when the outer "with" statement
-    ends.
-
-    Args:
-      path: path to file being locked
-      mode: mode to open file with ('w', 'r', etc.)
-    """
-    with open(path, mode) as f:
-        # Apply FD_CLOEXEC argument to fd. This ensures that the file descriptor
-        # won't be leaked to any child processes.
-        current_args = fcntl.fcntl(f.fileno(), fcntl.F_GETFD)
-        fcntl.fcntl(f.fileno(), fcntl.F_SETFD, current_args | fcntl.FD_CLOEXEC)
-
-        # Reads can share the lock as no race conditions exist. If write is needed,
-        # give writing process exclusive access to the file.
-        if f.mode == "r" or f.mode == "rb":
-            lock_type = fcntl.LOCK_SH
-        else:
-            lock_type = fcntl.LOCK_EX
-
-        try:
-            fcntl.lockf(f, lock_type)
-            yield f
-            f.flush()
-        finally:
-            fcntl.lockf(f, fcntl.LOCK_UN)
-
-
-def log_to_file(path, execargs, link_from=None, link_to=None):
-    """Common logging function.
-
-    Log current working directory, current execargs, and a from-to relationship
-    between files.
-    """
-    with lock_file(path, "a") as log:
-        log.write("cd: %s; %s\n" % (os.getcwd(), " ".join(execargs)))
-        if link_from and link_to:
-            log.write("%s -> %s\n" % (link_from, link_to))
-
-
-def exec_and_return(execargs):
-    """Execute process and return.
-
-    Execute according to execargs and return immediately. Don't inspect
-    stderr or stdout.
-    """
-    return subprocess.call(execargs)
-
-
-def which_cache(obj_file):
-    """Determine which cache an object belongs to.
-
-    The binary search tool creates two files for each search iteration listing
-    the full set of bad objects and full set of good objects. We use this to
-    determine where an object file should be linked from (good or bad).
-    """
-    bad_set_file = os.environ.get("BISECT_BAD_SET")
-    if in_object_list(obj_file, bad_set_file):
-        return BAD_CACHE
-    else:
-        return GOOD_CACHE
-
-
-def makedirs(path):
-    """Try to create directories in path."""
-    try:
-        os.makedirs(path)
-    except os.error:
-        if not os.path.isdir(path):
-            raise
-
-
-def get_obj_path(execargs):
-    """Get the object path for the object file in the list of arguments.
-
-    Returns:
-      Absolute object path from execution args (-o argument). If no object being
-      outputted, then return empty string. -o argument is checked only if -c is
-      also present.
-    """
-    try:
-        i = execargs.index("-o")
-        _ = execargs.index("-c")
-    except ValueError:
-        return ""
-
-    obj_path = execargs[i + 1]
-    # Ignore args that do not create a file.
-    if obj_path in (
-        "-",
-        "/dev/null",
-    ):
-        return ""
-    # Ignore files ending in .tmp.
-    if obj_path.endswith((".tmp",)):
-        return ""
-    # Ignore configuration files generated by Automake/Autoconf/CMake etc.
-    if (
-        obj_path.endswith("conftest.o")
-        or obj_path.endswith("CMakeFiles/test.o")
-        or obj_path.find("CMakeTmp") != -1
-        or os.path.abspath(obj_path).find("CMakeTmp") != -1
-    ):
-        return ""
-
-    return os.path.abspath(obj_path)
-
-
-def get_dep_path(execargs):
-    """Get the dep file path for the dep file in the list of arguments.
-
-    Returns:
-      Absolute path of dependency file path from execution args (-o argument). If
-      no dependency being outputted then return empty string.
-    """
-    if "-MD" not in execargs and "-MMD" not in execargs:
-        return ""
-
-    # If -MF is given this is the path of the dependency file. Otherwise the
-    # dependency file is the value of -o but with a .d extension
-    if "-MF" in execargs:
-        i = execargs.index("-MF")
-        dep_path = execargs[i + 1]
-        return os.path.abspath(dep_path)
-
-    full_obj_path = get_obj_path(execargs)
-    if not full_obj_path:
-        return ""
-
-    return full_obj_path[:-2] + ".d"
-
-
-def get_dwo_path(execargs):
-    """Get the dwo file path for the dwo file in the list of arguments.
-
-    Returns:
-      Absolute dwo file path from execution args (-gsplit-dwarf argument) If no
-      dwo file being outputted then return empty string.
-    """
-    if "-gsplit-dwarf" not in execargs:
-        return ""
-
-    full_obj_path = get_obj_path(execargs)
-    if not full_obj_path:
-        return ""
-
-    return full_obj_path[:-2] + ".dwo"
-
-
-def in_object_list(obj_name, list_filename):
-    """Check if object file name exist in file with object list."""
-    if not obj_name:
-        return False
-
-    with lock_file(list_filename, "r") as list_file:
-        for line in list_file:
-            if line.strip() == obj_name:
-                return True
-
-        return False
-
-
-def get_side_effects(execargs):
-    """Determine side effects generated by compiler
-
-    Returns:
-      List of paths of objects that the compiler generates as side effects.
-    """
-    side_effects = []
-
-    # Cache dependency files
-    full_dep_path = get_dep_path(execargs)
-    if full_dep_path:
-        side_effects.append(full_dep_path)
-
-    # Cache dwo files
-    full_dwo_path = get_dwo_path(execargs)
-    if full_dwo_path:
-        side_effects.append(full_dwo_path)
-
-    return side_effects
-
-
-def cache_file(execargs, bisect_dir, cache, abs_file_path):
-    """Cache compiler output file (.o/.d/.dwo).
-
-    Args:
-      execargs: compiler execution arguments.
-      bisect_dir: The directory where bisection caches live.
-      cache: Which cache the file will be cached to (GOOD/BAD).
-      abs_file_path: Absolute path to file being cached.
-
-    Returns:
-      True if caching was successful, False otherwise.
-    """
-    # os.path.join fails with absolute paths, use + instead
-    bisect_path = os.path.join(bisect_dir, cache) + abs_file_path
-    bisect_path_dir = os.path.dirname(bisect_path)
-    makedirs(bisect_path_dir)
-    pop_log = os.path.join(bisect_dir, cache, "_POPULATE_LOG")
-    log_to_file(pop_log, execargs, abs_file_path, bisect_path)
-
-    try:
-        if os.path.exists(abs_file_path):
-            if os.path.exists(bisect_path):
-                # File exists
-                population_dir = os.path.join(bisect_dir, cache)
-                with lock_file(
-                    os.path.join(population_dir, "_DUPS"), "a"
-                ) as dup_object_list:
-                    dup_object_list.write("%s\n" % abs_file_path)
-                if CONTINUE_ON_REDUNDANCY:
-                    return True
-                raise Exception(
-                    "Trying to cache file %s multiple times. To avoid the error, set "
-                    "BISECT_CONTINUE_ON_REDUNDANCY to 1. For reference, the list of "
-                    "such files will be written to %s"
-                    % (abs_file_path, os.path.join(population_dir, "_DUPS"))
-                )
-
-            shutil.copy2(abs_file_path, bisect_path)
-            # Set cache object to be read-only so later compilations can't
-            # accidentally overwrite it.
-            os.chmod(bisect_path, 0o444)
-            return True
-        else:
-            # File not found (happens when compilation fails but error code is still
-            # 0)
-            return False
-    except Exception:
-        print("Could not cache file %s" % abs_file_path, file=sys.stderr)
-        raise
-
-
-def restore_file(bisect_dir, cache, abs_file_path):
-    """Restore file from cache (.o/.d/.dwo).
-
-    Args:
-      bisect_dir: The directory where bisection caches live.
-      cache: Which cache the file will be restored from (GOOD/BAD).
-      abs_file_path: Absolute path to file being restored.
-    """
-    # os.path.join fails with absolute paths, use + instead
-    cached_path = os.path.join(bisect_dir, cache) + abs_file_path
-    if os.path.exists(cached_path):
-        if os.path.exists(abs_file_path):
-            os.remove(abs_file_path)
-        shutil.copy2(cached_path, abs_file_path)
-        # Add write permission to the restored object files as some packages
-        # (such as kernels) may need write permission to delete files.
-        os.chmod(abs_file_path, os.stat(abs_file_path).st_mode | stat.S_IWUSR)
-    else:
-        raise Error(
-            (
-                "%s is missing from %s cache! Unsure how to proceed. Make "
-                "will now crash." % (cache, cached_path)
-            )
-        )
-
-
-def bisect_populate(execargs, bisect_dir, population_name):
-    """Add necessary information to the bisect cache for the given execution.
-
-    Extract the necessary information for bisection from the compiler
-    execution arguments and put it into the bisection cache. This
-    includes copying the created object file, adding the object
-    file path to the cache list and keeping a log of the execution.
-
-    Args:
-      execargs: compiler execution arguments.
-      bisect_dir: bisection directory.
-      population_name: name of the cache being populated (good/bad).
-    """
-    retval = exec_and_return(execargs)
-    if retval:
-        return retval
-
-    full_obj_path = get_obj_path(execargs)
-    # This is not a normal compiler call because it doesn't have a -o argument,
-    # or the -o argument has an unusable output file.
-    # It's likely that this compiler call was actually made to invoke the linker,
-    # or as part of a configuratoin test. In this case we want to simply call the
-    # compiler and return.
-    if not full_obj_path:
-        return retval
-
-    # Return if not able to cache the object file
-    if not cache_file(execargs, bisect_dir, population_name, full_obj_path):
-        return retval
-
-    population_dir = os.path.join(bisect_dir, population_name)
-    with lock_file(os.path.join(population_dir, "_LIST"), "a") as object_list:
-        object_list.write("%s\n" % full_obj_path)
-
-    for side_effect in get_side_effects(execargs):
-        _ = cache_file(execargs, bisect_dir, population_name, side_effect)
-
-    return retval
-
-
-def bisect_triage(execargs, bisect_dir):
-    """Use object object file from appropriate cache (good/bad).
-
-    Given a populated bisection directory, use the object file saved
-    into one of the caches (good/bad) according to what is specified
-    in the good/bad sets. The good/bad sets are generated by the
-    high level binary search tool. Additionally restore any possible
-    side effects of compiler.
-
-    Args:
-      execargs: compiler execution arguments.
-      bisect_dir: populated bisection directory.
-    """
-    full_obj_path = get_obj_path(execargs)
-    obj_list = os.path.join(bisect_dir, LIST_FILE)
-
-    # If the output isn't an object file just call compiler
-    if not full_obj_path:
-        return exec_and_return(execargs)
-
-    # If this isn't a bisected object just call compiler
-    # This shouldn't happen!
-    if not in_object_list(full_obj_path, obj_list):
-        if CONTINUE_ON_MISSING:
-            log_file = os.path.join(bisect_dir, "_MISSING_CACHED_OBJ_LOG")
-            log_to_file(log_file, execargs, "? compiler", full_obj_path)
-            return exec_and_return(execargs)
-        else:
-            raise Error(
-                (
-                    "%s is missing from cache! To ignore export "
-                    "BISECT_CONTINUE_ON_MISSING=1. See documentation for more "
-                    "details on this option." % full_obj_path
-                )
-            )
-
-    cache = which_cache(full_obj_path)
-
-    # If using safe WRAPPER_SAFE_MODE option call compiler and overwrite the
-    # result from the good/bad cache. This option is safe and covers all compiler
-    # side effects, but is very slow!
-    if WRAPPER_SAFE_MODE:
-        retval = exec_and_return(execargs)
-        if retval:
-            return retval
-        os.remove(full_obj_path)
-        restore_file(bisect_dir, cache, full_obj_path)
-        return retval
-
-    # Generate compiler side effects. Trick Make into thinking compiler was
-    # actually executed.
-    for side_effect in get_side_effects(execargs):
-        restore_file(bisect_dir, cache, side_effect)
-
-    # If generated object file happened to be pruned/cleaned by Make then link it
-    # over from cache again.
-    if not os.path.exists(full_obj_path):
-        restore_file(bisect_dir, cache, full_obj_path)
-
-    return 0
-
-
-def bisect_driver(bisect_stage, bisect_dir, execargs):
-    """Call appropriate bisection stage according to value in bisect_stage."""
-    if bisect_stage == "POPULATE_GOOD":
-        return bisect_populate(execargs, bisect_dir, GOOD_CACHE)
-    elif bisect_stage == "POPULATE_BAD":
-        return bisect_populate(execargs, bisect_dir, BAD_CACHE)
-    elif bisect_stage == "TRIAGE":
-        return bisect_triage(execargs, bisect_dir)
-    else:
-        raise ValueError("wrong value for BISECT_STAGE: %s" % bisect_stage)
diff --git a/binary_search_tool/common.py b/binary_search_tool/common.py
deleted file mode 100644
index f6165847..00000000
--- a/binary_search_tool/common.py
+++ /dev/null
@@ -1,323 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Common config and logic for binary search tool
-
-This module serves two main purposes:
-  1. Programatically include the utils module in PYTHONPATH
-  2. Create the argument parsing shared between binary_search_state.py and
-     run_bisect.py
-
-The argument parsing is handled by populating _ArgsDict with all arguments.
-_ArgsDict is required so that binary_search_state.py and run_bisect.py can
-share the argument parsing, but treat them slightly differently. For example,
-run_bisect.py requires that all argument defaults are suppressed so that
-overriding can occur properly (i.e. only options that are explicitly entered
-by the user end up in the resultant options dictionary).
-
-ArgumentDict inherits OrderedDict in order to preserve the order the args are
-created so the help text is made properly.
-"""
-
-
-import collections
-import os
-import sys
-
-
-# Programatically adding utils python path to PYTHONPATH
-if os.path.isabs(sys.argv[0]):
-    utils_pythonpath = os.path.abspath(
-        "{0}/..".format(os.path.dirname(sys.argv[0]))
-    )
-else:
-    wdir = os.getcwd()
-    utils_pythonpath = os.path.abspath(
-        "{0}/{1}/..".format(wdir, os.path.dirname(sys.argv[0]))
-    )
-sys.path.append(utils_pythonpath)
-
-
-class ArgumentDict(collections.OrderedDict):
-    """Wrapper around OrderedDict, represents CLI arguments for program.
-
-    AddArgument enforces the following layout:
-    {
-        ['-n', '--iterations'] : {
-            'dest': 'iterations',
-            'type': int,
-            'help': 'Number of iterations to try in the search.',
-            'default': 50
-        }
-        [arg_name1, arg_name2, ...] : {
-            arg_option1 : arg_option_val1,
-            ...
-        },
-        ...
-    }
-    """
-
-    _POSSIBLE_OPTIONS = [
-        "action",
-        "nargs",
-        "const",
-        "default",
-        "type",
-        "choices",
-        "required",
-        "help",
-        "metavar",
-        "dest",
-    ]
-
-    def AddArgument(self, *args, **kwargs):
-        """Add argument to ArgsDict, has same signature as argparse.add_argument
-
-        Emulates the the argparse.add_argument method so the internal OrderedDict
-        can be safely and easily populated. Each call to this method will have a 1-1
-        corresponding call to argparse.add_argument once BuildArgParser is called.
-
-        Args:
-          *args: The names for the argument (-V, --verbose, etc.)
-          **kwargs: The options for the argument, corresponds to the args of
-                    argparse.add_argument
-
-        Returns:
-          None
-
-        Raises:
-          TypeError: if args is empty or if option in kwargs is not a valid
-                     option for argparse.add_argument.
-        """
-        if not args:
-            raise TypeError("Argument needs at least one name")
-
-        for key in kwargs:
-            if key not in self._POSSIBLE_OPTIONS:
-                raise TypeError(
-                    'Invalid option "%s" for argument %s' % (key, args[0])
-                )
-
-        self[args] = kwargs
-
-
-_ArgsDict = ArgumentDict()
-
-
-def GetArgsDict():
-    """_ArgsDict singleton method"""
-    if not _ArgsDict:
-        _BuildArgsDict(_ArgsDict)
-    return _ArgsDict
-
-
-def BuildArgParser(parser, override=False):
-    """Add all arguments from singleton ArgsDict to parser.
-
-    Will take argparse parser and add all arguments in ArgsDict. Will ignore
-    the default and required options if override is set to True.
-
-    Args:
-      parser: type argparse.ArgumentParser, will call add_argument for every item
-              in _ArgsDict
-      override: True if being called from run_bisect.py. Used to say that default
-                and required options are to be ignored
-
-    Returns:
-      None
-    """
-    ArgsDict = GetArgsDict()
-
-    # Have no defaults when overriding
-    for arg_names, arg_options in ArgsDict.items():
-        if override:
-            arg_options = arg_options.copy()
-            arg_options.pop("default", None)
-            arg_options.pop("required", None)
-
-        parser.add_argument(*arg_names, **arg_options)
-
-
-def StrToBool(str_in):
-    if str_in.lower() in ["true", "t", "1"]:
-        return True
-    if str_in.lower() in ["false", "f", "0"]:
-        return False
-
-    raise AttributeError("%s is not a valid boolean string" % str_in)
-
-
-def _BuildArgsDict(args):
-    """Populate ArgumentDict with all arguments"""
-    args.AddArgument(
-        "-n",
-        "--iterations",
-        dest="iterations",
-        type=int,
-        help="Number of iterations to try in the search.",
-        default=50,
-    )
-    args.AddArgument(
-        "-i",
-        "--get_initial_items",
-        dest="get_initial_items",
-        help="Script to run to get the initial objects. "
-        "If your script requires user input "
-        "the --verbose option must be used",
-    )
-    args.AddArgument(
-        "-g",
-        "--switch_to_good",
-        dest="switch_to_good",
-        help="Script to run to switch to good. "
-        "If your switch script requires user input "
-        "the --verbose option must be used",
-    )
-    args.AddArgument(
-        "-b",
-        "--switch_to_bad",
-        dest="switch_to_bad",
-        help="Script to run to switch to bad. "
-        "If your switch script requires user input "
-        "the --verbose option must be used",
-    )
-    args.AddArgument(
-        "-I",
-        "--test_setup_script",
-        dest="test_setup_script",
-        help="Optional script to perform building, flashing, "
-        "and other setup before the test script runs.",
-    )
-    args.AddArgument(
-        "-t",
-        "--test_script",
-        dest="test_script",
-        help="Script to run to test the " "output after packages are built.",
-    )
-    # No input (evals to False),
-    # --prune (evals to True),
-    # --prune=False,
-    # --prune=True
-    args.AddArgument(
-        "-p",
-        "--prune",
-        dest="prune",
-        nargs="?",
-        const=True,
-        default=False,
-        type=StrToBool,
-        metavar="bool",
-        help="If True, continue until all bad items are found. "
-        "Defaults to False.",
-    )
-    args.AddArgument(
-        "-P",
-        "--pass_bisect",
-        dest="pass_bisect",
-        default=None,
-        help="Script to generate another script for pass level bisect, "
-        "which contains command line options to build bad item. "
-        "This will also turn on pass/transformation level bisection. "
-        "Needs support of `-opt-bisect-limit`(pass) and "
-        "`-print-debug-counter`(transformation) from LLVM. "
-        "For now it only supports one single bad item, so to use it, "
-        "prune must be set to False.",
-    )
-    # No input (evals to False),
-    # --ir_diff (evals to True),
-    # --ir_diff=False,
-    # --ir_diff=True
-    args.AddArgument(
-        "-d",
-        "--ir_diff",
-        dest="ir_diff",
-        nargs="?",
-        const=True,
-        default=False,
-        type=StrToBool,
-        metavar="bool",
-        help="Whether to print IR differences before and after bad "
-        "pass/transformation to verbose output. Defaults to False, "
-        "only works when pass_bisect is enabled.",
-    )
-    # No input (evals to False),
-    # --noincremental (evals to True),
-    # --noincremental=False,
-    # --noincremental=True
-    args.AddArgument(
-        "-c",
-        "--noincremental",
-        dest="noincremental",
-        nargs="?",
-        const=True,
-        default=False,
-        type=StrToBool,
-        metavar="bool",
-        help="If True, don't propagate good/bad changes "
-        "incrementally. Defaults to False.",
-    )
-    # No input (evals to False),
-    # --file_args (evals to True),
-    # --file_args=False,
-    # --file_args=True
-    args.AddArgument(
-        "-f",
-        "--file_args",
-        dest="file_args",
-        nargs="?",
-        const=True,
-        default=False,
-        type=StrToBool,
-        metavar="bool",
-        help="Whether to use a file to pass arguments to scripts. "
-        "Defaults to False.",
-    )
-    # No input (evals to True),
-    # --verify (evals to True),
-    # --verify=False,
-    # --verify=True
-    args.AddArgument(
-        "--verify",
-        dest="verify",
-        nargs="?",
-        const=True,
-        default=True,
-        type=StrToBool,
-        metavar="bool",
-        help="Whether to run verify iterations before searching. "
-        "Defaults to True.",
-    )
-    args.AddArgument(
-        "-N",
-        "--prune_iterations",
-        dest="prune_iterations",
-        type=int,
-        help="Number of prune iterations to try in the search.",
-        default=100,
-    )
-    # No input (evals to False),
-    # --verbose (evals to True),
-    # --verbose=False,
-    # --verbose=True
-    args.AddArgument(
-        "-V",
-        "--verbose",
-        dest="verbose",
-        nargs="?",
-        const=True,
-        default=False,
-        type=StrToBool,
-        metavar="bool",
-        help="If True, print full output to console.",
-    )
-    args.AddArgument(
-        "-r",
-        "--resume",
-        dest="resume",
-        action="store_true",
-        help="Resume bisection tool execution from state file."
-        "Useful if the last bisection was terminated "
-        "before it could properly finish.",
-    )
diff --git a/binary_search_tool/common/boot_test.sh b/binary_search_tool/common/boot_test.sh
deleted file mode 100755
index 384712b7..00000000
--- a/binary_search_tool/common/boot_test.sh
+++ /dev/null
@@ -1,22 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2016 Google LLC
-#
-# This script pings the chromebook to determine if it has successfully booted.
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on ChromeOS package and object files.
-# It waits for the test setup script to build and install the image, then pings
-# the machine. It should return '0' if the test succeeds (the image booted); '1'
-# if the test fails (the image did not boot); and '125' if it could not
-# determine (does not apply in this case).
-#
-
-source common/common.sh
-
-# Send 3 pings and wait 3 seconds for any responsed (then timeout).
-ping -c 3 -W 3 ${BISECT_REMOTE}
-retval=$?
-
-
-exit $retval
diff --git a/binary_search_tool/common/hash_test.sh b/binary_search_tool/common/hash_test.sh
deleted file mode 100755
index 338ee026..00000000
--- a/binary_search_tool/common/hash_test.sh
+++ /dev/null
@@ -1,57 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2016 Google LLC
-#
-# This script is intended to be used by binary_search_state.py. It is to
-# be used for testing/development of the binary search triage tool
-# itself.  It waits for the test setup script to build and install the
-# image, then checks the hashes in the provided file.
-# If the real hashes match the checksum hashes, then the image is 'good',
-# otherwise it is 'bad'.  This allows the rest of the bisecting tool
-# to run without requiring help from the user (as it would if we were
-# dealing with a real 'bad' image).
-#
-
-#
-# Initialize the value below before using this script!!!
-#
-# Make an md5sum of all the files you want to check. For example if you want
-# file1, file2, and file3 to be found as bad items:
-#
-#   md5sum file1 file2 file3 > checksum.out
-#
-# (Make sure you are hashing the files from your good build and that the hashes
-# from good to bad build differ)
-#
-# Then set HASHES_FILE to be the path to 'checksum.out'
-# In this example, file1, file2, file3 will be found as the bad files
-# because their hashes won't match when from the bad build tree. This is
-# assuming that the hashes between good/bad builds change. It is suggested to
-# build good and bad builds at different optimization levels to help ensure
-# each item has a different hash.
-#
-# WARNING:
-# Make sure paths to all files are absolute paths or relative to
-# binary_search_state.py
-#
-# cros_pkg bisector example:
-#   1. Build good packages with -O1, bad packages with -O2
-#   2. cros_pkg/switch_to_good.sh pkg1 pkg2 pkg3
-#   3. md5sum pkg1 pkg2 pkg3 > checksum.out.cros_pkg
-#   4. Set HASHES_FILE to be checksum.out.cros_pkg
-#   5. Run the bisector with this test script
-#
-#
-HASHES_FILE=
-
-if [[ -z "${HASHES_FILE}" || ! -f "${HASHES_FILE}" ]];
-then
-    echo "ERROR: HASHES_FILE must be intialized in common/hash_test.sh"
-    exit 3
-fi
-
-md5sum -c --status ${HASHES_FILE}
-md5_result=$?
-
-
-exit $md5_result
diff --git a/binary_search_tool/common/interactive_test.sh b/binary_search_tool/common/interactive_test.sh
deleted file mode 100755
index 05d47b7f..00000000
--- a/binary_search_tool/common/interactive_test.sh
+++ /dev/null
@@ -1,37 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2016 Google LLC
-#
-# This script pings the chromebook to determine if it successfully booted.
-# It then asks the user if the image is good or not, allowing the user to
-# conduct whatever tests the user wishes, and waiting for a response.
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on ChromeOS package and object files. It
-# waits for the test setup script to build and install the image, then asks the
-# user if the image is good or not. It should return '0' if the test succeeds
-# (the image is 'good'); '1' if the test fails (the image is 'bad'); and '125'
-# if it could not determine (does not apply in this case).
-#
-
-source common/common.sh
-
-ping -c 3 -W 3 ${BISECT_REMOTE}
-retval=$?
-
-if [[ ${retval} -eq 0 ]]; then
-    echo "ChromeOS image has been built and installed on ${BISECT_REMOTE}."
-else
-    exit 1
-fi
-
-while true; do
-    read -p "Is this a good ChromeOS image?" yn
-    case $yn in
-        [Yy]* ) exit 0;;
-        [Nn]* ) exit 1;;
-        * ) echo "Please answer yes or no.";;
-    esac
-done
-
-exit 125
diff --git a/binary_search_tool/common/interactive_test_noping.sh b/binary_search_tool/common/interactive_test_noping.sh
deleted file mode 100755
index d4e77d7c..00000000
--- a/binary_search_tool/common/interactive_test_noping.sh
+++ /dev/null
@@ -1,27 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2016 Google LLC
-#
-# This script asks the user if the image is good or not, allowing the user to
-# conduct whatever tests the user wishes, and waiting for a response.
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on ChromeOS package and object files. It
-# waits for the test setup script to build and install the image, then asks the
-# user if the image is good or not. It should return '0' if the test succeeds
-# (the image is 'good'); '1' if the test fails (the image is 'bad'); and '125'
-# if it could not determine (does not apply in this case).
-#
-
-source common/common.sh
-
-while true; do
-    read -p "Is this a good ChromeOS image?" yn
-    case $yn in
-        [Yy]* ) exit 0;;
-        [Nn]* ) exit 1;;
-        * ) echo "Please answer yes or no.";;
-    esac
-done
-
-exit 125
diff --git a/binary_search_tool/common/test_setup.sh b/binary_search_tool/common/test_setup.sh
deleted file mode 100755
index 06452346..00000000
--- a/binary_search_tool/common/test_setup.sh
+++ /dev/null
@@ -1,222 +0,0 @@
-#!/bin/bash
-#
-# Copyright 2021 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-#
-# This is a generic ChromeOS package/image test setup script. It is meant to
-# be used for either the object file or package bisection tools. This script
-# does one of the following depending on what ${BISECT_MODE} is set to:
-#
-# 1) ${BISECT_MODE} is PACKAGE_MODE:
-#   build_image is called and generates a new ChromeOS image using whatever
-#   packages are currently in the build tree. This image is then pushed to the
-#   remote machine using flash over ethernet (or usb flash if ethernet flash
-#   fails).
-#
-# 2) ${BISECT_MODE} is OBJECT_MODE:
-#   emerge is called for ${BISECT_PACKAGE} and generates a build for said
-#   package. This package is then deployed to the remote machine and the machine
-#   is rebooted. If deploying fails then a new ChromeOS image is built from
-#   scratch and pushed to the machine like in PACKAGE_MODE.
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on ChromeOS objects and packages. It should
-# return '0' if the setup succeeds; and '1' if the setup fails (the image
-# could not build or be flashed).
-#
-
-export PYTHONUNBUFFERED=1
-
-source common/common.sh
-
-usb_flash()
-{
-  echo
-  echo "Insert a usb stick into the current machine"
-  echo "Note: The cros flash will take time and doesn't give much output."
-  echo "      Be patient. If your usb access light is flashing it's working."
-  sleep 1
-  read -p "Press enter to continue" notused
-
-  cros flash --board=${BISECT_BOARD} --clobber-stateful usb:// ~/trunk/src/build/images/${BISECT_BOARD}/latest/chromiumos_test_image.bin
-
-  echo
-  echo "Flash to usb complete!"
-  echo "Plug the usb into your chromebook and install the image."
-  echo "Refer to the ChromiumOS Developer's Handbook for more details."
-  echo "http://www.chromium.org/chromium-os/developer-guide#TOC-Boot-from-your-USB-disk"
-  while true; do
-    sleep 1
-    read -p "Was the installation of the image successful? " choice
-    case $choice in
-        [Yy]*) return 0;;
-        [Nn]*) return 1;;
-        *) echo "Please answer y or n.";;
-    esac
-  done
-}
-
-ethernet_flash()
-{
-  echo
-  echo "Please ensure your Chromebook is up and running Chrome so"
-  echo "cros flash may run."
-  echo "If your Chromebook has a broken image you can try:"
-  echo "1. Rebooting your Chromebook 6 times to install the last working image"
-  echo "2. Alternatively, running the following command on the Chromebook"
-  echo "   will also rollback to the last working image:"
-  echo "   'update_engine_client --rollback --nopowerwash --reboot'"
-  echo "3. Flashing a new image through USB"
-  echo
-  sleep 1
-  read -p $'Press enter to continue and retry the ethernet flash' notused
-  cros flash --board=${BISECT_BOARD} --clobber-stateful ${BISECT_REMOTE} ~/trunk/src/build/images/${BISECT_BOARD}/latest/chromiumos_test_image.bin
-}
-
-reboot()
-{
-  ret_val=0
-  pushd ~/trunk/src/scripts > /dev/null
-  set -- --remote=${BISECT_REMOTE}
-  . ./common.sh || ret_val=1
-  . ./remote_access.sh || ret_val=1
-  TMP=$(mktemp -d)
-  FLAGS "$@" || ret_val=1
-  remote_access_init || ret_val=1
-  remote_reboot || ret_val=1
-  popd > /dev/null
-
-  return $ret_val
-}
-
-echo
-echo "INSTALLATION BEGIN"
-echo
-
-if [[ "${BISECT_MODE}" == "OBJECT_MODE" ]]; then
-  echo "EMERGING ${BISECT_PACKAGE}"
-  set -x
-  sudo rm -rf /build/${BISECT_BOARD}/var/cache/portage/*
-  sudo rm -rf /build/${BISECT_BOARD}/tmp/portage/${BISECT_PACKAGE}*
-  set +x
-  if [[ ${BISECT_PACKAGE} == *chromeos-chrome ]]; then
-    if [[ ${BISECT_USE_FLAGS} == *chrome_internal* && \
-      ${BISECT_USE_FLAGS} != *-chrome_internal* ]]; then
-      # for the pre-upload check of the length of lines
-      chrome_build_dir="/var/cache/chromeos-chrome/chrome-src-internal/src/"
-      chrome_build_dir+="out_${BISECT_BOARD}"
-    else
-      # for the pre-upload check of the length of lines
-      chrome_build_dir="/var/cache/chromeos-chrome/chrome-src/src/"
-      chrome_build_dir+="out_${BISECT_BOARD}"
-    fi
-    set -x
-    sudo rm -rf ${chrome_build_dir}
-    set +x
-  fi
-  set -x
-  CLEAN_DELAY=0 emerge-${BISECT_BOARD} -C ${BISECT_PACKAGE}
-  USE="${BISECT_USE_FLAGS}" emerge-${BISECT_BOARD} ${BISECT_PACKAGE}
-  set +x
-  emerge_status=$?
-
-  if [[ ${emerge_status} -ne 0 ]] ; then
-    echo "emerging ${BISECT_PACKAGE} returned a non-zero status: $emerge_status"
-    exit 1
-  fi
-
-  echo
-  echo "DEPLOYING TO ${BISECT_REMOTE}"
-
-  if [[ ${BISECT_PACKAGE} == *chromeos-kernel-* ]]; then
-    cmd="/mnt/host/source/src/scripts/update_kernel.sh --board=${BISECT_BOARD} --remote=${BISECT_REMOTE}"
-    if [[ ${BISECT_REMOTE} == *:* ]]; then
-      IP=$(echo $1 | cut -d ":" -f1)
-      PORT=$(echo $1 | cut -d ":" -f2)
-      cmd="/mnt/host/source/src/scripts/update_kernel.sh --board=${BISECT_BOARD} --remote=${IP} --ssh_port=${PORT}"
-    fi
-    if [[ ${BISECT_REBOOT_OPTION} == false ]]; then
-      cmd+=" --noreboot"
-    fi
-    set -x
-    # exec the command to make sure it always exit after
-    exec $cmd
-    set +x
-  fi
-
-  if [[ ${BISECT_PACKAGE} == *chromeos-chrome ]]; then
-    # deploy_chrome needs to run inside chrome source tree
-    pushd ~/chrome_root
-    set -x
-    deploy_chrome --force --build-dir=${chrome_build_dir}/Release \
-      --device=${BISECT_REMOTE}
-    set +x
-    popd
-  else
-    set -x
-    cros deploy ${BISECT_REMOTE} ${BISECT_PACKAGE} --log-level=info
-    set +x
-  fi
-  deploy_status=$?
-
-  if [[ ${BISECT_REBOOT_OPTION} == false ]]; then
-    exit 0
-  fi
-
-  if [[ ${deploy_status} -eq 0 ]] ; then
-    echo "Deploy successful. Rebooting device..."
-    reboot
-    if [[ $? -ne 0 ]] ; then
-      echo
-      echo "Could not automatically reboot device!"
-      read -p "Please manually reboot device and press enter to continue" notused
-    fi
-    exit 0
-  fi
-
-  echo "Deploy failed! Trying build_image/cros flash instead..."
-  echo
-fi
-
-echo "BUILDING IMAGE"
-pushd ~/trunk/src/scripts
-USE="${BISECT_USE_FLAGS}" ./build_image test --board=${BISECT_BOARD} \
-  --noenable_rootfs_verification --noeclean
-build_status=$?
-popd
-
-if [[ ${build_status} -eq 0 ]] ; then
-    echo
-    echo "FLASHING"
-    echo "Pushing built image onto device."
-    echo "cros flash --board=${BISECT_BOARD} --clobber-stateful ${BISECT_REMOTE} ~/trunk/src/build/images/${BISECT_BOARD}/latest/chromiumos_test_image.bin"
-    cros flash --board=${BISECT_BOARD} --clobber-stateful ${BISECT_REMOTE} ~/trunk/src/build/images/${BISECT_BOARD}/latest/chromiumos_test_image.bin
-    cros_flash_status=$?
-    while [[ ${cros_flash_status} -ne 0 ]] ; do
-        while true; do
-          echo
-          echo "cros flash has failed! From here you can:"
-          echo "1. Flash through USB"
-          echo "2. Retry flashing over ethernet"
-          echo "3. Abort this installation and skip this image"
-          echo "4. Abort this installation and mark test as failed"
-          sleep 1
-          read -p "Which method would you like to do? " choice
-          case $choice in
-              1) usb_flash && break;;
-              2) ethernet_flash && break;;
-              3) exit 125;;
-              4) exit 1;;
-              *) echo "Please answer 1, 2, 3, or 4.";;
-          esac
-        done
-
-        cros_flash_status=$?
-    done
-else
-    echo "build_image returned a non-zero status: ${build_status}"
-    exit 1
-fi
-
-exit 0
diff --git a/binary_search_tool/compiler_wrapper.py b/binary_search_tool/compiler_wrapper.py
deleted file mode 100755
index c32826b0..00000000
--- a/binary_search_tool/compiler_wrapper.py
+++ /dev/null
@@ -1,68 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Prototype compiler wrapper.
-
-Only tested with: gcc, g++, clang, clang++
-Installation instructions:
-  1. Rename compiler from <compiler_name> to <compiler_name>.real
-  2. Create symlink from this script (compiler_wrapper.py), and name it
-     <compiler_name>. compiler_wrapper.py can live anywhere as long as it is
-     executable.
-
-Reference page:
-https://sites.google.com/a/google.com/chromeos-toolchain-team-home2/home/team-tools-and-scripts/bisecting-chromeos-compiler-problems/bisection-compiler-wrapper
-
-Design doc:
-https://docs.google.com/document/d/1yDgaUIa2O5w6dc3sSTe1ry-1ehKajTGJGQCbyn0fcEM
-"""
-
-
-import os
-import shlex
-import sys
-
-from binary_search_tool import bisect_driver
-
-
-WRAPPED = "%s.real" % sys.argv[0]
-BISECT_STAGE = os.environ.get("BISECT_STAGE")
-DEFAULT_BISECT_DIR = os.path.expanduser("~/ANDROID_BISECT")
-BISECT_DIR = os.environ.get("BISECT_DIR") or DEFAULT_BISECT_DIR
-
-
-def ProcessArgFile(arg_file):
-    args = []
-    # Read in entire file at once and parse as if in shell
-    with open(arg_file, "r", encoding="utf-8") as f:
-        args.extend(shlex.split(f.read()))
-
-    return args
-
-
-def Main(_):
-    if not os.path.islink(sys.argv[0]):
-        print("Compiler wrapper can't be called directly!")
-        return 1
-
-    execargs = [WRAPPED] + sys.argv[1:]
-
-    if BISECT_STAGE not in bisect_driver.VALID_MODES or "-o" not in execargs:
-        os.execv(WRAPPED, [WRAPPED] + sys.argv[1:])
-
-    # Handle @file argument syntax with compiler
-    for idx, _ in enumerate(execargs):
-        # @file can be nested in other @file arguments, use While to re-evaluate
-        # the first argument of the embedded file.
-        while execargs[idx][0] == "@":
-            args_in_file = ProcessArgFile(execargs[idx][1:])
-            execargs = execargs[0:idx] + args_in_file + execargs[idx + 1 :]
-
-    bisect_driver.bisect_driver(BISECT_STAGE, BISECT_DIR, execargs)
-
-
-if __name__ == "__main__":
-    sys.exit(Main(sys.argv[1:]))
diff --git a/binary_search_tool/cros_pkg/README.cros_pkg_triage.md b/binary_search_tool/cros_pkg/README.cros_pkg_triage.md
deleted file mode 100644
index 17121dd7..00000000
--- a/binary_search_tool/cros_pkg/README.cros_pkg_triage.md
+++ /dev/null
@@ -1,193 +0,0 @@
-# CrOS's binary search tool
-
-`binary_search_state.py` is a general binary search triage tool that
-performs a binary search on a set of things to try to identify which
-thing or thing(s) in the set is 'bad'.  `binary_search_state.py` assumes
-that the user has two sets, one where everything is known to be good,
-ane one which contains at least one bad item.  `binary_search_state.py`
-then copies items from the good and bad sets into a working set and
-tests the result (good or bad).  `binary_search_state.py` requires that
-a set of scripts be supplied to it for any particular job.  For more
-information on `binary_search_state.py`, see
-
-https://sites.google.com/a/google.com/chromeos-toolchain-team-home2/home/team-tools-and-scripts/binary-searcher-tool-for-triage
-
-This particular set of scripts is designed to work wtih
-`binary_search_state.py` in order to find the bad package or set of
-packages in a ChromeOS build.
-
-
-## QUICKSTART
-
-After setting up your 3 build trees (see Prerequisites section), do the
-following:
-
--   Decide which test script to use (`boot_test.sh` or
-    `interactive_test.sh`)
--   Get the IP name or address of the chromebook you will use for testing.
--   Do the following inside your chroot:
-
-    ```
-    $ cd ~/trunk/src/third_party/toolchain_utils/binary_search_tool
-    $ ./cros_pkg/setup.sh <board-to-test> <IP-name-or-address-of-chromebook>
-    ```
-
-    If you chose the boot test, then:
-
-    ```
-    $ python ./binary_search_state.py \
-        --get_initial_items=cros_pkg/get_initial_items.sh \
-        --switch_to_good=cros_pkg/switch_to_good.sh \
-        --switch_to_bad=cros_pkg/switch_to_bad.sh \
-        --test_setup_script=cros_pkg/test_setup.sh \
-        --test_script=cros_pkg/boot_test.sh \
-        --file_args \
-        --prune
-    ```
-
-    Otherwise, if you chose the interactive test, then:
-
-    ```
-    $ python ./binary_search_state.py \
-        --get_initial_items=cros_pkg/get_initial_items.sh \
-        --switch_to_good=cros_pkg/switch_to_good.sh \
-        --switch_to_bad=cros_pkg/switch_to_bad.sh \
-        --test_setup_script=cros_pkg/test_setup.sh \
-        --test_script=cros_pkg/interactive_test.sh \
-        --file_args \
-        --prune
-    ```
-
-    Once you have completely finished doing the binary search/triage,
-    run the genereated cleanup script, to restore your chroot to the state
-    it was in before you ran the `setup.sh` script:
-
-    ```
-    $ cros_pkg/${BOARD}_cleanup.sh
-    ```
-
-
-## FILES AND SCRIPTS
-
-`boot_test.sh` - One of two possible test scripts used to determine
-                 if the ChromeOS image built from the packages is good
-                 or bad.  This script tests to see if the image
-                 booted, and requires no user intervention.
-
-`create_cleanup_script.py` - This is called by setup.sh, to
-                             generate ${BOARD}_cleanup.sh,
-                             which is supposed to be run by the user
-                             after the binary search triage process is
-                             finished, to undo the changes made by
-                             setup.sh and return everything
-                             to its original state.
-
-`get_initial_items.sh` - This script is used to determine the current
-                         set of ChromeOS packages.
-
-`test_setup.sh` - This script will build and flash your image to the
-                  remote machine. If the flash fails, this script will
-                  help the user troubleshoot by flashing through usb or
-                  by retrying the flash over ethernet.
-
-`interactive_test.sh` - One of two possible scripts used to determine
-                        if the ChromeOS image built from the packages
-                        is good or bad.  This script requires user
-                        interaction to determine if the image is
-                        good or bad.
-
-`setup.sh` - This is the first script the user should call, after
-             taking care of the prerequisites.  It sets up the
-             environment appropriately for running the ChromeOS
-             package binary search triage, and it generates two
-             necessary scripts (see below).
-
-`switch_to_bad.sh` - This script is used to copy packages from the
-                     'bad' build tree into the work area.
-
-`switch_to_good.sh` - This script is used to copy packages from the
-                      'good' build tree into the work area.
-
-
-## GENERATED SCRIPTS
-
-`common.sh`  - contains basic environment variable definitions for
-               this binary search triage session.
-
-`${BOARD}_cleanup.sh` - script to undo all the changes made by
-                        running setup.sh, and returning
-                        everything to its original state. The user
-                        should manually run this script once the
-                        binary search triage process is over.
-
-## ASSUMPTIONS
-
--   There are two different ChromeOS builds, for the same board, with the
-    same set of ChromeOS packages.  One build creates a good working ChromeOS
-    image and the other does not.
-
--   You have saved the complete build trees for both the good and bad builds.
-
-
-## PREREQUISITES FOR USING THESE SCRIPTS (inside the chroot)
-
--   The "good" build tree, for the board, is in /build/${board}.good
-    (e.g. /build/lumpy.good or /build/daisy.good).
-
--   The "bad" build tree is in /build/${board}.bad
-    (e.g. /build/lumpy.bad or /build/daisy.bad).
-
--   You made a complete copy of the "bad" build tree , and put it in
-    /build/${board}.work (e.g. /build/lumpy.work or /build/daisy.work.
-    The easiest way to do this is to use something similar to the
-    following set of commands (this example assumes the board is
-    'lumpy'):
-
-    ```
-    $ cd /build
-    $ sudo tar -cvf lumpy.bad.tar lumpy.bad
-    $ sudo mv lumpy.bad lumpy.work
-    $ sudo tar -xvf lumpy.bad.tar
-    ```
-
-
-## USING THESE SCRIPTS FOR BINARY TRIAGE OF PACKAGES
-
-To use these scripts, you must first run setup.sh, passing it two
-arguments (in order): the board for which you are building the image;
-and the name or ip address of the chromebook you want to use for
-testing your chromeos images.  setup.sh will do the following:
-
--   Verify that your build trees are set up correctly (with good, bad and work).
--   Create a soft link for /build/${board} pointing to the work build tree.
--   Create the common.sh file that the other scripts passed to the binary triage
-    tool will need.
--   Create a cleanup script, ${board}_cleanup.sh, for you to run after you are
-    done with the binary triages, to undo all of these various changes that
-    setup.sh did.
-
-This set of scripts comes with two alternate test scripts.  One test
-script, `boot_test.sh`, just checks to make sure that the image
-booted (i.e. responds to ping) and assumes that is enough.  The other
-test script, `interactive_test.sh`, is interactive and asks YOU
-to tell it whether the image on the chromebook is ok or not (it
-prompts you and waits for a response).
-
-
-Once you have run `setup.sh` (and decided which test script you
-want to use) run the binary triage tool using these scripts to
-isolate/identify the bad package:
-
-```
-~/trunk/src/third_party/toolchain_utils/binary_search_tool/binary_search_state.py \
-    --get_initial_items=cros_pkg/get_initial_items.sh \
-    --switch_to_good=cros_pkg/switch_to_good.sh \
-    --switch_to_bad=cros_pkg/switch_to_bad.sh \
-    --test_setup_script=cros_pkg/test_setup.sh \
-    --test_script=cros_pkg/boots_test.sh \  # could use interactive_test.sh instead
-    --prune
-```
-
-After you have finished running the tool and have identified the bad
-package(s), you will want to run the cleanup script that `setup.sh`
-generated (`cros_pkg/${BOARD}_cleanup.sh`).
diff --git a/binary_search_tool/cros_pkg/boot_test.sh b/binary_search_tool/cros_pkg/boot_test.sh
deleted file mode 120000
index 9a345617..00000000
--- a/binary_search_tool/cros_pkg/boot_test.sh
+++ /dev/null
@@ -1 +0,0 @@
-../common/boot_test.sh
\ No newline at end of file
diff --git a/binary_search_tool/cros_pkg/create_cleanup_script.py b/binary_search_tool/cros_pkg/create_cleanup_script.py
deleted file mode 100755
index abfea5eb..00000000
--- a/binary_search_tool/cros_pkg/create_cleanup_script.py
+++ /dev/null
@@ -1,129 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""The script to generate a cleanup script after setup.sh.
-
-This script takes a set of flags, telling it what setup.sh changed
-during the set up process. Based on the values of the input flags, it
-generates a cleanup script, named ${BOARD}_cleanup.sh, which will
-undo the changes made by setup.sh, returning everything to its
-original state.
-"""
-
-
-import argparse
-import sys
-
-
-def Usage(parser, msg):
-    print("ERROR: " + msg)
-    parser.print_help()
-    sys.exit(1)
-
-
-def Main(argv):
-    """Generate a script to undo changes done by setup.sh
-
-    The script setup.sh makes a change that needs to be
-    undone, namely it creates a soft link making /build/${board} point
-    to /build/${board}.work.  To do this, it had to see if
-    /build/${board} already existed, and if so, whether it was a real
-    tree or a soft link.  If it was soft link, it saved the old value
-    of the link, then deleted it and created the new link.  If it was
-    a real tree, it renamed the tree to /build/${board}.save, and then
-    created the new soft link.  If the /build/${board} did not
-    previously exist, then it just created the new soft link.
-
-    This function takes arguments that tell it exactly what setup.sh
-    actually did, then generates a script to undo those exact changes.
-    """
-
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "--board",
-        dest="board",
-        required=True,
-        help="Chromeos board for packages/image.",
-    )
-
-    parser.add_argument(
-        "--old_tree_missing",
-        dest="tree_existed",
-        action="store_false",
-        help="Did /build/${BOARD} exist.",
-        default=True,
-    )
-
-    parser.add_argument(
-        "--renamed_tree",
-        dest="renamed_tree",
-        action="store_true",
-        help="Was /build/${BOARD} saved & renamed.",
-        default=False,
-    )
-
-    parser.add_argument(
-        "--old_link",
-        dest="old_link",
-        help=("The original build tree soft link."),
-    )
-
-    options = parser.parse_args(argv[1:])
-
-    if options.old_link or options.renamed_tree:
-        if not options.tree_existed:
-            Usage(
-                parser,
-                "If --tree_existed is False, cannot have "
-                "--renamed_tree or --old_link",
-            )
-
-    if options.old_link and options.renamed_tree:
-        Usage(parser, "--old_link and --renamed_tree are incompatible options.")
-
-    if options.tree_existed:
-        if not options.old_link and not options.renamed_tree:
-            Usage(
-                parser,
-                "If --tree_existed is True, then must have either "
-                "--old_link or --renamed_tree",
-            )
-
-    out_filename = "cros_pkg/" + options.board + "_cleanup.sh"
-
-    with open(out_filename, "w", encoding="utf-8") as out_file:
-        out_file.write("#!/bin/bash\n\n")
-        # First, remove the 'new' soft link.
-        out_file.write("sudo rm /build/%s\n" % options.board)
-        if options.tree_existed:
-            if options.renamed_tree:
-                # Old build tree existed and was a real tree, so it got
-                # renamed.  Move the renamed tree back to the original tree.
-                out_file.write(
-                    "sudo mv /build/%s.save /build/%s\n"
-                    % (options.board, options.board)
-                )
-            else:
-                # Old tree existed and was already a soft link.  Re-create the
-                # original soft link.
-                original_link = options.old_link
-                if original_link[0] == "'":
-                    original_link = original_link[1:]
-                if original_link[-1] == "'":
-                    original_link = original_link[:-1]
-                out_file.write(
-                    "sudo ln -s %s /build/%s\n" % (original_link, options.board)
-                )
-        out_file.write("\n")
-        # Remove common.sh file
-        out_file.write("rm common/common.sh\n")
-
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv)
-    sys.exit(retval)
diff --git a/binary_search_tool/cros_pkg/get_initial_items.sh b/binary_search_tool/cros_pkg/get_initial_items.sh
deleted file mode 100755
index bc0fd2e6..00000000
--- a/binary_search_tool/cros_pkg/get_initial_items.sh
+++ /dev/null
@@ -1,15 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2015 Google LLC
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on ChromeOS packages.  This script
-# generates the list of current ChromeOS packages, that is then used
-# for doing the binary search.
-#
-
-source common/common.sh
-
-cd ${GOOD_BUILD}/packages
-find . -name "*.tbz2"
-
diff --git a/binary_search_tool/cros_pkg/interactive_test.sh b/binary_search_tool/cros_pkg/interactive_test.sh
deleted file mode 120000
index 18fe3958..00000000
--- a/binary_search_tool/cros_pkg/interactive_test.sh
+++ /dev/null
@@ -1 +0,0 @@
-../common/interactive_test.sh
\ No newline at end of file
diff --git a/binary_search_tool/cros_pkg/interactive_test_noping.sh b/binary_search_tool/cros_pkg/interactive_test_noping.sh
deleted file mode 120000
index c76f9404..00000000
--- a/binary_search_tool/cros_pkg/interactive_test_noping.sh
+++ /dev/null
@@ -1 +0,0 @@
-../common/interactive_test_noping.sh
\ No newline at end of file
diff --git a/binary_search_tool/cros_pkg/setup.sh b/binary_search_tool/cros_pkg/setup.sh
deleted file mode 100755
index 30a3a423..00000000
--- a/binary_search_tool/cros_pkg/setup.sh
+++ /dev/null
@@ -1,123 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2015 Google LLC
-#
-# This script is part of the ChromeOS package binary search triage process.
-# It should be the first script called by the user, after the user has set up
-# the three necessary build tree directories (see the prerequisites section of
-# README.cros_pkg_triage).
-#
-# This script requires two arguments.  The first argument must be the name of
-# the board for which this work is being done (e.g. 'daisy', 'lumpy','parrot',
-# etc.).  The second argument must be the name or IP address of the chromebook
-# on which the ChromeOS images will be pushed and tested.
-#
-# This script sets up a soft link definining /build/${board} to point
-# to the working build tree, for the binary search triags process.  In
-# addition, this script generates two other scripts, common.sh,
-# which generates enviroment variables used by the other scripts in the
-# package binary search triage process; and ${board}_cleanup.sh,
-# which undoes the various changes that this script performs, returning the
-# user's environment to its original state.
-#
-
-# Set up basic variables.
-
-BOARD=$1
-REMOTE=$2
-
-GOOD_BUILD=/build/${BOARD}.good
-BAD_BUILD=/build/${BOARD}.bad
-WORK_BUILD=/build/${BOARD}.work
-
-#
-# Verify that the necessary directories exist.
-#
-
-if [[ ! -d ${GOOD_BUILD} ]] ; then
-    echo "Error:  ${GOOD_BUILD} does not exist."
-    exit 1
-fi
-
-if [[ ! -d ${BAD_BUILD} ]] ; then
-    echo "Error:  ${BAD_BUILD} does not exist."
-    exit 1
-fi
-
-if [[ ! -d ${WORK_BUILD} ]] ; then
-    echo "Error:  ${WORK_BUILD} does not exist."
-    exit 1
-fi
-
-#
-# Check to see if /build/${BOARD} already exists and if so, in what state.
-# Set appropriate flags & values, in order to be able to undo these changes
-# in ${board}_cleanup.sh. If it's a soft link, remove it; if it's a
-# read tree, rename it.
-#
-
-build_tree_existed=0
-build_tree_was_soft_link=0
-build_tree_renamed=0
-build_tree_link=""
-
-if [[ -d "/build/${BOARD}" ]] ; then
-    build_tree_existed=1
-    if [[ -L "/build/${BOARD}" ]] ; then
-        build_tree_was_soft_link=1
-        build_tree_link=`readlink /build/${BOARD}`
-        sudo rm /build/${BOARD}
-    else
-        build_tree_renamed=1
-        sudo mv /build/${BOARD} /build/${BOARD}.save
-    fi
-fi
-
-# Make "working' tree the default board tree (set up soft link)
-
-sudo ln -s /build/${BOARD}.work /build/${BOARD}
-
-#
-# Create common.sh file, containing appropriate environment variables.
-#
-
-COMMON_FILE="common/common.sh"
-
-cat <<-EOF > ${COMMON_FILE}
-
-BISECT_BOARD=${BOARD}
-BISECT_REMOTE=${REMOTE}
-BISECT_MODE="PACKAGE_MODE"
-
-GOOD_BUILD=/build/${BOARD}.good
-BAD_BUILD=/build/${BOARD}.bad
-WORK_BUILD=/build/${BOARD}.work
-
-EOF
-
-chmod 755 ${COMMON_FILE}
-
-#
-# Create clean-up script, calling create_cleanup_script.py with
-# the appropriate flags.
-#
-
-if [[ ${build_tree_existed} -eq 0 ]] ; then
-
-    python cros_pkg/create_cleanup_script.py --board=${BOARD} \
-        --old_tree_missing
-
-elif [[ ${build_tree_was_soft_link} -eq 0 ]] ; then
-
-    python cros_pkg/create_cleanup_script.py --board=${BOARD} \
-        --renamed_tree
-
-else
-
-    python cros_pkg/create_cleanup_script.py --board=${BOARD} \
-        --old_link="'${build_tree_link}'"
-fi
-
-chmod 755 cros_pkg/${BOARD}_cleanup.sh
-
-exit 0
diff --git a/binary_search_tool/cros_pkg/switch_to_bad.sh b/binary_search_tool/cros_pkg/switch_to_bad.sh
deleted file mode 100755
index b4156a0e..00000000
--- a/binary_search_tool/cros_pkg/switch_to_bad.sh
+++ /dev/null
@@ -1,46 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2015 Google LLC
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on ChromeOS packages.  This script
-# copies a list of packages from the 'bad' build tree into the working
-# build tree, for testing.
-#
-
-source common/common.sh
-
-pushd ${WORK_BUILD}
-
-PKG_LIST_FILE=$1
-
-overall_status=0
-
-if [[ -f ${PKG_LIST_FILE} ]] ; then
-
-  # Read every line, and handle case where last line has no newline
-  while read pkg || [[ -n "$pkg" ]];
-  do
-    sudo cp ${BAD_BUILD}/packages/$pkg ${WORK_BUILD}/packages/$pkg
-    status=$?
-    if [[ ${status} -ne 0 ]] ; then
-      echo "Failed to copy ${pkg} to work build tree."
-      overall_status=2
-    fi
-  done < ${PKG_LIST_FILE}
-else
-
-  for o in "$@"
-  do
-    sudo cp ${BAD_BUILD}/packages/$o  ${WORK_BUILD}/packages/$o
-    status=$?
-    if [[ ${status} -ne 0 ]] ; then
-      echo "Failed to copy ${pkg} to work build tree."
-      overall_status=2
-    fi
-  done
-fi
-
-popd
-
-exit ${overall_status}
diff --git a/binary_search_tool/cros_pkg/switch_to_good.sh b/binary_search_tool/cros_pkg/switch_to_good.sh
deleted file mode 100755
index 5f7c2d77..00000000
--- a/binary_search_tool/cros_pkg/switch_to_good.sh
+++ /dev/null
@@ -1,46 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2015 Google LLC
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on ChromeOS packages.  This script
-# copies a list of packages from the 'good' build tree into the working
-# build tree, for testing.
-#
-
-source common/common.sh
-
-pushd ${WORK_BUILD}
-
-PKG_LIST_FILE=$1
-
-overall_status=0
-
-if [[ -f ${PKG_LIST_FILE} ]] ; then
-
-  # Read every line, and handle case where last line has no newline
-  while read pkg || [[ -n "$pkg" ]];
-  do
-    sudo cp ${GOOD_BUILD}/packages/$pkg ${WORK_BUILD}/packages/$pkg
-    status=$?
-    if [[ ${status} -ne 0 ]] ; then
-      echo "Failed to copy ${pkg} to work build tree."
-      overall_status=2
-    fi
-  done < ${PKG_LIST_FILE}
-else
-
-  for o in "$@"
-  do
-    sudo cp ${GOOD_BUILD}/packages/$o  ${WORK_BUILD}/packages/$o
-    status=$?
-    if [[ ${status} -ne 0 ]] ; then
-      echo "Failed to copy ${pkg} to work build tree."
-      overall_status=2
-    fi
-  done
-fi
-
-popd
-
-exit ${overall_status}
diff --git a/binary_search_tool/cros_pkg/test_setup.sh b/binary_search_tool/cros_pkg/test_setup.sh
deleted file mode 120000
index 39e715f6..00000000
--- a/binary_search_tool/cros_pkg/test_setup.sh
+++ /dev/null
@@ -1 +0,0 @@
-../common/test_setup.sh
\ No newline at end of file
diff --git a/binary_search_tool/cros_pkg/test_setup_usb.sh b/binary_search_tool/cros_pkg/test_setup_usb.sh
deleted file mode 100755
index 54d0baa1..00000000
--- a/binary_search_tool/cros_pkg/test_setup_usb.sh
+++ /dev/null
@@ -1,56 +0,0 @@
-#!/bin/bash
-#
-# Copyright 2016 Google LLC
-#
-# This is a generic ChromeOS package/image test setup script. It is meant to
-# be used for the package bisection tool, in particular when there is a booting
-# issue with the image, so the image MUST be 'flashed' via USB.
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on ChromeOS objects and packages. It should
-# return '0' if the setup succeeds; and '1' if the setup fails (the image
-# could not built or be flashed).
-#
-
-export PYTHONUNBUFFERED=1
-
-source common/common.sh
-
-echo "BUILDING IMAGE"
-pushd ~/trunk/src/scripts
-./build_image test --board=${BISECT_BOARD} --noenable_rootfs_verification --noeclean
-build_status=$?
-popd
-
-if [[ ${build_status} -eq 0 ]] ; then
-    echo
-    echo "INSTALLING IMAGE VIA USB (requires some manual intervention)"
-    echo
-    echo "Insert a usb stick into the current machine"
-    echo "Note: The cros flash will take time and doesn't give much output."
-    echo "      Be patient. If your usb access light is flashing it's working."
-    sleep 1
-    read -p "Press enter to continue" notused
-
-    cros flash --board=${BISECT_BOARD} --clobber-stateful usb:// ~/trunk/src/build/images/${BISECT_BOARD}/latest/chromiumos_test_image.bin
-
-    echo
-    echo "Flash to usb complete!"
-    echo "Plug the usb into your chromebook and install the image."
-    echo "Refer to the ChromiumOS Developer's Handbook for more details."
-    echo "http://www.chromium.org/chromium-os/developer-guide#TOC-Boot-from-your-USB-disk"
-    while true; do
-      sleep 1
-      read -p "Was the installation of the image successful? " choice
-      case $choice in
-        [Yy]*) exit 0;;
-        [Nn]*) exit 1;;
-        *) echo "Please answer y or n.";;
-      esac
-    done
-else
-    echo "build_image returned a non-zero status: ${build_status}"
-    exit 1
-fi
-
-exit 0
diff --git a/binary_search_tool/full_bisect_test/bad-objects-permanent/_LIST b/binary_search_tool/full_bisect_test/bad-objects-permanent/_LIST
deleted file mode 100644
index 07bc1aa9..00000000
--- a/binary_search_tool/full_bisect_test/bad-objects-permanent/_LIST
+++ /dev/null
@@ -1,7 +0,0 @@
-build.o
-inorder_norecurse.o
-inorder.o
-main.o
-preorder_norecurse.o
-preorder.o
-stack.o
diff --git a/binary_search_tool/full_bisect_test/bad-output-1.txt b/binary_search_tool/full_bisect_test/bad-output-1.txt
deleted file mode 100644
index dfd0bfc6..00000000
--- a/binary_search_tool/full_bisect_test/bad-output-1.txt
+++ /dev/null
@@ -1,11 +0,0 @@
-pre-order traversal, with recursion: 
-35 28 20 25 23 26 30 60 70 65 64 68 
-
-pre-order traversal, without recursion: 
-35 28 20 25 23 26 30 60 70 65 64 68 
-
-in-order traversal, with recursion: 
-20 23 25 26 28 30 35 60 64 65 68 70 
-
-in-order traversal, without recursion: 
-28 30 35 60 65 68 70 
diff --git a/binary_search_tool/full_bisect_test/bad-output-2.txt b/binary_search_tool/full_bisect_test/bad-output-2.txt
deleted file mode 100644
index e35ebddc..00000000
--- a/binary_search_tool/full_bisect_test/bad-output-2.txt
+++ /dev/null
@@ -1,11 +0,0 @@
-pre-order traversal, with recursion: 
-35 28 20 25 23 26 30 60 70 65 64 68 
-
-pre-order traversal, without recursion: 
-35 60 70 
-
-in-order traversal, with recursion: 
-20 23 25 26 28 30 35 60 64 65 68 70 
-
-in-order traversal, without recursion: 
-20 23 25 26 28 30 35 60 64 65 68 70 
diff --git a/binary_search_tool/full_bisect_test/bad-output-3.txt b/binary_search_tool/full_bisect_test/bad-output-3.txt
deleted file mode 100644
index 5f3bfef5..00000000
--- a/binary_search_tool/full_bisect_test/bad-output-3.txt
+++ /dev/null
@@ -1,11 +0,0 @@
-pre-order traversal, with recursion: 
-35 28 20 25 23 26 30 60 70 65 64 68 
-
-pre-order traversal, without recursion: 
-35 60 70 
-
-in-order traversal, with recursion: 
-20 23 25 26 28 30 35 60 64 65 68 70 
-
-in-order traversal, without recursion: 
-28 30 35 60 65 68 70 
diff --git a/binary_search_tool/full_bisect_test/bin-trees.h b/binary_search_tool/full_bisect_test/bin-trees.h
deleted file mode 100644
index 1c4fa199..00000000
--- a/binary_search_tool/full_bisect_test/bin-trees.h
+++ /dev/null
@@ -1,29 +0,0 @@
-#ifndef _BIN_TREES_H
-#define _BIN_TREES_H
-
-
-struct bin_tree_struct {
-  int data;
-  char c_data;
-  struct bin_tree_struct *left;
-  struct bin_tree_struct *right;
-};
-
-typedef struct bin_tree_struct * tree_ptr;
-
-
-struct stack_struct {
-  tree_ptr data;
-  struct stack_struct *next;
-};
-
-
-void search_tree_insert (tree_ptr *, int);
-void pre_order_traverse (tree_ptr);
-void pre_order_traverse_no_recurse (tree_ptr);
-void in_order_traverse (tree_ptr);
-void in_order_traverse_no_recurse (tree_ptr);
-void push (struct stack_struct **, tree_ptr);
-tree_ptr pop (struct stack_struct **);
-
-#endif /* _BIN_TREES_H */
diff --git a/binary_search_tool/full_bisect_test/build.c b/binary_search_tool/full_bisect_test/build.c
deleted file mode 100644
index ea1c8b49..00000000
--- a/binary_search_tool/full_bisect_test/build.c
+++ /dev/null
@@ -1,23 +0,0 @@
-#include <stdlib.h>
-#include "bin-trees.h"
-
-tree_ptr
-new_node (int value)
-{
-  tree_ptr node = (tree_ptr) malloc (sizeof (tree_ptr));
-  node->data = value;
-  node->left = NULL;
-  node->right = NULL;
-  return node;
-}
-
-void
-search_tree_insert (tree_ptr *root, int value)
-{
-  if (*root == NULL)
-    *root = new_node (value);
-  else if (value < (*root)->data)
-    search_tree_insert (&((*root)->left), value);
-  else if (value > (*root)->data)
-    search_tree_insert (&((*root)->right), value);
-}
diff --git a/binary_search_tool/full_bisect_test/build.sh b/binary_search_tool/full_bisect_test/build.sh
deleted file mode 100755
index 9d40fb55..00000000
--- a/binary_search_tool/full_bisect_test/build.sh
+++ /dev/null
@@ -1,22 +0,0 @@
-#!/bin/bash
-
-# This file compiles all the source files into .o files, then links them to form
-# the test binary, 'bin-trees'.  The .o files all go into the 'work' directory.
-# There are 'good' and 'bad' versions of inorder_norecurse and preorder_norecurse
-# (e.g. inorder_norecurse.c.good and inorder_norecurse.c.bad).  This script
-# assumes that the desired versions of those files have been copied into
-# inorder_norecurse.c and preorder_norecurse.c.  The script files
-# make_sources_good.sh and make_sources_bad.sh are meant to handle this.
-#
-#  This script is meant to be run directly in the full_bisect_test directory.
-#  Most other scripts assume they are being run from the parent directory.
-
-gcc -c build.c -o work/build.o
-gcc -c preorder.c -o work/preorder.o
-gcc -c inorder.c -o work/inorder.o
-gcc -c main.c -o work/main.o
-gcc -c stack.c -o work/stack.o 
-gcc -c preorder_norecurse.c -o work/preorder_norecurse.o
-gcc -c inorder_norecurse.c -o work/inorder_norecurse.o
-gcc -o bin-trees work/main.o work/preorder.o work/inorder.o work/build.o work/preorder_norecurse.o work/inorder_norecurse.o work/stack.o
-
diff --git a/binary_search_tool/full_bisect_test/chromeos_build.sh b/binary_search_tool/full_bisect_test/chromeos_build.sh
deleted file mode 100755
index f072bd07..00000000
--- a/binary_search_tool/full_bisect_test/chromeos_build.sh
+++ /dev/null
@@ -1,21 +0,0 @@
-#!/bin/bash
-
-# This file compiles all the source files into .o files, then links them to form
-# the test binary, 'bin-trees'.  The .o files all go into the 'work' directory.
-# There are 'good' and 'bad' versions of inorder_norecurse and preorder_norecurse
-# (e.g. inorder_norecurse.c.good and inorder_norecurse.c.bad).  This script
-# assumes that the desired versions of those files have been copied into
-# inorder_norecurse.c and preorder_norecurse.c.  The script files
-# make_sources_good.sh and make_sources_bad.sh are meant to handle this.
-#
-#  This script is meant to be run directly in the full_bisect_test directory.
-#  Most other scripts assume they are being run from the parent directory.
-
-x86_64-cros-linux-gnu-gcc -c build.c -o work/build.o
-x86_64-cros-linux-gnu-gcc -c preorder.c -o work/preorder.o
-x86_64-cros-linux-gnu-gcc -c inorder.c -o work/inorder.o
-x86_64-cros-linux-gnu-gcc -c main.c -o work/main.o
-x86_64-cros-linux-gnu-gcc -c stack.c -o work/stack.o 
-x86_64-cros-linux-gnu-gcc -c preorder_norecurse.c -o work/preorder_norecurse.o
-x86_64-cros-linux-gnu-gcc -c inorder_norecurse.c -o work/inorder_norecurse.o
-x86_64-cros-linux-gnu-gcc -o bin-trees work/main.o work/preorder.o work/inorder.o work/build.o work/preorder_norecurse.o work/inorder_norecurse.o work/stack.o
diff --git a/binary_search_tool/full_bisect_test/cleanup.sh b/binary_search_tool/full_bisect_test/cleanup.sh
deleted file mode 100755
index 48b44f30..00000000
--- a/binary_search_tool/full_bisect_test/cleanup.sh
+++ /dev/null
@@ -1,8 +0,0 @@
-#!/bin/bash
-
-# In keeping with the normal way of doing bisectin, this script is meant to
-# remove files specific to the particular run of the bisector.
-#
-# This file is called from main-bisect-test.sh
-
-rm full_bisect_test/common.sh
diff --git a/binary_search_tool/full_bisect_test/get_initial_items.sh b/binary_search_tool/full_bisect_test/get_initial_items.sh
deleted file mode 100755
index 4c4043f1..00000000
--- a/binary_search_tool/full_bisect_test/get_initial_items.sh
+++ /dev/null
@@ -1,9 +0,0 @@
-#!/bin/bash -u
-#
-# This is one of the test scripts that needs to be passed to
-# binary_search_state.py.
-
-source full_bisect_test/common.sh
-
-cat ${BISECT_GOOD_BUILD}/_LIST
-
diff --git a/binary_search_tool/full_bisect_test/good-objects-permanent/_LIST b/binary_search_tool/full_bisect_test/good-objects-permanent/_LIST
deleted file mode 100644
index 07bc1aa9..00000000
--- a/binary_search_tool/full_bisect_test/good-objects-permanent/_LIST
+++ /dev/null
@@ -1,7 +0,0 @@
-build.o
-inorder_norecurse.o
-inorder.o
-main.o
-preorder_norecurse.o
-preorder.o
-stack.o
diff --git a/binary_search_tool/full_bisect_test/good-output.txt b/binary_search_tool/full_bisect_test/good-output.txt
deleted file mode 100644
index 4db15eb2..00000000
--- a/binary_search_tool/full_bisect_test/good-output.txt
+++ /dev/null
@@ -1,11 +0,0 @@
-pre-order traversal, with recursion: 
-35 28 20 25 23 26 30 60 70 65 64 68 
-
-pre-order traversal, without recursion: 
-35 28 20 25 23 26 30 60 70 65 64 68 
-
-in-order traversal, with recursion: 
-20 23 25 26 28 30 35 60 64 65 68 70 
-
-in-order traversal, without recursion: 
-20 23 25 26 28 30 35 60 64 65 68 70 
diff --git a/binary_search_tool/full_bisect_test/inorder.c b/binary_search_tool/full_bisect_test/inorder.c
deleted file mode 100644
index ad093f3c..00000000
--- a/binary_search_tool/full_bisect_test/inorder.c
+++ /dev/null
@@ -1,22 +0,0 @@
-#include <stdio.h>
-#include <stdlib.h>
-#include "bin-trees.h"
-
-static void
-real_inorder (tree_ptr root)
-{
-  if (root == NULL)
-    return;
-
-  real_inorder (root->left);
-  printf ("%d ", root->data);
-  real_inorder (root->right);
-}
-
-void
-in_order_traverse (tree_ptr root)
-{
-  printf ("in-order traversal, with recursion: \n");
-  real_inorder (root);
-  printf ("\n");
-}
diff --git a/binary_search_tool/full_bisect_test/inorder_norecurse.c.bad b/binary_search_tool/full_bisect_test/inorder_norecurse.c.bad
deleted file mode 100644
index 27f0bb16..00000000
--- a/binary_search_tool/full_bisect_test/inorder_norecurse.c.bad
+++ /dev/null
@@ -1,42 +0,0 @@
-#include <stdlib.h>
-#include <stdio.h>
-#include "bin-trees.h"
-
-static void
-real_in_order_traverse_no_recurse (tree_ptr root)
-{
-  struct stack_struct *stack = NULL;
-  tree_ptr current= root;
-  int going_left = 1;   /* boolean variable */
-  while (current != NULL)
-  {
-    if ((current->left != NULL) && going_left)
-    {
-      push (&stack, current);
-      current = current->left;
-    }
-
-    printf ("%d ", current->data);
-    if (current->right)
-    {
-      current = current->right;
-      going_left = 1;
-    }
-    else if (stack != NULL)
-    {
-      current = pop(&stack);
-      going_left = 0;
-    }
-    else
-      current = NULL;
-  }
-}
-
-void
-in_order_traverse_no_recurse (tree_ptr root)
-{
-  printf ("in-order traversal, without recursion: \n");
-  real_in_order_traverse_no_recurse (root);
-  printf ("\n");
-  return;
-}
diff --git a/binary_search_tool/full_bisect_test/inorder_norecurse.c.good b/binary_search_tool/full_bisect_test/inorder_norecurse.c.good
deleted file mode 100644
index a03f4818..00000000
--- a/binary_search_tool/full_bisect_test/inorder_norecurse.c.good
+++ /dev/null
@@ -1,42 +0,0 @@
-#include <stdlib.h>
-#include <stdio.h>
-#include "bin-trees.h"
-
-static void
-real_in_order_traverse_no_recurse (tree_ptr root)
-{
-  struct stack_struct *stack = NULL;
-  tree_ptr current= root;
-  int going_left = 1;   /* boolean variable */
-  while (current != NULL)
-  {
-    while ((current->left != NULL) && going_left)
-    {
-      push (&stack, current);
-      current = current->left;
-    }
-
-    printf ("%d ", current->data);
-    if (current->right)
-    {
-      current = current->right;
-      going_left = 1;
-    }
-    else if (stack != NULL)
-    {
-      current = pop(&stack);
-      going_left = 0;
-    }
-    else
-      current = NULL;
-  }
-}
-
-void
-in_order_traverse_no_recurse (tree_ptr root)
-{
-  printf ("in-order traversal, without recursion: \n");
-  real_in_order_traverse_no_recurse (root);
-  printf ("\n");
-  return;
-}
diff --git a/binary_search_tool/full_bisect_test/interactive_test.sh b/binary_search_tool/full_bisect_test/interactive_test.sh
deleted file mode 100755
index 064e4ae5..00000000
--- a/binary_search_tool/full_bisect_test/interactive_test.sh
+++ /dev/null
@@ -1,56 +0,0 @@
-#!/bin/bash -u
-#
-# This script is one of the required scripts that get passed to
-# binary_search_state.py.  It's job is to test the executable that
-# was generated by mixing/matching good & bad object files, and determine
-# whether the resulting binary is good or bad.
-#
-# In this particular case, the generated binary is 'bin-trees'.  This
-# script runs the binary, captures its output, and compares the output
-# to a file containg the correct (good) output, and three files containing
-# what the bad output might look like, depending on if one of the two
-# possile bad .o files was used, or if both bad .o files were used.
-#
-# If the output matches the known good output, this returns 0.
-# If the output matches any known bad output, this returns 1.
-# If the output does not match the good or bad outputs, this returns 125.
-#
-
-source full_bisect_test/common.sh
-
-full_bisect_test/bin-trees > full_bisect_test/temp_output.txt
-
-diff full_bisect_test/temp_output.txt full_bisect_test/good-output.txt &> /dev/null
-retval=$?
-
-if [[ ${retval} -eq 0 ]]; then
-  rm -f full_bisect_test/temp_output.txt
-  exit 0
-fi
-
-diff full_bisect_test/temp_output.txt full_bisect_test/bad-output-1.txt &> /dev/null
-retval=$?
-
-if [[ ${retval} -eq 0 ]]; then
-  rm -f full_bisect_test/temp_output.txt
-  exit 1
-else
-  diff full_bisect_test/temp_output.txt full_bisect_test/bad-output-2.txt &> /dev/null
-  retval=$?
-  if [[ ${retval} -eq 0 ]]; then
-    rm -f full_bisect_test/temp_output.txt
-    exit 1
-  else
-    diff full_bisect_test/temp_output.txt full_bisect_test/bad-output-3.txt &> /dev/null
-    retval=$?
-    if [[ ${retval} -eq 0 ]]; then
-      rm -f full_bisect_test/temp_output.txt
-      exit 1
-    fi
-  fi
-fi
-
-rm -f full_bisect_test/temp_output.txt
-exit 125
-
-
diff --git a/binary_search_tool/full_bisect_test/main-bisect-test.sh b/binary_search_tool/full_bisect_test/main-bisect-test.sh
deleted file mode 100755
index af01c19c..00000000
--- a/binary_search_tool/full_bisect_test/main-bisect-test.sh
+++ /dev/null
@@ -1,104 +0,0 @@
-#!/bin/bash
-#
-#  This script is the heart of the bisection test.  It assumes the good-objects
-#  and bad-objects directories have been created and populated.  It runs three
-#  bisection tests:
-#   Test 1.  use --file_args, and no pruning, which passes the object file list
-#            in a file, and stops as soon as it finds the first bad file.
-#   Test 2.  do not use --file_args, and no pruning.  The object files are passed
-#            directly on the command line; stop as soon as it finds the first
-#            bad file.
-#   Test 3.  use --file_args and --prune.  Pass the object file list in a file
-#            and run until it finds ALL the bad files (there are two of them).
-#
-
-SAVE_DIR=`pwd`
-
-DIR=full_bisect_test
-
-# Make sure you are running this script from the parent directory.
-if [[ ! -f "${DIR}/setup.sh" ]] ; then
-  echo "Cannot find ${DIR}/setup.sh.  You are running this from the wrong directory."
-  echo "You need to run this from toolchain-utils/binary_search_tool ."
-  exit 1
-fi
-
-# Run Test 1.
-${DIR}/setup.sh
-
-./binary_search_state.py --get_initial_items="${DIR}/get_initial_items.sh" \
-  --switch_to_good="${DIR}/switch_to_good.sh" \
-  --switch_to_bad="${DIR}/switch_to_bad.sh" \
-  --test_setup_script="${DIR}/test_setup.sh" \
-  --test_script="${DIR}/interactive_test.sh" \
-  --file_args &> /tmp/full_bisect_test.log
-
-${DIR}/cleanup.sh
-
-grep "Search complete. First bad version: " /tmp/full_bisect_test.log &> /dev/null
-test_status=$?
-
-if [[ ${test_status} -ne 0 ]] ; then
-  echo "Test 1 FAILED. See /tmp/full_bisect_test.log for details."
-  exit 1
-else
-  echo "Test 1 passed."
-fi
-
-cd ${SAVE_DIR}
-
-# Run Test 2.
-${DIR}/setup.sh
-
-./binary_search_state.py --get_initial_items="${DIR}/get_initial_items.sh" \
-  --switch_to_good="${DIR}/switch_to_good.sh" \
-  --switch_to_bad="${DIR}/switch_to_bad.sh" \
-  --test_setup_script="${DIR}/test_setup.sh" \
-  --test_script="${DIR}/interactive_test.sh" \
-  &> /tmp/full_bisect_test.log
-
-${DIR}/cleanup.sh
-
-grep "Search complete. First bad version: " /tmp/full_bisect_test.log &> /dev/null
-test_status=$?
-
-if [[ ${test_status} -ne 0 ]] ; then
-  echo "Test 2 FAILED. See /tmp/full_bisect_test.log for details."
-  exit 1
-else
-  echo "Test 2 passed."
-fi
-
-cd ${SAVE_DIR}
-
-# Run Test 3.
-${DIR}/setup.sh
-
-./binary_search_state.py --get_initial_items="${DIR}/get_initial_items.sh" \
-  --switch_to_good="${DIR}/switch_to_good.sh" \
-  --switch_to_bad="${DIR}/switch_to_bad.sh" \
-  --test_setup_script="${DIR}/test_setup.sh" \
-  --test_script="${DIR}/interactive_test.sh" \
-  --file_args --prune &> /tmp/full_bisect_test.log
-
-${DIR}/cleanup.sh
-
-grep "Bad items are: " /tmp/full_bisect_test.log | grep inorder_norecurse.o &> /dev/null
-test_status_1=$?
-
-grep "Bad items are: " /tmp/full_bisect_test.log | grep preorder_norecurse.o &> /dev/null
-test_status_2=$?
-
-if [[ ${test_status_1} -ne 0 ]] ; then
-  echo "Test 3 FAILED. See /tmp/full_bisect_test.log for details."
-  exit 1
-elif [[ ${test_status_2} -ne 0 ]] ; then
-  echo "Test 3 FAILED. See /tmp/full_bisect_test.log for details."
-  exit 1
-else
-  echo "Test 3 passed."
-fi
-
-# All tests passed!
-exit 0
-
diff --git a/binary_search_tool/full_bisect_test/main.c b/binary_search_tool/full_bisect_test/main.c
deleted file mode 100644
index 55abc44b..00000000
--- a/binary_search_tool/full_bisect_test/main.c
+++ /dev/null
@@ -1,30 +0,0 @@
-#include <stdlib.h>
-#include <stdio.h>
-#include "bin-trees.h"
-
-int integers[] = {35, 28, 20, 30, 25, 23, 26, 60, 70, 65, 64, 68 };
-
-char pre_order[] = { '/', '-', '+', '*', 'a', '^', 'x', '2', '&', 'b', 'y',
-                     'c', '3' };
-char in_order[]  = { 'a', '*', 'x', '^', '2', '+', 'b', '&', 'y', '-', 'c',
-                     '/', '3' };
-
-int
-main (int argc, char ** argv)
-{
-  int intlist_size = 12;
-  int i;
-  tree_ptr root = NULL;
-  for (i = 0; i < intlist_size; ++i)
-    {
-      search_tree_insert (&root, integers[i]);
-    }
-  pre_order_traverse (root);
-  printf ("\n");
-  pre_order_traverse_no_recurse (root);
-  printf ("\n");
-  in_order_traverse (root);
-  printf ("\n");
-  in_order_traverse_no_recurse (root);
-  return 0;
-}
diff --git a/binary_search_tool/full_bisect_test/make_sources_bad.sh b/binary_search_tool/full_bisect_test/make_sources_bad.sh
deleted file mode 100755
index 507e8ca8..00000000
--- a/binary_search_tool/full_bisect_test/make_sources_bad.sh
+++ /dev/null
@@ -1,15 +0,0 @@
-#!/bin/bash -u
-#
-#  There are two versions (good & bad) of inorder_norecurse.c and
-#  preorder_norecurse.c.  This script makes sure the bad versions
-#  are copied into the .c files that will be built and copied into
-#  the bad-objects directory, for the bisection test. It is called
-#  from run-test-nowrapper.sh.
-#
-
-pushd full_bisect_test
-
-cp inorder_norecurse.c.bad inorder_norecurse.c
-cp preorder_norecurse.c.bad preorder_norecurse.c
-
-popd
diff --git a/binary_search_tool/full_bisect_test/make_sources_good.sh b/binary_search_tool/full_bisect_test/make_sources_good.sh
deleted file mode 100755
index 611e9442..00000000
--- a/binary_search_tool/full_bisect_test/make_sources_good.sh
+++ /dev/null
@@ -1,15 +0,0 @@
-#!/bin/bash -u
-#
-#  There are two versions (good & bad) of inorder_norecurse.c and
-#  preorder_norecurse.c.  This script makes sure the good versions
-#  are copied into the .c files that will be built and copied into
-#  the good-objects directory, for the bisection test.  It is called
-#  from run-test-nowrapper.sh.
-#
-
-pushd full_bisect_test
-
-cp inorder_norecurse.c.good inorder_norecurse.c
-cp preorder_norecurse.c.good preorder_norecurse.c
-
-popd
diff --git a/binary_search_tool/full_bisect_test/preorder.c b/binary_search_tool/full_bisect_test/preorder.c
deleted file mode 100644
index 11fe93a3..00000000
--- a/binary_search_tool/full_bisect_test/preorder.c
+++ /dev/null
@@ -1,23 +0,0 @@
-#include <stdio.h>
-#include <stdlib.h>
-#include "bin-trees.h"
-
-static void
-real_preorder (tree_ptr root)
-{
-  if (root == NULL)
-    return;
-
-  printf ("%d ", root->data);
-  real_preorder (root->left);
-  real_preorder (root->right);
-}
-
-
-void
-pre_order_traverse (tree_ptr root)
-{
-  printf ("pre-order traversal, with recursion: \n");
-  real_preorder (root) ;
-  printf ("\n");
-}
diff --git a/binary_search_tool/full_bisect_test/preorder_norecurse.c.bad b/binary_search_tool/full_bisect_test/preorder_norecurse.c.bad
deleted file mode 100644
index a8b4b487..00000000
--- a/binary_search_tool/full_bisect_test/preorder_norecurse.c.bad
+++ /dev/null
@@ -1,29 +0,0 @@
-#include <stdlib.h>
-#include <stdio.h>
-#include "bin-trees.h"
-
-static void
-real_pre_order_traverse_no_recurse (tree_ptr root)
-{
-  struct stack_struct *stack = NULL;
-
-  if (root != NULL)
-    push (&stack, root);
-
-  while (stack != NULL)
-  {
-    tree_ptr current = pop (&stack);
-    printf ("%d ", current->data);
-    if (current->right != NULL)
-      push (&stack, current->right);
-  }
-  return;
-}
-
-void
-pre_order_traverse_no_recurse (tree_ptr root)
-{
-  printf ("pre-order traversal, without recursion: \n");
-  real_pre_order_traverse_no_recurse (root);
-  printf ("\n");
-}
diff --git a/binary_search_tool/full_bisect_test/preorder_norecurse.c.good b/binary_search_tool/full_bisect_test/preorder_norecurse.c.good
deleted file mode 100644
index 98f40913..00000000
--- a/binary_search_tool/full_bisect_test/preorder_norecurse.c.good
+++ /dev/null
@@ -1,31 +0,0 @@
-#include <stdlib.h>
-#include <stdio.h>
-#include "bin-trees.h"
-
-static void
-real_pre_order_traverse_no_recurse (tree_ptr root)
-{
-  struct stack_struct *stack = NULL;
-
-  if (root != NULL)
-    push (&stack, root);
-
-  while (stack != NULL)
-  {
-    tree_ptr current = pop (&stack);
-    printf ("%d ", current->data);
-    if (current->right != NULL)
-      push (&stack, current->right);
-    if (current->left != NULL)
-      push (&stack, current->left);
-  }
-  return;
-}
-
-void
-pre_order_traverse_no_recurse (tree_ptr root)
-{
-  printf ("pre-order traversal, without recursion: \n");
-  real_pre_order_traverse_no_recurse (root);
-  printf ("\n");
-}
diff --git a/binary_search_tool/full_bisect_test/run-test-nowrapper.sh b/binary_search_tool/full_bisect_test/run-test-nowrapper.sh
deleted file mode 100755
index afc4a446..00000000
--- a/binary_search_tool/full_bisect_test/run-test-nowrapper.sh
+++ /dev/null
@@ -1,68 +0,0 @@
-#!/bin/bash
-#
-# This script is one of the two main driver scripts for testing the bisector.
-# It should be used to test the bisection tool, if you do NOT want to test
-# the compiler wrapper (e.g. don't bother with POPULATE_GOOD & POPULATE_BAD
-# stages).
-#
-# It makes sure the good & bad object directories exist (soft links); checks
-# to see if it needs to compile the good & bad sources & populate the
-# directories; does so if needed.
-#
-# Then it calls main-bisect-test, which runs the actual bisection tests.  This
-# script assumes it is being run from the parent directory.
-#
-# NOTE: Your PYTHONPATH environment variable needs to include both the
-# toolchain-utils directory and the
-# toolchain-utils/binary_search_tool directory for these testers to work.
-#
-
-SAVE_DIR=`pwd`
-
-DIR=full_bisect_test
-
-if [[ ! -d "${DIR}" ]] ; then
-  echo "Cannot find ${DIR}; you are running this script from the wrong place."
-  echo "You need to run this from toolchain-utils/binary_search_tool ."
-  exit 1
-fi
-
-# Set up object file soft links
-cd ${DIR}
-
-rm -f good-objects
-rm -f bad-objects
-
-ln -s good-objects-permanent good-objects
-ln -s bad-objects-permanent bad-objects
-
-if [[ ! -d work ]] ; then
-  mkdir work
-fi
-
-# Check to see if the object files need to be built.
-if [[ ! -f good-objects-permanent/build.o ]] ; then
-  # 'make clean'
-  rm -f work/*.o
-  # skip populate stages in bisect wrapper
-  unset BISECT_STAGE
-  # Set up the 'good' source files.
-  cd ..
-  ${DIR}/make_sources_good.sh
-  cd ${DIR}
-  # Build the 'good' .o files & copy to appropriate directory.
-  ./build.sh
-  mv work/*.o good-objects-permanent/.
-  # Set up the 'bad' source files.
-  cd ..
-  ${DIR}/make_sources_bad.sh
-  cd ${DIR}
-  # Build the 'bad' .o files & copy to appropriate directory.
-  ./build.sh
-  mv work/*.o bad-objects-permanent/.
-fi
-
-# Now we're ready for the main test.
-
-cd ${SAVE_DIR}
-${DIR}/main-bisect-test.sh
diff --git a/binary_search_tool/full_bisect_test/setup.sh b/binary_search_tool/full_bisect_test/setup.sh
deleted file mode 100755
index 1214de92..00000000
--- a/binary_search_tool/full_bisect_test/setup.sh
+++ /dev/null
@@ -1,36 +0,0 @@
-#!/bin/bash
-#
-# This script creates common.sh, which will be sourced by all the other
-# scripts, to set up the necessary environment variables for the bisection
-# to work properly.  It is called from main-bisect-test.sh.
-#
-
-DIR=`pwd`/"full_bisect_test"
-
-GOOD_BUILD=${DIR}/good-objects
-BAD_BUILD=${DIR}/bad-objects
-
-mkdir -p ${DIR}/work
-
-WORK_BUILD=${DIR}/work
-
-rm -f ${WORK_BUILD}/*
-
-COMMON_FILE="${DIR}/common.sh"
-
-cat <<-EOF > ${COMMON_FILE}
-
-BISECT_GOOD_BUILD=${GOOD_BUILD}
-BISECT_BAD_BUILD=${BAD_BUILD}
-BISECT_WORK_BUILD=${WORK_BUILD}
-
-BISECT_GOOD_SET=${GOOD_BUILD}/_LIST
-BISECT_BAD_BAD=${BAD_BUILD}/_LIST
-
-BISECT_STAGE="TRIAGE"
-
-EOF
-
-chmod 755 ${COMMON_FILE}
-
-exit 0
diff --git a/binary_search_tool/full_bisect_test/stack.c b/binary_search_tool/full_bisect_test/stack.c
deleted file mode 100644
index f8d0568f..00000000
--- a/binary_search_tool/full_bisect_test/stack.c
+++ /dev/null
@@ -1,25 +0,0 @@
-#include <stdlib.h>
-#include <stdio.h>
-#include "bin-trees.h"
-
-tree_ptr
-pop (struct stack_struct **stack)
-{
-  if (*stack == NULL)
-    return NULL;
-  else
-    {
-      tree_ptr value = (*stack)->data;
-      (*stack) = (*stack)->next;
-      return value;
-    }
-}
-
-void
-push (struct stack_struct **stack, tree_ptr value)
-{
-  struct stack_struct *new_node = (struct stack_struct *) malloc (sizeof (struct stack_struct *));
-  new_node->data = value;
-  new_node->next = *stack;
-  *stack = new_node;
-}
diff --git a/binary_search_tool/full_bisect_test/switch_to_bad.sh b/binary_search_tool/full_bisect_test/switch_to_bad.sh
deleted file mode 100755
index d88a4aa2..00000000
--- a/binary_search_tool/full_bisect_test/switch_to_bad.sh
+++ /dev/null
@@ -1,54 +0,0 @@
-#!/bin/bash -u
-#
-# This is one of the scripts that is passed to binary_search_state.py to do
-# the bisection.  This one takes a list of object files (either a real list or
-# a file containing the list) and copies the files from the bad objects
-# directory to the working directory.
-#
-
-source full_bisect_test/common.sh
-
-pushd ${BISECT_WORK_BUILD}
-chmod 644 *
-
-OBJ_LIST_FILES=$1
-FILE_ARGS=0
-
-if [[ -f ${OBJ_LIST_FILES} ]] ; then
-  file ${OBJ_LIST_FILES} &> ${BISECT_WORK_BUILD}/file_type.tmp
-  grep "ASCII text" ${BISECT_WORK_BUILD}/file_type.tmp
-  result=$?
-  if [[ ${result} -eq 0 ]] ; then
-    FILE_ARGS=1
-  fi
-  rm ${BISECT_WORK_BUILD}/file_type.tmp
-fi
-
-overall_status=0
-
-if [[ ${FILE_ARGS} -eq 1 ]] ; then
-  while read obj || [[ -n "${obj}" ]];
-  do
-    cp ${BISECT_BAD_BUILD}/${obj} ${BISECT_WORK_BUILD}
-    status=$?
-    if [[ ${status} -ne 0 ]] ; then
-      echo "Failed to copy ${obj} to work build tree."
-      overall_status=2
-    fi
-  done < ${OBJ_LIST_FILES}
-else
-
-  for o in "$@"
-  do
-    cp ${BISECT_BAD_BUILD}/${o} ${BISECT_WORK_BUILD}
-    status=$?
-    if [[ ${status} -ne 0 ]] ; then
-      echo "Failed to copy ${o} to work build tree."
-      overall_status=2
-    fi
-  done
-fi
-
-popd
-
-exit ${overall_status}
diff --git a/binary_search_tool/full_bisect_test/switch_to_good.sh b/binary_search_tool/full_bisect_test/switch_to_good.sh
deleted file mode 100755
index 9d8c29bc..00000000
--- a/binary_search_tool/full_bisect_test/switch_to_good.sh
+++ /dev/null
@@ -1,57 +0,0 @@
-#!/bin/bash -u
-#
-# This is one of the scripts that is passed to binary_search_state.py to do
-# the bisection.  This one takes a list of object files (either a real list or
-# a file containing the list) and copies the files from the good objects
-# directory to the working directory.
-#
-
-
-source full_bisect_test/common.sh
-
-pushd ${BISECT_WORK_BUILD}
-chmod 644 *
-
-OBJ_LIST_FILES=$1
-FILE_ARGS=0
-
-if [[ -f ${OBJ_LIST_FILES} ]] ; then
-  file ${OBJ_LIST_FILES} &> ${BISECT_WORK_BUILD}/file_type.tmp
-  grep "ASCII text" ${BISECT_WORK_BUILD}/file_type.tmp
-  result=$?
-  if [[ ${result} -eq 0 ]] ; then
-    FILE_ARGS=1
-  fi
-  rm ${BISECT_WORK_BUILD}/file_type.tmp
-fi
-
-overall_status=0
-
-if [[ ${FILE_ARGS} -eq 1 ]] ; then
-  while read obj || [[ -n "${obj}" ]];
-  do
-    echo "Copying {BISECT_GOOD_BUILD}/${obj} to ${BISECT_WORK_BUILD}"
-    cp ${BISECT_GOOD_BUILD}/${obj} ${BISECT_WORK_BUILD}
-#    cp ${obj} ${BISECT_WORK_BUILD}/.
-    status=$?
-    if [[ ${status} -ne 0 ]] ; then
-      echo "Failed to copy ${obj} to work build tree."
-      overall_status=2
-    fi
-  done < ${OBJ_LIST_FILES}
-else
-
-  for o in "$@"
-  do
-    cp ${BISECT_GOOD_BUILD}/${o} ${BISECT_WORK_BUILD}
-    status=$?
-    if [[ ${status} -ne 0 ]] ; then
-      echo "Failed to copy ${o} to work build tree."
-      overall_status=2
-    fi
-  done
-fi
-
-popd
-
-exit ${overall_status}
diff --git a/binary_search_tool/full_bisect_test/test_setup.sh b/binary_search_tool/full_bisect_test/test_setup.sh
deleted file mode 100755
index bb313831..00000000
--- a/binary_search_tool/full_bisect_test/test_setup.sh
+++ /dev/null
@@ -1,16 +0,0 @@
-#!/bin/bash
-#
-# This is one of the scripts that gets passed to binary_search_state.py.
-# It's supposed to generate the binary to be tested, from the mix of
-# good & bad object files.
-#
-source full_bisect_test/common.sh
-
-WD=`pwd`
-
-cd full_bisect_test
-
-echo "BUILDING IMAGE"
-
-gcc -o bin-trees work/*.o
-
diff --git a/binary_search_tool/ndk/DO_BISECTION.sh b/binary_search_tool/ndk/DO_BISECTION.sh
deleted file mode 100755
index e6eed765..00000000
--- a/binary_search_tool/ndk/DO_BISECTION.sh
+++ /dev/null
@@ -1,92 +0,0 @@
-#!/bin/bash
-#
-# Copyright 2016 Google LLC
-#
-# This is an example script to show users the steps for bisecting an NDK
-# application for Android. Our example is the Teapot app that comes bundled with
-# the NDK as a sample app.
-#
-# Our Teapot app only has 12 or so object files generated per build. Bisection
-# for just 12 object files is overkill, but this bisection process easily scales
-# to thousands of object files (as seen with the Android source).
-#
-# Setup:
-#   1. Install NDK (make sure it is in your PATH)
-#   2. Install compiler_wrapper.py
-#   3. Connect an arm7 device (tested with Nexus 5X)
-#     a. See README for supporting other device archs
-#
-# Tested in bash on Linux.
-
-# Set CWD to where this script lives
-pushd "$(dirname "$0")"
-
-# If Teapot dir already exists remove it.
-if [[ -d Teapot ]]; then
-  rm -rf Teapot
-fi
-
-# Unzip our repository we'll be testing with.
-tar -xzf Teapot.tar.gz
-
-# Apply small setup patch. This patch makes a small change to the build system
-# to make this bisecting example a little easier. It inserts the option to only
-# build for an arm7. See the patch file for details.
-# (This patch file was generated with git, -p1 will remove the a/ and b/)
-patch -p1 -i PATCH1
-
-# We want all of our cached files to be stored in ~/NDK_EXAMPLE_BISECT
-# Remove directory if already exists
-export BISECT_DIR=~/NDK_EXAMPLE_BISECT
-if [[ -d ${BISECT_DIR} ]]; then
-  rm -rf ${BISECT_DIR}
-fi
-
-# We will now take our normal "good compiler" and do a full build of the app. We
-# need to clean before building. This ensures that all objects are generated and
-# can be cached.
-pushd Teapot
-export BISECT_STAGE=POPULATE_GOOD
-./gradlew clean
-./gradlew installArm7Debug
-popd
-
-# Inserting "compiler error". Really this is just a patch that inserts a simple
-# error in the code, but this is used to simulate our compiler error. This patch
-# will simply cause the app to crash as soon as it starts. See the patch file
-# for details.
-# (This patch file was generated with git, -p1 will remove the a/ and b/)
-patch -p1 -i PATCH2
-
-# Now that we have installed our bad compiler (i.e. applied the above patch that
-# acts like a compiler error), we want to enumerate and cache all objects
-# generated by this "bad compiler". So again, we clean the build tree so that
-# all objects are regenerated and can be cached.
-pushd Teapot
-export BISECT_STAGE=POPULATE_BAD
-./gradlew clean
-./gradlew installArm7Debug
-popd
-
-# Now ~/NDK_EXAMPLE_BISECT holds the caches for both good and bad compiler
-# outputs. We will now use these to bisect our problem. We should find that
-# TeapotRenderer.o is the bad file (because this is where PATCH2 inserted the
-# "compiler error").
-
-# Tell the compiler wrapper to not cache outputs, and instead begin bisecting.
-export BISECT_STAGE=TRIAGE
-
-# Run the actual bisection tool. This will automatically narrow down which
-# object file has the error. The test_setup.sh script will rebuild our app
-# with gradle, and boot_test.sh will ping the device to see if the app crashed
-# or not.
-cd ..
-./binary_search_state.py \
-  --get_initial_items=ndk/get_initial_items.sh \
-  --switch_to_good=ndk/switch_to_good.sh \
-  --switch_to_bad=ndk/switch_to_bad.sh \
-  --test_setup_script=ndk/test_setup.sh \
-  --test_script=ndk/boot_test.sh \
-  --file_args
-
-popd
diff --git a/binary_search_tool/ndk/PATCH1 b/binary_search_tool/ndk/PATCH1
deleted file mode 100644
index eddf61cf..00000000
--- a/binary_search_tool/ndk/PATCH1
+++ /dev/null
@@ -1,40 +0,0 @@
-From 93395bf49f856abac5ab06d4bcaa7cdbf76a77fc Mon Sep 17 00:00:00 2001
-From: Cassidy Burden <cburden@google.com>
-Date: Tue, 9 Aug 2016 09:38:41 -0700
-Subject: [PATCH] FOR BINARY SEARCH TOOL: Add arm7 target
-
-Add arm7 target to build.gradle file. This is so the bisection tool only
-has to triage the object files generated for our specific device.
-Without this target we would have to binary search across object files
-meant for x86 targets (that we can't even test on our ARM device).
----
- Teapot/app/build.gradle | 7 ++++++-
- 1 file changed, 6 insertions(+), 1 deletion(-)
-
-diff --git a/Teapot/app/build.gradle b/Teapot/app/build.gradle
-index 78cf54d..c322114 100644
---- a/Teapot/app/build.gradle
-+++ b/Teapot/app/build.gradle
-@@ -29,7 +29,7 @@ model {
-             cppFlags.addAll(['-I' + "${ndkDir}/sources/android/cpufeatures",
-                              '-I' + file('src/main/jni/ndk_helper')])
-             cppFlags.addAll(['-std=c++11', '-Wall',
--                             '-fno-exceptions', '-fno-rtti'])
-+                             '-fno-exceptions', '-fno-rtti', '-gsplit-dwarf'])
-             ldLibs.addAll(['android', 'log', 'EGL', 'GLESv2','atomic'])
-         }
-         sources {
-@@ -51,5 +51,10 @@ model {
-                 proguardFiles.add(file('proguard-rules.txt'))
-             }
-         }
-+	productFlavors{
-+            create("arm7") {
-+                ndk.abiFilters.add("armeabi-v7a")
-+            }
-+	}
-     }
- }
--- 
-2.8.0.rc3.226.g39d4020
-
diff --git a/binary_search_tool/ndk/PATCH2 b/binary_search_tool/ndk/PATCH2
deleted file mode 100644
index 9fcf45d0..00000000
--- a/binary_search_tool/ndk/PATCH2
+++ /dev/null
@@ -1,34 +0,0 @@
-From 960134fb87a194595f2a0a36290be7961e12b946 Mon Sep 17 00:00:00 2001
-From: Cassidy Burden <cburden@google.com>
-Date: Tue, 9 Aug 2016 09:46:27 -0700
-Subject: [PATCH] FOR BISECTION TOOL: Insert error
-
-Insert error into code that will cause crash. This is the "compiler
-error" that we will be triaging. We will be pretending the compiler
-mistakenly inserted a nullptr where it shouldn't have.
-
-This error causes the app to immediately crash upon starting. This makes
-it very easy to automatically test the app through adb. Not all compiler
-problems will be this easy to test, and may require manual testing from
-you (the user). See android/interactive_test.sh for an example on
-manual testing from the user.
----
- Teapot/app/src/main/jni/TeapotRenderer.cpp | 2 +-
- 1 file changed, 1 insertion(+), 1 deletion(-)
-
-diff --git a/Teapot/app/src/main/jni/TeapotRenderer.cpp b/Teapot/app/src/main/jni/TeapotRenderer.cpp
-index 7cafdb3..75cadbf 100644
---- a/Teapot/app/src/main/jni/TeapotRenderer.cpp
-+++ b/Teapot/app/src/main/jni/TeapotRenderer.cpp
-@@ -58,7 +58,7 @@ void TeapotRenderer::Init() {
-   num_vertices_ = sizeof(teapotPositions) / sizeof(teapotPositions[0]) / 3;
-   int32_t stride = sizeof(TEAPOT_VERTEX);
-   int32_t index = 0;
--  TEAPOT_VERTEX* p = new TEAPOT_VERTEX[num_vertices_];
-+  TEAPOT_VERTEX* p = nullptr; //new TEAPOT_VERTEX[num_vertices_];
-   for (int32_t i = 0; i < num_vertices_; ++i) {
-     p[i].pos[0] = teapotPositions[index];
-     p[i].pos[1] = teapotPositions[index + 1];
--- 
-2.8.0.rc3.226.g39d4020
-
diff --git a/binary_search_tool/ndk/README.md b/binary_search_tool/ndk/README.md
deleted file mode 100644
index e41311a1..00000000
--- a/binary_search_tool/ndk/README.md
+++ /dev/null
@@ -1,89 +0,0 @@
-# NDK Bisection tool
-
-This is an example bisection for an NDK build system. This example specifically
-bisects the sample NDK Teapot app. All steps (setup and otherwise) for bisection
-can be found in `DO_BISECTION.sh`. This shell script is meant to show the
-process required to bisect a compiler problem in an arbitrary NDK app build
-system.
-
-There are three necessary setup steps to run this example:
-
-1.  Install the NDK (known to work with r12b)
-
-    1. See here for NDK: https://developer.android.com/ndk/index.html
-    2. Go here for older NDK downloads: https://github.com/android-ndk/ndk/wiki
-
-1.  Install the compiler wrapper provided with this repo. See
-    compiler_wrapper.py for more details.
-
-    1.  Essentially you must go into the NDK source (or where you build system
-        stores its toolchain), rename your compilers to <compiler>.real, and
-        create a symlink pointing to compiler_wrapper.py named <compiler>
-        (where your compiler used to be).
-
-    2.  If you're using the toolchains that come with the NDK they live at:
-        `<ndk_path>/toolchains/<arch>/prebuilt/<host>/bin`
-        example:
-        `<ndk_path>/toolchains/llvm/prebuilt/linux-x86_64/bin/clang`
-
-1.  Plug in an Arm7 compatible Android device with usb debugging enabled.
-
-    1. This bisection example was tested with a Nexus 5X
-
-    2. It should be relatively simple to change the example to work with other
-       types of devices. Just change the scripts, and change PATCH1 to use a
-       different build flavor (like x86). See below for more details.
-
-This example contains two patches:
-
-`PATCH1` - This is the necessary changes to the build system to make the
-bisection easier. More specifically, it adds an arm7 build flavor to gradle.
-By default, this project will build objects for all possible architectures and
-package them into one big apk. These object files meant for another
-architecture just sit there and don't actually execute. By adding a build
-flavor for arm7, our compiler wrapper won't try to bisect object files meant
-for another device.
-
-`PATCH2` - This patch is what inserts the "compiler error". This is a simple
-nullptr error in one of the source files, but it is meant to mimic bad code
-generation. The result of the error is the app simply crashes immediately
-after starting.
-
-## Using another device architecture
-
-If we want to bisect for an x86-64 device we first need to provide a arch
-specific build flavor in our app/build.gradle file:
-
-```
-create("x86-64") {
-  ndk.abiFilters.add("x86_64")
-}
-```
-
-We want to add this under the same "productFlavors" section that our arm7
-build flavor is in (see PATCH1). Now we should have the "installx86-64Debug"
-task in our build system. We can use this to build and install an x86-64
-version of our app.
-
-Now we want to change our `test_setup.sh` script to run our new gradle task:
-```
-./gradlew installx86-64Debug
-```
-
-Keep in mind, these specific build flavors are not required. If your build
-system makes these device specific builds difficult to implement, the
-bisection tool will function perfectly fine without them. However, the
-downside of not having targetting a single architecture is the bisection will
-take longer (as it will need to search across more object files).
-
-## Additional Documentation
-
-These are internal Google documents, if you are a developer external to
-Google please ask whoever gave you this sample for access or copies to the
-documentation. If you cannot gain access, the various READMEs paired with the
-bisector should help you.
-
-* Ahmad's original presentation: https://goto.google.com/zxdfyi
-* Bisection tool update design doc: https://goto.google.com/zcwei
-* Bisection tool webpage: https://goto.google.com/ruwpyi
-* Compiler wrapper webpage: https://goto.google.com/xossn
diff --git a/binary_search_tool/ndk/Teapot.tar.gz b/binary_search_tool/ndk/Teapot.tar.gz
deleted file mode 100644
index 87faf54b..00000000
Binary files a/binary_search_tool/ndk/Teapot.tar.gz and /dev/null differ
diff --git a/binary_search_tool/ndk/boot_test.sh b/binary_search_tool/ndk/boot_test.sh
deleted file mode 100755
index 0b66ddfa..00000000
--- a/binary_search_tool/ndk/boot_test.sh
+++ /dev/null
@@ -1,27 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2016 Google LLC
-#
-# This script checks the android device to determine if the app is currently
-# running. For our specific test case we will be checking if the Teapot app
-# has crashed.
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on the Android NDK apps. It
-# waits for the test setup script to build and install the app, then checks if
-# app boots or not. It should return '0' if the test succeeds
-# (the image is 'good'); '1' if the test fails (the image is 'bad'); and '125'
-# if it could not determine (does not apply in this case).
-#
-
-echo "Starting Teapot app..."
-adb shell am start -n com.sample.teapot/com.sample.teapot.TeapotNativeActivity
-sleep 3
-
-echo "Checking if Teapot app crashed..."
-adb shell ps | grep com.sample.teapot
-
-retval=$?
-
-
-exit ${retval}
diff --git a/binary_search_tool/ndk/get_initial_items.sh b/binary_search_tool/ndk/get_initial_items.sh
deleted file mode 100755
index 5dd3396d..00000000
--- a/binary_search_tool/ndk/get_initial_items.sh
+++ /dev/null
@@ -1,11 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2016 Google LLC
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on the Android NDK apps.  This script
-# generates the list of object files to be bisected. This list is generated
-# by the compiler wrapper during the POPULATE_GOOD and POPULATE_BAD stages.
-#
-
-cat ${BISECT_DIR}/good/_LIST
diff --git a/binary_search_tool/ndk/switch_to_bad.sh b/binary_search_tool/ndk/switch_to_bad.sh
deleted file mode 120000
index 0172bce5..00000000
--- a/binary_search_tool/ndk/switch_to_bad.sh
+++ /dev/null
@@ -1 +0,0 @@
-switch_to_good.sh
\ No newline at end of file
diff --git a/binary_search_tool/ndk/switch_to_good.sh b/binary_search_tool/ndk/switch_to_good.sh
deleted file mode 100755
index c98de67c..00000000
--- a/binary_search_tool/ndk/switch_to_good.sh
+++ /dev/null
@@ -1,45 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2016 Google LLC
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on Android NDK apps. This script simply
-# deletes all given objects, signaling gradle to execute a recompilation of said
-# object files.
-#
-
-# Input is a file, with newline seperated list of files we will be switching
-OBJ_LIST_FILE=$1
-
-# Check that number of arguments == 1
-if [ $# -ne 1 ] ; then
-  echo "ERROR:"
-  echo "Got multiple inputs to switch script!"
-  echo "Run binary_search_state.py with --file_args"
-  exit 1
-fi
-
-# Remove any file that's being switched. This is because Gradle only recompiles
-# if:
-#   1. The resultant object file doesn't exist
-#   2. The hash of the source file has changed
-#
-# Because we have no reliable way to edit the source file, we instead remove the
-# object file and have the compiler wrapper insert the file from the appropriate
-# cache (good or bad).
-#
-# Not entirely relevant to the Teapot example, but something to consider:
-# This removing strategy has the side effect that all switched items cause the
-# invocation of the compiler wrapper, which can add up and slow the build
-# process. With Android's source tree, Make checks the timestamp of the object
-# file. So we symlink in the appropriate file and touch it to tell Make it needs
-# to be relinked. This avoids having to call the compiler wrapper in the
-# majority of cases.
-#
-# However, a similar construct doesn't seem to exist in Gradle. It may be better
-# to add a build target to Gradle that will always relink all given object
-# files. This way we can avoid calling the compiler wrapper while Triaging and
-# save some time. Not really necessary
-
-cat $OBJ_LIST_FILE | xargs rm
-exit 0
diff --git a/binary_search_tool/ndk/test_setup.sh b/binary_search_tool/ndk/test_setup.sh
deleted file mode 100755
index 8f3ce04e..00000000
--- a/binary_search_tool/ndk/test_setup.sh
+++ /dev/null
@@ -1,27 +0,0 @@
-#!/bin/bash
-#
-# Copyright 2016 Google LLC
-#
-# This is the setup script for generating and installing the ndk app.
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on the Android source tree. It should
-# return '0' if the setup succeeds; and '1' if the setup fails (the image
-# could not build or be flashed).
-#
-
-echo
-echo "INSTALLATION BEGIN"
-echo
-
-# This normally shouldn't be hardcoded, but this is a sample bisection.
-# Also keep in mind that the bisection tool mandates all paths are
-# relative to binary_search_state.py
-cd ndk/Teapot
-
-echo "BUILDING APP"
-
-./gradlew installArm7Debug
-gradle_status=$?
-
-exit ${gradle_status}
diff --git a/binary_search_tool/pass_mapping.py b/binary_search_tool/pass_mapping.py
deleted file mode 100644
index 33c023a9..00000000
--- a/binary_search_tool/pass_mapping.py
+++ /dev/null
@@ -1,26 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Config file for pass level bisection
-
-Provides a mapping from pass info from -opt-bisect result to DebugCounter name.
-"""
-pass_name = {
-    # The list now contains all the passes in LLVM that support DebugCounter at
-    # transformation level.
-    # We will need to keep updating this map after more DebugCounter added to
-    # each pass in LLVM.
-    # For users who make local changes to passes, please add a map from pass
-    # description to newly introduced DebugCounter name for transformation
-    # level bisection purpose.
-    "Hoist/decompose integer division and remainder": "div-rem-pairs-transform",
-    "Early CSE": "early-cse",
-    "Falkor HW Prefetch Fix Late Phase": "falkor-hwpf",
-    "Combine redundant instructions": "instcombine-visit",
-    "Machine Copy Propagation Pass": "machine-cp-fwd",
-    "Global Value Numbering": "newgvn-phi",
-    "PredicateInfo Printer": "predicateinfo-rename",
-    "SI Insert Waitcnts": "si-insert-waitcnts-forceexp",
-}
diff --git a/binary_search_tool/run_bisect.py b/binary_search_tool/run_bisect.py
deleted file mode 100755
index f54e00e1..00000000
--- a/binary_search_tool/run_bisect.py
+++ /dev/null
@@ -1,472 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""The unified package/object bisecting tool."""
-
-
-import abc
-import argparse
-from argparse import RawTextHelpFormatter
-import os
-import shlex
-import sys
-
-from binary_search_tool import binary_search_state
-from binary_search_tool import common
-from cros_utils import command_executer
-from cros_utils import logger
-
-
-class Bisector(object, metaclass=abc.ABCMeta):
-    """The abstract base class for Bisectors."""
-
-    def __init__(self, options, overrides=None):
-        """Constructor for Bisector abstract base class
-
-        Args:
-          options: positional arguments for specific mode (board, remote, etc.)
-          overrides: optional dict of overrides for argument defaults
-        """
-        self.options = options
-        self.overrides = overrides
-        if not overrides:
-            self.overrides = {}
-        self.logger = logger.GetLogger()
-        self.ce = command_executer.GetCommandExecuter()
-
-    def _PrettyPrintArgs(self, args, overrides):
-        """Output arguments in a nice, human readable format
-
-        Will print and log all arguments for the bisecting tool and make note of
-        which arguments have been overridden.
-
-        Example output:
-          ./run_bisect.py package daisy 172.17.211.184 -I "" -t cros_pkg/my_test.sh
-          Performing ChromeOS Package bisection
-          Method Config:
-            board : daisy
-           remote : 172.17.211.184
-
-          Bisection Config: (* = overridden)
-             get_initial_items : cros_pkg/get_initial_items.sh
-                switch_to_good : cros_pkg/switch_to_good.sh
-                 switch_to_bad : cros_pkg/switch_to_bad.sh
-           * test_setup_script :
-           *       test_script : cros_pkg/my_test.sh
-                         prune : True
-                 noincremental : False
-                     file_args : True
-
-        Args:
-          args: The args to be given to binary_search_state.Run. This represents
-                how the bisection tool will run (with overridden arguments already
-                added in).
-          overrides: The dict of overriden arguments provided by the user. This is
-                     provided so the user can be told which arguments were
-                     overriden and with what value.
-        """
-        # Output method config (board, remote, etc.)
-        options = vars(self.options)
-        out = "\nPerforming %s bisection\n" % self.method_name
-        out += "Method Config:\n"
-        max_key_len = max([len(str(x)) for x in options.keys()])
-        for key in sorted(options):
-            val = options[key]
-            key_str = str(key).rjust(max_key_len)
-            val_str = str(val)
-            out += " %s : %s\n" % (key_str, val_str)
-
-        # Output bisection config (scripts, prune, etc.)
-        out += "\nBisection Config: (* = overridden)\n"
-        max_key_len = max([len(str(x)) for x in args.keys()])
-        # Print args in common._ArgsDict order
-        args_order = [x["dest"] for x in common.GetArgsDict().values()]
-        for key in sorted(args, key=args_order.index):
-            val = args[key]
-            key_str = str(key).rjust(max_key_len)
-            val_str = str(val)
-            changed_str = "*" if key in overrides else " "
-
-            out += " %s %s : %s\n" % (changed_str, key_str, val_str)
-
-        out += "\n"
-        self.logger.LogOutput(out)
-
-    def ArgOverride(self, args, overrides, pretty_print=True):
-        """Override arguments based on given overrides and provide nice output
-
-        Args:
-          args: dict of arguments to be passed to binary_search_state.Run (runs
-                dict.update, causing args to be mutated).
-          overrides: dict of arguments to update args with
-          pretty_print: if True print out args/overrides to user in pretty format
-        """
-        args.update(overrides)
-        if pretty_print:
-            self._PrettyPrintArgs(args, overrides)
-
-    @abc.abstractmethod
-    def PreRun(self):
-        pass
-
-    @abc.abstractmethod
-    def Run(self):
-        pass
-
-    @abc.abstractmethod
-    def PostRun(self):
-        pass
-
-
-class BisectPackage(Bisector):
-    """The class for package bisection steps."""
-
-    cros_pkg_setup = "cros_pkg/setup.sh"
-    cros_pkg_cleanup = "cros_pkg/%s_cleanup.sh"
-
-    def __init__(self, options, overrides):
-        super(BisectPackage, self).__init__(options, overrides)
-        self.method_name = "ChromeOS Package"
-        self.default_kwargs = {
-            "get_initial_items": "cros_pkg/get_initial_items.sh",
-            "switch_to_good": "cros_pkg/switch_to_good.sh",
-            "switch_to_bad": "cros_pkg/switch_to_bad.sh",
-            "test_setup_script": "cros_pkg/test_setup.sh",
-            "test_script": "cros_pkg/interactive_test.sh",
-            "noincremental": False,
-            "prune": True,
-            "file_args": True,
-        }
-        self.setup_cmd = " ".join(
-            (self.cros_pkg_setup, self.options.board, self.options.remote)
-        )
-        self.ArgOverride(self.default_kwargs, self.overrides)
-
-    def PreRun(self):
-        ret, _, _ = self.ce.RunCommandWExceptionCleanup(
-            self.setup_cmd, print_to_console=True
-        )
-        if ret:
-            self.logger.LogError(
-                "Package bisector setup failed w/ error %d" % ret
-            )
-            return 1
-        return 0
-
-    def Run(self):
-        return binary_search_state.Run(**self.default_kwargs)
-
-    def PostRun(self):
-        cmd = self.cros_pkg_cleanup % self.options.board
-        ret, _, _ = self.ce.RunCommandWExceptionCleanup(
-            cmd, print_to_console=True
-        )
-        if ret:
-            self.logger.LogError(
-                "Package bisector cleanup failed w/ error %d" % ret
-            )
-            return 1
-
-        self.logger.LogOutput(
-            (
-                "Cleanup successful! To restore the bisection "
-                "environment run the following:\n"
-                "  cd %s; %s"
-            )
-            % (os.getcwd(), self.setup_cmd)
-        )
-        return 0
-
-
-class BisectObject(Bisector):
-    """The class for object bisection steps."""
-
-    sysroot_wrapper_setup = "sysroot_wrapper/setup.sh"
-    sysroot_wrapper_cleanup = "sysroot_wrapper/cleanup.sh"
-
-    def __init__(self, options, overrides):
-        super(BisectObject, self).__init__(options, overrides)
-        self.method_name = "ChromeOS Object"
-        self.default_kwargs = {
-            "get_initial_items": "sysroot_wrapper/get_initial_items.sh",
-            "switch_to_good": "sysroot_wrapper/switch_to_good.sh",
-            "switch_to_bad": "sysroot_wrapper/switch_to_bad.sh",
-            "test_setup_script": "sysroot_wrapper/test_setup.sh",
-            "test_script": "sysroot_wrapper/interactive_test.sh",
-            "noincremental": False,
-            "prune": True,
-            "file_args": True,
-        }
-        self.options = options
-        if options.dir:
-            os.environ["BISECT_DIR"] = options.dir
-        self.options.dir = os.environ.get("BISECT_DIR", "/tmp/sysroot_bisect")
-        self.setup_cmd = " ".join(
-            (
-                self.sysroot_wrapper_setup,
-                self.options.board,
-                self.options.remote,
-                self.options.package,
-                str(self.options.reboot).lower(),
-                shlex.quote(self.options.use_flags),
-            )
-        )
-
-        self.ArgOverride(self.default_kwargs, overrides)
-
-    def PreRun(self):
-        ret, _, _ = self.ce.RunCommandWExceptionCleanup(
-            self.setup_cmd, print_to_console=True
-        )
-        if ret:
-            self.logger.LogError(
-                "Object bisector setup failed w/ error %d" % ret
-            )
-            return 1
-
-        os.environ["BISECT_STAGE"] = "TRIAGE"
-        return 0
-
-    def Run(self):
-        return binary_search_state.Run(**self.default_kwargs)
-
-    def PostRun(self):
-        cmd = self.sysroot_wrapper_cleanup
-        ret, _, _ = self.ce.RunCommandWExceptionCleanup(
-            cmd, print_to_console=True
-        )
-        if ret:
-            self.logger.LogError(
-                "Object bisector cleanup failed w/ error %d" % ret
-            )
-            return 1
-        self.logger.LogOutput(
-            (
-                "Cleanup successful! To restore the bisection "
-                "environment run the following:\n"
-                "  cd %s; %s"
-            )
-            % (os.getcwd(), self.setup_cmd)
-        )
-        return 0
-
-
-class BisectAndroid(Bisector):
-    """The class for Android bisection steps."""
-
-    android_setup = "android/setup.sh"
-    android_cleanup = "android/cleanup.sh"
-    default_dir = os.path.expanduser("~/ANDROID_BISECT")
-
-    def __init__(self, options, overrides):
-        super(BisectAndroid, self).__init__(options, overrides)
-        self.method_name = "Android"
-        self.default_kwargs = {
-            "get_initial_items": "android/get_initial_items.sh",
-            "switch_to_good": "android/switch_to_good.sh",
-            "switch_to_bad": "android/switch_to_bad.sh",
-            "test_setup_script": "android/test_setup.sh",
-            "test_script": "android/interactive_test.sh",
-            "prune": True,
-            "file_args": True,
-            "noincremental": False,
-        }
-        self.options = options
-        if options.dir:
-            os.environ["BISECT_DIR"] = options.dir
-        self.options.dir = os.environ.get("BISECT_DIR", self.default_dir)
-
-        num_jobs = "NUM_JOBS='%s'" % self.options.num_jobs
-        device_id = ""
-        if self.options.device_id:
-            device_id = "ANDROID_SERIAL='%s'" % self.options.device_id
-
-        self.setup_cmd = " ".join(
-            (num_jobs, device_id, self.android_setup, self.options.android_src)
-        )
-
-        self.ArgOverride(self.default_kwargs, overrides)
-
-    def PreRun(self):
-        ret, _, _ = self.ce.RunCommandWExceptionCleanup(
-            self.setup_cmd, print_to_console=True
-        )
-        if ret:
-            self.logger.LogError(
-                "Android bisector setup failed w/ error %d" % ret
-            )
-            return 1
-
-        os.environ["BISECT_STAGE"] = "TRIAGE"
-        return 0
-
-    def Run(self):
-        return binary_search_state.Run(**self.default_kwargs)
-
-    def PostRun(self):
-        cmd = self.android_cleanup
-        ret, _, _ = self.ce.RunCommandWExceptionCleanup(
-            cmd, print_to_console=True
-        )
-        if ret:
-            self.logger.LogError(
-                "Android bisector cleanup failed w/ error %d" % ret
-            )
-            return 1
-        self.logger.LogOutput(
-            (
-                "Cleanup successful! To restore the bisection "
-                "environment run the following:\n"
-                "  cd %s; %s"
-            )
-            % (os.getcwd(), self.setup_cmd)
-        )
-        return 0
-
-
-def Run(bisector):
-    log = logger.GetLogger()
-
-    log.LogOutput("Setting up Bisection tool")
-    ret = bisector.PreRun()
-    if ret:
-        return ret
-
-    log.LogOutput("Running Bisection tool")
-    ret = bisector.Run()
-    if ret:
-        return ret
-
-    log.LogOutput("Cleaning up Bisection tool")
-    ret = bisector.PostRun()
-    if ret:
-        return ret
-
-    return 0
-
-
-_HELP_EPILOG = """
-Run ./run_bisect.py {method} --help for individual method help/args
-
-------------------
-
-See README.bisect for examples on argument overriding
-
-See below for full override argument reference:
-"""
-
-
-def Main(argv):
-    override_parser = argparse.ArgumentParser(
-        add_help=False,
-        argument_default=argparse.SUPPRESS,
-        usage="run_bisect.py {mode} [options]",
-    )
-    common.BuildArgParser(override_parser, override=True)
-
-    epilog = _HELP_EPILOG + override_parser.format_help()
-    parser = argparse.ArgumentParser(
-        epilog=epilog, formatter_class=RawTextHelpFormatter
-    )
-    subparsers = parser.add_subparsers(
-        title="Bisect mode",
-        description=(
-            "Which bisection method to "
-            "use. Each method has "
-            "specific setup and "
-            "arguments. Please consult "
-            "the README for more "
-            "information."
-        ),
-    )
-
-    parser_package = subparsers.add_parser("package")
-    parser_package.add_argument("board", help="Board to target")
-    parser_package.add_argument("remote", help="Remote machine to test on")
-    parser_package.set_defaults(handler=BisectPackage)
-
-    parser_object = subparsers.add_parser("object")
-    parser_object.add_argument("board", help="Board to target")
-    parser_object.add_argument("remote", help="Remote machine to test on")
-    parser_object.add_argument("package", help="Package to emerge and test")
-    parser_object.add_argument(
-        "--use_flags",
-        required=False,
-        default="",
-        help="Use flags passed to emerge",
-    )
-    parser_object.add_argument(
-        "--noreboot",
-        action="store_false",
-        dest="reboot",
-        help="Do not reboot after updating the package (default: False)",
-    )
-    parser_object.add_argument(
-        "--dir",
-        help=(
-            "Bisection directory to use, sets "
-            "$BISECT_DIR if provided. Defaults to "
-            "current value of $BISECT_DIR (or "
-            "/tmp/sysroot_bisect if $BISECT_DIR is "
-            "empty)."
-        ),
-    )
-    parser_object.set_defaults(handler=BisectObject)
-
-    parser_android = subparsers.add_parser("android")
-    parser_android.add_argument(
-        "android_src", help="Path to android source tree"
-    )
-    parser_android.add_argument(
-        "--dir",
-        help=(
-            "Bisection directory to use, sets "
-            "$BISECT_DIR if provided. Defaults to "
-            "current value of $BISECT_DIR (or "
-            "~/ANDROID_BISECT/ if $BISECT_DIR is "
-            "empty)."
-        ),
-    )
-    parser_android.add_argument(
-        "-j",
-        "--num_jobs",
-        type=int,
-        default=1,
-        help=(
-            "Number of jobs that make and various "
-            "scripts for bisector can spawn. Setting "
-            "this value too high can freeze up your "
-            "machine!"
-        ),
-    )
-    parser_android.add_argument(
-        "--device_id",
-        default="",
-        help=(
-            "Device id for device used for testing. "
-            "Use this if you have multiple Android "
-            "devices plugged into your machine."
-        ),
-    )
-    parser_android.set_defaults(handler=BisectAndroid)
-
-    options, remaining = parser.parse_known_args(argv)
-    if remaining:
-        overrides = override_parser.parse_args(remaining)
-        overrides = vars(overrides)
-    else:
-        overrides = {}
-
-    subcmd = options.handler
-    del options.handler
-
-    bisector = subcmd(options, overrides)
-    return Run(bisector)
-
-
-if __name__ == "__main__":
-    os.chdir(os.path.dirname(__file__))
-    sys.exit(Main(sys.argv[1:]))
diff --git a/binary_search_tool/run_bisect_tests.py b/binary_search_tool/run_bisect_tests.py
deleted file mode 100755
index ca7077d3..00000000
--- a/binary_search_tool/run_bisect_tests.py
+++ /dev/null
@@ -1,198 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Run full bisection test."""
-
-
-import argparse
-import os
-import sys
-
-from cros_utils import command_executer
-
-
-TEST_DIR = "full_bisect_test"
-DEFAULT_BISECT_DIR = "/tmp/sysroot_bisect"
-
-
-def populate_good_files(top_dir, ce, bisect_dir=DEFAULT_BISECT_DIR):
-    # 'make clean'
-    work_dir = os.path.join(top_dir, TEST_DIR, "work")
-    cmd = "rm -f %s/*.o" % work_dir
-    status = ce.RunCommand(cmd)
-    if status != 0:
-        print("Error trying to clean out work directory: %s" % cmd)
-        return status
-
-    # set up the 'good' source files
-    script = os.path.join(top_dir, TEST_DIR, "make_sources_good.sh")
-    status = ce.RunCommand(script)
-    if status != 0:
-        print('Error setting up "good" source files: %s' % script)
-        return status
-
-    export_bisect = "export BISECT_DIR=%s; " % bisect_dir
-    # build the good source files
-    script_path = os.path.join(top_dir, TEST_DIR)
-    if os.path.exists("/usr/bin/x86_64-cros-linux-gnu-gcc"):
-        build_script = "chromeos_build.sh"
-    else:
-        build_script = "build.sh"
-    cmd = "%s export BISECT_STAGE=POPULATE_GOOD; pushd %s; ./%s; popd" % (
-        export_bisect,
-        script_path,
-        build_script,
-    )
-    status = ce.RunCommand(cmd)
-    return status
-
-
-def populate_bad_files(top_dir, ce, bisect_dir=DEFAULT_BISECT_DIR):
-    # 'make clean'
-    work_dir = os.path.join(top_dir, TEST_DIR, "work")
-    cmd = "rm -f %s/*.o" % work_dir
-    status = ce.RunCommand(cmd)
-    if status != 0:
-        print("Error trying to clean out work directory: %s" % cmd)
-        return status
-
-    # set up the 'bad' source files
-    script = os.path.join(top_dir, TEST_DIR, "make_sources_bad.sh")
-    status = ce.RunCommand(script)
-    if status != 0:
-        print('Error setting up "bad" source files: %s' % script)
-        return status
-
-    export_bisect = "export BISECT_DIR=%s; " % bisect_dir
-    # build the bad source files
-    script_path = os.path.join(top_dir, TEST_DIR)
-    if os.path.exists("/usr/bin/x86_64-cros-linux-gnu-gcc"):
-        build_script = "chromeos_build.sh"
-    else:
-        build_script = "build.sh"
-    cmd = "%s export BISECT_STAGE=POPULATE_BAD; pushd %s; ./%s ; popd" % (
-        export_bisect,
-        script_path,
-        build_script,
-    )
-    status = ce.RunCommand(cmd)
-    return status
-
-
-def run_main_bisection_test(top_dir, ce):
-    test_script = os.path.join(top_dir, TEST_DIR, "main-bisect-test.sh")
-    status = ce.RunCommand(test_script)
-    return status
-
-
-def verify_compiler_and_wrapper():
-    # We don't need to do any special setup if running inside a ChromeOS
-    # chroot.
-    if os.path.exists("/usr/bin/x86_64-cros-linux-gnu-gcc"):
-        return True
-
-    message = """
-*** IMPORTANT --- READ THIS CAREFULLY!! ***
-
-This test uses the command 'gcc' to compile the good/bad versions of the
-source program.  BEFORE you can run this script you must make sure that
-your compiler wrapper is in the right place, with the right name, so that
-a call to 'gcc' will go through your compiler wrapper and "do the right
-thing".
-
-Is your compiler wrapper properly set up? [Y/n]
-"""
-
-    print(message)
-    inp = sys.stdin.readline()
-    inp = inp.strip()
-    inp = inp.lower()
-    return not inp or inp == "y" or inp == "yes"
-
-
-def Main(argv):
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "--dir",
-        dest="directory",
-        help="Bisection work tree, where good  & bad object "
-        "files go.  Default is /tmp/sysroot_bisect",
-    )
-
-    options = parser.parse_args(argv)
-
-    # Make sure the compiler wrapper & soft links are properly set up.
-    wrapper_is_setup = verify_compiler_and_wrapper()
-    if not wrapper_is_setup:
-        print(
-            "Exiting now.  Please re-run after you have set up the compiler "
-            "wrapper."
-        )
-        return 0
-
-    # Make sure we're in the correct directory for running this test.
-    cwd = os.getcwd()
-    if not os.path.exists(os.path.join(cwd, "full_bisect_test")):
-        print(
-            "Error:  Wrong directory.  This script must be run from the top level"
-            " of the binary_search_tool tree (under toolchain_utils)."
-        )
-        return 1
-
-    ce = command_executer.GetCommandExecuter()
-    bisect_dir = options.directory
-    if not bisect_dir:
-        bisect_dir = DEFAULT_BISECT_DIR
-
-    # Make sure BISECT_DIR is clean
-    if os.path.exists(bisect_dir):
-        cmd = "rm -Rf %s/*" % bisect_dir
-        retv = ce.RunCommand(cmd)
-        if retv != 0:
-            return retv
-
-    retv = populate_good_files(cwd, ce, bisect_dir)
-    if retv != 0:
-        return retv
-
-    retv = populate_bad_files(cwd, ce, bisect_dir)
-    if retv != 0:
-        return retv
-
-    # Set up good/bad work soft links
-    cmd = "rm -f %s/%s/good-objects; ln -s %s/good %s/%s/good-objects" % (
-        cwd,
-        TEST_DIR,
-        bisect_dir,
-        cwd,
-        TEST_DIR,
-    )
-
-    status = ce.RunCommand(cmd)
-    if status != 0:
-        print("Error executing: %s; exiting now." % cmd)
-        return status
-
-    cmd = "rm -f %s/%s/bad-objects; ln -s %s/bad %s/%s/bad-objects" % (
-        cwd,
-        TEST_DIR,
-        bisect_dir,
-        cwd,
-        TEST_DIR,
-    )
-
-    status = ce.RunCommand(cmd)
-    if status != 0:
-        print("Error executing: %s; exiting now." % cmd)
-        return status
-
-    retv = run_main_bisection_test(cwd, ce)
-    return retv
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv[1:])
-    sys.exit(retval)
diff --git a/binary_search_tool/sysroot_wrapper/README.md b/binary_search_tool/sysroot_wrapper/README.md
deleted file mode 100644
index 77ce4b8f..00000000
--- a/binary_search_tool/sysroot_wrapper/README.md
+++ /dev/null
@@ -1,35 +0,0 @@
-# Sysroot wrapper
-
-This is a set of scripts to use when triaging compiler problem by using
-the bisecting functionality included in the `sysroot_wrapper.hardened`.
-The only script that you need to create for your triaging problem is the
-`test_script.sh` (The ones in this directory are here only as an example).
-
-Before running the binary searcher tool you will need to run the setup script:
-
-```
-./sysroot_wrapper/setup.sh ${board} ${remote_ip} ${package} ${reboot_option} ${use_flags}
-```
-
-This setup script will ensure your `$BISECT_DIR` is properly populated and
-generate a common variable script for the convenience of the scripts in
-`./sysroot_wrapper`
-
-To run the binary searcher tool with these scripts, execute it like this:
-
-```
-./binary_search_state.py \
-  --get_initial_items=./sysroot_wrapper/get_initial_items.sh \
-  --switch_to_good=./sysroot_wrapper/switch_to_good.sh \
-  --switch_to_bad=./sysroot_wrapper/switch_to_bad.sh \
-  --test_script=./sysroot_wrapper/test_script.sh \
-  --noincremental \
-  --file_args \
-  2>&1 | tee /tmp/binary_search.log
-```
-
-Finally once done you will want to run the cleanup script:
-`./sysroot_wrapper/cleanup.sh`
-
-For more information on how to use the `sysroot_wrapper` to do object file
-triaging see: https://sites.google.com/a/google.com/chromeos-toolchain-team-home2/home/team-tools-and-scripts/bisecting-compiler-problems
diff --git a/binary_search_tool/sysroot_wrapper/boot_test.sh b/binary_search_tool/sysroot_wrapper/boot_test.sh
deleted file mode 120000
index 9a345617..00000000
--- a/binary_search_tool/sysroot_wrapper/boot_test.sh
+++ /dev/null
@@ -1 +0,0 @@
-../common/boot_test.sh
\ No newline at end of file
diff --git a/binary_search_tool/sysroot_wrapper/cleanup.sh b/binary_search_tool/sysroot_wrapper/cleanup.sh
deleted file mode 100755
index b3ae2dd9..00000000
--- a/binary_search_tool/sysroot_wrapper/cleanup.sh
+++ /dev/null
@@ -1,11 +0,0 @@
-#!/bin/bash
-#
-# Copyright 2016 Google LLC
-#
-# This script is part of the ChromeOS object binary search triage process.
-# It should be the last script called by the user, after the user has
-# successfully run the bisection tool and found their bad items. This script
-# will perform all necessary cleanup for the bisection tool.
-#
-
-rm common/common.sh
diff --git a/binary_search_tool/sysroot_wrapper/get_initial_items.sh b/binary_search_tool/sysroot_wrapper/get_initial_items.sh
deleted file mode 100755
index c1beb972..00000000
--- a/binary_search_tool/sysroot_wrapper/get_initial_items.sh
+++ /dev/null
@@ -1,5 +0,0 @@
-#!/bin/bash -u
-
-source common/common.sh
-
-cat ${bisect_dir}/good/_LIST
diff --git a/binary_search_tool/sysroot_wrapper/glibc_test_script.sh b/binary_search_tool/sysroot_wrapper/glibc_test_script.sh
deleted file mode 100755
index 58413ad1..00000000
--- a/binary_search_tool/sysroot_wrapper/glibc_test_script.sh
+++ /dev/null
@@ -1,49 +0,0 @@
-#!/bin/bash -u
-
-# This is an example execution script.
-# This script changes with the problem you are trying to fix.
-# This particular script was used to triage a problem where a glibc
-# compiled with a new compiler would expose a problem in piglit.
-# Note it returns 0 only when the installation of the image succeeded
-# (ie: the machine booted after installation)
-
-source common/common.sh
-
-#export BISECT_STAGE=TRIAGE
-echo "BISECT_STAGE=${BISECT_STAGE}"
-
-echo "State of sets"
-wc -l ${bisect_dir}/*_SET
-
-board=x86-alex
-DUT=172.17.186.180
-
-echo "Cleaning up"
-{ sudo emerge -C cross-i686-pc-linux-gnu/glibc || exit 125; } &>> /tmp/glibc_triage.log
-
-echo "Building"
-{ sudo -E emerge cross-i686-pc-linux-gnu/glibc || exit 125; } &>> /tmp/glibc_triage.log
-
-echo "Building image"
-{ /home/llozano/trunk/src/scripts/build_image --board=${board} test || exit 125; } &>> /tmp/glibc_triage.log
-
-echo "Installing image"
-cros flash ${DUT} latest &> /tmp/tmp_cros_flash_result.log
-
-cat /tmp/tmp_cros_flash_result.log >> /tmp/cros_flash_result.log
-
-grep "Cros Flash completed successfully" /tmp/tmp_cros_flash_result.log || exit 125
-
-echo "Trying piglit"
-
-echo "export DISPLAY=:0.0; echo \$DISPLAY; /usr/local/piglit/lib/piglit/bin/glx-close-display -auto" > /tmp/repro.sh
-SSH_OPTS="-oUserKnownHostsFile=/dev/null -oStrictHostKeyChecking=no -oServerAliveInterval=10 -i /var/cache/chromeos-cache/distfiles/target/./chrome-src/src/third_party/chromite/ssh_keys/testing_rsa"
-scp ${SSH_OPTS} /tmp/repro.sh root@${DUT}:/tmp
-
-# notice the bash -l here. Otherwise the DISPLAY cannot be set
-( ssh ${SSH_OPTS} root@${DUT} -- /bin/bash -l /tmp/repro.sh ) > /tmp/result
-grep pass /tmp/result || { echo "PIGLIT FAILED"; exit 1; }
-
-echo "PIGLIT PASSED"
-
-exit 0
diff --git a/binary_search_tool/sysroot_wrapper/interactive_test.sh b/binary_search_tool/sysroot_wrapper/interactive_test.sh
deleted file mode 120000
index 18fe3958..00000000
--- a/binary_search_tool/sysroot_wrapper/interactive_test.sh
+++ /dev/null
@@ -1 +0,0 @@
-../common/interactive_test.sh
\ No newline at end of file
diff --git a/binary_search_tool/sysroot_wrapper/interactive_test_host.sh b/binary_search_tool/sysroot_wrapper/interactive_test_host.sh
deleted file mode 100755
index bd84936c..00000000
--- a/binary_search_tool/sysroot_wrapper/interactive_test_host.sh
+++ /dev/null
@@ -1,25 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2017 Google LLC
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on ChromeOS package and object files for a
-# host package. It waits for the test setup script to build the image, then asks
-# the user if the image is good or not. (Since this is a host package, there is
-# no 'install' phase needed.)  This script should return '0' if the test succeeds
-# (the image is 'good'); '1' if the test fails (the image is 'bad'); and '125'
-# if it could not determine (does not apply in this case).
-#
-
-source common/common.sh
-
-while true; do
-    read -p "Is this a good ChromeOS image?" yn
-    case $yn in
-        [Yy]* ) exit 0;;
-        [Nn]* ) exit 1;;
-        * ) echo "Please answer yes or no.";;
-    esac
-done
-
-exit 125
diff --git a/binary_search_tool/sysroot_wrapper/setup.sh b/binary_search_tool/sysroot_wrapper/setup.sh
deleted file mode 100755
index f9ecb0ea..00000000
--- a/binary_search_tool/sysroot_wrapper/setup.sh
+++ /dev/null
@@ -1,79 +0,0 @@
-#!/bin/bash -u
-#
-# Copyright 2021 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-#
-# This script is part of the ChromeOS object binary search triage process.
-# It should be the first script called by the user, after the user has set up
-# the two necessary build tree directories (see sysroot_wrapper/README).
-#
-# This script requires three arguments.  The first argument must be the name of
-# the board for which this work is being done (e.g. 'daisy', 'lumpy','parrot',
-# etc.).  The second argument must be the name or IP address of the chromebook
-# on which the ChromeOS images will be pushed and tested. The third argument
-# must be the name of the package being bisected (e.g. 'chromeos-chrome',
-# 'cryptohome', etc.).
-#
-# This script generates common/common.sh, which generates enviroment variables
-# used by the other scripts in the object file binary search triage process.
-#
-
-# Set up basic variables.
-bisect_dir=${BISECT_DIR:-/tmp/sysroot_bisect}
-
-BOARD=$1
-REMOTE=$2
-PACKAGE=$3
-REBOOT_OPTION=$4
-USE_FLAGS=$5
-
-GOOD_BUILD=${bisect_dir}/good
-BAD_BUILD=${bisect_dir}/bad
-GOOD_LIST=${GOOD_BUILD}/_LIST
-BAD_LIST=${BAD_BUILD}/_LIST
-
-#
-# Verify that the necessary directories exist.
-#
-
-if [[ ! -d ${GOOD_BUILD} ]] ; then
-    echo "Error:  ${GOOD_BUILD} does not exist."
-    exit 1
-fi
-
-if [[ ! -d ${BAD_BUILD} ]] ; then
-    echo "Error:  ${BAD_BUILD} does not exist."
-    exit 1
-fi
-
-if [[ ! -e ${GOOD_LIST} ]] ; then
-    echo "Error:  ${GOOD_LIST} does not exist."
-    exit 1
-fi
-
-if [[ ! -e ${BAD_LIST} ]] ; then
-    echo "Error: ${BAD_LIST} does not exist."
-    exit 1
-fi
-
-COMMON_FILE="common/common.sh"
-
-cat <<-EOF > ${COMMON_FILE}
-
-BISECT_BOARD=${BOARD}
-BISECT_REMOTE=${REMOTE}
-BISECT_PACKAGE=${PACKAGE}
-BISECT_REBOOT_OPTION=${REBOOT_OPTION}
-BISECT_USE_FLAGS="${USE_FLAGS}"
-BISECT_MODE="OBJECT_MODE"
-
-bisect_dir=${bisect_dir}
-
-export BISECT_STAGE=TRIAGE
-
-EOF
-
-chmod 755 ${COMMON_FILE}
-
-exit 0
diff --git a/binary_search_tool/sysroot_wrapper/switch_to_bad.sh b/binary_search_tool/sysroot_wrapper/switch_to_bad.sh
deleted file mode 100755
index 32f96780..00000000
--- a/binary_search_tool/sysroot_wrapper/switch_to_bad.sh
+++ /dev/null
@@ -1,9 +0,0 @@
-#!/bin/bash -u
-
-source common/common.sh
-
-# Remove file, signaling to emerge that it needs to be rebuilt. The compiler
-# wrapper will insert the correct object file based on $BISECT_BAD_SET
-cat $1 | sudo xargs rm -f
-
-exit 0
diff --git a/binary_search_tool/sysroot_wrapper/switch_to_good.sh b/binary_search_tool/sysroot_wrapper/switch_to_good.sh
deleted file mode 100755
index f59b278d..00000000
--- a/binary_search_tool/sysroot_wrapper/switch_to_good.sh
+++ /dev/null
@@ -1,9 +0,0 @@
-#!/bin/bash -u
-
-source common/common.sh
-
-# Remove file, signaling to emerge that it needs to be rebuilt. The compiler
-# wrapper will insert the correct object file based on $BISECT_GOOD_SET
-cat $1 | sudo xargs rm -f
-
-exit 0
diff --git a/binary_search_tool/sysroot_wrapper/test_script.sh b/binary_search_tool/sysroot_wrapper/test_script.sh
deleted file mode 100755
index 2629a187..00000000
--- a/binary_search_tool/sysroot_wrapper/test_script.sh
+++ /dev/null
@@ -1,34 +0,0 @@
-#!/bin/bash -u
-
-# This is an example execution script.
-# This script changes with the problem you are trying to fix.
-# This particular script was used to triage a problem where the kernel
-# would not boot while migrating to GCC 4.9.
-# Note it returns 0 only when the installation of the image succeeded
-# (ie: the machine booted after installation)
-
-source common/common.sh
-
-export BISECT_STAGE=TRIAGE
-echo "BISECT_STAGE=${BISECT_STAGE}"
-
-echo "State of sets"
-wc -l ${bisect_dir}/*_SET
-
-echo "Cleaning up"
-{ /usr/bin/sudo rm -rf /build/falco/var/cache/portage/sys-kernel && emerge-falco -C sys-kernel/chromeos-kernel-3_8-3.8.11-r96 || exit 125; } &>> /tmp/kernel_triage.log
-
-echo "Building"
-{ /usr/local/bin/emerge-falco =sys-kernel/chromeos-kernel-3_8-3.8.11-r96 || exit 125; } &>> /tmp/kernel_triage.log
-
-echo "Building image"
-{ /home/llozano/trunk/src/scripts/build_image --board=falco test || exit 125; } &>> /tmp/kernel_triage.log
-
-echo "Installing image"
-cros flash 172.17.187.150 latest &> /tmp/tmp_cros_flash_result.log
-
-cat /tmp/tmp_cros_flash_result.log >> /tmp/cros_flash_result.log
-
-grep "Cros Flash completed successfully" /tmp/tmp_cros_flash_result.log || exit 1
-
-exit 0
diff --git a/binary_search_tool/sysroot_wrapper/test_setup.sh b/binary_search_tool/sysroot_wrapper/test_setup.sh
deleted file mode 120000
index 39e715f6..00000000
--- a/binary_search_tool/sysroot_wrapper/test_setup.sh
+++ /dev/null
@@ -1 +0,0 @@
-../common/test_setup.sh
\ No newline at end of file
diff --git a/binary_search_tool/sysroot_wrapper/test_setup_host.sh b/binary_search_tool/sysroot_wrapper/test_setup_host.sh
deleted file mode 100755
index e61bc367..00000000
--- a/binary_search_tool/sysroot_wrapper/test_setup_host.sh
+++ /dev/null
@@ -1,37 +0,0 @@
-#!/bin/bash
-#
-# Copyright 2017 Google LLC
-#
-# This is a generic ChromeOS package/image test setup script. It is meant to
-# be used for either the object file or package bisection tools. This script
-# is intended to be used with host object bisection, to bisect the object
-# files in a host package.  Since it deals with a host package, there is no
-# building an image or flashing a device -- just building the host package
-# itself.
-#
-# This script is intended to be used by binary_search_state.py, as
-# part of the binary search triage on ChromeOS objects and packages. It should
-# return '0' if the setup succeeds; and '1' if the setup fails (the image
-# could not build or be flashed).
-#
-
-export PYTHONUNBUFFERED=1
-
-source common/common.sh
-
-
-if [[ "${BISECT_MODE}" == "OBJECT_MODE" ]]; then
-  echo "EMERGING ${BISECT_PACKAGE}"
-  sudo -E emerge ${BISECT_PACKAGE}
-  emerge_status=$?
-
-  if [[ ${emerge_status} -ne 0 ]] ; then
-    echo "emerging ${BISECT_PACKAGE} returned a non-zero status: $emerge_status"
-    exit 1
-  fi
-
-  exit 0
-fi
-
-
-exit 0
diff --git a/binary_search_tool/sysroot_wrapper/testing_test.py b/binary_search_tool/sysroot_wrapper/testing_test.py
deleted file mode 100755
index af884be9..00000000
--- a/binary_search_tool/sysroot_wrapper/testing_test.py
+++ /dev/null
@@ -1,45 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Test for sysroot_wrapper bisector.
-
-All files in bad_files will be determined to be bad. This test was made for
-chromeos-chrome built for a daisy board, if you are using another package you
-will need to change the base_path accordingly.
-"""
-
-
-import subprocess
-import sys
-import os
-
-base_path = (
-    "/var/cache/chromeos-chrome/chrome-src-internal/src/out_daisy/"
-    "Release/obj/"
-)
-bad_files = [
-    os.path.join(base_path, "base/base.cpu.o"),
-    os.path.join(base_path, "base/base.version.o"),
-    os.path.join(base_path, "apps/apps.launcher.o"),
-]
-
-bisect_dir = os.environ.get("BISECT_DIR", "/tmp/sysroot_bisect")
-
-
-def Main(_):
-    for test_file in bad_files:
-        test_file = test_file.strip()
-        cmd = ["grep", test_file, os.path.join(bisect_dir, "BAD_SET")]
-        ret = subprocess.call(
-            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE
-        )
-        if not ret:
-            return 1
-    return 0
-
-
-if __name__ == "__main__":
-    sys.exit(Main(sys.argv[1:]))
diff --git a/binary_search_tool/test/__init__.py b/binary_search_tool/test/__init__.py
deleted file mode 100644
index 6e3ade4a..00000000
--- a/binary_search_tool/test/__init__.py
+++ /dev/null
@@ -1,4 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
diff --git a/binary_search_tool/test/binary_search_tool_test.py b/binary_search_tool/test/binary_search_tool_test.py
deleted file mode 100755
index a79c9a1d..00000000
--- a/binary_search_tool/test/binary_search_tool_test.py
+++ /dev/null
@@ -1,619 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Tests for bisecting tool."""
-
-
-__author__ = "shenhan@google.com (Han Shen)"
-
-import os
-import random
-import sys
-import unittest
-
-from cros_utils import command_executer
-from binary_search_tool import binary_search_state
-from binary_search_tool import run_bisect
-
-from binary_search_tool.test import common
-from binary_search_tool.test import gen_obj
-
-
-def GenObj():
-    obj_num = random.randint(100, 1000)
-    bad_obj_num = random.randint(obj_num // 100, obj_num // 20)
-    if bad_obj_num == 0:
-        bad_obj_num = 1
-    gen_obj.Main(["--obj_num", str(obj_num), "--bad_obj_num", str(bad_obj_num)])
-
-
-def CleanObj():
-    os.remove(common.OBJECTS_FILE)
-    os.remove(common.WORKING_SET_FILE)
-    print(
-        'Deleted "{0}" and "{1}"'.format(
-            common.OBJECTS_FILE, common.WORKING_SET_FILE
-        )
-    )
-
-
-class BisectTest(unittest.TestCase):
-    """Tests for run_bisect.py"""
-
-    def setUp(self):
-        with open("./is_setup", "w", encoding="utf-8"):
-            pass
-
-        try:
-            os.remove(binary_search_state.STATE_FILE)
-        except OSError:
-            pass
-
-    def tearDown(self):
-        try:
-            os.remove("./is_setup")
-            os.remove(os.readlink(binary_search_state.STATE_FILE))
-            os.remove(binary_search_state.STATE_FILE)
-        except OSError:
-            pass
-
-    class FullBisector(run_bisect.Bisector):
-        """Test bisector to test run_bisect.py with"""
-
-        def __init__(self, options, overrides):
-            super(BisectTest.FullBisector, self).__init__(options, overrides)
-
-        def PreRun(self):
-            GenObj()
-            return 0
-
-        def Run(self):
-            return binary_search_state.Run(
-                get_initial_items="./gen_init_list.py",
-                switch_to_good="./switch_to_good.py",
-                switch_to_bad="./switch_to_bad.py",
-                test_script="./is_good.py",
-                prune=True,
-                file_args=True,
-            )
-
-        def PostRun(self):
-            CleanObj()
-            return 0
-
-    def test_full_bisector(self):
-        ret = run_bisect.Run(self.FullBisector({}, {}))
-        self.assertEqual(ret, 0)
-        self.assertFalse(os.path.exists(common.OBJECTS_FILE))
-        self.assertFalse(os.path.exists(common.WORKING_SET_FILE))
-
-    def check_output(self):
-        _, out, _ = command_executer.GetCommandExecuter().RunCommandWOutput(
-            (
-                'grep "Bad items are: " logs/binary_search_tool_test.py.out | '
-                "tail -n1"
-            )
-        )
-        ls = out.splitlines()
-        self.assertEqual(len(ls), 1)
-        line = ls[0]
-
-        _, _, bad_ones = line.partition("Bad items are: ")
-        bad_ones = bad_ones.split()
-        expected_result = common.ReadObjectsFile()
-
-        # Reconstruct objects file from bad_ones and compare
-        actual_result = [0] * len(expected_result)
-        for bad_obj in bad_ones:
-            actual_result[int(bad_obj)] = 1
-
-        self.assertEqual(actual_result, expected_result)
-
-
-class BisectingUtilsTest(unittest.TestCase):
-    """Tests for bisecting tool."""
-
-    def setUp(self):
-        """Generate [100-1000] object files, and 1-5% of which are bad ones."""
-        GenObj()
-
-        with open("./is_setup", "w", encoding="utf-8"):
-            pass
-
-        try:
-            os.remove(binary_search_state.STATE_FILE)
-        except OSError:
-            pass
-
-    def tearDown(self):
-        """Cleanup temp files."""
-        CleanObj()
-
-        try:
-            os.remove(os.readlink(binary_search_state.STATE_FILE))
-        except OSError:
-            pass
-
-        cleanup_list = [
-            "./is_setup",
-            binary_search_state.STATE_FILE,
-            "noinc_prune_bad",
-            "noinc_prune_good",
-            "./cmd_script.sh",
-        ]
-        for f in cleanup_list:
-            if os.path.exists(f):
-                os.remove(f)
-
-    def runTest(self):
-        ret = binary_search_state.Run(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_good.py",
-            switch_to_bad="./switch_to_bad.py",
-            test_script="./is_good.py",
-            prune=True,
-            file_args=True,
-        )
-        self.assertEqual(ret, 0)
-        self.check_output()
-
-    def test_arg_parse(self):
-        args = [
-            "--get_initial_items",
-            "./gen_init_list.py",
-            "--switch_to_good",
-            "./switch_to_good.py",
-            "--switch_to_bad",
-            "./switch_to_bad.py",
-            "--test_script",
-            "./is_good.py",
-            "--prune",
-            "--file_args",
-        ]
-        ret = binary_search_state.Main(args)
-        self.assertEqual(ret, 0)
-        self.check_output()
-
-    def test_test_setup_script(self):
-        os.remove("./is_setup")
-        with self.assertRaises(AssertionError):
-            ret = binary_search_state.Run(
-                get_initial_items="./gen_init_list.py",
-                switch_to_good="./switch_to_good.py",
-                switch_to_bad="./switch_to_bad.py",
-                test_script="./is_good.py",
-                prune=True,
-                file_args=True,
-            )
-
-        ret = binary_search_state.Run(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_good.py",
-            switch_to_bad="./switch_to_bad.py",
-            test_script="./is_good.py",
-            test_setup_script="./test_setup.py",
-            prune=True,
-            file_args=True,
-        )
-        self.assertEqual(ret, 0)
-        self.check_output()
-
-    def test_bad_test_setup_script(self):
-        with self.assertRaises(AssertionError):
-            binary_search_state.Run(
-                get_initial_items="./gen_init_list.py",
-                switch_to_good="./switch_to_good.py",
-                switch_to_bad="./switch_to_bad.py",
-                test_script="./is_good.py",
-                test_setup_script="./test_setup_bad.py",
-                prune=True,
-                file_args=True,
-            )
-
-    def test_bad_save_state(self):
-        state_file = binary_search_state.STATE_FILE
-        hidden_state_file = os.path.basename(
-            binary_search_state.HIDDEN_STATE_FILE
-        )
-
-        with open(state_file, "w", encoding="utf-8") as f:
-            f.write("test123")
-
-        bss = binary_search_state.MockBinarySearchState()
-        with self.assertRaises(OSError):
-            bss.SaveState()
-
-        with open(state_file, "r", encoding="utf-8") as f:
-            self.assertEqual(f.read(), "test123")
-
-        os.remove(state_file)
-
-        # Cleanup generated save state that has no symlink
-        files = os.listdir(os.getcwd())
-        save_states = [x for x in files if x.startswith(hidden_state_file)]
-        _ = [os.remove(x) for x in save_states]
-
-    def test_save_state(self):
-        state_file = binary_search_state.STATE_FILE
-
-        bss = binary_search_state.MockBinarySearchState()
-        bss.SaveState()
-        self.assertTrue(os.path.exists(state_file))
-        first_state = os.readlink(state_file)
-
-        bss.SaveState()
-        second_state = os.readlink(state_file)
-        self.assertTrue(os.path.exists(state_file))
-        self.assertTrue(second_state != first_state)
-        self.assertFalse(os.path.exists(first_state))
-
-        bss.RemoveState()
-        self.assertFalse(os.path.islink(state_file))
-        self.assertFalse(os.path.exists(second_state))
-
-    def test_load_state(self):
-        test_items = [1, 2, 3, 4, 5]
-
-        bss = binary_search_state.MockBinarySearchState()
-        bss.all_items = test_items
-        bss.currently_good_items = set([1, 2, 3])
-        bss.currently_bad_items = set([4, 5])
-        bss.SaveState()
-
-        bss = None
-
-        bss2 = binary_search_state.MockBinarySearchState.LoadState()
-        self.assertEqual(bss2.all_items, test_items)
-        self.assertEqual(bss2.currently_good_items, set([]))
-        self.assertEqual(bss2.currently_bad_items, set([]))
-
-    def test_tmp_cleanup(self):
-        bss = binary_search_state.MockBinarySearchState(
-            get_initial_items='echo "0\n1\n2\n3"',
-            switch_to_good="./switch_tmp.py",
-            file_args=True,
-        )
-        bss.SwitchToGood(["0", "1", "2", "3"])
-
-        tmp_file = None
-        with open("tmp_file", "r", encoding="utf-8") as f:
-            tmp_file = f.read()
-        os.remove("tmp_file")
-
-        self.assertFalse(os.path.exists(tmp_file))
-        ws = common.ReadWorkingSet()
-        for i in range(3):
-            self.assertEqual(ws[i], 42)
-
-    def test_verify_fail(self):
-        bss = binary_search_state.MockBinarySearchState(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_bad.py",
-            switch_to_bad="./switch_to_good.py",
-            test_script="./is_good.py",
-            prune=True,
-            file_args=True,
-            verify=True,
-        )
-        with self.assertRaises(AssertionError):
-            bss.DoVerify()
-
-    def test_early_terminate(self):
-        bss = binary_search_state.MockBinarySearchState(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_good.py",
-            switch_to_bad="./switch_to_bad.py",
-            test_script="./is_good.py",
-            prune=True,
-            file_args=True,
-            iterations=1,
-        )
-        bss.DoSearchBadItems()
-        self.assertFalse(bss.found_items)
-
-    def test_no_prune(self):
-        bss = binary_search_state.MockBinarySearchState(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_good.py",
-            switch_to_bad="./switch_to_bad.py",
-            test_script="./is_good.py",
-            test_setup_script="./test_setup.py",
-            prune=False,
-            file_args=True,
-        )
-        bss.DoSearchBadItems()
-        self.assertEqual(len(bss.found_items), 1)
-
-        bad_objs = common.ReadObjectsFile()
-        found_obj = int(bss.found_items.pop())
-        self.assertEqual(bad_objs[found_obj], 1)
-
-    def test_set_file(self):
-        binary_search_state.Run(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_good_set_file.py",
-            switch_to_bad="./switch_to_bad_set_file.py",
-            test_script="./is_good.py",
-            prune=True,
-            file_args=True,
-            verify=True,
-        )
-        self.check_output()
-
-    def test_noincremental_prune(self):
-        ret = binary_search_state.Run(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_good_noinc_prune.py",
-            switch_to_bad="./switch_to_bad_noinc_prune.py",
-            test_script="./is_good_noinc_prune.py",
-            test_setup_script="./test_setup.py",
-            prune=True,
-            noincremental=True,
-            file_args=True,
-            verify=False,
-        )
-        self.assertEqual(ret, 0)
-        self.check_output()
-
-    def check_output(self):
-        _, out, _ = command_executer.GetCommandExecuter().RunCommandWOutput(
-            (
-                'grep "Bad items are: " logs/binary_search_tool_test.py.out | '
-                "tail -n1"
-            )
-        )
-        ls = out.splitlines()
-        self.assertEqual(len(ls), 1)
-        line = ls[0]
-
-        _, _, bad_ones = line.partition("Bad items are: ")
-        bad_ones = bad_ones.split()
-        expected_result = common.ReadObjectsFile()
-
-        # Reconstruct objects file from bad_ones and compare
-        actual_result = [0] * len(expected_result)
-        for bad_obj in bad_ones:
-            actual_result[int(bad_obj)] = 1
-
-        self.assertEqual(actual_result, expected_result)
-
-
-class BisectingUtilsPassTest(BisectingUtilsTest):
-    """Tests for bisecting tool at pass/transformation level."""
-
-    def check_pass_output(self, pass_name, pass_num, trans_num):
-        _, out, _ = command_executer.GetCommandExecuter().RunCommandWOutput(
-            (
-                'grep "Bad pass: " logs/binary_search_tool_test.py.out | '
-                "tail -n1"
-            )
-        )
-        ls = out.splitlines()
-        self.assertEqual(len(ls), 1)
-        line = ls[0]
-        _, _, bad_info = line.partition("Bad pass: ")
-        actual_info = pass_name + " at number " + str(pass_num)
-        self.assertEqual(actual_info, bad_info)
-
-        _, out, _ = command_executer.GetCommandExecuter().RunCommandWOutput(
-            (
-                'grep "Bad transformation number: '
-                '" logs/binary_search_tool_test.py.out | '
-                "tail -n1"
-            )
-        )
-        ls = out.splitlines()
-        self.assertEqual(len(ls), 1)
-        line = ls[0]
-        _, _, bad_info = line.partition("Bad transformation number: ")
-        actual_info = str(trans_num)
-        self.assertEqual(actual_info, bad_info)
-
-    def test_with_prune(self):
-        ret = binary_search_state.Run(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_good.py",
-            switch_to_bad="./switch_to_bad.py",
-            test_script="./is_good.py",
-            pass_bisect="./generate_cmd.py",
-            prune=True,
-            file_args=True,
-        )
-        self.assertEqual(ret, 1)
-
-    def test_gen_cmd_script(self):
-        bss = binary_search_state.MockBinarySearchState(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_good.py",
-            switch_to_bad="./switch_to_bad.py",
-            test_script="./is_good.py",
-            pass_bisect="./generate_cmd.py",
-            prune=False,
-            file_args=True,
-        )
-        bss.DoSearchBadItems()
-        cmd_script_path = bss.cmd_script
-        self.assertTrue(os.path.exists(cmd_script_path))
-
-    def test_no_pass_support(self):
-        bss = binary_search_state.MockBinarySearchState(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_good.py",
-            switch_to_bad="./switch_to_bad.py",
-            test_script="./is_good.py",
-            pass_bisect="./generate_cmd.py",
-            prune=False,
-            file_args=True,
-        )
-        bss.cmd_script = "./cmd_script_no_support.py"
-        # No support for -opt-bisect-limit
-        with self.assertRaises(RuntimeError):
-            bss.BuildWithPassLimit(-1)
-
-    def test_no_transform_support(self):
-        bss = binary_search_state.MockBinarySearchState(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_good.py",
-            switch_to_bad="./switch_to_bad.py",
-            test_script="./is_good.py",
-            pass_bisect="./generate_cmd.py",
-            prune=False,
-            file_args=True,
-        )
-        bss.cmd_script = "./cmd_script_no_support.py"
-        # No support for -print-debug-counter
-        with self.assertRaises(RuntimeError):
-            bss.BuildWithTransformLimit(-1, "counter_name")
-
-    def test_pass_transform_bisect(self):
-        bss = binary_search_state.MockBinarySearchState(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_good.py",
-            switch_to_bad="./switch_to_bad.py",
-            test_script="./is_good.py",
-            pass_bisect="./generate_cmd.py",
-            prune=False,
-            file_args=True,
-        )
-        pass_num = 4
-        trans_num = 19
-        bss.cmd_script = "./cmd_script.py %d %d" % (pass_num, trans_num)
-        bss.DoSearchBadPass()
-        self.check_pass_output("instcombine-visit", pass_num, trans_num)
-
-    def test_result_not_reproduced_pass(self):
-        bss = binary_search_state.MockBinarySearchState(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_good.py",
-            switch_to_bad="./switch_to_bad.py",
-            test_script="./is_good.py",
-            pass_bisect="./generate_cmd.py",
-            prune=False,
-            file_args=True,
-        )
-        # Fails reproducing at pass level.
-        pass_num = 0
-        trans_num = 19
-        bss.cmd_script = "./cmd_script.py %d %d" % (pass_num, trans_num)
-        with self.assertRaises(ValueError):
-            bss.DoSearchBadPass()
-
-    def test_result_not_reproduced_transform(self):
-        bss = binary_search_state.MockBinarySearchState(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_good.py",
-            switch_to_bad="./switch_to_bad.py",
-            test_script="./is_good.py",
-            pass_bisect="./generate_cmd.py",
-            prune=False,
-            file_args=True,
-        )
-        # Fails reproducing at transformation level.
-        pass_num = 4
-        trans_num = 0
-        bss.cmd_script = "./cmd_script.py %d %d" % (pass_num, trans_num)
-        with self.assertRaises(ValueError):
-            bss.DoSearchBadPass()
-
-
-class BisectStressTest(unittest.TestCase):
-    """Stress tests for bisecting tool."""
-
-    def test_every_obj_bad(self):
-        amt = 25
-        gen_obj.Main(["--obj_num", str(amt), "--bad_obj_num", str(amt)])
-        ret = binary_search_state.Run(
-            get_initial_items="./gen_init_list.py",
-            switch_to_good="./switch_to_good.py",
-            switch_to_bad="./switch_to_bad.py",
-            test_script="./is_good.py",
-            prune=True,
-            file_args=True,
-            verify=False,
-        )
-        self.assertEqual(ret, 0)
-        self.check_output()
-
-    def test_every_index_is_bad(self):
-        amt = 25
-        for i in range(amt):
-            obj_list = ["0"] * amt
-            obj_list[i] = "1"
-            obj_list = ",".join(obj_list)
-            gen_obj.Main(["--obj_list", obj_list])
-            ret = binary_search_state.Run(
-                get_initial_items="./gen_init_list.py",
-                switch_to_good="./switch_to_good.py",
-                switch_to_bad="./switch_to_bad.py",
-                test_setup_script="./test_setup.py",
-                test_script="./is_good.py",
-                prune=True,
-                file_args=True,
-            )
-            self.assertEqual(ret, 0)
-            self.check_output()
-
-    def check_output(self):
-        _, out, _ = command_executer.GetCommandExecuter().RunCommandWOutput(
-            (
-                'grep "Bad items are: " logs/binary_search_tool_test.py.out | '
-                "tail -n1"
-            )
-        )
-        ls = out.splitlines()
-        self.assertEqual(len(ls), 1)
-        line = ls[0]
-
-        _, _, bad_ones = line.partition("Bad items are: ")
-        bad_ones = bad_ones.split()
-        expected_result = common.ReadObjectsFile()
-
-        # Reconstruct objects file from bad_ones and compare
-        actual_result = [0] * len(expected_result)
-        for bad_obj in bad_ones:
-            actual_result[int(bad_obj)] = 1
-
-        self.assertEqual(actual_result, expected_result)
-
-
-def Main(argv):
-    num_tests = 2
-    if len(argv) > 1:
-        num_tests = int(argv[1])
-
-    suite = unittest.TestSuite()
-    for _ in range(0, num_tests):
-        suite.addTest(BisectingUtilsTest())
-    suite.addTest(BisectingUtilsTest("test_arg_parse"))
-    suite.addTest(BisectingUtilsTest("test_test_setup_script"))
-    suite.addTest(BisectingUtilsTest("test_bad_test_setup_script"))
-    suite.addTest(BisectingUtilsTest("test_bad_save_state"))
-    suite.addTest(BisectingUtilsTest("test_save_state"))
-    suite.addTest(BisectingUtilsTest("test_load_state"))
-    suite.addTest(BisectingUtilsTest("test_tmp_cleanup"))
-    suite.addTest(BisectingUtilsTest("test_verify_fail"))
-    suite.addTest(BisectingUtilsTest("test_early_terminate"))
-    suite.addTest(BisectingUtilsTest("test_no_prune"))
-    suite.addTest(BisectingUtilsTest("test_set_file"))
-    suite.addTest(BisectingUtilsTest("test_noincremental_prune"))
-    suite.addTest(BisectingUtilsPassTest("test_with_prune"))
-    suite.addTest(BisectingUtilsPassTest("test_gen_cmd_script"))
-    suite.addTest(BisectingUtilsPassTest("test_no_pass_support"))
-    suite.addTest(BisectingUtilsPassTest("test_no_transform_support"))
-    suite.addTest(BisectingUtilsPassTest("test_pass_transform_bisect"))
-    suite.addTest(BisectingUtilsPassTest("test_result_not_reproduced_pass"))
-    suite.addTest(
-        BisectingUtilsPassTest("test_result_not_reproduced_transform")
-    )
-    suite.addTest(BisectTest("test_full_bisector"))
-    suite.addTest(BisectStressTest("test_every_obj_bad"))
-    suite.addTest(BisectStressTest("test_every_index_is_bad"))
-    runner = unittest.TextTestRunner()
-    runner.run(suite)
-
-
-if __name__ == "__main__":
-    Main(sys.argv)
diff --git a/binary_search_tool/test/cmd_script.py b/binary_search_tool/test/cmd_script.py
deleted file mode 100755
index b0475c70..00000000
--- a/binary_search_tool/test/cmd_script.py
+++ /dev/null
@@ -1,80 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Command script without compiler support for pass level bisection.
-
-This script generates a pseudo log which a workable compiler should print out.
-It assumes that -opt-bisect-limit and -print-debug-counter are supported by the
-compiler.
-"""
-
-
-import os
-import sys
-
-from binary_search_tool.test import common
-
-
-def Main(argv):
-    if not os.path.exists("./is_setup"):
-        return 1
-
-    if len(argv) != 3:
-        return 1
-
-    limit_flags = os.environ["LIMIT_FLAGS"]
-    opt_bisect_exist = False
-    debug_counter_exist = False
-
-    for option in limit_flags.split():
-        if "-opt-bisect-limit" in option:
-            opt_bisect_limit = int(option.split("=")[-1])
-            opt_bisect_exist = True
-        if "-debug-counter=" in option:
-            debug_counter = int(option.split("=")[-1])
-            debug_counter_exist = True
-
-    if not opt_bisect_exist:
-        return 1
-
-    # Manually set total number and bad number
-    total_pass = 10
-    total_transform = 20
-    bad_pass = int(argv[1])
-    bad_transform = int(argv[2])
-
-    if opt_bisect_limit == -1:
-        opt_bisect_limit = total_pass
-
-    for i in range(1, total_pass + 1):
-        bisect_str = (
-            "BISECT: %srunning pass (%d) Combine redundant "
-            "instructions on function (f1)"
-            % ("NOT " if i > opt_bisect_limit else "", i)
-        )
-        print(bisect_str, file=sys.stderr)
-
-    if debug_counter_exist:
-        print("Counters and values:", file=sys.stderr)
-        print(
-            "instcombine-visit : {%d, 0, %d}"
-            % (total_transform, debug_counter),
-            file=sys.stderr,
-        )
-
-    if opt_bisect_limit > bad_pass or (
-        debug_counter_exist and debug_counter > bad_transform
-    ):
-        common.WriteWorkingSet([1])
-    else:
-        common.WriteWorkingSet([0])
-
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv)
-    sys.exit(retval)
diff --git a/binary_search_tool/test/cmd_script_no_support.py b/binary_search_tool/test/cmd_script_no_support.py
deleted file mode 100644
index f1c2bcbe..00000000
--- a/binary_search_tool/test/cmd_script_no_support.py
+++ /dev/null
@@ -1,29 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Command script without compiler support for pass level bisection.
-
-This script generates a pseudo log when certain bisecting flags are not
-supported by compiler.
-"""
-
-
-import os
-import sys
-
-
-def Main():
-    if not os.path.exists("./is_setup"):
-        return 1
-    print(
-        "No support for -opt-bisect-limit or -print-debug-counter.",
-        file=sys.stderr,
-    )
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main()
-    sys.exit(retval)
diff --git a/binary_search_tool/test/common.py b/binary_search_tool/test/common.py
deleted file mode 100755
index 6632a4c7..00000000
--- a/binary_search_tool/test/common.py
+++ /dev/null
@@ -1,42 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Common utility functions."""
-
-DEFAULT_OBJECT_NUMBER = 1238
-DEFAULT_BAD_OBJECT_NUMBER = 23
-OBJECTS_FILE = "objects.txt"
-WORKING_SET_FILE = "working_set.txt"
-
-
-def ReadWorkingSet():
-    working_set = []
-    with open(WORKING_SET_FILE, "r", encoding="utf-8") as f:
-        for l in f:
-            working_set.append(int(l))
-    return working_set
-
-
-def WriteWorkingSet(working_set):
-    with open(WORKING_SET_FILE, "w", encoding="utf-8") as f:
-        for o in working_set:
-            f.write("{0}\n".format(o))
-
-
-def ReadObjectsFile():
-    objects_file = []
-    with open(OBJECTS_FILE, "r", encoding="utf-8") as f:
-        for l in f:
-            objects_file.append(int(l))
-    return objects_file
-
-
-def ReadObjectIndex(filename):
-    object_index = []
-    with open(filename, "r", encoding="utf-8") as f:
-        for o in f:
-            object_index.append(int(o))
-    return object_index
diff --git a/binary_search_tool/test/gen_init_list.py b/binary_search_tool/test/gen_init_list.py
deleted file mode 100755
index 138e949c..00000000
--- a/binary_search_tool/test/gen_init_list.py
+++ /dev/null
@@ -1,27 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Prints out index for every object file, starting from 0."""
-
-
-import sys
-
-from cros_utils import command_executer
-from binary_search_tool.test import common
-
-
-def Main():
-    ce = command_executer.GetCommandExecuter()
-    _, l, _ = ce.RunCommandWOutput(
-        "cat {0} | wc -l".format(common.OBJECTS_FILE), print_to_console=False
-    )
-    for i in range(0, int(l)):
-        print(i)
-
-
-if __name__ == "__main__":
-    Main()
-    sys.exit(0)
diff --git a/binary_search_tool/test/gen_obj.py b/binary_search_tool/test/gen_obj.py
deleted file mode 100755
index 394445f0..00000000
--- a/binary_search_tool/test/gen_obj.py
+++ /dev/null
@@ -1,110 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Script to generate a list of object files.
-
-0 represents a good object file.
-1 represents a bad object file.
-"""
-
-
-import argparse
-import os
-import random
-import sys
-
-from binary_search_tool.test import common
-
-
-def Main(argv):
-    """Generates a list, the value of each element is 0 or 1.
-
-    The number of 1s in the list is specified by bad_obj_num.
-    The others are all 0s. The total number of 0s and 1s is specified by obj_num.
-
-    Args:
-      argv: argument from command line
-
-    Returns:
-      0 always.
-    """
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "-n",
-        "--obj_num",
-        dest="obj_num",
-        default=common.DEFAULT_OBJECT_NUMBER,
-        help=("Number of total objects."),
-    )
-    parser.add_argument(
-        "-b",
-        "--bad_obj_num",
-        dest="bad_obj_num",
-        default=common.DEFAULT_BAD_OBJECT_NUMBER,
-        help=(
-            "Number of bad objects. Must be great than or "
-            "equal to zero and less than total object "
-            "number."
-        ),
-    )
-    parser.add_argument(
-        "-o",
-        "--obj_list",
-        dest="obj_list",
-        default="",
-        help=(
-            "List of comma seperated objects to generate. "
-            "A 0 means the object is good, a 1 means the "
-            "object is bad."
-        ),
-    )
-    options = parser.parse_args(argv)
-
-    obj_num = int(options.obj_num)
-    bad_obj_num = int(options.bad_obj_num)
-    bad_to_gen = int(options.bad_obj_num)
-    obj_list = options.obj_list
-    if not obj_list:
-        obj_list = []
-        for i in range(obj_num):
-            if bad_to_gen > 0 and random.randint(1, obj_num) <= bad_obj_num:
-                obj_list.append(1)
-                bad_to_gen -= 1
-            else:
-                obj_list.append(0)
-        while bad_to_gen > 0:
-            t = random.randint(0, obj_num - 1)
-            if obj_list[t] == 0:
-                obj_list[t] = 1
-                bad_to_gen -= 1
-    else:
-        obj_list = obj_list.split(",")
-
-    if os.path.isfile(common.OBJECTS_FILE):
-        os.remove(common.OBJECTS_FILE)
-    if os.path.isfile(common.WORKING_SET_FILE):
-        os.remove(common.WORKING_SET_FILE)
-
-    with open(common.OBJECTS_FILE, "w", encoding="utf-8") as f:
-        with open(common.WORKING_SET_FILE, "w", encoding="utf-8") as w:
-            for i in obj_list:
-                f.write("{0}\n".format(i))
-                w.write("{0}\n".format(i))
-
-    obj_num = len(obj_list)
-    bad_obj_num = obj_list.count(1)
-    print(
-        "Generated {0} object files, with {1} bad ones.".format(
-            obj_num, bad_obj_num
-        )
-    )
-
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv[1:])
-    sys.exit(retval)
diff --git a/binary_search_tool/test/generate_cmd.py b/binary_search_tool/test/generate_cmd.py
deleted file mode 100755
index 96fa720c..00000000
--- a/binary_search_tool/test/generate_cmd.py
+++ /dev/null
@@ -1,29 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Generate a virtual cmd script for pass level bisection.
-
-This is a required argument for pass level bisecting. For unit test, we use
-this script to verify if cmd_script.sh is generated correctly.
-"""
-
-
-import os
-import sys
-
-
-def Main():
-    if not os.path.exists("./is_setup"):
-        return 1
-    file_name = "cmd_script.sh"
-    with open(file_name, "w", encoding="utf-8") as f:
-        f.write("Generated by generate_cmd.py")
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main()
-    sys.exit(retval)
diff --git a/binary_search_tool/test/is_good.py b/binary_search_tool/test/is_good.py
deleted file mode 100755
index fd3f908f..00000000
--- a/binary_search_tool/test/is_good.py
+++ /dev/null
@@ -1,28 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Check to see if the working set produces a good executable."""
-
-
-import os
-import sys
-
-from binary_search_tool.test import common
-
-
-def Main():
-    if not os.path.exists("./is_setup"):
-        return 1
-    working_set = common.ReadWorkingSet()
-    for w in working_set:
-        if w == 1:
-            return 1  ## False, linking failure
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main()
-    sys.exit(retval)
diff --git a/binary_search_tool/test/is_good_noinc_prune.py b/binary_search_tool/test/is_good_noinc_prune.py
deleted file mode 100755
index 654fcd25..00000000
--- a/binary_search_tool/test/is_good_noinc_prune.py
+++ /dev/null
@@ -1,50 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Check to see if the working set produces a good executable.
-
-This test script is made for the noincremental-prune test. This makes sure
-that, after pruning starts (>1 bad item is found), that the number of args sent
-to the switch scripts is equals to the actual number of items (i.e. checking
-that noincremental always holds).
-"""
-
-
-import os
-import sys
-
-from binary_search_tool.test import common
-
-
-def Main():
-    working_set = common.ReadWorkingSet()
-
-    with open("noinc_prune_good", "r", encoding="utf-8") as good_args:
-        num_good_args = len(good_args.readlines())
-
-    with open("noinc_prune_bad", "r", encoding="utf-8") as bad_args:
-        num_bad_args = len(bad_args.readlines())
-
-    num_args = num_good_args + num_bad_args
-    if num_args != len(working_set):
-        print("Only %d args, expected %d" % (num_args, len(working_set)))
-        print("%d good args, %d bad args" % (num_good_args, num_bad_args))
-        return 3
-
-    os.remove("noinc_prune_bad")
-    os.remove("noinc_prune_good")
-
-    if not os.path.exists("./is_setup"):
-        return 1
-    for w in working_set:
-        if w == 1:
-            return 1  ## False, linking failure
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main()
-    sys.exit(retval)
diff --git a/binary_search_tool/test/switch_tmp.py b/binary_search_tool/test/switch_tmp.py
deleted file mode 100755
index acc0393d..00000000
--- a/binary_search_tool/test/switch_tmp.py
+++ /dev/null
@@ -1,38 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Change portions of the object files to good.
-
-This file is a test switch script. Used only for the test test_tmp_cleanup.
-The "portion" is defined by the file (which is passed as the only argument to
-this script) content. Every line in the file is an object index, which will be
-set to good (mark as 42).
-"""
-
-
-import sys
-
-from binary_search_tool.test import common
-
-
-def Main(argv):
-    working_set = common.ReadWorkingSet()
-    object_index = common.ReadObjectIndex(argv[1])
-
-    # Random number so the results can be checked
-    for oi in object_index:
-        working_set[int(oi)] = 42
-
-    common.WriteWorkingSet(working_set)
-    with open("tmp_file", "w", encoding="utf-8") as f:
-        f.write(argv[1])
-
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv)
-    sys.exit(retval)
diff --git a/binary_search_tool/test/switch_to_bad.py b/binary_search_tool/test/switch_to_bad.py
deleted file mode 100755
index bc32f3cc..00000000
--- a/binary_search_tool/test/switch_to_bad.py
+++ /dev/null
@@ -1,31 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Switch part of the objects file in working set to (possible) bad ones."""
-
-
-import sys
-
-from binary_search_tool.test import common
-
-
-def Main(argv):
-    """Switch part of the objects file in working set to (possible) bad ones."""
-    working_set = common.ReadWorkingSet()
-    objects_file = common.ReadObjectsFile()
-    object_index = common.ReadObjectIndex(argv[1])
-
-    for oi in object_index:
-        working_set[oi] = objects_file[oi]
-
-    common.WriteWorkingSet(working_set)
-
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv)
-    sys.exit(retval)
diff --git a/binary_search_tool/test/switch_to_bad_noinc_prune.py b/binary_search_tool/test/switch_to_bad_noinc_prune.py
deleted file mode 100755
index e5574f95..00000000
--- a/binary_search_tool/test/switch_to_bad_noinc_prune.py
+++ /dev/null
@@ -1,46 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Switch part of the objects file in working set to (possible) bad ones.
-
-The "portion" is defined by the file (which is passed as the only argument to
-this script) content. Every line in the file is an object index, which will be
-set to good (mark as 0).
-
-This switch script is made for the noincremental-prune test. This makes sure
-that, after pruning starts (>1 bad item is found), that the number of args sent
-to the switch scripts is equals to the actual number of items (i.e. checking
-that noincremental always holds).
-
-Warning: This switch script assumes the --file_args option
-"""
-
-
-import shutil
-import sys
-
-from binary_search_tool.test import common
-
-
-def Main(argv):
-    """Switch part of the objects file in working set to (possible) bad ones."""
-    working_set = common.ReadWorkingSet()
-    objects_file = common.ReadObjectsFile()
-    object_index = common.ReadObjectIndex(argv[1])
-
-    for oi in object_index:
-        working_set[oi] = objects_file[oi]
-
-    shutil.copy(argv[1], "./noinc_prune_bad")
-
-    common.WriteWorkingSet(working_set)
-
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv)
-    sys.exit(retval)
diff --git a/binary_search_tool/test/switch_to_bad_set_file.py b/binary_search_tool/test/switch_to_bad_set_file.py
deleted file mode 100755
index 9d4bee6f..00000000
--- a/binary_search_tool/test/switch_to_bad_set_file.py
+++ /dev/null
@@ -1,41 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Switch part of the objects file in working set to (possible) bad ones.
-
-This script is meant to be specifically used with the set_file test. This uses
-the set files generated by binary_search_state to do the switching.
-"""
-
-
-import os
-import sys
-
-from binary_search_tool.test import common
-
-
-def Main(_):
-    """Switch part of the objects file in working set to (possible) bad ones."""
-    working_set = common.ReadWorkingSet()
-    objects_file = common.ReadObjectsFile()
-
-    if not os.path.exists(os.environ["BISECT_BAD_SET"]):
-        print("Bad set file does not exist!")
-        return 1
-
-    object_index = common.ReadObjectIndex(os.environ["BISECT_BAD_SET"])
-
-    for oi in object_index:
-        working_set[int(oi)] = objects_file[oi]
-
-    common.WriteWorkingSet(working_set)
-
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv)
-    sys.exit(retval)
diff --git a/binary_search_tool/test/switch_to_good.py b/binary_search_tool/test/switch_to_good.py
deleted file mode 100755
index 61a59a2a..00000000
--- a/binary_search_tool/test/switch_to_good.py
+++ /dev/null
@@ -1,34 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Change portions of the object files to good.
-
-The "portion" is defined by the file (which is passed as the only argument to
-this script) content. Every line in the file is an object index, which will be
-set to good (mark as 0).
-"""
-
-
-import sys
-
-from binary_search_tool.test import common
-
-
-def Main(argv):
-    working_set = common.ReadWorkingSet()
-    object_index = common.ReadObjectIndex(argv[1])
-
-    for oi in object_index:
-        working_set[int(oi)] = 0
-
-    common.WriteWorkingSet(working_set)
-
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv)
-    sys.exit(retval)
diff --git a/binary_search_tool/test/switch_to_good_noinc_prune.py b/binary_search_tool/test/switch_to_good_noinc_prune.py
deleted file mode 100755
index 3bda1d78..00000000
--- a/binary_search_tool/test/switch_to_good_noinc_prune.py
+++ /dev/null
@@ -1,44 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Change portions of the object files to good.
-
-The "portion" is defined by the file (which is passed as the only argument to
-this script) content. Every line in the file is an object index, which will be
-set to good (mark as 0).
-
-This switch script is made for the noincremental-prune test. This makes sure
-that, after pruning starts (>1 bad item is found), that the number of args sent
-to the switch scripts is equals to the actual number of items (i.e. checking
-that noincremental always holds).
-
-Warning: This switch script assumes the --file_args option
-"""
-
-
-import shutil
-import sys
-
-from binary_search_tool.test import common
-
-
-def Main(argv):
-    working_set = common.ReadWorkingSet()
-    object_index = common.ReadObjectIndex(argv[1])
-
-    for oi in object_index:
-        working_set[int(oi)] = 0
-
-    shutil.copy(argv[1], "./noinc_prune_good")
-
-    common.WriteWorkingSet(working_set)
-
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv)
-    sys.exit(retval)
diff --git a/binary_search_tool/test/switch_to_good_set_file.py b/binary_search_tool/test/switch_to_good_set_file.py
deleted file mode 100755
index b83cbe3f..00000000
--- a/binary_search_tool/test/switch_to_good_set_file.py
+++ /dev/null
@@ -1,43 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Change portions of the object files to good.
-
-The "portion" is defined by the file (which is passed as the only argument to
-this script) content. Every line in the file is an object index, which will be
-set to good (mark as 0).
-
-This script is meant to be specifically used with the set_file test. This uses
-the set files generated by binary_search_state to do the switching.
-"""
-
-
-import os
-import sys
-
-from binary_search_tool.test import common
-
-
-def Main(_):
-    working_set = common.ReadWorkingSet()
-
-    if not os.path.exists(os.environ["BISECT_GOOD_SET"]):
-        print("Good set file does not exist!")
-        return 1
-
-    object_index = common.ReadObjectIndex(os.environ["BISECT_GOOD_SET"])
-
-    for oi in object_index:
-        working_set[int(oi)] = 0
-
-    common.WriteWorkingSet(working_set)
-
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv)
-    sys.exit(retval)
diff --git a/binary_search_tool/test/test_setup.py b/binary_search_tool/test/test_setup.py
deleted file mode 100755
index 52486a28..00000000
--- a/binary_search_tool/test/test_setup.py
+++ /dev/null
@@ -1,23 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Emulate running of test setup script, is_good.py should fail without this."""
-
-
-import sys
-
-
-def Main():
-    # create ./is_setup
-    with open("./is_setup", "w", encoding="utf-8"):
-        pass
-
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main()
-    sys.exit(retval)
diff --git a/binary_search_tool/test/test_setup_bad.py b/binary_search_tool/test/test_setup_bad.py
deleted file mode 100755
index 518a69fd..00000000
--- a/binary_search_tool/test/test_setup_bad.py
+++ /dev/null
@@ -1,19 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Emulate test setup that fails (i.e. failed flash to device)"""
-
-
-import sys
-
-
-def Main():
-    return 1  ## False, flashing failure
-
-
-if __name__ == "__main__":
-    retval = Main()
-    sys.exit(retval)
diff --git a/bot_tools/fetch_all_subtest_logs.py b/bot_tools/fetch_all_subtest_logs.py
new file mode 100644
index 00000000..934055fb
--- /dev/null
+++ b/bot_tools/fetch_all_subtest_logs.py
@@ -0,0 +1,169 @@
+# Copyright 2025 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Given the `bb` id for a cq-orchestrator, fetches all test logs on gs://.
+
+Note that "all" can be _a lot_; some runs have dozens of GB of logs.
+"""
+
+import argparse
+import json
+import logging
+from pathlib import Path
+import re
+import shlex
+import subprocess
+from typing import Any, Dict, Iterable, List
+
+
+def get_bb_output(subcmd: List[str]):
+    return subprocess.run(
+        ["bb"] + subcmd,
+        check=True,
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+        encoding="utf-8",
+    ).stdout
+
+
+def get_bb_json_output(subcmd: str, args: Iterable[str]):
+    cmd = [subcmd, "-json"]
+    cmd += args
+    output = get_bb_output(cmd)
+    return json.loads(output)
+
+
+def build_id_to_link(build_id: int) -> str:
+    return f"https://ci.chromium.org/b/{build_id}"
+
+
+def find_cros_test_platform_child_of_cq_orchestrator(
+    cq_orchestrator_id: int,
+) -> int:
+    """Looks for the cros_test_platform invocation run by the cq-orch."""
+    # At the time of writing, this shows up like in a link inside of a 'check
+    # test results' log.
+
+    output = get_bb_json_output("get", ("-steps", str(cq_orchestrator_id)))
+    summary_markdown_re = re.compile(
+        re.escape("https://cr-buildbucket.appspot.com/build/") + r"(\d+)"
+    )
+    # If this JSON isn't perfectly formed, the below may `raise`. Given the
+    # simplicity of this script, that's fine.
+    for step in output["steps"]:
+        if step.get("name") != "check test results":
+            continue
+
+        summary = step.get("summaryMarkdown", "")
+        match = summary_markdown_re.search(summary)
+        if not match:
+            raise ValueError(
+                "cq-orchestrator's summary had no cros_test_platform link"
+            )
+        return int(match.group(1))
+
+    raise ValueError(
+        f"No `check test results` step found in "
+        f"{build_id_to_link(cq_orchestrator_id)}"
+    )
+
+
+def find_gs_links_in_test_log(log: Dict[str, Any]) -> List[str]:
+    empty_dict = {}
+    results = []
+    for result_sets in log.values():
+        for result_set in result_sets:
+            gs_url = (
+                result_set.get("Results", empty_dict)
+                .get("log_data", empty_dict)
+                .get("gs_url")
+            )
+            if gs_url:
+                results.append(gs_url)
+    if not results:
+        raise ValueError(
+            "No gs_urls found in test results; detection is probably broken"
+        )
+    return results
+
+
+def find_all_gs_log_test_links(cq_orchestrator_id: int) -> List[str]:
+    cros_test_platform_id = find_cros_test_platform_child_of_cq_orchestrator(
+        cq_orchestrator_id
+    )
+    log_output = get_bb_output(
+        [
+            "log",
+            str(cros_test_platform_id),
+            "ctpv2 sub-build (async)|Summarize",
+            "all test results",
+        ]
+    )
+    parsed_log_output = json.loads(log_output)
+    return find_gs_links_in_test_log(parsed_log_output)
+
+
+def download_gs_logs_to(
+    output_dir: Path, gs_logs: List[str], dry_run: bool = False
+):
+    gsutil_command = ["gsutil", "-m", "cp", "-r"] + gs_logs + [str(output_dir)]
+    logging.info("Running `%s`...", shlex.join(gsutil_command))
+    if dry_run:
+        logging.info("--dry-run passed; running skipped")
+    else:
+        subprocess.run(gsutil_command, check=True)
+
+
+def main(argv: List[str]) -> None:
+    logging.basicConfig(
+        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
+        "%(message)s",
+        level=logging.INFO,
+    )
+
+    parser = argparse.ArgumentParser(
+        description=__doc__,
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+    )
+    parser.add_argument(
+        "-c",
+        "--cq-orchestrator-id",
+        type=int,
+        required=True,
+        help="cq-orchestrator builder ID",
+    )
+    parser.add_argument(
+        "-o",
+        "--output-dir",
+        type=Path,
+        required=True,
+        help="directory to write results into",
+    )
+    parser.add_argument(
+        "-n",
+        "--dry-run",
+        action="store_true",
+        help="""
+        Don't actually do the download; just print the command
+        that would have been run.
+        """,
+    )
+    opts = parser.parse_args(argv)
+
+    cq_orchestrator_id: int = opts.cq_orchestrator_id
+    dry_run: bool = opts.dry_run
+    output_dir: Path = opts.output_dir
+
+    if not dry_run and output_dir.exists():
+        parser.error("--output-dir exists; refusing to overwrite")
+
+    logging.info("Finding all relevant test log locations...")
+    all_log_locations = find_all_gs_log_test_links(cq_orchestrator_id)
+
+    logging.info(
+        "Found %d test logs on gs://; downloading...", len(all_log_locations)
+    )
+    if not dry_run:
+        output_dir.mkdir(parents=True)
+    download_gs_logs_to(output_dir, all_log_locations, dry_run)
diff --git a/bot_tools/fetch_all_subtest_logs_test.py b/bot_tools/fetch_all_subtest_logs_test.py
new file mode 100644
index 00000000..ac61ac29
--- /dev/null
+++ b/bot_tools/fetch_all_subtest_logs_test.py
@@ -0,0 +1,86 @@
+# Copyright 2025 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Tests for fetch_all_subtest_logs.py."""
+
+from pathlib import Path
+import subprocess
+import unittest
+from unittest import mock
+
+# Rename this, since its original name (& function names) lead to lines that're
+# too long.
+from bot_tools import fetch_all_subtest_logs as main
+
+
+class Test(unittest.TestCase):
+    """Tests for fetch_all_subtest_logs."""
+
+    @mock.patch.object(subprocess, "run")
+    def test_log_downloading_doesnt_crash(self, subprocess_run_mock):
+        main.download_gs_logs_to(
+            Path("/path/does/not/exist"), gs_logs=["gs://foo", "gs://bar"]
+        )
+        subprocess_run_mock.assert_called_once()
+
+    @mock.patch.object(main, "get_bb_json_output")
+    def test_find_cros_test_platform_raises_if_none_found(
+        self, get_bb_json_output_mock
+    ):
+        get_bb_json_output_mock.return_value = {"steps": []}
+        with self.assertRaisesRegex(ValueError, "No `check test results`.*"):
+            main.find_cros_test_platform_child_of_cq_orchestrator(
+                cq_orchestrator_id=1
+            )
+
+    @mock.patch.object(main, "get_bb_json_output")
+    def test_find_cros_test_platform_works(self, get_bb_json_output_mock):
+        summary_md = "[foo](https://cr-buildbucket.appspot.com/build/123)"
+        get_bb_json_output_mock.return_value = {
+            "steps": [
+                {
+                    "name": "foo",
+                },
+                {
+                    "name": "check test results",
+                    "summaryMarkdown": summary_md,
+                },
+            ]
+        }
+        result = main.find_cros_test_platform_child_of_cq_orchestrator(
+            cq_orchestrator_id=1
+        )
+        self.assertEqual(result, 123)
+
+    def test_gs_link_finding_fails_if_none_found(self):
+        with self.assertRaisesRegex(ValueError, "No gs_urls.*"):
+            main.find_gs_links_in_test_log({})
+
+    def test_gs_link_finding_finds_links(self):
+        results = main.find_gs_links_in_test_log(
+            {
+                "suite-name-1": [
+                    {
+                        "Results": {
+                            "log_data": {
+                                "gs_url": "gs_url_1",
+                            }
+                        }
+                    },
+                    {},
+                    {"Results": {}},
+                    {"Results": {"log_data": {}}},
+                ],
+                "suite-name-2": [
+                    {
+                        "Results": {
+                            "log_data": {
+                                "gs_url": "gs_url_2",
+                            }
+                        }
+                    },
+                ],
+            }
+        )
+        self.assertEqual(results, ["gs_url_1", "gs_url_2"])
diff --git a/build_chromeos.py b/build_chromeos.py
deleted file mode 100755
index 84ee0b84..00000000
--- a/build_chromeos.py
+++ /dev/null
@@ -1,377 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Script to checkout the ChromeOS source.
-
-This script sets up the ChromeOS source in the given directory, matching a
-particular release of ChromeOS.
-"""
-
-
-__author__ = (
-    "asharif@google.com (Ahmad Sharif) "
-    "llozano@google.com (Luis Lozano) "
-    "raymes@google.com (Raymes Khoury) "
-    "shenhan@google.com (Han Shen)"
-)
-
-import argparse
-import os
-import sys
-
-from cros_utils import command_executer
-from cros_utils import logger
-from cros_utils import misc
-
-
-def Usage(parser, message):
-    print("ERROR: %s" % message)
-    parser.print_help()
-    sys.exit(0)
-
-
-def Main(argv):
-    """Build ChromeOS."""
-    # Common initializations
-    cmd_executer = command_executer.GetCommandExecuter()
-
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "--chromeos_root",
-        dest="chromeos_root",
-        help="Target directory for ChromeOS installation.",
-    )
-    parser.add_argument(
-        "--clobber_chroot",
-        dest="clobber_chroot",
-        action="store_true",
-        help="Delete the chroot and start fresh",
-        default=False,
-    )
-    parser.add_argument(
-        "--clobber_board",
-        dest="clobber_board",
-        action="store_true",
-        help="Delete the board and start fresh",
-        default=False,
-    )
-    parser.add_argument(
-        "--rebuild",
-        dest="rebuild",
-        action="store_true",
-        help="Rebuild all board packages except the toolchain.",
-        default=False,
-    )
-    parser.add_argument(
-        "--cflags",
-        dest="cflags",
-        default="",
-        help="CFLAGS for the ChromeOS packages",
-    )
-    parser.add_argument(
-        "--cxxflags",
-        dest="cxxflags",
-        default="",
-        help="CXXFLAGS for the ChromeOS packages",
-    )
-    parser.add_argument(
-        "--ldflags",
-        dest="ldflags",
-        default="",
-        help="LDFLAGS for the ChromeOS packages",
-    )
-    parser.add_argument(
-        "--board", dest="board", help="ChromeOS target board, e.g. x86-generic"
-    )
-    parser.add_argument(
-        "--package", dest="package", help="The package needs to be built"
-    )
-    parser.add_argument(
-        "--label",
-        dest="label",
-        help="Optional label symlink to point to build dir.",
-    )
-    parser.add_argument(
-        "--dev",
-        dest="dev",
-        default=False,
-        action="store_true",
-        help=(
-            "Make the final image in dev mode (eg writable, "
-            "more space on image). Defaults to False."
-        ),
-    )
-    parser.add_argument(
-        "--debug",
-        dest="debug",
-        default=False,
-        action="store_true",
-        help=(
-            'Optional. Build chrome browser with "-g -O0". '
-            "Notice, this also turns on '--dev'. "
-            "Defaults to False."
-        ),
-    )
-    parser.add_argument(
-        "--env", dest="env", default="", help="Env to pass to build_packages."
-    )
-    parser.add_argument(
-        "--vanilla",
-        dest="vanilla",
-        default=False,
-        action="store_true",
-        help="Use default ChromeOS toolchain.",
-    )
-    parser.add_argument(
-        "--vanilla_image",
-        dest="vanilla_image",
-        default=False,
-        action="store_true",
-        help=(
-            "Use prebuild packages for building the image. "
-            "It also implies the --vanilla option is set."
-        ),
-    )
-
-    options = parser.parse_args(argv[1:])
-
-    if options.chromeos_root is None:
-        Usage(parser, "--chromeos_root must be set")
-    options.chromeos_root = os.path.expanduser(options.chromeos_root)
-    scripts_dir = os.path.join(options.chromeos_root, "src", "scripts")
-    if not os.path.isdir(scripts_dir):
-        Usage(
-            parser,
-            "--chromeos_root must be set up first. Use setup_chromeos.py",
-        )
-
-    if options.board is None:
-        Usage(parser, "--board must be set")
-
-    if options.debug:
-        options.dev = True
-
-    build_packages_env = options.env
-    if build_packages_env.find("EXTRA_BOARD_FLAGS=") != -1:
-        logger.GetLogger().LogFatal(
-            (
-                'Passing "EXTRA_BOARD_FLAGS" in "--env" is not supported. '
-                "This flags is used internally by this script. "
-                "Contact the author for more detail."
-            )
-        )
-
-    if options.rebuild:
-        build_packages_env += " EXTRA_BOARD_FLAGS=-e"
-        # EXTRA_BOARD_FLAGS=-e should clean up the object files for the chrome
-        # browser but it doesn't. So do it here.
-        misc.RemoveChromeBrowserObjectFiles(
-            options.chromeos_root, options.board
-        )
-
-    # Build with afdo_use by default.
-    # To change the default use --env="USE=-afdo_use".
-    build_packages_env = misc.MergeEnvStringWithDict(
-        build_packages_env, {"USE": "chrome_internal afdo_use -cros-debug"}
-    )
-
-    build_packages_command = misc.GetBuildPackagesCommand(
-        board=options.board, usepkg=options.vanilla_image, debug=options.debug
-    )
-
-    if options.package:
-        build_packages_command += " {0}".format(options.package)
-
-    build_image_command = misc.GetBuildImageCommand(options.board, options.dev)
-
-    if options.vanilla or options.vanilla_image:
-        command = misc.GetSetupBoardCommand(
-            options.board,
-            usepkg=options.vanilla_image,
-            force=options.clobber_board,
-        )
-        command += "; " + build_packages_env + " " + build_packages_command
-        command += "&& " + build_packages_env + " " + build_image_command
-        ret = cmd_executer.ChrootRunCommand(options.chromeos_root, command)
-        return ret
-
-    # Setup board
-    if (
-        not os.path.isdir(
-            options.chromeos_root + "/chroot/build/" + options.board
-        )
-        or options.clobber_board
-    ):
-        # Run build_tc.py from binary package
-        ret = cmd_executer.ChrootRunCommand(
-            options.chromeos_root,
-            misc.GetSetupBoardCommand(
-                options.board, force=options.clobber_board
-            ),
-        )
-        logger.GetLogger().LogFatalIf(ret, "setup_board failed")
-    else:
-        logger.GetLogger().LogOutput(
-            "Did not setup_board " "because it already exists"
-        )
-
-    if options.debug:
-        # Perform 2-step build_packages to build a debug chrome browser.
-
-        # Firstly, build everything that chromeos-chrome depends on normally.
-        if options.rebuild:
-            # Give warning about "--rebuild" and "--debug". Under this combination,
-            # only dependencies of "chromeos-chrome" get rebuilt.
-            logger.GetLogger().LogWarning(
-                '--rebuild" does not correctly re-build every package when '
-                '"--debug" is enabled. '
-            )
-
-            # Replace EXTRA_BOARD_FLAGS=-e with "-e --onlydeps"
-            build_packages_env = build_packages_env.replace(
-                "EXTRA_BOARD_FLAGS=-e", 'EXTRA_BOARD_FLAGS="-e --onlydeps"'
-            )
-        else:
-            build_packages_env += " EXTRA_BOARD_FLAGS=--onlydeps"
-
-        ret = cmd_executer.ChrootRunCommand(
-            options.chromeos_root,
-            'CFLAGS="$(portageq-%s envvar CFLAGS) %s" '
-            'CXXFLAGS="$(portageq-%s envvar CXXFLAGS) %s" '
-            'LDFLAGS="$(portageq-%s envvar LDFLAGS) %s" '
-            "CHROME_ORIGIN=SERVER_SOURCE "
-            "%s "
-            "%s --skip_chroot_upgrade"
-            "chromeos-chrome"
-            % (
-                options.board,
-                options.cflags,
-                options.board,
-                options.cxxflags,
-                options.board,
-                options.ldflags,
-                build_packages_env,
-                build_packages_command,
-            ),
-        )
-
-        logger.GetLogger().LogFatalIf(
-            ret,
-            "build_packages failed while trying to build chromeos-chrome deps.",
-        )
-
-        # Secondly, build chromeos-chrome using debug mode.
-        # Replace '--onlydeps' with '--nodeps'.
-        if options.rebuild:
-            build_packages_env = build_packages_env.replace(
-                'EXTRA_BOARD_FLAGS="-e --onlydeps"',
-                "EXTRA_BOARD_FLAGS=--nodeps",
-            )
-        else:
-            build_packages_env = build_packages_env.replace(
-                "EXTRA_BOARD_FLAGS=--onlydeps", "EXTRA_BOARD_FLAGS=--nodeps"
-            )
-        ret = cmd_executer.ChrootRunCommand(
-            options.chromeos_root,
-            'CFLAGS="$(portageq-%s envvar CFLAGS) %s" '
-            'CXXFLAGS="$(portageq-%s envvar CXXFLAGS) %s" '
-            'LDFLAGS="$(portageq-%s envvar LDFLAGS) %s" '
-            "CHROME_ORIGIN=SERVER_SOURCE BUILDTYPE=Debug "
-            "%s "
-            "%s --skip_chroot_upgrade"
-            "chromeos-chrome"
-            % (
-                options.board,
-                options.cflags,
-                options.board,
-                options.cxxflags,
-                options.board,
-                options.ldflags,
-                build_packages_env,
-                build_packages_command,
-            ),
-        )
-        logger.GetLogger().LogFatalIf(
-            ret,
-            "build_packages failed while trying to build debug chromeos-chrome.",
-        )
-
-        # Now, we have built chromeos-chrome and all dependencies.
-        # Finally, remove '-e' from EXTRA_BOARD_FLAGS,
-        # otherwise, chromeos-chrome gets rebuilt.
-        build_packages_env = build_packages_env.replace(
-            "EXTRA_BOARD_FLAGS=--nodeps", ""
-        )
-
-        # Up to now, we have a debug built chromos-chrome browser.
-        # Fall through to build the rest of the world.
-
-    # Build packages
-    ret = cmd_executer.ChrootRunCommand(
-        options.chromeos_root,
-        'CFLAGS="$(portageq-%s envvar CFLAGS) %s" '
-        'CXXFLAGS="$(portageq-%s envvar CXXFLAGS) %s" '
-        'LDFLAGS="$(portageq-%s envvar LDFLAGS) %s" '
-        "CHROME_ORIGIN=SERVER_SOURCE "
-        "%s "
-        "%s --skip_chroot_upgrade"
-        % (
-            options.board,
-            options.cflags,
-            options.board,
-            options.cxxflags,
-            options.board,
-            options.ldflags,
-            build_packages_env,
-            build_packages_command,
-        ),
-    )
-
-    logger.GetLogger().LogFatalIf(ret, "build_packages failed")
-    if options.package:
-        return 0
-    # Build image
-    ret = cmd_executer.ChrootRunCommand(
-        options.chromeos_root, build_packages_env + " " + build_image_command
-    )
-
-    logger.GetLogger().LogFatalIf(ret, "build_image failed")
-
-    flags_file_name = "flags.txt"
-    flags_file_path = "%s/src/build/images/%s/latest/%s" % (
-        options.chromeos_root,
-        options.board,
-        flags_file_name,
-    )
-    with open(flags_file_path, "w", encoding="utf-8") as flags_file:
-        flags_file.write("CFLAGS=%s\n" % options.cflags)
-        flags_file.write("CXXFLAGS=%s\n" % options.cxxflags)
-        flags_file.write("LDFLAGS=%s\n" % options.ldflags)
-
-    if options.label:
-        image_dir_path = "%s/src/build/images/%s/latest" % (
-            options.chromeos_root,
-            options.board,
-        )
-        real_image_dir_path = os.path.realpath(image_dir_path)
-        command = "ln -sf -T %s %s/%s" % (
-            os.path.basename(real_image_dir_path),
-            os.path.dirname(real_image_dir_path),
-            options.label,
-        )
-
-        ret = cmd_executer.RunCommand(command)
-        logger.GetLogger().LogFatalIf(
-            ret, "Failed to apply symlink label %s" % options.label
-        )
-
-    return ret
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv)
-    sys.exit(retval)
diff --git a/build_tc.py b/build_tc.py
deleted file mode 100755
index 08f80e69..00000000
--- a/build_tc.py
+++ /dev/null
@@ -1,417 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2010 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Script to build the ChromeOS toolchain.
-
-This script sets up the toolchain if you give it the gcctools directory.
-"""
-
-
-__author__ = "asharif@google.com (Ahmad Sharif)"
-
-import argparse
-import getpass
-import os
-import sys
-import tempfile
-
-from cros_utils import command_executer
-from cros_utils import constants
-from cros_utils import misc
-import tc_enter_chroot
-
-
-class ToolchainPart(object):
-    """Class to hold the toolchain pieces."""
-
-    def __init__(
-        self,
-        name,
-        source_path,
-        chromeos_root,
-        board,
-        incremental,
-        build_env,
-        gcc_enable_ccache=False,
-    ):
-        self._name = name
-        self._source_path = misc.CanonicalizePath(source_path)
-        self._chromeos_root = chromeos_root
-        self._board = board
-        self._ctarget = misc.GetCtargetFromBoard(
-            self._board, self._chromeos_root
-        )
-        self._gcc_libs_dest = misc.GetGccLibsDestForBoard(
-            self._board, self._chromeos_root
-        )
-        self.tag = "%s-%s" % (name, self._ctarget)
-        self._ce = command_executer.GetCommandExecuter()
-        self._mask_file = os.path.join(
-            self._chromeos_root,
-            "chroot",
-            "etc/portage/package.mask/cross-%s" % self._ctarget,
-        )
-        self._new_mask_file = None
-
-        self._chroot_source_path = os.path.join(
-            constants.MOUNTED_TOOLCHAIN_ROOT, self._name
-        ).lstrip("/")
-        self._incremental = incremental
-        self._build_env = build_env
-        self._gcc_enable_ccache = gcc_enable_ccache
-
-    def RunSetupBoardIfNecessary(self):
-        cross_symlink = os.path.join(
-            self._chromeos_root,
-            "chroot",
-            "usr/local/bin/emerge-%s" % self._board,
-        )
-        if not os.path.exists(cross_symlink):
-            command = "setup_board --board=%s" % self._board
-            self._ce.ChrootRunCommand(self._chromeos_root, command)
-
-    def Build(self):
-        rv = 1
-        try:
-            self.UninstallTool()
-            self.MoveMaskFile()
-            self.MountSources(False)
-            self.RemoveCompiledFile()
-            rv = self.BuildTool()
-        finally:
-            self.UnMoveMaskFile()
-        return rv
-
-    def RemoveCompiledFile(self):
-        compiled_file = os.path.join(
-            self._chromeos_root,
-            "chroot",
-            "var/tmp/portage/cross-%s" % self._ctarget,
-            "%s-9999" % self._name,
-            ".compiled",
-        )
-        command = "rm -f %s" % compiled_file
-        self._ce.RunCommand(command)
-
-    def MountSources(self, unmount_source):
-        mount_points = []
-        mounted_source_path = os.path.join(
-            self._chromeos_root, "chroot", self._chroot_source_path
-        )
-        src_mp = tc_enter_chroot.MountPoint(
-            self._source_path, mounted_source_path, getpass.getuser(), "ro"
-        )
-        mount_points.append(src_mp)
-
-        build_suffix = "build-%s" % self._ctarget
-        build_dir = "%s-%s" % (self._source_path, build_suffix)
-
-        if not self._incremental and os.path.exists(build_dir):
-            command = "rm -rf %s/*" % build_dir
-            self._ce.RunCommand(command)
-
-        # Create a -build directory for the objects.
-        command = "mkdir -p %s" % build_dir
-        self._ce.RunCommand(command)
-
-        mounted_build_dir = os.path.join(
-            self._chromeos_root,
-            "chroot",
-            "%s-%s" % (self._chroot_source_path, build_suffix),
-        )
-        build_mp = tc_enter_chroot.MountPoint(
-            build_dir, mounted_build_dir, getpass.getuser()
-        )
-        mount_points.append(build_mp)
-
-        if unmount_source:
-            unmount_statuses = [mp.UnMount() == 0 for mp in mount_points]
-            assert all(unmount_statuses), "Could not unmount all mount points!"
-        else:
-            mount_statuses = [mp.DoMount() == 0 for mp in mount_points]
-
-            if not all(mount_statuses):
-                mounted = [
-                    mp
-                    for mp, status in zip(mount_points, mount_statuses)
-                    if status
-                ]
-                unmount_statuses = [mp.UnMount() == 0 for mp in mounted]
-                assert all(
-                    unmount_statuses
-                ), "Could not unmount all mount points!"
-
-    def UninstallTool(self):
-        command = "sudo CLEAN_DELAY=0 emerge -C cross-%s/%s" % (
-            self._ctarget,
-            self._name,
-        )
-        self._ce.ChrootRunCommand(self._chromeos_root, command)
-
-    def BuildTool(self):
-        env = self._build_env
-        # FEATURES=buildpkg adds minutes of time so we disable it.
-        # TODO(shenhan): keep '-sandbox' for a while for compatibility, then remove
-        # it after a while.
-        features = (
-            "nostrip userpriv userfetch -usersandbox -sandbox noclean "
-            "-buildpkg"
-        )
-        env["FEATURES"] = features
-
-        if self._incremental:
-            env["FEATURES"] += " keepwork"
-
-        if "USE" in env:
-            env["USE"] += " multislot mounted_%s" % self._name
-        else:
-            env["USE"] = "multislot mounted_%s" % self._name
-
-        # Disable ccache in our compilers. cache may be problematic for us.
-        # It ignores compiler environments settings and it is not clear if
-        # the cache hit algorithm verifies all the compiler binaries or
-        # just the driver.
-        if self._name == "gcc" and not self._gcc_enable_ccache:
-            env["USE"] += " -wrapper_ccache"
-
-        env["%s_SOURCE_PATH" % self._name.upper()] = os.path.join(
-            "/", self._chroot_source_path
-        )
-        env["ACCEPT_KEYWORDS"] = "~*"
-        env_string = " ".join(['%s="%s"' % var for var in env.items()])
-        command = "emerge =cross-%s/%s-9999" % (self._ctarget, self._name)
-        full_command = "sudo %s %s" % (env_string, command)
-        rv = self._ce.ChrootRunCommand(self._chromeos_root, full_command)
-        if rv != 0:
-            return rv
-        if self._name == "gcc":
-            command = "sudo cp -r /usr/lib/gcc/%s %s" % (
-                self._ctarget,
-                self._gcc_libs_dest,
-            )
-            rv = self._ce.ChrootRunCommand(self._chromeos_root, command)
-        return rv
-
-    def MoveMaskFile(self):
-        self._new_mask_file = None
-        if os.path.isfile(self._mask_file):
-            self._new_mask_file = tempfile.mktemp()
-            command = "sudo mv %s %s" % (self._mask_file, self._new_mask_file)
-            self._ce.RunCommand(command)
-
-    def UnMoveMaskFile(self):
-        if self._new_mask_file:
-            command = "sudo mv %s %s" % (self._new_mask_file, self._mask_file)
-            self._ce.RunCommand(command)
-
-
-def Main(argv):
-    """The main function."""
-    # Common initializations
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "-c",
-        "--chromeos_root",
-        dest="chromeos_root",
-        default="../../",
-        help=("ChromeOS root checkout directory" " uses ../.. if none given."),
-    )
-    parser.add_argument(
-        "-g",
-        "--gcc_dir",
-        dest="gcc_dir",
-        help="The directory where gcc resides.",
-    )
-    parser.add_argument(
-        "--binutils_dir",
-        dest="binutils_dir",
-        help="The directory where binutils resides.",
-    )
-    parser.add_argument(
-        "-x",
-        "--gdb_dir",
-        dest="gdb_dir",
-        help="The directory where gdb resides.",
-    )
-    parser.add_argument(
-        "-b",
-        "--board",
-        dest="board",
-        default="x86-alex",
-        help="The target board.",
-    )
-    parser.add_argument(
-        "-n",
-        "--noincremental",
-        dest="noincremental",
-        default=False,
-        action="store_true",
-        help="Use FEATURES=keepwork to do incremental builds.",
-    )
-    parser.add_argument(
-        "--cflags",
-        dest="cflags",
-        default="",
-        help="Build a compiler with specified CFLAGS",
-    )
-    parser.add_argument(
-        "--cxxflags",
-        dest="cxxflags",
-        default="",
-        help="Build a compiler with specified CXXFLAGS",
-    )
-    parser.add_argument(
-        "--cflags_for_target",
-        dest="cflags_for_target",
-        default="",
-        help="Build the target libraries with specified flags",
-    )
-    parser.add_argument(
-        "--cxxflags_for_target",
-        dest="cxxflags_for_target",
-        default="",
-        help="Build the target libraries with specified flags",
-    )
-    parser.add_argument(
-        "--ldflags",
-        dest="ldflags",
-        default="",
-        help="Build a compiler with specified LDFLAGS",
-    )
-    parser.add_argument(
-        "-d",
-        "--debug",
-        dest="debug",
-        default=False,
-        action="store_true",
-        help="Build a compiler with -g3 -O0 appended to both"
-        " CFLAGS and CXXFLAGS.",
-    )
-    parser.add_argument(
-        "-m",
-        "--mount_only",
-        dest="mount_only",
-        default=False,
-        action="store_true",
-        help="Just mount the tool directories.",
-    )
-    parser.add_argument(
-        "-u",
-        "--unmount_only",
-        dest="unmount_only",
-        default=False,
-        action="store_true",
-        help="Just unmount the tool directories.",
-    )
-    parser.add_argument(
-        "--extra_use_flags",
-        dest="extra_use_flags",
-        default="",
-        help="Extra flag for USE, to be passed to the ebuild. "
-        "('multislot' and 'mounted_<tool>' are always passed.)",
-    )
-    parser.add_argument(
-        "--gcc_enable_ccache",
-        dest="gcc_enable_ccache",
-        default=False,
-        action="store_true",
-        help="Enable ccache for the gcc invocations",
-    )
-
-    options = parser.parse_args(argv)
-
-    chromeos_root = misc.CanonicalizePath(options.chromeos_root)
-    if options.gcc_dir:
-        gcc_dir = misc.CanonicalizePath(options.gcc_dir)
-        assert gcc_dir and os.path.isdir(gcc_dir), "gcc_dir does not exist!"
-    if options.binutils_dir:
-        binutils_dir = misc.CanonicalizePath(options.binutils_dir)
-        assert os.path.isdir(binutils_dir), "binutils_dir does not exist!"
-    if options.gdb_dir:
-        gdb_dir = misc.CanonicalizePath(options.gdb_dir)
-        assert os.path.isdir(gdb_dir), "gdb_dir does not exist!"
-    if options.unmount_only:
-        options.mount_only = False
-    elif options.mount_only:
-        options.unmount_only = False
-    build_env = {}
-    if options.cflags:
-        build_env["CFLAGS"] = "`portageq envvar CFLAGS` " + options.cflags
-    if options.cxxflags:
-        build_env["CXXFLAGS"] = "`portageq envvar CXXFLAGS` " + options.cxxflags
-    if options.cflags_for_target:
-        build_env["CFLAGS_FOR_TARGET"] = options.cflags_for_target
-    if options.cxxflags_for_target:
-        build_env["CXXFLAGS_FOR_TARGET"] = options.cxxflags_for_target
-    if options.ldflags:
-        build_env["LDFLAGS"] = options.ldflags
-    if options.debug:
-        debug_flags = "-g3 -O0"
-        if "CFLAGS" in build_env:
-            build_env["CFLAGS"] += " %s" % (debug_flags)
-        else:
-            build_env["CFLAGS"] = debug_flags
-        if "CXXFLAGS" in build_env:
-            build_env["CXXFLAGS"] += " %s" % (debug_flags)
-        else:
-            build_env["CXXFLAGS"] = debug_flags
-    if options.extra_use_flags:
-        build_env["USE"] = options.extra_use_flags
-
-    # Create toolchain parts
-    toolchain_parts = {}
-    for board in options.board.split(","):
-        if options.gcc_dir:
-            tp = ToolchainPart(
-                "gcc",
-                gcc_dir,
-                chromeos_root,
-                board,
-                not options.noincremental,
-                build_env,
-                options.gcc_enable_ccache,
-            )
-            toolchain_parts[tp.tag] = tp
-            tp.RunSetupBoardIfNecessary()
-        if options.binutils_dir:
-            tp = ToolchainPart(
-                "binutils",
-                binutils_dir,
-                chromeos_root,
-                board,
-                not options.noincremental,
-                build_env,
-            )
-            toolchain_parts[tp.tag] = tp
-            tp.RunSetupBoardIfNecessary()
-        if options.gdb_dir:
-            tp = ToolchainPart(
-                "gdb",
-                gdb_dir,
-                chromeos_root,
-                board,
-                not options.noincremental,
-                build_env,
-            )
-            toolchain_parts[tp.tag] = tp
-            tp.RunSetupBoardIfNecessary()
-
-    rv = 0
-    try:
-        for tag in toolchain_parts:
-            tp = toolchain_parts[tag]
-            if options.mount_only or options.unmount_only:
-                tp.MountSources(options.unmount_only)
-            else:
-                rv = rv + tp.Build()
-    finally:
-        print("Exiting...")
-    return rv
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv[1:])
-    sys.exit(retval)
diff --git a/buildbot_test_toolchains.py b/buildbot_test_toolchains.py
deleted file mode 100755
index 88ab4052..00000000
--- a/buildbot_test_toolchains.py
+++ /dev/null
@@ -1,427 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Copyright 2016 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Script for running nightly compiler tests on ChromeOS.
-
-This script launches a buildbot to build ChromeOS with the latest compiler on
-a particular board; then it finds and downloads the trybot image and the
-corresponding official image, and runs crosperf performance tests comparing
-the two.  It then generates a report, emails it to the c-compiler-chrome, as
-well as copying the images into the seven-day reports directory.
-"""
-
-# Script to test different toolchains against ChromeOS benchmarks.
-
-
-import argparse
-import datetime
-import os
-import re
-import shutil
-import sys
-import time
-
-from cros_utils import buildbot_utils
-from cros_utils import command_executer
-from cros_utils import logger
-
-
-CROSTC_ROOT = "/usr/local/google/crostc"
-NIGHTLY_TESTS_DIR = os.path.join(CROSTC_ROOT, "nightly-tests")
-ROLE_ACCOUNT = "mobiletc-prebuild"
-TOOLCHAIN_DIR = os.path.dirname(os.path.realpath(__file__))
-TMP_TOOLCHAIN_TEST = "/tmp/toolchain-tests"
-MAIL_PROGRAM = "~/var/bin/mail-detective"
-PENDING_ARCHIVES_DIR = os.path.join(CROSTC_ROOT, "pending_archives")
-NIGHTLY_TESTS_RESULTS = os.path.join(CROSTC_ROOT, "nightly_test_reports")
-
-IMAGE_DIR = "{board}-{image_type}"
-IMAGE_VERSION_STR = r"{chrome_version}-{tip}\.{branch}\.{branch_branch}"
-IMAGE_FS = IMAGE_DIR + "/" + IMAGE_VERSION_STR
-TRYBOT_IMAGE_FS = IMAGE_FS + "-{build_id}"
-IMAGE_RE_GROUPS = {
-    "board": r"(?P<board>\S+)",
-    "image_type": r"(?P<image_type>\S+)",
-    "chrome_version": r"(?P<chrome_version>R\d+)",
-    "tip": r"(?P<tip>\d+)",
-    "branch": r"(?P<branch>\d+)",
-    "branch_branch": r"(?P<branch_branch>\d+)",
-    "build_id": r"(?P<build_id>b\d+)",
-}
-TRYBOT_IMAGE_RE = TRYBOT_IMAGE_FS.format(**IMAGE_RE_GROUPS)
-
-RECIPE_IMAGE_FS = IMAGE_FS + "-{build_id}-{buildbucket_id}"
-RECIPE_IMAGE_RE_GROUPS = {
-    "board": r"(?P<board>\S+)",
-    "image_type": r"(?P<image_type>\S+)",
-    "chrome_version": r"(?P<chrome_version>R\d+)",
-    "tip": r"(?P<tip>\d+)",
-    "branch": r"(?P<branch>\d+)",
-    "branch_branch": r"(?P<branch_branch>\d+)",
-    "build_id": r"(?P<build_id>\d+)",
-    "buildbucket_id": r"(?P<buildbucket_id>\d+)",
-}
-RECIPE_IMAGE_RE = RECIPE_IMAGE_FS.format(**RECIPE_IMAGE_RE_GROUPS)
-
-# CL that uses LLVM-Next to build the images (includes chrome).
-USE_LLVM_NEXT_PATCH = "513590"
-
-
-class ToolchainComparator(object):
-    """Class for doing the nightly tests work."""
-
-    def __init__(
-        self,
-        board,
-        remotes,
-        chromeos_root,
-        weekday,
-        patches,
-        recipe=False,
-        test=False,
-        noschedv2=False,
-        chrome_src="",
-    ):
-        self._board = board
-        self._remotes = remotes
-        self._chromeos_root = chromeos_root
-        self._chrome_src = chrome_src
-        self._base_dir = os.getcwd()
-        self._ce = command_executer.GetCommandExecuter()
-        self._l = logger.GetLogger()
-        self._build = "%s-release-tryjob" % board
-        self._patches = patches.split(",") if patches else []
-        self._patches_string = "_".join(str(p) for p in self._patches)
-        self._recipe = recipe
-        self._test = test
-        self._noschedv2 = noschedv2
-
-        if not weekday:
-            self._weekday = time.strftime("%a")
-        else:
-            self._weekday = weekday
-        self._date = datetime.date.today().strftime("%Y/%m/%d")
-        timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H:%M:%S")
-        self._reports_dir = os.path.join(
-            TMP_TOOLCHAIN_TEST if self._test else NIGHTLY_TESTS_RESULTS,
-            "%s.%s" % (timestamp, board),
-        )
-
-    def _GetVanillaImageName(self, trybot_image):
-        """Given a trybot artifact name, get latest vanilla image name.
-
-        Args:
-          trybot_image: artifact name such as
-            'daisy-release-tryjob/R40-6394.0.0-b1389'
-            for recipe images, name is in this format:
-            'lulu-llvm-next-nightly/R84-13037.0.0-31011-8883172717979984032/'
-
-        Returns:
-          Latest official image name, e.g. 'daisy-release/R57-9089.0.0'.
-        """
-        # For board names with underscores, we need to fix the trybot image name
-        # to replace the hyphen (for the recipe builder) with the underscore.
-        # Currently the only such board we use is 'veyron_tiger'.
-        if trybot_image.find("veyron-tiger") != -1:
-            trybot_image = trybot_image.replace("veyron-tiger", "veyron_tiger")
-        # We need to filter out -tryjob in the trybot_image.
-        if self._recipe:
-            trybot = re.sub("-llvm-next-nightly", "-release", trybot_image)
-            mo = re.search(RECIPE_IMAGE_RE, trybot)
-        else:
-            trybot = re.sub("-tryjob", "", trybot_image)
-            mo = re.search(TRYBOT_IMAGE_RE, trybot)
-        assert mo
-        dirname = IMAGE_DIR.replace("\\", "").format(**mo.groupdict())
-        return buildbot_utils.GetLatestImage(self._chromeos_root, dirname)
-
-    def _TestImages(self, trybot_image, vanilla_image):
-        """Create crosperf experiment file.
-
-        Given the names of the trybot, vanilla and non-AFDO images, create the
-        appropriate crosperf experiment file and launch crosperf on it.
-        """
-        if self._test:
-            experiment_file_dir = TMP_TOOLCHAIN_TEST
-        else:
-            experiment_file_dir = os.path.join(NIGHTLY_TESTS_DIR, self._weekday)
-        experiment_file_name = "%s_toolchain_experiment.txt" % self._board
-
-        compiler_string = "llvm"
-        if USE_LLVM_NEXT_PATCH in self._patches_string:
-            experiment_file_name = "%s_llvm_next_experiment.txt" % self._board
-            compiler_string = "llvm_next"
-
-        experiment_file = os.path.join(
-            experiment_file_dir, experiment_file_name
-        )
-        experiment_header = """
-    board: %s
-    remote: %s
-    retries: 1
-    """ % (
-            self._board,
-            self._remotes,
-        )
-        # TODO(b/244607231): Add graphic benchmarks removed in crrev.com/c/3869851.
-        experiment_tests = """
-    benchmark: all_toolchain_perf {
-      suite: telemetry_Crosperf
-      iterations: 5
-      run_local: False
-    }
-
-    benchmark: loading.desktop {
-      suite: telemetry_Crosperf
-      test_args: --story-tag-filter=typical
-      iterations: 3
-      run_local: False
-    }
-
-    benchmark: platform.ReportDiskUsage {
-      suite: tast
-      iterations: 1
-      run_local: False
-    }
-    """
-
-        with open(experiment_file, "w", encoding="utf-8") as f:
-            f.write(experiment_header)
-            f.write(experiment_tests)
-
-            # Now add vanilla to test file.
-            official_image = """
-      vanilla_image {
-        chromeos_root: %s
-        chrome_src: %s
-        build: %s
-        compiler: llvm
-      }
-      """ % (
-                self._chromeos_root,
-                self._chrome_src,
-                vanilla_image,
-            )
-            f.write(official_image)
-
-            label_string = "%s_trybot_image" % compiler_string
-
-            # Reuse autotest files from vanilla image for trybot images
-            autotest_files = os.path.join(
-                "/tmp", vanilla_image, "autotest_files"
-            )
-            experiment_image = """
-      %s {
-        chromeos_root: %s
-        chrome_src: %s
-        build: %s
-        autotest_path: %s
-        compiler: %s
-      }
-      """ % (
-                label_string,
-                self._chromeos_root,
-                self._chrome_src,
-                trybot_image,
-                autotest_files,
-                compiler_string,
-            )
-            f.write(experiment_image)
-
-        crosperf = os.path.join(TOOLCHAIN_DIR, "crosperf", "crosperf")
-        noschedv2_opts = "--noschedv2" if self._noschedv2 else ""
-        no_email = not self._test
-        command = (
-            f"{crosperf} --no_email={no_email} "
-            f"--results_dir={self._reports_dir} --logging_level=verbose "
-            f"--json_report=True {noschedv2_opts} {experiment_file}"
-        )
-
-        return self._ce.RunCommand(command)
-
-    def _SendEmail(self):
-        """Find email message generated by crosperf and send it."""
-        filename = os.path.join(self._reports_dir, "msg_body.html")
-        if os.path.exists(filename) and os.path.exists(
-            os.path.expanduser(MAIL_PROGRAM)
-        ):
-            email_title = "buildbot llvm test results"
-            if USE_LLVM_NEXT_PATCH in self._patches_string:
-                email_title = "buildbot llvm_next test results"
-            command = 'cat %s | %s -s "%s, %s %s" -team -html' % (
-                filename,
-                MAIL_PROGRAM,
-                email_title,
-                self._board,
-                self._date,
-            )
-            self._ce.RunCommand(command)
-
-    def _CopyJson(self):
-        # Make sure a destination directory exists.
-        os.makedirs(PENDING_ARCHIVES_DIR, exist_ok=True)
-        # Copy json report to pending archives directory.
-        command = "cp %s/*.json %s/." % (
-            self._reports_dir,
-            PENDING_ARCHIVES_DIR,
-        )
-        ret = self._ce.RunCommand(command)
-        # Failing to access json report means that crosperf terminated or all tests
-        # failed, raise an error.
-        if ret != 0:
-            raise RuntimeError(
-                "Crosperf failed to run tests, cannot copy json report!"
-            )
-
-    def DoAll(self):
-        """Main function inside ToolchainComparator class.
-
-        Launch trybot, get image names, create crosperf experiment file, run
-        crosperf, and copy images into seven-day report directories.
-        """
-        if self._recipe:
-            print("Using recipe buckets to get latest image.")
-            # crbug.com/1077313: Some boards are not consistently
-            # spelled, having underscores in some places and dashes in others.
-            # The image directories consistenly use dashes, so convert underscores
-            # to dashes to work around this.
-            trybot_image = buildbot_utils.GetLatestRecipeImage(
-                self._chromeos_root,
-                "%s-llvm-next-nightly" % self._board.replace("_", "-"),
-            )
-        else:
-            # Launch tryjob and wait to get image location.
-            buildbucket_id, trybot_image = buildbot_utils.GetTrybotImage(
-                self._chromeos_root,
-                self._build,
-                self._patches,
-                tryjob_flags=["--notests"],
-                build_toolchain=True,
-            )
-            print(
-                "trybot_url: \
-            http://cros-goldeneye/chromeos/healthmonitoring/buildDetails?buildbucketId=%s"
-                % buildbucket_id
-            )
-
-        if not trybot_image:
-            self._l.LogError("Unable to find trybot_image!")
-            return 2
-
-        vanilla_image = self._GetVanillaImageName(trybot_image)
-
-        print("trybot_image: %s" % trybot_image)
-        print("vanilla_image: %s" % vanilla_image)
-
-        ret = self._TestImages(trybot_image, vanilla_image)
-        # Always try to send report email as crosperf will generate report when
-        # tests partially succeeded.
-        if not self._test:
-            self._SendEmail()
-            self._CopyJson()
-        # Non-zero ret here means crosperf tests partially failed, raise error here
-        # so that toolchain summary report can catch it.
-        if ret != 0:
-            raise RuntimeError("Crosperf tests partially failed!")
-
-        return 0
-
-
-def Main(argv):
-    """The main function."""
-
-    # Common initializations
-    command_executer.InitCommandExecuter()
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "--remote", dest="remote", help="Remote machines to run tests on."
-    )
-    parser.add_argument(
-        "--board", dest="board", default="x86-zgb", help="The target board."
-    )
-    parser.add_argument(
-        "--chromeos_root",
-        dest="chromeos_root",
-        help="The chromeos root from which to run tests.",
-    )
-    parser.add_argument(
-        "--chrome_src",
-        dest="chrome_src",
-        default="",
-        help="The path to the source of chrome. "
-        "This is used to run telemetry benchmarks. "
-        "The default one is the src inside chroot.",
-    )
-    parser.add_argument(
-        "--weekday",
-        default="",
-        dest="weekday",
-        help="The day of the week for which to run tests.",
-    )
-    parser.add_argument(
-        "--patch",
-        dest="patches",
-        help="The patches to use for the testing, "
-        "seprate the patch numbers with ',' "
-        "for more than one patches.",
-    )
-    parser.add_argument(
-        "--noschedv2",
-        dest="noschedv2",
-        action="store_true",
-        default=False,
-        help="Pass --noschedv2 to crosperf.",
-    )
-    parser.add_argument(
-        "--recipe",
-        dest="recipe",
-        default=True,
-        help="Use images generated from recipe rather than"
-        "launching tryjob to get images.",
-    )
-    parser.add_argument(
-        "--test",
-        dest="test",
-        default=False,
-        help="Test this script on local desktop, "
-        "disabling mobiletc checking and email sending."
-        "Artifacts stored in /tmp/toolchain-tests",
-    )
-
-    options = parser.parse_args(argv[1:])
-    if not options.board:
-        print("Please give a board.")
-        return 1
-    if not options.remote:
-        print("Please give at least one remote machine.")
-        return 1
-    if not options.chromeos_root:
-        print("Please specify the ChromeOS root directory.")
-        return 1
-    if options.test:
-        print("Cleaning local test directory for this script.")
-        if os.path.exists(TMP_TOOLCHAIN_TEST):
-            shutil.rmtree(TMP_TOOLCHAIN_TEST)
-        os.mkdir(TMP_TOOLCHAIN_TEST)
-
-    fc = ToolchainComparator(
-        options.board,
-        options.remote,
-        options.chromeos_root,
-        options.weekday,
-        options.patches,
-        options.recipe,
-        options.test,
-        options.noschedv2,
-        chrome_src=options.chrome_src,
-    )
-    return fc.DoAll()
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv)
-    sys.exit(retval)
diff --git a/check_portable_toolchains.py b/check_portable_toolchains.py
old mode 100755
new mode 100644
index 3e3bce8d..ab9f8099
--- a/check_portable_toolchains.py
+++ b/check_portable_toolchains.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2023 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -16,7 +15,6 @@ import os
 from pathlib import Path
 import re
 import subprocess
-import sys
 import tempfile
 from typing import List, Optional, Tuple
 
@@ -163,7 +161,9 @@ def _autodetect_latest_llvm_next_sdk_version() -> str:
             "1",
             "-status",
             "success",
-            "chromeos/infra/build-chromiumos-sdk-llvm-next",
+            "-t",
+            "toolchain:non-cq-llvm-next-testing",
+            "chromeos/staging/staging-build-chromiumos-sdk",
         ],
         check=True,
         stdin=subprocess.DEVNULL,
@@ -218,7 +218,3 @@ def parse_args() -> argparse.Namespace:
         help="Top level gs:// path. (default: %(default)s)",
     )
     return parser.parse_args()
-
-
-if __name__ == "__main__":
-    sys.exit(main())
diff --git a/chromiumos_image_diff.py b/chromiumos_image_diff.py
deleted file mode 100755
index ed840cb0..00000000
--- a/chromiumos_image_diff.py
+++ /dev/null
@@ -1,418 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Diff 2 chromiumos images by comparing each elf file.
-
-   The script diffs every *ELF* files by dissembling every *executable*
-   section, which means it is not a FULL elf differ.
-
-   A simple usage example -
-     chromiumos_image_diff.py --image1 image-path-1 --image2 image-path-2
-
-   Note that image path should be inside the chroot, if not (ie, image is
-   downloaded from web), please specify a chromiumos checkout via
-   "--chromeos_root".
-
-   And this script should be executed outside chroot.
-"""
-
-
-__author__ = "shenhan@google.com (Han Shen)"
-
-import argparse
-import os
-import re
-import sys
-import tempfile
-
-from cros_utils import command_executer
-from cros_utils import logger
-from cros_utils import misc
-import image_chromeos
-
-
-class CrosImage(object):
-    """A cros image object."""
-
-    def __init__(self, image, chromeos_root, no_unmount):
-        self.image = image
-        self.chromeos_root = chromeos_root
-        self.mounted = False
-        self._ce = command_executer.GetCommandExecuter()
-        self.logger = logger.GetLogger()
-        self.elf_files = []
-        self.no_unmount = no_unmount
-        self.unmount_script = ""
-        self.stateful = ""
-        self.rootfs = ""
-
-    def MountImage(self, mount_basename):
-        """Mount/unpack the image."""
-
-        if mount_basename:
-            self.rootfs = "/tmp/{0}.rootfs".format(mount_basename)
-            self.stateful = "/tmp/{0}.stateful".format(mount_basename)
-            self.unmount_script = "/tmp/{0}.unmount.sh".format(mount_basename)
-        else:
-            self.rootfs = tempfile.mkdtemp(
-                suffix=".rootfs", prefix="chromiumos_image_diff"
-            )
-            ## rootfs is like /tmp/tmpxyz012.rootfs.
-            match = re.match(r"^(.*)\.rootfs$", self.rootfs)
-            basename = match.group(1)
-            self.stateful = basename + ".stateful"
-            os.mkdir(self.stateful)
-            self.unmount_script = "{0}.unmount.sh".format(basename)
-
-        self.logger.LogOutput(
-            'Mounting "{0}" onto "{1}" and "{2}"'.format(
-                self.image, self.rootfs, self.stateful
-            )
-        )
-        ## First of all creating an unmount image
-        self.CreateUnmountScript()
-        command = image_chromeos.GetImageMountCommand(
-            self.image, self.rootfs, self.stateful
-        )
-        rv = self._ce.RunCommand(command, print_to_console=True)
-        self.mounted = rv == 0
-        if not self.mounted:
-            self.logger.LogError(
-                'Failed to mount "{0}" onto "{1}" and "{2}".'.format(
-                    self.image, self.rootfs, self.stateful
-                )
-            )
-        return self.mounted
-
-    def CreateUnmountScript(self):
-        command = (
-            "sudo umount {r}/usr/local {r}/usr/share/oem "
-            "{r}/var {r}/mnt/stateful_partition {r}; sudo umount {s} ; "
-            "rmdir {r} ; rmdir {s}\n"
-        ).format(r=self.rootfs, s=self.stateful)
-        f = open(self.unmount_script, "w", encoding="utf-8")
-        f.write(command)
-        f.close()
-        self._ce.RunCommand(
-            "chmod +x {}".format(self.unmount_script), print_to_console=False
-        )
-        self.logger.LogOutput(
-            'Created an unmount script - "{0}"'.format(self.unmount_script)
-        )
-
-    def UnmountImage(self):
-        """Unmount the image and delete mount point."""
-
-        self.logger.LogOutput(
-            'Unmounting image "{0}" from "{1}" and "{2}"'.format(
-                self.image, self.rootfs, self.stateful
-            )
-        )
-        if self.mounted:
-            command = 'bash "{0}"'.format(self.unmount_script)
-            if self.no_unmount:
-                self.logger.LogOutput(
-                    (
-                        "Please unmount manually - \n"
-                        '\t bash "{0}"'.format(self.unmount_script)
-                    )
-                )
-            else:
-                if self._ce.RunCommand(command, print_to_console=True) == 0:
-                    self._ce.RunCommand("rm {0}".format(self.unmount_script))
-                    self.mounted = False
-                    self.rootfs = None
-                    self.stateful = None
-                    self.unmount_script = None
-
-        return not self.mounted
-
-    def FindElfFiles(self):
-        """Find all elf files for the image.
-
-        Returns:
-          Always true
-        """
-
-        self.logger.LogOutput(
-            'Finding all elf files in "{0}" ...'.format(self.rootfs)
-        )
-        # Note '\;' must be prefixed by 'r'.
-        command = (
-            'find "{0}" -type f -exec '
-            'bash -c \'file -b "{{}}" | grep -q "ELF"\''
-            r" \; "
-            r'-exec echo "{{}}" \;'
-        ).format(self.rootfs)
-        self.logger.LogCmd(command)
-        _, out, _ = self._ce.RunCommandWOutput(command, print_to_console=False)
-        self.elf_files = out.splitlines()
-        self.logger.LogOutput(
-            "Total {0} elf files found.".format(len(self.elf_files))
-        )
-        return True
-
-
-class ImageComparator(object):
-    """A class that wraps comparsion actions."""
-
-    def __init__(self, images, diff_file):
-        self.images = images
-        self.logger = logger.GetLogger()
-        self.diff_file = diff_file
-        self.tempf1 = None
-        self.tempf2 = None
-
-    def Cleanup(self):
-        if self.tempf1 and self.tempf2:
-            command_executer.GetCommandExecuter().RunCommand(
-                "rm {0} {1}".format(self.tempf1, self.tempf2)
-            )
-            logger.GetLogger(
-                'Removed "{0}" and "{1}".'.format(self.tempf1, self.tempf2)
-            )
-
-    def CheckElfFileSetEquality(self):
-        """Checking whether images have exactly number of elf files."""
-
-        self.logger.LogOutput("Checking elf file equality ...")
-        i1 = self.images[0]
-        i2 = self.images[1]
-        t1 = i1.rootfs + "/"
-        elfset1 = {e.replace(t1, "") for e in i1.elf_files}
-        t2 = i2.rootfs + "/"
-        elfset2 = {e.replace(t2, "") for e in i2.elf_files}
-        dif1 = elfset1.difference(elfset2)
-        msg = None
-        if dif1:
-            msg = 'The following files are not in "{image}" - "{rootfs}":\n'.format(
-                image=i2.image, rootfs=i2.rootfs
-            )
-            for d in dif1:
-                msg += "\t" + d + "\n"
-        dif2 = elfset2.difference(elfset1)
-        if dif2:
-            msg = 'The following files are not in "{image}" - "{rootfs}":\n'.format(
-                image=i1.image, rootfs=i1.rootfs
-            )
-            for d in dif2:
-                msg += "\t" + d + "\n"
-        if msg:
-            self.logger.LogError(msg)
-            return False
-        return True
-
-    def CompareImages(self):
-        """Do the comparsion work."""
-
-        if not self.CheckElfFileSetEquality():
-            return False
-
-        mismatch_list = []
-        match_count = 0
-        i1 = self.images[0]
-        i2 = self.images[1]
-        self.logger.LogOutput(
-            "Start comparing {0} elf file by file ...".format(len(i1.elf_files))
-        )
-        ## Note - i1.elf_files and i2.elf_files have exactly the same entries here.
-
-        ## Create 2 temp files to be used for all disassembed files.
-        handle, self.tempf1 = tempfile.mkstemp()
-        os.close(handle)  # We do not need the handle
-        handle, self.tempf2 = tempfile.mkstemp()
-        os.close(handle)
-
-        cmde = command_executer.GetCommandExecuter()
-        for elf1 in i1.elf_files:
-            tmp_rootfs = i1.rootfs + "/"
-            f1 = elf1.replace(tmp_rootfs, "")
-            full_path1 = elf1
-            full_path2 = elf1.replace(i1.rootfs, i2.rootfs)
-
-            if full_path1 == full_path2:
-                self.logger.LogError(
-                    "Error:  We're comparing the SAME file - {0}".format(f1)
-                )
-                continue
-
-            command = (
-                'objdump -d "{f1}" > {tempf1} ; '
-                'objdump -d "{f2}" > {tempf2} ; '
-                # Remove path string inside the dissemble
-                "sed -i 's!{rootfs1}!!g' {tempf1} ; "
-                "sed -i 's!{rootfs2}!!g' {tempf2} ; "
-                "diff {tempf1} {tempf2} 1>/dev/null 2>&1"
-            ).format(
-                f1=full_path1,
-                f2=full_path2,
-                rootfs1=i1.rootfs,
-                rootfs2=i2.rootfs,
-                tempf1=self.tempf1,
-                tempf2=self.tempf2,
-            )
-            ret = cmde.RunCommand(command, print_to_console=False)
-            if ret != 0:
-                self.logger.LogOutput(
-                    '*** Not match - "{0}" "{1}"'.format(full_path1, full_path2)
-                )
-                mismatch_list.append(f1)
-                if self.diff_file:
-                    command = (
-                        'echo "Diffs of disassemble of "{f1}" and "{f2}"" '
-                        ">> {diff_file} ; diff {tempf1} {tempf2} "
-                        ">> {diff_file}"
-                    ).format(
-                        f1=full_path1,
-                        f2=full_path2,
-                        diff_file=self.diff_file,
-                        tempf1=self.tempf1,
-                        tempf2=self.tempf2,
-                    )
-                    cmde.RunCommand(command, print_to_console=False)
-            else:
-                match_count += 1
-        ## End of comparing every elf files.
-
-        if not mismatch_list:
-            self.logger.LogOutput(
-                "** COOL, ALL {0} BINARIES MATCHED!! **".format(match_count)
-            )
-            return True
-
-        mismatch_str = "Found {0} mismatch:\n".format(len(mismatch_list))
-        for b in mismatch_list:
-            mismatch_str += "\t" + b + "\n"
-
-        self.logger.LogOutput(mismatch_str)
-        return False
-
-
-def Main(argv):
-    """The main function."""
-
-    command_executer.InitCommandExecuter()
-    images = []
-
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "--no_unmount",
-        action="store_true",
-        dest="no_unmount",
-        default=False,
-        help="Do not unmount after finish, this is useful for debugging.",
-    )
-    parser.add_argument(
-        "--chromeos_root",
-        dest="chromeos_root",
-        default=None,
-        action="store",
-        help=(
-            "[Optional] Specify a chromeos tree instead of "
-            "deducing it from image path so that we can compare "
-            "2 images that are downloaded."
-        ),
-    )
-    parser.add_argument(
-        "--mount_basename",
-        dest="mount_basename",
-        default=None,
-        action="store",
-        help=(
-            "Specify a meaningful name for the mount point. With this being "
-            'set, the mount points would be "/tmp/mount_basename.x.rootfs" '
-            ' and "/tmp/mount_basename.x.stateful". (x is 1 or 2).'
-        ),
-    )
-    parser.add_argument(
-        "--diff_file",
-        dest="diff_file",
-        default=None,
-        help="Dumping all the diffs (if any) to the diff file",
-    )
-    parser.add_argument(
-        "--image1",
-        dest="image1",
-        default=None,
-        required=True,
-        help=("Image 1 file name."),
-    )
-    parser.add_argument(
-        "--image2",
-        dest="image2",
-        default=None,
-        required=True,
-        help=("Image 2 file name."),
-    )
-    options = parser.parse_args(argv[1:])
-
-    if options.mount_basename and options.mount_basename.find("/") >= 0:
-        logger.GetLogger().LogError(
-            '"--mount_basename" must be a name, not a path.'
-        )
-        parser.print_help()
-        return 1
-
-    result = False
-    image_comparator = None
-    try:
-        for i, image_path in enumerate(
-            [options.image1, options.image2], start=1
-        ):
-            image_path = os.path.realpath(image_path)
-            if not os.path.isfile(image_path):
-                logger.GetLogger().LogError(
-                    '"{0}" is not a file.'.format(image_path)
-                )
-                return 1
-
-            chromeos_root = None
-            if options.chromeos_root:
-                chromeos_root = options.chromeos_root
-            else:
-                ## Deduce chromeos root from image
-                t = image_path
-                while t != "/":
-                    if misc.IsChromeOsTree(t):
-                        break
-                    t = os.path.dirname(t)
-                if misc.IsChromeOsTree(t):
-                    chromeos_root = t
-
-            if not chromeos_root:
-                logger.GetLogger().LogError(
-                    "Please provide a valid chromeos root via --chromeos_root"
-                )
-                return 1
-
-            image = CrosImage(image_path, chromeos_root, options.no_unmount)
-
-            if options.mount_basename:
-                mount_basename = "{basename}.{index}".format(
-                    basename=options.mount_basename, index=i
-                )
-            else:
-                mount_basename = None
-
-            if image.MountImage(mount_basename):
-                images.append(image)
-                image.FindElfFiles()
-
-        if len(images) == 2:
-            image_comparator = ImageComparator(images, options.diff_file)
-            result = image_comparator.CompareImages()
-    finally:
-        for image in images:
-            image.UnmountImage()
-        if image_comparator:
-            image_comparator.Cleanup()
-
-    return 0 if result else 1
-
-
-if __name__ == "__main__":
-    Main(sys.argv)
diff --git a/compiler_wrapper/.gitignore b/compiler_wrapper/.gitignore
new file mode 100644
index 00000000..b715f87d
--- /dev/null
+++ b/compiler_wrapper/.gitignore
@@ -0,0 +1,3 @@
+# The `install_compiler_wrapper.sh` script may leave this hanging around;
+# we never actually want to commit the `compiler_wrapper` binary. Leave it.
+compiler_wrapper
diff --git a/compiler_wrapper/android_config_test.go b/compiler_wrapper/android_config_test.go
index 57669f8c..b9f6fa38 100644
--- a/compiler_wrapper/android_config_test.go
+++ b/compiler_wrapper/android_config_test.go
@@ -41,7 +41,6 @@ func TestAndroidConfig(t *testing.T) {
 
 		runGoldenRecords(ctx, androidGoldenDir, []goldenFile{
 			createAndroidClangPathGoldenInputs(ctx),
-			createBisectGoldenInputs(filepath.Join(ctx.tempDir, "clang")),
 			createAndroidCompileWithFallbackGoldenInputs(ctx),
 		})
 	})
diff --git a/compiler_wrapper/bisect_flag.go b/compiler_wrapper/bisect_flag.go
deleted file mode 100644
index 2dc8daf2..00000000
--- a/compiler_wrapper/bisect_flag.go
+++ /dev/null
@@ -1,77 +0,0 @@
-// Copyright 2019 The ChromiumOS Authors
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE file.
-
-package main
-
-import (
-	"errors"
-	"os"
-	"os/exec"
-	"path/filepath"
-)
-
-// Note: We keep this code in python as golang has no builtin
-// shlex function.
-const bisectPythonCommand = `
-import bisect_driver
-import shlex
-import sys
-
-def ExpandArgs(args, target):
-	for arg in args:
-		if arg[0] == '@':
-			with open(arg[1:], 'r', encoding='utf-8') as f:
-				ExpandArgs(shlex.split(f.read()), target)
-		else:
-			target.append(arg)
-	return target
-
-stage = sys.argv[1]
-dir = sys.argv[2]
-execargs = ExpandArgs(sys.argv[3:], [])
-
-sys.exit(bisect_driver.bisect_driver(stage, dir, execargs))
-`
-
-func getBisectStage(env env) string {
-	value, _ := env.getenv("BISECT_STAGE")
-	return value
-}
-
-func calcBisectCommand(env env, cfg *config, bisectStage string, compilerCmd *command) (*command, error) {
-	bisectDir, _ := env.getenv("BISECT_DIR")
-	if bisectDir == "" {
-		if cfg.isAndroidWrapper {
-			homeDir, ok := env.getenv("HOME")
-			if !ok {
-				return nil, errors.New("$HOME is not set")
-			}
-			bisectDir = filepath.Join(homeDir, "ANDROID_BISECT")
-		} else {
-			bisectDir = "/tmp/sysroot_bisect"
-		}
-	}
-	absCompilerPath := getAbsCmdPath(env, compilerCmd)
-	pythonPath, err := exec.LookPath(os.Args[0])
-	if err != nil {
-		return nil, err
-	}
-	pythonPath, err = filepath.EvalSymlinks(pythonPath)
-	if err != nil {
-		return nil, err
-	}
-	pythonPath = filepath.Dir(pythonPath)
-	return &command{
-		Path: "/usr/bin/env",
-		Args: append([]string{
-			"python3",
-			"-c",
-			bisectPythonCommand,
-			bisectStage,
-			bisectDir,
-			absCompilerPath,
-		}, compilerCmd.Args...),
-		EnvUpdates: append(compilerCmd.EnvUpdates, "PYTHONPATH="+pythonPath),
-	}, nil
-}
diff --git a/compiler_wrapper/bisect_flag_test.go b/compiler_wrapper/bisect_flag_test.go
deleted file mode 100644
index 2071a5b2..00000000
--- a/compiler_wrapper/bisect_flag_test.go
+++ /dev/null
@@ -1,184 +0,0 @@
-// Copyright 2019 The ChromiumOS Authors
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE file.
-
-package main
-
-import (
-	"errors"
-	"fmt"
-	"io"
-	"path/filepath"
-	"strings"
-	"testing"
-)
-
-func TestCallBisectDriver(t *testing.T) {
-	withBisectTestContext(t, func(ctx *testContext) {
-		ctx.env = []string{
-			"BISECT_STAGE=someBisectStage",
-			"BISECT_DIR=someBisectDir",
-		}
-		cmd := mustCallBisectDriver(ctx, callCompiler(ctx, ctx.cfg, ctx.newCommand(gccX86_64, mainCc)))
-		if err := verifyPath(cmd, "bisect_driver"); err != nil {
-			t.Error(err)
-		}
-		if err := verifyArgOrder(cmd,
-			"someBisectStage", "someBisectDir", filepath.Join(ctx.tempDir, gccX86_64+".real"), "--sysroot=.*", mainCc); err != nil {
-			t.Error(err)
-		}
-	})
-}
-
-func TestCallBisectDriverWithParamsFile(t *testing.T) {
-	withBisectTestContext(t, func(ctx *testContext) {
-		ctx.env = []string{
-			"BISECT_STAGE=someBisectStage",
-			"BISECT_DIR=someBisectDir",
-		}
-		paramsFile1 := filepath.Join(ctx.tempDir, "params1")
-		ctx.writeFile(paramsFile1, "a\n#comment\n@params2")
-		paramsFile2 := filepath.Join(ctx.tempDir, "params2")
-		ctx.writeFile(paramsFile2, "b\n"+mainCc)
-
-		cmd := mustCallBisectDriver(ctx, callCompiler(ctx, ctx.cfg, ctx.newCommand(gccX86_64, "@"+paramsFile1)))
-		if err := verifyArgOrder(cmd,
-			"a", "b", mainCc); err != nil {
-			t.Error(err)
-		}
-	})
-}
-
-func TestCallBisectDriverWithCCache(t *testing.T) {
-	withBisectTestContext(t, func(ctx *testContext) {
-		ctx.cfg.useCCache = true
-		cmd := ctx.must(callCompiler(ctx, ctx.cfg, ctx.newCommand(gccX86_64, mainCc)))
-		if err := verifyPath(cmd, "/usr/bin/env"); err != nil {
-			t.Error(err)
-		}
-		if err := verifyArgOrder(cmd, "python3", "/usr/bin/ccache"); err != nil {
-			t.Error(err)
-		}
-		if err := verifyEnvUpdate(cmd, "CCACHE_DIR=.*"); err != nil {
-			t.Error(err)
-		}
-	})
-}
-
-func TestDefaultBisectDirCros(t *testing.T) {
-	withBisectTestContext(t, func(ctx *testContext) {
-		ctx.env = []string{
-			"BISECT_STAGE=someBisectStage",
-		}
-		cmd := mustCallBisectDriver(ctx, callCompiler(ctx, ctx.cfg, ctx.newCommand(gccX86_64, mainCc)))
-		if err := verifyArgOrder(cmd,
-			"someBisectStage", "/tmp/sysroot_bisect"); err != nil {
-			t.Error(err)
-		}
-	})
-}
-
-func TestDefaultBisectDirAndroid(t *testing.T) {
-	withBisectTestContext(t, func(ctx *testContext) {
-		ctx.env = []string{
-			"BISECT_STAGE=someBisectStage",
-			"HOME=/somehome",
-		}
-		ctx.cfg.isAndroidWrapper = true
-		cmd := mustCallBisectDriver(ctx, callCompiler(ctx, ctx.cfg, ctx.newCommand(clangAndroid, mainCc)))
-		if err := verifyArgOrder(cmd,
-			"someBisectStage", filepath.Join("/somehome", "ANDROID_BISECT")); err != nil {
-			t.Error(err)
-		}
-	})
-}
-
-func TestForwardStdOutAndStdErrAndExitCodeFromBisect(t *testing.T) {
-	withBisectTestContext(t, func(ctx *testContext) {
-		ctx.cmdMock = func(cmd *command, stdin io.Reader, stdout io.Writer, stderr io.Writer) error {
-			fmt.Fprint(stdout, "somemessage")
-			fmt.Fprint(stderr, "someerror")
-			return newExitCodeError(23)
-		}
-		exitCode := callCompiler(ctx, ctx.cfg, ctx.newCommand(gccX86_64, mainCc))
-		if exitCode != 23 {
-			t.Errorf("unexpected exit code. Got: %d", exitCode)
-		}
-		if ctx.stdoutString() != "somemessage" {
-			t.Errorf("stdout was not forwarded. Got: %s", ctx.stdoutString())
-		}
-		if ctx.stderrString() != "someerror" {
-			t.Errorf("stderr was not forwarded. Got: %s", ctx.stderrString())
-		}
-	})
-}
-
-func TestForwardGeneralErrorFromBisect(t *testing.T) {
-	withBisectTestContext(t, func(ctx *testContext) {
-		ctx.cmdMock = func(cmd *command, stdin io.Reader, stdout io.Writer, stderr io.Writer) error {
-			return errors.New("someerror")
-		}
-		stderr := ctx.mustFail(callCompiler(ctx, ctx.cfg,
-			ctx.newCommand(gccX86_64, mainCc)))
-		if err := verifyInternalError(stderr); err != nil {
-			t.Fatal(err)
-		}
-		if !strings.Contains(stderr, "someerror") {
-			t.Errorf("unexpected error. Got: %s", stderr)
-		}
-	})
-}
-
-func withBisectTestContext(t *testing.T, work func(ctx *testContext)) {
-	withTestContext(t, func(ctx *testContext) {
-		ctx.env = []string{"BISECT_STAGE=xyz"}
-		// We execute the python script but replace the call to the bisect_driver with
-		// a mock that logs the data.
-		ctx.cmdMock = func(cmd *command, stdin io.Reader, stdout io.Writer, stderr io.Writer) error {
-			if err := verifyPath(cmd, "/usr/bin/env"); err != nil {
-				return err
-			}
-			if cmd.Args[0] != "python3" {
-				return fmt.Errorf("expected a call to python. Got: %s", cmd.Args[0])
-			}
-			if cmd.Args[1] != "-c" {
-				return fmt.Errorf("expected an inline python script. Got: %s", cmd.Args)
-			}
-			script := cmd.Args[2]
-			mock := `
-class BisectDriver:
-	def __init__(self):
-		self.VALID_MODES = ['POPULATE_GOOD', 'POPULATE_BAD', 'TRIAGE']
-	def bisect_driver(self, bisect_stage, bisect_dir, execargs):
-		print('command bisect_driver')
-		print('arg %s' % bisect_stage)
-		print('arg %s' % bisect_dir)
-		for arg in execargs:
-			print('arg %s' % arg)
-
-bisect_driver = BisectDriver()
-`
-			script = mock + script
-			script = strings.Replace(script, "import bisect_driver", "", -1)
-			cmdCopy := *cmd
-			cmdCopy.Args = append(append(cmd.Args[:2], script), cmd.Args[3:]...)
-			// Evaluate the python script, but replace the call to the bisect_driver
-			// with a log statement so that we can assert it.
-			return runCmd(ctx, &cmdCopy, nil, stdout, stderr)
-		}
-		work(ctx)
-	})
-}
-
-func mustCallBisectDriver(ctx *testContext, exitCode int) *command {
-	ctx.must(exitCode)
-	cmd := &command{}
-	for _, line := range strings.Split(ctx.stdoutString(), "\n") {
-		if prefix := "command "; strings.HasPrefix(line, prefix) {
-			cmd.Path = line[len(prefix):]
-		} else if prefix := "arg "; strings.HasPrefix(line, prefix) {
-			cmd.Args = append(cmd.Args, line[len(prefix):])
-		}
-	}
-	return cmd
-}
diff --git a/compiler_wrapper/build.py b/compiler_wrapper/build.py
index bea888db..093bb548 100755
--- a/compiler_wrapper/build.py
+++ b/compiler_wrapper/build.py
@@ -23,7 +23,7 @@ def parse_args():
         "--use_ccache", required=True, choices=["true", "false"]
     )
     parser.add_argument(
-        "--use_llvm_next", required=True, choices=["true", "false"]
+        "--use_llvm_next", default="false", choices=["true", "false"]
     )
     parser.add_argument("--output_file", required=True, type=str)
     parser.add_argument(
diff --git a/compiler_wrapper/clang_flags.go b/compiler_wrapper/clang_flags.go
index 80e5174a..85a63dce 100644
--- a/compiler_wrapper/clang_flags.go
+++ b/compiler_wrapper/clang_flags.go
@@ -14,26 +14,21 @@ import (
 
 func processClangFlags(builder *commandBuilder) error {
 	env := builder.env
-	clangDir, _ := env.getenv("CLANG")
-
-	if clangDir == "" {
-		if builder.cfg.isHostWrapper {
-			clangDir = filepath.Dir(builder.absWrapperPath)
-		} else {
-			clangDir = filepath.Join(builder.rootPath, "usr/bin/")
-			if !filepath.IsAbs(builder.path) {
-				// If sysroot_wrapper is invoked by relative path, call actual compiler in
-				// relative form. This is neccesary to remove absolute path from compile
-				// outputs.
-				var err error
-				clangDir, err = filepath.Rel(env.getwd(), clangDir)
-				if err != nil {
-					return wrapErrorwithSourceLocf(err, "failed to make clangDir %s relative to %s.", clangDir, env.getwd())
-				}
+	clangDir := ""
+	if builder.cfg.isHostWrapper {
+		clangDir = filepath.Dir(builder.absWrapperPath)
+	} else {
+		clangDir = filepath.Join(builder.rootPath, "usr/bin/")
+		if !filepath.IsAbs(builder.path) {
+			// If sysroot_wrapper is invoked by relative path, call actual compiler in
+			// relative form. This is necessary to remove absolute path from compile
+			// outputs.
+			var err error
+			clangDir, err = filepath.Rel(env.getwd(), clangDir)
+			if err != nil {
+				return wrapErrorwithSourceLocf(err, "failed to make clangDir %s relative to %s.", clangDir, env.getwd())
 			}
 		}
-	} else {
-		clangDir = filepath.Dir(clangDir)
 	}
 
 	clangBasename := "clang"
@@ -71,6 +66,11 @@ func processClangFlags(builder *commandBuilder) error {
 	// based on a single input argument, and also be able to return errors.
 	newArgs := []builderArg{}
 
+	// Note: Used for detecting whether we are building non-thumb2 compatible
+	// baremetal arm targets. This is used to override the target triple for
+	// armv6m targets, as Clang target triple normalization does not work
+	// correctly when it's set to "arm" instead of "armv6m". See b/373947346.
+	isArmV6M := false
 	for _, arg := range builder.args {
 		// Adds an argument with the given value, preserving the
 		// fromUser value of the original argument.
@@ -81,6 +81,10 @@ func processClangFlags(builder *commandBuilder) error {
 			})
 		}
 
+		if arg.value == "-march=armv6m" || strings.HasPrefix(arg.value, "-target=armv6m") || strings.HasPrefix(arg.value, "--target=armv6m") {
+			isArmV6M = true
+		}
+
 		if mapped, ok := gccToClang[arg.value]; ok {
 			addNewArg(mapped)
 			continue
@@ -135,7 +139,15 @@ func processClangFlags(builder *commandBuilder) error {
 		prefixPath := path.Join(relLinkerPath, builder.target.target+"-")
 		builder.addPreUserArgs("--prefix=" + prefixPath)
 		builder.addPostUserArgs("-B" + relLinkerPath)
-		builder.addPostUserArgs("-target", builder.target.target)
+
+		// Override for armv6m, as it doesn't support thumb2, and Clang target
+		// triple normalization does not work correctly when it's set to "arm" instead
+		// of "armv6m". See b/373947346.
+		if isArmV6M {
+			builder.addPostUserArgs("-target", "armv6m-none-eabi")
+		} else {
+			builder.addPostUserArgs("-target", builder.target.target)
+		}
 	}
 	return nil
 }
diff --git a/compiler_wrapper/clang_flags_test.go b/compiler_wrapper/clang_flags_test.go
index cce79be2..24ed33f8 100644
--- a/compiler_wrapper/clang_flags_test.go
+++ b/compiler_wrapper/clang_flags_test.go
@@ -33,17 +33,6 @@ func TestClangBasename(t *testing.T) {
 	})
 }
 
-func TestClangPathGivenClangEnv(t *testing.T) {
-	withTestContext(t, func(ctx *testContext) {
-		ctx.env = []string{"CLANG=/a/b/clang"}
-		cmd := ctx.must(callCompiler(ctx, ctx.cfg,
-			ctx.newCommand(clangX86_64, mainCc)))
-		if err := verifyPath(cmd, "/a/b/clang"); err != nil {
-			t.Error(err)
-		}
-	})
-}
-
 func TestAbsoluteClangPathBasedOnRootPath(t *testing.T) {
 	withTestContext(t, func(ctx *testContext) {
 		ctx.cfg.clangRootRelPath = "somepath"
@@ -107,7 +96,7 @@ func TestClangPathForAndroidWrapper(t *testing.T) {
 		ctx.cfg.isAndroidWrapper = true
 		cmd := ctx.must(callCompiler(ctx, ctx.cfg,
 			ctx.newCommand("somedir/clang", mainCc)))
-		if err := verifyPath(cmd, "somedir/clang.real"); err != nil {
+		if err := verifyPath(cmd, "somedir/clang-real"); err != nil {
 			t.Error(err)
 		}
 	})
@@ -120,7 +109,7 @@ func TestClangPathForAndroidWrapperWithSymlinks(t *testing.T) {
 		ctx.symlink("base/some_clang", "linked/clang")
 		cmd := ctx.must(callCompiler(ctx, ctx.cfg,
 			ctx.newCommand("linked/clang", mainCc)))
-		if err := verifyPath(cmd, "linked/some_clang.real"); err != nil {
+		if err := verifyPath(cmd, "linked/some_clang-real"); err != nil {
 			t.Error(err)
 		}
 	})
diff --git a/compiler_wrapper/clang_tidy_flag.go b/compiler_wrapper/clang_tidy_flag.go
index 9722b39e..ad5ad135 100644
--- a/compiler_wrapper/clang_tidy_flag.go
+++ b/compiler_wrapper/clang_tidy_flag.go
@@ -7,7 +7,6 @@ package main
 import (
 	"encoding/json"
 	"fmt"
-	"io/ioutil"
 	"os"
 	"path"
 	"path/filepath"
@@ -104,7 +103,7 @@ func runClangTidyForTricium(env env, clangCmd *command, cSrcFile string, extraTi
 		return fmt.Errorf("creating fixes directory at %q: %v", fixesDir, err)
 	}
 
-	f, err := ioutil.TempFile(fixesDir, "lints-")
+	f, err := os.CreateTemp(fixesDir, "lints-")
 	if err != nil {
 		return fmt.Errorf("making tempfile for tidy: %v", err)
 	}
@@ -163,7 +162,7 @@ func runClangTidyForTricium(env env, clangCmd *command, cSrcFile string, extraTi
 			return fmt.Errorf("creating crash artifacts directory at %q: %v", tidyCrashArtifacts, err)
 		}
 
-		f, err := ioutil.TempFile(tidyCrashArtifacts, "crash-")
+		f, err := os.CreateTemp(tidyCrashArtifacts, "crash-")
 		if err != nil {
 			return fmt.Errorf("making tempfile for crash output: %v", err)
 		}
diff --git a/compiler_wrapper/compile_with_fallback.go b/compiler_wrapper/compile_with_fallback.go
index d0b6a163..2d70fee7 100644
--- a/compiler_wrapper/compile_with_fallback.go
+++ b/compiler_wrapper/compile_with_fallback.go
@@ -32,7 +32,7 @@ func compileWithFallback(env env, cfg *config, originalCmd *command, absWrapperP
 		EnvUpdates: originalCmd.EnvUpdates,
 	}
 	// We only want to pass extra flags to clang and clang++.
-	if base := filepath.Base(originalCmd.Path); base == "clang.real" || base == "clang++.real" {
+	if base := filepath.Base(originalCmd.Path); base == "clang-real" || base == "clang++-real" {
 		// We may introduce some new warnings after rebasing and we need to
 		// disable them before we fix those warnings.
 		extraArgs, _ := env.getenv("ANDROID_LLVM_FALLBACK_DISABLED_WARNINGS")
diff --git a/compiler_wrapper/compile_with_fallback_test.go b/compiler_wrapper/compile_with_fallback_test.go
index 67530a24..5b7c41f1 100644
--- a/compiler_wrapper/compile_with_fallback_test.go
+++ b/compiler_wrapper/compile_with_fallback_test.go
@@ -8,7 +8,6 @@ import (
 	"errors"
 	"fmt"
 	"io"
-	"io/ioutil"
 	"os"
 	"path/filepath"
 	"strings"
@@ -240,7 +239,7 @@ func TestCompileWithFallbackLogCommandAndErrors(t *testing.T) {
 
 		log := readCompileWithFallbackErrorLog(ctx)
 		if log != `==================COMMAND:====================
-./clang.real main.cc -fno-color-diagnostics -a -b
+./clang-real main.cc -fno-color-diagnostics -a -b
 
 someerror
 ==============================================
@@ -276,7 +275,7 @@ func TestCompileWithFallbackAppendToLog(t *testing.T) {
 		if !strings.Contains(log, "oldContent") {
 			t.Errorf("old content not present: %s", log)
 		}
-		if !strings.Contains(log, "clang.real") {
+		if !strings.Contains(log, "clang-real") {
 			t.Errorf("new content not present: %s", log)
 		}
 	})
@@ -295,7 +294,7 @@ func withCompileWithFallbackTestContext(t *testing.T, work func(ctx *testContext
 
 func readCompileWithFallbackErrorLog(ctx *testContext) string {
 	logFile := filepath.Join(ctx.tempDir, "fallback_stderr")
-	data, err := ioutil.ReadFile(logFile)
+	data, err := os.ReadFile(logFile)
 	if err != nil {
 		ctx.t.Fatalf("error reading log file %s: %s", logFile, err)
 	}
diff --git a/compiler_wrapper/compiler_wrapper.go b/compiler_wrapper/compiler_wrapper.go
index 7f2c8d11..fee559fb 100644
--- a/compiler_wrapper/compiler_wrapper.go
+++ b/compiler_wrapper/compiler_wrapper.go
@@ -57,7 +57,7 @@ func calculateAndroidWrapperPath(mainBuilderPath string, absWrapperPath string)
 
 	// We need to be careful here: path.Join Clean()s its result, so `./foo` will get
 	// transformed to `foo`, which isn't good since we're passing this path to exec.
-	basePart := filepath.Base(absWrapperPath) + ".real"
+	basePart := filepath.Base(absWrapperPath) + "-real"
 	if !strings.ContainsRune(mainBuilderPath, filepath.Separator) {
 		return basePart
 	}
@@ -168,6 +168,10 @@ func callCompilerInternal(env env, cfg *config, inputCmd *command) (exitCode int
 			return 0, newErrorwithSourceLocf("unsupported compiler: %s", mainBuilder.target.compiler)
 		}
 	} else {
+		if isRiscvBuildWithoutAckFlag(env, mainBuilder) {
+			return 0, errors.New(riscvExperimentalUseError)
+		}
+
 		cSrcFile, tidyFlags, tidyMode := processClangTidyFlags(mainBuilder)
 		crashArtifactsDir := detectCrashArtifactsDir(env, cfg)
 		if mainBuilder.target.compilerType == clangType {
@@ -230,36 +234,19 @@ func callCompilerInternal(env env, cfg *config, inputCmd *command) (exitCode int
 		return buildWithAutocrash(env, cfg, compilerCmd)
 	}
 
-	bisectStage := getBisectStage(env)
-
 	if rusageEnabled {
 		compilerCmd = removeRusageFromCommand(compilerCmd)
 	}
 
 	if disableWerrorConfig.enabled {
-		if bisectStage != "" {
-			return 0, newUserErrorf("BISECT_STAGE is meaningless with FORCE_DISABLE_WERROR")
-		}
 		return doubleBuildWithWNoError(env, cfg, compilerCmd, disableWerrorConfig)
 	}
 	if shouldCompileWithFallback(env) {
 		if rusageEnabled {
 			return 0, newUserErrorf("TOOLCHAIN_RUSAGE_OUTPUT is meaningless with ANDROID_LLVM_PREBUILT_COMPILER_PATH")
 		}
-		if bisectStage != "" {
-			return 0, newUserErrorf("BISECT_STAGE is meaningless with ANDROID_LLVM_PREBUILT_COMPILER_PATH")
-		}
 		return compileWithFallback(env, cfg, compilerCmd, mainBuilder.absWrapperPath)
 	}
-	if bisectStage != "" {
-		if rusageEnabled {
-			return 0, newUserErrorf("TOOLCHAIN_RUSAGE_OUTPUT is meaningless with BISECT_STAGE")
-		}
-		compilerCmd, err = calcBisectCommand(env, cfg, bisectStage, compilerCmd)
-		if err != nil {
-			return 0, err
-		}
-	}
 
 	errRetryCompilation := errors.New("compilation retry requested")
 	var runCompiler func(willLogRusage bool) (int, error)
diff --git a/compiler_wrapper/compiler_wrapper_test.go b/compiler_wrapper/compiler_wrapper_test.go
index 79edab6e..e224d8bb 100644
--- a/compiler_wrapper/compiler_wrapper_test.go
+++ b/compiler_wrapper/compiler_wrapper_test.go
@@ -157,36 +157,6 @@ func TestLogRusageAndForceDisableWError(t *testing.T) {
 	})
 }
 
-func TestErrorOnLogRusageAndBisect(t *testing.T) {
-	withTestContext(t, func(ctx *testContext) {
-		ctx.NoteTestWritesToUmask()
-
-		ctx.env = []string{
-			"BISECT_STAGE=xyz",
-			"TOOLCHAIN_RUSAGE_OUTPUT=rusage.log",
-		}
-		stderr := ctx.mustFail(callCompiler(ctx, ctx.cfg, ctx.newCommand(gccX86_64, mainCc)))
-		if err := verifyNonInternalError(stderr, "TOOLCHAIN_RUSAGE_OUTPUT is meaningless with BISECT_STAGE"); err != nil {
-			t.Error(err)
-		}
-	})
-}
-
-func TestErrorOnBisectAndForceDisableWError(t *testing.T) {
-	withTestContext(t, func(ctx *testContext) {
-		ctx.NoteTestWritesToUmask()
-
-		ctx.env = []string{
-			"BISECT_STAGE=xyz",
-			"FORCE_DISABLE_WERROR=1",
-		}
-		stderr := ctx.mustFail(callCompiler(ctx, ctx.cfg, ctx.newCommand(clangX86_64, mainCc)))
-		if err := verifyNonInternalError(stderr, "BISECT_STAGE is meaningless with FORCE_DISABLE_WERROR"); err != nil {
-			t.Error(err)
-		}
-	})
-}
-
 func TestPrintUserCompilerError(t *testing.T) {
 	buffer := bytes.Buffer{}
 	printCompilerError(&buffer, newUserErrorf("abcd"))
@@ -227,22 +197,22 @@ func TestCalculateAndroidWrapperPath(t *testing.T) {
 		{
 			mainBuilderPath: "/foo/bar",
 			absWrapperPath:  "/bar/baz",
-			want:            "/foo/baz.real",
+			want:            "/foo/baz-real",
 		},
 		{
 			mainBuilderPath: "/my_wrapper",
 			absWrapperPath:  "/bar/baz",
-			want:            "/baz.real",
+			want:            "/baz-real",
 		},
 		{
 			mainBuilderPath: "no_seps",
 			absWrapperPath:  "/bar/baz",
-			want:            "baz.real",
+			want:            "baz-real",
 		},
 		{
 			mainBuilderPath: "./a_sep",
 			absWrapperPath:  "/bar/baz",
-			want:            "./baz.real",
+			want:            "./baz-real",
 		},
 	}
 
diff --git a/compiler_wrapper/config.go b/compiler_wrapper/config.go
index b445ce95..44f72e01 100644
--- a/compiler_wrapper/config.go
+++ b/compiler_wrapper/config.go
@@ -74,16 +74,16 @@ func isAndroidConfig() bool {
 }
 
 func getConfig(configName string, useCCache bool, useLlvmNext bool, version string) (*config, error) {
-	cfg := config{}
+	cfg := (*config)(nil)
 	switch configName {
 	case "cros.hardened":
-		cfg = crosHardenedConfig
+		cfg = getCrosHardenedConfig()
 	case "cros.nonhardened":
-		cfg = crosNonHardenedConfig
+		cfg = getCrosNonHardenedConfig()
 	case "cros.host":
-		cfg = crosHostConfig
+		cfg = getCrosHostConfig()
 	case "android":
-		cfg = androidConfig
+		cfg = getAndroidConfig()
 	default:
 		return nil, newErrorwithSourceLocf("unknown config name: %s", configName)
 	}
@@ -94,7 +94,7 @@ func getConfig(configName string, useCCache bool, useLlvmNext bool, version stri
 		cfg.clangPostFlags = append(cfg.clangPostFlags, llvmNextPostFlags...)
 	}
 	cfg.version = version
-	return &cfg, nil
+	return cfg, nil
 }
 
 func crosCommonClangFlags() []string {
@@ -107,13 +107,8 @@ func crosCommonClangFlags() []string {
 		"-Qunused-arguments",
 		"-Werror=poison-system-directories",
 		"-Wno-deprecated-declarations",
-		"-Wno-enum-constexpr-conversion",
 		"-Wno-error=implicit-function-declaration",
 		"-Wno-error=implicit-int",
-		"-Wno-final-dtor-non-final-class",
-		"-Wno-single-bit-bitfield-constant-conversion",
-		"-Wno-tautological-constant-compare",
-		"-Wno-tautological-unsigned-enum-zero-compare",
 		"-Wno-unknown-warning-option",
 		"-fdebug-default-version=5",
 		"-Wno-int-conversion",
@@ -121,8 +116,6 @@ func crosCommonClangFlags() []string {
 		// TODO(b/316021385): Temporarily disables warnings for variable length arrays.
 		"-Wno-error=vla-cxx-extension",
 		"-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-		// TODO(b/315504245): Temporarily prevents new mangling rules from taking effect.
-		"-fclang-abi-compat=17",
 	}
 }
 
@@ -134,89 +127,97 @@ func crosCommonClangPostFlags() []string {
 }
 
 // Full hardening.
-// Temporarily disable function splitting because of chromium:434751.
-var crosHardenedConfig = config{
-	clangRootRelPath: "../..",
-	gccRootRelPath:   "../../../../..",
-	// Pass "-fcommon" till the packages are fixed to work with new clang/gcc
-	// default of "-fno-common", crbug.com/1060413.
-	commonFlags: []string{
-		"-fcommon",
-		"-fstack-protector-strong",
-		"-D_FORTIFY_SOURCE=3",
-		"-fno-omit-frame-pointer",
-	},
-	gccFlags: []string{
-		"-fno-reorder-blocks-and-partition",
-		"-Wno-unused-local-typedefs",
-		"-Wno-maybe-uninitialized",
-	},
-	// Temporarily disable Wsection since kernel gets a bunch of these. chromium:778867
-	// Disable "-faddrsig" since it produces object files that strip doesn't understand, chromium:915742.
-	// crbug.com/1103065: -grecord-gcc-switches pollutes the Goma cache;
-	//   removed that flag for now.
-	clangFlags: append(
-		crosCommonClangFlags(),
-		"--unwindlib=libunwind",
-		"-Wno-section",
-		"-fno-addrsig",
-		"-ftrivial-auto-var-init=zero",
-	),
-	clangPostFlags: crosCommonClangPostFlags(),
+func getCrosHardenedConfig() *config {
+	// Temporarily disable function splitting because of chromium:434751.
+	return &config{
+		clangRootRelPath: "../..",
+		gccRootRelPath:   "../../../../..",
+		// Pass "-fcommon" till the packages are fixed to work with new clang/gcc
+		// default of "-fno-common", crbug.com/1060413.
+		commonFlags: []string{
+			"-fcommon",
+			"-fstack-protector-strong",
+			"-D_FORTIFY_SOURCE=3",
+			"-fno-omit-frame-pointer",
+		},
+		gccFlags: []string{
+			"-fno-reorder-blocks-and-partition",
+			"-Wno-unused-local-typedefs",
+			"-Wno-maybe-uninitialized",
+		},
+		// Temporarily disable Wsection since kernel gets a bunch of these. chromium:778867
+		// Disable "-faddrsig" since it produces object files that strip doesn't understand, chromium:915742.
+		// crbug.com/1103065: -grecord-gcc-switches pollutes the Goma cache;
+		//   removed that flag for now.
+		clangFlags: append(
+			crosCommonClangFlags(),
+			"--unwindlib=libunwind",
+			"-Wno-section",
+			"-fno-addrsig",
+			"-ftrivial-auto-var-init=zero",
+		),
+		clangPostFlags: crosCommonClangPostFlags(),
+	}
 }
 
 // Flags to be added to non-hardened toolchain.
-var crosNonHardenedConfig = config{
-	clangRootRelPath: "../..",
-	gccRootRelPath:   "../../../../..",
-	commonFlags:      []string{},
-	gccFlags: []string{
-		"-Wno-maybe-uninitialized",
-		"-Wno-unused-local-typedefs",
-		"-Wno-deprecated-declarations",
-		"-Wtrampolines",
-	},
-	// Temporarily disable Wsection since kernel gets a bunch of these. chromium:778867
-	clangFlags: append(
-		crosCommonClangFlags(),
-		"-Wno-section",
-	),
-	clangPostFlags: crosCommonClangPostFlags(),
+func getCrosNonHardenedConfig() *config {
+	return &config{
+		clangRootRelPath: "../..",
+		gccRootRelPath:   "../../../../..",
+		commonFlags:      []string{},
+		gccFlags: []string{
+			"-Wno-maybe-uninitialized",
+			"-Wno-unused-local-typedefs",
+			"-Wno-deprecated-declarations",
+			"-Wtrampolines",
+		},
+		// Temporarily disable Wsection since kernel gets a bunch of these. chromium:778867
+		clangFlags: append(
+			crosCommonClangFlags(),
+			"-Wno-section",
+		),
+		clangPostFlags: crosCommonClangPostFlags(),
+	}
 }
 
 // Flags to be added to host toolchain.
-var crosHostConfig = config{
-	isHostWrapper:    true,
-	clangRootRelPath: "../..",
-	gccRootRelPath:   "../..",
-	// Pass "-fcommon" till the packages are fixed to work with new clang/gcc
-	// default of "-fno-common", crbug.com/1060413.
-	commonFlags: []string{
-		"-fcommon",
-	},
-	gccFlags: []string{
-		"-Wno-maybe-uninitialized",
-		"-Wno-unused-local-typedefs",
-		"-Wno-deprecated-declarations",
-	},
-	// crbug.com/1103065: -grecord-gcc-switches pollutes the Goma cache;
-	//   removed that flag for now.
-	clangFlags: append(
-		crosCommonClangFlags(),
-		"-Wno-unused-local-typedefs",
-		"-fno-addrsig",
-	),
-	// Temporarily disable Wdeprecated-copy. b/191479033
-	clangPostFlags: crosCommonClangPostFlags(),
+func getCrosHostConfig() *config {
+	return &config{
+		isHostWrapper:    true,
+		clangRootRelPath: "../..",
+		gccRootRelPath:   "../..",
+		// Pass "-fcommon" till the packages are fixed to work with new clang/gcc
+		// default of "-fno-common", crbug.com/1060413.
+		commonFlags: []string{
+			"-fcommon",
+		},
+		gccFlags: []string{
+			"-Wno-maybe-uninitialized",
+			"-Wno-unused-local-typedefs",
+			"-Wno-deprecated-declarations",
+		},
+		// crbug.com/1103065: -grecord-gcc-switches pollutes the Goma cache;
+		//   removed that flag for now.
+		clangFlags: append(
+			crosCommonClangFlags(),
+			"-Wno-unused-local-typedefs",
+			"-fno-addrsig",
+		),
+		// Temporarily disable Wdeprecated-copy. b/191479033
+		clangPostFlags: crosCommonClangPostFlags(),
+	}
 }
 
-var androidConfig = config{
-	isHostWrapper:    false,
-	isAndroidWrapper: true,
-	gccRootRelPath:   "./",
-	clangRootRelPath: "./",
-	commonFlags:      []string{},
-	gccFlags:         []string{},
-	clangFlags:       []string{},
-	clangPostFlags:   []string{},
+func getAndroidConfig() *config {
+	return &config{
+		isHostWrapper:    false,
+		isAndroidWrapper: true,
+		gccRootRelPath:   "./",
+		clangRootRelPath: "./",
+		commonFlags:      []string{},
+		gccFlags:         []string{},
+		clangFlags:       []string{},
+		clangPostFlags:   []string{},
+	}
 }
diff --git a/compiler_wrapper/cros_hardened_config_test.go b/compiler_wrapper/cros_hardened_config_test.go
index 361ba707..22396855 100644
--- a/compiler_wrapper/cros_hardened_config_test.go
+++ b/compiler_wrapper/cros_hardened_config_test.go
@@ -55,7 +55,6 @@ func TestCrosHardenedConfigWithoutCCache(t *testing.T) {
 			createGccPathGoldenInputs(ctx, gomaEnv),
 			createClangPathGoldenInputs(ctx, gomaEnv),
 			createClangSyntaxGoldenInputs(gomaEnv),
-			createBisectGoldenInputs(clangX86_64),
 			createForceDisableWErrorGoldenInputs(),
 			createClangTidyGoldenInputs(gomaEnv),
 		})
@@ -80,7 +79,6 @@ func TestCrosHardenedConfigWithLlvmNext(t *testing.T) {
 			createGccPathGoldenInputs(ctx, gomaEnv),
 			createClangPathGoldenInputs(ctx, gomaEnv),
 			createClangSyntaxGoldenInputs(gomaEnv),
-			createBisectGoldenInputs(clangX86_64),
 			createForceDisableWErrorGoldenInputs(),
 			createClangTidyGoldenInputs(gomaEnv),
 		})
@@ -105,7 +103,6 @@ func createSyswrapperGoldenInputs(ctx *testContext) []goldenFile {
 		createSysrootWrapperCommonGoldenInputs("clang", gomaEnv),
 		createSanitizerGoldenInputs("clang"),
 		createClangArgsGoldenInputs(),
-		createBisectGoldenInputs(clangX86_64),
 		createForceDisableWErrorGoldenInputs(),
 		createClangTidyGoldenInputs(gomaEnv),
 	}
@@ -156,40 +153,6 @@ func createGoldenInputsForAllTargets(compiler string, args ...string) goldenFile
 	}
 }
 
-func createBisectGoldenInputs(compiler string) goldenFile {
-	return goldenFile{
-		Name: "bisect.json",
-		Records: []goldenRecord{
-			{
-				WrapperCmd: newGoldenCmd(compiler, mainCc),
-				Env: []string{
-					"BISECT_STAGE=someBisectStage",
-					"HOME=/user/home",
-				},
-				Cmds: okResults,
-			},
-			{
-				WrapperCmd: newGoldenCmd(compiler, mainCc),
-				Env: []string{
-					"BISECT_STAGE=someBisectStage",
-					"BISECT_DIR=someBisectDir",
-					"HOME=/user/home",
-				},
-				Cmds: okResults,
-			},
-			{
-				WrapperCmd: newGoldenCmd(compiler, mainCc),
-				Env: []string{
-					"BISECT_STAGE=someBisectStage",
-					"BISECT_DIR=someBisectDir",
-					"HOME=/user/home",
-				},
-				Cmds: errorResults,
-			},
-		},
-	}
-}
-
 func createForceDisableWErrorGoldenInputs() goldenFile {
 	return goldenFile{
 		Name: "force_disable_werror.json",
@@ -282,11 +245,6 @@ func createClangPathGoldenInputs(ctx *testContext, gomaEnv string) goldenFile {
 				WrapperCmd: newGoldenCmd("./x86_64-cros-linux-gnu-clang++", mainCc),
 				Cmds:       okResults,
 			},
-			{
-				WrapperCmd: newGoldenCmd(clangX86_64, mainCc),
-				Env:        []string{"CLANG=somepath/clang"},
-				Cmds:       okResults,
-			},
 			{
 				WrapperCmd: newGoldenCmd(clangX86_64, "-Xclang-path=/somedir", mainCc),
 				Cmds: []commandResult{
diff --git a/compiler_wrapper/cros_host_config_test.go b/compiler_wrapper/cros_host_config_test.go
index 4eb9027c..1da81c37 100644
--- a/compiler_wrapper/cros_host_config_test.go
+++ b/compiler_wrapper/cros_host_config_test.go
@@ -32,7 +32,6 @@ func TestCrosClangHostConfig(t *testing.T) {
 			createGoldenInputsForAllTargets("clang", "-ftrapv", mainCc),
 			createSanitizerGoldenInputs("clang"),
 			createClangArgsGoldenInputs(),
-			createBisectGoldenInputs(clangX86_64),
 			createForceDisableWErrorGoldenInputs(),
 			createClangTidyGoldenInputs(gomaEnv),
 			createClangHostWrapperInputs(),
diff --git a/compiler_wrapper/disable_werror_flag.go b/compiler_wrapper/disable_werror_flag.go
index 45e818fa..b0b05408 100644
--- a/compiler_wrapper/disable_werror_flag.go
+++ b/compiler_wrapper/disable_werror_flag.go
@@ -9,7 +9,6 @@ import (
 	"encoding/json"
 	"fmt"
 	"io"
-	"io/ioutil"
 	"os"
 	"path"
 	"regexp"
@@ -104,12 +103,18 @@ func disableWerrorFlags(originalArgs, extraFlags []string) []string {
 	return append(newArgs, allExtraFlags...)
 }
 
-func isLikelyAConfTest(cfg *config, cmd *command) bool {
+func isLikelyAConfTest(env env, cfg *config, cmd *command) bool {
 	// Android doesn't do mid-build `configure`s, so we don't need to worry about this there.
 	if cfg.isAndroidWrapper {
 		return false
 	}
 
+	// Ignore anything that's likely to be a cmake configuration step. These put the compiler
+	// into a TryCompile dir.
+	if strings.Contains(env.getwd(), "CMakeFiles/CMakeScratch/TryCompile-") {
+		return true
+	}
+
 	for _, a := range cmd.Args {
 		// The kernel, for example, will do configure tests with /dev/null as a source file.
 		if a == "/dev/null" || strings.HasPrefix(a, "conftest.c") {
@@ -176,7 +181,7 @@ func doubleBuildWithWNoError(env env, cfg *config, originalCmd *command, werrorC
 	// The only way we can do anything useful is if it looks like the failure
 	// was -Werror-related.
 	retryWithExtraFlags := []string{}
-	if originalExitCode != 0 && !isLikelyAConfTest(cfg, originalCmd) {
+	if originalExitCode != 0 && !isLikelyAConfTest(env, cfg, originalCmd) {
 		retryWithExtraFlags = getWnoErrorFlags(originalStdoutBuffer.Bytes(), originalStderrBuffer.Bytes())
 	}
 	if len(retryWithExtraFlags) == 0 {
@@ -283,7 +288,7 @@ func doubleBuildWithWNoError(env env, cfg *config, originalCmd *command, werrorC
 	// Coming up with a consistent name for this is difficult (compiler command's
 	// SHA can clash in the case of identically named files in different
 	// directories, or similar); let's use a random one.
-	tmpFile, err := ioutil.TempFile(reportDir, "warnings_report*.json"+incompleteSuffix)
+	tmpFile, err := os.CreateTemp(reportDir, "warnings_report*.json"+incompleteSuffix)
 	if err != nil {
 		return 0, wrapErrorwithSourceLocf(err, "error creating warnings file")
 	}
@@ -335,7 +340,7 @@ func collectProcessData(pid int) (args, env []string, parentPid int, err error)
 	procDir := fmt.Sprintf("/proc/%d", pid)
 
 	readFile := func(fileName string) (string, error) {
-		s, err := ioutil.ReadFile(path.Join(procDir, fileName))
+		s, err := os.ReadFile(path.Join(procDir, fileName))
 		if err != nil {
 			return "", fmt.Errorf("reading %s: %v", fileName, err)
 		}
diff --git a/compiler_wrapper/disable_werror_flag_test.go b/compiler_wrapper/disable_werror_flag_test.go
index dce0b29e..15dee38c 100644
--- a/compiler_wrapper/disable_werror_flag_test.go
+++ b/compiler_wrapper/disable_werror_flag_test.go
@@ -9,7 +9,6 @@ import (
 	"errors"
 	"fmt"
 	"io"
-	"io/ioutil"
 	"os"
 	"path"
 	"path/filepath"
@@ -130,13 +129,23 @@ func TestDoubleBuildDoesntRecompileIfNoObviousWerrorsExist(t *testing.T) {
 func TestKnownConfigureFileParsing(t *testing.T) {
 	withTestContext(t, func(ctx *testContext) {
 		for _, f := range []string{"conftest.c", "conftest.cpp", "/dev/null"} {
-			if !isLikelyAConfTest(ctx.cfg, ctx.newCommand(clangX86_64, f)) {
+			if !isLikelyAConfTest(ctx, ctx.cfg, ctx.newCommand(clangX86_64, f)) {
 				t.Errorf("%q isn't considered a conf test file", f)
 			}
 		}
 	})
 }
 
+func TestKnownConfigureDirParsing(t *testing.T) {
+	withTestContext(t, func(ctx *testContext) {
+		wd := "foo/bar/CMakeFiles/CMakeScratch/TryCompile-abc123"
+		ctx.wd = wd
+		if !isLikelyAConfTest(ctx, ctx.cfg, ctx.newCommand(clangX86_64, "foo.c")) {
+			t.Errorf("%q isn't considered a conf test pwd", wd)
+		}
+	})
+}
+
 func TestDoubleBuildWithKnownConfigureFile(t *testing.T) {
 	withForceDisableWErrorTestContext(t, func(ctx *testContext) {
 		ctx.cmdMock = func(cmd *command, stdin io.Reader, stdout io.Writer, stderr io.Writer) error {
@@ -387,7 +396,7 @@ func withForceDisableWErrorTestContext(t *testing.T, work func(ctx *testContext)
 
 func readLoggedWarnings(ctx *testContext) *warningsJSONData {
 	warningsDir := getForceDisableWerrorDir(ctx, ctx.cfg)
-	files, err := ioutil.ReadDir(warningsDir)
+	files, err := os.ReadDir(warningsDir)
 	if err != nil {
 		if _, ok := err.(*os.PathError); ok {
 			return nil
@@ -397,7 +406,7 @@ func readLoggedWarnings(ctx *testContext) *warningsJSONData {
 	if len(files) != 1 {
 		ctx.t.Fatalf("expected 1 warning log file. Got: %s", files)
 	}
-	data, err := ioutil.ReadFile(filepath.Join(warningsDir, files[0].Name()))
+	data, err := os.ReadFile(filepath.Join(warningsDir, files[0].Name()))
 	if err != nil {
 		ctx.t.Fatal(err)
 	}
@@ -625,9 +634,15 @@ func TestClangTidyNoDoubleBuild(t *testing.T) {
 	})
 }
 
-func withAndroidClangTidyTestContext(t *testing.T, work func(ctx *testContext)) {
+func withAndroidTestContext(t *testing.T, work func(ctx *testContext)) {
 	withTestContext(t, func(ctx *testContext) {
 		ctx.cfg.isAndroidWrapper = true
+		work(ctx)
+	})
+}
+
+func withAndroidClangTidyTestContext(t *testing.T, work func(ctx *testContext)) {
+	withAndroidTestContext(t, func(ctx *testContext) {
 		ctx.cfg.useLlvmNext = true
 		ctx.env = []string{"OUT_DIR=/tmp"}
 
diff --git a/compiler_wrapper/env_test.go b/compiler_wrapper/env_test.go
index c10942de..bce36769 100644
--- a/compiler_wrapper/env_test.go
+++ b/compiler_wrapper/env_test.go
@@ -9,7 +9,6 @@ import (
 	"context"
 	"errors"
 	"flag"
-	"io/ioutil"
 	"os"
 	"os/exec"
 	"path"
@@ -239,7 +238,7 @@ func TestNewProcessEnvResolvesPwdAwayProperly(t *testing.T) {
 		}
 	}()
 
-	tempDir, err := ioutil.TempDir("", "wrapper_env_test")
+	tempDir, err := os.MkdirTemp("", "wrapper_env_test")
 	if err != nil {
 		t.Fatalf("Failed making temp dir: %v", err)
 	}
diff --git a/compiler_wrapper/goldenutil_test.go b/compiler_wrapper/goldenutil_test.go
index 593266ea..aede1883 100644
--- a/compiler_wrapper/goldenutil_test.go
+++ b/compiler_wrapper/goldenutil_test.go
@@ -10,7 +10,6 @@ import (
 	"flag"
 	"fmt"
 	"io"
-	"io/ioutil"
 	"log"
 	"os"
 	"path/filepath"
@@ -83,7 +82,7 @@ func runGoldenRecords(ctx *testContext, goldenDir string, files []goldenFile) {
 			compareBuffer := &bytes.Buffer{}
 			writeGoldenRecords(ctx, compareBuffer, file.Records)
 			filePath := filepath.Join(goldenDir, file.Name)
-			goldenFileData, err := ioutil.ReadFile(filePath)
+			goldenFileData, err := os.ReadFile(filePath)
 			if err != nil {
 				ctx.t.Error(err)
 				continue
@@ -195,6 +194,9 @@ func writeGoldenRecords(ctx *testContext, writer io.Writer, records []goldenReco
 		}
 		return strings.ReplaceAll(asString, ctx.tempDir, stableTempDir)
 	})
+	if err != nil {
+		ctx.t.Fatalf("transforming JSON values: %v", err)
+	}
 
 	encoder := json.NewEncoder(writer)
 	encoder.SetIndent("", "  ")
diff --git a/compiler_wrapper/install_compiler_wrapper.sh b/compiler_wrapper/install_compiler_wrapper.sh
index c3c9b714..6b3f89b8 100755
--- a/compiler_wrapper/install_compiler_wrapper.sh
+++ b/compiler_wrapper/install_compiler_wrapper.sh
@@ -1,4 +1,4 @@
-#!/bin/bash
+#!/bin/bash -eu
 #
 # Copyright 2020 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
@@ -10,7 +10,6 @@ if [[ ! -e /etc/cros_chroot_version ]]; then
   echo "Please run this script inside chroot"
   exit 1
 fi
-set -e
 
 # Use a unique value here, since folks doing wrapper dev _likely_ want builds
 # to always be redone.
@@ -22,6 +21,53 @@ build_py() {
   ./build.py --version_suffix="${version_suffix}" "$@"
 }
 
+install_sysroot_wrappers() {
+  local hardness="$1"
+  local wrapper_prefix="$2"
+
+  local wrapper="${wrapper_prefix}.noccache"
+  local wrapper_ccache="${wrapper_prefix}.ccache"
+
+  echo "Updating ${hardness} wrappers: ${wrapper} and ${wrapper_ccache}"
+  build_py \
+    --config="cros.${hardness}" \
+    --use_ccache=false \
+    --use_llvm_next=false \
+    --output_file="./${wrapper}"
+  build_py \
+    --config="cros.${hardness}" \
+    --use_ccache=true \
+    --use_llvm_next=false \
+    --output_file="./${wrapper_ccache}"
+
+  # Update clang target wrappers.
+  sudo cp "./${wrapper}" "./${wrapper_ccache}" /usr/bin
+  echo "Updated clang wrapper /usr/bin/${wrapper}"
+  echo "Updated clang wrapper /usr/bin/${wrapper_ccache}"
+
+  if [[ "${hardness}" == "hardened" ]]; then
+    # Update hardened GCC target wrappers.
+    local GCC gcc_files
+    for GCC in cross-x86_64-cros-linux-gnu/gcc cross-armv7a-cros-linux-gnueabihf/gcc cross-aarch64-cros-linux-gnu/gcc; do
+      if ! gcc_files="$(equery f "${GCC}")"; then
+        if [[ $(equery l "${GCC}" 2>&1 | wc -c) -eq 0 ]]; then
+          echo "no ${GCC} package found; skipping" >&2
+          continue
+        fi
+        # Something went wrong, and the equery above probably complained about it.
+        return 1
+      fi
+      echo "Updating non-ccached ${GCC} wrapper."
+      sudo cp "./${wrapper}" "$(grep "${wrapper}" <<< "${gcc_files}")"
+      grep "${wrapper}" <<< "${gcc_files}"
+      echo "Updating ccached ${GCC} wrapper."
+      sudo cp "./${wrapper_ccache}" "$(grep "${wrapper_ccache}" <<< "${gcc_files}")"
+      grep "${wrapper_ccache}" <<< "${gcc_files}"
+    done
+  fi
+  rm -f "./${wrapper}" "./${wrapper_ccache}"
+}
+
 echo "Updated files:"
 # Update the host wrapper
 build_py \
@@ -31,42 +77,6 @@ build_py \
   --output_file=./clang_host_wrapper
 sudo mv ./clang_host_wrapper /usr/bin/clang_host_wrapper
 echo "/usr/bin/clang_host_wrapper"
-sudo cp ../bisect_driver.py /usr/bin
-echo "/usr/bin/clang_host_wrapper/bisect_driver.py"
 
-# Update the target wrappers
-build_py \
-  --config=cros.hardened \
-  --use_ccache=false \
-  --use_llvm_next=false \
-  --output_file=./sysroot_wrapper.hardened.noccache
-build_py \
-  --config=cros.hardened \
-  --use_ccache=true \
-  --use_llvm_next=false \
-  --output_file=./sysroot_wrapper.hardened.ccache
-
-# Update clang target wrappers.
-sudo cp ./sysroot_wrapper.hardened.noccache ./sysroot_wrapper.hardened.ccache /usr/bin
-echo "Updated clang wrapper /usr/bin/sysroot_wrapper.hardened.noccache"
-echo "Updated clang wrapper /usr/bin/sysroot_wrapper.hardened.ccache"
-
-# Update GCC target wrappers.
-for GCC in cross-x86_64-cros-linux-gnu/gcc cross-armv7a-cros-linux-gnueabihf/gcc cross-aarch64-cros-linux-gnu/gcc; do
-  if ! FILES="$(equery f "${GCC}")"; then
-    if [[ $(equery l "${GCC}" 2>&1 | wc -c) -eq 0 ]]; then
-      echo "no ${GCC} package found; skipping" >&2
-      continue
-    fi
-    # Something went wrong, and the equery above probably complained about it.
-    exit 1
-  fi
-  echo "Updating ${GCC} wrapper."
-  sudo cp ./sysroot_wrapper.hardened.noccache "$(grep sysroot_wrapper.hardened.noccache <<< "${FILES}")"
-  grep sysroot_wrapper.hardened.noccache <<< "${FILES}"
-  sudo cp ./sysroot_wrapper.hardened.ccache "$(grep sysroot_wrapper.hardened.ccache <<< "${FILES}")"
-  grep sysroot_wrapper.hardened.ccache <<< "${FILES}"
-  sudo cp ../bisect_driver.py "$(grep bisect_driver.py <<< "${FILES}")"
-  grep bisect_driver.py <<< "${FILES}"
-done
-rm -f ./sysroot_wrapper.hardened.noccache ./sysroot_wrapper.hardened.ccache
+install_sysroot_wrappers "hardened" "sysroot_wrapper.hardened"
+install_sysroot_wrappers "nonhardened" "sysroot_wrapper"
diff --git a/compiler_wrapper/riscv_check.go b/compiler_wrapper/riscv_check.go
new file mode 100644
index 00000000..0bc9eb27
--- /dev/null
+++ b/compiler_wrapper/riscv_check.go
@@ -0,0 +1,39 @@
+// Copyright 2025 The ChromiumOS Authors
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package main
+
+import "strings"
+
+const (
+	riscvExperimentalAckFlag  = "-D_CROSTC_USER_ACKNOWLEDGES_THAT_RISCV_IS_EXPERIMENTAL"
+	riscvExperimentalEnvVar   = "CROSTC_USER_ACKNOWLEDGES_THAT_RISCV_IS_EXPERIMENTAL"
+	riscvExperimentalUseError = "error: use of riscv is experimental. If you're not sure what " +
+		"this implies, please reach out to chromeos-toolchain@google.com. If you've talked with " +
+		"the toolchain team, pass '" + riscvExperimentalAckFlag + "' " +
+		"to bypass this error, or set '" + riscvExperimentalEnvVar + "' in your " +
+		"environment"
+)
+
+func isRiscvBuildWithoutAckFlag(env env, builder *commandBuilder) bool {
+	// This is only relevant for CrOS.
+	if builder.cfg.isAndroidWrapper {
+		return false
+	}
+
+	if !strings.HasPrefix(builder.target.arch, "riscv") {
+		return false
+	}
+
+	if _, ok := env.getenv(riscvExperimentalEnvVar); ok {
+		return false
+	}
+
+	for _, arg := range builder.args {
+		if arg.value == riscvExperimentalAckFlag {
+			return false
+		}
+	}
+	return true
+}
diff --git a/compiler_wrapper/riscv_check_test.go b/compiler_wrapper/riscv_check_test.go
new file mode 100644
index 00000000..4b756fb1
--- /dev/null
+++ b/compiler_wrapper/riscv_check_test.go
@@ -0,0 +1,47 @@
+// Copyright 2025 The ChromiumOS Authors
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package main
+
+import (
+	"strings"
+	"testing"
+)
+
+func TestRiscvBuildWithAckFlag(t *testing.T) {
+	withTestContext(t, func(ctx *testContext) {
+		// Just make sure no errors are raised.
+		ctx.must(callCompiler(ctx, ctx.cfg, ctx.newCommand(clangRiscv, riscvExperimentalAckFlag, mainCc)))
+	})
+}
+
+func TestRiscvBuildWithEnvVar(t *testing.T) {
+	withTestContext(t, func(ctx *testContext) {
+		ctx.env = append(ctx.env, riscvExperimentalEnvVar+"=")
+		// Just make sure no errors are raised.
+		ctx.must(callCompiler(ctx, ctx.cfg, ctx.newCommand(clangRiscv, riscvExperimentalAckFlag, mainCc)))
+	})
+}
+
+func TestRiscvBuildWithoutAckFlagOrEnvVar(t *testing.T) {
+	withTestContext(t, func(ctx *testContext) {
+		exitCode := callCompiler(ctx, ctx.cfg, ctx.newCommand(clangRiscv, mainCc))
+		if exitCode == 0 {
+			t.Errorf("riscv-clang without ack flag or env var should've exited with a non-zero code")
+		}
+
+		stderr := ctx.stderrBuffer.String()
+		if !strings.Contains(stderr, riscvExperimentalUseError) {
+			t.Errorf("riscv-clang without ack flag didn't produce stderr with error; got: %q", stderr)
+		}
+	})
+}
+
+func TestRiscvBuildWithoutAckFlagOnAndroid(t *testing.T) {
+	withAndroidTestContext(t, func(ctx *testContext) {
+		// Just make sure no errors are raised. (Yes, this technically uses a cros triple on android;
+		// it's close enough).
+		ctx.must(callCompiler(ctx, ctx.cfg, ctx.newCommand(clangRiscv, mainCc)))
+	})
+}
diff --git a/compiler_wrapper/rusage_flag_test.go b/compiler_wrapper/rusage_flag_test.go
index 67021662..3573b187 100644
--- a/compiler_wrapper/rusage_flag_test.go
+++ b/compiler_wrapper/rusage_flag_test.go
@@ -9,7 +9,6 @@ import (
 	"errors"
 	"fmt"
 	"io"
-	"io/ioutil"
 	"os"
 	"path/filepath"
 	"regexp"
@@ -88,7 +87,7 @@ func TestLogRusageFileContent(t *testing.T) {
 		ctx.env = []string{"TOOLCHAIN_RUSAGE_OUTPUT=" + logFileName}
 		ctx.must(callCompiler(ctx, ctx.cfg, ctx.newCommand(gccX86_64, mainCc)))
 
-		data, err := ioutil.ReadFile(logFileName)
+		data, err := os.ReadFile(logFileName)
 		if err != nil {
 			t.Errorf("could not read the rusage log file. Error: %s", err)
 		}
@@ -121,7 +120,7 @@ func TestLogRusageAppendsToFile(t *testing.T) {
 		ctx.env = []string{"TOOLCHAIN_RUSAGE_OUTPUT=" + logFileName}
 
 		ctx.must(callCompiler(ctx, ctx.cfg, ctx.newCommand(gccX86_64, mainCc)))
-		data, err := ioutil.ReadFile(logFileName)
+		data, err := os.ReadFile(logFileName)
 		if err != nil {
 			t.Errorf("could not read the rusage log file. Error: %s", err)
 		}
@@ -137,7 +136,7 @@ func TestLogRusageAppendsToFile(t *testing.T) {
 		}
 
 		ctx.must(callCompiler(ctx, ctx.cfg, ctx.newCommand(gccX86_64, mainCc)))
-		data, err = ioutil.ReadFile(logFileName)
+		data, err = os.ReadFile(logFileName)
 		if err != nil {
 			t.Errorf("could not read the rusage log file. Error: %s", err)
 		}
diff --git a/compiler_wrapper/testdata/android_golden/bisect.json b/compiler_wrapper/testdata/android_golden/bisect.json
deleted file mode 100644
index 1e657030..00000000
--- a/compiler_wrapper/testdata/android_golden/bisect.json
+++ /dev/null
@@ -1,112 +0,0 @@
-[
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "/user/home/ANDROID_BISECT",
-            "/tmp/stable/clang.real",
-            "main.cc"
-          ],
-          "env_updates": [
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        }
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "/tmp/stable/clang"
-      }
-    }
-  },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "someBisectDir",
-            "/tmp/stable/clang.real",
-            "main.cc"
-          ],
-          "env_updates": [
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        }
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "BISECT_DIR=someBisectDir",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "/tmp/stable/clang"
-      }
-    }
-  },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "someBisectDir",
-            "/tmp/stable/clang.real",
-            "main.cc"
-          ],
-          "env_updates": [
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        },
-        "exitcode": 1,
-        "stderr": "someerror",
-        "stdout": "somemessage"
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "BISECT_DIR=someBisectDir",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "/tmp/stable/clang"
-      },
-      "exitcode": 1,
-      "stderr": "someerror",
-      "stdout": "somemessage"
-    }
-  }
-]
diff --git a/compiler_wrapper/testdata/android_golden/clang_path.json b/compiler_wrapper/testdata/android_golden/clang_path.json
index 42d487b8..aacbbeca 100644
--- a/compiler_wrapper/testdata/android_golden/clang_path.json
+++ b/compiler_wrapper/testdata/android_golden/clang_path.json
@@ -6,7 +6,7 @@
           "args": [
             "main.cc"
           ],
-          "path": "/tmp/stable/clang.real"
+          "path": "/tmp/stable/clang-real"
         }
       }
     ],
@@ -27,7 +27,7 @@
           "args": [
             "main.cc"
           ],
-          "path": "/tmp/stable/clang.real"
+          "path": "/tmp/stable/clang-real"
         },
         "exitcode": 1,
         "stderr": "someerror",
@@ -54,7 +54,7 @@
           "args": [
             "main.cc"
           ],
-          "path": "/tmp/stable/clang.real"
+          "path": "/tmp/stable/clang-real"
         }
       }
     ],
@@ -78,7 +78,7 @@
           "args": [
             "main.cc"
           ],
-          "path": "/tmp/stable/clang++.real"
+          "path": "/tmp/stable/clang++-real"
         }
       }
     ],
@@ -99,7 +99,7 @@
           "args": [
             "main.cc"
           ],
-          "path": "/tmp/stable/clang-tidy.real"
+          "path": "/tmp/stable/clang-tidy-real"
         }
       }
     ],
@@ -120,7 +120,7 @@
           "args": [
             "main.cc"
           ],
-          "path": "/tmp/stable/clang-tidy.real"
+          "path": "/tmp/stable/clang-tidy-real"
         }
       }
     ],
@@ -144,7 +144,7 @@
           "args": [
             "main.cc"
           ],
-          "path": "a/b/c/d/e/f/g/clang.real"
+          "path": "a/b/c/d/e/f/g/clang-real"
         }
       }
     ],
@@ -165,7 +165,7 @@
           "args": [
             "main.cc"
           ],
-          "path": "symlinked/clang.real"
+          "path": "symlinked/clang-real"
         }
       }
     ],
@@ -186,7 +186,7 @@
           "args": [
             "main.cc"
           ],
-          "path": "/tmp/stable/pathenv/clang.real"
+          "path": "/tmp/stable/pathenv/clang-real"
         }
       }
     ],
@@ -208,7 +208,7 @@
       {
         "cmd": {
           "args": [
-            "/tmp/stable/clang.real",
+            "/tmp/stable/clang-real",
             "main.cc"
           ],
           "path": "/tmp/stable/gomacc"
diff --git a/compiler_wrapper/testdata/android_golden/compile_with_fallback.json b/compiler_wrapper/testdata/android_golden/compile_with_fallback.json
index b8ef740d..eb285f60 100644
--- a/compiler_wrapper/testdata/android_golden/compile_with_fallback.json
+++ b/compiler_wrapper/testdata/android_golden/compile_with_fallback.json
@@ -9,7 +9,7 @@
             "-a",
             "-b"
           ],
-          "path": "/tmp/stable/clang.real"
+          "path": "/tmp/stable/clang-real"
         }
       }
     ],
@@ -38,7 +38,7 @@
             "-a",
             "-b"
           ],
-          "path": "/tmp/stable/clang.real"
+          "path": "/tmp/stable/clang-real"
         },
         "exitcode": 1
       },
@@ -79,7 +79,7 @@
             "-a",
             "-b"
           ],
-          "path": "/tmp/stable/clang.real"
+          "path": "/tmp/stable/clang-real"
         },
         "exitcode": 1
       },
diff --git a/compiler_wrapper/testdata/cros_clang_host_golden/bisect.json b/compiler_wrapper/testdata/cros_clang_host_golden/bisect.json
deleted file mode 100644
index 7526162a..00000000
--- a/compiler_wrapper/testdata/cros_clang_host_golden/bisect.json
+++ /dev/null
@@ -1,172 +0,0 @@
-[
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "/tmp/sysroot_bisect",
-            "/tmp/stable/clang",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "-Wno-unused-local-typedefs",
-            "-fno-addrsig",
-            "-fcommon",
-            "main.cc"
-          ],
-          "env_updates": [
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        }
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "someBisectDir",
-            "/tmp/stable/clang",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "-Wno-unused-local-typedefs",
-            "-fno-addrsig",
-            "-fcommon",
-            "main.cc"
-          ],
-          "env_updates": [
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        }
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "BISECT_DIR=someBisectDir",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "someBisectDir",
-            "/tmp/stable/clang",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "-Wno-unused-local-typedefs",
-            "-fno-addrsig",
-            "-fcommon",
-            "main.cc"
-          ],
-          "env_updates": [
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        },
-        "exitcode": 1,
-        "stderr": "someerror",
-        "stdout": "somemessage"
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "BISECT_DIR=someBisectDir",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      },
-      "exitcode": 1,
-      "stderr": "someerror",
-      "stdout": "somemessage"
-    }
-  }
-]
diff --git a/compiler_wrapper/testdata/cros_clang_host_golden/clang_ftrapv_maincc_target_specific.json b/compiler_wrapper/testdata/cros_clang_host_golden/clang_ftrapv_maincc_target_specific.json
index e60ff52b..786bdd9f 100644
--- a/compiler_wrapper/testdata/cros_clang_host_golden/clang_ftrapv_maincc_target_specific.json
+++ b/compiler_wrapper/testdata/cros_clang_host_golden/clang_ftrapv_maincc_target_specific.json
@@ -7,20 +7,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -49,20 +43,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -91,20 +79,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -133,20 +115,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -175,20 +151,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -217,20 +187,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -259,20 +223,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -301,20 +259,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -343,20 +295,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
diff --git a/compiler_wrapper/testdata/cros_clang_host_golden/clang_host_wrapper.json b/compiler_wrapper/testdata/cros_clang_host_golden/clang_host_wrapper.json
index 5bd833fa..501c77d3 100644
--- a/compiler_wrapper/testdata/cros_clang_host_golden/clang_host_wrapper.json
+++ b/compiler_wrapper/testdata/cros_clang_host_golden/clang_host_wrapper.json
@@ -7,20 +7,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
diff --git a/compiler_wrapper/testdata/cros_clang_host_golden/clang_maincc_target_specific.json b/compiler_wrapper/testdata/cros_clang_host_golden/clang_maincc_target_specific.json
index f0b6b976..8133e58b 100644
--- a/compiler_wrapper/testdata/cros_clang_host_golden/clang_maincc_target_specific.json
+++ b/compiler_wrapper/testdata/cros_clang_host_golden/clang_maincc_target_specific.json
@@ -7,20 +7,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -48,20 +42,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -89,20 +77,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -130,20 +112,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -171,20 +147,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -212,20 +182,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -253,20 +217,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -294,20 +252,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -335,20 +287,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
diff --git a/compiler_wrapper/testdata/cros_clang_host_golden/clang_path.json b/compiler_wrapper/testdata/cros_clang_host_golden/clang_path.json
index f44863b1..8a636137 100644
--- a/compiler_wrapper/testdata/cros_clang_host_golden/clang_path.json
+++ b/compiler_wrapper/testdata/cros_clang_host_golden/clang_path.json
@@ -7,20 +7,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -48,20 +42,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -95,20 +83,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -128,50 +110,6 @@
       }
     }
   },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "-Wno-unused-local-typedefs",
-            "-fno-addrsig",
-            "-fcommon",
-            "main.cc"
-          ],
-          "path": "somepath/clang"
-        }
-      }
-    ],
-    "env": [
-      "CLANG=somepath/clang"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
   {
     "cmds": [
       {
@@ -189,20 +127,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -243,20 +175,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -299,20 +225,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -349,20 +269,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -390,20 +304,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -431,20 +339,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -472,20 +374,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -513,20 +409,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
diff --git a/compiler_wrapper/testdata/cros_clang_host_golden/clang_sanitizer_args.json b/compiler_wrapper/testdata/cros_clang_host_golden/clang_sanitizer_args.json
index 707915c8..3907d27d 100644
--- a/compiler_wrapper/testdata/cros_clang_host_golden/clang_sanitizer_args.json
+++ b/compiler_wrapper/testdata/cros_clang_host_golden/clang_sanitizer_args.json
@@ -7,20 +7,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -51,20 +45,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -95,20 +83,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -139,20 +121,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -183,20 +159,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -226,20 +196,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -271,20 +235,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -314,20 +272,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
diff --git a/compiler_wrapper/testdata/cros_clang_host_golden/clang_specific_args.json b/compiler_wrapper/testdata/cros_clang_host_golden/clang_specific_args.json
index 9e236bd5..c1a59574 100644
--- a/compiler_wrapper/testdata/cros_clang_host_golden/clang_specific_args.json
+++ b/compiler_wrapper/testdata/cros_clang_host_golden/clang_specific_args.json
@@ -7,20 +7,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -66,20 +60,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -109,20 +97,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -152,20 +134,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
diff --git a/compiler_wrapper/testdata/cros_clang_host_golden/clangtidy.json b/compiler_wrapper/testdata/cros_clang_host_golden/clangtidy.json
index cc3269ce..9c4d634c 100644
--- a/compiler_wrapper/testdata/cros_clang_host_golden/clangtidy.json
+++ b/compiler_wrapper/testdata/cros_clang_host_golden/clangtidy.json
@@ -20,20 +20,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -48,20 +42,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -105,20 +93,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -134,20 +116,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -192,20 +168,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -224,20 +194,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -284,20 +248,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -313,20 +271,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
diff --git a/compiler_wrapper/testdata/cros_clang_host_golden/force_disable_werror.json b/compiler_wrapper/testdata/cros_clang_host_golden/force_disable_werror.json
index 9851d54d..6732db88 100644
--- a/compiler_wrapper/testdata/cros_clang_host_golden/force_disable_werror.json
+++ b/compiler_wrapper/testdata/cros_clang_host_golden/force_disable_werror.json
@@ -7,20 +7,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -51,20 +45,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -81,20 +69,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -128,20 +110,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
@@ -158,20 +134,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-unused-local-typedefs",
             "-fno-addrsig",
             "-fcommon",
diff --git a/compiler_wrapper/testdata/cros_hardened_golden/bisect.json b/compiler_wrapper/testdata/cros_hardened_golden/bisect.json
deleted file mode 100644
index fac6fd4f..00000000
--- a/compiler_wrapper/testdata/cros_hardened_golden/bisect.json
+++ /dev/null
@@ -1,223 +0,0 @@
-[
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "/tmp/sysroot_bisect",
-            "/usr/bin/ccache",
-            "../../usr/bin/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "--unwindlib=libunwind",
-            "-Wno-section",
-            "-fno-addrsig",
-            "-ftrivial-auto-var-init=zero",
-            "-fcommon",
-            "-fstack-protector-strong",
-            "-D_FORTIFY_SOURCE=3",
-            "-fno-omit-frame-pointer",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "CCACHE_DIR=/var/cache/distfiles/ccache",
-            "CCACHE_UMASK=002",
-            "CCACHE_CPP2=yes",
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        }
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "someBisectDir",
-            "/usr/bin/ccache",
-            "../../usr/bin/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "--unwindlib=libunwind",
-            "-Wno-section",
-            "-fno-addrsig",
-            "-ftrivial-auto-var-init=zero",
-            "-fcommon",
-            "-fstack-protector-strong",
-            "-D_FORTIFY_SOURCE=3",
-            "-fno-omit-frame-pointer",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "CCACHE_DIR=/var/cache/distfiles/ccache",
-            "CCACHE_UMASK=002",
-            "CCACHE_CPP2=yes",
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        }
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "BISECT_DIR=someBisectDir",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "someBisectDir",
-            "/usr/bin/ccache",
-            "../../usr/bin/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "--unwindlib=libunwind",
-            "-Wno-section",
-            "-fno-addrsig",
-            "-ftrivial-auto-var-init=zero",
-            "-fcommon",
-            "-fstack-protector-strong",
-            "-D_FORTIFY_SOURCE=3",
-            "-fno-omit-frame-pointer",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "CCACHE_DIR=/var/cache/distfiles/ccache",
-            "CCACHE_UMASK=002",
-            "CCACHE_CPP2=yes",
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        },
-        "exitcode": 1,
-        "stderr": "someerror",
-        "stdout": "somemessage"
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "BISECT_DIR=someBisectDir",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      },
-      "exitcode": 1,
-      "stderr": "someerror",
-      "stdout": "somemessage"
-    }
-  }
-]
diff --git a/compiler_wrapper/testdata/cros_hardened_golden/clang_ftrapv_maincc_target_specific.json b/compiler_wrapper/testdata/cros_hardened_golden/clang_ftrapv_maincc_target_specific.json
index cd188ba5..2bf314bd 100644
--- a/compiler_wrapper/testdata/cros_hardened_golden/clang_ftrapv_maincc_target_specific.json
+++ b/compiler_wrapper/testdata/cros_hardened_golden/clang_ftrapv_maincc_target_specific.json
@@ -9,20 +9,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -71,20 +65,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -133,20 +121,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -195,20 +177,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -256,20 +232,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -317,20 +287,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -378,20 +342,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -439,20 +397,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -500,20 +452,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_golden/clang_maincc_target_specific.json b/compiler_wrapper/testdata/cros_hardened_golden/clang_maincc_target_specific.json
index 09b3dbf3..be4f98e7 100644
--- a/compiler_wrapper/testdata/cros_hardened_golden/clang_maincc_target_specific.json
+++ b/compiler_wrapper/testdata/cros_hardened_golden/clang_maincc_target_specific.json
@@ -9,20 +9,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -69,20 +63,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -129,20 +117,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -189,20 +171,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -248,20 +224,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -307,20 +277,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -366,20 +330,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -425,20 +383,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -484,20 +436,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_golden/clang_path.json b/compiler_wrapper/testdata/cros_hardened_golden/clang_path.json
index 88e62ba6..8b2fd43c 100644
--- a/compiler_wrapper/testdata/cros_hardened_golden/clang_path.json
+++ b/compiler_wrapper/testdata/cros_hardened_golden/clang_path.json
@@ -9,20 +9,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -69,20 +63,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -135,20 +123,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -185,69 +167,6 @@
       }
     }
   },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "somepath/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "--unwindlib=libunwind",
-            "-Wno-section",
-            "-fno-addrsig",
-            "-ftrivial-auto-var-init=zero",
-            "-fcommon",
-            "-fstack-protector-strong",
-            "-D_FORTIFY_SOURCE=3",
-            "-fno-omit-frame-pointer",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "CCACHE_DIR=/var/cache/distfiles/ccache",
-            "CCACHE_UMASK=002",
-            "CCACHE_CPP2=yes"
-          ],
-          "path": "/usr/bin/ccache"
-        }
-      }
-    ],
-    "env": [
-      "CLANG=somepath/clang"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
   {
     "cmds": [
       {
@@ -267,20 +186,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -339,20 +252,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -409,20 +316,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -478,20 +379,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -538,20 +433,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -598,20 +487,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -658,20 +541,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -718,20 +595,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_golden/clang_sanitizer_args.json b/compiler_wrapper/testdata/cros_hardened_golden/clang_sanitizer_args.json
index b9573920..17ea871d 100644
--- a/compiler_wrapper/testdata/cros_hardened_golden/clang_sanitizer_args.json
+++ b/compiler_wrapper/testdata/cros_hardened_golden/clang_sanitizer_args.json
@@ -9,20 +9,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -71,20 +65,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -133,20 +121,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -195,20 +177,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -257,20 +233,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -318,20 +288,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -381,20 +345,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -442,20 +400,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_golden/clang_specific_args.json b/compiler_wrapper/testdata/cros_hardened_golden/clang_specific_args.json
index 65733360..7fa682d1 100644
--- a/compiler_wrapper/testdata/cros_hardened_golden/clang_specific_args.json
+++ b/compiler_wrapper/testdata/cros_hardened_golden/clang_specific_args.json
@@ -9,20 +9,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -87,20 +81,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -149,20 +137,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -211,20 +193,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_golden/clang_sysroot_wrapper_common.json b/compiler_wrapper/testdata/cros_hardened_golden/clang_sysroot_wrapper_common.json
index e2bd5d2f..8b6a3826 100644
--- a/compiler_wrapper/testdata/cros_hardened_golden/clang_sysroot_wrapper_common.json
+++ b/compiler_wrapper/testdata/cros_hardened_golden/clang_sysroot_wrapper_common.json
@@ -42,20 +42,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -105,20 +99,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -163,20 +151,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -225,20 +207,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -287,20 +263,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -347,20 +317,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_golden/clangtidy.json b/compiler_wrapper/testdata/cros_hardened_golden/clangtidy.json
index dcad603d..5ef70ff4 100644
--- a/compiler_wrapper/testdata/cros_hardened_golden/clangtidy.json
+++ b/compiler_wrapper/testdata/cros_hardened_golden/clangtidy.json
@@ -21,20 +21,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -62,20 +56,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -132,20 +120,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -174,20 +156,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -245,20 +221,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -290,20 +260,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -363,20 +327,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -405,20 +363,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_golden/force_disable_werror.json b/compiler_wrapper/testdata/cros_hardened_golden/force_disable_werror.json
index ab17a915..793b8acc 100644
--- a/compiler_wrapper/testdata/cros_hardened_golden/force_disable_werror.json
+++ b/compiler_wrapper/testdata/cros_hardened_golden/force_disable_werror.json
@@ -9,20 +9,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -72,20 +66,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -121,20 +109,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -187,20 +169,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -236,20 +212,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_golden/gcc_clang_syntax.json b/compiler_wrapper/testdata/cros_hardened_golden/gcc_clang_syntax.json
index 719261f7..8d1b68d3 100644
--- a/compiler_wrapper/testdata/cros_hardened_golden/gcc_clang_syntax.json
+++ b/compiler_wrapper/testdata/cros_hardened_golden/gcc_clang_syntax.json
@@ -8,20 +8,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -90,20 +84,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -170,20 +158,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -233,20 +215,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/bisect.json b/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/bisect.json
deleted file mode 100644
index fac6fd4f..00000000
--- a/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/bisect.json
+++ /dev/null
@@ -1,223 +0,0 @@
-[
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "/tmp/sysroot_bisect",
-            "/usr/bin/ccache",
-            "../../usr/bin/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "--unwindlib=libunwind",
-            "-Wno-section",
-            "-fno-addrsig",
-            "-ftrivial-auto-var-init=zero",
-            "-fcommon",
-            "-fstack-protector-strong",
-            "-D_FORTIFY_SOURCE=3",
-            "-fno-omit-frame-pointer",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "CCACHE_DIR=/var/cache/distfiles/ccache",
-            "CCACHE_UMASK=002",
-            "CCACHE_CPP2=yes",
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        }
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "someBisectDir",
-            "/usr/bin/ccache",
-            "../../usr/bin/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "--unwindlib=libunwind",
-            "-Wno-section",
-            "-fno-addrsig",
-            "-ftrivial-auto-var-init=zero",
-            "-fcommon",
-            "-fstack-protector-strong",
-            "-D_FORTIFY_SOURCE=3",
-            "-fno-omit-frame-pointer",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "CCACHE_DIR=/var/cache/distfiles/ccache",
-            "CCACHE_UMASK=002",
-            "CCACHE_CPP2=yes",
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        }
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "BISECT_DIR=someBisectDir",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "someBisectDir",
-            "/usr/bin/ccache",
-            "../../usr/bin/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "--unwindlib=libunwind",
-            "-Wno-section",
-            "-fno-addrsig",
-            "-ftrivial-auto-var-init=zero",
-            "-fcommon",
-            "-fstack-protector-strong",
-            "-D_FORTIFY_SOURCE=3",
-            "-fno-omit-frame-pointer",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "CCACHE_DIR=/var/cache/distfiles/ccache",
-            "CCACHE_UMASK=002",
-            "CCACHE_CPP2=yes",
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        },
-        "exitcode": 1,
-        "stderr": "someerror",
-        "stdout": "somemessage"
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "BISECT_DIR=someBisectDir",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      },
-      "exitcode": 1,
-      "stderr": "someerror",
-      "stdout": "somemessage"
-    }
-  }
-]
diff --git a/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/clang_path.json b/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/clang_path.json
index 88e62ba6..8b2fd43c 100644
--- a/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/clang_path.json
+++ b/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/clang_path.json
@@ -9,20 +9,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -69,20 +63,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -135,20 +123,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -185,69 +167,6 @@
       }
     }
   },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "somepath/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "--unwindlib=libunwind",
-            "-Wno-section",
-            "-fno-addrsig",
-            "-ftrivial-auto-var-init=zero",
-            "-fcommon",
-            "-fstack-protector-strong",
-            "-D_FORTIFY_SOURCE=3",
-            "-fno-omit-frame-pointer",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "CCACHE_DIR=/var/cache/distfiles/ccache",
-            "CCACHE_UMASK=002",
-            "CCACHE_CPP2=yes"
-          ],
-          "path": "/usr/bin/ccache"
-        }
-      }
-    ],
-    "env": [
-      "CLANG=somepath/clang"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
   {
     "cmds": [
       {
@@ -267,20 +186,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -339,20 +252,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -409,20 +316,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -478,20 +379,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -538,20 +433,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -598,20 +487,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -658,20 +541,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -718,20 +595,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/clangtidy.json b/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/clangtidy.json
index dcad603d..5ef70ff4 100644
--- a/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/clangtidy.json
+++ b/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/clangtidy.json
@@ -21,20 +21,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -62,20 +56,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -132,20 +120,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -174,20 +156,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -245,20 +221,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -290,20 +260,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -363,20 +327,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -405,20 +363,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/force_disable_werror.json b/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/force_disable_werror.json
index ab17a915..793b8acc 100644
--- a/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/force_disable_werror.json
+++ b/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/force_disable_werror.json
@@ -9,20 +9,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -72,20 +66,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -121,20 +109,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -187,20 +169,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -236,20 +212,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/gcc_clang_syntax.json b/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/gcc_clang_syntax.json
index 719261f7..8d1b68d3 100644
--- a/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/gcc_clang_syntax.json
+++ b/compiler_wrapper/testdata/cros_hardened_llvmnext_golden/gcc_clang_syntax.json
@@ -8,20 +8,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -90,20 +84,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -170,20 +158,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -233,20 +215,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_noccache_golden/bisect.json b/compiler_wrapper/testdata/cros_hardened_noccache_golden/bisect.json
deleted file mode 100644
index 783d8650..00000000
--- a/compiler_wrapper/testdata/cros_hardened_noccache_golden/bisect.json
+++ /dev/null
@@ -1,211 +0,0 @@
-[
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "/tmp/sysroot_bisect",
-            "/usr/bin/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "--unwindlib=libunwind",
-            "-Wno-section",
-            "-fno-addrsig",
-            "-ftrivial-auto-var-init=zero",
-            "-fcommon",
-            "-fstack-protector-strong",
-            "-D_FORTIFY_SOURCE=3",
-            "-fno-omit-frame-pointer",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        }
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "someBisectDir",
-            "/usr/bin/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "--unwindlib=libunwind",
-            "-Wno-section",
-            "-fno-addrsig",
-            "-ftrivial-auto-var-init=zero",
-            "-fcommon",
-            "-fstack-protector-strong",
-            "-D_FORTIFY_SOURCE=3",
-            "-fno-omit-frame-pointer",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        }
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "BISECT_DIR=someBisectDir",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "someBisectDir",
-            "/usr/bin/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "--unwindlib=libunwind",
-            "-Wno-section",
-            "-fno-addrsig",
-            "-ftrivial-auto-var-init=zero",
-            "-fcommon",
-            "-fstack-protector-strong",
-            "-D_FORTIFY_SOURCE=3",
-            "-fno-omit-frame-pointer",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        },
-        "exitcode": 1,
-        "stderr": "someerror",
-        "stdout": "somemessage"
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "BISECT_DIR=someBisectDir",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      },
-      "exitcode": 1,
-      "stderr": "someerror",
-      "stdout": "somemessage"
-    }
-  }
-]
diff --git a/compiler_wrapper/testdata/cros_hardened_noccache_golden/clang_path.json b/compiler_wrapper/testdata/cros_hardened_noccache_golden/clang_path.json
index 95c1e482..2eeddea5 100644
--- a/compiler_wrapper/testdata/cros_hardened_noccache_golden/clang_path.json
+++ b/compiler_wrapper/testdata/cros_hardened_noccache_golden/clang_path.json
@@ -8,20 +8,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -62,20 +56,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -122,20 +110,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -167,63 +149,6 @@
       }
     }
   },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "--unwindlib=libunwind",
-            "-Wno-section",
-            "-fno-addrsig",
-            "-ftrivial-auto-var-init=zero",
-            "-fcommon",
-            "-fstack-protector-strong",
-            "-D_FORTIFY_SOURCE=3",
-            "-fno-omit-frame-pointer",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "path": "somepath/clang"
-        }
-      }
-    ],
-    "env": [
-      "CLANG=somepath/clang"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
   {
     "cmds": [
       {
@@ -242,20 +167,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -309,20 +228,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -378,20 +291,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -441,20 +348,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -495,20 +396,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -549,20 +444,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -603,20 +492,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -657,20 +540,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_noccache_golden/clangtidy.json b/compiler_wrapper/testdata/cros_hardened_noccache_golden/clangtidy.json
index dcad603d..5ef70ff4 100644
--- a/compiler_wrapper/testdata/cros_hardened_noccache_golden/clangtidy.json
+++ b/compiler_wrapper/testdata/cros_hardened_noccache_golden/clangtidy.json
@@ -21,20 +21,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -62,20 +56,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -132,20 +120,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -174,20 +156,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -245,20 +221,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -290,20 +260,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -363,20 +327,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -405,20 +363,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_noccache_golden/force_disable_werror.json b/compiler_wrapper/testdata/cros_hardened_noccache_golden/force_disable_werror.json
index 7fef9a1d..7b520ed0 100644
--- a/compiler_wrapper/testdata/cros_hardened_noccache_golden/force_disable_werror.json
+++ b/compiler_wrapper/testdata/cros_hardened_noccache_golden/force_disable_werror.json
@@ -8,20 +8,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -65,20 +59,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -108,20 +96,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -168,20 +150,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -211,20 +187,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_hardened_noccache_golden/gcc_clang_syntax.json b/compiler_wrapper/testdata/cros_hardened_noccache_golden/gcc_clang_syntax.json
index f0469d47..59bfa8dd 100644
--- a/compiler_wrapper/testdata/cros_hardened_noccache_golden/gcc_clang_syntax.json
+++ b/compiler_wrapper/testdata/cros_hardened_noccache_golden/gcc_clang_syntax.json
@@ -8,20 +8,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -85,20 +79,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -165,20 +153,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
@@ -228,20 +210,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "--unwindlib=libunwind",
             "-Wno-section",
             "-fno-addrsig",
diff --git a/compiler_wrapper/testdata/cros_nonhardened_golden/bisect.json b/compiler_wrapper/testdata/cros_nonhardened_golden/bisect.json
deleted file mode 100644
index 8c25b715..00000000
--- a/compiler_wrapper/testdata/cros_nonhardened_golden/bisect.json
+++ /dev/null
@@ -1,202 +0,0 @@
-[
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "/tmp/sysroot_bisect",
-            "/usr/bin/ccache",
-            "../../usr/bin/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "-Wno-section",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "CCACHE_DIR=/var/cache/distfiles/ccache",
-            "CCACHE_UMASK=002",
-            "CCACHE_CPP2=yes",
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        }
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "someBisectDir",
-            "/usr/bin/ccache",
-            "../../usr/bin/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "-Wno-section",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "CCACHE_DIR=/var/cache/distfiles/ccache",
-            "CCACHE_UMASK=002",
-            "CCACHE_CPP2=yes",
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        }
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "BISECT_DIR=someBisectDir",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "python3",
-            "-c",
-            "\nimport bisect_driver\nimport shlex\nimport sys\n\ndef ExpandArgs(args, target):\n\tfor arg in args:\n\t\tif arg[0] == '@':\n\t\t\twith open(arg[1:], 'r', encoding='utf-8') as f:\n\t\t\t\tExpandArgs(shlex.split(f.read()), target)\n\t\telse:\n\t\t\ttarget.append(arg)\n\treturn target\n\nstage = sys.argv[1]\ndir = sys.argv[2]\nexecargs = ExpandArgs(sys.argv[3:], [])\n\nsys.exit(bisect_driver.bisect_driver(stage, dir, execargs))\n",
-            "someBisectStage",
-            "someBisectDir",
-            "/usr/bin/ccache",
-            "../../usr/bin/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "-Wno-section",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "CCACHE_DIR=/var/cache/distfiles/ccache",
-            "CCACHE_UMASK=002",
-            "CCACHE_CPP2=yes",
-            "PYTHONPATH=/somepath/test_binary"
-          ],
-          "path": "/usr/bin/env"
-        },
-        "exitcode": 1,
-        "stderr": "someerror",
-        "stdout": "somemessage"
-      }
-    ],
-    "env": [
-      "BISECT_STAGE=someBisectStage",
-      "BISECT_DIR=someBisectDir",
-      "HOME=/user/home"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      },
-      "exitcode": 1,
-      "stderr": "someerror",
-      "stdout": "somemessage"
-    }
-  }
-]
diff --git a/compiler_wrapper/testdata/cros_nonhardened_golden/clang_ftrapv_maincc_target_specific.json b/compiler_wrapper/testdata/cros_nonhardened_golden/clang_ftrapv_maincc_target_specific.json
index f81d88c6..a93711a9 100644
--- a/compiler_wrapper/testdata/cros_nonhardened_golden/clang_ftrapv_maincc_target_specific.json
+++ b/compiler_wrapper/testdata/cros_nonhardened_golden/clang_ftrapv_maincc_target_specific.json
@@ -9,20 +9,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -64,20 +58,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-eabi-",
@@ -119,20 +107,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-win-gnu-",
@@ -174,20 +156,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "-mthumb",
@@ -229,20 +205,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/armv7m-cros-eabi-",
@@ -283,20 +253,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "-mthumb",
@@ -338,20 +302,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "-mthumb",
@@ -393,20 +351,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/armv8m-cros-eabi-",
@@ -447,20 +399,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "-mthumb",
diff --git a/compiler_wrapper/testdata/cros_nonhardened_golden/clang_maincc_target_specific.json b/compiler_wrapper/testdata/cros_nonhardened_golden/clang_maincc_target_specific.json
index 5d8f5bea..3548ec8e 100644
--- a/compiler_wrapper/testdata/cros_nonhardened_golden/clang_maincc_target_specific.json
+++ b/compiler_wrapper/testdata/cros_nonhardened_golden/clang_maincc_target_specific.json
@@ -9,20 +9,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -62,20 +56,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-eabi-",
@@ -115,20 +103,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-win-gnu-",
@@ -168,20 +150,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "-mthumb",
@@ -221,20 +197,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/armv7m-cros-eabi-",
@@ -273,20 +243,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "-mthumb",
@@ -326,20 +290,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "-mthumb",
@@ -379,20 +337,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/armv8m-cros-eabi-",
@@ -431,20 +383,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "-mthumb",
diff --git a/compiler_wrapper/testdata/cros_nonhardened_golden/clang_path.json b/compiler_wrapper/testdata/cros_nonhardened_golden/clang_path.json
index 768f2b28..a089f568 100644
--- a/compiler_wrapper/testdata/cros_nonhardened_golden/clang_path.json
+++ b/compiler_wrapper/testdata/cros_nonhardened_golden/clang_path.json
@@ -9,20 +9,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -62,20 +56,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -121,20 +109,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -164,62 +146,6 @@
       }
     }
   },
-  {
-    "cmds": [
-      {
-        "cmd": {
-          "args": [
-            "somepath/clang",
-            "--sysroot=/usr/x86_64-cros-linux-gnu",
-            "-Qunused-arguments",
-            "-Werror=poison-system-directories",
-            "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
-            "-Wno-error=implicit-function-declaration",
-            "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
-            "-Wno-unknown-warning-option",
-            "-fdebug-default-version=5",
-            "-Wno-int-conversion",
-            "-Wno-incompatible-function-pointer-types",
-            "-Wno-error=vla-cxx-extension",
-            "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
-            "-Wno-section",
-            "-static-libgcc",
-            "--prefix=../../bin/x86_64-cros-linux-gnu-",
-            "main.cc",
-            "-L/usr/x86_64-cros-linux-gnu/usr/lib64",
-            "-mno-movbe",
-            "-B../../bin",
-            "-target",
-            "x86_64-cros-linux-gnu"
-          ],
-          "env_updates": [
-            "CCACHE_DIR=/var/cache/distfiles/ccache",
-            "CCACHE_UMASK=002",
-            "CCACHE_CPP2=yes"
-          ],
-          "path": "/usr/bin/ccache"
-        }
-      }
-    ],
-    "env": [
-      "CLANG=somepath/clang"
-    ],
-    "wd": "/tmp/stable",
-    "wrapper": {
-      "cmd": {
-        "args": [
-          "main.cc"
-        ],
-        "path": "./x86_64-cros-linux-gnu-clang"
-      }
-    }
-  },
   {
     "cmds": [
       {
@@ -239,20 +165,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -304,20 +224,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -367,20 +281,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -429,20 +337,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -482,20 +384,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=a/b/c/d/e/bin/x86_64-cros-linux-gnu-",
@@ -535,20 +431,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=a/b/c/d/e/bin/x86_64-cros-linux-gnu-",
@@ -588,20 +478,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../bin/x86_64-cros-linux-gnu-",
@@ -641,20 +525,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../bin/x86_64-cros-linux-gnu-",
diff --git a/compiler_wrapper/testdata/cros_nonhardened_golden/clang_sanitizer_args.json b/compiler_wrapper/testdata/cros_nonhardened_golden/clang_sanitizer_args.json
index 10329a58..341612ff 100644
--- a/compiler_wrapper/testdata/cros_nonhardened_golden/clang_sanitizer_args.json
+++ b/compiler_wrapper/testdata/cros_nonhardened_golden/clang_sanitizer_args.json
@@ -9,20 +9,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -65,20 +59,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -121,20 +109,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -177,20 +159,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -233,20 +209,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -288,20 +258,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -345,20 +309,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -400,20 +358,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
diff --git a/compiler_wrapper/testdata/cros_nonhardened_golden/clang_specific_args.json b/compiler_wrapper/testdata/cros_nonhardened_golden/clang_specific_args.json
index be4f57a5..56500e9d 100644
--- a/compiler_wrapper/testdata/cros_nonhardened_golden/clang_specific_args.json
+++ b/compiler_wrapper/testdata/cros_nonhardened_golden/clang_specific_args.json
@@ -9,20 +9,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -80,20 +74,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -135,20 +123,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -190,20 +172,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
diff --git a/compiler_wrapper/testdata/cros_nonhardened_golden/clang_sysroot_wrapper_common.json b/compiler_wrapper/testdata/cros_nonhardened_golden/clang_sysroot_wrapper_common.json
index 4f2bddb6..95b0cd1a 100644
--- a/compiler_wrapper/testdata/cros_nonhardened_golden/clang_sysroot_wrapper_common.json
+++ b/compiler_wrapper/testdata/cros_nonhardened_golden/clang_sysroot_wrapper_common.json
@@ -39,20 +39,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -95,20 +89,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -146,20 +134,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -201,20 +183,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "-fno-stack-protector",
@@ -257,20 +233,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "-mthumb",
@@ -312,20 +282,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
diff --git a/compiler_wrapper/testdata/cros_nonhardened_golden/clangtidy.json b/compiler_wrapper/testdata/cros_nonhardened_golden/clangtidy.json
index e85c065f..8fcd8080 100644
--- a/compiler_wrapper/testdata/cros_nonhardened_golden/clangtidy.json
+++ b/compiler_wrapper/testdata/cros_nonhardened_golden/clangtidy.json
@@ -21,20 +21,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -55,20 +49,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -118,20 +106,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -153,20 +135,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -217,20 +193,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -255,20 +225,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -321,20 +285,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -356,20 +314,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
diff --git a/compiler_wrapper/testdata/cros_nonhardened_golden/force_disable_werror.json b/compiler_wrapper/testdata/cros_nonhardened_golden/force_disable_werror.json
index 7e4b4d6f..eb29a81f 100644
--- a/compiler_wrapper/testdata/cros_nonhardened_golden/force_disable_werror.json
+++ b/compiler_wrapper/testdata/cros_nonhardened_golden/force_disable_werror.json
@@ -9,20 +9,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -65,20 +59,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -107,20 +95,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -166,20 +148,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -208,20 +184,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
diff --git a/compiler_wrapper/testdata/cros_nonhardened_golden/gcc_clang_syntax.json b/compiler_wrapper/testdata/cros_nonhardened_golden/gcc_clang_syntax.json
index daa023fc..f1ab3406 100644
--- a/compiler_wrapper/testdata/cros_nonhardened_golden/gcc_clang_syntax.json
+++ b/compiler_wrapper/testdata/cros_nonhardened_golden/gcc_clang_syntax.json
@@ -8,20 +8,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -80,20 +74,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -150,20 +138,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
@@ -206,20 +188,14 @@
             "-Qunused-arguments",
             "-Werror=poison-system-directories",
             "-Wno-deprecated-declarations",
-            "-Wno-enum-constexpr-conversion",
             "-Wno-error=implicit-function-declaration",
             "-Wno-error=implicit-int",
-            "-Wno-final-dtor-non-final-class",
-            "-Wno-single-bit-bitfield-constant-conversion",
-            "-Wno-tautological-constant-compare",
-            "-Wno-tautological-unsigned-enum-zero-compare",
             "-Wno-unknown-warning-option",
             "-fdebug-default-version=5",
             "-Wno-int-conversion",
             "-Wno-incompatible-function-pointer-types",
             "-Wno-error=vla-cxx-extension",
             "-D_LIBCPP_ENABLE_CXX17_REMOVED_FEATURES",
-            "-fclang-abi-compat=17",
             "-Wno-section",
             "-static-libgcc",
             "--prefix=../../bin/x86_64-cros-linux-gnu-",
diff --git a/compiler_wrapper/testutil_test.go b/compiler_wrapper/testutil_test.go
index 6c5f1da8..4dbc4984 100644
--- a/compiler_wrapper/testutil_test.go
+++ b/compiler_wrapper/testutil_test.go
@@ -8,7 +8,6 @@ import (
 	"bytes"
 	"fmt"
 	"io"
-	"io/ioutil"
 	"os"
 	"os/exec"
 	"path/filepath"
@@ -24,6 +23,7 @@ const (
 	mainCc           = "main.cc"
 	clangAndroid     = "./clang"
 	clangTidyAndroid = "./clang-tidy"
+	clangRiscv       = "./riscv32-cros-elf-clang"
 	clangX86_64      = "./x86_64-cros-linux-gnu-clang"
 	gccX86_64        = "./x86_64-cros-linux-gnu-gcc"
 	gccX86_64Eabi    = "./x86_64-cros-eabi-gcc"
@@ -56,7 +56,7 @@ var umaskModificationLock sync.RWMutex
 
 func withTestContext(t *testing.T, work func(ctx *testContext)) {
 	t.Parallel()
-	tempDir, err := ioutil.TempDir("", "compiler_wrapper")
+	tempDir, err := os.MkdirTemp("", "compiler_wrapper")
 	if err != nil {
 		t.Fatalf("Unable to create the temp dir. Error: %s", err)
 	}
@@ -215,7 +215,7 @@ func (ctx *testContext) writeFile(fullFileName string, fileContent string) {
 	if err := os.MkdirAll(filepath.Dir(fullFileName), 0777); err != nil {
 		ctx.t.Fatal(err)
 	}
-	if err := ioutil.WriteFile(fullFileName, []byte(fileContent), 0777); err != nil {
+	if err := os.WriteFile(fullFileName, []byte(fileContent), 0777); err != nil {
 		ctx.t.Fatal(err)
 	}
 }
@@ -239,7 +239,7 @@ func (ctx *testContext) readAllString(r io.Reader) string {
 	if r == nil {
 		return ""
 	}
-	bytes, err := ioutil.ReadAll(r)
+	bytes, err := io.ReadAll(r)
 	if err != nil {
 		ctx.t.Fatal(err)
 	}
diff --git a/crate_ebuild_help.py b/crate_ebuild_help.py
old mode 100755
new mode 100644
index c66b9897..9a9dc5af
--- a/crate_ebuild_help.py
+++ b/crate_ebuild_help.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2022 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -171,7 +170,3 @@ def main():
                 j = i + 1
                 print(f"[{j}/{crates_len}] {s}")
     print()
-
-
-if __name__ == "__main__":
-    main()
diff --git a/cros_utils/bugs.py b/cros_utils/bugs.py
old mode 100755
new mode 100644
index 423faa8b..63633125
--- a/cros_utils/bugs.py
+++ b/cros_utils/bugs.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2021 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
diff --git a/cros_utils/bugs_test.py b/cros_utils/bugs_test.py
old mode 100755
new mode 100644
index 21ed2154..70156a20
--- a/cros_utils/bugs_test.py
+++ b/cros_utils/bugs_test.py
@@ -239,7 +239,3 @@ class Tests(unittest.TestCase):
             bugs.AppendToExistingBug(1, "body", directory=tmpdir)
             json_files = list(Path(tmpdir).glob("*.json"))
             self.assertEqual(len(json_files), 1, json_files)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/cros_utils/buildbot_utils.py b/cros_utils/buildbot_utils.py
deleted file mode 100644
index 36c68c13..00000000
--- a/cros_utils/buildbot_utils.py
+++ /dev/null
@@ -1,306 +0,0 @@
-# Copyright 2017 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Utilities for launching and accessing ChromeOS buildbots."""
-
-
-import ast
-import json
-import os
-import re
-import time
-
-from cros_utils import command_executer
-from cros_utils import logger
-
-
-INITIAL_SLEEP_TIME = 7200  # 2 hours; wait time before polling buildbot.
-SLEEP_TIME = 600  # 10 minutes; time between polling of buildbot.
-
-# Some of our slower builders (llvm-next) are taking more
-# than 12 hours. So, increase this TIME_OUT to 15 hours.
-TIME_OUT = 15 * 60 * 60  # Decide the build is dead or will never finish
-
-
-class BuildbotTimeout(Exception):
-    """Exception to throw when a buildbot operation timesout."""
-
-
-def RunCommandInPath(path, cmd):
-    ce = command_executer.GetCommandExecuter()
-    cwd = os.getcwd()
-    os.chdir(path)
-    status, stdout, stderr = ce.RunCommandWOutput(cmd, print_to_console=False)
-    os.chdir(cwd)
-    return status, stdout, stderr
-
-
-def PeekTrybotImage(chromeos_root, buildbucket_id):
-    """Get the artifact URL of a given tryjob.
-
-    Args:
-        buildbucket_id: buildbucket-id
-        chromeos_root: root dir of chrome os checkout
-
-    Returns:
-        (status, url) where status can be 'pass', 'fail', 'running',
-                    and url looks like:
-        gs://chromeos-image-archive/trybot-elm-release-tryjob/R67-10468.0.0-b20789
-    """
-    command = (
-        "cros buildresult --report json --buildbucket-id %s" % buildbucket_id
-    )
-    rc, out, _ = RunCommandInPath(chromeos_root, command)
-
-    # Current implementation of cros buildresult returns fail when a job is still
-    # running.
-    if rc != 0:
-        return ("running", None)
-
-    results = json.loads(out)[buildbucket_id]
-
-    # Handle the case where the tryjob failed to launch correctly.
-    if results["artifacts_url"] is None:
-        return (results["status"], "")
-
-    return (results["status"], results["artifacts_url"].rstrip("/"))
-
-
-def ParseTryjobBuildbucketId(msg):
-    """Find the buildbucket-id in the messages from `cros tryjob`.
-
-    Args:
-        msg: messages from `cros tryjob`
-
-    Returns:
-        buildbucket-id, which will be passed to `cros buildresult`
-    """
-    output_list = ast.literal_eval(msg)
-    output_dict = output_list[0]
-    if "buildbucket_id" in output_dict:
-        return output_dict["buildbucket_id"]
-    return None
-
-
-def SubmitTryjob(
-    chromeos_root,
-    buildbot_name,
-    patch_list,
-    tryjob_flags=None,
-    build_toolchain=False,
-):
-    """Calls `cros tryjob ...`
-
-    Args:
-        chromeos_root: the path to the ChromeOS root, needed for finding chromite
-            and launching the buildbot.
-        buildbot_name: the name of the buildbot queue, such as lumpy-release or
-            daisy-paladin.
-        patch_list: a python list of the patches, if any, for the buildbot to use.
-        tryjob_flags: See cros tryjob --help for available options.
-        build_toolchain: builds and uses the latest toolchain, rather than the
-            prebuilt one in SDK.
-
-    Returns:
-        buildbucket id
-    """
-    patch_arg = ""
-    if patch_list:
-        for p in patch_list:
-            patch_arg = patch_arg + " -g " + repr(p)
-    if not tryjob_flags:
-        tryjob_flags = []
-    if build_toolchain:
-        tryjob_flags.append("--latest-toolchain")
-    tryjob_flags = " ".join(tryjob_flags)
-
-    # Launch buildbot with appropriate flags.
-    build = buildbot_name
-    command = "cros_sdk -- cros tryjob --yes --json --nochromesdk  %s %s %s" % (
-        tryjob_flags,
-        patch_arg,
-        build,
-    )
-    print("CMD: %s" % command)
-    _, out, _ = RunCommandInPath(chromeos_root, command)
-    buildbucket_id = ParseTryjobBuildbucketId(out)
-    print("buildbucket_id: %s" % repr(buildbucket_id))
-    if not buildbucket_id:
-        logger.GetLogger().LogFatal(
-            "Error occurred while launching trybot job: " "%s" % command
-        )
-    return buildbucket_id
-
-
-def GetTrybotImage(
-    chromeos_root,
-    buildbot_name,
-    patch_list,
-    tryjob_flags=None,
-    build_toolchain=False,
-    asynchronous=False,
-):
-    """Launch buildbot and get resulting trybot artifact name.
-
-    This function launches a buildbot with the appropriate flags to
-    build the test ChromeOS image, with the current ToT mobile compiler.  It
-    checks every 10 minutes to see if the trybot has finished.  When the trybot
-    has finished, it parses the resulting report logs to find the trybot
-    artifact (if one was created), and returns that artifact name.
-
-    Args:
-        chromeos_root: the path to the ChromeOS root, needed for finding chromite
-            and launching the buildbot.
-        buildbot_name: the name of the buildbot queue, such as lumpy-release or
-            daisy-paladin.
-        patch_list: a python list of the patches, if any, for the buildbot to use.
-        tryjob_flags: See cros tryjob --help for available options.
-        build_toolchain: builds and uses the latest toolchain, rather than the
-                       prebuilt one in SDK.
-        asynchronous: don't wait for artifacts; just return the buildbucket id
-
-    Returns:
-        (buildbucket id, partial image url) e.g.
-        (8952271933586980528, trybot-elm-release-tryjob/R67-10480.0.0-b2373596)
-    """
-    buildbucket_id = SubmitTryjob(
-        chromeos_root, buildbot_name, patch_list, tryjob_flags, build_toolchain
-    )
-    if asynchronous:
-        return buildbucket_id, " "
-
-    # The trybot generally takes more than 2 hours to finish.
-    # Wait two hours before polling the status.
-    time.sleep(INITIAL_SLEEP_TIME)
-    elapsed = INITIAL_SLEEP_TIME
-    status = "running"
-    image = ""
-    while True:
-        status, image = PeekTrybotImage(chromeos_root, buildbucket_id)
-        if status == "running":
-            if elapsed > TIME_OUT:
-                logger.GetLogger().LogFatal(
-                    "Unable to get build result for target %s." % buildbot_name
-                )
-            else:
-                wait_msg = "Unable to find build result; job may be running."
-                logger.GetLogger().LogOutput(wait_msg)
-            logger.GetLogger().LogOutput(f"{elapsed / 60} minutes elapsed.")
-            logger.GetLogger().LogOutput(f"Sleeping {SLEEP_TIME} seconds.")
-            time.sleep(SLEEP_TIME)
-            elapsed += SLEEP_TIME
-        else:
-            break
-
-    if not buildbot_name.endswith("-toolchain") and status == "fail":
-        # For rotating testers, we don't care about their status
-        # result, because if any HWTest failed it will be non-zero.
-        #
-        # The nightly performance tests do not run HWTests, so if
-        # their status is non-zero, we do care.  In this case
-        # non-zero means the image itself probably did not build.
-        image = ""
-
-    if not image:
-        logger.GetLogger().LogError(
-            "Trybot job (buildbucket id: %s) failed with"
-            "status %s; no trybot image generated. " % (buildbucket_id, status)
-        )
-    else:
-        # Convert full gs path to what crosperf expects. For example, convert
-        # gs://chromeos-image-archive/trybot-elm-release-tryjob/R67-10468.0.0-b20789
-        # to
-        # trybot-elm-release-tryjob/R67-10468.0.0-b20789
-        image = "/".join(image.split("/")[-2:])
-
-    logger.GetLogger().LogOutput("image is '%s'" % image)
-    logger.GetLogger().LogOutput("status is %s" % status)
-    return buildbucket_id, image
-
-
-def DoesImageExist(chromeos_root, build):
-    """Check if the image for the given build exists."""
-
-    ce = command_executer.GetCommandExecuter()
-    command = (
-        "gsutil ls gs://chromeos-image-archive/%s"
-        "/chromiumos_test_image.tar.xz" % (build)
-    )
-    ret = ce.ChrootRunCommand(chromeos_root, command, print_to_console=False)
-    return not ret
-
-
-def WaitForImage(chromeos_root, build):
-    """Wait for an image to be ready."""
-
-    elapsed_time = 0
-    while elapsed_time < TIME_OUT:
-        if DoesImageExist(chromeos_root, build):
-            return
-        logger.GetLogger().LogOutput(
-            "Image %s not ready, waiting for 10 minutes" % build
-        )
-        time.sleep(SLEEP_TIME)
-        elapsed_time += SLEEP_TIME
-
-    logger.GetLogger().LogOutput(
-        "Image %s not found, waited for %d hours" % (build, (TIME_OUT / 3600))
-    )
-    raise BuildbotTimeout("Timeout while waiting for image %s" % build)
-
-
-def GetLatestImage(chromeos_root, path):
-    """Get latest image"""
-
-    fmt = re.compile(r"R([0-9]+)-([0-9]+).([0-9]+).([0-9]+)")
-
-    ce = command_executer.GetCommandExecuter()
-    command = "gsutil ls gs://chromeos-image-archive/%s" % path
-    ret, out, _ = ce.ChrootRunCommandWOutput(
-        chromeos_root, command, print_to_console=False
-    )
-    if ret != 0:
-        raise RuntimeError("Failed to list buckets with command: %s." % command)
-    candidates = [l.split("/")[-2] for l in out.split()]
-    candidates = [fmt.match(c) for c in candidates]
-    candidates = [
-        [int(r) for r in m.group(1, 2, 3, 4)] for m in candidates if m
-    ]
-    candidates.sort(reverse=True)
-    for c in candidates:
-        build = "%s/R%d-%d.%d.%d" % (path, c[0], c[1], c[2], c[3])
-        if DoesImageExist(chromeos_root, build):
-            return build
-
-
-def GetLatestRecipeImage(chromeos_root, path):
-    """Get latest nightly test image from recipe bucket.
-
-    Image location example:
-    $ARCHIVE/lulu-llvm-next-nightly/R84-13037.0.0-31011-8883172717979984032
-    """
-
-    fmt = re.compile(r"R([0-9]+)-([0-9]+).([0-9]+).([0-9]+)-([0-9]+)")
-
-    ce = command_executer.GetCommandExecuter()
-    command = "gsutil ls gs://chromeos-image-archive/%s" % path
-    ret, out, _ = ce.ChrootRunCommandWOutput(
-        chromeos_root, command, print_to_console=False
-    )
-    if ret != 0:
-        raise RuntimeError("Failed to list buckets with command: %s." % command)
-    candidates = [l.split("/")[-2] for l in out.split()]
-    candidates = [(fmt.match(c), c) for c in candidates]
-    candidates = [
-        ([int(r) for r in m[0].group(1, 2, 3, 4, 5)], m[1])
-        for m in candidates
-        if m
-    ]
-    candidates.sort(key=lambda x: x[0], reverse=True)
-    # Try to get ony last two days of images since nightly tests are run once
-    # another day.
-    for c in candidates[:2]:
-        build = "%s/%s" % (path, c[1])
-        if DoesImageExist(chromeos_root, build):
-            return build
diff --git a/cros_utils/buildbot_utils_unittest.py b/cros_utils/buildbot_utils_unittest.py
deleted file mode 100755
index 9e006f20..00000000
--- a/cros_utils/buildbot_utils_unittest.py
+++ /dev/null
@@ -1,236 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2018 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unittest for buildbot_utils.py."""
-
-
-import time
-import unittest
-from unittest.mock import patch
-
-from cros_utils import buildbot_utils
-from cros_utils import command_executer
-
-
-class TrybotTest(unittest.TestCase):
-    """Test for CommandExecuter class."""
-
-    tryjob_out = (
-        '[{"buildbucket_id": "8952721143823688176", "build_config": '
-        '"cave-llvm-toolchain-tryjob", "url": '
-        # pylint: disable=line-too-long
-        '"http://cros-goldeneye/chromeos/healthmonitoring/buildDetails?buildbucketId=8952721143823688176"}]'
-    )
-
-    GSUTILS_LS = "\n".join(
-        [
-            "gs://chromeos-image-archive/{0}/R78-12421.0.0/",
-            "gs://chromeos-image-archive/{0}/R78-12422.0.0/",
-            "gs://chromeos-image-archive/{0}/R78-12423.0.0/",
-        ]
-    )
-
-    GSUTILS_LS_RECIPE = "\n".join(
-        [
-            "gs://chromeos-image-archive/{0}/R83-12995.0.0-30031-8885075268947031/",
-            "gs://chromeos-image-archive/{0}/R83-13003.0.0-30196-8884755532184725/",
-            "gs://chromeos-image-archive/{0}/R83-13003.0.0-30218-8884712858556419/",
-        ]
-    )
-
-    buildresult_out = (
-        '{"8952721143823688176": {"status": "pass", "artifacts_url":'
-        '"gs://chromeos-image-archive/trybot-elm-release-tryjob/R67-10468.0.0-'
-        'b20789"}}'
-    )
-
-    buildbucket_id = "8952721143823688176"
-    counter_1 = 10
-
-    def testGetTrybotImage(self):
-        with patch.object(buildbot_utils, "SubmitTryjob") as mock_submit:
-            with patch.object(buildbot_utils, "PeekTrybotImage") as mock_peek:
-                with patch.object(time, "sleep", return_value=None):
-
-                    def peek(_chromeos_root, _buildbucket_id):
-                        self.counter_1 -= 1
-                        if self.counter_1 >= 0:
-                            return ("running", "")
-                        return (
-                            "pass",
-                            "gs://chromeos-image-archive/trybot-elm-release-tryjob/"
-                            "R67-10468.0.0-b20789",
-                        )
-
-                    mock_peek.side_effect = peek
-                    mock_submit.return_value = self.buildbucket_id
-
-                    # sync
-                    buildbucket_id, image = buildbot_utils.GetTrybotImage(
-                        "/tmp", "falco-release-tryjob", []
-                    )
-                    self.assertEqual(buildbucket_id, self.buildbucket_id)
-                    self.assertEqual(
-                        "trybot-elm-release-tryjob/" "R67-10468.0.0-b20789",
-                        image,
-                    )
-
-                    # async
-                    buildbucket_id, image = buildbot_utils.GetTrybotImage(
-                        "/tmp", "falco-release-tryjob", [], asynchronous=True
-                    )
-                    self.assertEqual(buildbucket_id, self.buildbucket_id)
-                    self.assertEqual(" ", image)
-
-    def testSubmitTryjob(self):
-        with patch.object(
-            command_executer.CommandExecuter, "RunCommandWOutput"
-        ) as mocked_run:
-            mocked_run.return_value = (0, self.tryjob_out, "")
-            buildbucket_id = buildbot_utils.SubmitTryjob(
-                "/", "falco-release-tryjob", [], []
-            )
-            self.assertEqual(buildbucket_id, self.buildbucket_id)
-
-    def testPeekTrybotImage(self):
-        with patch.object(
-            command_executer.CommandExecuter, "RunCommandWOutput"
-        ) as mocked_run:
-            # pass
-            mocked_run.return_value = (0, self.buildresult_out, "")
-            status, image = buildbot_utils.PeekTrybotImage(
-                "/", self.buildbucket_id
-            )
-            self.assertEqual("pass", status)
-            self.assertEqual(
-                "gs://chromeos-image-archive/trybot-elm-release-tryjob/"
-                "R67-10468.0.0-b20789",
-                image,
-            )
-
-            # running
-            mocked_run.return_value = (1, "", "")
-            status, image = buildbot_utils.PeekTrybotImage(
-                "/", self.buildbucket_id
-            )
-            self.assertEqual("running", status)
-            self.assertEqual(None, image)
-
-            # fail
-            buildresult_fail = self.buildresult_out.replace('"pass"', '"fail"')
-            mocked_run.return_value = (0, buildresult_fail, "")
-            status, image = buildbot_utils.PeekTrybotImage(
-                "/", self.buildbucket_id
-            )
-            self.assertEqual("fail", status)
-            self.assertEqual(
-                "gs://chromeos-image-archive/trybot-elm-release-tryjob/"
-                "R67-10468.0.0-b20789",
-                image,
-            )
-
-    def testParseTryjobBuildbucketId(self):
-        buildbucket_id = buildbot_utils.ParseTryjobBuildbucketId(
-            self.tryjob_out
-        )
-        self.assertEqual(buildbucket_id, self.buildbucket_id)
-
-    def testGetLatestImageValid(self):
-        with patch.object(
-            command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-        ) as mocked_run:
-            with patch.object(
-                buildbot_utils, "DoesImageExist"
-            ) as mocked_imageexist:
-                IMAGE_DIR = "lulu-release"
-                mocked_run.return_value = (
-                    0,
-                    self.GSUTILS_LS.format(IMAGE_DIR),
-                    "",
-                )
-                mocked_imageexist.return_value = True
-                image = buildbot_utils.GetLatestImage("", IMAGE_DIR)
-                self.assertEqual(image, f"{IMAGE_DIR}/R78-12423.0.0")
-
-    def testGetLatestImageInvalid(self):
-        with patch.object(
-            command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-        ) as mocked_run:
-            with patch.object(
-                buildbot_utils, "DoesImageExist"
-            ) as mocked_imageexist:
-                IMAGE_DIR = "kefka-release"
-                mocked_run.return_value = (
-                    0,
-                    self.GSUTILS_LS.format(IMAGE_DIR),
-                    "",
-                )
-                mocked_imageexist.return_value = False
-                image = buildbot_utils.GetLatestImage("", IMAGE_DIR)
-                self.assertIsNone(image)
-
-    def testGetLatestRecipeImageValid(self):
-        with patch.object(
-            command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-        ) as mocked_run:
-            with patch.object(
-                buildbot_utils, "DoesImageExist"
-            ) as mocked_imageexist:
-                IMAGE_DIR = "lulu-llvm-next-nightly"
-                mocked_run.return_value = (
-                    0,
-                    self.GSUTILS_LS_RECIPE.format(IMAGE_DIR),
-                    "",
-                )
-                mocked_imageexist.return_value = True
-                image = buildbot_utils.GetLatestRecipeImage("", IMAGE_DIR)
-                self.assertEqual(
-                    image,
-                    f"{IMAGE_DIR}/R83-13003.0.0-30218-8884712858556419",
-                )
-
-    def testGetLatestRecipeImageInvalid(self):
-        with patch.object(
-            command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-        ) as mocked_run:
-            with patch.object(
-                buildbot_utils, "DoesImageExist"
-            ) as mocked_imageexist:
-                IMAGE_DIR = "kefka-llvm-next-nightly"
-                mocked_run.return_value = (
-                    0,
-                    self.GSUTILS_LS_RECIPE.format(IMAGE_DIR),
-                    "",
-                )
-                mocked_imageexist.return_value = False
-                image = buildbot_utils.GetLatestRecipeImage("", IMAGE_DIR)
-                self.assertIsNone(image)
-
-    def testGetLatestRecipeImageTwodays(self):
-        with patch.object(
-            command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-        ) as mocked_run:
-            with patch.object(
-                buildbot_utils, "DoesImageExist"
-            ) as mocked_imageexist:
-                IMAGE_DIR = "lulu-llvm-next-nightly"
-                mocked_run.return_value = (
-                    0,
-                    self.GSUTILS_LS_RECIPE.format(IMAGE_DIR),
-                    "",
-                )
-                mocked_imageexist.side_effect = [False, False, True]
-                image = buildbot_utils.GetLatestRecipeImage("", IMAGE_DIR)
-                self.assertIsNone(image)
-                mocked_imageexist.side_effect = [False, True, True]
-                image = buildbot_utils.GetLatestRecipeImage("", IMAGE_DIR)
-                self.assertEqual(
-                    image,
-                    f"{IMAGE_DIR}/R83-13003.0.0-30196-8884755532184725",
-                )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/cros_utils/command_executer.py b/cros_utils/command_executer.py
deleted file mode 100755
index 573bb2d6..00000000
--- a/cros_utils/command_executer.py
+++ /dev/null
@@ -1,793 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2011 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Utilities to run commands in outside/inside chroot and on the board."""
-
-
-import getpass
-import os
-import re
-import select
-import signal
-import subprocess
-import sys
-import tempfile
-import time
-
-from cros_utils import logger
-
-
-mock_default = False
-
-CHROMEOS_SCRIPTS_DIR = "/mnt/host/source/src/scripts"
-LOG_LEVEL = ("none", "quiet", "average", "verbose")
-
-
-def InitCommandExecuter(mock=False):
-    # pylint: disable=global-statement
-    global mock_default
-    # Whether to default to a mock command executer or not
-    mock_default = mock
-
-
-def GetCommandExecuter(logger_to_set=None, mock=False, log_level="verbose"):
-    # If the default is a mock executer, always return one.
-    if mock_default or mock:
-        return MockCommandExecuter(log_level, logger_to_set)
-    else:
-        return CommandExecuter(log_level, logger_to_set)
-
-
-class CommandExecuter(object):
-    """Provides several methods to execute commands on several environments."""
-
-    def __init__(self, log_level, logger_to_set=None):
-        self.log_level = log_level
-        if log_level == "none":
-            self.logger = None
-        else:
-            if logger_to_set is not None:
-                self.logger = logger_to_set
-            else:
-                self.logger = logger.GetLogger()
-
-    def GetLogLevel(self):
-        return self.log_level
-
-    def SetLogLevel(self, log_level):
-        self.log_level = log_level
-
-    def RunCommandGeneric(
-        self,
-        cmd,
-        return_output=False,
-        machine=None,
-        username=None,
-        command_terminator=None,
-        command_timeout=None,
-        terminated_timeout=10,
-        print_to_console=True,
-        env=None,
-        except_handler=lambda p, e: None,
-    ):
-        """Run a command.
-
-        Returns triplet (returncode, stdout, stderr).
-        """
-
-        cmd = str(cmd)
-
-        if self.log_level == "quiet":
-            print_to_console = False
-
-        if self.log_level == "verbose":
-            self.logger.LogCmd(cmd, machine, username, print_to_console)
-        elif self.logger:
-            self.logger.LogCmdToFileOnly(cmd, machine, username)
-        if command_terminator and command_terminator.IsTerminated():
-            if self.logger:
-                self.logger.LogError(
-                    "Command was terminated!", print_to_console
-                )
-            return (1, "", "")
-
-        if machine is not None:
-            user = ""
-            if username is not None:
-                user = username + "@"
-            cmd = "ssh -t -t %s%s -- '%s'" % (user, machine, cmd)
-
-        # We use setsid so that the child will have a different session id
-        # and we can easily kill the process group. This is also important
-        # because the child will be disassociated from the parent terminal.
-        # In this way the child cannot mess the parent's terminal.
-        p = None
-        try:
-            # pylint: disable=bad-option-value, subprocess-popen-preexec-fn
-            p = subprocess.Popen(
-                cmd,
-                stdout=subprocess.PIPE,
-                stderr=subprocess.PIPE,
-                shell=True,
-                preexec_fn=os.setsid,
-                executable="/bin/bash",
-                env=env,
-            )
-
-            full_stdout = ""
-            full_stderr = ""
-
-            # Pull output from pipes, send it to file/stdout/string
-            out = err = None
-            pipes = [p.stdout, p.stderr]
-
-            my_poll = select.poll()
-            my_poll.register(p.stdout, select.POLLIN)
-            my_poll.register(p.stderr, select.POLLIN)
-
-            terminated_time = None
-            started_time = time.time()
-
-            while pipes:
-                if command_terminator and command_terminator.IsTerminated():
-                    os.killpg(os.getpgid(p.pid), signal.SIGTERM)
-                    if self.logger:
-                        self.logger.LogError(
-                            "Command received termination request. "
-                            "Killed child process group.",
-                            print_to_console,
-                        )
-                    break
-
-                l = my_poll.poll(100)
-                for (fd, _) in l:
-                    if fd == p.stdout.fileno():
-                        out = os.read(p.stdout.fileno(), 16384).decode("utf8")
-                        if return_output:
-                            full_stdout += out
-                        if self.logger:
-                            self.logger.LogCommandOutput(out, print_to_console)
-                        if out == "":
-                            pipes.remove(p.stdout)
-                            my_poll.unregister(p.stdout)
-                    if fd == p.stderr.fileno():
-                        err = os.read(p.stderr.fileno(), 16384).decode("utf8")
-                        if return_output:
-                            full_stderr += err
-                        if self.logger:
-                            self.logger.LogCommandError(err, print_to_console)
-                        if err == "":
-                            pipes.remove(p.stderr)
-                            my_poll.unregister(p.stderr)
-
-                if p.poll() is not None:
-                    if terminated_time is None:
-                        terminated_time = time.time()
-                    elif (
-                        terminated_timeout is not None
-                        and time.time() - terminated_time > terminated_timeout
-                    ):
-                        if self.logger:
-                            self.logger.LogWarning(
-                                "Timeout of %s seconds reached since "
-                                "process termination." % terminated_timeout,
-                                print_to_console,
-                            )
-                        break
-
-                if (
-                    command_timeout is not None
-                    and time.time() - started_time > command_timeout
-                ):
-                    os.killpg(os.getpgid(p.pid), signal.SIGTERM)
-                    if self.logger:
-                        self.logger.LogWarning(
-                            "Timeout of %s seconds reached since process"
-                            "started. Killed child process group."
-                            % command_timeout,
-                            print_to_console,
-                        )
-                    break
-
-                if out == err == "":
-                    break
-
-            p.wait()
-            if return_output:
-                return (p.returncode, full_stdout, full_stderr)
-            return (p.returncode, "", "")
-        except BaseException as err:
-            except_handler(p, err)
-            raise
-
-    def RunCommand(self, *args, **kwargs):
-        """Run a command.
-
-        Takes the same arguments as RunCommandGeneric except for return_output.
-        Returns a single value returncode.
-        """
-        # Make sure that args does not overwrite 'return_output'
-        assert len(args) <= 1
-        assert "return_output" not in kwargs
-        kwargs["return_output"] = False
-        return self.RunCommandGeneric(*args, **kwargs)[0]
-
-    def RunCommandWExceptionCleanup(self, *args, **kwargs):
-        """Run a command and kill process if exception is thrown.
-
-        Takes the same arguments as RunCommandGeneric except for except_handler.
-        Returns same as RunCommandGeneric.
-        """
-
-        def KillProc(proc, _):
-            if proc:
-                os.killpg(os.getpgid(proc.pid), signal.SIGTERM)
-
-        # Make sure that args does not overwrite 'except_handler'
-        assert len(args) <= 8
-        assert "except_handler" not in kwargs
-        kwargs["except_handler"] = KillProc
-        return self.RunCommandGeneric(*args, **kwargs)
-
-    def RunCommandWOutput(self, *args, **kwargs):
-        """Run a command.
-
-        Takes the same arguments as RunCommandGeneric except for return_output.
-        Returns a triplet (returncode, stdout, stderr).
-        """
-        # Make sure that args does not overwrite 'return_output'
-        assert len(args) <= 1
-        assert "return_output" not in kwargs
-        kwargs["return_output"] = True
-        return self.RunCommandGeneric(*args, **kwargs)
-
-    def RemoteAccessInitCommand(self, chromeos_root, machine, port=None):
-        command = ""
-        command += "\nset -- --remote=" + machine
-        if port:
-            command += " --ssh_port=" + port
-        command += "\n. " + chromeos_root + "/src/scripts/common.sh"
-        command += "\n. " + chromeos_root + "/src/scripts/remote_access.sh"
-        command += "\nTMP=$(mktemp -d)"
-        command += '\nFLAGS "$@" || exit 1'
-        command += "\nremote_access_init"
-        return command
-
-    def WriteToTempShFile(self, contents):
-        with tempfile.NamedTemporaryFile(
-            "w",
-            encoding="utf-8",
-            delete=False,
-            prefix=os.uname()[1],
-            suffix=".sh",
-        ) as f:
-            f.write("#!/bin/bash\n")
-            f.write(contents)
-            f.flush()
-        return f.name
-
-    def CrosLearnBoard(self, chromeos_root, machine):
-        command = self.RemoteAccessInitCommand(chromeos_root, machine)
-        command += "\nlearn_board"
-        command += "\necho ${FLAGS_board}"
-        retval, output, _ = self.RunCommandWOutput(command)
-        if self.logger:
-            self.logger.LogFatalIf(retval, "learn_board command failed")
-        elif retval:
-            sys.exit(1)
-        return output.split()[-1]
-
-    def CrosRunCommandGeneric(
-        self,
-        cmd,
-        return_output=False,
-        machine=None,
-        command_terminator=None,
-        chromeos_root=None,
-        command_timeout=None,
-        terminated_timeout=10,
-        print_to_console=True,
-    ):
-        """Run a command on a ChromeOS box.
-
-        Returns triplet (returncode, stdout, stderr).
-        """
-
-        if self.log_level != "verbose":
-            print_to_console = False
-
-        if self.logger:
-            self.logger.LogCmd(cmd, print_to_console=print_to_console)
-            self.logger.LogFatalIf(not machine, "No machine provided!")
-            self.logger.LogFatalIf(
-                not chromeos_root, "chromeos_root not given!"
-            )
-        else:
-            if not chromeos_root or not machine:
-                sys.exit(1)
-        chromeos_root = os.path.expanduser(chromeos_root)
-
-        port = None
-        if ":" in machine:
-            machine, port = machine.split(":")
-        # Write all commands to a file.
-        command_file = self.WriteToTempShFile(cmd)
-        retval = self.CopyFiles(
-            command_file,
-            command_file,
-            dest_machine=machine,
-            dest_port=port,
-            command_terminator=command_terminator,
-            chromeos_root=chromeos_root,
-            dest_cros=True,
-            recursive=False,
-            print_to_console=print_to_console,
-        )
-        if retval:
-            if self.logger:
-                self.logger.LogError(
-                    "Could not run remote command on machine."
-                    " Is the machine up?"
-                )
-            return (retval, "", "")
-
-        command = self.RemoteAccessInitCommand(chromeos_root, machine, port)
-        command += "\nremote_sh bash %s" % command_file
-        command += '\nl_retval=$?; echo "$REMOTE_OUT"; exit $l_retval'
-        retval = self.RunCommandGeneric(
-            command,
-            return_output,
-            command_terminator=command_terminator,
-            command_timeout=command_timeout,
-            terminated_timeout=terminated_timeout,
-            print_to_console=print_to_console,
-        )
-        if return_output:
-            connect_signature = (
-                "Initiating first contact with remote host\n"
-                + "Connection OK\n"
-            )
-            connect_signature_re = re.compile(connect_signature)
-            modded_retval = list(retval)
-            modded_retval[1] = connect_signature_re.sub("", retval[1])
-            return modded_retval
-        return retval
-
-    def CrosRunCommand(self, *args, **kwargs):
-        """Run a command on a ChromeOS box.
-
-        Takes the same arguments as CrosRunCommandGeneric except for return_output.
-        Returns a single value returncode.
-        """
-        # Make sure that args does not overwrite 'return_output'
-        assert len(args) <= 1
-        assert "return_output" not in kwargs
-        kwargs["return_output"] = False
-        return self.CrosRunCommandGeneric(*args, **kwargs)[0]
-
-    def CrosRunCommandWOutput(self, *args, **kwargs):
-        """Run a command on a ChromeOS box.
-
-        Takes the same arguments as CrosRunCommandGeneric except for return_output.
-        Returns a triplet (returncode, stdout, stderr).
-        """
-        # Make sure that args does not overwrite 'return_output'
-        assert len(args) <= 1
-        assert "return_output" not in kwargs
-        kwargs["return_output"] = True
-        return self.CrosRunCommandGeneric(*args, **kwargs)
-
-    def ChrootRunCommandGeneric(
-        self,
-        chromeos_root,
-        command,
-        return_output=False,
-        command_terminator=None,
-        command_timeout=None,
-        terminated_timeout=10,
-        print_to_console=True,
-        cros_sdk_options="",
-        env=None,
-    ):
-        """Runs a command within the chroot.
-
-        Returns triplet (returncode, stdout, stderr).
-        """
-
-        if self.log_level != "verbose":
-            print_to_console = False
-
-        if self.logger:
-            self.logger.LogCmd(command, print_to_console=print_to_console)
-
-        with tempfile.NamedTemporaryFile(
-            "w",
-            encoding="utf-8",
-            delete=False,
-            dir=os.path.join(chromeos_root, "src/scripts"),
-            suffix=".sh",
-            prefix="in_chroot_cmd",
-        ) as f:
-            f.write("#!/bin/bash\n")
-            f.write(command)
-            f.write("\n")
-            f.flush()
-
-        command_file = f.name
-        os.chmod(command_file, 0o777)
-
-        # if return_output is set, run a test command first to make sure that
-        # the chroot already exists. We want the final returned output to skip
-        # the output from chroot creation steps.
-        if return_output:
-            ret = self.RunCommand(
-                "cd %s; cros_sdk %s -- true"
-                % (chromeos_root, cros_sdk_options),
-                env=env,
-                # Give this command a long time to execute; it might involve setting
-                # the chroot up, or running fstrim on its image file. Both of these
-                # operations can take well over the timeout default of 10 seconds.
-                terminated_timeout=5 * 60,
-            )
-            if ret:
-                return (ret, "", "")
-
-        # Run command_file inside the chroot, making sure that any "~" is expanded
-        # by the shell inside the chroot, not outside.
-        command = "cd %s; cros_sdk %s -- bash -c '%s/%s'" % (
-            chromeos_root,
-            cros_sdk_options,
-            CHROMEOS_SCRIPTS_DIR,
-            os.path.basename(command_file),
-        )
-        ret = self.RunCommandGeneric(
-            command,
-            return_output,
-            command_terminator=command_terminator,
-            command_timeout=command_timeout,
-            terminated_timeout=terminated_timeout,
-            print_to_console=print_to_console,
-            env=env,
-        )
-        os.remove(command_file)
-        return ret
-
-    def ChrootRunCommand(self, *args, **kwargs):
-        """Runs a command within the chroot.
-
-        Takes the same arguments as ChrootRunCommandGeneric except for
-        return_output.
-        Returns a single value returncode.
-        """
-        # Make sure that args does not overwrite 'return_output'
-        assert len(args) <= 2
-        assert "return_output" not in kwargs
-        kwargs["return_output"] = False
-        return self.ChrootRunCommandGeneric(*args, **kwargs)[0]
-
-    def ChrootRunCommandWOutput(self, *args, **kwargs):
-        """Runs a command within the chroot.
-
-        Takes the same arguments as ChrootRunCommandGeneric except for
-        return_output.
-        Returns a triplet (returncode, stdout, stderr).
-        """
-        # Make sure that args does not overwrite 'return_output'
-        assert len(args) <= 2
-        assert "return_output" not in kwargs
-        kwargs["return_output"] = True
-        return self.ChrootRunCommandGeneric(*args, **kwargs)
-
-    def RunCommands(
-        self, cmdlist, machine=None, username=None, command_terminator=None
-    ):
-        cmd = " ;\n".join(cmdlist)
-        return self.RunCommand(
-            cmd,
-            machine=machine,
-            username=username,
-            command_terminator=command_terminator,
-        )
-
-    def CopyFiles(
-        self,
-        src,
-        dest,
-        src_machine=None,
-        src_port=None,
-        dest_machine=None,
-        dest_port=None,
-        src_user=None,
-        dest_user=None,
-        recursive=True,
-        command_terminator=None,
-        chromeos_root=None,
-        src_cros=False,
-        dest_cros=False,
-        print_to_console=True,
-    ):
-        src = os.path.expanduser(src)
-        dest = os.path.expanduser(dest)
-
-        if recursive:
-            src = src + "/"
-            dest = dest + "/"
-
-        if src_cros or dest_cros:
-            if self.logger:
-                self.logger.LogFatalIf(
-                    src_cros == dest_cros,
-                    "Only one of src_cros and desc_cros can " "be True.",
-                )
-                self.logger.LogFatalIf(
-                    not chromeos_root, "chromeos_root not given!"
-                )
-            elif src_cros == dest_cros or not chromeos_root:
-                sys.exit(1)
-            if src_cros:
-                cros_machine = src_machine
-                cros_port = src_port
-                host_machine = dest_machine
-                host_user = dest_user
-            else:
-                cros_machine = dest_machine
-                cros_port = dest_port
-                host_machine = src_machine
-                host_user = src_user
-
-            command = self.RemoteAccessInitCommand(
-                chromeos_root, cros_machine, cros_port
-            )
-            ssh_command = (
-                "ssh -o StrictHostKeyChecking=no"
-                + " -o UserKnownHostsFile=$(mktemp)"
-                + " -i $TMP_PRIVATE_KEY"
-            )
-            if cros_port:
-                ssh_command += " -p %s" % cros_port
-            rsync_prefix = '\nrsync -r -e "%s" ' % ssh_command
-            if dest_cros:
-                command += rsync_prefix + "%s root@%s:%s" % (
-                    src,
-                    cros_machine,
-                    dest,
-                )
-            else:
-                command += rsync_prefix + "root@%s:%s %s" % (
-                    cros_machine,
-                    src,
-                    dest,
-                )
-
-            return self.RunCommand(
-                command,
-                machine=host_machine,
-                username=host_user,
-                command_terminator=command_terminator,
-                print_to_console=print_to_console,
-            )
-
-        if dest_machine == src_machine:
-            command = "rsync -a %s %s" % (src, dest)
-        else:
-            if src_machine is None:
-                src_machine = os.uname()[1]
-                src_user = getpass.getuser()
-            command = "rsync -a %s@%s:%s %s" % (
-                src_user,
-                src_machine,
-                src,
-                dest,
-            )
-        return self.RunCommand(
-            command,
-            machine=dest_machine,
-            username=dest_user,
-            command_terminator=command_terminator,
-            print_to_console=print_to_console,
-        )
-
-    def RunCommand2(
-        self,
-        cmd,
-        cwd=None,
-        line_consumer=None,
-        timeout=None,
-        shell=True,
-        join_stderr=True,
-        env=None,
-        except_handler=lambda p, e: None,
-    ):
-        """Run the command with an extra feature line_consumer.
-
-        This version allow developers to provide a line_consumer which will be
-        fed execution output lines.
-
-        A line_consumer is a callback, which is given a chance to run for each
-        line the execution outputs (either to stdout or stderr). The
-        line_consumer must accept one and exactly one dict argument, the dict
-        argument has these items -
-          'line'   -  The line output by the binary. Notice, this string includes
-                      the trailing '\n'.
-          'output' -  Whether this is a stdout or stderr output, values are either
-                      'stdout' or 'stderr'. When join_stderr is True, this value
-                      will always be 'output'.
-          'pobject' - The object used to control execution, for example, call
-                      pobject.kill().
-
-        Note: As this is written, the stdin for the process executed is
-        not associated with the stdin of the caller of this routine.
-
-        Args:
-          cmd: Command in a single string.
-          cwd: Working directory for execution.
-          line_consumer: A function that will ba called by this function. See above
-            for details.
-          timeout: terminate command after this timeout.
-          shell: Whether to use a shell for execution.
-          join_stderr: Whether join stderr to stdout stream.
-          env: Execution environment.
-          except_handler: Callback for when exception is thrown during command
-            execution. Passed process object and exception.
-
-        Returns:
-          Execution return code.
-
-        Raises:
-          child_exception: if fails to start the command process (missing
-                           permission, no such file, etc)
-        """
-
-        class StreamHandler(object):
-            """Internal utility class."""
-
-            def __init__(self, pobject, fd, name, line_consumer):
-                self._pobject = pobject
-                self._fd = fd
-                self._name = name
-                self._buf = ""
-                self._line_consumer = line_consumer
-
-            def read_and_notify_line(self):
-                t = os.read(fd, 1024)
-                self._buf = self._buf + t
-                self.notify_line()
-
-            def notify_line(self):
-                p = self._buf.find("\n")
-                while p >= 0:
-                    self._line_consumer(
-                        line=self._buf[: p + 1],
-                        output=self._name,
-                        pobject=self._pobject,
-                    )
-                    if p < len(self._buf) - 1:
-                        self._buf = self._buf[p + 1 :]
-                        p = self._buf.find("\n")
-                    else:
-                        self._buf = ""
-                        p = -1
-                        break
-
-            def notify_eos(self):
-                # Notify end of stream. The last line may not end with a '\n'.
-                if self._buf != "":
-                    self._line_consumer(
-                        line=self._buf, output=self._name, pobject=self._pobject
-                    )
-                    self._buf = ""
-
-        if self.log_level == "verbose":
-            self.logger.LogCmd(cmd)
-        elif self.logger:
-            self.logger.LogCmdToFileOnly(cmd)
-
-        # We use setsid so that the child will have a different session id
-        # and we can easily kill the process group. This is also important
-        # because the child will be disassociated from the parent terminal.
-        # In this way the child cannot mess the parent's terminal.
-        pobject = None
-        try:
-            # pylint: disable=bad-option-value, subprocess-popen-preexec-fn
-            pobject = subprocess.Popen(
-                cmd,
-                cwd=cwd,
-                bufsize=1024,
-                env=env,
-                shell=shell,
-                universal_newlines=True,
-                stdout=subprocess.PIPE,
-                stderr=subprocess.STDOUT if join_stderr else subprocess.PIPE,
-                preexec_fn=os.setsid,
-            )
-
-            # We provide a default line_consumer
-            if line_consumer is None:
-                line_consumer = lambda **d: None
-            start_time = time.time()
-            poll = select.poll()
-            outfd = pobject.stdout.fileno()
-            poll.register(outfd, select.POLLIN | select.POLLPRI)
-            handlermap = {
-                outfd: StreamHandler(pobject, outfd, "stdout", line_consumer)
-            }
-            if not join_stderr:
-                errfd = pobject.stderr.fileno()
-                poll.register(errfd, select.POLLIN | select.POLLPRI)
-                handlermap[errfd] = StreamHandler(
-                    pobject, errfd, "stderr", line_consumer
-                )
-            while handlermap:
-                readables = poll.poll(300)
-                for (fd, evt) in readables:
-                    handler = handlermap[fd]
-                    if evt & (select.POLLPRI | select.POLLIN):
-                        handler.read_and_notify_line()
-                    elif evt & (
-                        select.POLLHUP | select.POLLERR | select.POLLNVAL
-                    ):
-                        handler.notify_eos()
-                        poll.unregister(fd)
-                        del handlermap[fd]
-
-                if timeout is not None and (time.time() - start_time > timeout):
-                    os.killpg(os.getpgid(pobject.pid), signal.SIGTERM)
-
-            return pobject.wait()
-        except BaseException as err:
-            except_handler(pobject, err)
-            raise
-
-
-class MockCommandExecuter(CommandExecuter):
-    """Mock class for class CommandExecuter."""
-
-    def RunCommandGeneric(
-        self,
-        cmd,
-        return_output=False,
-        machine=None,
-        username=None,
-        command_terminator=None,
-        command_timeout=None,
-        terminated_timeout=10,
-        print_to_console=True,
-        env=None,
-        except_handler=lambda p, e: None,
-    ):
-        assert not command_timeout
-        cmd = str(cmd)
-        if machine is None:
-            machine = "localhost"
-        if username is None:
-            username = "current"
-        logger.GetLogger().LogCmd(
-            "(Mock) " + cmd, machine, username, print_to_console
-        )
-        return (0, "", "")
-
-    def RunCommand(self, *args, **kwargs):
-        assert "return_output" not in kwargs
-        kwargs["return_output"] = False
-        return self.RunCommandGeneric(*args, **kwargs)[0]
-
-    def RunCommandWOutput(self, *args, **kwargs):
-        assert "return_output" not in kwargs
-        kwargs["return_output"] = True
-        return self.RunCommandGeneric(*args, **kwargs)
-
-
-class CommandTerminator(object):
-    """Object to request termination of a command in execution."""
-
-    def __init__(self):
-        self.terminated = False
-
-    def Terminate(self):
-        self.terminated = True
-
-    def IsTerminated(self):
-        return self.terminated
diff --git a/cros_utils/command_executer_timeout_test.py b/cros_utils/command_executer_timeout_test.py
deleted file mode 100755
index 3af9bd3e..00000000
--- a/cros_utils/command_executer_timeout_test.py
+++ /dev/null
@@ -1,36 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Timeout test for command_executer."""
-
-
-__author__ = "asharif@google.com (Ahmad Sharif)"
-
-import argparse
-import sys
-
-from cros_utils import command_executer
-
-
-def Usage(parser, message):
-    print("ERROR: %s" % message)
-    parser.print_help()
-    sys.exit(0)
-
-
-def Main(argv):
-    parser = argparse.ArgumentParser()
-    _ = parser.parse_args(argv)
-
-    command = "sleep 1000"
-    ce = command_executer.GetCommandExecuter()
-    ce.RunCommand(command, command_timeout=1)
-    return 0
-
-
-if __name__ == "__main__":
-    Main(sys.argv[1:])
diff --git a/cros_utils/command_executer_unittest.py b/cros_utils/command_executer_unittest.py
deleted file mode 100755
index 7cd46a71..00000000
--- a/cros_utils/command_executer_unittest.py
+++ /dev/null
@@ -1,33 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unittest for command_executer.py."""
-
-
-import time
-import unittest
-
-from cros_utils import command_executer
-
-
-class CommandExecuterTest(unittest.TestCase):
-    """Test for CommandExecuter class."""
-
-    def testTimeout(self):
-        timeout = 1
-        logging_level = "average"
-        ce = command_executer.CommandExecuter(logging_level)
-        start = time.time()
-        command = "sleep 20"
-        ce.RunCommand(
-            command, command_timeout=timeout, terminated_timeout=timeout
-        )
-        end = time.time()
-        self.assertTrue(round(end - start) == timeout)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/cros_utils/constants.py b/cros_utils/constants.py
deleted file mode 100644
index 47c16686..00000000
--- a/cros_utils/constants.py
+++ /dev/null
@@ -1,14 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Generic constants used accross modules.
-"""
-
-__author__ = "shenhan@google.com (Han Shen)"
-
-MOUNTED_TOOLCHAIN_ROOT = "/usr/local/toolchain_root"
-
-# Root directory for night testing run.
-CROSTC_WORKSPACE = "/usr/local/google/crostc"
diff --git a/cros_utils/cros_image_tools.py b/cros_utils/cros_image_tools.py
new file mode 100644
index 00000000..ccc3c1d6
--- /dev/null
+++ b/cros_utils/cros_image_tools.py
@@ -0,0 +1,140 @@
+# Copyright 2025 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Tools to work with ChromeOS images."""
+
+import contextlib
+import json
+import logging
+from pathlib import Path
+import subprocess
+from typing import Generator
+
+
+def _find_losetup(chromeos_root: Path) -> Path:
+    """Returns the path to the cros_losetup script."""
+    return chromeos_root / "chromite" / "scripts" / "cros_losetup"
+
+
+@contextlib.contextmanager
+def _mount_image_loopback(
+    chromeos_root: Path, image_path: Path
+) -> Generator[Path, None, None]:
+    """Mounts loopback for an image, yielding the /dev/loop${N} result.
+
+    Cleans up the loopback on exit.
+    """
+    cros_losetup = _find_losetup(chromeos_root)
+    losetup_stdout = subprocess.run(
+        (cros_losetup, "attach", image_path),
+        check=True,
+        # Don't set stdin to DEVNULL, since sudo may prompt for a password.
+        stdout=subprocess.PIPE,
+        encoding="utf-8",
+    ).stdout
+
+    # The stdout of losetup is simply a JSON object containing
+    # {"path": "/dev/loop${N}"}
+    lo_object: Path = json.loads(losetup_stdout)["path"]
+    try:
+        yield lo_object
+    finally:
+        returncode = subprocess.run(
+            (cros_losetup, "detach", lo_object),
+            check=False,
+            # Don't set stdin to DEVNULL, since sudo may prompt for a password.
+        ).returncode
+        # There's not much to do here, especially if we're already handling
+        # an exception.
+        if returncode:
+            logging.error("Detaching %s unexpectedly failed", lo_object)
+
+
+def _find_root_partition(loop_device: Path) -> Path:
+    """Given a path to a loopback device, returns the partition of the rootfs.
+
+    >>> _find_root_partition(Path("/dev/loop1"))
+    Path("/dev/loop1p3")
+    """
+    fdisk_stdout = subprocess.run(
+        ("sudo", "fdisk", "-x", loop_device),
+        check=True,
+        # Don't set stdin to DEVNULL, since sudo may prompt for a password.
+        stdout=subprocess.PIPE,
+        encoding="utf-8",
+    ).stdout
+
+    # The output format here, broadly, is:
+    # ```
+    # A bunch of metadata that
+    # this function does not
+    # care about
+    #
+    # Device Start End Sectors Type-UUID UUID Name Attrs
+    # row1
+    # row2
+    # row3
+    # ...
+    # ```
+    #
+    # The goal is to extract the row named ROOT-A.
+    loop_device_str = str(loop_device)
+    for line in fdisk_stdout.splitlines():
+        if not line.startswith(loop_device_str):
+            continue
+
+        columns = line.split()
+        if len(columns) <= 6:
+            continue
+
+        device_name = columns[6]
+        if device_name == "ROOT-A":
+            device_partition = Path(columns[0])
+            return device_partition
+    raise ValueError(f"Could not find rootfs in {fdisk_stdout!r}")
+
+
+@contextlib.contextmanager
+def mount_image(
+    chromeos_root: Path, image_path: Path, mount_dir: Path
+) -> Generator[None, None, None]:
+    """Mounts a ChromeOS image rootfs, like chromiumos_base_image.bin.
+
+    On exit of the context manager, the image is cleaned up.
+
+    Args:
+        chromeos_root: Root of a ChromeOS checkout.
+        image_path: The path to the image to mount.
+        mount_dir: The directory to mount the image on.
+    """
+    with _mount_image_loopback(chromeos_root, image_path) as lo_device:
+        loop_partition = _find_root_partition(lo_device)
+        logging.info(
+            "Mounting root partition %s at %s", loop_partition, mount_dir
+        )
+        subprocess.run(
+            (
+                "sudo",
+                "mount",
+                "-o",
+                "ro",
+                loop_partition,
+                mount_dir,
+            ),
+            check=True,
+            # Don't set stdin to DEVNULL, since sudo may prompt for a password.
+        )
+
+        try:
+            yield
+        finally:
+            returncode = subprocess.run(
+                ("sudo", "umount", mount_dir),
+                check=False,
+                # Don't set stdin to DEVNULL, since sudo may prompt.
+            ).returncode
+            # There's not much to do here, especially if we're already handling
+            # an exception. Just log it and continue.
+            if returncode:
+                logging.error("Umounting %s unexpectedly failed", mount_dir)
diff --git a/cros_utils/cros_image_tools_test.py b/cros_utils/cros_image_tools_test.py
new file mode 100644
index 00000000..98e7ad05
--- /dev/null
+++ b/cros_utils/cros_image_tools_test.py
@@ -0,0 +1,62 @@
+# Copyright 2025 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Tests for cros_image_tools"""
+
+from pathlib import Path
+import subprocess
+import unittest
+from unittest import mock
+
+from cros_utils import cros_image_tools
+
+
+# These are tests, so protected-access into cros_image_tools is OK.
+# pylint: disable=protected-access
+
+_EXAMPLE_FDISK_OUTPUT = """\
+Disk /dev/loop1: 10.34 GiB, 11102403072 bytes, 21684381 sectors
+Units: sectors of 1 * 512 = 512 bytes
+Sector size (logical/physical): 512 bytes / 512 bytes
+I/O size (minimum/optimal): 512 bytes / 512 bytes
+Disklabel type: gpt
+Disk identifier: 452D7BB8-D902-E746-BF46-2B59A1DCE197
+First usable LBA: 34
+Last usable LBA: 21684347
+Alternative LBA: 21684380
+Partition entries starting LBA: 2
+Allocated partition entries: 128
+Partition entries ending LBA: 33
+
+Device          Start      End  Sectors Type-UUID UUID Name       Attrs
+/dev/loop1p1  4907008 21684332 16777325 UUID UUID2 STATE
+/dev/loop1p2    20480   151551   131072 UUID UUID2 KERN-A     GUID:48,49,50,51,52,53,54,55,56
+/dev/loop1p3   712704  4907007  4194304 UUID UUID2 ROOT-A
+/dev/loop1p4   151552   282623   131072 UUID UUID2 KERN-B
+/dev/loop1p5   708608   712703     4096 UUID UUID2 ROOT-B
+/dev/loop1p6    16448    16448        1 UUID UUID2 KERN-C
+/dev/loop1p7    16449    16449        1 UUID UUID2 ROOT-C
+/dev/loop1p8   282624   315391    32768 UUID UUID2 OEM
+/dev/loop1p9    16450    16450        1 UUID UUID2 reserved
+/dev/loop1p10   16451    16451        1 UUID UUID2 reserved
+/dev/loop1p11      64    16447    16384 UUID UUID2 RWFW
+/dev/loop1p12  446464   708607   262144 UUID UUID2 EFI-SYSTEM LegacyBIOSBootable
+
+Partition table entries are not in disk order.
+"""
+
+
+class Test(unittest.TestCase):
+    """Tests for cros_image_tools."""
+
+    @mock.patch.object(subprocess, "run")
+    def test_find_root_partition_identifies_the_right_partition(self, run_mock):
+        run_mock_return = mock.Mock()
+        run_mock_return.stdout = _EXAMPLE_FDISK_OUTPUT
+        run_mock.return_value = run_mock_return
+
+        self.assertEqual(
+            cros_image_tools._find_root_partition(Path("/dev/loop1")),
+            Path("/dev/loop1p3"),
+        )
diff --git a/cros_utils/cros_paths.py b/cros_utils/cros_paths.py
new file mode 100644
index 00000000..c59c7bdb
--- /dev/null
+++ b/cros_utils/cros_paths.py
@@ -0,0 +1,69 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""A series of utilities for working with paths in ChromeOS."""
+
+import functools
+from pathlib import Path
+import sys
+from typing import Optional
+
+
+# Paths to often-used directories from a CrOS root.
+THIRD_PARTY_DIR = Path("src") / "third_party"
+
+CHROMIUMOS_OVERLAY = THIRD_PARTY_DIR / "chromiumos-overlay"
+LLVM_PROJECT = THIRD_PARTY_DIR / "llvm-project"
+TOOLCHAINS_OVERLAY = THIRD_PARTY_DIR / "toolchains-overlay"
+TOOLCHAIN_UTILS = THIRD_PARTY_DIR / "toolchain-utils"
+TOOLCHAIN_UTILS_PYBIN = TOOLCHAIN_UTILS / "py" / "bin"
+
+CHROOT_SOURCE_ROOT = Path("/mnt") / "host" / "source"
+
+DEFAULT_LLVM_PKG_PATH = CHROMIUMOS_OVERLAY / "sys-devel" / "llvm"
+DEFAULT_PATCHES_PATH_IN_TOOLCHAIN_UTILS = Path("llvm_patches") / "PATCHES.json"
+DEFAULT_PATCHES_PATH = TOOLCHAIN_UTILS / DEFAULT_PATCHES_PATH_IN_TOOLCHAIN_UTILS
+
+
+# Don't bind absolute paths to variables; functions are easier to mock.
+#
+# Functions that perform filesystem ops have results cached, since doing so is
+# very cheap & the results should never change in production.
+
+
+@functools.lru_cache(1)
+def _script_path() -> Path:
+    return Path(__file__).resolve()
+
+
+def script_toolchain_utils_root() -> Path:
+    """Returns the absolute path to the root of toolchain-utils/."""
+    return _script_path().parent.parent
+
+
+@functools.lru_cache(1)
+def script_chromiumos_checkout() -> Optional[Path]:
+    """Returns the absolute path to the CrOS checkout this script resides in.
+
+    Returns None if this toolchain-utils checkout isn't part of a CrOS repo.
+    """
+    # toolchain-utils resides in src/third_party/toolchain-utils.
+    result = script_toolchain_utils_root().parent.parent.parent
+    if (result / ".repo").is_dir():
+        return result
+    return None
+
+
+def script_chromiumos_checkout_or_exit() -> Path:
+    """Returns the absolute path to the CrOS checkout this script resides in.
+
+    Returns None if this toolchain-utils checkout isn't part of a CrOS repo.
+    """
+    result = script_chromiumos_checkout()
+    if not result:
+        sys.exit(
+            "This script must be invoked from a toolchain-utils checkout "
+            "residing in a ChromiumOS checkout."
+        )
+    return result
diff --git a/cros_utils/email_sender.py b/cros_utils/email_sender.py
old mode 100755
new mode 100644
index b47c3beb..e39efa42
--- a/cros_utils/email_sender.py
+++ b/cros_utils/email_sender.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
diff --git a/cros_utils/email_sender_unittest.py b/cros_utils/email_sender_test.py
old mode 100755
new mode 100644
similarity index 97%
rename from cros_utils/email_sender_unittest.py
rename to cros_utils/email_sender_test.py
index 66ec6a2d..493b0fa2
--- a/cros_utils/email_sender_unittest.py
+++ b/cros_utils/email_sender_test.py
@@ -14,7 +14,7 @@ import json
 import unittest
 import unittest.mock as mock
 
-import cros_utils.email_sender as email_sender
+from cros_utils import email_sender
 
 
 class Test(unittest.TestCase):
@@ -116,7 +116,3 @@ class Test(unittest.TestCase):
                 "html_body": "html",
             },
         )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/cros_utils/file_utils.py b/cros_utils/file_utils.py
deleted file mode 100644
index 743edefa..00000000
--- a/cros_utils/file_utils.py
+++ /dev/null
@@ -1,96 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Utilities for operations on files."""
-
-
-import errno
-import os
-import shutil
-
-from cros_utils import command_executer
-
-
-class FileUtils(object):
-    """Utilities for operations on files."""
-
-    _instance = None
-    DRY_RUN = False
-
-    @classmethod
-    def Configure(cls, dry_run):
-        cls.DRY_RUN = dry_run
-
-    def __new__(cls, *args, **kwargs):
-        if not cls._instance:
-            if cls.DRY_RUN:
-                cls._instance = super(FileUtils, cls).__new__(
-                    MockFileUtils, *args, **kwargs
-                )
-            else:
-                cls._instance = super(FileUtils, cls).__new__(
-                    cls, *args, **kwargs
-                )
-        return cls._instance
-
-    def Md5File(self, filename, log_level="verbose", _block_size=2 ** 10):
-        command = "md5sum %s" % filename
-        ce = command_executer.GetCommandExecuter(log_level=log_level)
-        ret, out, _ = ce.RunCommandWOutput(command)
-        if ret:
-            raise RuntimeError("Could not run md5sum on: %s" % filename)
-
-        return out.strip().split()[0]
-
-    def CanonicalizeChromeOSRoot(self, chromeos_root):
-        chromeos_root = os.path.expanduser(chromeos_root)
-        if os.path.isdir(os.path.join(chromeos_root, "chromite")):
-            return chromeos_root
-        else:
-            return None
-
-    def ChromeOSRootFromImage(self, chromeos_image):
-        chromeos_root = os.path.join(
-            os.path.dirname(chromeos_image), "../../../../.."
-        )
-        return self.CanonicalizeChromeOSRoot(chromeos_root)
-
-    def MkDirP(self, path):
-        try:
-            os.makedirs(path)
-        except OSError as exc:
-            if exc.errno == errno.EEXIST:
-                pass
-            else:
-                raise
-
-    def RmDir(self, path):
-        shutil.rmtree(path, ignore_errors=True)
-
-    def WriteFile(self, path, contents):
-        with open(path, "w", encoding="utf-8") as f:
-            f.write(contents)
-
-
-class MockFileUtils(FileUtils):
-    """Mock class for file utilities."""
-
-    def Md5File(self, filename, log_level="verbose", _block_size=2 ** 10):
-        return "d41d8cd98f00b204e9800998ecf8427e"
-
-    def CanonicalizeChromeOSRoot(self, chromeos_root):
-        return "/tmp/chromeos_root"
-
-    def ChromeOSRootFromImage(self, chromeos_image):
-        return "/tmp/chromeos_root"
-
-    def RmDir(self, path):
-        pass
-
-    def MkDirP(self, path):
-        pass
-
-    def WriteFile(self, path, contents):
-        pass
diff --git a/cros_utils/git_utils.py b/cros_utils/git_utils.py
index d8c11d0a..b79ceae9 100644
--- a/cros_utils/git_utils.py
+++ b/cros_utils/git_utils.py
@@ -5,17 +5,111 @@
 """Shared utilities for working with git."""
 
 import contextlib
+import dataclasses
+import enum
 import logging
 from pathlib import Path
 import re
-import shlex
 import subprocess
 import tempfile
-from typing import Generator, Iterable, List
+from typing import Dict, Generator, Iterable, List, Optional
 
 
-# Email address used to tag the detective as a reviewer.
+# Email address used to tag the detective/mage as a reviewer.
 REVIEWER_DETECTIVE = "c-compiler-chrome@google.com"
+REVIEWER_MAGE = "chromeos-toolchain-mage@google.com"
+
+# Default git naming conventions throughout ChromeOS.
+CROS_EXTERNAL_REMOTE = "cros"
+CROS_INTERNAL_REMOTE = "cros-internal"
+CROS_MAIN_BRANCH = "main"
+
+# Gerrit labels
+GERRIT_LABEL_AUTOSUBMIT = "label-as"
+GERRIT_LABEL_CQ = "label-cq"
+GERRIT_LABEL_VERIFIED = "label-v"
+
+
+class Channel(enum.Enum):
+    """An enum that represents ChromeOS channels."""
+
+    # Ordered from closest-to-ToT to farthest-from-ToT
+    CANARY = "canary"
+    BETA = "beta"
+    STABLE = "stable"
+
+    @classmethod
+    def parse(cls, val: str) -> "Channel":
+        for x in cls:
+            if val == x.value:
+                return x
+        raise ValueError(
+            f"No such channel: {val!r}; try one of {[x.value for x in cls]}"
+        )
+
+
+@dataclasses.dataclass(frozen=True, eq=True, order=True)
+class ChannelBranch:
+    """Represents a ChromeOS branch."""
+
+    # Name of the remote that has the branch.
+    remote: str
+    # The ChromeOS release number associated with the branch (e.g., 127 for
+    # M127).
+    release_number: int
+    # The name of the branch.
+    branch_name: str
+
+
+def autodetect_cros_channels(git_repo: Path) -> Dict[Channel, ChannelBranch]:
+    """Autodetects the current ChromeOS channels from a git repo.
+
+    Returns:
+        A map of channels to their associated git branches. There will be one
+        entry per Channel enum value.
+    """
+    stdout = subprocess.run(
+        [
+            "git",
+            "branch",
+            "-r",
+        ],
+        cwd=git_repo,
+        check=True,
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+        encoding="utf-8",
+    ).stdout.strip()
+
+    # Match "${remote}/release-R${branch_number}-${build}.B"
+    branch_re = re.compile(r"([^/]+)/(release-R(\d+)-\d+\.B)")
+    branches = []
+    for line in stdout.splitlines():
+        line = line.strip()
+        if m := branch_re.fullmatch(line):
+            remote, branch_name, branch_number = m.groups()
+            branches.append(
+                ChannelBranch(remote, int(branch_number), branch_name)
+            )
+
+    branches.sort(key=lambda x: x.release_number)
+    if len(branches) < 2:
+        raise ValueError(
+            f"Expected at least two branches, but only found {len(branches)}"
+        )
+
+    stable = branches[-2]
+    beta = branches[-1]
+    canary = ChannelBranch(
+        remote=beta.remote,
+        release_number=beta.release_number + 1,
+        branch_name="main",
+    )
+    return {
+        Channel.CANARY: canary,
+        Channel.BETA: beta,
+        Channel.STABLE: stable,
+    }
 
 
 def _parse_cls_from_upload_output(upload_output: str) -> List[int]:
@@ -36,33 +130,105 @@ def _parse_cls_from_upload_output(upload_output: str) -> List[int]:
     return [int(x) for x in results]
 
 
-def upload_to_gerrit(
-    git_repo: Path,
+def is_full_git_sha(s: str) -> bool:
+    """Returns if `s` looks like a git SHA."""
+    return len(s) == 40 and all(x.isdigit() or "a" <= x <= "f" for x in s)
+
+
+def create_branch(git_repo: Path, branch_name: str) -> None:
+    """Creates a branch in the given repo.
+
+    Args:
+        git_repo: The path to the repo.
+        branch_name: The name of the branch to create.
+    """
+    subprocess.run(
+        ["repo", "start", branch_name, "--head"],
+        check=True,
+        cwd=git_repo,
+    )
+
+
+def generate_upload_to_gerrit_cmd(
     remote: str,
     branch: str,
     reviewers: Iterable[str] = (),
     cc: Iterable[str] = (),
     ref: str = "HEAD",
-) -> List[int]:
-    """Uploads `ref` to gerrit, optionally adding reviewers/CCs."""
+    topic: Optional[str] = None,
+) -> List[str]:
+    """Create a git push CLI command to upload to Gerrit.
+
+    This is similar to `upload_to_gerrit`, but doesn't actually
+    run the command. The returned command here is the same
+    as what `upload_to_gerrit` would have run.
+
+    Args:
+        remote: The remote to upload to.
+        branch: The branch to upload to.
+        reviewers: Reviewers to add to the CLs.
+        cc: CCs to add to the CLs.
+        ref: The ref (generally a SHA) to upload. Note that any parents of this
+            that Gerrit does not recognize will be uploaded.
+        topic: Gerrit topic to add the change to.
+
+    Returns:
+        A list representing the command line args to push to the gerrit
+        upstream.
+    """
     # https://gerrit-review.googlesource.com/Documentation/user-upload.html#reviewers
     # for more info on the `%` params.
     option_list = [f"r={x}" for x in reviewers]
     option_list += (f"cc={x}" for x in cc)
+    if topic is not None:
+        option_list.append(f"topic={topic}")
     if option_list:
         trailing_options = "%" + ",".join(option_list)
     else:
         trailing_options = ""
 
+    return [
+        "git",
+        "push",
+        remote,
+        f"{ref}:refs/for/{branch}{trailing_options}",
+    ]
+
+
+def upload_to_gerrit(
+    git_repo: Path,
+    remote: str,
+    branch: str,
+    reviewers: Iterable[str] = (),
+    cc: Iterable[str] = (),
+    ref: str = "HEAD",
+    topic: Optional[str] = None,
+) -> List[int]:
+    """Uploads `ref` to gerrit, optionally adding reviewers/CCs.
+
+    Args:
+        git_repo: The git repo to upload.
+        remote: The remote to upload to.
+        branch: The branch to upload to.
+        reviewers: Reviewers to add to the CLs.
+        cc: CCs to add to the CLs.
+        ref: The ref (generally a SHA) to upload. Note that any parents of this
+            that Gerrit does not recognize will be uploaded.
+        topic: Gerrit topic to add the change to.
+
+    Returns:
+        A list of CL numbers uploaded.
+    """
+    cmd = generate_upload_to_gerrit_cmd(
+        remote,
+        branch,
+        reviewers,
+        cc,
+        ref,
+        topic,
+    )
     run_result = subprocess.run(
-        [
-            "git",
-            "push",
-            remote,
-            # https://gerrit-review.googlesource.com/Documentation/user-upload.html#reviewers
-            # for more info on the `%` params.
-            f"{ref}:refs/for/{branch}{trailing_options}",
-        ],
+        cmd,
         cwd=git_repo,
         check=False,
         stdin=subprocess.DEVNULL,
@@ -82,6 +248,38 @@ def upload_to_gerrit(
     return _parse_cls_from_upload_output(run_result.stdout)
 
 
+def set_gerrit_label(cwd: Path, cl_id: int, label_name: str, label_value: str):
+    """Sets the given gerrit label to the given value for `cl_id`.
+
+    Args:
+        cwd: the directory that the `gerrit` tool should be run in. Anywhere in
+            a ChromeOS tree will do. The `gerrit` command fails if it isn't run
+            from within a ChromeOS tree.
+        cl_id: The CL number to apply the label to.
+        label_name: Name of the gerrit label to apply, e.g., "label-as"
+        label_value: Value of the label, e.g., "1"
+
+    Raises:
+        subprocess.CalledProcessError if the label wasn't set.
+    """
+    subprocess.run(
+        ("gerrit", label_name, str(cl_id), label_value),
+        cwd=cwd,
+        check=True,
+        stdin=subprocess.DEVNULL,
+    )
+
+
+def set_autoreview_topic(cwd: Path, cl_id: int) -> None:
+    """Sets the autoreview topic on the given CL.
+
+    The autoreview topic integrates with other infra we have to ping
+    Chrotomation's CL reviews en masse every day. This allows these CLs to land
+    in a timely manner (or more timely than tagging random reviewers, at least).
+    """
+    set_gerrit_label(cwd, cl_id, "topic", "crostc-auto-cl")
+
+
 def try_set_autosubmit_labels(cwd: Path, cl_id: int) -> None:
     """Sets autosubmit on a CL. Logs - not raises - on failure.
 
@@ -94,47 +292,67 @@ def try_set_autosubmit_labels(cwd: Path, cl_id: int) -> None:
             from within a ChromeOS tree.
         cl_id: The CL number to apply labels to.
     """
-    gerrit_cl_id = str(cl_id)
-    gerrit_commands = (
-        ["gerrit", "label-as", gerrit_cl_id, "1"],
-        ["gerrit", "label-cq", gerrit_cl_id, "1"],
-        ["gerrit", "label-v", gerrit_cl_id, "1"],
-    )
-    for cmd in gerrit_commands:
-        # Run the gerrit commands inside of toolchain_utils, since `gerrit`
-        # needs to be run inside of a ChromeOS tree to work. While
-        # `toolchain-utils` can be checked out on its own, that's not how this
-        # script is expeted to be used.
-        return_code = subprocess.run(
-            cmd,
-            cwd=cwd,
-            check=False,
-            stdin=subprocess.DEVNULL,
-        ).returncode
-        if return_code:
+    labels = (
+        (GERRIT_LABEL_AUTOSUBMIT, "1"),
+        (GERRIT_LABEL_CQ, "1"),
+        (GERRIT_LABEL_VERIFIED, "1"),
+    )
+    for label_name, label_value in labels:
+        try:
+            set_gerrit_label(cwd, cl_id, label_name, label_value)
+        except subprocess.CalledProcessError:
             logging.warning(
-                "Failed to run gerrit command %s. Ignoring.",
-                shlex.join(cmd),
+                "Failed setting label %s on CL %d; ignoring", label_name, cl_id
             )
 
 
+def set_autoreview_topic_and_labels(cwd: Path, cl_id: int) -> None:
+    """Combines set_autoreview_topic and try_set_autosubmit_labels.
+
+    These often go hand-in-hand.
+    """
+    set_autoreview_topic(cwd, cl_id)
+    try_set_autosubmit_labels(cwd, cl_id)
+
+
 @contextlib.contextmanager
-def create_worktree(git_directory: Path) -> Generator[Path, None, None]:
-    """Creates a temp worktree of `git_directory`, yielding the result."""
-    with tempfile.TemporaryDirectory(prefix="update_kernel_afdo_") as t:
+def create_worktree(
+    git_directory: Path,
+    in_dir: Optional[Path] = None,
+    commitish: Optional[str] = None,
+) -> Generator[Path, None, None]:
+    """Creates a temp worktree of `git_directory`, yielding the result.
+
+    Args:
+        git_directory: The directory to create a worktree of.
+        in_dir: The directory to make the worktree in. If None, uses the same
+            default as tempfile.TemporaryDirectory.
+        commitish: A commit-like reference to checkout the worktree at.
+            If not, set, uses HEAD.
+
+    Yields:
+        A worktree to work in. This is cleaned up once the contextmanager is
+        exited.
+    """
+    with tempfile.TemporaryDirectory(
+        prefix="git_utils_worktree_", dir=in_dir
+    ) as t:
         tempdir = Path(t)
         logging.info(
             "Establishing worktree of %s in %s", git_directory, tempdir
         )
+        cmd = [
+            "git",
+            "worktree",
+            "add",
+            "--detach",
+            "--force",
+            tempdir,
+        ]
+        if commitish:
+            cmd.append(commitish)
         subprocess.run(
-            [
-                "git",
-                "worktree",
-                "add",
-                "--detach",
-                "--force",
-                tempdir,
-            ],
+            cmd,
             cwd=git_directory,
             check=True,
             stdin=subprocess.DEVNULL,
@@ -157,3 +375,401 @@ def create_worktree(git_directory: Path) -> Generator[Path, None, None]:
                 check=False,
                 stdin=subprocess.DEVNULL,
             )
+
+
+def resolve_ref(git_dir: Path, ref: str) -> str:
+    """Resolves the given ref or SHA shorthand to a full SHA.
+
+    Raises:
+        subprocess.CalledProcessError if resolution fails
+    """
+    return subprocess.run(
+        ["git", "rev-parse", ref],
+        check=True,
+        cwd=git_dir,
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+        encoding="utf-8",
+    ).stdout.strip()
+
+
+def commit_all_changes(git_dir: Path, message: str) -> str:
+    """Commits all changes in `git_dir`, with the given commit message.
+
+    This also commits any untracked files in `git_dir`.
+
+    Args:
+        git_dir: Anywhere in the git directory in which changes should be
+            committed.
+        message: Message of the commit message.
+
+    Returns:
+        The SHA of the committed change.
+    """
+    # Explicitly add using `git add -A`, since that stages all unstaged changes
+    # & adds any files that aren't tracked. `git commit -a` skips adding
+    # untracked files.
+    subprocess.run(
+        ["git", "add", "-A"],
+        check=True,
+        cwd=git_dir,
+        stdin=subprocess.DEVNULL,
+    )
+    subprocess.run(
+        ["git", "commit", "-m", message],
+        check=True,
+        cwd=git_dir,
+        stdin=subprocess.DEVNULL,
+    )
+    return resolve_ref(git_dir, "HEAD")
+
+
+def fetch(
+    git_dir: Path, remote: Optional[str] = None, branch: Optional[str] = None
+) -> None:
+    """Runs `git fetch`.
+
+    Args:
+        git_dir: Directory to execute in.
+        remote: If specified, only the given remote will be fetched.
+        branch: If specified, only the given branch will be fetched. If branch
+            is specified, remote must be, also.
+    """
+    if branch and not remote:
+        raise ValueError("If `branch` is specified, `remote` must also be.")
+
+    cmd = ["git", "fetch"]
+    if remote:
+        cmd.append(remote)
+        if branch:
+            cmd.append(branch)
+    subprocess.run(
+        cmd,
+        check=True,
+        cwd=git_dir,
+        stdin=subprocess.DEVNULL,
+    )
+
+
+def checkout(git_dir: Path, ref: str) -> None:
+    """Runs `git checkout ${ref}."""
+    subprocess.run(
+        ("git", "checkout", ref),
+        check=True,
+        cwd=git_dir,
+        stdin=subprocess.DEVNULL,
+    )
+
+
+def fetch_and_checkout(git_dir: Path, remote: str, branch: str) -> None:
+    """Fetches contents of `git_dir`, and checks out `remote/branch`."""
+    logging.info(
+        "Fetching %s and checking out to %s/%s...", git_dir, remote, branch
+    )
+    fetch(git_dir, remote, branch)
+    checkout(git_dir, ref=f"{remote}/{branch}")
+
+
+def has_discardable_changes(git_dir: Path) -> bool:
+    """Returns whether discard_changes_and_checkout will discard changes."""
+    stdout = subprocess.run(
+        ["git", "status", "--porcelain"],
+        check=True,
+        cwd=git_dir,
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+    ).stdout
+    return bool(stdout.strip())
+
+
+def discard_changes_and_checkout(git_dir: Path, ref: str):
+    """Discards local changes, and checks `ref` out."""
+    subprocess.run(
+        ["git", "clean", "-fd"],
+        check=True,
+        cwd=git_dir,
+        stdin=subprocess.DEVNULL,
+    )
+    # `git reset --hard HEAD` independently of the checkout, since we may be on
+    # a branch. The goal isn't to update the potential branch to point to
+    # `ref`.
+    subprocess.run(
+        ["git", "reset", "--hard", "HEAD"],
+        check=True,
+        cwd=git_dir,
+        stdin=subprocess.DEVNULL,
+    )
+    checkout(git_dir, ref)
+
+
+def maybe_show_file_at_commit(
+    git_dir: Path, ref: str, path_from_git_root: str
+) -> Optional[str]:
+    """Returns the given file's contents at `ref`.
+
+    Args:
+        git_dir: Directory to execute in.
+        ref: SHA or ref to get the file's contents from
+        path_from_git_root: The path from the git dir's root to get contents
+            for.
+
+    Returns:
+        File contents, or None if the file does not exist at the given ref.
+    """
+    result = subprocess.run(
+        ["git", "show", f"{ref}:{path_from_git_root}"],
+        check=False,
+        cwd=git_dir,
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+        stderr=subprocess.PIPE,
+        encoding="utf-8",
+    )
+    if not result.returncode:
+        return result.stdout
+
+    # If this file does not exist, git will exit with code 128 and we'll get a
+    # stderr message like either:
+    # - `fatal: path 'foo' does not exist in 'bar'`, or
+    # - `fatal: path 'foo' exists on disk, but not in 'bar'`
+    is_dne = result.returncode == 128 and (
+        "' does not exist in '" in result.stderr
+        or "' exists on disk, but not in '" in result.stderr
+    )
+    if not is_dne:
+        # Put `check_returncode` in a branch before the return, since mypy
+        # can't determine that it always `raise`s.
+        result.check_returncode()
+    return None
+
+
+def maybe_list_dir_contents_at_commit(
+    git_dir: Path, ref: str, path_from_git_root: str
+) -> Optional[List[str]]:
+    """Returns files contained in the given directory at the given commit.
+
+    Args:
+        git_dir: Directory to execute in.
+        ref: SHA or ref to get the directory's contents from
+        path_from_git_root: The path from the git dir's root to get directory
+            contents for.
+
+    Returns:
+        None if the directory did not exist; otherwise, a nonempty list of
+        files/directories contained, relative to the directory they're
+        contained in. Directory names end with a trailing `/`.
+
+    Raises:
+        ValueError if the given path exists, but isn't a directory.
+    """
+    raw_contents = maybe_show_file_at_commit(git_dir, ref, path_from_git_root)
+    if not raw_contents:
+        return None
+
+    not_a_dir = lambda: ValueError(
+        f"{path_from_git_root} at {ref} in {git_dir} isn't a directory"
+    )
+    # If this is a directory, stdout will always start with `tree
+    # ${description}\n\n` before listing entries, one line per entry.
+    raw_contents_lines = raw_contents.splitlines()
+    if len(raw_contents_lines) < 3:
+        raise not_a_dir()
+
+    header_line, empty_line, *results = raw_contents_lines
+    if not header_line.startswith("tree "):
+        raise not_a_dir()
+
+    if empty_line.lstrip():
+        raise not_a_dir()
+
+    return results
+
+
+def commits_between(git_dir: Path, from_ref: str, to_ref: str) -> Iterable[str]:
+    """Return a list of git SHAs between `from_ref` and `to_ref`.
+
+    Args:
+        git_dir: git root directory to get the commits of.
+        from_ref: Starting git ref, exclusive.
+        to_ref: Ending git ref, inclusive.
+
+    Returns:
+        Iterator of git SHAs between the two refs, oldest to newest.
+    """
+    return reversed(
+        subprocess.run(
+            ["git", "log", "--format=%H", f"{from_ref}..{to_ref}"],
+            check=True,
+            cwd=git_dir,
+            stdout=subprocess.PIPE,
+            encoding="utf-8",
+        )
+        .stdout.strip()
+        .splitlines()
+    )
+
+
+def format_patch(git_dir: Path, ref: str) -> str:
+    """Format a patch for a single git ref.
+
+    Args:
+        git_dir: Root directory for a given local git repository.
+        ref: Git ref to make a patch for.
+
+    Returns:
+        The patch file contents.
+    """
+    logging.debug("Formatting patch for %s^..%s", ref, ref)
+    proc = subprocess.run(
+        ["git", "format-patch", "--stdout", f"{ref}^..{ref}"],
+        cwd=git_dir,
+        encoding="utf-8",
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+        check=True,
+    )
+    contents = proc.stdout.strip()
+    if not contents:
+        raise ValueError(f"No git diff between {ref}^..{ref}")
+    logging.debug("Patch diff is %d lines long", contents.count("\n"))
+    return contents
+
+
+def get_message_subject(git_dir: Path, ref: str) -> str:
+    """Return the commit message's subject line."""
+    return subprocess.run(
+        ["git", "show", "--format=%s", "-s", ref],
+        cwd=git_dir,
+        encoding="utf-8",
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+        check=True,
+    ).stdout.strip()
+
+
+def get_commit_message_metadata(git_dir: Path, ref: str) -> Dict[str, str]:
+    """Return footer information for a given commit."""
+    commit_msg = (
+        subprocess.run(
+            ["git", "show", "--format=%b", "-s", ref],
+            cwd=git_dir,
+            encoding="utf-8",
+            stdin=subprocess.DEVNULL,
+            stdout=subprocess.PIPE,
+            check=True,
+        )
+        .stdout.strip()
+        .splitlines()
+    )
+    return parse_message_metadata(commit_msg)
+
+
+def parse_message_metadata(message_lines: Iterable[str]) -> Dict[str, str]:
+    """Return a dictionary of commit message lines' directives."""
+    regex = re.compile(r"([-\w.]+):(.+)")
+    result = {}
+    for line in message_lines:
+        # Must not lstrip the line, as leading whitespace here is important.
+        line = line.rstrip()
+        if match := regex.match(line):
+            key, value = match.groups()
+            result[key] = value.strip()
+    return result
+
+
+def merge_base(git_dir: Path, refs: List[str]) -> Optional[str]:
+    """Return the git merge-base --octopus between branches.
+
+    Args:
+        git_dir: Root directory for a given local git repository.
+        refs: List of commit refs to find the merge base of.
+
+    Returns:
+        An Optional string which is the git SHA of the merge base.
+        If no merge-base exists or there was an error, return None.
+    """
+    proc = subprocess.run(
+        ["git", "merge-base", "--octopus"] + refs,
+        check=False,
+        cwd=git_dir,
+        encoding="utf-8",
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+    )
+    if not proc.returncode:
+        return proc.stdout.strip()
+    return None
+
+
+def branch_list(git_dir: Path, glob: Optional[str] = None) -> List[str]:
+    """List branches, optionally matching a given glob."""
+    addendum = [glob] if glob else []
+    return (
+        subprocess.run(
+            ["git", "branch", "--format=%(refname)", "-a", "-l"] + addendum,
+            check=True,
+            cwd=git_dir,
+            encoding="utf-8",
+            stdin=subprocess.DEVNULL,
+            stdout=subprocess.PIPE,
+        )
+        .stdout.strip()
+        .splitlines()
+    )
+
+
+def commit_author_email(git_dir: Path, ref: str) -> str:
+    """Return the author email of a given git ref."""
+    return subprocess.run(
+        ["git", "show", "--format=%aE", ref],
+        cwd=git_dir,
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+        encoding="utf-8",
+        check=True,
+    ).stdout.strip()
+
+
+def log(
+    git_dir: Path,
+    head: str,
+    stop_at: Optional[str] = None,
+    log_format: Optional[str] = None,
+) -> str:
+    """Runs `git log` between `head` and `stop_at`.
+
+    Args:
+        git_dir: Directory to run `git log` in.
+        head: Commit to start at. This is always included in the log.
+        stop_at: Optional commit to stop at. This is _not_ included in the log.
+        log_format: String to pass to `git log`'s `--format` flag.
+
+    Returns:
+        The output out `git log`.
+    """
+    cmd = ["git", "log"]
+    if log_format:
+        cmd.append(f"--format={log_format}")
+
+    cmd.append(f"{stop_at}..{head}" if stop_at else head)
+    return subprocess.run(
+        cmd,
+        check=True,
+        cwd=git_dir,
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+        encoding="utf-8",
+    ).stdout
+
+
+def query_gerrit(chromeos_root: Path, query: str) -> List[int]:
+    """Returns CLs that match the given `query`."""
+    results = subprocess.run(
+        ("gerrit", "--raw", "search", query),
+        check=True,
+        cwd=chromeos_root,
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+        encoding="utf-8",
+    ).stdout
+    return [int(x) for x in results.split()]
diff --git a/cros_utils/git_utils_test.py b/cros_utils/git_utils_test.py
old mode 100755
new mode 100644
index f6060a7b..7d804868
--- a/cros_utils/git_utils_test.py
+++ b/cros_utils/git_utils_test.py
@@ -1,16 +1,21 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
+
 """Tests for git_utils."""
 
-import unittest
+import subprocess
+import textwrap
+from unittest import mock
 
 from cros_utils import git_utils
+from llvm_tools import test_helpers
 
 
 # pylint: disable=protected-access
 
+EXAMPLE_GIT_SHA = "d46d9c1a23118e3943f43fe2dfc9f9c9c0b4aefe"
+
 GERRIT_OUTPUT_WITH_ONE_CL = r"""
 Enumerating objects: 4, done.
 Counting objects: 100% (4/4), done.
@@ -76,9 +81,25 @@ To https://chrome-internal-review.googlesource.com/chromeos/manifest-internal
 """
 
 
-class Test(unittest.TestCase):
+class Test(test_helpers.TempDirTestCase):
     """Tests for git_utils."""
 
+    def test_is_full_git_sha_success_cases(self):
+        shas = ("a" * 40, EXAMPLE_GIT_SHA)
+        for s in shas:
+            self.assertTrue(git_utils.is_full_git_sha(s), s)
+
+    def test_is_full_git_sha_failure_cases(self):
+        shas = (
+            "",
+            "A" * 40,
+            "g" * 40,
+            EXAMPLE_GIT_SHA[1:],
+            EXAMPLE_GIT_SHA + "a",
+        )
+        for s in shas:
+            self.assertFalse(git_utils.is_full_git_sha(s), s)
+
     def test_cl_parsing_complains_if_no_output(self):
         with self.assertRaisesRegex(ValueError, ".*; found 0"):
             git_utils._parse_cls_from_upload_output("")
@@ -103,6 +124,202 @@ class Test(unittest.TestCase):
             [7190037],
         )
 
+    def test_parse_message_metadata(self):
+        """Test we can parse commit metadata."""
+
+        message_lines = [
+            "Some subject line here",
+            "",
+            "Here is my commit message!",
+            "",
+            "BUG=None",
+            "TEST=None",
+            "",
+            "patch.cherry: true",
+            "patch.version_range.from: 1245",
+            "patch.version_range.until: null",
+            "Commit-Id: abcdef1234567890",
+        ]
+        parsed = git_utils.parse_message_metadata(message_lines)
+        self.assertEqual(parsed["patch.cherry"], "true")
+        self.assertEqual(parsed["patch.version_range.from"], "1245")
+        self.assertEqual(parsed["patch.version_range.until"], "null")
+        self.assertEqual(parsed.get("BUG"), None)
+
+    def test_channel_parsing(self):
+        with self.assertRaisesRegex(ValueError, "No such channel.*"):
+            git_utils.Channel.parse("not a channel")
+
+        # Ensure these round-trip.
+        for channel in git_utils.Channel:
+            self.assertEqual(channel, git_utils.Channel.parse(channel.value))
+
+    @mock.patch.object(subprocess, "run")
+    def test_branch_autodetection(self, subprocess_run):
+        subprocess_run.return_value = subprocess.CompletedProcess(
+            args=[],
+            returncode=0,
+            stdout=textwrap.dedent(
+                """
+                cros/not-a-release-branch
+                cros/release-R121-15699.B
+                cros/release-R122-15753.B
+                cros/release-R123-15786.B
+                cros/also-not-a-release-branch
+                m/main
+                """
+            ),
+        )
+
+        branch_dict = git_utils.autodetect_cros_channels(
+            git_repo=self.make_tempdir()
+        )
 
-if __name__ == "__main__":
-    unittest.main()
+        self.assertEqual(
+            branch_dict,
+            {
+                git_utils.Channel.CANARY: git_utils.ChannelBranch(
+                    remote="cros",
+                    release_number=124,
+                    branch_name="main",
+                ),
+                git_utils.Channel.BETA: git_utils.ChannelBranch(
+                    remote="cros",
+                    release_number=123,
+                    branch_name="release-R123-15786.B",
+                ),
+                git_utils.Channel.STABLE: git_utils.ChannelBranch(
+                    remote="cros",
+                    release_number=122,
+                    branch_name="release-R122-15753.B",
+                ),
+            },
+        )
+
+
+class ShowFileAtRevTest(test_helpers.TempDirTestCase):
+    """Class for testing the show-file-at-rev functionality.
+
+    This is tested against git since it has heuristics matching against git
+    output in error cases.
+    """
+
+    def test_show_file_at_rev_works(self):
+        temp_dir = self.make_tempdir()
+        subprocess.run(
+            ["git", "init"],
+            check=True,
+            cwd=temp_dir,
+            stdin=subprocess.DEVNULL,
+        )
+        (temp_dir / "foo").write_text("old text")
+        git_utils.commit_all_changes(temp_dir, message="commit 1")
+        (temp_dir / "foo").write_text("new text")
+        git_utils.commit_all_changes(temp_dir, message="commit 2")
+
+        # Test multiple cases here to avoid setting up multiple git dirs on
+        # every invocation of this test. They're reasonably self-contained
+        # anyway.
+        self.assertEqual(
+            git_utils.maybe_show_file_at_commit(temp_dir, "HEAD", "foo"),
+            "new text",
+        )
+        self.assertEqual(
+            git_utils.maybe_show_file_at_commit(temp_dir, "HEAD~", "foo"),
+            "old text",
+        )
+
+        self.assertIsNone(
+            git_utils.maybe_show_file_at_commit(temp_dir, "HEAD", "bar")
+        )
+
+    def test_show_dir_at_rev_works(self):
+        temp_dir = self.make_tempdir()
+        subprocess.run(
+            ["git", "init"],
+            check=True,
+            cwd=temp_dir,
+            stdin=subprocess.DEVNULL,
+        )
+        (temp_dir / "file").write_text("foo")
+        git_utils.commit_all_changes(temp_dir, message="commit 1")
+        (temp_dir / "dir").mkdir()
+        (temp_dir / "dir" / "subfile1").touch()
+        git_utils.commit_all_changes(temp_dir, message="commit 2")
+
+        (temp_dir / "dir" / "subfile2").touch()
+        (temp_dir / "dir" / "subdir").mkdir()
+        (temp_dir / "dir" / "subdir" / "subfile3").touch()
+        git_utils.commit_all_changes(temp_dir, message="commit 3")
+
+        # Test multiple cases here to avoid setting up multiple git dirs on
+        # every invocation of this test. They're reasonably self-contained
+        # anyway.
+        self.assertIsNone(
+            git_utils.maybe_list_dir_contents_at_commit(
+                temp_dir, "HEAD~~", "dir"
+            ),
+        )
+
+        self.assertEqual(
+            git_utils.maybe_list_dir_contents_at_commit(
+                temp_dir, "HEAD~", "dir"
+            ),
+            ["subfile1"],
+        )
+
+        self.assertEqual(
+            sorted(
+                git_utils.maybe_list_dir_contents_at_commit(
+                    temp_dir, "HEAD", "dir"
+                )
+            ),
+            ["subdir/", "subfile1", "subfile2"],
+        )
+
+        with self.assertRaisesRegex(ValueError, ".*isn't a directory$"):
+            git_utils.maybe_list_dir_contents_at_commit(
+                temp_dir, "HEAD", "file"
+            )
+
+
+class FormatPatchTest(test_helpers.TempDirTestCase):
+    """Class for testing format_patch.
+
+    This is separated because it derives from TempDirTestCase,
+    which gives us nice temp directories.
+    """
+
+    def setUp(self):
+        """Set up the tests."""
+
+        # This cleans up automatically. No tearDown needed.
+        self.temp_dir = self.make_tempdir()
+        subprocess.run(
+            ["git", "init"],
+            check=True,
+            cwd=self.temp_dir,
+            stdin=subprocess.DEVNULL,
+        )
+        self.foo_contents = "initial commit text"
+        (self.temp_dir / "foo").write_text(self.foo_contents, encoding="utf-8")
+        subject = "Initial commit"
+        git_utils.commit_all_changes(self.temp_dir, message=subject)
+
+        self.foo_contents = "here is special test text :)"
+        (self.temp_dir / "foo").write_text(self.foo_contents, encoding="utf-8")
+        subject = "Second commit"
+        git_utils.commit_all_changes(self.temp_dir, message=subject)
+
+    def test_format_patch(self):
+        """Test that we can format patches correctly."""
+
+        formatted_patch = git_utils.format_patch(self.temp_dir, "HEAD")
+        self.assertIn(
+            "Subject: [PATCH] Second commit",
+            formatted_patch,
+        )
+        self.assertIn(
+            self.foo_contents,
+            formatted_patch,
+        )
diff --git a/cros_utils/gs.py b/cros_utils/gs.py
new file mode 100644
index 00000000..b8c6d01b
--- /dev/null
+++ b/cros_utils/gs.py
@@ -0,0 +1,114 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Utilities for interacting with gs://."""
+
+import dataclasses
+import datetime
+import logging
+import re
+import shlex
+import subprocess
+from typing import List, Optional
+
+
+# Determine which gsutil to use.
+# 'gsutil.py' is provided by depot_tools, whereas 'gsutil'
+# is provided by either https://cloud.google.com/sdk/docs/install, or
+# the 'google-cloud-cli' package. Since we need depot_tools to even
+# use 'repo', 'gsutil.py' is guaranteed to exist.
+GSUTIL = "gsutil.py"
+
+
+@dataclasses.dataclass(frozen=True)
+class GsEntry:
+    """An entry of `gsutil ls -l` output."""
+
+    # When this was last modified (or created). `None` if the entry is a
+    # directory.
+    last_modified: Optional[datetime.datetime]
+    # The full gs:// path to the artifact.
+    gs_path: str
+
+
+def _datetime_from_gs_time(timestamp_str: str) -> datetime.datetime:
+    """Parses a datetime from gs."""
+    return datetime.datetime.strptime(
+        timestamp_str, "%Y-%m-%dT%H:%M:%SZ"
+    ).replace(tzinfo=datetime.timezone.utc)
+
+
+def _parse_ls_output(stdout: str) -> List[GsEntry]:
+    """Parses output of `gsutil ls`."""
+    stdout_lines = stdout.splitlines()
+    # Ignore the last line, since that's always "TOTAL:"
+    stdout_lines.pop()
+
+    line_re = re.compile(
+        # Entries can take one of two forms:
+        r"(?:"
+        # 1. The entry has a size, mod date, and name
+        r"\d+\s+(\S+T\S+)\s+(gs://.+)"
+        r"|"
+        # 2. The entry has none of those, and is just a gs URL.
+        r"(gs://.+)"
+        r")"
+    )
+    results = []
+
+    for line in stdout_lines:
+        # If the line starts with gs://, it's a header for a directory's
+        # contents. Skip it.
+        if line.startswith("gs://"):
+            continue
+
+        line = line.strip()
+        if not line:
+            continue
+        m = line_re.fullmatch(line)
+        if m is None:
+            raise ValueError(f"Unexpected line from gs: {line!r}")
+        timestamp_str, gs_url, alt_gs_url = m.groups()
+        if timestamp_str:
+            last_modified = _datetime_from_gs_time(timestamp_str)
+            gs_path = gs_url
+        else:
+            last_modified = None
+            gs_path = alt_gs_url
+        results.append(GsEntry(last_modified=last_modified, gs_path=gs_path))
+    return results
+
+
+def ls(gs_url: str) -> List[GsEntry]:
+    """Runs `gsutil ls` on the given `path`.
+
+    Globs are forwarded to gs://
+
+    Returns:
+        A list of GsEntrys matching `path`. If the list is entry, no paths
+        matched the URL.
+    """
+    cmd = [
+        GSUTIL,
+        "ls",
+        "-l",
+        gs_url,
+    ]
+    result = subprocess.run(
+        cmd,
+        check=False,
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+        stderr=subprocess.PIPE,
+        encoding="utf-8",
+    )
+
+    if result.returncode:
+        # If nothing could be found, gsutil will exit after printing this.
+        if "One or more URLs matched no objects." in result.stderr:
+            return []
+        logging.error("%s failed; stderr:\n%s", shlex.join(cmd), result.stderr)
+        result.check_returncode()
+        assert False, "unreachable"
+    return _parse_ls_output(result.stdout)
diff --git a/cros_utils/gs_test.py b/cros_utils/gs_test.py
new file mode 100644
index 00000000..c9383c0c
--- /dev/null
+++ b/cros_utils/gs_test.py
@@ -0,0 +1,156 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Tests for the gs module."""
+
+import datetime
+import subprocess
+import unittest
+from unittest import mock
+
+from cros_utils import gs
+
+
+# Protected access in tests is fine.
+# pylint: disable=protected-access
+
+
+class Test(unittest.TestCase):
+    """Tests for the gs module."""
+
+    def test_gs_time_parsing(self):
+        self.assertEqual(
+            gs._datetime_from_gs_time("2024-03-04T10:38:50Z"),
+            datetime.datetime(
+                year=2024,
+                month=3,
+                day=4,
+                hour=10,
+                minute=38,
+                second=50,
+                tzinfo=datetime.timezone.utc,
+            ),
+        )
+
+    @mock.patch.object(subprocess, "run")
+    def test_ls_handles_no_matches(self, run_mock):
+        run_mock.return_value = subprocess.CompletedProcess(
+            args=[],
+            returncode=1,
+            stderr="\nCommandException: One or more URLs matched no objects.\n",
+        )
+        self.assertEqual(
+            gs.ls("gs://here/path/does/not/exist"),
+            [],
+        )
+
+    @mock.patch.object(subprocess, "run")
+    def test_ls_works_on_single_file(self, run_mock):
+        run_mock.return_value = subprocess.CompletedProcess(
+            args=[],
+            returncode=0,
+            # Don't use textwrap.dedent; linter complains about the line being
+            # too long in that case.
+            stdout="""
+753112  2024-03-04T10:38:50Z gs://here/5.4/R124-15786.10-1709548729.gcov.xz
+TOTAL: 2 objects, 1234 bytes (1.1KiB)
+""",
+        )
+        self.assertEqual(
+            gs.ls("gs://here/5.4/R124-15786.10-1709548729.gcov.xz"),
+            [
+                gs.GsEntry(
+                    last_modified=gs._datetime_from_gs_time(
+                        "2024-03-04T10:38:50Z"
+                    ),
+                    gs_path="gs://here/5.4/R124-15786.10-1709548729.gcov.xz",
+                ),
+            ],
+        )
+
+    @mock.patch.object(subprocess, "run")
+    def test_ls_works_on_dir(self, run_mock):
+        run_mock.return_value = subprocess.CompletedProcess(
+            args=[],
+            returncode=0,
+            stdout="""
+     0  2024-03-04T10:38:49Z gs://here/5.4/
+753112  2024-03-04T10:38:50Z gs://here/5.4/R124-15786.10-1709548729.gcov.xz
+TOTAL: 2 objects, 1234 bytes (1.1KiB)
+""",
+        )
+        self.assertEqual(
+            gs.ls("gs://here/5.4"),
+            [
+                gs.GsEntry(
+                    last_modified=gs._datetime_from_gs_time(
+                        "2024-03-04T10:38:49Z"
+                    ),
+                    gs_path="gs://here/5.4/",
+                ),
+                gs.GsEntry(
+                    last_modified=gs._datetime_from_gs_time(
+                        "2024-03-04T10:38:50Z"
+                    ),
+                    gs_path="gs://here/5.4/R124-15786.10-1709548729.gcov.xz",
+                ),
+            ],
+        )
+
+    @mock.patch.object(subprocess, "run")
+    def test_ls_works_with_subdirs(self, run_mock):
+        run_mock.return_value = subprocess.CompletedProcess(
+            args=[],
+            returncode=0,
+            stdout="""
+     0  2024-03-04T10:38:49Z gs://here/
+                             gs://here/5.4/
+TOTAL: 2 objects, 1234 bytes (1.1KiB)
+""",
+        )
+        self.assertEqual(
+            gs.ls("gs://here/"),
+            [
+                gs.GsEntry(
+                    last_modified=gs._datetime_from_gs_time(
+                        "2024-03-04T10:38:49Z"
+                    ),
+                    gs_path="gs://here/",
+                ),
+                gs.GsEntry(
+                    last_modified=None,
+                    gs_path="gs://here/5.4/",
+                ),
+            ],
+        )
+
+    @mock.patch.object(subprocess, "run")
+    def test_ls_works_with_globs(self, run_mock):
+        run_mock.return_value = subprocess.CompletedProcess(
+            args=[],
+            returncode=0,
+            stdout="""
+gs://here/5.4/:
+     0  2024-03-04T10:38:49Z gs://here/5.4/
+753112  2024-03-04T10:38:50Z gs://here/5.4/R124-15786.10-1709548729.gcov.xz
+TOTAL: 2 objects, 1234 bytes (1.1KiB)
+""",
+        )
+        self.assertEqual(
+            gs.ls("gs://here/*/"),
+            [
+                gs.GsEntry(
+                    last_modified=gs._datetime_from_gs_time(
+                        "2024-03-04T10:38:49Z"
+                    ),
+                    gs_path="gs://here/5.4/",
+                ),
+                gs.GsEntry(
+                    last_modified=gs._datetime_from_gs_time(
+                        "2024-03-04T10:38:50Z"
+                    ),
+                    gs_path="gs://here/5.4/R124-15786.10-1709548729.gcov.xz",
+                ),
+            ],
+        )
diff --git a/cros_utils/html_tools.py b/cros_utils/html_tools.py
deleted file mode 100644
index 202bef05..00000000
--- a/cros_utils/html_tools.py
+++ /dev/null
@@ -1,100 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Utilities for generating html."""
-
-
-def GetPageHeader(page_title):
-    return (
-        """<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
-"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
-<html>
-<head>
-<style type="text/css">
-table
-{
-border-collapse:collapse;
-}
-table, td, th
-{
-border:1px solid black;
-}
-</style>
-<script type="text/javascript">
-function displayRow(id){
-  var row = document.getElementById("group_"+id);
-  if (row.style.display == '')  row.style.display = 'none';
-    else row.style.display = '';
-  }
-</script>
-<title>%s</title>
-</head>
-<body>
-
-"""
-        % page_title
-    )
-
-
-def GetListHeader():
-    return "<ul>"
-
-
-def GetListItem(text):
-    return "<li>%s</li>" % text
-
-
-def GetListFooter():
-    return "</ul>"
-
-
-def GetList(items):
-    return "<ul>%s</ul>" % "".join(["<li>%s</li>" % item for item in items])
-
-
-def GetParagraph(text):
-    return "<p>%s</p>" % text
-
-
-def GetFooter():
-    return "</body>\n</html>"
-
-
-def GetHeader(text, h=1):
-    return "<h%s>%s</h%s>" % (h, text, h)
-
-
-def GetTableHeader(headers):
-    row = "".join(["<th>%s</th>" % header for header in headers])
-    return "<table><tr>%s</tr>" % row
-
-
-def GetTableFooter():
-    return "</table>"
-
-
-def FormatLineBreaks(text):
-    return text.replace("\n", "<br/>")
-
-
-def GetTableCell(text):
-    return "<td>%s</td>" % FormatLineBreaks(str(text))
-
-
-def GetTableRow(columns):
-    return "<tr>%s</tr>" % "\n".join(
-        [GetTableCell(column) for column in columns]
-    )
-
-
-def GetTable(headers, rows):
-    table = [GetTableHeader(headers)]
-    table.extend([GetTableRow(row) for row in rows])
-    table.append(GetTableFooter())
-    return "\n".join(table)
-
-
-def GetLink(link, text):
-    return "<a href='%s'>%s</a>" % (link, text)
diff --git a/cros_utils/locks.py b/cros_utils/locks.py
deleted file mode 100644
index db6f4343..00000000
--- a/cros_utils/locks.py
+++ /dev/null
@@ -1,52 +0,0 @@
-# -*- coding: utf-8 -*-
-#
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Utilities for locking machines."""
-
-
-import time
-
-from cros_utils import logger
-import lock_machine
-
-
-def AcquireLock(machines, chromeos_root, timeout=1200):
-    """Acquire lock for machine(s) with timeout."""
-    start_time = time.time()
-    locked = True
-    sleep_time = min(10, timeout / 10.0)
-    while True:
-        try:
-            lock_machine.LockManager(
-                machines, False, chromeos_root
-            ).UpdateMachines(True)
-            break
-        except Exception as e:
-            if time.time() - start_time > timeout:
-                locked = False
-                logger.GetLogger().LogWarning(
-                    "Could not acquire lock on {0} within {1} seconds: {2}".format(
-                        repr(machines), timeout, str(e)
-                    )
-                )
-                break
-            time.sleep(sleep_time)
-    return locked
-
-
-def ReleaseLock(machines, chromeos_root):
-    """Release locked machine(s)."""
-    unlocked = True
-    try:
-        lock_machine.LockManager(machines, False, chromeos_root).UpdateMachines(
-            False
-        )
-    except Exception as e:
-        unlocked = False
-        logger.GetLogger().LogWarning(
-            "Could not unlock %s. %s" % (repr(machines), str(e))
-        )
-    return unlocked
diff --git a/cros_utils/logger.py b/cros_utils/logger.py
deleted file mode 100644
index 7df3f8ff..00000000
--- a/cros_utils/logger.py
+++ /dev/null
@@ -1,393 +0,0 @@
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Logging helper module."""
-
-
-# System modules
-import os.path
-import sys
-import traceback
-from typing import Union
-
-
-# TODO(yunlian@google.com): Use GetRoot from misc
-def GetRoot(scr_name):
-    """Break up pathname into (dir+name)."""
-    abs_path = os.path.abspath(scr_name)
-    return (os.path.dirname(abs_path), os.path.basename(abs_path))
-
-
-class Logger:
-    """Logging helper class."""
-
-    MAX_LOG_FILES = 10
-
-    def __init__(self, rootdir, basefilename, print_console, subdir="logs"):
-        logdir = os.path.join(rootdir, subdir)
-        basename = os.path.join(logdir, basefilename)
-
-        try:
-            os.makedirs(logdir)
-        except OSError:
-            pass
-            # print("Warning: Logs directory '%s' already exists." % logdir)
-
-        self.print_console = print_console
-
-        self._CreateLogFileHandles(basename)
-
-        self._WriteTo(self.cmdfd, " ".join(sys.argv), True)
-
-    def _AddSuffix(self, basename, suffix):
-        return "%s%s" % (basename, suffix)
-
-    def _FindSuffix(self, basename):
-        timestamps = []
-        found_suffix = None
-        for i in range(self.MAX_LOG_FILES):
-            suffix = str(i)
-            suffixed_basename = self._AddSuffix(basename, suffix)
-            cmd_file = "%s.cmd" % suffixed_basename
-            if not os.path.exists(cmd_file):
-                found_suffix = suffix
-                break
-            timestamps.append(os.stat(cmd_file).st_mtime)
-
-        if found_suffix:
-            return found_suffix
-
-        # Try to pick the oldest file with the suffix and return that one.
-        suffix = str(timestamps.index(min(timestamps)))
-        # print ("Warning: Overwriting log file: %s" %
-        #       self._AddSuffix(basename, suffix))
-        return suffix
-
-    def _CreateLogFileHandle(self, name):
-        fd = None
-        try:
-            fd = open(name, "w", encoding="utf-8")
-        except IOError:
-            print("Warning: could not open %s for writing." % name)
-        return fd
-
-    def _CreateLogFileHandles(self, basename):
-        suffix = self._FindSuffix(basename)
-        suffixed_basename = self._AddSuffix(basename, suffix)
-
-        self.cmdfd = self._CreateLogFileHandle("%s.cmd" % suffixed_basename)
-        self.stdout = self._CreateLogFileHandle("%s.out" % suffixed_basename)
-        self.stderr = self._CreateLogFileHandle("%s.err" % suffixed_basename)
-
-        self._CreateLogFileSymlinks(basename, suffixed_basename)
-
-    # Symlink unsuffixed basename to currently suffixed one.
-    def _CreateLogFileSymlinks(self, basename, suffixed_basename):
-        try:
-            for extension in ["cmd", "out", "err"]:
-                src_file = "%s.%s" % (
-                    os.path.basename(suffixed_basename),
-                    extension,
-                )
-                dest_file = "%s.%s" % (basename, extension)
-                if os.path.exists(dest_file):
-                    os.remove(dest_file)
-                os.symlink(src_file, dest_file)
-        except Exception as ex:
-            print("Exception while creating symlinks: %s" % str(ex))
-
-    def _WriteTo(self, fd, msg, flush):
-        if fd:
-            fd.write(msg)
-            if flush:
-                fd.flush()
-
-    def LogStartDots(self, print_to_console=True):
-        term_fd = self._GetStdout(print_to_console)
-        if term_fd:
-            term_fd.flush()
-            term_fd.write(". ")
-            term_fd.flush()
-
-    def LogAppendDot(self, print_to_console=True):
-        term_fd = self._GetStdout(print_to_console)
-        if term_fd:
-            term_fd.write(". ")
-            term_fd.flush()
-
-    def LogEndDots(self, print_to_console=True):
-        term_fd = self._GetStdout(print_to_console)
-        if term_fd:
-            term_fd.write("\n")
-            term_fd.flush()
-
-    def LogMsg(self, file_fd, term_fd, msg, flush=True):
-        if file_fd:
-            self._WriteTo(file_fd, msg, flush)
-        if self.print_console:
-            self._WriteTo(term_fd, msg, flush)
-
-    def _GetStdout(self, print_to_console):
-        if print_to_console:
-            return sys.stdout
-        return None
-
-    def _GetStderr(self, print_to_console):
-        if print_to_console:
-            return sys.stderr
-        return None
-
-    def LogCmdToFileOnly(self, cmd, machine="", user=None):
-        if not self.cmdfd:
-            return
-
-        host = ("%s@%s" % (user, machine)) if user else machine
-        flush = True
-        cmd_string = "CMD (%s): %s\n" % (host, cmd)
-        self._WriteTo(self.cmdfd, cmd_string, flush)
-
-    def LogCmd(self, cmd, machine="", user=None, print_to_console=True):
-        if user:
-            host = "%s@%s" % (user, machine)
-        else:
-            host = machine
-
-        self.LogMsg(
-            self.cmdfd,
-            self._GetStdout(print_to_console),
-            "CMD (%s): %s\n" % (host, cmd),
-        )
-
-    def LogFatal(self, msg, print_to_console=True):
-        self.LogMsg(
-            self.stderr, self._GetStderr(print_to_console), "FATAL: %s\n" % msg
-        )
-        self.LogMsg(
-            self.stderr,
-            self._GetStderr(print_to_console),
-            "\n".join(traceback.format_stack()),
-        )
-        sys.exit(1)
-
-    def LogError(self, msg, print_to_console=True):
-        self.LogMsg(
-            self.stderr, self._GetStderr(print_to_console), "ERROR: %s\n" % msg
-        )
-
-    def LogWarning(self, msg, print_to_console=True):
-        self.LogMsg(
-            self.stderr,
-            self._GetStderr(print_to_console),
-            "WARNING: %s\n" % msg,
-        )
-
-    def LogOutput(self, msg, print_to_console=True):
-        self.LogMsg(
-            self.stdout, self._GetStdout(print_to_console), "OUTPUT: %s\n" % msg
-        )
-
-    def LogFatalIf(self, condition, msg):
-        if condition:
-            self.LogFatal(msg)
-
-    def LogErrorIf(self, condition, msg):
-        if condition:
-            self.LogError(msg)
-
-    def LogWarningIf(self, condition, msg):
-        if condition:
-            self.LogWarning(msg)
-
-    def LogCommandOutput(self, msg, print_to_console=True):
-        self.LogMsg(
-            self.stdout, self._GetStdout(print_to_console), msg, flush=False
-        )
-
-    def LogCommandError(self, msg, print_to_console=True):
-        self.LogMsg(
-            self.stderr, self._GetStderr(print_to_console), msg, flush=False
-        )
-
-    def Flush(self):
-        self.cmdfd.flush()
-        self.stdout.flush()
-        self.stderr.flush()
-
-
-class MockLogger:
-    """Logging helper class."""
-
-    MAX_LOG_FILES = 10
-
-    def __init__(self, *_args, **_kwargs):
-        self.stdout = sys.stdout
-        self.stderr = sys.stderr
-
-    def _AddSuffix(self, basename, suffix):
-        return "%s%s" % (basename, suffix)
-
-    def _FindSuffix(self, basename):
-        timestamps = []
-        found_suffix = None
-        for i in range(self.MAX_LOG_FILES):
-            suffix = str(i)
-            suffixed_basename = self._AddSuffix(basename, suffix)
-            cmd_file = "%s.cmd" % suffixed_basename
-            if not os.path.exists(cmd_file):
-                found_suffix = suffix
-                break
-            timestamps.append(os.stat(cmd_file).st_mtime)
-
-        if found_suffix:
-            return found_suffix
-
-        # Try to pick the oldest file with the suffix and return that one.
-        suffix = str(timestamps.index(min(timestamps)))
-        # print ("Warning: Overwriting log file: %s" %
-        #       self._AddSuffix(basename, suffix))
-        return suffix
-
-    def _CreateLogFileHandle(self, name):
-        print("MockLogger: creating open file handle for %s (writing)" % name)
-
-    def _CreateLogFileHandles(self, basename):
-        suffix = self._FindSuffix(basename)
-        suffixed_basename = self._AddSuffix(basename, suffix)
-
-        print("MockLogger: opening file %s.cmd" % suffixed_basename)
-        print("MockLogger: opening file %s.out" % suffixed_basename)
-        print("MockLogger: opening file %s.err" % suffixed_basename)
-
-        self._CreateLogFileSymlinks(basename, suffixed_basename)
-
-    # Symlink unsuffixed basename to currently suffixed one.
-    def _CreateLogFileSymlinks(self, basename, suffixed_basename):
-        for extension in ["cmd", "out", "err"]:
-            src_file = "%s.%s" % (
-                os.path.basename(suffixed_basename),
-                extension,
-            )
-            dest_file = "%s.%s" % (basename, extension)
-            print(
-                "MockLogger: Calling os.symlink(%s, %s)" % (src_file, dest_file)
-            )
-
-    def _WriteTo(self, _fd, msg, _flush):
-        print("MockLogger: %s" % msg)
-
-    def LogStartDots(self, _print_to_console=True):
-        print(". ")
-
-    def LogAppendDot(self, _print_to_console=True):
-        print(". ")
-
-    def LogEndDots(self, _print_to_console=True):
-        print("\n")
-
-    def LogMsg(self, _file_fd, _term_fd, msg, **_kwargs):
-        print("MockLogger: %s" % msg)
-
-    def _GetStdout(self, _print_to_console):
-        return None
-
-    def _GetStderr(self, _print_to_console):
-        return None
-
-    def LogCmdToFileOnly(self, *_args, **_kwargs):
-        return
-
-    # def LogCmdToFileOnly(self, cmd, machine='', user=None):
-    #   host = ('%s@%s' % (user, machine)) if user else machine
-    #   cmd_string = 'CMD (%s): %s\n' % (host, cmd)
-    #   print('MockLogger: Writing to file ONLY: %s' % cmd_string)
-
-    def LogCmd(self, cmd, machine="", user=None, print_to_console=True):
-        if user:
-            host = "%s@%s" % (user, machine)
-        else:
-            host = machine
-
-        self.LogMsg(
-            0, self._GetStdout(print_to_console), "CMD (%s): %s\n" % (host, cmd)
-        )
-
-    def LogFatal(self, msg, print_to_console=True):
-        self.LogMsg(0, self._GetStderr(print_to_console), "FATAL: %s\n" % msg)
-        self.LogMsg(
-            0,
-            self._GetStderr(print_to_console),
-            "\n".join(traceback.format_stack()),
-        )
-        print("MockLogger: Calling sysexit(1)")
-
-    def LogError(self, msg, print_to_console=True):
-        self.LogMsg(0, self._GetStderr(print_to_console), "ERROR: %s\n" % msg)
-
-    def LogWarning(self, msg, print_to_console=True):
-        self.LogMsg(0, self._GetStderr(print_to_console), "WARNING: %s\n" % msg)
-
-    def LogOutput(self, msg, print_to_console=True):
-        self.LogMsg(0, self._GetStdout(print_to_console), "OUTPUT: %s\n" % msg)
-
-    def LogFatalIf(self, condition, msg):
-        if condition:
-            self.LogFatal(msg)
-
-    def LogErrorIf(self, condition, msg):
-        if condition:
-            self.LogError(msg)
-
-    def LogWarningIf(self, condition, msg):
-        if condition:
-            self.LogWarning(msg)
-
-    def LogCommandOutput(self, msg, print_to_console=True):
-        self.LogMsg(
-            self.stdout, self._GetStdout(print_to_console), msg, flush=False
-        )
-
-    def LogCommandError(self, msg, print_to_console=True):
-        self.LogMsg(
-            self.stderr, self._GetStderr(print_to_console), msg, flush=False
-        )
-
-    def Flush(self):
-        print("MockLogger: Flushing cmdfd, stdout, stderr")
-
-
-main_logger = None
-
-
-def InitLogger(script_name, log_dir, print_console=True, mock=False):
-    """Initialize a global logger. To be called only once."""
-    # pylint: disable=global-statement
-    global main_logger
-    if main_logger:
-        return main_logger
-    rootdir, basefilename = GetRoot(script_name)
-    if not log_dir:
-        log_dir = rootdir
-    if not mock:
-        main_logger = Logger(log_dir, basefilename, print_console)
-    else:
-        main_logger = MockLogger(log_dir, basefilename, print_console)
-    return main_logger
-
-
-def GetLogger(log_dir="", mock=False) -> Union[Logger, MockLogger]:
-    return InitLogger(sys.argv[0], log_dir, mock=mock)
-
-
-def HandleUncaughtExceptions(fun):
-    """Catches all exceptions that would go outside decorated fun scope."""
-
-    def _Interceptor(*args, **kwargs):
-        try:
-            return fun(*args, **kwargs)
-        except Exception:
-            GetLogger().LogFatal(
-                "Uncaught exception:\n%s" % traceback.format_exc()
-            )
-
-    return _Interceptor
diff --git a/cros_utils/machines.py b/cros_utils/machines.py
deleted file mode 100644
index a5385731..00000000
--- a/cros_utils/machines.py
+++ /dev/null
@@ -1,26 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2015 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Utilities relating to machine-specific functions."""
-
-
-from cros_utils import command_executer
-
-
-def MachineIsPingable(machine, logging_level="average"):
-    """Checks to see if a machine is responding to 'ping'.
-
-    Args:
-      machine: String containing the name or ip address of the machine to check.
-      logging_level: The logging level with which to initialize the
-        command_executer (from command_executor.LOG_LEVEL enum list).
-
-    Returns:
-      Boolean indicating whether machine is responding to ping or not.
-    """
-    ce = command_executer.GetCommandExecuter(log_level=logging_level)
-    cmd = "ping -c 1 -w 3 %s" % machine
-    status = ce.RunCommand(cmd)
-    return status == 0
diff --git a/cros_utils/misc.py b/cros_utils/misc.py
deleted file mode 100644
index 72cfb8b8..00000000
--- a/cros_utils/misc.py
+++ /dev/null
@@ -1,561 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Utilities for toolchain build."""
-
-
-__author__ = "asharif@google.com (Ahmad Sharif)"
-
-from contextlib import contextmanager
-import os
-import re
-import shutil
-import sys
-
-from cros_utils import command_executer
-from cros_utils import logger
-
-
-CHROMEOS_SCRIPTS_DIR = "/mnt/host/source/src/scripts"
-TOOLCHAIN_UTILS_PATH = (
-    "/mnt/host/source/src/third_party/toolchain-utils/"
-    "cros_utils/toolchain_utils.sh"
-)
-
-
-def GetChromeOSVersionFromLSBVersion(lsb_version):
-    """Get Chromeos version from Lsb version."""
-    ce = command_executer.GetCommandExecuter()
-    command = (
-        "git ls-remote "
-        "https://chromium.googlesource.com/chromiumos/manifest.git "
-        "refs/heads/release-R*"
-    )
-    ret, out, _ = ce.RunCommandWOutput(command, print_to_console=False)
-    assert ret == 0, "Command %s failed" % command
-    lower = []
-    for line in out.splitlines():
-        mo = re.search(r"refs/heads/release-R(\d+)-(\d+)\.B", line)
-        if mo:
-            revision = int(mo.group(1))
-            build = int(mo.group(2))
-            lsb_build = int(lsb_version.split(".")[0])
-            if lsb_build > build:
-                lower.append(revision)
-    lower = sorted(lower)
-    if lower:
-        return "R%d-%s" % (lower[-1] + 1, lsb_version)
-    else:
-        return "Unknown"
-
-
-def ApplySubs(string, *substitutions):
-    for pattern, replacement in substitutions:
-        string = re.sub(pattern, replacement, string)
-    return string
-
-
-def UnitToNumber(unit_num, base=1000):
-    """Convert a number with unit to float."""
-    unit_dict = {"kilo": base, "mega": base**2, "giga": base**3}
-    unit_num = unit_num.lower()
-    mo = re.search(r"(\d*)(.+)?", unit_num)
-    number = mo.group(1)
-    unit = mo.group(2)
-    if not unit:
-        return float(number)
-    for k, v in unit_dict.items():
-        if k.startswith(unit):
-            return float(number) * v
-    raise RuntimeError("Unit: %s not found in byte: %s!" % (unit, unit_num))
-
-
-def GetFilenameFromString(string):
-    return ApplySubs(
-        string,
-        (r"/", "__"),
-        (r"\s", "_"),
-        (r'[\\$="?^]', ""),
-    )
-
-
-def GetRoot(scr_name):
-    """Break up pathname into (dir+name)."""
-    abs_path = os.path.abspath(scr_name)
-    return (os.path.dirname(abs_path), os.path.basename(abs_path))
-
-
-def GetChromeOSKeyFile(chromeos_root):
-    return os.path.join(
-        chromeos_root,
-        "chromite",
-        "ssh_keys",
-        "testing_rsa",
-    )
-
-
-def GetInsideChrootPath(chromeos_root, file_path):
-    sys.path.insert(0, chromeos_root)
-
-    from chromite.lib import path_util
-
-    return path_util.ToChrootPath(path=file_path, source_path=chromeos_root)
-
-
-def GetOutsideChrootPath(chromeos_root, file_path):
-    sys.path.insert(0, chromeos_root)
-
-    from chromite.lib import path_util
-
-    return path_util.FromChrootPath(path=file_path, source_path=chromeos_root)
-
-
-def FormatQuotedCommand(command):
-    return ApplySubs(command, ('"', r"\""))
-
-
-def FormatCommands(commands):
-    return ApplySubs(
-        str(commands), ("&&", "&&\n"), (";", ";\n"), (r"\n+\s*", "\n")
-    )
-
-
-def GetImageDir(chromeos_root, board):
-    return GetOutsideChrootPath(
-        chromeos_root,
-        os.path.join(chromeos_root, "src", "build", "images", board),
-    )
-
-
-def LabelLatestImage(chromeos_root, board, label, vanilla_path=None):
-    image_dir = GetImageDir(chromeos_root, board)
-    latest_image_dir = os.path.join(image_dir, "latest")
-    latest_image_dir = os.path.realpath(latest_image_dir)
-    latest_image_dir = os.path.basename(latest_image_dir)
-    retval = 0
-    with WorkingDirectory(image_dir):
-        command = "ln -sf -T %s %s" % (latest_image_dir, label)
-        ce = command_executer.GetCommandExecuter()
-        retval = ce.RunCommand(command)
-        if retval:
-            return retval
-        if vanilla_path:
-            command = "ln -sf -T %s %s" % (vanilla_path, "vanilla")
-            retval2 = ce.RunCommand(command)
-            return retval2
-    return retval
-
-
-def DoesLabelExist(chromeos_root, board, label):
-    image_label = os.path.join(GetImageDir(chromeos_root, board), label)
-    return os.path.exists(image_label)
-
-
-def GetBuildPackagesCommand(board, usepkg=False, debug=False):
-    if usepkg:
-        usepkg_flag = "--usepkg"
-    else:
-        usepkg_flag = "--nousepkg"
-    if debug:
-        withdebug_flag = "--withdebug"
-    else:
-        withdebug_flag = "--nowithdebug"
-    return (
-        "%s/build_packages %s --withdev --withtest --withautotest "
-        "--skip_toolchain_update %s --board=%s "
-        "--accept_licenses=@CHROMEOS"
-        % (CHROMEOS_SCRIPTS_DIR, usepkg_flag, withdebug_flag, board)
-    )
-
-
-def GetBuildImageCommand(board, dev=False):
-    dev_args = ""
-    if dev:
-        dev_args = "--noenable_rootfs_verification --disk_layout=2gb-rootfs"
-    return "%s/build_image --board=%s %s test" % (
-        CHROMEOS_SCRIPTS_DIR,
-        board,
-        dev_args,
-    )
-
-
-def GetSetupBoardCommand(board, usepkg=None, force=None):
-    """Get setup_board command."""
-    options = []
-
-    if usepkg:
-        options.append("--usepkg")
-    else:
-        options.append("--nousepkg")
-
-    if force:
-        options.append("--force")
-
-    options.append("--accept-licenses=@CHROMEOS")
-
-    return "setup_board --board=%s %s" % (board, " ".join(options))
-
-
-def CanonicalizePath(path):
-    path = os.path.expanduser(path)
-    path = os.path.realpath(path)
-    return path
-
-
-def GetCtargetFromBoard(board, chromeos_root):
-    """Get Ctarget from board."""
-    base_board = board.split("_")[0]
-    command = "source %s; get_ctarget_from_board %s" % (
-        TOOLCHAIN_UTILS_PATH,
-        base_board,
-    )
-    ce = command_executer.GetCommandExecuter()
-    ret, out, _ = ce.ChrootRunCommandWOutput(chromeos_root, command)
-    if ret != 0:
-        raise ValueError("Board %s is invalid!" % board)
-    # Remove ANSI escape sequences.
-    out = StripANSIEscapeSequences(out)
-    return out.strip()
-
-
-def GetArchFromBoard(board, chromeos_root):
-    """Get Arch from board."""
-    base_board = board.split("_")[0]
-    command = "source %s; get_board_arch %s" % (
-        TOOLCHAIN_UTILS_PATH,
-        base_board,
-    )
-    ce = command_executer.GetCommandExecuter()
-    ret, out, _ = ce.ChrootRunCommandWOutput(chromeos_root, command)
-    if ret != 0:
-        raise ValueError("Board %s is invalid!" % board)
-    # Remove ANSI escape sequences.
-    out = StripANSIEscapeSequences(out)
-    return out.strip()
-
-
-def GetGccLibsDestForBoard(board, chromeos_root):
-    """Get gcc libs destination from board."""
-    arch = GetArchFromBoard(board, chromeos_root)
-    if arch == "x86":
-        return "/build/%s/usr/lib/gcc/" % board
-    if arch == "amd64":
-        return "/build/%s/usr/lib64/gcc/" % board
-    if arch == "arm":
-        return "/build/%s/usr/lib/gcc/" % board
-    if arch == "arm64":
-        return "/build/%s/usr/lib/gcc/" % board
-    raise ValueError("Arch %s is invalid!" % arch)
-
-
-def StripANSIEscapeSequences(string):
-    string = re.sub(r"\x1b\[[0-9]*[a-zA-Z]", "", string)
-    return string
-
-
-def GetChromeSrcDir():
-    return "var/cache/distfiles/target/chrome-src/src"
-
-
-def GetEnvStringFromDict(env_dict):
-    return " ".join(['%s="%s"' % var for var in env_dict.items()])
-
-
-def MergeEnvStringWithDict(env_string, env_dict, prepend=True):
-    """Merge env string with dict."""
-    if not env_string.strip():
-        return GetEnvStringFromDict(env_dict)
-    override_env_list = []
-    ce = command_executer.GetCommandExecuter()
-    for k, v in env_dict.items():
-        v = v.strip("\"'")
-        if prepend:
-            new_env = '%s="%s $%s"' % (k, v, k)
-        else:
-            new_env = '%s="$%s %s"' % (k, k, v)
-        command = "; ".join([env_string, new_env, "echo $%s" % k])
-        ret, out, _ = ce.RunCommandWOutput(command)
-        override_env_list.append("%s=%r" % (k, out.strip()))
-    ret = env_string + " " + " ".join(override_env_list)
-    return ret.strip()
-
-
-def GetAllImages(chromeos_root, board):
-    ce = command_executer.GetCommandExecuter()
-    command = "find %s/src/build/images/%s -name chromiumos_test_image.bin" % (
-        chromeos_root,
-        board,
-    )
-    ret, out, _ = ce.RunCommandWOutput(command)
-    assert ret == 0, "Could not run command: %s" % command
-    return out.splitlines()
-
-
-def IsFloat(text):
-    if text is None:
-        return False
-    try:
-        float(text)
-        return True
-    except ValueError:
-        return False
-
-
-def RemoveChromeBrowserObjectFiles(chromeos_root, board):
-    """Remove any object files from all the posible locations."""
-    out_dir = GetOutsideChrootPath(
-        chromeos_root,
-        "/var/cache/chromeos-chrome/chrome-src/src/out_%s" % board,
-    )
-    if os.path.exists(out_dir):
-        shutil.rmtree(out_dir)
-        logger.GetLogger().LogCmd("rm -rf %s" % out_dir)
-    out_dir = GetOutsideChrootPath(
-        chromeos_root,
-        "/var/cache/chromeos-chrome/chrome-src-internal/src/out_%s" % board,
-    )
-    if os.path.exists(out_dir):
-        shutil.rmtree(out_dir)
-        logger.GetLogger().LogCmd("rm -rf %s" % out_dir)
-
-
-@contextmanager
-def WorkingDirectory(new_dir):
-    """Get the working directory."""
-    old_dir = os.getcwd()
-    if old_dir != new_dir:
-        msg = "cd %s" % new_dir
-        logger.GetLogger().LogCmd(msg)
-    os.chdir(new_dir)
-    yield new_dir
-    if old_dir != new_dir:
-        msg = "cd %s" % old_dir
-        logger.GetLogger().LogCmd(msg)
-    os.chdir(old_dir)
-
-
-def HasGitStagedChanges(git_dir):
-    """Return True if git repository has staged changes."""
-    command = f"cd {git_dir} && git diff --quiet --cached --exit-code HEAD"
-    return command_executer.GetCommandExecuter().RunCommand(
-        command, print_to_console=False
-    )
-
-
-def HasGitUnstagedChanges(git_dir):
-    """Return True if git repository has un-staged changes."""
-    command = f"cd {git_dir} && git diff --quiet --exit-code HEAD"
-    return command_executer.GetCommandExecuter().RunCommand(
-        command, print_to_console=False
-    )
-
-
-def HasGitUntrackedChanges(git_dir):
-    """Return True if git repository has un-tracked changes."""
-    command = (
-        f"cd {git_dir} && test -z "
-        "$(git ls-files --exclude-standard --others)"
-    )
-    return command_executer.GetCommandExecuter().RunCommand(
-        command, print_to_console=False
-    )
-
-
-def GitGetCommitHash(git_dir, commit_symbolic_name):
-    """Return githash for the symbolic git commit.
-
-    For example, commit_symbolic_name could be
-    "cros/gcc.gnu.org/branches/gcc/gcc-4_8-mobile, this function returns the git
-    hash for this symbolic name.
-
-    Args:
-      git_dir: a git working tree.
-      commit_symbolic_name: a symbolic name for a particular git commit.
-
-    Returns:
-      The git hash for the symbolic name or None if fails.
-    """
-
-    command = (
-        f"cd {git_dir} && git log -n 1"
-        f' --pretty="format:%H" {commit_symbolic_name}'
-    )
-    rv, out, _ = command_executer.GetCommandExecuter().RunCommandWOutput(
-        command, print_to_console=False
-    )
-    if rv == 0:
-        return out.strip()
-    return None
-
-
-def IsGitTreeClean(git_dir):
-    """Test if git tree has no local changes.
-
-    Args:
-      git_dir: git tree directory.
-
-    Returns:
-      True if git dir is clean.
-    """
-    if HasGitStagedChanges(git_dir):
-        logger.GetLogger().LogWarning("Git tree has staged changes.")
-        return False
-    if HasGitUnstagedChanges(git_dir):
-        logger.GetLogger().LogWarning("Git tree has unstaged changes.")
-        return False
-    if HasGitUntrackedChanges(git_dir):
-        logger.GetLogger().LogWarning("Git tree has un-tracked changes.")
-        return False
-    return True
-
-
-def GetGitChangesAsList(git_dir, path=None, staged=False):
-    """Get changed files as a list.
-
-    Args:
-      git_dir: git tree directory.
-      path: a relative path that is part of the tree directory, could be null.
-      staged: whether to include staged files as well.
-
-    Returns:
-      A list containing all the changed files.
-    """
-    command = f"cd {git_dir} && git diff --name-only"
-    if staged:
-        command += " --cached"
-    if path:
-        command += " -- " + path
-    _, out, _ = command_executer.GetCommandExecuter().RunCommandWOutput(
-        command, print_to_console=False
-    )
-    rv = []
-    for line in out.splitlines():
-        rv.append(line)
-    return rv
-
-
-def IsChromeOsTree(chromeos_root):
-    return os.path.isdir(
-        os.path.join(chromeos_root, "src/third_party/chromiumos-overlay")
-    ) and os.path.isdir(os.path.join(chromeos_root, "manifest"))
-
-
-def DeleteChromeOsTree(chromeos_root, dry_run=False):
-    """Delete a ChromeOs tree *safely*.
-
-    Args:
-      chromeos_root: dir of the tree, could be a relative one (but be careful)
-      dry_run: only prints out the command if True
-
-    Returns:
-      True if everything is ok.
-    """
-    if not IsChromeOsTree(chromeos_root):
-        logger.GetLogger().LogWarning(
-            f'"{chromeos_root}" does not seem to be a'
-            " valid chromeos tree, do nothing."
-        )
-        return False
-    cmd0 = f"cd {chromeos_root} && cros_sdk --delete"
-    if dry_run:
-        print(cmd0)
-    else:
-        if (
-            command_executer.GetCommandExecuter().RunCommand(
-                cmd0, print_to_console=True
-            )
-            != 0
-        ):
-            return False
-
-    cmd1 = (
-        f'export CHROMEOSDIRNAME="$(dirname $(cd {chromeos_root} && pwd))" && '
-        f'export CHROMEOSBASENAME="$(basename $(cd {chromeos_root} && pwd))" && '
-        "cd $CHROMEOSDIRNAME && sudo rm -fr $CHROMEOSBASENAME"
-    )
-    if dry_run:
-        print(cmd1)
-        return True
-
-    return (
-        command_executer.GetCommandExecuter().RunCommand(
-            cmd1, print_to_console=True
-        )
-        == 0
-    )
-
-
-def BooleanPrompt(
-    prompt="Do you want to continue?",
-    default=True,
-    true_value="yes",
-    false_value="no",
-    prolog=None,
-):
-    """Helper function for processing boolean choice prompts.
-
-    Args:
-      prompt: The question to present to the user.
-      default: Boolean to return if the user just presses enter.
-      true_value: The text to display that represents a True returned.
-      false_value: The text to display that represents a False returned.
-      prolog: The text to display before prompt.
-
-    Returns:
-      True or False.
-    """
-    true_value, false_value = true_value.lower(), false_value.lower()
-    true_text, false_text = true_value, false_value
-    if true_value == false_value:
-        raise ValueError(
-            "true_value and false_value must differ: got %r" % true_value
-        )
-
-    if default:
-        true_text = true_text[0].upper() + true_text[1:]
-    else:
-        false_text = false_text[0].upper() + false_text[1:]
-
-    prompt = "\n%s (%s/%s)? " % (prompt, true_text, false_text)
-
-    if prolog:
-        prompt = "\n%s\n%s" % (prolog, prompt)
-
-    while True:
-        try:
-            # pylint: disable=input-builtin, bad-builtin
-            response = input(prompt).lower()
-        except EOFError:
-            # If the user hits CTRL+D, or stdin is disabled, use the default.
-            print()
-            response = None
-        except KeyboardInterrupt:
-            # If the user hits CTRL+C, just exit the process.
-            print()
-            print("CTRL+C detected; exiting")
-            sys.exit()
-
-        if not response:
-            return default
-        if true_value.startswith(response):
-            if not false_value.startswith(response):
-                return True
-            # common prefix between the two...
-        elif false_value.startswith(response):
-            return False
-
-
-# pylint: disable=unused-argument
-def rgb2short(r, g, b):
-    """Converts RGB values to xterm-256 color."""
-
-    redcolor = [255, 124, 160, 196, 9]
-    greencolor = [255, 118, 82, 46, 10]
-
-    if g == 0:
-        return redcolor[r // 52]
-    if r == 0:
-        return greencolor[g // 52]
-    return 4
diff --git a/cros_utils/misc_test.py b/cros_utils/misc_test.py
deleted file mode 100755
index 9e2d1107..00000000
--- a/cros_utils/misc_test.py
+++ /dev/null
@@ -1,67 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Tests for misc."""
-
-
-__author__ = "asharif@google.com (Ahmad Sharif)"
-
-# System modules
-import unittest
-
-# Local modules
-from cros_utils import misc
-
-
-class UtilsTest(unittest.TestCase):
-    """Tests for misc."""
-
-    def testGetFilenameFromString(self):
-        string = 'a /b=c"d^$?\\'
-        filename = misc.GetFilenameFromString(string)
-        self.assertEqual(filename, "a___bcd")
-
-    def testPrependMergeEnv(self):
-        var = "USE"
-        use_flags = "hello 123"
-        added_use_flags = "bla bla"
-        env_string = "%s=%r" % (var, use_flags)
-        new_env_string = misc.MergeEnvStringWithDict(
-            env_string, {var: added_use_flags}
-        )
-        expected_new_env = "%s=%r" % (
-            var,
-            " ".join([added_use_flags, use_flags]),
-        )
-        self.assertEqual(
-            new_env_string, " ".join([env_string, expected_new_env])
-        )
-
-    def testGetChromeOSVersionFromLSBVersion(self):
-        versions_dict = {"2630.0.0": "22", "2030.0.0": "19"}
-        f = misc.GetChromeOSVersionFromLSBVersion
-        for k, v in versions_dict.items():
-            self.assertEqual(f(k), "R%s-%s" % (v, k))
-
-    def testPostpendMergeEnv(self):
-        var = "USE"
-        use_flags = "hello 123"
-        added_use_flags = "bla bla"
-        env_string = "%s=%r" % (var, use_flags)
-        new_env_string = misc.MergeEnvStringWithDict(
-            env_string, {var: added_use_flags}, False
-        )
-        expected_new_env = "%s=%r" % (
-            var,
-            " ".join([use_flags, added_use_flags]),
-        )
-        self.assertEqual(
-            new_env_string, " ".join([env_string, expected_new_env])
-        )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/cros_utils/perf_diff.py b/cros_utils/perf_diff.py
deleted file mode 100755
index 6647b76a..00000000
--- a/cros_utils/perf_diff.py
+++ /dev/null
@@ -1,357 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""One-line documentation for perf_diff module.
-
-A detailed description of perf_diff.
-"""
-
-
-__author__ = "asharif@google.com (Ahmad Sharif)"
-
-import argparse
-import functools
-import re
-import sys
-
-from cros_utils import misc
-from cros_utils import tabulator
-
-
-ROWS_TO_SHOW = "Rows_to_show_in_the_perf_table"
-TOTAL_EVENTS = "Total_events_of_this_profile"
-
-
-def GetPerfDictFromReport(report_file):
-    output = {}
-    perf_report = PerfReport(report_file)
-    for k, v in perf_report.sections.items():
-        if k not in output:
-            output[k] = {}
-        output[k][ROWS_TO_SHOW] = 0
-        output[k][TOTAL_EVENTS] = 0
-        for function in v.functions:
-            out_key = "%s" % (function.name)
-            output[k][out_key] = function.count
-            output[k][TOTAL_EVENTS] += function.count
-            if function.percent > 1:
-                output[k][ROWS_TO_SHOW] += 1
-    return output
-
-
-def _SortDictionaryByValue(d):
-    l = d.items()
-
-    def GetFloat(x):
-        if misc.IsFloat(x):
-            return float(x)
-        else:
-            return x
-
-    sorted_l = sorted(l, key=lambda x: GetFloat(x[1]))
-    sorted_l.reverse()
-    return [f[0] for f in sorted_l]
-
-
-class Tabulator(object):
-    """Make tables."""
-
-    def __init__(self, all_dicts):
-        self._all_dicts = all_dicts
-
-    def PrintTable(self):
-        for dicts in self._all_dicts:
-            self.PrintTableHelper(dicts)
-
-    def PrintTableHelper(self, dicts):
-        """Transfrom dicts to tables."""
-        fields = {}
-        for d in dicts:
-            for f in d.keys():
-                if f not in fields:
-                    fields[f] = d[f]
-                else:
-                    fields[f] = max(fields[f], d[f])
-        table = []
-        header = ["name"]
-        for i in range(len(dicts)):
-            header.append(i)
-
-        table.append(header)
-
-        sorted_fields = _SortDictionaryByValue(fields)
-
-        for f in sorted_fields:
-            row = [f]
-            for d in dicts:
-                if f in d:
-                    row.append(d[f])
-                else:
-                    row.append("0")
-            table.append(row)
-
-        print(tabulator.GetSimpleTable(table))
-
-
-class Function(object):
-    """Function for formatting."""
-
-    def __init__(self):
-        self.count = 0
-        self.name = ""
-        self.percent = 0
-
-
-class Section(object):
-    """Section formatting."""
-
-    def __init__(self, contents):
-        self.name = ""
-        self.raw_contents = contents
-        self._ParseSection()
-
-    def _ParseSection(self):
-        matches = re.findall(r"Events: (\w+)\s+(.*)", self.raw_contents)
-        assert len(matches) <= 1, "More than one event found in 1 section"
-        if not matches:
-            return
-        match = matches[0]
-        self.name = match[1]
-        self.count = misc.UnitToNumber(match[0])
-
-        self.functions = []
-        for line in self.raw_contents.splitlines():
-            if not line.strip():
-                continue
-            if "%" not in line:
-                continue
-            if not line.startswith("#"):
-                fields = [f for f in line.split(" ") if f]
-                function = Function()
-                function.percent = float(fields[0].strip("%"))
-                function.count = int(fields[1])
-                function.name = " ".join(fields[2:])
-                self.functions.append(function)
-
-
-class PerfReport(object):
-    """Get report from raw report."""
-
-    def __init__(self, perf_file):
-        self.perf_file = perf_file
-        self._ReadFile()
-        self.sections = {}
-        self.metadata = {}
-        self._section_contents = []
-        self._section_header = ""
-        self._SplitSections()
-        self._ParseSections()
-        self._ParseSectionHeader()
-
-    def _ParseSectionHeader(self):
-        """Parse a header of a perf report file."""
-        # The "captured on" field is inaccurate - this actually refers to when the
-        # report was generated, not when the data was captured.
-        for line in self._section_header.splitlines():
-            line = line[2:]
-            if ":" in line:
-                key, val = line.strip().split(":", 1)
-                key = key.strip()
-                val = val.strip()
-                self.metadata[key] = val
-
-    def _ReadFile(self):
-        self._perf_contents = open(self.perf_file).read()
-
-    def _ParseSections(self):
-        self.event_counts = {}
-        self.sections = {}
-        for section_content in self._section_contents:
-            section = Section(section_content)
-            section.name = self._GetHumanReadableName(section.name)
-            self.sections[section.name] = section
-
-    # TODO(asharif): Do this better.
-    def _GetHumanReadableName(self, section_name):
-        if not "raw" in section_name:
-            return section_name
-        raw_number = section_name.strip().split(" ")[-1]
-        for line in self._section_header.splitlines():
-            if raw_number in line:
-                name = line.strip().split(" ")[5]
-                return name
-
-    def _SplitSections(self):
-        self._section_contents = []
-        indices = [
-            m.start() for m in re.finditer("# Events:", self._perf_contents)
-        ]
-        indices.append(len(self._perf_contents))
-        for i in range(len(indices) - 1):
-            section_content = self._perf_contents[indices[i] : indices[i + 1]]
-            self._section_contents.append(section_content)
-        self._section_header = ""
-        if indices:
-            self._section_header = self._perf_contents[0 : indices[0]]
-
-
-class PerfDiffer(object):
-    """Perf differ class."""
-
-    def __init__(self, reports, num_symbols, common_only):
-        self._reports = reports
-        self._num_symbols = num_symbols
-        self._common_only = common_only
-        self._common_function_names = {}
-
-    def DoDiff(self):
-        """The function that does the diff."""
-        section_names = self._FindAllSections()
-
-        filename_dicts = []
-        summary_dicts = []
-        for report in self._reports:
-            d = {}
-            filename_dicts.append({"file": report.perf_file})
-            for section_name in section_names:
-                if section_name in report.sections:
-                    d[section_name] = report.sections[section_name].count
-            summary_dicts.append(d)
-
-        all_dicts = [filename_dicts, summary_dicts]
-
-        for section_name in section_names:
-            function_names = self._GetTopFunctions(
-                section_name, self._num_symbols
-            )
-            self._FindCommonFunctions(section_name)
-            dicts = []
-            for report in self._reports:
-                d = {}
-                if section_name in report.sections:
-                    section = report.sections[section_name]
-
-                    # Get a common scaling factor for this report.
-                    common_scaling_factor = self._GetCommonScalingFactor(
-                        section
-                    )
-
-                    for function in section.functions:
-                        if function.name in function_names:
-                            key = "%s %s" % (section.name, function.name)
-                            d[key] = function.count
-                            # Compute a factor to scale the function count by in common_only
-                            # mode.
-                            if self._common_only and (
-                                function.name
-                                in self._common_function_names[section.name]
-                            ):
-                                d[key + " scaled"] = (
-                                    common_scaling_factor * function.count
-                                )
-                dicts.append(d)
-
-            all_dicts.append(dicts)
-
-        mytabulator = Tabulator(all_dicts)
-        mytabulator.PrintTable()
-
-    def _FindAllSections(self):
-        sections = {}
-        for report in self._reports:
-            for section in report.sections.values():
-                if section.name not in sections:
-                    sections[section.name] = section.count
-                else:
-                    sections[section.name] = max(
-                        sections[section.name], section.count
-                    )
-        return _SortDictionaryByValue(sections)
-
-    def _GetCommonScalingFactor(self, section):
-        unique_count = self._GetCount(
-            section, lambda x: x in self._common_function_names[section.name]
-        )
-        return 100.0 / unique_count
-
-    def _GetCount(self, section, filter_fun=None):
-        total_count = 0
-        for function in section.functions:
-            if not filter_fun or filter_fun(function.name):
-                total_count += int(function.count)
-        return total_count
-
-    def _FindCommonFunctions(self, section_name):
-        function_names_list = []
-        for report in self._reports:
-            if section_name in report.sections:
-                section = report.sections[section_name]
-                function_names = {f.name for f in section.functions}
-                function_names_list.append(function_names)
-
-        self._common_function_names[section_name] = functools.reduce(
-            set.intersection, function_names_list
-        )
-
-    def _GetTopFunctions(self, section_name, num_functions):
-        all_functions = {}
-        for report in self._reports:
-            if section_name in report.sections:
-                section = report.sections[section_name]
-                for f in section.functions[:num_functions]:
-                    if f.name in all_functions:
-                        all_functions[f.name] = max(
-                            all_functions[f.name], f.count
-                        )
-                    else:
-                        all_functions[f.name] = f.count
-        # FIXME(asharif): Don't really need to sort these...
-        return _SortDictionaryByValue(all_functions)
-
-    def _GetFunctionsDict(self, section, function_names):
-        d = {}
-        for function in section.functions:
-            if function.name in function_names:
-                d[function.name] = function.count
-        return d
-
-
-def Main(argv):
-    """The entry of the main."""
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "-n",
-        "--num_symbols",
-        dest="num_symbols",
-        default="5",
-        help="The number of symbols to show.",
-    )
-    parser.add_argument(
-        "-c",
-        "--common_only",
-        dest="common_only",
-        action="store_true",
-        default=False,
-        help="Diff common symbols only.",
-    )
-
-    options, args = parser.parse_known_args(argv)
-
-    try:
-        reports = []
-        for report in args[1:]:
-            report = PerfReport(report)
-            reports.append(report)
-        pd = PerfDiffer(reports, int(options.num_symbols), options.common_only)
-        pd.DoDiff()
-    finally:
-        pass
-
-    return 0
-
-
-if __name__ == "__main__":
-    sys.exit(Main(sys.argv))
diff --git a/cros_utils/tabulator.py b/cros_utils/tabulator.py
deleted file mode 100644
index 2cfd5d35..00000000
--- a/cros_utils/tabulator.py
+++ /dev/null
@@ -1,1700 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Table generating, analyzing and printing functions.
-
-This defines several classes that are used to generate, analyze and print
-tables.
-
-Example usage:
-
-  from cros_utils import tabulator
-
-  data = [["benchmark1", "33", "44"],["benchmark2", "44", "33"]]
-  tabulator.GetSimpleTable(data)
-
-You could also use it to generate more complex tables with analysis such as
-p-values, custom colors, etc. Tables are generated by TableGenerator and
-analyzed/formatted by TableFormatter. TableFormatter can take in a list of
-columns with custom result computation and coloring, and will compare values in
-each row according to taht scheme. Here is a complex example on printing a
-table:
-
-  from cros_utils import tabulator
-
-  runs = [[{"k1": "10", "k2": "12", "k5": "40", "k6": "40",
-            "ms_1": "20", "k7": "FAIL", "k8": "PASS", "k9": "PASS",
-            "k10": "0"},
-           {"k1": "13", "k2": "14", "k3": "15", "ms_1": "10", "k8": "PASS",
-            "k9": "FAIL", "k10": "0"}],
-          [{"k1": "50", "k2": "51", "k3": "52", "k4": "53", "k5": "35", "k6":
-            "45", "ms_1": "200", "ms_2": "20", "k7": "FAIL", "k8": "PASS", "k9":
-            "PASS"}]]
-  labels = ["vanilla", "modified"]
-  tg = TableGenerator(runs, labels, TableGenerator.SORT_BY_VALUES_DESC)
-  table = tg.GetTable()
-  columns = [Column(LiteralResult(),
-                    Format(),
-                    "Literal"),
-             Column(AmeanResult(),
-                    Format()),
-             Column(StdResult(),
-                    Format()),
-             Column(CoeffVarResult(),
-                    CoeffVarFormat()),
-             Column(NonEmptyCountResult(),
-                    Format()),
-             Column(AmeanRatioResult(),
-                    PercentFormat()),
-             Column(AmeanRatioResult(),
-                    RatioFormat()),
-             Column(GmeanRatioResult(),
-                    RatioFormat()),
-             Column(PValueResult(),
-                    PValueFormat()),
-            ]
-  tf = TableFormatter(table, columns)
-  cell_table = tf.GetCellTable()
-  tp = TablePrinter(cell_table, out_to)
-  print tp.Print()
-"""
-
-
-import collections
-import getpass
-import math
-import statistics
-import sys
-from typing import Tuple, Union
-
-from cros_utils import misc
-from cros_utils.email_sender import EmailSender
-import numpy as np
-
-
-def _ttest_ind(
-    sample: Union[np.ndarray, list], baseline: Union[np.ndarray, list]
-) -> Tuple[float, float]:
-    """Independent, two-sided student's T test.
-
-    Reimplementation of scipy.stats.ttest_ind.
-    """
-    if isinstance(sample, list):
-        sample = np.asarray(sample)
-    if isinstance(baseline, list):
-        baseline = np.asarray(baseline)
-    diff = np.mean(sample) - np.mean(baseline)
-    diff_stderr = np.sqrt(sample.var(ddof=1) + baseline.var(ddof=1))
-    t_value = np.mean(diff) / (diff_stderr / np.sqrt(len(sample)))
-    samples = _sample_student_t(len(sample), 1000)
-    # Assuming two-sided student's t
-    if t_value < 0:
-        # Lower tail
-        return t_value, 2 * np.sum(samples < t_value) / len(samples)
-    # Upper tail
-    return t_value, 2 * np.sum(samples > t_value) / len(samples)
-
-
-def _sample_student_t(
-    dof: float, num_samples: int
-) -> np.ndarray:
-    # In theory this probably should be memoized. However,
-    # that's a lot of data points to store in memory for
-    # the lifetime of the program?
-    sample_generator = np.random.default_rng()
-    return sample_generator.standard_t(dof, num_samples)
-
-
-def _AllFloat(values):
-    return all([misc.IsFloat(v) for v in values])
-
-
-def _GetFloats(values):
-    return [float(v) for v in values]
-
-
-def _StripNone(results):
-    res = []
-    for result in results:
-        if result is not None:
-            res.append(result)
-    return res
-
-
-def _RemoveMinMax(cell, values):
-    if len(values) < 3:
-        print(
-            "WARNING: Values count is less than 3, not ignoring min/max values"
-        )
-        print("WARNING: Cell name:", cell.name, "Values:", values)
-        return values
-
-    values.remove(min(values))
-    values.remove(max(values))
-    return values
-
-
-class TableGenerator(object):
-    """Creates a table from a list of list of dicts.
-
-    The main public function is called GetTable().
-    """
-
-    SORT_BY_KEYS = 0
-    SORT_BY_KEYS_DESC = 1
-    SORT_BY_VALUES = 2
-    SORT_BY_VALUES_DESC = 3
-    NO_SORT = 4
-
-    MISSING_VALUE = "x"
-
-    def __init__(self, d, l, sort=NO_SORT, key_name="keys"):
-        self._runs = d
-        self._labels = l
-        self._sort = sort
-        self._key_name = key_name
-
-    def _AggregateKeys(self):
-        keys = collections.OrderedDict()
-        for run_list in self._runs:
-            for run in run_list:
-                keys.update(dict.fromkeys(run.keys()))
-        return list(keys.keys())
-
-    def _GetHighestValue(self, key):
-        values = []
-        for run_list in self._runs:
-            for run in run_list:
-                if key in run:
-                    values.append(run[key])
-        values = _StripNone(values)
-        if _AllFloat(values):
-            values = _GetFloats(values)
-        values = [
-            float(v)
-            for v in values
-            if isinstance(v, float)
-            or isinstance(v, int)
-            or v.lower() in ("nan", "inf")
-        ]
-        if not values:
-            return float("nan")
-        return max(values)
-
-    def _GetLowestValue(self, key):
-        values = []
-        for run_list in self._runs:
-            for run in run_list:
-                if key in run:
-                    values.append(run[key])
-        values = _StripNone(values)
-        if _AllFloat(values):
-            values = _GetFloats(values)
-        values = [
-            float(v)
-            for v in values
-            if isinstance(v, float)
-            or isinstance(v, int)
-            or v.lower() in ("nan", "inf")
-        ]
-        if not values:
-            return float("nan")
-        return min(values)
-
-    def _SortKeys(self, keys):
-        if self._sort == self.SORT_BY_KEYS:
-            return sorted(keys)
-        elif self._sort == self.SORT_BY_VALUES:
-            # pylint: disable=unnecessary-lambda
-            return sorted(keys, key=lambda x: self._GetLowestValue(x))
-        elif self._sort == self.SORT_BY_VALUES_DESC:
-            # pylint: disable=unnecessary-lambda
-            return sorted(
-                keys, key=lambda x: self._GetHighestValue(x), reverse=True
-            )
-        elif self._sort == self.NO_SORT:
-            return keys
-        else:
-            assert 0, "Unimplemented sort %s" % self._sort
-
-    def _GetKeys(self):
-        keys = self._AggregateKeys()
-        return self._SortKeys(keys)
-
-    def GetTable(self, number_of_rows=sys.maxsize):
-        """Returns a table from a list of list of dicts.
-
-        Examples:
-          We have the following runs:
-            [[{"k1": "v1", "k2": "v2"}, {"k1": "v3"}],
-             [{"k1": "v4", "k4": "v5"}]]
-          and the following labels:
-            ["vanilla", "modified"]
-          it will return:
-            [["Key", "vanilla", "modified"]
-             ["k1", ["v1", "v3"], ["v4"]]
-             ["k2", ["v2"], []]
-             ["k4", [], ["v5"]]]
-          The returned table can then be processed further by other classes in this
-          module.
-
-        The list of list of dicts is passed into the constructor of TableGenerator.
-        This method converts that into a canonical list of lists which represents a
-        table of values.
-
-        Args:
-          number_of_rows: Maximum number of rows to return from the table.
-
-        Returns:
-          A list of lists which is the table.
-        """
-        keys = self._GetKeys()
-        header = [self._key_name] + self._labels
-        table = [header]
-        rows = 0
-        for k in keys:
-            row = [k]
-            unit = None
-            for run_list in self._runs:
-                v = []
-                for run in run_list:
-                    if k in run:
-                        if isinstance(run[k], list):
-                            val = run[k][0]
-                            unit = run[k][1]
-                        else:
-                            val = run[k]
-                        v.append(val)
-                    else:
-                        v.append(None)
-                row.append(v)
-            # If we got a 'unit' value, append the units name to the key name.
-            if unit:
-                keyname = row[0] + " (%s) " % unit
-                row[0] = keyname
-            table.append(row)
-            rows += 1
-            if rows == number_of_rows:
-                break
-        return table
-
-
-class SamplesTableGenerator(TableGenerator):
-    """Creates a table with only samples from the results
-
-    The main public function is called GetTable().
-
-    Different than TableGenerator, self._runs is now a dict of {benchmark: runs}
-    We are expecting there is 'samples' in `runs`.
-    """
-
-    def __init__(self, run_keyvals, label_list, iter_counts, weights):
-        TableGenerator.__init__(
-            self, run_keyvals, label_list, key_name="Benchmarks"
-        )
-        self._iter_counts = iter_counts
-        self._weights = weights
-
-    def _GetKeys(self):
-        keys = self._runs.keys()
-        return self._SortKeys(keys)
-
-    def GetTable(self, number_of_rows=sys.maxsize):
-        """Returns a tuple, which contains three args:
-
-          1) a table from a list of list of dicts.
-          2) updated benchmark_results run_keyvals with composite benchmark
-          3) updated benchmark_results iter_count with composite benchmark
-
-        The dict of list of list of dicts is passed into the constructor of
-        SamplesTableGenerator.
-        This method converts that into a canonical list of lists which
-        represents a table of values.
-
-        Examples:
-          We have the following runs:
-            {bench1: [[{"samples": "v1"}, {"samples": "v2"}],
-                      [{"samples": "v3"}, {"samples": "v4"}]]
-             bench2: [[{"samples": "v21"}, None],
-                      [{"samples": "v22"}, {"samples": "v23"}]]}
-          and weights of benchmarks:
-            {bench1: w1, bench2: w2}
-          and the following labels:
-            ["vanilla", "modified"]
-          it will return:
-            [["Benchmark", "Weights", "vanilla", "modified"]
-             ["bench1", w1,
-                ((2, 0), ["v1*w1", "v2*w1"]), ((2, 0), ["v3*w1", "v4*w1"])]
-             ["bench2", w2,
-                ((1, 1), ["v21*w2", None]), ((2, 0), ["v22*w2", "v23*w2"])]
-             ["Composite Benchmark", N/A,
-                ((1, 1), ["v1*w1+v21*w2", None]),
-                ((2, 0), ["v3*w1+v22*w2", "v4*w1+ v23*w2"])]]
-          The returned table can then be processed further by other classes in this
-          module.
-
-        Args:
-          number_of_rows: Maximum number of rows to return from the table.
-
-        Returns:
-          A list of lists which is the table.
-        """
-        keys = self._GetKeys()
-        header = [self._key_name, "Weights"] + self._labels
-        table = [header]
-        rows = 0
-        iterations = 0
-
-        for k in keys:
-            bench_runs = self._runs[k]
-            unit = None
-            all_runs_empty = all(
-                not dict for label in bench_runs for dict in label
-            )
-            if all_runs_empty:
-                cell = Cell()
-                cell.string_value = (
-                    "Benchmark %s contains no result."
-                    " Is the benchmark name valid?" % k
-                )
-                table.append([cell])
-            else:
-                row = [k]
-                row.append(self._weights[k])
-                for run_list in bench_runs:
-                    run_pass = 0
-                    run_fail = 0
-                    v = []
-                    for run in run_list:
-                        if "samples" in run:
-                            if isinstance(run["samples"], list):
-                                val = run["samples"][0] * self._weights[k]
-                                unit = run["samples"][1]
-                            else:
-                                val = run["samples"] * self._weights[k]
-                            v.append(val)
-                            run_pass += 1
-                        else:
-                            v.append(None)
-                            run_fail += 1
-                    one_tuple = ((run_pass, run_fail), v)
-                    if iterations not in (0, run_pass + run_fail):
-                        raise ValueError(
-                            "Iterations of each benchmark run "
-                            "are not the same"
-                        )
-                    iterations = run_pass + run_fail
-                    row.append(one_tuple)
-                if unit:
-                    keyname = row[0] + " (%s) " % unit
-                    row[0] = keyname
-                table.append(row)
-                rows += 1
-                if rows == number_of_rows:
-                    break
-
-        k = "Composite Benchmark"
-        if k in keys:
-            raise RuntimeError("Composite benchmark already exists in results")
-
-        # Create a new composite benchmark row at the bottom of the summary table
-        # The new row will be like the format in example:
-        # ["Composite Benchmark", N/A,
-        #        ((1, 1), ["v1*w1+v21*w2", None]),
-        #        ((2, 0), ["v3*w1+v22*w2", "v4*w1+ v23*w2"])]]
-        # First we will create a row of [key, weight, [[0] * iterations] * labels]
-        row = [None] * len(header)
-        row[0] = "%s (samples)" % k
-        row[1] = "N/A"
-        for label_index in range(2, len(row)):
-            row[label_index] = [0] * iterations
-
-        for cur_row in table[1:]:
-            # Iterate through each benchmark
-            if len(cur_row) > 1:
-                for label_index in range(2, len(cur_row)):
-                    # Iterate through each run in a single benchmark
-                    # each result should look like ((pass, fail), [values_list])
-                    bench_runs = cur_row[label_index][1]
-                    for index in range(iterations):
-                        # Accumulate each run result to composite benchmark run
-                        # If any run fails, then we set this run for composite benchmark
-                        # to None so that we know it fails.
-                        if (
-                            bench_runs[index]
-                            and row[label_index][index] is not None
-                        ):
-                            row[label_index][index] += bench_runs[index]
-                        else:
-                            row[label_index][index] = None
-            else:
-                # One benchmark totally fails, no valid data will be in final result
-                for label_index in range(2, len(row)):
-                    row[label_index] = [None] * iterations
-                break
-        # Calculate pass and fail count for composite benchmark
-        for label_index in range(2, len(row)):
-            run_pass = 0
-            run_fail = 0
-            for run in row[label_index]:
-                if run:
-                    run_pass += 1
-                else:
-                    run_fail += 1
-            row[label_index] = ((run_pass, run_fail), row[label_index])
-        table.append(row)
-
-        # Now that we have the table genearted, we want to store this new composite
-        # benchmark into the benchmark_result in ResultReport object.
-        # This will be used to generate a full table which contains our composite
-        # benchmark.
-        # We need to create composite benchmark result and add it to keyvals in
-        # benchmark_results.
-        v = []
-        for label in row[2:]:
-            # each label's result looks like ((pass, fail), [values])
-            benchmark_runs = label[1]
-            # List of values of each label
-            single_run_list = []
-            for run in benchmark_runs:
-                # Result of each run under the same label is a dict of keys.
-                # Here the only key we will add for composite benchmark is the
-                # weighted_samples we added up.
-                one_dict = {}
-                if run:
-                    one_dict["weighted_samples"] = [run, "samples"]
-                    one_dict["retval"] = 0
-                else:
-                    one_dict["retval"] = 1
-                single_run_list.append(one_dict)
-            v.append(single_run_list)
-
-        self._runs[k] = v
-        self._iter_counts[k] = iterations
-
-        return (table, self._runs, self._iter_counts)
-
-
-class Result(object):
-    """A class that respresents a single result.
-
-    This single result is obtained by condensing the information from a list of
-    runs and a list of baseline runs.
-    """
-
-    def __init__(self):
-        pass
-
-    def _AllStringsSame(self, values):
-        values_set = set(values)
-        return len(values_set) == 1
-
-    def NeedsBaseline(self):
-        return False
-
-    # pylint: disable=unused-argument
-    def _Literal(self, cell, values, baseline_values):
-        cell.value = " ".join([str(v) for v in values])
-
-    def _ComputeFloat(self, cell, values, baseline_values):
-        self._Literal(cell, values, baseline_values)
-
-    def _ComputeString(self, cell, values, baseline_values):
-        self._Literal(cell, values, baseline_values)
-
-    def _InvertIfLowerIsBetter(self, cell):
-        pass
-
-    def _GetGmean(self, values):
-        if not values:
-            return float("nan")
-        if any([v < 0 for v in values]):
-            return float("nan")
-        if any([v == 0 for v in values]):
-            return 0.0
-        log_list = [math.log(v) for v in values]
-        gmean_log = sum(log_list) / len(log_list)
-        return math.exp(gmean_log)
-
-    def Compute(self, cell, values, baseline_values):
-        """Compute the result given a list of values and baseline values.
-
-        Args:
-          cell: A cell data structure to populate.
-          values: List of values.
-          baseline_values: List of baseline values. Can be none if this is the
-          baseline itself.
-        """
-        all_floats = True
-        values = _StripNone(values)
-        if not values:
-            cell.value = ""
-            return
-        if _AllFloat(values):
-            float_values = _GetFloats(values)
-        else:
-            all_floats = False
-        if baseline_values:
-            baseline_values = _StripNone(baseline_values)
-        if baseline_values:
-            if _AllFloat(baseline_values):
-                float_baseline_values = _GetFloats(baseline_values)
-            else:
-                all_floats = False
-        else:
-            if self.NeedsBaseline():
-                cell.value = ""
-                return
-            float_baseline_values = None
-        if all_floats:
-            self._ComputeFloat(cell, float_values, float_baseline_values)
-            self._InvertIfLowerIsBetter(cell)
-        else:
-            self._ComputeString(cell, values, baseline_values)
-
-
-class LiteralResult(Result):
-    """A literal result."""
-
-    def __init__(self, iteration=0):
-        super(LiteralResult, self).__init__()
-        self.iteration = iteration
-
-    def Compute(self, cell, values, baseline_values):
-        try:
-            cell.value = values[self.iteration]
-        except IndexError:
-            cell.value = "-"
-
-
-class NonEmptyCountResult(Result):
-    """A class that counts the number of non-empty results.
-
-    The number of non-empty values will be stored in the cell.
-    """
-
-    def Compute(self, cell, values, baseline_values):
-        """Put the number of non-empty values in the cell result.
-
-        Args:
-          cell: Put the result in cell.value.
-          values: A list of values for the row.
-          baseline_values: A list of baseline values for the row.
-        """
-        cell.value = len(_StripNone(values))
-        if not baseline_values:
-            return
-        base_value = len(_StripNone(baseline_values))
-        if cell.value == base_value:
-            return
-        f = ColorBoxFormat()
-        len_values = len(values)
-        len_baseline_values = len(baseline_values)
-        tmp_cell = Cell()
-        tmp_cell.value = 1.0 + (
-            float(cell.value - base_value)
-            / (max(len_values, len_baseline_values))
-        )
-        f.Compute(tmp_cell)
-        cell.bgcolor = tmp_cell.bgcolor
-
-
-class StringMeanResult(Result):
-    """Mean of string values."""
-
-    def _ComputeString(self, cell, values, baseline_values):
-        if self._AllStringsSame(values):
-            cell.value = str(values[0])
-        else:
-            cell.value = "?"
-
-
-class AmeanResult(StringMeanResult):
-    """Arithmetic mean."""
-
-    def __init__(self, ignore_min_max=False):
-        super(AmeanResult, self).__init__()
-        self.ignore_min_max = ignore_min_max
-
-    def _ComputeFloat(self, cell, values, baseline_values):
-        if self.ignore_min_max:
-            values = _RemoveMinMax(cell, values)
-        cell.value = statistics.mean(values)
-
-
-class RawResult(Result):
-    """Raw result."""
-
-
-class IterationResult(Result):
-    """Iteration result."""
-
-
-class MinResult(Result):
-    """Minimum."""
-
-    def _ComputeFloat(self, cell, values, baseline_values):
-        cell.value = min(values)
-
-    def _ComputeString(self, cell, values, baseline_values):
-        if values:
-            cell.value = min(values)
-        else:
-            cell.value = ""
-
-
-class MaxResult(Result):
-    """Maximum."""
-
-    def _ComputeFloat(self, cell, values, baseline_values):
-        cell.value = max(values)
-
-    def _ComputeString(self, cell, values, baseline_values):
-        if values:
-            cell.value = max(values)
-        else:
-            cell.value = ""
-
-
-class NumericalResult(Result):
-    """Numerical result."""
-
-    def _ComputeString(self, cell, values, baseline_values):
-        cell.value = "?"
-
-
-class StdResult(NumericalResult):
-    """Standard deviation."""
-
-    def __init__(self, ignore_min_max=False):
-        super(StdResult, self).__init__()
-        self.ignore_min_max = ignore_min_max
-
-    def _ComputeFloat(self, cell, values, baseline_values):
-        if self.ignore_min_max:
-            values = _RemoveMinMax(cell, values)
-        cell.value = statistics.pstdev(values)
-
-
-class CoeffVarResult(NumericalResult):
-    """Standard deviation / Mean"""
-
-    def __init__(self, ignore_min_max=False):
-        super(CoeffVarResult, self).__init__()
-        self.ignore_min_max = ignore_min_max
-
-    def _ComputeFloat(self, cell, values, baseline_values):
-        if self.ignore_min_max:
-            values = _RemoveMinMax(cell, values)
-        if statistics.mean(values) != 0.0:
-            noise = abs(statistics.pstdev(values) / statistics.mean(values))
-        else:
-            noise = 0.0
-        cell.value = noise
-
-
-class ComparisonResult(Result):
-    """Same or Different."""
-
-    def NeedsBaseline(self):
-        return True
-
-    def _ComputeString(self, cell, values, baseline_values):
-        value = None
-        baseline_value = None
-        if self._AllStringsSame(values):
-            value = values[0]
-        if self._AllStringsSame(baseline_values):
-            baseline_value = baseline_values[0]
-        if value is not None and baseline_value is not None:
-            if value == baseline_value:
-                cell.value = "SAME"
-            else:
-                cell.value = "DIFFERENT"
-        else:
-            cell.value = "?"
-
-
-class PValueResult(ComparisonResult):
-    """P-value."""
-
-    def __init__(self, ignore_min_max=False):
-        super(PValueResult, self).__init__()
-        self.ignore_min_max = ignore_min_max
-
-    def _ComputeFloat(self, cell, values, baseline_values):
-        if self.ignore_min_max:
-            values = _RemoveMinMax(cell, values)
-            baseline_values = _RemoveMinMax(cell, baseline_values)
-        if len(values) < 2 or len(baseline_values) < 2:
-            cell.value = float("nan")
-            return
-        _, cell.value = _ttest_ind(values, baseline_values)
-
-    def _ComputeString(self, cell, values, baseline_values):
-        return float("nan")
-
-
-class KeyAwareComparisonResult(ComparisonResult):
-    """Automatic key aware comparison."""
-
-    def _IsLowerBetter(self, key):
-        # Units in histograms should include directions
-        if "smallerIsBetter" in key:
-            return True
-        if "biggerIsBetter" in key:
-            return False
-
-        # For units in chartjson:
-        # TODO(llozano): Trying to guess direction by looking at the name of the
-        # test does not seem like a good idea. Test frameworks should provide this
-        # info explicitly. I believe Telemetry has this info. Need to find it out.
-        #
-        # Below are some test names for which we are not sure what the
-        # direction is.
-        #
-        # For these we dont know what the direction is. But, since we dont
-        # specify anything, crosperf will assume higher is better:
-        # --percent_impl_scrolled--percent_impl_scrolled--percent
-        # --solid_color_tiles_analyzed--solid_color_tiles_analyzed--count
-        # --total_image_cache_hit_count--total_image_cache_hit_count--count
-        # --total_texture_upload_time_by_url
-        #
-        # About these we are doubtful but we made a guess:
-        # --average_num_missing_tiles_by_url--*--units (low is good)
-        # --experimental_mean_frame_time_by_url--*--units (low is good)
-        # --experimental_median_frame_time_by_url--*--units (low is good)
-        # --texture_upload_count--texture_upload_count--count (high is good)
-        # --total_deferred_image_decode_count--count (low is good)
-        # --total_tiles_analyzed--total_tiles_analyzed--count (high is good)
-        lower_is_better_keys = [
-            "milliseconds",
-            "ms_",
-            "seconds_",
-            "KB",
-            "rdbytes",
-            "wrbytes",
-            "dropped_percent",
-            "(ms)",
-            "(seconds)",
-            "--ms",
-            "--average_num_missing_tiles",
-            "--experimental_jank",
-            "--experimental_mean_frame",
-            "--experimental_median_frame_time",
-            "--total_deferred_image_decode_count",
-            "--seconds",
-            "samples",
-            "bytes",
-        ]
-
-        return any([l in key for l in lower_is_better_keys])
-
-    def _InvertIfLowerIsBetter(self, cell):
-        if self._IsLowerBetter(cell.name):
-            if cell.value:
-                cell.value = 1.0 / cell.value
-
-
-class AmeanRatioResult(KeyAwareComparisonResult):
-    """Ratio of arithmetic means of values vs. baseline values."""
-
-    def __init__(self, ignore_min_max=False):
-        super(AmeanRatioResult, self).__init__()
-        self.ignore_min_max = ignore_min_max
-
-    def _ComputeFloat(self, cell, values, baseline_values):
-        if self.ignore_min_max:
-            values = _RemoveMinMax(cell, values)
-            baseline_values = _RemoveMinMax(cell, baseline_values)
-
-        baseline_mean = statistics.mean(baseline_values)
-        values_mean = statistics.mean(values)
-        if baseline_mean != 0:
-            cell.value = values_mean / baseline_mean
-        elif values_mean != 0:
-            cell.value = 0.00
-            # cell.value = 0 means the values and baseline_values have big difference
-        else:
-            cell.value = 1.00
-            # no difference if both values and baseline_values are 0
-
-
-class GmeanRatioResult(KeyAwareComparisonResult):
-    """Ratio of geometric means of values vs. baseline values."""
-
-    def __init__(self, ignore_min_max=False):
-        super(GmeanRatioResult, self).__init__()
-        self.ignore_min_max = ignore_min_max
-
-    def _ComputeFloat(self, cell, values, baseline_values):
-        if self.ignore_min_max:
-            values = _RemoveMinMax(cell, values)
-            baseline_values = _RemoveMinMax(cell, baseline_values)
-        if self._GetGmean(baseline_values) != 0:
-            cell.value = self._GetGmean(values) / self._GetGmean(
-                baseline_values
-            )
-        elif self._GetGmean(values) != 0:
-            cell.value = 0.00
-        else:
-            cell.value = 1.00
-
-
-class Color(object):
-    """Class that represents color in RGBA format."""
-
-    def __init__(self, r=0, g=0, b=0, a=0):
-        self.r = r
-        self.g = g
-        self.b = b
-        self.a = a
-
-    def __str__(self):
-        return "r: %s g: %s: b: %s: a: %s" % (self.r, self.g, self.b, self.a)
-
-    def Round(self):
-        """Round RGBA values to the nearest integer."""
-        self.r = int(self.r)
-        self.g = int(self.g)
-        self.b = int(self.b)
-        self.a = int(self.a)
-
-    def GetRGB(self):
-        """Get a hex representation of the color."""
-        return "%02x%02x%02x" % (self.r, self.g, self.b)
-
-    @classmethod
-    def Lerp(cls, ratio, a, b):
-        """Perform linear interpolation between two colors.
-
-        Args:
-          ratio: The ratio to use for linear polation.
-          a: The first color object (used when ratio is 0).
-          b: The second color object (used when ratio is 1).
-
-        Returns:
-          Linearly interpolated color.
-        """
-        ret = cls()
-        ret.r = (b.r - a.r) * ratio + a.r
-        ret.g = (b.g - a.g) * ratio + a.g
-        ret.b = (b.b - a.b) * ratio + a.b
-        ret.a = (b.a - a.a) * ratio + a.a
-        return ret
-
-
-class Format(object):
-    """A class that represents the format of a column."""
-
-    def __init__(self):
-        pass
-
-    def Compute(self, cell):
-        """Computes the attributes of a cell based on its value.
-
-        Attributes typically are color, width, etc.
-
-        Args:
-          cell: The cell whose attributes are to be populated.
-        """
-        if cell.value is None:
-            cell.string_value = ""
-        if isinstance(cell.value, float):
-            self._ComputeFloat(cell)
-        else:
-            self._ComputeString(cell)
-
-    def _ComputeFloat(self, cell):
-        cell.string_value = "{0:.2f}".format(cell.value)
-
-    def _ComputeString(self, cell):
-        cell.string_value = str(cell.value)
-
-    def _GetColor(self, value, low, mid, high, power=6, mid_value=1.0):
-        min_value = 0.0
-        max_value = 2.0
-        if math.isnan(value):
-            return mid
-        if value > mid_value:
-            value = max_value - mid_value / value
-
-        return self._GetColorBetweenRange(
-            value, min_value, mid_value, max_value, low, mid, high, power
-        )
-
-    def _GetColorBetweenRange(
-        self,
-        value,
-        min_value,
-        mid_value,
-        max_value,
-        low_color,
-        mid_color,
-        high_color,
-        power,
-    ):
-        assert value <= max_value
-        assert value >= min_value
-        if value > mid_value:
-            value = (max_value - value) / (max_value - mid_value)
-            value **= power
-            ret = Color.Lerp(value, high_color, mid_color)
-        else:
-            value = (value - min_value) / (mid_value - min_value)
-            value **= power
-            ret = Color.Lerp(value, low_color, mid_color)
-        ret.Round()
-        return ret
-
-
-class PValueFormat(Format):
-    """Formatting for p-value."""
-
-    def _ComputeFloat(self, cell):
-        cell.string_value = "%0.2f" % float(cell.value)
-        if float(cell.value) < 0.05:
-            cell.bgcolor = self._GetColor(
-                cell.value,
-                Color(255, 255, 0, 0),
-                Color(255, 255, 255, 0),
-                Color(255, 255, 255, 0),
-                mid_value=0.05,
-                power=1,
-            )
-
-
-class WeightFormat(Format):
-    """Formatting for weight in cwp mode."""
-
-    def _ComputeFloat(self, cell):
-        cell.string_value = "%0.4f" % float(cell.value)
-
-
-class StorageFormat(Format):
-    """Format the cell as a storage number.
-
-    Examples:
-      If the cell contains a value of 1024, the string_value will be 1.0K.
-    """
-
-    def _ComputeFloat(self, cell):
-        base = 1024
-        suffices = ["K", "M", "G"]
-        v = float(cell.value)
-        current = 0
-        while v >= base ** (current + 1) and current < len(suffices):
-            current += 1
-
-        if current:
-            divisor = base**current
-            cell.string_value = "%1.1f%s" % (
-                (v / divisor),
-                suffices[current - 1],
-            )
-        else:
-            cell.string_value = str(cell.value)
-
-
-class CoeffVarFormat(Format):
-    """Format the cell as a percent.
-
-    Examples:
-      If the cell contains a value of 1.5, the string_value will be +150%.
-    """
-
-    def _ComputeFloat(self, cell):
-        cell.string_value = "%1.1f%%" % (float(cell.value) * 100)
-        cell.color = self._GetColor(
-            cell.value,
-            Color(0, 255, 0, 0),
-            Color(0, 0, 0, 0),
-            Color(255, 0, 0, 0),
-            mid_value=0.02,
-            power=1,
-        )
-
-
-class PercentFormat(Format):
-    """Format the cell as a percent.
-
-    Examples:
-      If the cell contains a value of 1.5, the string_value will be +50%.
-    """
-
-    def _ComputeFloat(self, cell):
-        cell.string_value = "%+1.1f%%" % ((float(cell.value) - 1) * 100)
-        cell.color = self._GetColor(
-            cell.value,
-            Color(255, 0, 0, 0),
-            Color(0, 0, 0, 0),
-            Color(0, 255, 0, 0),
-        )
-
-
-class RatioFormat(Format):
-    """Format the cell as a ratio.
-
-    Examples:
-      If the cell contains a value of 1.5642, the string_value will be 1.56.
-    """
-
-    def _ComputeFloat(self, cell):
-        cell.string_value = "%+1.1f%%" % ((cell.value - 1) * 100)
-        cell.color = self._GetColor(
-            cell.value,
-            Color(255, 0, 0, 0),
-            Color(0, 0, 0, 0),
-            Color(0, 255, 0, 0),
-        )
-
-
-class ColorBoxFormat(Format):
-    """Format the cell as a color box.
-
-    Examples:
-      If the cell contains a value of 1.5, it will get a green color.
-      If the cell contains a value of 0.5, it will get a red color.
-      The intensity of the green/red will be determined by how much above or below
-      1.0 the value is.
-    """
-
-    def _ComputeFloat(self, cell):
-        cell.string_value = "--"
-        bgcolor = self._GetColor(
-            cell.value,
-            Color(255, 0, 0, 0),
-            Color(255, 255, 255, 0),
-            Color(0, 255, 0, 0),
-        )
-        cell.bgcolor = bgcolor
-        cell.color = bgcolor
-
-
-class Cell(object):
-    """A class to represent a cell in a table.
-
-    Attributes:
-      value: The raw value of the cell.
-      color: The color of the cell.
-      bgcolor: The background color of the cell.
-      string_value: The string value of the cell.
-      suffix: A string suffix to be attached to the value when displaying.
-      prefix: A string prefix to be attached to the value when displaying.
-      color_row: Indicates whether the whole row is to inherit this cell's color.
-      bgcolor_row: Indicates whether the whole row is to inherit this cell's
-      bgcolor.
-      width: Optional specifier to make a column narrower than the usual width.
-      The usual width of a column is the max of all its cells widths.
-      colspan: Set the colspan of the cell in the HTML table, this is used for
-      table headers. Default value is 1.
-      name: the test name of the cell.
-      header: Whether this is a header in html.
-    """
-
-    def __init__(self):
-        self.value = None
-        self.color = None
-        self.bgcolor = None
-        self.string_value = None
-        self.suffix = None
-        self.prefix = None
-        # Entire row inherits this color.
-        self.color_row = False
-        self.bgcolor_row = False
-        self.width = 0
-        self.colspan = 1
-        self.name = None
-        self.header = False
-
-    def __str__(self):
-        l = []
-        l.append("value: %s" % self.value)
-        l.append("string_value: %s" % self.string_value)
-        return " ".join(l)
-
-
-class Column(object):
-    """Class representing a column in a table.
-
-    Attributes:
-      result: an object of the Result class.
-      fmt: an object of the Format class.
-    """
-
-    def __init__(self, result, fmt, name=""):
-        self.result = result
-        self.fmt = fmt
-        self.name = name
-
-
-# Takes in:
-# ["Key", "Label1", "Label2"]
-# ["k", ["v", "v2"], [v3]]
-# etc.
-# Also takes in a format string.
-# Returns a table like:
-# ["Key", "Label1", "Label2"]
-# ["k", avg("v", "v2"), stddev("v", "v2"), etc.]]
-# according to format string
-class TableFormatter(object):
-    """Class to convert a plain table into a cell-table.
-
-    This class takes in a table generated by TableGenerator and a list of column
-    formats to apply to the table and returns a table of cells.
-    """
-
-    def __init__(self, table, columns, samples_table=False):
-        """The constructor takes in a table and a list of columns.
-
-        Args:
-          table: A list of lists of values.
-          columns: A list of column containing what to produce and how to format
-                   it.
-          samples_table: A flag to check whether we are generating a table of
-                         samples in CWP apporximation mode.
-        """
-        self._table = table
-        self._columns = columns
-        self._samples_table = samples_table
-        self._table_columns = []
-        self._out_table = []
-
-    def GenerateCellTable(self, table_type):
-        row_index = 0
-        all_failed = False
-
-        for row in self._table[1:]:
-            # If we are generating samples_table, the second value will be weight
-            # rather than values.
-            start_col = 2 if self._samples_table else 1
-            # It does not make sense to put retval in the summary table.
-            if str(row[0]) == "retval" and table_type == "summary":
-                # Check to see if any runs passed, and update all_failed.
-                all_failed = True
-                for values in row[start_col:]:
-                    if 0 in values:
-                        all_failed = False
-                continue
-            key = Cell()
-            key.string_value = str(row[0])
-            out_row = [key]
-            if self._samples_table:
-                # Add one column for weight if in samples_table mode
-                weight = Cell()
-                weight.value = row[1]
-                f = WeightFormat()
-                f.Compute(weight)
-                out_row.append(weight)
-            baseline = None
-            for results in row[start_col:]:
-                column_start = 0
-                values = None
-                # If generating sample table, we will split a tuple of iterations info
-                # from the results
-                if isinstance(results, tuple):
-                    it, values = results
-                    column_start = 1
-                    cell = Cell()
-                    cell.string_value = "[%d: %d]" % (it[0], it[1])
-                    out_row.append(cell)
-                    if not row_index:
-                        self._table_columns.append(self._columns[0])
-                else:
-                    values = results
-                # Parse each column
-                for column in self._columns[column_start:]:
-                    cell = Cell()
-                    cell.name = key.string_value
-                    if (
-                        not column.result.NeedsBaseline()
-                        or baseline is not None
-                    ):
-                        column.result.Compute(cell, values, baseline)
-                        column.fmt.Compute(cell)
-                        out_row.append(cell)
-                        if not row_index:
-                            self._table_columns.append(column)
-
-                if baseline is None:
-                    baseline = values
-            self._out_table.append(out_row)
-            row_index += 1
-
-        # If this is a summary table, and the only row in it is 'retval', and
-        # all the test runs failed, we need to a 'Results' row to the output
-        # table.
-        if table_type == "summary" and all_failed and len(self._table) == 2:
-            labels_row = self._table[0]
-            key = Cell()
-            key.string_value = "Results"
-            out_row = [key]
-            baseline = None
-            for _ in labels_row[1:]:
-                for column in self._columns:
-                    cell = Cell()
-                    cell.name = key.string_value
-                    column.result.Compute(cell, ["Fail"], baseline)
-                    column.fmt.Compute(cell)
-                    out_row.append(cell)
-                    if not row_index:
-                        self._table_columns.append(column)
-            self._out_table.append(out_row)
-
-    def AddColumnName(self):
-        """Generate Column name at the top of table."""
-        key = Cell()
-        key.header = True
-        key.string_value = "Keys" if not self._samples_table else "Benchmarks"
-        header = [key]
-        if self._samples_table:
-            weight = Cell()
-            weight.header = True
-            weight.string_value = "Weights"
-            header.append(weight)
-        for column in self._table_columns:
-            cell = Cell()
-            cell.header = True
-            if column.name:
-                cell.string_value = column.name
-            else:
-                result_name = column.result.__class__.__name__
-                format_name = column.fmt.__class__.__name__
-
-                cell.string_value = "%s %s" % (
-                    result_name.replace("Result", ""),
-                    format_name.replace("Format", ""),
-                )
-
-            header.append(cell)
-
-        self._out_table = [header] + self._out_table
-
-    def AddHeader(self, s):
-        """Put additional string on the top of the table."""
-        cell = Cell()
-        cell.header = True
-        cell.string_value = str(s)
-        header = [cell]
-        colspan = max(1, max(len(row) for row in self._table))
-        cell.colspan = colspan
-        self._out_table = [header] + self._out_table
-
-    def GetPassesAndFails(self, values):
-        passes = 0
-        fails = 0
-        for val in values:
-            if val == 0:
-                passes = passes + 1
-            else:
-                fails = fails + 1
-        return passes, fails
-
-    def AddLabelName(self):
-        """Put label on the top of the table."""
-        top_header = []
-        base_colspan = len(
-            [c for c in self._columns if not c.result.NeedsBaseline()]
-        )
-        compare_colspan = len(self._columns)
-        # Find the row with the key 'retval', if it exists.  This
-        # will be used to calculate the number of iterations that passed and
-        # failed for each image label.
-        retval_row = None
-        for row in self._table:
-            if row[0] == "retval":
-                retval_row = row
-        # The label is organized as follows
-        # "keys" label_base, label_comparison1, label_comparison2
-        # The first cell has colspan 1, the second is base_colspan
-        # The others are compare_colspan
-        column_position = 0
-        for label in self._table[0]:
-            cell = Cell()
-            cell.header = True
-            # Put the number of pass/fail iterations in the image label header.
-            if column_position > 0 and retval_row:
-                retval_values = retval_row[column_position]
-                if isinstance(retval_values, list):
-                    passes, fails = self.GetPassesAndFails(retval_values)
-                    cell.string_value = str(label) + "  (pass:%d fail:%d)" % (
-                        passes,
-                        fails,
-                    )
-                else:
-                    cell.string_value = str(label)
-            else:
-                cell.string_value = str(label)
-            if top_header:
-                if not self._samples_table or (
-                    self._samples_table and len(top_header) == 2
-                ):
-                    cell.colspan = base_colspan
-            if len(top_header) > 1:
-                if not self._samples_table or (
-                    self._samples_table and len(top_header) > 2
-                ):
-                    cell.colspan = compare_colspan
-            top_header.append(cell)
-            column_position = column_position + 1
-        self._out_table = [top_header] + self._out_table
-
-    def _PrintOutTable(self):
-        o = ""
-        for row in self._out_table:
-            for cell in row:
-                o += str(cell) + " "
-            o += "\n"
-        print(o)
-
-    def GetCellTable(self, table_type="full", headers=True):
-        """Function to return a table of cells.
-
-        The table (list of lists) is converted into a table of cells by this
-        function.
-
-        Args:
-          table_type: Can be 'full' or 'summary'
-          headers: A boolean saying whether we want default headers
-
-        Returns:
-          A table of cells with each cell having the properties and string values as
-          requiested by the columns passed in the constructor.
-        """
-        # Generate the cell table, creating a list of dynamic columns on the fly.
-        if not self._out_table:
-            self.GenerateCellTable(table_type)
-        if headers:
-            self.AddColumnName()
-            self.AddLabelName()
-        return self._out_table
-
-
-class TablePrinter(object):
-    """Class to print a cell table to the console, file or html."""
-
-    PLAIN = 0
-    CONSOLE = 1
-    HTML = 2
-    TSV = 3
-    EMAIL = 4
-
-    def __init__(self, table, output_type):
-        """Constructor that stores the cell table and output type."""
-        self._table = table
-        self._output_type = output_type
-        self._row_styles = []
-        self._column_styles = []
-
-    # Compute whole-table properties like max-size, etc.
-    def _ComputeStyle(self):
-        self._row_styles = []
-        for row in self._table:
-            row_style = Cell()
-            for cell in row:
-                if cell.color_row:
-                    assert cell.color, "Cell color not set but color_row set!"
-                    assert (
-                        not row_style.color
-                    ), "Multiple row_style.colors found!"
-                    row_style.color = cell.color
-                if cell.bgcolor_row:
-                    assert (
-                        cell.bgcolor
-                    ), "Cell bgcolor not set but bgcolor_row set!"
-                    assert (
-                        not row_style.bgcolor
-                    ), "Multiple row_style.bgcolors found!"
-                    row_style.bgcolor = cell.bgcolor
-            self._row_styles.append(row_style)
-
-        self._column_styles = []
-        if len(self._table) < 2:
-            return
-
-        for i in range(max(len(row) for row in self._table)):
-            column_style = Cell()
-            for row in self._table:
-                if not any([cell.colspan != 1 for cell in row]):
-                    column_style.width = max(
-                        column_style.width, len(row[i].string_value)
-                    )
-            self._column_styles.append(column_style)
-
-    def _GetBGColorFix(self, color):
-        if self._output_type == self.CONSOLE:
-            prefix = misc.rgb2short(color.r, color.g, color.b)
-            # pylint: disable=anomalous-backslash-in-string
-            prefix = "\033[48;5;%sm" % prefix
-            suffix = "\033[0m"
-        elif self._output_type in [self.EMAIL, self.HTML]:
-            rgb = color.GetRGB()
-            prefix = '<FONT style="BACKGROUND-COLOR:#{0}">'.format(rgb)
-            suffix = "</FONT>"
-        elif self._output_type in [self.PLAIN, self.TSV]:
-            prefix = ""
-            suffix = ""
-        return prefix, suffix
-
-    def _GetColorFix(self, color):
-        if self._output_type == self.CONSOLE:
-            prefix = misc.rgb2short(color.r, color.g, color.b)
-            # pylint: disable=anomalous-backslash-in-string
-            prefix = "\033[38;5;%sm" % prefix
-            suffix = "\033[0m"
-        elif self._output_type in [self.EMAIL, self.HTML]:
-            rgb = color.GetRGB()
-            prefix = "<FONT COLOR=#{0}>".format(rgb)
-            suffix = "</FONT>"
-        elif self._output_type in [self.PLAIN, self.TSV]:
-            prefix = ""
-            suffix = ""
-        return prefix, suffix
-
-    def Print(self):
-        """Print the table to a console, html, etc.
-
-        Returns:
-          A string that contains the desired representation of the table.
-        """
-        self._ComputeStyle()
-        return self._GetStringValue()
-
-    def _GetCellValue(self, i, j):
-        cell = self._table[i][j]
-        out = cell.string_value
-        raw_width = len(out)
-
-        if cell.color:
-            p, s = self._GetColorFix(cell.color)
-            out = "%s%s%s" % (p, out, s)
-
-        if cell.bgcolor:
-            p, s = self._GetBGColorFix(cell.bgcolor)
-            out = "%s%s%s" % (p, out, s)
-
-        if self._output_type in [self.PLAIN, self.CONSOLE, self.EMAIL]:
-            if cell.width:
-                width = cell.width
-            else:
-                if self._column_styles:
-                    width = self._column_styles[j].width
-                else:
-                    width = len(cell.string_value)
-            if cell.colspan > 1:
-                width = 0
-                start = 0
-                for k in range(j):
-                    start += self._table[i][k].colspan
-                for k in range(cell.colspan):
-                    width += self._column_styles[start + k].width
-            if width > raw_width:
-                padding = ("%" + str(width - raw_width) + "s") % ""
-                out = padding + out
-
-        if self._output_type == self.HTML:
-            if cell.header:
-                tag = "th"
-            else:
-                tag = "td"
-            out = '<{0} colspan = "{2}"> {1} </{0}>'.format(
-                tag, out, cell.colspan
-            )
-
-        return out
-
-    def _GetHorizontalSeparator(self):
-        if self._output_type in [self.CONSOLE, self.PLAIN, self.EMAIL]:
-            return " "
-        if self._output_type == self.HTML:
-            return ""
-        if self._output_type == self.TSV:
-            return "\t"
-
-    def _GetVerticalSeparator(self):
-        if self._output_type in [
-            self.PLAIN,
-            self.CONSOLE,
-            self.TSV,
-            self.EMAIL,
-        ]:
-            return "\n"
-        if self._output_type == self.HTML:
-            return "</tr>\n<tr>"
-
-    def _GetPrefix(self):
-        if self._output_type in [
-            self.PLAIN,
-            self.CONSOLE,
-            self.TSV,
-            self.EMAIL,
-        ]:
-            return ""
-        if self._output_type == self.HTML:
-            return '<p></p><table id="box-table-a">\n<tr>'
-
-    def _GetSuffix(self):
-        if self._output_type in [
-            self.PLAIN,
-            self.CONSOLE,
-            self.TSV,
-            self.EMAIL,
-        ]:
-            return ""
-        if self._output_type == self.HTML:
-            return "</tr>\n</table>"
-
-    def _GetStringValue(self):
-        o = ""
-        o += self._GetPrefix()
-        for i in range(len(self._table)):
-            row = self._table[i]
-            # Apply row color and bgcolor.
-            p = s = bgp = bgs = ""
-            if self._row_styles[i].bgcolor:
-                bgp, bgs = self._GetBGColorFix(self._row_styles[i].bgcolor)
-            if self._row_styles[i].color:
-                p, s = self._GetColorFix(self._row_styles[i].color)
-            o += p + bgp
-            for j in range(len(row)):
-                out = self._GetCellValue(i, j)
-                o += out + self._GetHorizontalSeparator()
-            o += s + bgs
-            o += self._GetVerticalSeparator()
-        o += self._GetSuffix()
-        return o
-
-
-# Some common drivers
-def GetSimpleTable(table, out_to=TablePrinter.CONSOLE):
-    """Prints a simple table.
-
-    This is used by code that has a very simple list-of-lists and wants to
-    produce a table with ameans, a percentage ratio of ameans and a colorbox.
-
-    Examples:
-      GetSimpleConsoleTable([["binary", "b1", "b2"],["size", "300", "400"]])
-      will produce a colored table that can be printed to the console.
-
-    Args:
-      table: a list of lists.
-      out_to: specify the fomat of output. Currently it supports HTML and CONSOLE.
-
-    Returns:
-      A string version of the table that can be printed to the console.
-    """
-    columns = [
-        Column(AmeanResult(), Format()),
-        Column(AmeanRatioResult(), PercentFormat()),
-        Column(AmeanRatioResult(), ColorBoxFormat()),
-    ]
-    our_table = [table[0]]
-    for row in table[1:]:
-        our_row = [row[0]]
-        for v in row[1:]:
-            our_row.append([v])
-        our_table.append(our_row)
-
-    tf = TableFormatter(our_table, columns)
-    cell_table = tf.GetCellTable()
-    tp = TablePrinter(cell_table, out_to)
-    return tp.Print()
-
-
-# pylint: disable=redefined-outer-name
-def GetComplexTable(runs, labels, out_to=TablePrinter.CONSOLE):
-    """Prints a complex table.
-
-    This can be used to generate a table with arithmetic mean, standard deviation,
-    coefficient of variation, p-values, etc.
-
-    Args:
-      runs: A list of lists with data to tabulate.
-      labels: A list of labels that correspond to the runs.
-      out_to: specifies the format of the table (example CONSOLE or HTML).
-
-    Returns:
-      A string table that can be printed to the console or put in an HTML file.
-    """
-    tg = TableGenerator(runs, labels, TableGenerator.SORT_BY_VALUES_DESC)
-    table = tg.GetTable()
-    columns = [
-        Column(LiteralResult(), Format(), "Literal"),
-        Column(AmeanResult(), Format()),
-        Column(StdResult(), Format()),
-        Column(CoeffVarResult(), CoeffVarFormat()),
-        Column(NonEmptyCountResult(), Format()),
-        Column(AmeanRatioResult(), PercentFormat()),
-        Column(AmeanRatioResult(), RatioFormat()),
-        Column(GmeanRatioResult(), RatioFormat()),
-        Column(PValueResult(), PValueFormat()),
-    ]
-    tf = TableFormatter(table, columns)
-    cell_table = tf.GetCellTable()
-    tp = TablePrinter(cell_table, out_to)
-    return tp.Print()
-
-
-if __name__ == "__main__":
-    # Run a few small tests here.
-    run1 = {
-        "k1": "10",
-        "k2": "12",
-        "k5": "40",
-        "k6": "40",
-        "ms_1": "20",
-        "k7": "FAIL",
-        "k8": "PASS",
-        "k9": "PASS",
-        "k10": "0",
-    }
-    run2 = {
-        "k1": "13",
-        "k2": "14",
-        "k3": "15",
-        "ms_1": "10",
-        "k8": "PASS",
-        "k9": "FAIL",
-        "k10": "0",
-    }
-    run3 = {
-        "k1": "50",
-        "k2": "51",
-        "k3": "52",
-        "k4": "53",
-        "k5": "35",
-        "k6": "45",
-        "ms_1": "200",
-        "ms_2": "20",
-        "k7": "FAIL",
-        "k8": "PASS",
-        "k9": "PASS",
-    }
-    runs = [[run1, run2], [run3]]
-    labels = ["vanilla", "modified"]
-    t = GetComplexTable(runs, labels, TablePrinter.CONSOLE)
-    print(t)
-    email = GetComplexTable(runs, labels, TablePrinter.EMAIL)
-
-    runs = [
-        [{"k1": "1"}, {"k1": "1.1"}, {"k1": "1.2"}],
-        [{"k1": "5"}, {"k1": "5.1"}, {"k1": "5.2"}],
-    ]
-    t = GetComplexTable(runs, labels, TablePrinter.CONSOLE)
-    print(t)
-
-    simple_table = [
-        ["binary", "b1", "b2", "b3"],
-        ["size", 100, 105, 108],
-        ["rodata", 100, 80, 70],
-        ["data", 100, 100, 100],
-        ["debug", 100, 140, 60],
-    ]
-    t = GetSimpleTable(simple_table)
-    print(t)
-    email += GetSimpleTable(simple_table, TablePrinter.HTML)
-    email_to = [getpass.getuser()]
-    email = "<pre style='font-size: 13px'>%s</pre>" % email
-    EmailSender().SendEmail(email_to, "SimpleTableTest", email, msg_type="html")
diff --git a/cros_utils/tabulator_test.py b/cros_utils/tabulator_test.py
deleted file mode 100755
index 91ce8fd5..00000000
--- a/cros_utils/tabulator_test.py
+++ /dev/null
@@ -1,213 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2012 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Tests for the tabulator module."""
-
-
-__author__ = 'asharif@google.com (Ahmad Sharif)'
-
-# System modules
-import unittest
-
-# Local modules
-from cros_utils import tabulator
-
-
-class TabulatorTest(unittest.TestCase):
-  """Tests for the Tabulator class."""
-
-  def testResult(self):
-    table = ['k1', ['1', '3'], ['55']]
-    result = tabulator.Result()
-    cell = tabulator.Cell()
-    result.Compute(cell, table[2], table[1])
-    expected = ' '.join([str(float(v)) for v in table[2]])
-    self.assertTrue(cell.value == expected)
-
-    result = tabulator.AmeanResult()
-    cell = tabulator.Cell()
-    result.Compute(cell, table[2], table[1])
-    self.assertTrue(cell.value == float(table[2][0]))
-
-  def testStdResult(self):
-    table = ['k1', [], ['1', '2']]
-    result = tabulator.StdResult()
-    cell = tabulator.Cell()
-    result.Compute(cell, table[2], table[1])
-    self.assertTrue(cell.value == 0.5)
-
-  def testStdResultOfSampleSize1(self):
-    table = ['k1', [], ['1']]
-    result = tabulator.StdResult()
-    cell = tabulator.Cell()
-    result.Compute(cell, table[2], table[1])
-    self.assertTrue(cell.value == 0.0)
-
-  def testStringMean(self):
-    smr = tabulator.StringMeanResult()
-    cell = tabulator.Cell()
-    value = 'PASS'
-    values = [value for _ in range(3)]
-    smr.Compute(cell, values, None)
-    self.assertTrue(cell.value == value)
-    values.append('FAIL')
-    smr.Compute(cell, values, None)
-    self.assertTrue(cell.value == '?')
-
-  def testStorageFormat(self):
-    sf = tabulator.StorageFormat()
-    cell = tabulator.Cell()
-    base = 1024.0
-    cell.value = base
-    sf.Compute(cell)
-    self.assertTrue(cell.string_value == '1.0K')
-    cell.value = base**2
-    sf.Compute(cell)
-    self.assertTrue(cell.string_value == '1.0M')
-    cell.value = base**3
-    sf.Compute(cell)
-    self.assertTrue(cell.string_value == '1.0G')
-
-  def testLerp(self):
-    c1 = tabulator.Color(0, 0, 0, 0)
-    c2 = tabulator.Color(255, 0, 0, 0)
-    c3 = tabulator.Color.Lerp(0.5, c1, c2)
-    self.assertTrue(c3.r == 127.5)
-    self.assertTrue(c3.g == 0)
-    self.assertTrue(c3.b == 0)
-    self.assertTrue(c3.a == 0)
-    c3.Round()
-    self.assertTrue(c3.r == 127)
-
-  def testGmean(self):
-    a = [1.0e+308] * 3
-    # pylint: disable=protected-access
-    b = tabulator.Result()._GetGmean(a)
-    self.assertTrue(0.99e+308 <= b <= 1.01e+308)
-
-  def testIgnoreMinMax(self):
-    amr = tabulator.AmeanResult(ignore_min_max=True)
-    cell = tabulator.Cell()
-    values = [1, 2]
-    amr.Compute(cell, values, None)
-    self.assertTrue(cell.value == 1.5)
-    values = [1, 2, 8]
-    amr.Compute(cell, values, None)
-    self.assertTrue(cell.value == 2)
-
-  def testTableGenerator(self):
-    # yapf: disable
-    runs = [[{'k1': '10', 'k2': '12'},
-             {'k1': '13', 'k2': '14', 'k3': '15'}],
-            [{'k1': '50', 'k2': '51', 'k3': '52', 'k4': '53'}]]
-    # yapf: enable
-    labels = ['vanilla', 'modified']
-    tg = tabulator.TableGenerator(runs, labels)
-    table = tg.GetTable()
-    header = table.pop(0)
-
-    self.assertTrue(header == ['keys', 'vanilla', 'modified'])
-    row = table.pop(0)
-    self.assertTrue(row == ['k1', ['10', '13'], ['50']])
-    row = table.pop(0)
-    self.assertTrue(row == ['k2', ['12', '14'], ['51']])
-    row = table.pop(0)
-    self.assertTrue(row == ['k3', [None, '15'], ['52']])
-    row = table.pop(0)
-    self.assertTrue(row == ['k4', [None, None], ['53']])
-
-    table = tg.GetTable()
-    columns = [
-        tabulator.Column(tabulator.AmeanResult(), tabulator.Format()),
-        tabulator.Column(tabulator.AmeanRatioResult(),
-                         tabulator.PercentFormat()),
-    ]
-    tf = tabulator.TableFormatter(table, columns)
-    table = tf.GetCellTable()
-    self.assertTrue(table)
-
-  def testSamplesTableGenerator(self):
-    # yapf: disable
-    keyvals = {
-        'bench1': [[{'samples': 1}, {'samples': 2}],
-                   [{'samples': 3}, {'samples': 4}]],
-        'bench2': [[{'samples': 5}, {}],
-                   [{'samples': 6}, {'samples': 7}]]
-    }
-    # yapf: enable
-    weights = {'bench1': 0.2, 'bench2': 0.7}
-    iter_counts = {'bench1': 2, 'bench2': 2}
-    labels = ['vanilla', 'modified']
-    tg = tabulator.SamplesTableGenerator(keyvals, labels, iter_counts, weights)
-    (table, new_keyvals, new_iter_counts) = tg.GetTable()
-
-    columns = [
-        tabulator.Column(tabulator.IterationResult(), tabulator.Format()),
-        tabulator.Column(tabulator.AmeanResult(), tabulator.Format()),
-        tabulator.Column(tabulator.AmeanRatioResult(),
-                         tabulator.PercentFormat()),
-    ]
-    # This is the function to load column info.
-    tf = tabulator.TableFormatter(table, columns, samples_table=True)
-    # This is the function where to do all weighting calculation.
-    cell_table = tf.GetCellTable('summary')
-    self.assertTrue(cell_table)
-
-    header = table.pop(0)
-    self.assertTrue(header == ['Benchmarks', 'Weights', 'vanilla', 'modified'])
-    row = table.pop(0)
-    # yapf: disable
-    self.assertTrue(row == ['bench1', 0.2,
-                            ((2, 0), [1 * 0.2, 2 * 0.2]),
-                            ((2, 0), [3 * 0.2, 4 * 0.2])])
-    row = table.pop(0)
-    self.assertTrue(row == ['bench2', 0.7,
-                            ((1, 1), [5 * 0.7, None]),
-                            ((2, 0), [6 * 0.7, 7 * 0.7])])
-    row = table.pop(0)
-    self.assertTrue(row == ['Composite Benchmark (samples)', 'N/A',
-                            ((1, 1), [1 * 0.2 + 5 * 0.7, None]),
-                            ((2, 0), [3 * 0.2 + 6 * 0.7, 4 * 0.2 + 7 * 0.7])])
-    # yapf: enable
-    self.assertTrue('Composite Benchmark' in new_keyvals.keys())
-    self.assertTrue('Composite Benchmark' in new_iter_counts.keys())
-
-  def testColspan(self):
-    simple_table = [
-        ['binary', 'b1', 'b2', 'b3'],
-        ['size', 100, 105, 108],
-        ['rodata', 100, 80, 70],
-        ['data', 100, 100, 100],
-        ['debug', 100, 140, 60],
-    ]
-    columns = [
-        tabulator.Column(tabulator.AmeanResult(), tabulator.Format()),
-        tabulator.Column(tabulator.MinResult(), tabulator.Format()),
-        tabulator.Column(tabulator.AmeanRatioResult(),
-                         tabulator.PercentFormat()),
-        tabulator.Column(tabulator.AmeanRatioResult(),
-                         tabulator.ColorBoxFormat()),
-    ]
-    our_table = [simple_table[0]]
-    for row in simple_table[1:]:
-      our_row = [row[0]]
-      for v in row[1:]:
-        our_row.append([v])
-      our_table.append(our_row)
-
-    tf = tabulator.TableFormatter(our_table, columns)
-    cell_table = tf.GetCellTable()
-    self.assertTrue(cell_table[0][0].colspan == 1)
-    self.assertTrue(cell_table[0][1].colspan == 2)
-    self.assertTrue(cell_table[0][2].colspan == 4)
-    self.assertTrue(cell_table[0][3].colspan == 4)
-    for row in cell_table[1:]:
-      for cell in row:
-        self.assertTrue(cell.colspan == 1)
-
-
-if __name__ == '__main__':
-  unittest.main()
diff --git a/cros_utils/timeline.py b/cros_utils/timeline.py
deleted file mode 100644
index f18a39bb..00000000
--- a/cros_utils/timeline.py
+++ /dev/null
@@ -1,55 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Tools for recording and reporting timeline of benchmark_run."""
-
-
-__author__ = "yunlian@google.com (Yunlian Jiang)"
-
-import time
-
-
-class Event(object):
-    """One event on the timeline."""
-
-    def __init__(self, name="", cur_time=0):
-        self.name = name
-        self.timestamp = cur_time
-
-
-class Timeline(object):
-    """Use a dict to store the timeline."""
-
-    def __init__(self):
-        self.events = []
-
-    def Record(self, event):
-        for e in self.events:
-            assert e.name != event, "The event {0} is already recorded.".format(
-                event
-            )
-        cur_event = Event(name=event, cur_time=time.time())
-        self.events.append(cur_event)
-
-    def GetEvents(self):
-        return [e.name for e in self.events]
-
-    def GetEventDict(self):
-        tl = {}
-        for e in self.events:
-            tl[e.name] = e.timestamp
-        return tl
-
-    def GetEventTime(self, event):
-        for e in self.events:
-            if e.name == event:
-                return e.timestamp
-        raise IndexError("The event {0} is not recorded".format(event))
-
-    def GetLastEventTime(self):
-        return self.events[-1].timestamp
-
-    def GetLastEvent(self):
-        return self.events[-1].name
diff --git a/cros_utils/timeline_test.py b/cros_utils/timeline_test.py
deleted file mode 100755
index aceab2df..00000000
--- a/cros_utils/timeline_test.py
+++ /dev/null
@@ -1,61 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Tests for time_line.py."""
-
-
-__author__ = "yunlian@google.com (Yunlian Jiang)"
-
-import time
-import unittest
-
-from cros_utils import timeline
-
-
-class TimeLineTest(unittest.TestCase):
-    """Tests for the Timeline class."""
-
-    def testRecord(self):
-        tl = timeline.Timeline()
-        tl.Record("A")
-        t = time.time()
-        t1 = tl.events[0].timestamp
-        self.assertEqual(int(t1 - t), 0)
-        self.assertRaises(AssertionError, tl.Record, "A")
-
-    def testGetEvents(self):
-        tl = timeline.Timeline()
-        tl.Record("A")
-        e = tl.GetEvents()
-        self.assertEqual(e, ["A"])
-        tl.Record("B")
-        e = tl.GetEvents()
-        self.assertEqual(e, ["A", "B"])
-
-    def testGetEventTime(self):
-        tl = timeline.Timeline()
-        tl.Record("A")
-        t = time.time()
-        t1 = tl.GetEventTime("A")
-        self.assertEqual(int(t1 - t), 0)
-        self.assertRaises(IndexError, tl.GetEventTime, "B")
-
-    def testGetLastEventTime(self):
-        tl = timeline.Timeline()
-        self.assertRaises(IndexError, tl.GetLastEventTime)
-        tl.Record("A")
-        t = time.time()
-        t1 = tl.GetLastEventTime()
-        self.assertEqual(int(t1 - t), 0)
-        time.sleep(2)
-        tl.Record("B")
-        t = time.time()
-        t1 = tl.GetLastEventTime()
-        self.assertEqual(int(t1 - t), 0)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/cros_utils/tiny_render_test.py b/cros_utils/tiny_render_test.py
old mode 100755
new mode 100644
index 9c4d750b..c213ad5c
--- a/cros_utils/tiny_render_test.py
+++ b/cros_utils/tiny_render_test.py
@@ -9,7 +9,7 @@
 
 import unittest
 
-import tiny_render
+from cros_utils import tiny_render
 
 
 # Admittedly, the HTML generated by this isn't always _beautiful_ to read
@@ -194,7 +194,3 @@ class Test(unittest.TestCase):
                 )
             ),
         )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/README.md b/crosperf/README.md
deleted file mode 100644
index f1429513..00000000
--- a/crosperf/README.md
+++ /dev/null
@@ -1,44 +0,0 @@
-# experiment_files
-
-To use these experiment files, replace the board, remote and images
-placeholders and run crosperf on them.
-
-Further information about crosperf: https://goto.google.com/crostc-crosperf
-
-The final experiment file should look something like the following (but with
-different actual values for the fields):
-
-```
-board: lumpy
-remote: 123.45.67.089
-
-# Add images you want to test:
-
-my_image {
-  chromeos_image: /usr/local/chromeos/src/build/images/lumpy/chromiumos_test_image.bin
-}
-
-vanilla_image {
-   chromeos_root: /usr/local/chromeos
-   build: lumpy-release/R35-5672.0.0
-}
-
-# Paste experiment benchmarks here. Example, I pasted
-# `page_cycler_v2.morejs` here.
-
-# This experiment just runs a short autotest which measures the performance
-# of Telemetry's `page_cycler_v2.morejs`. In addition, it profiles cycles.
-
-perf_args: record -e cycles
-
-benchmark: page_cycler_v2.morejs {
-   suite: telemetry_Crosperf
-   iterations: 1
-}
-```
-
-# default_remotes
-
-This is the list of machines allocated for toolchain team.
-This should be kept in sync with:
-https://chromeos-swarming.appspot.com/botlist?c=id&c=task&c=label-board&c=label-pool&c=os&c=status&d=asc&f=label-pool%3Atoolchain&k=label-pool&s=id
diff --git a/crosperf/benchmark.py b/crosperf/benchmark.py
deleted file mode 100644
index eb8661e9..00000000
--- a/crosperf/benchmark.py
+++ /dev/null
@@ -1,129 +0,0 @@
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Define a type that wraps a Benchmark instance."""
-
-
-import math
-import statistics
-from typing import Any
-
-import numpy as np
-
-
-# See crbug.com/673558 for how these are estimated.
-_estimated_stddev = {
-    "octane": 0.015,
-    "kraken": 0.019,
-    "speedometer": 0.007,
-    "speedometer2": 0.006,
-    "dromaeo.domcoreattr": 0.023,
-    "dromaeo.domcoremodify": 0.011,
-    "graphics_WebGLAquarium": 0.008,
-    "page_cycler_v2.typical_25": 0.021,
-    "loading.desktop": 0.021,  # Copied from page_cycler initially
-}
-
-# Numpy makes it hard to know the real type of some inputs
-# and outputs, so this type alias is just for docs.
-FloatLike = Any
-
-
-def isf(x: FloatLike, mu=0.0, sigma=1.0, pitch=0.01) -> FloatLike:
-    """Compute the inverse survival function for value x.
-
-    In the abscence of using scipy.stats.norm's isf(), this function
-    attempts to re-implement the inverse survival function by calculating
-    the numerical inverse of the survival function, interpolating between
-    table values. See bug b/284489250 for details.
-
-    Survival function as defined by:
-    https://en.wikipedia.org/wiki/Survival_function
-
-    Examples:
-        >>> -2.0e-16 < isf(0.5) <  2.0e-16
-        True
-
-    Args:
-        x: float or numpy array-like to compute the ISF for.
-        mu: Center of the underlying normal distribution.
-        sigma: Spread of the underlying normal distribution.
-        pitch: Absolute spacing between y-value interpolation points.
-
-    Returns:
-        float or numpy array-like representing the ISF of `x`.
-    """
-    norm = statistics.NormalDist(mu, sigma)
-    # np.interp requires a monotonically increasing x table.
-    # Because the survival table is monotonically decreasing, we have to
-    # reverse the y_vals too.
-    y_vals = np.flip(np.arange(-4.0, 4.0, pitch))
-    survival_table = np.fromiter(
-        (1.0 - norm.cdf(y) for y in y_vals), y_vals.dtype
-    )
-    return np.interp(x, survival_table, y_vals)
-
-
-# Get #samples needed to guarantee a given confidence interval, assuming the
-# samples follow normal distribution.
-def _samples(b: str) -> int:
-    # TODO: Make this an option
-    # CI = (0.9, 0.02), i.e., 90% chance that |sample mean - true mean| < 2%.
-    p = 0.9
-    e = 0.02
-    if b not in _estimated_stddev:
-        return 1
-    d = _estimated_stddev[b]
-    # Get at least 2 samples so as to calculate standard deviation, which is
-    # needed in T-test for p-value.
-    n = int(math.ceil((isf((1 - p) / 2) * d / e) ** 2))
-    return n if n > 1 else 2
-
-
-class Benchmark(object):
-    """Class representing a benchmark to be run.
-
-    Contains details of the benchmark suite, arguments to pass to the suite,
-    iterations to run the benchmark suite and so on. Note that the benchmark name
-    can be different to the test suite name. For example, you may want to have
-    two different benchmarks which run the same test_name with different
-    arguments.
-    """
-
-    def __init__(
-        self,
-        name,
-        test_name,
-        test_args,
-        iterations,
-        rm_chroot_tmp,
-        perf_args,
-        suite="",
-        show_all_results=False,
-        retries=0,
-        run_local=False,
-        cwp_dso="",
-        weight=0,
-    ):
-        self.name = name
-        # For telemetry, this is the benchmark name.
-        self.test_name = test_name
-        # For telemetry, this is the data.
-        self.test_args = test_args
-        self.iterations = iterations if iterations > 0 else _samples(name)
-        self.perf_args = perf_args
-        self.rm_chroot_tmp = rm_chroot_tmp
-        self.iteration_adjusted = False
-        self.suite = suite
-        self.show_all_results = show_all_results
-        self.retries = retries
-        if self.suite == "telemetry":
-            self.show_all_results = True
-        if run_local and self.suite != "telemetry_Crosperf":
-            raise RuntimeError(
-                "run_local is only supported by telemetry_Crosperf."
-            )
-        self.run_local = run_local
-        self.cwp_dso = cwp_dso
-        self.weight = weight
diff --git a/crosperf/benchmark_run.py b/crosperf/benchmark_run.py
deleted file mode 100644
index 84797d1c..00000000
--- a/crosperf/benchmark_run.py
+++ /dev/null
@@ -1,350 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Module of benchmark runs."""
-
-import datetime
-import threading
-import time
-import traceback
-
-from cros_utils import command_executer
-from cros_utils import timeline
-from results_cache import MockResult
-from results_cache import MockResultsCache
-from results_cache import Result
-from results_cache import ResultsCache
-from suite_runner import SuiteRunner
-
-
-STATUS_FAILED = "FAILED"
-STATUS_SUCCEEDED = "SUCCEEDED"
-STATUS_IMAGING = "IMAGING"
-STATUS_RUNNING = "RUNNING"
-STATUS_WAITING = "WAITING"
-STATUS_PENDING = "PENDING"
-
-
-class BenchmarkRun(threading.Thread):
-    """The benchmarkrun class."""
-
-    def __init__(
-        self,
-        name,
-        benchmark,
-        label,
-        iteration,
-        cache_conditions,
-        machine_manager,
-        logger_to_use,
-        log_level,
-        share_cache,
-        dut_config,
-    ):
-        threading.Thread.__init__(self)
-        self.name = name
-        self._logger = logger_to_use
-        self.log_level = log_level
-        self.benchmark = benchmark
-        self.iteration = iteration
-        self.label = label
-        self.result = None
-        self.terminated = False
-        self.retval = None
-        self.run_completed = False
-        self.machine_manager = machine_manager
-        self.suite_runner = SuiteRunner(
-            dut_config, self._logger, self.log_level
-        )
-        self.machine = None
-        self.cache_conditions = cache_conditions
-        self.runs_complete = 0
-        self.cache_hit = False
-        self.failure_reason = ""
-        self.test_args = benchmark.test_args
-        self.cache = None
-        self.profiler_args = self.GetExtraAutotestArgs()
-        self._ce = command_executer.GetCommandExecuter(
-            self._logger, log_level=self.log_level
-        )
-        self.timeline = timeline.Timeline()
-        self.timeline.Record(STATUS_PENDING)
-        self.share_cache = share_cache
-        self.cache_has_been_read = False
-
-        # This is used by schedv2.
-        self.owner_thread = None
-
-    def ReadCache(self):
-        # Just use the first machine for running the cached version,
-        # without locking it.
-        self.cache = ResultsCache()
-        self.cache.Init(
-            self.label.chromeos_image,
-            self.label.chromeos_root,
-            self.benchmark.test_name,
-            self.iteration,
-            self.test_args,
-            self.profiler_args,
-            self.machine_manager,
-            self.machine,
-            self.label.board,
-            self.cache_conditions,
-            self._logger,
-            self.log_level,
-            self.label,
-            self.share_cache,
-            self.benchmark.suite,
-            self.benchmark.show_all_results,
-            self.benchmark.run_local,
-            self.benchmark.cwp_dso,
-        )
-
-        self.result = self.cache.ReadResult()
-        self.cache_hit = self.result is not None
-        self.cache_has_been_read = True
-
-    def run(self):
-        try:
-            if not self.cache_has_been_read:
-                self.ReadCache()
-
-            if self.result:
-                self._logger.LogOutput("%s: Cache hit." % self.name)
-                self._logger.LogOutput(self.result.out, print_to_console=False)
-                self._logger.LogError(self.result.err, print_to_console=False)
-
-            elif self.label.cache_only:
-                self._logger.LogOutput("%s: No cache hit." % self.name)
-                output = "%s: No Cache hit." % self.name
-                retval = 1
-                err = "No cache hit."
-                self.result = Result.CreateFromRun(
-                    self._logger,
-                    self.log_level,
-                    self.label,
-                    self.machine,
-                    output,
-                    err,
-                    retval,
-                    self.benchmark.test_name,
-                    self.benchmark.suite,
-                    self.benchmark.cwp_dso,
-                )
-
-            else:
-                self._logger.LogOutput("%s: No cache hit." % self.name)
-                self.timeline.Record(STATUS_WAITING)
-                # Try to acquire a machine now.
-                self.machine = self.AcquireMachine()
-                self.cache.machine = self.machine
-                self.result = self.RunTest(self.machine)
-
-                self.cache.remote = self.machine.name
-                self.label.chrome_version = (
-                    self.machine_manager.GetChromeVersion(self.machine)
-                )
-                self.cache.StoreResult(self.result)
-
-            if not self.label.chrome_version:
-                if self.machine:
-                    self.label.chrome_version = (
-                        self.machine_manager.GetChromeVersion(self.machine)
-                    )
-                elif self.result.chrome_version:
-                    self.label.chrome_version = self.result.chrome_version
-
-            if self.terminated:
-                return
-
-            if not self.result.retval:
-                self.timeline.Record(STATUS_SUCCEEDED)
-            else:
-                if self.timeline.GetLastEvent() != STATUS_FAILED:
-                    self.failure_reason = (
-                        "Return value of test suite was non-zero."
-                    )
-                    self.timeline.Record(STATUS_FAILED)
-
-        except Exception as e:
-            self._logger.LogError(
-                "Benchmark run: '%s' failed: %s" % (self.name, e)
-            )
-            traceback.print_exc()
-            if self.timeline.GetLastEvent() != STATUS_FAILED:
-                self.timeline.Record(STATUS_FAILED)
-                self.failure_reason = str(e)
-        finally:
-            if self.owner_thread is not None:
-                # In schedv2 mode, we do not lock machine locally. So noop here.
-                pass
-            elif self.machine:
-                if not self.machine.IsReachable():
-                    self._logger.LogOutput(
-                        "Machine %s is not reachable, removing it."
-                        % self.machine.name
-                    )
-                    self.machine_manager.RemoveMachine(self.machine.name)
-                self._logger.LogOutput(
-                    "Releasing machine: %s" % self.machine.name
-                )
-                self.machine_manager.ReleaseMachine(self.machine)
-                self._logger.LogOutput(
-                    "Released machine: %s" % self.machine.name
-                )
-
-    def Terminate(self):
-        self.terminated = True
-        self.suite_runner.Terminate()
-        if self.timeline.GetLastEvent() != STATUS_FAILED:
-            self.timeline.Record(STATUS_FAILED)
-            self.failure_reason = "Thread terminated."
-
-    def AcquireMachine(self):
-        if self.owner_thread is not None:
-            # No need to lock machine locally, DutWorker, which is a thread, is
-            # responsible for running br.
-            return self.owner_thread.dut()
-        while True:
-            machine = None
-            if self.terminated:
-                raise RuntimeError(
-                    "Thread terminated while trying to acquire machine."
-                )
-
-            machine = self.machine_manager.AcquireMachine(self.label)
-
-            if machine:
-                self._logger.LogOutput(
-                    "%s: Machine %s acquired at %s"
-                    % (self.name, machine.name, datetime.datetime.now())
-                )
-                break
-            time.sleep(10)
-        return machine
-
-    def GetExtraAutotestArgs(self):
-        if (
-            self.benchmark.perf_args
-            and self.benchmark.suite != "telemetry_Crosperf"
-        ):
-            self._logger.LogError(
-                "Non-telemetry benchmark does not support profiler."
-            )
-            self.benchmark.perf_args = ""
-
-        if self.benchmark.perf_args:
-            perf_args_list = self.benchmark.perf_args.split(" ")
-            perf_args_list = [perf_args_list[0]] + ["-a"] + perf_args_list[1:]
-            perf_args = " ".join(perf_args_list)
-            if not perf_args_list[0] in ["record", "stat"]:
-                raise SyntaxError(
-                    "perf_args must start with either record or stat"
-                )
-            extra_test_args = [
-                "--profiler=custom_perf",
-                ("--profiler_args='perf_options=\"%s\"'" % perf_args),
-            ]
-            return " ".join(extra_test_args)
-        else:
-            return ""
-
-    def RunTest(self, machine):
-        self.timeline.Record(STATUS_IMAGING)
-        if self.owner_thread is not None:
-            # In schedv2 mode, do not even call ImageMachine. Machine image is
-            # guarenteed.
-            pass
-        else:
-            self.machine_manager.ImageMachine(machine, self.label)
-        self.timeline.Record(STATUS_RUNNING)
-        retval, out, err = self.suite_runner.Run(
-            machine,
-            self.label,
-            self.benchmark,
-            self.test_args,
-            self.profiler_args,
-        )
-        self.run_completed = True
-        return Result.CreateFromRun(
-            self._logger,
-            self.log_level,
-            self.label,
-            self.machine,
-            out,
-            err,
-            retval,
-            self.benchmark.test_name,
-            self.benchmark.suite,
-            self.benchmark.cwp_dso,
-        )
-
-    def SetCacheConditions(self, cache_conditions):
-        self.cache_conditions = cache_conditions
-
-    def logger(self):
-        """Return the logger, only used by unittest.
-
-        Returns:
-          self._logger
-        """
-
-        return self._logger
-
-    def __str__(self):
-        """For better debugging."""
-
-        return 'BenchmarkRun[name="{}"]'.format(self.name)
-
-
-class MockBenchmarkRun(BenchmarkRun):
-    """Inherited from BenchmarkRun."""
-
-    def ReadCache(self):
-        # Just use the first machine for running the cached version,
-        # without locking it.
-        self.cache = MockResultsCache()
-        self.cache.Init(
-            self.label.chromeos_image,
-            self.label.chromeos_root,
-            self.benchmark.test_name,
-            self.iteration,
-            self.test_args,
-            self.profiler_args,
-            self.machine_manager,
-            self.machine,
-            self.label.board,
-            self.cache_conditions,
-            self._logger,
-            self.log_level,
-            self.label,
-            self.share_cache,
-            self.benchmark.suite,
-            self.benchmark.show_all_results,
-            self.benchmark.run_local,
-            self.benchmark.cwp_dso,
-        )
-
-        self.result = self.cache.ReadResult()
-        self.cache_hit = self.result is not None
-
-    def RunTest(self, machine):
-        """Remove Result.CreateFromRun for testing."""
-        self.timeline.Record(STATUS_IMAGING)
-        self.machine_manager.ImageMachine(machine, self.label)
-        self.timeline.Record(STATUS_RUNNING)
-        [retval, out, err] = self.suite_runner.Run(
-            machine,
-            self.label,
-            self.benchmark,
-            self.test_args,
-            self.profiler_args,
-        )
-        self.run_completed = True
-        rr = MockResult("logger", self.label, self.log_level, machine)
-        rr.out = out
-        rr.err = err
-        rr.retval = retval
-        return rr
diff --git a/crosperf/benchmark_run_unittest.py b/crosperf/benchmark_run_unittest.py
deleted file mode 100755
index 0013e19b..00000000
--- a/crosperf/benchmark_run_unittest.py
+++ /dev/null
@@ -1,545 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Testing of benchmark_run."""
-
-
-import inspect
-import unittest
-import unittest.mock as mock
-
-from benchmark import Benchmark
-import benchmark_run
-from cros_utils import logger
-from label import MockLabel
-from machine_manager import MachineManager
-from machine_manager import MockCrosMachine
-from machine_manager import MockMachineManager
-from results_cache import CacheConditions
-from results_cache import MockResultsCache
-from results_cache import Result
-from results_cache import ResultsCache
-from suite_runner import MockSuiteRunner
-from suite_runner import SuiteRunner
-
-
-class BenchmarkRunTest(unittest.TestCase):
-    """Unit tests for the BenchmarkRun class and all of its methods."""
-
-    def setUp(self):
-        self.status = []
-        self.called_ReadCache = None
-        self.log_error = []
-        self.log_output = []
-        self.err_msg = None
-        self.test_benchmark = Benchmark(
-            "page_cycler.netsim.top_10",  # name
-            "page_cycler.netsim.top_10",  # test_name
-            "",  # test_args
-            1,  # iterations
-            False,  # rm_chroot_tmp
-            "",  # perf_args
-            suite="telemetry_Crosperf",
-        )  # suite
-
-        self.test_label = MockLabel(
-            "test1",
-            "build",
-            "image1",
-            "autotest_dir",
-            "debug_dir",
-            "/tmp/test_benchmark_run",
-            "x86-alex",
-            "chromeos2-row1-rack4-host9.cros",
-            image_args="",
-            cache_dir="",
-            cache_only=False,
-            log_level="average",
-            compiler="gcc",
-            crosfleet=False,
-        )
-
-        self.test_cache_conditions = [
-            CacheConditions.CACHE_FILE_EXISTS,
-            CacheConditions.CHECKSUMS_MATCH,
-        ]
-
-        self.mock_logger = logger.GetLogger(log_dir="", mock=True)
-
-        self.mock_machine_manager = mock.Mock(spec=MachineManager)
-
-    def testDryRun(self):
-        my_label = MockLabel(
-            "test1",
-            "build",
-            "image1",
-            "autotest_dir",
-            "debug_dir",
-            "/tmp/test_benchmark_run",
-            "x86-alex",
-            "chromeos2-row1-rack4-host9.cros",
-            image_args="",
-            cache_dir="",
-            cache_only=False,
-            log_level="average",
-            compiler="gcc",
-            crosfleet=False,
-        )
-
-        logging_level = "average"
-        m = MockMachineManager("/tmp/chromeos_root", 0, logging_level, "")
-        m.AddMachine("chromeos2-row1-rack4-host9.cros")
-        bench = Benchmark(
-            "page_cycler.netsim.top_10",  # name
-            "page_cycler.netsim.top_10",  # test_name
-            "",  # test_args
-            1,  # iterations
-            False,  # rm_chroot_tmp
-            "",  # perf_args
-            suite="telemetry_Crosperf",
-        )  # suite
-        dut_conf = {
-            "cooldown_time": 5,
-            "cooldown_temp": 45,
-            "governor": "powersave",
-            "cpu_usage": "big_only",
-            "cpu_freq_pct": 80,
-        }
-        b = benchmark_run.MockBenchmarkRun(
-            "test run",
-            bench,
-            my_label,
-            1,
-            [],
-            m,
-            logger.GetLogger(),
-            logging_level,
-            "",
-            dut_conf,
-        )
-        b.cache = MockResultsCache()
-        b.suite_runner = MockSuiteRunner()
-        b.start()
-
-        # Make sure the arguments to BenchmarkRun.__init__ have not changed
-        # since the last time this test was updated:
-        args_list = [
-            "self",
-            "name",
-            "benchmark",
-            "label",
-            "iteration",
-            "cache_conditions",
-            "machine_manager",
-            "logger_to_use",
-            "log_level",
-            "share_cache",
-            "dut_config",
-        ]
-        arg_spec = inspect.getfullargspec(benchmark_run.BenchmarkRun.__init__)
-        self.assertEqual(len(arg_spec.args), len(args_list))
-        self.assertEqual(arg_spec.args, args_list)
-
-    def test_init(self):
-        # Nothing really worth testing here; just field assignments.
-        pass
-
-    def test_read_cache(self):
-        # Nothing really worth testing here, either.
-        pass
-
-    def test_run(self):
-        br = benchmark_run.BenchmarkRun(
-            "test_run",
-            self.test_benchmark,
-            self.test_label,
-            1,
-            self.test_cache_conditions,
-            self.mock_machine_manager,
-            self.mock_logger,
-            "average",
-            "",
-            {},
-        )
-
-        def MockLogOutput(msg, print_to_console=False):
-            """Helper function for test_run."""
-            del print_to_console
-            self.log_output.append(msg)
-
-        def MockLogError(msg, print_to_console=False):
-            """Helper function for test_run."""
-            del print_to_console
-            self.log_error.append(msg)
-
-        def MockRecordStatus(msg):
-            """Helper function for test_run."""
-            self.status.append(msg)
-
-        def FakeReadCache():
-            """Helper function for test_run."""
-            br.cache = mock.Mock(spec=ResultsCache)
-            self.called_ReadCache = True
-            return 0
-
-        def FakeReadCacheSucceed():
-            """Helper function for test_run."""
-            br.cache = mock.Mock(spec=ResultsCache)
-            br.result = mock.Mock(spec=Result)
-            br.result.out = "result.out stuff"
-            br.result.err = "result.err stuff"
-            br.result.retval = 0
-            self.called_ReadCache = True
-            return 0
-
-        def FakeReadCacheException():
-            """Helper function for test_run."""
-            raise RuntimeError(
-                "This is an exception test; it is supposed to happen"
-            )
-
-        def FakeAcquireMachine():
-            """Helper function for test_run."""
-            mock_machine = MockCrosMachine(
-                "chromeos1-row3-rack5-host7.cros", "chromeos", "average"
-            )
-            return mock_machine
-
-        def FakeRunTest(_machine):
-            """Helper function for test_run."""
-            mock_result = mock.Mock(spec=Result)
-            mock_result.retval = 0
-            return mock_result
-
-        def FakeRunTestFail(_machine):
-            """Helper function for test_run."""
-            mock_result = mock.Mock(spec=Result)
-            mock_result.retval = 1
-            return mock_result
-
-        def ResetTestValues():
-            """Helper function for test_run."""
-            self.log_output = []
-            self.log_error = []
-            self.status = []
-            br.result = None
-            self.called_ReadCache = False
-
-        # Assign all the fake functions to the appropriate objects.
-        br.logger().LogOutput = MockLogOutput
-        br.logger().LogError = MockLogError
-        br.timeline.Record = MockRecordStatus
-        br.ReadCache = FakeReadCache
-        br.RunTest = FakeRunTest
-        br.AcquireMachine = FakeAcquireMachine
-
-        # First test:  No cache hit, all goes well.
-        ResetTestValues()
-        br.run()
-        self.assertTrue(self.called_ReadCache)
-        self.assertEqual(
-            self.log_output,
-            [
-                "test_run: No cache hit.",
-                "Releasing machine: chromeos1-row3-rack5-host7.cros",
-                "Released machine: chromeos1-row3-rack5-host7.cros",
-            ],
-        )
-        self.assertEqual(len(self.log_error), 0)
-        self.assertEqual(self.status, ["WAITING", "SUCCEEDED"])
-
-        # Second test: No cached result found; test run was "terminated" for some
-        # reason.
-        ResetTestValues()
-        br.terminated = True
-        br.run()
-        self.assertTrue(self.called_ReadCache)
-        self.assertEqual(
-            self.log_output,
-            [
-                "test_run: No cache hit.",
-                "Releasing machine: chromeos1-row3-rack5-host7.cros",
-                "Released machine: chromeos1-row3-rack5-host7.cros",
-            ],
-        )
-        self.assertEqual(len(self.log_error), 0)
-        self.assertEqual(self.status, ["WAITING"])
-
-        # Third test.  No cached result found; RunTest failed for some reason.
-        ResetTestValues()
-        br.terminated = False
-        br.RunTest = FakeRunTestFail
-        br.run()
-        self.assertTrue(self.called_ReadCache)
-        self.assertEqual(
-            self.log_output,
-            [
-                "test_run: No cache hit.",
-                "Releasing machine: chromeos1-row3-rack5-host7.cros",
-                "Released machine: chromeos1-row3-rack5-host7.cros",
-            ],
-        )
-        self.assertEqual(len(self.log_error), 0)
-        self.assertEqual(self.status, ["WAITING", "FAILED"])
-
-        # Fourth test: ReadCache found a cached result.
-        ResetTestValues()
-        br.RunTest = FakeRunTest
-        br.ReadCache = FakeReadCacheSucceed
-        br.run()
-        self.assertTrue(self.called_ReadCache)
-        self.assertEqual(
-            self.log_output,
-            [
-                "test_run: Cache hit.",
-                "result.out stuff",
-                "Releasing machine: chromeos1-row3-rack5-host7.cros",
-                "Released machine: chromeos1-row3-rack5-host7.cros",
-            ],
-        )
-        self.assertEqual(self.log_error, ["result.err stuff"])
-        self.assertEqual(self.status, ["SUCCEEDED"])
-
-        # Fifth test: ReadCache generates an exception; does the try/finally block
-        # work?
-        ResetTestValues()
-        br.ReadCache = FakeReadCacheException
-        br.machine = FakeAcquireMachine()
-        br.run()
-        self.assertEqual(
-            self.log_error,
-            [
-                "Benchmark run: 'test_run' failed: This is an exception test; it is "
-                "supposed to happen"
-            ],
-        )
-        self.assertEqual(self.status, ["FAILED"])
-
-    def test_terminate_pass(self):
-        br = benchmark_run.BenchmarkRun(
-            "test_run",
-            self.test_benchmark,
-            self.test_label,
-            1,
-            self.test_cache_conditions,
-            self.mock_machine_manager,
-            self.mock_logger,
-            "average",
-            "",
-            {},
-        )
-
-        def GetLastEventPassed():
-            """Helper function for test_terminate_pass"""
-            return benchmark_run.STATUS_SUCCEEDED
-
-        def RecordStub(status):
-            """Helper function for test_terminate_pass"""
-            self.status = status
-
-        self.status = benchmark_run.STATUS_SUCCEEDED
-        self.assertFalse(br.terminated)
-        self.assertFalse(br.suite_runner.CommandTerminator().IsTerminated())
-
-        br.timeline.GetLastEvent = GetLastEventPassed
-        br.timeline.Record = RecordStub
-
-        br.Terminate()
-
-        self.assertTrue(br.terminated)
-        self.assertTrue(br.suite_runner.CommandTerminator().IsTerminated())
-        self.assertEqual(self.status, benchmark_run.STATUS_FAILED)
-
-    def test_terminate_fail(self):
-        br = benchmark_run.BenchmarkRun(
-            "test_run",
-            self.test_benchmark,
-            self.test_label,
-            1,
-            self.test_cache_conditions,
-            self.mock_machine_manager,
-            self.mock_logger,
-            "average",
-            "",
-            {},
-        )
-
-        def GetLastEventFailed():
-            """Helper function for test_terminate_fail"""
-            return benchmark_run.STATUS_FAILED
-
-        def RecordStub(status):
-            """Helper function for test_terminate_fail"""
-            self.status = status
-
-        self.status = benchmark_run.STATUS_SUCCEEDED
-        self.assertFalse(br.terminated)
-        self.assertFalse(br.suite_runner.CommandTerminator().IsTerminated())
-
-        br.timeline.GetLastEvent = GetLastEventFailed
-        br.timeline.Record = RecordStub
-
-        br.Terminate()
-
-        self.assertTrue(br.terminated)
-        self.assertTrue(br.suite_runner.CommandTerminator().IsTerminated())
-        self.assertEqual(self.status, benchmark_run.STATUS_SUCCEEDED)
-
-    def test_acquire_machine(self):
-        br = benchmark_run.BenchmarkRun(
-            "test_run",
-            self.test_benchmark,
-            self.test_label,
-            1,
-            self.test_cache_conditions,
-            self.mock_machine_manager,
-            self.mock_logger,
-            "average",
-            "",
-            {},
-        )
-
-        br.terminated = True
-        self.assertRaises(Exception, br.AcquireMachine)
-
-        br.terminated = False
-        mock_machine = MockCrosMachine(
-            "chromeos1-row3-rack5-host7.cros", "chromeos", "average"
-        )
-        self.mock_machine_manager.AcquireMachine.return_value = mock_machine
-
-        machine = br.AcquireMachine()
-        self.assertEqual(machine.name, "chromeos1-row3-rack5-host7.cros")
-
-    def test_get_extra_autotest_args(self):
-        br = benchmark_run.BenchmarkRun(
-            "test_run",
-            self.test_benchmark,
-            self.test_label,
-            1,
-            self.test_cache_conditions,
-            self.mock_machine_manager,
-            self.mock_logger,
-            "average",
-            "",
-            {},
-        )
-
-        def MockLogError(err_msg):
-            """Helper function for test_get_extra_autotest_args"""
-            self.err_msg = err_msg
-
-        self.mock_logger.LogError = MockLogError
-
-        result = br.GetExtraAutotestArgs()
-        self.assertEqual(result, "")
-
-        self.test_benchmark.perf_args = "record -e cycles"
-        result = br.GetExtraAutotestArgs()
-        self.assertEqual(
-            result,
-            "--profiler=custom_perf --profiler_args='perf_options=\"record -a -e "
-            "cycles\"'",
-        )
-
-        self.test_benchmark.perf_args = "record -e cycles"
-        self.test_benchmark.suite = "test_that"
-        result = br.GetExtraAutotestArgs()
-        self.assertEqual(result, "")
-        self.assertEqual(
-            self.err_msg, "Non-telemetry benchmark does not support profiler."
-        )
-
-        self.test_benchmark.perf_args = "junk args"
-        self.test_benchmark.suite = "telemetry_Crosperf"
-        self.assertRaises(Exception, br.GetExtraAutotestArgs)
-
-    @mock.patch.object(SuiteRunner, "Run")
-    @mock.patch.object(Result, "CreateFromRun")
-    def test_run_test(self, mock_result, mock_runner):
-        br = benchmark_run.BenchmarkRun(
-            "test_run",
-            self.test_benchmark,
-            self.test_label,
-            1,
-            self.test_cache_conditions,
-            self.mock_machine_manager,
-            self.mock_logger,
-            "average",
-            "",
-            {},
-        )
-
-        self.status = []
-
-        def MockRecord(status):
-            self.status.append(status)
-
-        br.timeline.Record = MockRecord
-        mock_machine = MockCrosMachine(
-            "chromeos1-row3-rack5-host7.cros", "chromeos", "average"
-        )
-        mock_runner.return_value = [0, "{'Score':100}", ""]
-
-        br.RunTest(mock_machine)
-
-        self.assertTrue(br.run_completed)
-        self.assertEqual(
-            self.status,
-            [benchmark_run.STATUS_IMAGING, benchmark_run.STATUS_RUNNING],
-        )
-
-        self.assertEqual(br.machine_manager.ImageMachine.call_count, 1)
-        br.machine_manager.ImageMachine.assert_called_with(
-            mock_machine, self.test_label
-        )
-        self.assertEqual(mock_runner.call_count, 1)
-        mock_runner.assert_called_with(
-            mock_machine, br.label, br.benchmark, "", br.profiler_args
-        )
-
-        self.assertEqual(mock_result.call_count, 1)
-        mock_result.assert_called_with(
-            self.mock_logger,
-            "average",
-            self.test_label,
-            None,
-            "{'Score':100}",
-            "",
-            0,
-            "page_cycler.netsim.top_10",
-            "telemetry_Crosperf",
-            "",
-        )
-
-    def test_set_cache_conditions(self):
-        br = benchmark_run.BenchmarkRun(
-            "test_run",
-            self.test_benchmark,
-            self.test_label,
-            1,
-            self.test_cache_conditions,
-            self.mock_machine_manager,
-            self.mock_logger,
-            "average",
-            "",
-            {},
-        )
-
-        phony_cache_conditions = [123, 456, True, False]
-
-        self.assertEqual(br.cache_conditions, self.test_cache_conditions)
-
-        br.SetCacheConditions(phony_cache_conditions)
-        self.assertEqual(br.cache_conditions, phony_cache_conditions)
-
-        br.SetCacheConditions(self.test_cache_conditions)
-        self.assertEqual(br.cache_conditions, self.test_cache_conditions)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/benchmark_unittest.py b/crosperf/benchmark_unittest.py
deleted file mode 100755
index bb23bdbb..00000000
--- a/crosperf/benchmark_unittest.py
+++ /dev/null
@@ -1,84 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Copyright 2014 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unit tests for the Crosperf Benchmark class."""
-
-
-import inspect
-import unittest
-
-from benchmark import Benchmark
-
-
-class BenchmarkTestCase(unittest.TestCase):
-    """Individual tests for the Benchmark class."""
-
-    def test_benchmark(self):
-        # Test creating a benchmark with all the fields filled out.
-        b1 = Benchmark(
-            "b1_test",  # name
-            "octane",  # test_name
-            "",  # test_args
-            3,  # iterations
-            False,  # rm_chroot_tmp
-            "record -e cycles",  # perf_args
-            "telemetry_Crosperf",  # suite
-            True,
-        )  # show_all_results
-        self.assertTrue(b1.suite, "telemetry_Crosperf")
-
-        # Test creating a benchmark field with default fields left out.
-        b2 = Benchmark(
-            "b2_test",  # name
-            "octane",  # test_name
-            "",  # test_args
-            3,  # iterations
-            False,  # rm_chroot_tmp
-            "record -e cycles",
-        )  # perf_args
-        self.assertEqual(b2.suite, "")
-        self.assertFalse(b2.show_all_results)
-
-        # Test explicitly creating 'suite=Telemetry' and 'show_all_results=False"
-        # and see what happens.
-        b3 = Benchmark(
-            "b3_test",  # name
-            "octane",  # test_name
-            "",  # test_args
-            3,  # iterations
-            False,  # rm_chroot_tmp
-            "record -e cycles",  # perf_args
-            "telemetry",  # suite
-            False,
-        )  # show_all_results
-        self.assertTrue(b3.show_all_results)
-
-        # Check to see if the args to Benchmark have changed since the last time
-        # this test was updated.
-        args_list = [
-            "self",
-            "name",
-            "test_name",
-            "test_args",
-            "iterations",
-            "rm_chroot_tmp",
-            "perf_args",
-            "suite",
-            "show_all_results",
-            "retries",
-            "run_local",
-            "cwp_dso",
-            "weight",
-        ]
-        arg_spec = inspect.getfullargspec(Benchmark.__init__)
-        self.assertEqual(len(arg_spec.args), len(args_list))
-        for arg in args_list:
-            self.assertIn(arg, arg_spec.args)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/column_chart.py b/crosperf/column_chart.py
deleted file mode 100644
index 6ed99bf0..00000000
--- a/crosperf/column_chart.py
+++ /dev/null
@@ -1,69 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2011 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Module to draw column chart."""
-
-
-class ColumnChart(object):
-    """class to draw column chart."""
-
-    def __init__(self, title, width, height):
-        self.title = title
-        self.chart_div = "".join(t for t in title if t.isalnum())
-        self.width = width
-        self.height = height
-        self.columns = []
-        self.rows = []
-        self.series = []
-
-    def AddSeries(self, column_name, series_type, color):
-        for i in range(len(self.columns)):
-            if column_name == self.columns[i][1]:
-                self.series.append((i - 1, series_type, color))
-                break
-
-    def AddColumn(self, name, column_type):
-        self.columns.append((column_type, name))
-
-    def AddRow(self, row):
-        self.rows.append(row)
-
-    def GetJavascript(self):
-        res = "var data = new google.visualization.DataTable();\n"
-        for column in self.columns:
-            res += "data.addColumn('%s', '%s');\n" % column
-        res += "data.addRows(%s);\n" % len(self.rows)
-        for row in range(len(self.rows)):
-            for column in range(len(self.columns)):
-                val = self.rows[row][column]
-                if isinstance(val, str):
-                    val = "'%s'" % val
-                res += "data.setValue(%s, %s, %s);\n" % (row, column, val)
-
-        series_javascript = ""
-        for series in self.series:
-            series_javascript += "%s: {type: '%s', color: '%s'}, " % series
-
-        chart_add_javascript = """
-var chart_%s = new google.visualization.ComboChart(
-  document.getElementById('%s'));
-chart_%s.draw(data, {width: %s, height: %s, title: '%s', legend: 'none',
-  seriesType: "bars", lineWidth: 0, pointSize: 5, series: {%s},
-  vAxis: {minValue: 0}})
-"""
-
-        res += chart_add_javascript % (
-            self.chart_div,
-            self.chart_div,
-            self.chart_div,
-            self.width,
-            self.height,
-            self.title,
-            series_javascript,
-        )
-        return res
-
-    def GetDiv(self):
-        return "<div id='%s' class='chart'></div>" % self.chart_div
diff --git a/crosperf/compare_machines.py b/crosperf/compare_machines.py
deleted file mode 100644
index 756753a2..00000000
--- a/crosperf/compare_machines.py
+++ /dev/null
@@ -1,71 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2014 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Module to compare two machines."""
-
-
-import argparse
-import os.path
-import sys
-
-from machine_manager import CrosMachine
-
-
-def PrintUsage(msg):
-    print(msg)
-    print("Usage: ")
-    print(
-        "\n compare_machines.py --chromeos_root=/path/to/chroot/ "
-        "machine1 machine2 ..."
-    )
-
-
-def Main(argv):
-
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "--chromeos_root",
-        default="/path/to/chromeos",
-        dest="chromeos_root",
-        help="ChromeOS root checkout directory",
-    )
-    parser.add_argument("remotes", nargs=argparse.REMAINDER)
-
-    options = parser.parse_args(argv)
-
-    machine_list = options.remotes
-    if len(machine_list) < 2:
-        PrintUsage("ERROR: Must specify at least two machines.")
-        return 1
-    elif not os.path.exists(options.chromeos_root):
-        PrintUsage(
-            "Error: chromeos_root does not exist %s" % options.chromeos_root
-        )
-        return 1
-
-    chroot = options.chromeos_root
-    cros_machines = []
-    test_machine_checksum = None
-    for m in machine_list:
-        cm = CrosMachine(m, chroot, "average")
-        cros_machines = cros_machines + [cm]
-        test_machine_checksum = cm.machine_checksum
-
-    ret = 0
-    for cm in cros_machines:
-        print("checksum for %s : %s" % (cm.name, cm.machine_checksum))
-        if cm.machine_checksum != test_machine_checksum:
-            ret = 1
-            print("Machine checksums do not all match")
-
-    if ret == 0:
-        print("Machines all match.")
-
-    return ret
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv[1:])
-    sys.exit(retval)
diff --git a/crosperf/config.py b/crosperf/config.py
deleted file mode 100644
index c2a7fe5d..00000000
--- a/crosperf/config.py
+++ /dev/null
@@ -1,15 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2011 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""A configure file."""
-config = {}
-
-
-def GetConfig(key):
-    return config.get(key)
-
-
-def AddConfig(key, value):
-    config[key] = value
diff --git a/crosperf/config_unittest.py b/crosperf/config_unittest.py
deleted file mode 100755
index fdff7ea6..00000000
--- a/crosperf/config_unittest.py
+++ /dev/null
@@ -1,52 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2014 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unit tests for config.py"""
-
-
-import unittest
-
-import config
-
-
-class ConfigTestCase(unittest.TestCase):
-    """Class for the config unit tests."""
-
-    def test_config(self):
-        # Verify that config exists, that it's a dictionary, and that it's
-        # empty.
-        self.assertTrue(isinstance(config.config, dict))
-        self.assertEqual(len(config.config), 0)
-
-        # Verify that attempting to get a non-existant key out of the
-        # dictionary returns None.
-        self.assertIsNone(config.GetConfig("rabbit"))
-        self.assertIsNone(config.GetConfig("key1"))
-
-        config.AddConfig("key1", 16)
-        config.AddConfig("key2", 32)
-        config.AddConfig("key3", "third value")
-
-        # Verify that after 3 calls to AddConfig we have 3 values in the
-        # dictionary.
-        self.assertEqual(len(config.config), 3)
-
-        # Verify that GetConfig works and gets the expected values.
-        self.assertIs(config.GetConfig("key2"), 32)
-        self.assertIs(config.GetConfig("key3"), "third value")
-        self.assertIs(config.GetConfig("key1"), 16)
-
-        # Re-set config.
-        config.config.clear()
-
-        # Verify that config exists, that it's a dictionary, and that it's
-        # empty.
-        self.assertTrue(isinstance(config.config, dict))
-        self.assertEqual(len(config.config), 0)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/crosperf b/crosperf/crosperf
deleted file mode 100755
index 9a7bde0a..00000000
--- a/crosperf/crosperf
+++ /dev/null
@@ -1,7 +0,0 @@
-#!/bin/bash
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-PYTHONPATH=$(dirname "$0")/..:${PYTHONPATH} exec \
-python3 "$(dirname "$0")/crosperf.py" "$@"
diff --git a/crosperf/crosperf.py b/crosperf/crosperf.py
deleted file mode 100755
index aace2c80..00000000
--- a/crosperf/crosperf.py
+++ /dev/null
@@ -1,157 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2011 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""The driver script for running performance benchmarks on ChromeOS."""
-
-
-import argparse
-import atexit
-import os
-import signal
-import sys
-
-# This import causes pylint to warn about "No name 'logger' in module
-# 'cros_utils'". I do not understand why. The import works fine in python.
-# pylint: disable=no-name-in-module
-from cros_utils import logger
-from experiment_factory import ExperimentFactory
-from experiment_file import ExperimentFile
-from experiment_runner import ExperimentRunner
-from experiment_runner import MockExperimentRunner
-from settings_factory import GlobalSettings
-import test_flag
-
-
-HAS_FAILURE = 1
-ALL_FAILED = 2
-
-
-def SetupParserOptions(parser):
-    """Add all options to the parser."""
-    parser.add_argument(
-        "--dry_run",
-        dest="dry_run",
-        help=("Parse the experiment file and " "show what will be done"),
-        action="store_true",
-        default=False,
-    )
-    # Allow each of the global fields to be overridden by passing in
-    # options. Add each global field as an option.
-    option_settings = GlobalSettings("")
-    for field_name in option_settings.fields:
-        field = option_settings.fields[field_name]
-        parser.add_argument(
-            "--%s" % field.name,
-            dest=field.name,
-            help=field.description,
-            action="store",
-        )
-
-
-def ConvertOptionsToSettings(options):
-    """Convert options passed in into global settings."""
-    option_settings = GlobalSettings("option_settings")
-    for option_name in options.__dict__:
-        if (
-            options.__dict__[option_name] is not None
-            and option_name in option_settings.fields
-        ):
-            option_settings.SetField(option_name, options.__dict__[option_name])
-    return option_settings
-
-
-def Cleanup(experiment):
-    """Handler function which is registered to the atexit handler."""
-    experiment.Cleanup()
-
-
-def CallExitHandler(signum, _):
-    """Signal handler that transforms a signal into a call to exit.
-
-    This is useful because functionality registered by "atexit" will
-    be called. It also means you can "catch" the signal by catching
-    the SystemExit exception.
-    """
-    sys.exit(128 + signum)
-
-
-def RunCrosperf(argv):
-    parser = argparse.ArgumentParser()
-
-    parser.add_argument(
-        "--noschedv2",
-        dest="noschedv2",
-        default=False,
-        action="store_true",
-        help=("Do not use new scheduler. " "Use original scheduler instead."),
-    )
-    parser.add_argument(
-        "-l",
-        "--log_dir",
-        dest="log_dir",
-        default="",
-        help="The log_dir, default is under <crosperf_logs>/logs",
-    )
-
-    SetupParserOptions(parser)
-    options, args = parser.parse_known_args(argv)
-
-    # Convert the relevant options that are passed in into a settings
-    # object which will override settings in the experiment file.
-    option_settings = ConvertOptionsToSettings(options)
-    log_dir = os.path.abspath(os.path.expanduser(options.log_dir))
-    logger.GetLogger(log_dir)
-
-    if len(args) == 2:
-        experiment_filename = args[1]
-    else:
-        parser.error("Invalid number arguments.")
-
-    working_directory = os.getcwd()
-    if options.dry_run:
-        test_flag.SetTestMode(True)
-
-    experiment_file = ExperimentFile(
-        open(experiment_filename, encoding="utf-8"), option_settings
-    )
-    if not experiment_file.GetGlobalSettings().GetField("name"):
-        experiment_name = os.path.basename(experiment_filename)
-        experiment_file.GetGlobalSettings().SetField("name", experiment_name)
-    experiment = ExperimentFactory().GetExperiment(
-        experiment_file, working_directory, log_dir
-    )
-
-    json_report = experiment_file.GetGlobalSettings().GetField("json_report")
-
-    signal.signal(signal.SIGTERM, CallExitHandler)
-    atexit.register(Cleanup, experiment)
-
-    if options.dry_run:
-        runner = MockExperimentRunner(experiment, json_report)
-    else:
-        runner = ExperimentRunner(
-            experiment, json_report, using_schedv2=(not options.noschedv2)
-        )
-
-    ret = runner.Run()
-    if ret == HAS_FAILURE:
-        raise RuntimeError("One or more benchmarks failed.")
-    if ret == ALL_FAILED:
-        raise RuntimeError("All benchmarks failed to run.")
-
-
-def Main(argv):
-    try:
-        RunCrosperf(argv)
-    except Exception:
-        # Flush buffers before exiting to avoid out of order printing
-        sys.stdout.flush()
-        # Raise exception prints out traceback
-        raise
-
-
-if __name__ == "__main__":
-    Main(sys.argv)
diff --git a/crosperf/crosperf_autolock.py b/crosperf/crosperf_autolock.py
deleted file mode 100755
index 011f01e3..00000000
--- a/crosperf/crosperf_autolock.py
+++ /dev/null
@@ -1,313 +0,0 @@
-#!/usr/bin/env python3
-
-# Copyright 2021 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Wrapper script to automatically lock devices for crosperf."""
-
-import argparse
-import contextlib
-import dataclasses
-import json
-import os
-import subprocess
-import sys
-from typing import Any, Dict, List, Optional, Tuple
-
-
-# Have to do sys.path hackery because crosperf relies on PYTHONPATH
-# modifications.
-PARENT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
-sys.path.append(PARENT_DIR)
-
-
-def main(sys_args: List[str]) -> Optional[str]:
-    """Run crosperf_autolock. Returns error msg or None"""
-    args, leftover_args = parse_args(sys_args)
-    fleet_params = [
-        CrosfleetParams(
-            board=args.board, pool=args.pool, lease_time=args.lease_time
-        )
-        for _ in range(args.num_leases)
-    ]
-    if not fleet_params:
-        return (
-            "No board names identified. If you want to use"
-            " a known host, just use crosperf directly."
-        )
-    try:
-        _run_crosperf(fleet_params, args.dut_lock_timeout, leftover_args)
-    except BoardLockError as e:
-        _eprint("ERROR:", e)
-        _eprint('May need to login to crosfleet? Run "crosfleet login"')
-        _eprint(
-            "The leases may also be successful later on. "
-            'Check with "crosfleet dut leases"'
-        )
-        return "crosperf_autolock failed"
-    except BoardReleaseError as e:
-        _eprint("ERROR:", e)
-        _eprint('May need to re-run "crosfleet dut abandon"')
-        return "crosperf_autolock failed"
-    return None
-
-
-def parse_args(args: List[str]) -> Tuple[Any, List]:
-    """Parse the CLI arguments."""
-    parser = argparse.ArgumentParser(
-        "crosperf_autolock",
-        description="Wrapper around crosperf"
-        " to autolock DUTs from crosfleet.",
-        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
-    )
-    parser.add_argument(
-        "--board",
-        type=str,
-        help="Space or comma separated list of boards to lock",
-        required=True,
-        default=argparse.SUPPRESS,
-    )
-    parser.add_argument(
-        "--num-leases",
-        type=int,
-        help="Number of boards to lock.",
-        metavar="NUM",
-        default=1,
-    )
-    parser.add_argument(
-        "--pool", type=str, help="Pool to pull from.", default="DUT_POOL_QUOTA"
-    )
-    parser.add_argument(
-        "--dut-lock-timeout",
-        type=float,
-        metavar="SEC",
-        help="Number of seconds we want to try to lease a board"
-        " from crosfleet. This option does NOT change the"
-        " lease length.",
-        default=600,
-    )
-    parser.add_argument(
-        "--lease-time",
-        type=int,
-        metavar="MIN",
-        help="Number of minutes to lock the board. Max is 1440.",
-        default=1440,
-    )
-    parser.epilog = (
-        "For more detailed flags, you have to read the args taken by the"
-        " crosperf executable. Args are passed transparently to crosperf."
-    )
-    return parser.parse_known_args(args)
-
-
-class BoardLockError(Exception):
-    """Error to indicate failure to lock a board."""
-
-    def __init__(self, msg: str):
-        self.msg = "BoardLockError: " + msg
-        super().__init__(self.msg)
-
-
-class BoardReleaseError(Exception):
-    """Error to indicate failure to release a board."""
-
-    def __init__(self, msg: str):
-        self.msg = "BoardReleaseError: " + msg
-        super().__init__(self.msg)
-
-
-@dataclasses.dataclass(frozen=True)
-class CrosfleetParams:
-    """Dataclass to hold all crosfleet parameterizations."""
-
-    board: str
-    pool: str
-    lease_time: int
-
-
-def _eprint(*msg, **kwargs):
-    print(*msg, file=sys.stderr, **kwargs)
-
-
-def _run_crosperf(
-    crosfleet_params: List[CrosfleetParams],
-    lock_timeout: float,
-    leftover_args: List[str],
-):
-    """Autolock devices and run crosperf with leftover arguments.
-
-    Raises:
-      BoardLockError: When board was unable to be locked.
-      BoardReleaseError: When board was unable to be released.
-    """
-    if not crosfleet_params:
-        raise ValueError("No crosfleet params given; cannot call crosfleet.")
-
-    # We'll assume all the boards are the same type, which seems to be the case
-    # in experiments that actually get used.
-    passed_board_arg = crosfleet_params[0].board
-    with contextlib.ExitStack() as stack:
-        dut_hostnames = []
-        for param in crosfleet_params:
-            print(
-                f"Sent lock request for {param.board} for {param.lease_time} minutes"
-                '\nIf this fails, you may need to run "crosfleet dut abandon <...>"'
-            )
-            # May raise BoardLockError, abandoning previous DUTs.
-            dut_hostname = stack.enter_context(
-                crosfleet_machine_ctx(
-                    param.board,
-                    param.lease_time,
-                    lock_timeout,
-                    {"label-pool": param.pool},
-                )
-            )
-            if dut_hostname:
-                print(f"Locked {param.board} machine: {dut_hostname}")
-                dut_hostnames.append(dut_hostname)
-
-        # We import crosperf late, because this import is extremely slow.
-        # We don't want the user to wait several seconds just to get
-        # help info.
-        import crosperf
-
-        for dut_hostname in dut_hostnames:
-            crosperf.Main(
-                [
-                    sys.argv[0],
-                    "--no_lock",
-                    "True",
-                    "--remote",
-                    dut_hostname,
-                    "--board",
-                    passed_board_arg,
-                ]
-                + leftover_args
-            )
-
-
-@contextlib.contextmanager
-def crosfleet_machine_ctx(
-    board: str,
-    lease_minutes: int,
-    lock_timeout: float,
-    dims: Dict[str, Any],
-    abandon_timeout: float = 120.0,
-) -> Any:
-    """Acquire dut from crosfleet, and release once it leaves the context.
-
-    Args:
-      board: Board type to lease.
-      lease_minutes: Length of lease, in minutes.
-      lock_timeout: How long to wait for a lock until quitting.
-      dims: Dictionary of dimension arguments to pass to crosfleet's '-dims'
-      abandon_timeout: How long to wait for releasing until quitting.
-
-    Yields:
-      A string representing the crosfleet DUT hostname.
-
-    Raises:
-      BoardLockError: When board was unable to be locked.
-      BoardReleaseError: When board was unable to be released.
-    """
-    # This lock may raise an exception, but if it does, we can't release
-    # the DUT anyways as we won't have the dut_hostname.
-    dut_hostname = crosfleet_autolock(board, lease_minutes, dims, lock_timeout)
-    try:
-        yield dut_hostname
-    finally:
-        if dut_hostname:
-            crosfleet_release(dut_hostname, abandon_timeout)
-
-
-def crosfleet_autolock(
-    board: str, lease_minutes: int, dims: Dict[str, Any], timeout_sec: float
-) -> str:
-    """Lock a device using crosfleet, paramaterized by the board type.
-
-    Args:
-      board: Board of the DUT we want to lock.
-      lease_minutes: Number of minutes we're trying to lease the DUT for.
-      dims: Dictionary of dimension arguments to pass to crosfleet's '-dims'
-      timeout_sec: Number of seconds to try to lease the DUT. Default 120s.
-
-    Returns:
-      The hostname of the board, or empty string if it couldn't be parsed.
-
-    Raises:
-      BoardLockError: When board was unable to be locked.
-    """
-    crosfleet_cmd_args = [
-        "crosfleet",
-        "dut",
-        "lease",
-        "-json",
-        '-reason="crosperf autolock"',
-        f"-board={board}",
-        f"-minutes={lease_minutes}",
-    ]
-    if dims:
-        dims_arg = ",".join(f"{k}={v}" for k, v in dims.items())
-        crosfleet_cmd_args.extend(["-dims", f"{dims_arg}"])
-
-    try:
-        output = subprocess.check_output(
-            crosfleet_cmd_args, timeout=timeout_sec, encoding="utf-8"
-        )
-    except subprocess.CalledProcessError as e:
-        raise BoardLockError(
-            f"crosfleet dut lease failed with exit code: {e.returncode}"
-        )
-    except subprocess.TimeoutExpired as e:
-        raise BoardLockError(
-            f"crosfleet dut lease timed out after {timeout_sec}s;"
-            " please abandon the dut manually."
-        )
-
-    try:
-        json_obj = json.loads(output)
-        dut_hostname = json_obj["DUT"]["Hostname"]
-        if not isinstance(dut_hostname, str):
-            raise TypeError("dut_hostname was not a string")
-    except (json.JSONDecodeError, IndexError, KeyError, TypeError) as e:
-        raise BoardLockError(
-            f"crosfleet dut lease output was parsed incorrectly: {e!r};"
-            f" observed output was {output}"
-        )
-    return _maybe_append_suffix(dut_hostname)
-
-
-def crosfleet_release(dut_hostname: str, timeout_sec: float = 120.0):
-    """Release a crosfleet device.
-
-    Consider using the context managed crosfleet_machine_context
-
-    Args:
-      dut_hostname: Name of the device we want to release.
-      timeout_sec: Number of seconds to try to release the DUT. Default is 120s.
-
-    Raises:
-      BoardReleaseError: Potentially failed to abandon the lease.
-    """
-    crosfleet_cmd_args = [
-        "crosfleet",
-        "dut",
-        "abandon",
-        dut_hostname,
-    ]
-    exit_code = subprocess.call(crosfleet_cmd_args, timeout=timeout_sec)
-    if exit_code != 0:
-        raise BoardReleaseError(
-            f'"crosfleet dut abandon" had exit code {exit_code}'
-        )
-
-
-def _maybe_append_suffix(hostname: str) -> str:
-    if hostname.endswith(".cros") or ".cros." in hostname:
-        return hostname
-    return hostname + ".cros"
-
-
-if __name__ == "__main__":
-    sys.exit(main(sys.argv[1:]))
diff --git a/crosperf/crosperf_unittest.py b/crosperf/crosperf_unittest.py
deleted file mode 100755
index bbcb1712..00000000
--- a/crosperf/crosperf_unittest.py
+++ /dev/null
@@ -1,90 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Copyright 2014 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unittest for crosperf."""
-
-
-import argparse
-import io
-import tempfile
-import unittest
-import unittest.mock as mock
-
-import crosperf
-import experiment_file
-import settings_factory
-
-
-EXPERIMENT_FILE_1 = """
-  board: x86-alex
-  remote: chromeos-alex3
-  locks_dir: /tmp
-  perf_args: record -a -e cycles
-  benchmark: PageCycler {
-    iterations: 3
-  }
-
-  image1 {
-    chromeos_image: /usr/local/google/cros_image1.bin
-  }
-
-  image2 {
-    remote: chromeos-lumpy1
-    chromeos_image: /usr/local/google/cros_image2.bin
-  }
-  """
-
-
-class CrosperfTest(unittest.TestCase):
-    """Crosperf test class."""
-
-    def setUp(self):
-        input_file = io.StringIO(EXPERIMENT_FILE_1)
-        self.exp_file = experiment_file.ExperimentFile(input_file)
-
-    def testDryRun(self):
-        with tempfile.NamedTemporaryFile("w", encoding="utf-8") as f:
-            f.write(EXPERIMENT_FILE_1)
-            f.flush()
-            crosperf.Main(["", f.name, "--dry_run"])
-
-    def testConvertOptionsToSettings(self):
-        parser = argparse.ArgumentParser()
-        parser.add_argument(
-            "-l",
-            "--log_dir",
-            dest="log_dir",
-            default="",
-            help="The log_dir, default is under " "<crosperf_logs>/logs",
-        )
-        crosperf.SetupParserOptions(parser)
-        argv = ["crosperf/crosperf.py", "temp.exp", "--rerun=True"]
-        options, _ = parser.parse_known_args(argv)
-        settings = crosperf.ConvertOptionsToSettings(options)
-        self.assertIsNotNone(settings)
-        self.assertIsInstance(settings, settings_factory.GlobalSettings)
-        self.assertEqual(len(settings.fields), 42)
-        self.assertTrue(settings.GetField("rerun"))
-        argv = ["crosperf/crosperf.py", "temp.exp"]
-        options, _ = parser.parse_known_args(argv)
-        settings = crosperf.ConvertOptionsToSettings(options)
-        self.assertFalse(settings.GetField("rerun"))
-
-    def testExceptionPrintTraceback(self):
-        """Test the main function can print traceback in exception."""
-
-        def mock_RunCrosperf(*_args, **_kwargs):
-            return 10 / 0
-
-        with mock.patch("crosperf.RunCrosperf", new=mock_RunCrosperf):
-            with self.assertRaises(ZeroDivisionError) as context:
-                crosperf.Main([])
-            self.assertEqual("division by zero", str(context.exception))
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/default-telemetry-results.json b/crosperf/default-telemetry-results.json
deleted file mode 100644
index 3dd22f86..00000000
--- a/crosperf/default-telemetry-results.json
+++ /dev/null
@@ -1,176 +0,0 @@
-{
-  "peacekeeper.html": [
-    "Total__Score",
-    "workerContrast01__Score",
-    "workerContrast02__Score"
-  ],
-  "page_cycler_v2.intl_hi_ru": [
-    "cold_times__page_load_time",
-    "warm_times__page_load_time",
-    "pcv1-warm@@timeToOnload_avg__summary",
-    "pcv1-cold@@timeToOnload_avg__summary",
-    "cold@@timeToOnload_avg__summary",
-    "warm@@timeToOnload_avg__summary"
-  ],
-  "smoothness.tough_webgl_cases": [
-    "percentage_smooth__percentage_smooth",
-    "percentage_smooth__summary"
-  ],
-  "loading.desktop@@typical": [
-    "timeToFirstContentfulPaint__cache_temperature:cold",
-    "timeToFirstContentfulPaint__cache_temperature:warm"
-  ],
-  "page_cycler_v2.intl_es_fr_pt-BR": [
-    "cold_times__page_load_time",
-    "warm_times__page_load_time",
-    "pcv1-warm@@timeToOnload_avg__summary",
-    "pcv1-cold@@timeToOnload_avg__summary",
-    "cold@@timeToOnload_avg__summary",
-    "warm@@timeToOnload_avg__summary"
-  ],
-  "browsermark": [
-    "Score__Score"
-  ],
-  "smoothness.top_25": [
-    "frame_times__frame_times",
-    "mean_frame_time__mean_frame_time"
-  ],
-  "page_cycler_v2.morejs": [
-    "warm_times__page_load_time",
-    "cold_times__page_load_time",
-    "pcv1-warm@@timeToOnload_avg__summary",
-    "pcv1-cold@@timeToOnload_avg__summary",
-    "cold@@timeToOnload_avg__summary",
-    "warm@@timeToOnload_avg__summary"
-  ],
-  "page_cycler_v2.dhtml": [
-    "warm_times__page_load_time",
-    "cold_times__page_load_time",
-    "pcv1-warm@@timeToOnload_avg__summary",
-    "pcv1-cold@@timeToOnload_avg__summary",
-    "cold@@timeToOnload_avg__summary",
-    "warm@@timeToOnload_avg__summary"
-  ],
-  "page_cycler_v2.bloat": [
-    "warm_times__page_load_time",
-    "cold_times__page_load_time",
-    "pcv1-warm@@timeToOnload_avg__summary",
-    "pcv1-cold@@timeToOnload_avg__summary",
-    "cold@@timeToOnload_avg__summary",
-    "warm@@timeToOnload_avg__summary"
-  ],
-  "page_cycler_v2.moz": [
-    "warm_times__page_load_time",
-    "cold_times__page_load_time",
-    "pcv1-warm@@timeToOnload_avg__summary",
-    "pcv1-cold@@timeToOnload_avg__summary",
-    "cold@@timeToOnload_avg__summary",
-    "warm@@timeToOnload_avg__summary"
-  ],
-  "speedometer": [
-    "RunsPerMinute",
-    "Total"
-  ],
-  "speedometer2": [
-    "RunsPerMinute",
-    "Total"
-  ],
-  "octane": [
-    "Total.Score"
-  ],
-  "jsgamebench": [
-    "Score__Score"
-  ],
-  "page_cycler_v2.indexed_db.basic_insert": [
-    "warm_times__page_load_time",
-    "cold_times__page_load_time",
-    "pcv1-warm@@timeToOnload_avg__summary",
-    "pcv1-cold@@timeToOnload_avg__summary",
-    "cold@@timeToOnload_avg__summary",
-    "warm@@timeToOnload_avg__summary"
-  ],
-  "spaceport": [
-    "Score__Score"
-  ],
-  "page_cycler_v2.netsim.top_10": [
-    "cold_times__page_load_time",
-    "warm_times__page_load_time",
-    "pcv1-warm@@timeToOnload_avg__summary",
-    "pcv1-cold@@timeToOnload_avg__summary",
-    "cold@@timeToOnload_avg__summary",
-    "warm@@timeToOnload_avg__summary"
-  ],
-  "page_cycler_v2.typical_25": [
-    "warm_times-page_load_time__warm_times-page_load_time",
-    "cold_times-page_load_time__cold_times-page_load_time",
-    "pcv1-warm@@timeToOnload_avg__summary",
-    "pcv1-cold@@timeToOnload_avg__summary",
-    "cold@@timeToOnload_avg__summary",
-    "warm@@timeToOnload_avg__summary"
-  ],
-  "page_cycler_v2.intl_ar_fa_he": [
-    "warm_times__page_load_time",
-    "cold_times__page_load_time",
-    "pcv1-warm@@timeToOnload_avg__summary",
-    "pcv1-cold@@timeToOnload_avg__summary",
-    "cold@@timeToOnload_avg__summary",
-    "warm@@timeToOnload_avg__summary"
-  ],
-  "page_cycler_v2.intl_ja_zh": [
-    "warm_times__page_load_time",
-    "cold_times__page_load_time",
-    "pcv1-warm@@timeToOnload_avg__summary",
-    "pcv1-cold@@timeToOnload_avg__summary",
-    "cold@@timeToOnload_avg__summary",
-    "warm@@timeToOnload_avg__summary"
-  ],
-  "graphics_WebGLAquarium": [
-    "avg_fps_1000_fishes",
-    "avg_fps_1000_fishes__summary"
-  ],
-  "page_cycler_v2.intl_ko_th_vi": [
-    "warm_times__page_load_time",
-    "cold_times__page_load_time",
-    "pcv1-warm@@timeToOnload_avg__summary",
-    "pcv1-cold@@timeToOnload_avg__summary",
-    "cold@@timeToOnload_avg__summary",
-    "warm@@timeToOnload_avg__summary"
-  ],
-  "canvasmark": [
-    "Score__Score"
-  ],
-  "page_cycler_v2.tough_layout_cases": [
-    "warm_times__page_load_time",
-    "cold_times__page_load_time",
-    "pcv1-warm@@timeToOnload_avg__summary",
-    "pcv1-cold@@timeToOnload_avg__summary",
-    "cold@@timeToOnload_avg__summary",
-    "warm@@timeToOnload_avg__summary"
-  ],
-  "kraken": [
-    "Total"
-  ],
-  "jetstream": [
-    "Score"
-  ],
-  "jetstream2": [
-    "Score"
-  ],
-  "cros_ui_smoothness": [
-    "ui_percentage_smooth"
-  ],
-  "rendering.desktop": [
-    "Event.Latency.ScrollUpdate.Touch.TimeToScrollUpdateSwapBegin4_avg"
-  ],
-  "rendering.desktop@@aquarium$": [
-    "avg_surface_fps",
-    "exp_avg_surface_fps"
-  ],
-  "rendering.desktop@@aquarium_20k$": [
-    "avg_surface_fps",
-    "exp_avg_surface_fps"
-  ],
-  "platform.ReportDiskUsage": [
-    "bytes_rootfs_prod__summary"
-  ]
-}
diff --git a/crosperf/default_remotes b/crosperf/default_remotes
deleted file mode 100644
index 80563664..00000000
--- a/crosperf/default_remotes
+++ /dev/null
@@ -1,6 +0,0 @@
-bob           : chromeos8-row12-rack16-host2
-chell         : chromeos6-row16-rack5-host6    chromeos6-row16-rack5-host7
-coral         : chromeos6-row5-rack6-host1     chromeos6-row5-rack6-host3    chromeos6-row5-rack6-host5
-elm           : chromeos6-row14-rack15-host21
-nautilus      : chromeos6-row5-rack10-host1    chromeos6-row5-rack10-host3
-snappy        : chromeos8-row12-rack17-host1        chromeos8-row12-rack17-host2
diff --git a/crosperf/download_images.py b/crosperf/download_images.py
deleted file mode 100644
index 38d5d68a..00000000
--- a/crosperf/download_images.py
+++ /dev/null
@@ -1,412 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2014-2015 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Download images from Cloud Storage."""
-
-
-import ast
-import os
-import shlex
-
-from cros_utils import command_executer
-from cros_utils import misc
-import test_flag
-
-
-GS_UTIL = "src/chromium/depot_tools/gsutil.py"
-
-
-class MissingImage(Exception):
-    """Raised when the requested image does not exist in gs://"""
-
-
-class MissingFile(Exception):
-    """Raised when the requested file does not exist in gs://"""
-
-
-class RunCommandExceptionHandler(object):
-    """Handle Exceptions from calls to RunCommand"""
-
-    def __init__(self, logger_to_use, log_level, cmd_exec, command):
-        self.logger = logger_to_use
-        self.log_level = log_level
-        self.ce = cmd_exec
-        self.cleanup_command = command
-
-    def HandleException(self, _, e):
-        # Exception handler, Run specified command
-        if self.log_level != "verbose" and self.cleanup_command is not None:
-            self.logger.LogOutput("CMD: %s" % self.cleanup_command)
-        if self.cleanup_command is not None:
-            _ = self.ce.RunCommand(self.cleanup_command)
-        # Raise exception again
-        raise e
-
-
-class ImageDownloader(object):
-    """Download images from Cloud Storage."""
-
-    def __init__(self, logger_to_use=None, log_level="verbose", cmd_exec=None):
-        self._logger = logger_to_use
-        self.log_level = log_level
-        self._ce = cmd_exec or command_executer.GetCommandExecuter(
-            self._logger, log_level=self.log_level
-        )
-
-    def GetBuildID(self, chromeos_root, xbuddy_label):
-        # Get the translation of the xbuddy_label into the real Google Storage
-        # image name.
-        command = (
-            "cd /mnt/host/source/src/third_party/toolchain-utils/crosperf; "
-            "./translate_xbuddy.py '%s'" % xbuddy_label
-        )
-        _, build_id_tuple_str, _ = self._ce.ChrootRunCommandWOutput(
-            chromeos_root, command
-        )
-        if not build_id_tuple_str:
-            raise MissingImage("Unable to find image for '%s'" % xbuddy_label)
-
-        build_id_tuple = ast.literal_eval(build_id_tuple_str)
-        build_id = build_id_tuple[0]
-
-        return build_id
-
-    def DownloadImage(self, chromeos_root, build_id, image_name):
-        if self.log_level == "average":
-            self._logger.LogOutput(
-                "Preparing to download %s image to local "
-                "directory." % build_id
-            )
-
-        # Make sure the directory for downloading the image exists.
-        download_path = misc.GetOutsideChrootPath(
-            chromeos_root, os.path.join("/tmp", build_id)
-        )
-        image_path = os.path.join(download_path, "chromiumos_test_image.bin")
-        if not os.path.exists(download_path):
-            os.makedirs(download_path)
-
-        # Check to see if the image has already been downloaded.  If not,
-        # download the image.
-        if not os.path.exists(image_path):
-            gsutil_cmd = os.path.join(chromeos_root, GS_UTIL)
-            command = "%s cp %s %s" % (gsutil_cmd, image_name, download_path)
-
-            if self.log_level != "verbose":
-                self._logger.LogOutput("CMD: %s" % command)
-            status = self._ce.RunCommand(command)
-            downloaded_image_name = os.path.join(
-                download_path, "chromiumos_test_image.tar.xz"
-            )
-            if status != 0 or not os.path.exists(downloaded_image_name):
-                raise MissingImage(
-                    "Cannot download image: %s." % downloaded_image_name
-                )
-
-        return image_path
-
-    def UncompressImage(self, chromeos_root, build_id):
-        # Check to see if the file has already been uncompresssed, etc.
-        download_path = misc.GetOutsideChrootPath(
-            chromeos_root,
-            os.path.join(
-                "/tmp",
-                build_id,
-            ),
-        )
-        if os.path.exists(
-            os.path.join(download_path, "chromiumos_test_image.bin")
-        ):
-            return
-
-        # Uncompress and untar the downloaded image.
-        command = (
-            "cd %s ; tar -Jxf chromiumos_test_image.tar.xz " % download_path
-        )
-        # Cleanup command for exception handler
-        clean_cmd = "cd %s ; rm -f chromiumos_test_image.bin " % download_path
-        exception_handler = RunCommandExceptionHandler(
-            self._logger, self.log_level, self._ce, clean_cmd
-        )
-        if self.log_level != "verbose":
-            self._logger.LogOutput("CMD: %s" % command)
-            print(
-                "(Uncompressing and un-tarring may take a couple of minutes..."
-                "please be patient.)"
-            )
-        retval = self._ce.RunCommand(
-            command, except_handler=exception_handler.HandleException
-        )
-        if retval != 0:
-            if self.log_level != "verbose":
-                self._logger.LogOutput("CMD: %s" % clean_cmd)
-                print("(Removing file chromiumos_test_image.bin.)")
-            # Remove partially uncompressed file
-            _ = self._ce.RunCommand(clean_cmd)
-            # Raise exception for failure to uncompress
-            raise MissingImage("Cannot uncompress image: %s." % build_id)
-
-        # Remove compressed image
-        command = "cd %s ; rm -f chromiumos_test_image.tar.xz; " % download_path
-        if self.log_level != "verbose":
-            self._logger.LogOutput("CMD: %s" % command)
-            print("(Removing file chromiumos_test_image.tar.xz.)")
-        # try removing file, its ok to have an error, print if encountered
-        retval = self._ce.RunCommand(command)
-        if retval != 0:
-            print(
-                "(Warning: Could not remove file chromiumos_test_image.tar.xz .)"
-            )
-
-    def DownloadSingleFile(self, chromeos_root, build_id, package_file_name):
-        # Verify if package files exist
-        status = 0
-        gs_package_name = "gs://chromeos-image-archive/%s/%s" % (
-            build_id,
-            package_file_name,
-        )
-        gsutil_cmd = os.path.join(chromeos_root, GS_UTIL)
-        if not test_flag.GetTestMode():
-            cmd = "%s ls %s" % (gsutil_cmd, gs_package_name)
-            status = self._ce.RunCommand(cmd)
-        if status != 0:
-            raise MissingFile(
-                "Cannot find package file: %s." % package_file_name
-            )
-
-        if self.log_level == "average":
-            self._logger.LogOutput(
-                "Preparing to download %s package to local "
-                "directory." % package_file_name
-            )
-
-        # Make sure the directory for downloading the package exists.
-        download_path = misc.GetOutsideChrootPath(
-            chromeos_root, os.path.join("/tmp", build_id)
-        )
-        package_path = os.path.join(download_path, package_file_name)
-        if not os.path.exists(download_path):
-            os.makedirs(download_path)
-
-        # Check to see if the package file has already been downloaded.  If not,
-        # download it.
-        if not os.path.exists(package_path):
-            command = "%s cp %s %s" % (
-                gsutil_cmd,
-                gs_package_name,
-                download_path,
-            )
-
-            if self.log_level != "verbose":
-                self._logger.LogOutput("CMD: %s" % command)
-            status = self._ce.RunCommand(command)
-            if status != 0 or not os.path.exists(package_path):
-                raise MissingFile(
-                    "Cannot download package: %s ." % package_path
-                )
-
-    def UncompressSingleFile(
-        self, chromeos_root, build_id, package_file_name, uncompress_cmd
-    ):
-        # Uncompress file
-        download_path = misc.GetOutsideChrootPath(
-            chromeos_root, os.path.join("/tmp", build_id)
-        )
-        command = "cd %s ; %s %s" % (
-            download_path,
-            uncompress_cmd,
-            package_file_name,
-        )
-
-        if self.log_level != "verbose":
-            self._logger.LogOutput("CMD: %s" % command)
-            print("(Uncompressing file %s .)" % package_file_name)
-        retval = self._ce.RunCommand(command)
-        if retval != 0:
-            raise MissingFile("Cannot uncompress file: %s." % package_file_name)
-        # Remove uncompressed downloaded file
-        command = "cd %s ; rm -f %s" % (download_path, package_file_name)
-        if self.log_level != "verbose":
-            self._logger.LogOutput("CMD: %s" % command)
-            print("(Removing processed file %s .)" % package_file_name)
-        # try removing file, its ok to have an error, print if encountered
-        retval = self._ce.RunCommand(command)
-        if retval != 0:
-            print("(Warning: Could not remove file %s .)" % package_file_name)
-
-    def VerifyFileExists(self, chromeos_root, build_id, package_file):
-        # Quickly verify if the files are there
-        status = 0
-        gs_package_name = "gs://chromeos-image-archive/%s/%s" % (
-            build_id,
-            package_file,
-        )
-        gsutil_cmd = os.path.join(chromeos_root, GS_UTIL)
-        if not test_flag.GetTestMode():
-            cmd = "%s ls %s" % (gsutil_cmd, gs_package_name)
-            if self.log_level != "verbose":
-                self._logger.LogOutput("CMD: %s" % cmd)
-            status = self._ce.RunCommand(cmd)
-            if status != 0:
-                print("(Warning: Could not find file %s )" % gs_package_name)
-                return 1
-        # Package exists on server
-        return 0
-
-    def DownloadAutotestFiles(self, chromeos_root, build_id):
-        # Download autest package files (3 files)
-        autotest_packages_name = "autotest_packages.tar"
-        autotest_server_package_name = "autotest_server_package.tar.bz2"
-        autotest_control_files_name = "control_files.tar"
-
-        download_path = misc.GetOutsideChrootPath(
-            chromeos_root, os.path.join("/tmp", build_id)
-        )
-        # Autotest directory relative path wrt chroot
-        autotest_rel_path = os.path.join("/tmp", build_id, "autotest_files")
-        # Absolute Path to download files
-        autotest_path = os.path.join(download_path, "autotest_files")
-
-        if not os.path.exists(autotest_path):
-            # Quickly verify if the files are present on server
-            # If not, just exit with warning
-            status = self.VerifyFileExists(
-                chromeos_root, build_id, autotest_packages_name
-            )
-            if status != 0:
-                default_autotest_dir = (
-                    "/mnt/host/source/src/third_party/autotest/files"
-                )
-                print(
-                    "(Warning: Could not find autotest packages .)\n"
-                    "(Warning: Defaulting autotest path to %s ."
-                    % default_autotest_dir
-                )
-                return default_autotest_dir
-
-            # Files exist on server, download and uncompress them
-            self.DownloadSingleFile(
-                chromeos_root, build_id, autotest_packages_name
-            )
-            self.DownloadSingleFile(
-                chromeos_root, build_id, autotest_server_package_name
-            )
-            self.DownloadSingleFile(
-                chromeos_root, build_id, autotest_control_files_name
-            )
-
-            self.UncompressSingleFile(
-                chromeos_root, build_id, autotest_packages_name, "tar -xf "
-            )
-            self.UncompressSingleFile(
-                chromeos_root,
-                build_id,
-                autotest_server_package_name,
-                "tar -jxf ",
-            )
-            self.UncompressSingleFile(
-                chromeos_root, build_id, autotest_control_files_name, "tar -xf "
-            )
-            # Rename created autotest directory to autotest_files
-            command = "cd %s ; mv autotest autotest_files" % download_path
-            if self.log_level != "verbose":
-                self._logger.LogOutput("CMD: %s" % command)
-                print("(Moving downloaded autotest files to autotest_files)")
-            retval = self._ce.RunCommand(command)
-            if retval != 0:
-                raise MissingFile("Could not create directory autotest_files")
-
-        return autotest_rel_path
-
-    def DownloadDebugFile(self, chromeos_root, build_id):
-        # Download autest package files (3 files)
-        debug_archive_name = "debug.tgz"
-
-        download_path = misc.GetOutsideChrootPath(
-            chromeos_root, os.path.join("/tmp", build_id)
-        )
-        # Debug directory relative path wrt chroot
-        debug_rel_path = os.path.join("/tmp", build_id, "debug_files")
-        # Debug path to download files
-        debug_path = misc.GetOutsideChrootPath(
-            chromeos_root, os.path.join("/tmp", build_id, "debug_files")
-        )
-
-        if not os.path.exists(debug_path):
-            # Quickly verify if the file is present on server
-            # If not, just exit with warning
-            status = self.VerifyFileExists(
-                chromeos_root, build_id, debug_archive_name
-            )
-            if status != 0:
-                self._logger.LogOutput(
-                    "WARNING: Could not find debug archive on gs"
-                )
-                return ""
-
-            # File exists on server, download and uncompress it
-            self.DownloadSingleFile(chromeos_root, build_id, debug_archive_name)
-
-            # Extract and move debug files into the proper location.
-            debug_dir = "debug_files/usr/lib/debug"
-            command = (
-                f"cd {shlex.quote(download_path)}; "
-                f"mkdir -p {shlex.quote(debug_dir)}"
-            )
-            if self.log_level != "verbose":
-                self._logger.LogOutput("CMD: %s" % command)
-                print("Moving downloaded debug files to %s" % debug_dir)
-            retval = self._ce.RunCommand(command)
-            if retval != 0:
-                raise MissingFile(
-                    "Could not create directory %s"
-                    % os.path.join(debug_dir, "debug")
-                )
-            self.UncompressSingleFile(
-                chromeos_root,
-                build_id,
-                debug_archive_name,
-                f"tar -C {shlex.quote(debug_dir)} -xf ",
-            )
-
-        return debug_rel_path
-
-    def Run(
-        self,
-        chromeos_root,
-        xbuddy_label,
-        autotest_path,
-        debug_path,
-        download_debug,
-    ):
-        build_id = self.GetBuildID(chromeos_root, xbuddy_label)
-        image_name = (
-            "gs://chromeos-image-archive/%s/chromiumos_test_image.tar.xz"
-            % build_id
-        )
-
-        # Verify that image exists for build_id, before attempting to
-        # download it.
-        status = 0
-        if not test_flag.GetTestMode():
-            gsutil_cmd = os.path.join(chromeos_root, GS_UTIL)
-            cmd = "%s ls %s" % (gsutil_cmd, image_name)
-            status = self._ce.RunCommand(cmd)
-        if status != 0:
-            raise MissingImage("Cannot find official image: %s." % image_name)
-
-        image_path = self.DownloadImage(chromeos_root, build_id, image_name)
-        self.UncompressImage(chromeos_root, build_id)
-
-        if self.log_level != "quiet":
-            self._logger.LogOutput("Using image from %s." % image_path)
-
-        if autotest_path == "":
-            autotest_path = self.DownloadAutotestFiles(chromeos_root, build_id)
-
-        if debug_path == "" and download_debug:
-            debug_path = self.DownloadDebugFile(chromeos_root, build_id)
-
-        return image_path, autotest_path, debug_path
diff --git a/crosperf/download_images_buildid_test.py b/crosperf/download_images_buildid_test.py
deleted file mode 100755
index 20dd13c5..00000000
--- a/crosperf/download_images_buildid_test.py
+++ /dev/null
@@ -1,132 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2014 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Test translation of xbuddy names."""
-
-
-import argparse
-import sys
-
-import download_images
-
-
-# On May 1, 2014:
-# latest         : lumpy-release/R34-5500.132.0
-# latest-beta    : lumpy-release/R35-5712.43.0
-# latest-official: lumpy-release/R36-5814.0.0
-# latest-dev     : lumpy-release/R36-5814.0.0
-# latest-canary  : lumpy-release/R36-5814.0.0
-
-
-class ImageDownloaderBuildIDTest(object):
-    """Test translation of xbuddy names."""
-
-    def __init__(self):
-        parser = argparse.ArgumentParser()
-        parser.add_argument(
-            "-c",
-            "--chromeos_root",
-            dest="chromeos_root",
-            help="Directory containing ChromeOS root.",
-        )
-
-        options = parser.parse_known_args(sys.argv[1:])[0]
-        if options.chromeos_root is None:
-            self._usage(parser, "--chromeos_root must be set")
-        self.chromeos_root = options.chromeos_root
-        self.tests_passed = 0
-        self.tests_run = 0
-        self.tests_failed = 0
-
-    def _usage(self, parser, message):
-        print("ERROR: " + message)
-        parser.print_help()
-        sys.exit(0)
-
-    def print_test_status(self):
-        print("----------------------------------------\n")
-        print("Tests attempted: %d" % self.tests_run)
-        print("Tests passed:    %d" % self.tests_passed)
-        print("Tests failed:    %d" % self.tests_failed)
-        print("\n----------------------------------------")
-
-    def assert_failure(self, msg):
-        print("Assert failure: %s" % msg)
-        self.print_test_status()
-        sys.exit(1)
-
-    def assertIsNotNone(self, arg, arg_name):
-        if arg is None:
-            self.tests_failed = self.tests_failed + 1
-            self.assert_failure("%s is not None" % arg_name)
-
-    def assertNotEqual(self, arg1, arg2, arg1_name, arg2_name):
-        if arg1 == arg2:
-            self.tests_failed = self.tests_failed + 1
-            self.assert_failure(
-                "%s is not NotEqual to %s" % (arg1_name, arg2_name)
-            )
-
-    def assertEqual(self, arg1, arg2, arg1_name, arg2_name):
-        if arg1 != arg2:
-            self.tests_failed = self.tests_failed + 1
-            self.assert_failure(
-                "%s is not Equal to %s" % (arg1_name, arg2_name)
-            )
-
-    def test_one_id(self, downloader, test_id, result_string, exact_match):
-        print("Translating '%s'" % test_id)
-        self.tests_run = self.tests_run + 1
-
-        result = downloader.GetBuildID(self.chromeos_root, test_id)
-        # Verify that we got a build id back.
-        self.assertIsNotNone(result, "result")
-
-        # Verify that the result either contains or exactly matches the
-        # result_string, depending on the exact_match argument.
-        if exact_match:
-            self.assertEqual(result, result_string, "result", result_string)
-        else:
-            self.assertNotEqual(
-                result.find(result_string), -1, "result.find", "-1"
-            )
-        self.tests_passed = self.tests_passed + 1
-
-    def test_get_build_id(self):
-        """Test that the actual translating of xbuddy names is working properly."""
-        downloader = download_images.ImageDownloader(log_level="quiet")
-
-        self.test_one_id(
-            downloader, "remote/lumpy/latest-dev", "lumpy-release/R", False
-        )
-        self.test_one_id(
-            downloader,
-            "remote/trybot-lumpy-release-afdo-use/R35-5672.0.0-b86",
-            "trybot-lumpy-release-afdo-use/R35-5672.0.0-b86",
-            True,
-        )
-        self.test_one_id(
-            downloader,
-            "remote/lumpy-release/R35-5672.0.0",
-            "lumpy-release/R35-5672.0.0",
-            True,
-        )
-        self.test_one_id(
-            downloader, "remote/lumpy/latest-dev", "lumpy-release/R", False
-        )
-        self.test_one_id(
-            downloader, "remote/lumpy/latest-official", "lumpy-release/R", False
-        )
-        self.test_one_id(
-            downloader, "remote/lumpy/latest-beta", "lumpy-release/R", False
-        )
-
-        self.print_test_status()
-
-
-if __name__ == "__main__":
-    tester = ImageDownloaderBuildIDTest()
-    tester.test_get_build_id()
diff --git a/crosperf/download_images_unittest.py b/crosperf/download_images_unittest.py
deleted file mode 100755
index 0e47e757..00000000
--- a/crosperf/download_images_unittest.py
+++ /dev/null
@@ -1,348 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Download image unittest."""
-
-
-import os
-import re
-import unittest
-import unittest.mock as mock
-
-from cros_utils import command_executer
-from cros_utils import logger
-import download_images
-import test_flag
-
-
-MOCK_LOGGER = logger.GetLogger(log_dir="", mock=True)
-
-
-class RegexMatcher:
-    """A regex matcher, for passing to mocks."""
-
-    def __init__(self, regex):
-        self._regex = re.compile(regex)
-
-    def __eq__(self, string):
-        return self._regex.search(string) is not None
-
-
-class ImageDownloaderTestcast(unittest.TestCase):
-    """The image downloader test class."""
-
-    def __init__(self, *args, **kwargs):
-        super(ImageDownloaderTestcast, self).__init__(*args, **kwargs)
-        self.called_download_image = False
-        self.called_uncompress_image = False
-        self.called_get_build_id = False
-        self.called_download_autotest_files = False
-        self.called_download_debug_file = False
-
-    @mock.patch.object(os, "makedirs")
-    @mock.patch.object(os.path, "exists")
-    def test_download_image(self, mock_path_exists, mock_mkdirs):
-        # Set mock and test values.
-        mock_cmd_exec = mock.Mock(spec=command_executer.CommandExecuter)
-        test_chroot = "/usr/local/home/chromeos"
-        test_build_id = "lumpy-release/R36-5814.0.0"
-        image_path = (
-            "gs://chromeos-image-archive/%s/chromiumos_test_image.tar.xz"
-            % test_build_id
-        )
-
-        downloader = download_images.ImageDownloader(
-            logger_to_use=MOCK_LOGGER, cmd_exec=mock_cmd_exec
-        )
-
-        # Set os.path.exists to always return False and run downloader
-        mock_path_exists.return_value = False
-        test_flag.SetTestMode(True)
-        self.assertRaises(
-            download_images.MissingImage,
-            downloader.DownloadImage,
-            test_chroot,
-            test_build_id,
-            image_path,
-        )
-
-        # Verify os.path.exists was called thrice, with proper arguments.
-        self.assertEqual(mock_path_exists.call_count, 3)
-        mock_path_exists.assert_any_call(
-            RegexMatcher(
-                "/usr/local/home/chromeos/.*tmp/lumpy-release/"
-                "R36-5814.0.0/chromiumos_test_image.bin"
-            )
-        )
-        mock_path_exists.assert_any_call(
-            RegexMatcher(
-                "/usr/local/home/chromeos/.*tmp/lumpy-release/R36-5814.0.0"
-            )
-        )
-        mock_path_exists.assert_any_call("/etc/cros_chroot_version")
-
-        # Verify we called os.mkdirs
-        self.assertEqual(mock_mkdirs.call_count, 1)
-        mock_mkdirs.assert_called_with(
-            RegexMatcher(
-                "/usr/local/home/chromeos/.*tmp/lumpy-release/R36-5814.0.0"
-            )
-        )
-
-        # Verify we called RunCommand once, with proper arguments.
-        self.assertEqual(mock_cmd_exec.RunCommand.call_count, 1)
-        expected_args = RegexMatcher(
-            "/usr/local/home/chromeos/src/chromium/depot_tools/gsutil.py "
-            "cp gs://chromeos-image-archive/lumpy-release/R36-5814.0.0/"
-            "chromiumos_test_image.tar.xz "
-            "/usr/local/home/chromeos/.*tmp/lumpy-release/R36-5814.0.0"
-        )
-
-        mock_cmd_exec.RunCommand.assert_called_with(expected_args)
-
-        # Reset the velues in the mocks; set os.path.exists to always return
-        # True (except for "inside chroot" check).
-        mock_path_exists.reset_mock()
-        mock_cmd_exec.reset_mock()
-        mock_path_exists.side_effect = lambda x: x != "/etc/cros_chroot_version"
-
-        # Run downloader
-        downloader.DownloadImage(test_chroot, test_build_id, image_path)
-
-        # Verify os.path.exists was called thrice, with proper arguments.
-        self.assertEqual(mock_path_exists.call_count, 3)
-        mock_path_exists.assert_called_with(
-            RegexMatcher(
-                "/usr/local/home/chromeos/.*tmp/lumpy-release/"
-                "R36-5814.0.0/chromiumos_test_image.bin"
-            )
-        )
-        mock_path_exists.assert_any_call(
-            RegexMatcher(
-                "/usr/local/home/chromeos/.*tmp/lumpy-release/R36-5814.0.0"
-            )
-        )
-        mock_path_exists.assert_any_call("/etc/cros_chroot_version")
-
-        # Verify we made no RunCommand or ChrootRunCommand calls (since
-        # os.path.exists returned True, there was no work do be done).
-        self.assertEqual(mock_cmd_exec.RunCommand.call_count, 0)
-        self.assertEqual(mock_cmd_exec.ChrootRunCommand.call_count, 0)
-
-    @mock.patch.object(os.path, "exists")
-    def test_uncompress_image(self, mock_path_exists):
-        # set mock and test values.
-        mock_cmd_exec = mock.Mock(spec=command_executer.CommandExecuter)
-        test_chroot = "/usr/local/home/chromeos"
-        test_build_id = "lumpy-release/R36-5814.0.0"
-
-        downloader = download_images.ImageDownloader(
-            logger_to_use=MOCK_LOGGER, cmd_exec=mock_cmd_exec
-        )
-
-        # Set os.path.exists to always return False and run uncompress.
-        mock_path_exists.return_value = False
-        self.assertRaises(
-            download_images.MissingImage,
-            downloader.UncompressImage,
-            test_chroot,
-            test_build_id,
-        )
-
-        # Verify os.path.exists was called twice, with correct arguments.
-        self.assertEqual(mock_path_exists.call_count, 2)
-        mock_path_exists.assert_called_with(
-            RegexMatcher(
-                "/usr/local/home/chromeos/.*tmp/lumpy-release/"
-                "R36-5814.0.0/chromiumos_test_image.bin"
-            )
-        )
-        mock_path_exists.assert_any_call("/etc/cros_chroot_version")
-
-        # Verify RunCommand was called twice with correct arguments.
-        self.assertEqual(mock_cmd_exec.RunCommand.call_count, 2)
-        # Call 1, should have 2 arguments
-        self.assertEqual(len(mock_cmd_exec.RunCommand.call_args_list[0]), 2)
-        actual_arg = mock_cmd_exec.RunCommand.call_args_list[0][0]
-        expected_arg = (
-            RegexMatcher(
-                "cd /usr/local/home/chromeos/.*tmp/lumpy-release/R36-5814.0.0 ; "
-                "tar -Jxf chromiumos_test_image.tar.xz "
-            ),
-        )
-        self.assertEqual(expected_arg, actual_arg)
-        # 2nd arg must be exception handler
-        except_handler_string = "RunCommandExceptionHandler.HandleException"
-        self.assertTrue(
-            except_handler_string
-            in repr(mock_cmd_exec.RunCommand.call_args_list[0][1])
-        )
-
-        # Call 2, should have 2 arguments
-        self.assertEqual(len(mock_cmd_exec.RunCommand.call_args_list[1]), 2)
-        actual_arg = mock_cmd_exec.RunCommand.call_args_list[1][0]
-        expected_arg = (
-            RegexMatcher(
-                "cd /usr/local/home/chromeos/.*tmp/lumpy-release/R36-5814.0.0 ; "
-                "rm -f chromiumos_test_image.bin "
-            ),
-        )
-        self.assertEqual(expected_arg, actual_arg)
-        # 2nd arg must be empty
-        self.assertTrue(
-            "{}" in repr(mock_cmd_exec.RunCommand.call_args_list[1][1])
-        )
-
-        # Set os.path.exists to always return True (except for "inside chroot"
-        # check) and run uncompress.
-        mock_path_exists.reset_mock()
-        mock_cmd_exec.reset_mock()
-        mock_path_exists.side_effect = lambda x: x != "/etc/cros_chroot_version"
-        downloader.UncompressImage(test_chroot, test_build_id)
-
-        # Verify os.path.exists was called once, with correct arguments.
-        self.assertEqual(mock_path_exists.call_count, 2)
-        mock_path_exists.assert_called_with(
-            RegexMatcher(
-                "/usr/local/home/chromeos/.*tmp/lumpy-release/"
-                "R36-5814.0.0/chromiumos_test_image.bin"
-            )
-        )
-        mock_path_exists.assert_any_call("/etc/cros_chroot_version")
-
-        # Verify RunCommand was not called.
-        self.assertEqual(mock_cmd_exec.RunCommand.call_count, 0)
-
-    def test_run(self):
-        # Set test arguments
-        test_chroot = "/usr/local/home/chromeos"
-        test_build_id = "remote/lumpy/latest-dev"
-        test_empty_autotest_path = ""
-        test_empty_debug_path = ""
-        test_autotest_path = "/tmp/autotest"
-        test_debug_path = "/tmp/debug"
-        download_debug = True
-
-        # Set values to test/check.
-        self.called_download_image = False
-        self.called_uncompress_image = False
-        self.called_get_build_id = False
-        self.called_download_autotest_files = False
-        self.called_download_debug_file = False
-
-        # Define fake stub functions for Run to call
-        def FakeGetBuildID(unused_root, unused_xbuddy_label):
-            self.called_get_build_id = True
-            return "lumpy-release/R36-5814.0.0"
-
-        def GoodDownloadImage(root, build_id, image_path):
-            if root or build_id or image_path:
-                pass
-            self.called_download_image = True
-            return "chromiumos_test_image.bin"
-
-        def BadDownloadImage(root, build_id, image_path):
-            if root or build_id or image_path:
-                pass
-            self.called_download_image = True
-            raise download_images.MissingImage("Could not download image")
-
-        def FakeUncompressImage(root, build_id):
-            if root or build_id:
-                pass
-            self.called_uncompress_image = True
-            return 0
-
-        def FakeDownloadAutotestFiles(root, build_id):
-            if root or build_id:
-                pass
-            self.called_download_autotest_files = True
-            return "autotest"
-
-        def FakeDownloadDebugFile(root, build_id):
-            if root or build_id:
-                pass
-            self.called_download_debug_file = True
-            return "debug"
-
-        # Initialize downloader
-        downloader = download_images.ImageDownloader(logger_to_use=MOCK_LOGGER)
-
-        # Set downloader to call fake stubs.
-        downloader.GetBuildID = FakeGetBuildID
-        downloader.UncompressImage = FakeUncompressImage
-        downloader.DownloadImage = GoodDownloadImage
-        downloader.DownloadAutotestFiles = FakeDownloadAutotestFiles
-        downloader.DownloadDebugFile = FakeDownloadDebugFile
-
-        # Call Run.
-        image_path, autotest_path, debug_path = downloader.Run(
-            test_chroot,
-            test_build_id,
-            test_empty_autotest_path,
-            test_empty_debug_path,
-            download_debug,
-        )
-
-        # Make sure it called both _DownloadImage and _UncompressImage
-        self.assertTrue(self.called_download_image)
-        self.assertTrue(self.called_uncompress_image)
-        # Make sure it called DownloadAutotestFiles
-        self.assertTrue(self.called_download_autotest_files)
-        # Make sure it called DownloadDebugFile
-        self.assertTrue(self.called_download_debug_file)
-        # Make sure it returned an image and autotest path returned from this call
-        self.assertTrue(image_path == "chromiumos_test_image.bin")
-        self.assertTrue(autotest_path == "autotest")
-        self.assertTrue(debug_path == "debug")
-
-        # Call Run with a non-empty autotest and debug path
-        self.called_download_autotest_files = False
-        self.called_download_debug_file = False
-
-        image_path, autotest_path, debug_path = downloader.Run(
-            test_chroot,
-            test_build_id,
-            test_autotest_path,
-            test_debug_path,
-            download_debug,
-        )
-
-        # Verify that downloadAutotestFiles was not called
-        self.assertFalse(self.called_download_autotest_files)
-        # Make sure it returned the specified autotest path returned from this call
-        self.assertTrue(autotest_path == test_autotest_path)
-        # Make sure it returned the specified debug path returned from this call
-        self.assertTrue(debug_path == test_debug_path)
-
-        # Reset values; Now use fake stub that simulates DownloadImage failing.
-        self.called_download_image = False
-        self.called_uncompress_image = False
-        self.called_download_autotest_files = False
-        self.called_download_debug_file = False
-        downloader.DownloadImage = BadDownloadImage
-
-        # Call Run again.
-        self.assertRaises(
-            download_images.MissingImage,
-            downloader.Run,
-            test_chroot,
-            test_autotest_path,
-            test_debug_path,
-            test_build_id,
-            download_debug,
-        )
-
-        # Verify that UncompressImage and downloadAutotestFiles were not called,
-        # since _DownloadImage "failed"
-        self.assertTrue(self.called_download_image)
-        self.assertFalse(self.called_uncompress_image)
-        self.assertFalse(self.called_download_autotest_files)
-        self.assertFalse(self.called_download_debug_file)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/experiment.py b/crosperf/experiment.py
deleted file mode 100644
index 7d35e319..00000000
--- a/crosperf/experiment.py
+++ /dev/null
@@ -1,272 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""The experiment setting module."""
-
-
-import os
-from threading import Lock
-import time
-
-import benchmark_run
-from cros_utils import logger
-from cros_utils import misc
-from machine_manager import BadChecksum
-from machine_manager import MachineManager
-from machine_manager import MockMachineManager
-import test_flag
-
-
-class Experiment(object):
-    """Class representing an Experiment to be run."""
-
-    def __init__(
-        self,
-        name,
-        remote,
-        working_directory,
-        chromeos_root,
-        cache_conditions,
-        labels,
-        benchmarks,
-        experiment_file,
-        email_to,
-        acquire_timeout,
-        log_dir,
-        log_level,
-        share_cache,
-        results_directory,
-        compress_results,
-        locks_directory,
-        cwp_dso,
-        ignore_min_max,
-        crosfleet,
-        dut_config,
-        keep_stateful: bool,
-        no_lock: bool,
-    ):
-        self.name = name
-        self.working_directory = working_directory
-        self.remote = remote
-        self.chromeos_root = chromeos_root
-        self.cache_conditions = cache_conditions
-        self.experiment_file = experiment_file
-        self.email_to = email_to
-        if not results_directory:
-            self.results_directory = os.path.join(
-                self.working_directory, self.name + "_results"
-            )
-        else:
-            self.results_directory = misc.CanonicalizePath(results_directory)
-        self.compress_results = compress_results
-        self.log_dir = log_dir
-        self.log_level = log_level
-        self.labels = labels
-        self.benchmarks = benchmarks
-        self.num_complete = 0
-        self.num_run_complete = 0
-        self.share_cache = share_cache
-        self.active_threads = []
-        self.locks_dir = locks_directory
-        self.locked_machines = []
-        self.lock_mgr = None
-        self.cwp_dso = cwp_dso
-        self.ignore_min_max = ignore_min_max
-        self.crosfleet = crosfleet
-        self.no_lock = no_lock
-        self.l = logger.GetLogger(log_dir)
-
-        if not self.benchmarks:
-            raise RuntimeError("No benchmarks specified")
-        if not self.labels:
-            raise RuntimeError("No labels specified")
-        if not remote and not self.crosfleet:
-            raise RuntimeError("No remote hosts specified")
-
-        # We need one chromeos_root to run the benchmarks in, but it doesn't
-        # matter where it is, unless the ABIs are different.
-        if not chromeos_root:
-            for label in self.labels:
-                if label.chromeos_root:
-                    chromeos_root = label.chromeos_root
-                    break
-        if not chromeos_root:
-            raise RuntimeError(
-                "No chromeos_root given and could not determine "
-                "one from the image path."
-            )
-
-        machine_manager_fn = MachineManager
-        if test_flag.GetTestMode():
-            machine_manager_fn = MockMachineManager
-        self.machine_manager = machine_manager_fn(
-            chromeos_root,
-            acquire_timeout,
-            log_level,
-            locks_directory,
-            keep_stateful=keep_stateful,
-        )
-        self.l = logger.GetLogger(log_dir)
-
-        for machine in self.remote:
-            # machine_manager.AddMachine only adds reachable machines.
-            self.machine_manager.AddMachine(machine)
-        # Now machine_manager._all_machines contains a list of reachable
-        # machines. This is a subset of self.remote. We make both lists the same.
-        self.remote = [m.name for m in self.machine_manager.GetAllMachines()]
-        if not self.remote:
-            raise RuntimeError("No machine available for running experiment.")
-
-        # Initialize checksums for all machines, ignore errors at this time.
-        # The checksum will be double checked, and image will be flashed after
-        # duts are locked/leased.
-        self.SetCheckSums()
-
-        self.start_time = None
-        self.benchmark_runs = self._GenerateBenchmarkRuns(dut_config)
-
-        self._schedv2 = None
-        self._internal_counter_lock = Lock()
-
-    def set_schedv2(self, schedv2):
-        self._schedv2 = schedv2
-
-    def schedv2(self):
-        return self._schedv2
-
-    def _GenerateBenchmarkRuns(self, dut_config):
-        """Generate benchmark runs from labels and benchmark defintions."""
-        benchmark_runs = []
-        for label in self.labels:
-            for benchmark in self.benchmarks:
-                for iteration in range(1, benchmark.iterations + 1):
-                    benchmark_run_name = "%s: %s (%s)" % (
-                        label.name,
-                        benchmark.name,
-                        iteration,
-                    )
-                    full_name = "%s_%s_%s" % (
-                        label.name,
-                        benchmark.name,
-                        iteration,
-                    )
-                    logger_to_use = logger.Logger(
-                        self.log_dir, "run.%s" % (full_name), True
-                    )
-                    benchmark_runs.append(
-                        benchmark_run.BenchmarkRun(
-                            benchmark_run_name,
-                            benchmark,
-                            label,
-                            iteration,
-                            self.cache_conditions,
-                            self.machine_manager,
-                            logger_to_use,
-                            self.log_level,
-                            self.share_cache,
-                            dut_config,
-                        )
-                    )
-
-        return benchmark_runs
-
-    def SetCheckSums(self, forceSameImage=False):
-        for label in self.labels:
-            # We filter out label remotes that are not reachable (not in
-            # self.remote). So each label.remote is a sublist of experiment.remote.
-            label.remote = [r for r in label.remote if r in self.remote]
-            try:
-                self.machine_manager.ComputeCommonCheckSum(label)
-            except BadChecksum:
-                # Force same image on all machines, then we do checksum again. No
-                # bailout if checksums still do not match.
-                # TODO (zhizhouy): Need to figure out how flashing image will influence
-                # the new checksum.
-                if forceSameImage:
-                    self.machine_manager.ForceSameImageToAllMachines(label)
-                    self.machine_manager.ComputeCommonCheckSum(label)
-
-            self.machine_manager.ComputeCommonCheckSumString(label)
-
-    def Build(self):
-        pass
-
-    def Terminate(self):
-        if self._schedv2 is not None:
-            self._schedv2.terminate()
-        else:
-            for t in self.benchmark_runs:
-                if t.isAlive():
-                    self.l.LogError("Terminating run: '%s'." % t.name)
-                    t.Terminate()
-
-    def IsComplete(self):
-        if self._schedv2:
-            return self._schedv2.is_complete()
-        if self.active_threads:
-            for t in self.active_threads:
-                if t.isAlive():
-                    t.join(0)
-                if not t.isAlive():
-                    self.num_complete += 1
-                    if not t.cache_hit:
-                        self.num_run_complete += 1
-                    self.active_threads.remove(t)
-            return False
-        return True
-
-    def BenchmarkRunFinished(self, br):
-        """Update internal counters after br finishes.
-
-        Note this is only used by schedv2 and is called by multiple threads.
-        Never throw any exception here.
-        """
-
-        assert self._schedv2 is not None
-        with self._internal_counter_lock:
-            self.num_complete += 1
-            if not br.cache_hit:
-                self.num_run_complete += 1
-
-    def Run(self):
-        self.start_time = time.time()
-        if self._schedv2 is not None:
-            self._schedv2.run_sched()
-        else:
-            self.active_threads = []
-            for run in self.benchmark_runs:
-                # Set threads to daemon so program exits when ctrl-c is pressed.
-                run.daemon = True
-                run.start()
-                self.active_threads.append(run)
-
-    def SetCacheConditions(self, cache_conditions):
-        for run in self.benchmark_runs:
-            run.SetCacheConditions(cache_conditions)
-
-    def Cleanup(self):
-        """Make sure all machines are unlocked."""
-        if self.locks_dir:
-            # We are using the file locks mechanism, so call machine_manager.Cleanup
-            # to unlock everything.
-            self.machine_manager.Cleanup()
-
-        if test_flag.GetTestMode() or not self.locked_machines:
-            return
-
-        # If we locked any machines earlier, make sure we unlock them now.
-        if self.lock_mgr:
-            machine_states = self.lock_mgr.GetMachineStates("unlock")
-            self.lock_mgr.CheckMachineLocks(machine_states, "unlock")
-            unlocked_machines = self.lock_mgr.UpdateMachines(False)
-            failed_machines = [
-                m for m in self.locked_machines if m not in unlocked_machines
-            ]
-            if failed_machines:
-                raise RuntimeError(
-                    "These machines are not unlocked correctly: %s"
-                    % failed_machines
-                )
-            self.lock_mgr = None
diff --git a/crosperf/experiment_factory.py b/crosperf/experiment_factory.py
deleted file mode 100644
index e89adb87..00000000
--- a/crosperf/experiment_factory.py
+++ /dev/null
@@ -1,582 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""A module to generate experiments."""
-
-
-import os
-import re
-import socket
-import sys
-
-from benchmark import Benchmark
-from cros_utils import command_executer
-from cros_utils import logger
-from experiment import Experiment
-import file_lock_machine
-from label import Label
-from label import MockLabel
-from results_cache import CacheConditions
-import test_flag
-
-import config
-
-
-# Users may want to run Telemetry tests either individually, or in
-# specified sets.  Here we define sets of tests that users may want
-# to run together.
-
-telemetry_toolchain_perf_tests = [
-    "octane",
-    "speedometer",
-    "speedometer2",
-    "jetstream2",
-]
-graphics_perf_tests = [
-    "graphics_GLBench",
-    "graphics_GLMark2",
-    "graphics_SanAngeles",
-    "graphics_WebGLAquarium",
-    "graphics_WebGLPerformance",
-]
-# TODO: disable rendering.desktop by default as the benchmark is
-# currently in a bad state
-telemetry_crosbolt_perf_tests = [
-    "octane",
-    "speedometer2",
-    "jetstream2",
-    "loading.desktop",
-    # 'rendering.desktop',
-]
-
-crosbolt_perf_tests = [
-    "graphics_WebGLAquarium",
-    "tast.video.PlaybackPerfVP91080P30FPS",
-]
-
-dso_list = [
-    "all",
-    "chrome",
-    "kallsyms",
-]
-
-
-class ExperimentFactory(object):
-    """Factory class for building an Experiment, given an ExperimentFile as input.
-
-    This factory is currently hardcoded to produce an experiment for running
-    ChromeOS benchmarks, but the idea is that in the future, other types
-    of experiments could be produced.
-    """
-
-    def AppendBenchmarkSet(
-        self,
-        benchmarks,
-        benchmark_list,
-        test_args,
-        iterations,
-        rm_chroot_tmp,
-        perf_args,
-        suite,
-        show_all_results,
-        retries,
-        run_local,
-        cwp_dso,
-        weight,
-    ):
-        """Add all the tests in a set to the benchmarks list."""
-        for test_name in benchmark_list:
-            telemetry_benchmark = Benchmark(
-                test_name,
-                test_name,
-                test_args,
-                iterations,
-                rm_chroot_tmp,
-                perf_args,
-                suite,
-                show_all_results,
-                retries,
-                run_local,
-                cwp_dso,
-                weight,
-            )
-            benchmarks.append(telemetry_benchmark)
-
-    def GetExperiment(self, experiment_file, working_directory, log_dir):
-        """Construct an experiment from an experiment file."""
-        global_settings = experiment_file.GetGlobalSettings()
-        experiment_name = global_settings.GetField("name")
-        board = global_settings.GetField("board")
-        chromeos_root = global_settings.GetField("chromeos_root")
-        log_level = global_settings.GetField("logging_level")
-        if log_level not in ("quiet", "average", "verbose"):
-            log_level = "verbose"
-
-        crosfleet = global_settings.GetField("crosfleet")
-        no_lock = bool(global_settings.GetField("no_lock"))
-        # Check whether crosfleet tool is installed correctly for crosfleet mode.
-        if crosfleet and not self.CheckCrosfleetTool(chromeos_root, log_level):
-            sys.exit(0)
-
-        remote = global_settings.GetField("remote")
-        # This is used to remove the ",' from the remote if user
-        # add them to the remote string.
-        new_remote = []
-        if remote:
-            for i in remote:
-                c = re.sub("[\"']", "", i)
-                new_remote.append(c)
-        remote = new_remote
-        rm_chroot_tmp = global_settings.GetField("rm_chroot_tmp")
-        perf_args = global_settings.GetField("perf_args")
-        download_debug = global_settings.GetField("download_debug")
-        # Do not download debug symbols when perf_args is not specified.
-        if not perf_args and download_debug:
-            download_debug = False
-        acquire_timeout = global_settings.GetField("acquire_timeout")
-        cache_dir = global_settings.GetField("cache_dir")
-        cache_only = global_settings.GetField("cache_only")
-        config.AddConfig("no_email", global_settings.GetField("no_email"))
-        share_cache = global_settings.GetField("share_cache")
-        results_dir = global_settings.GetField("results_dir")
-        compress_results = global_settings.GetField("compress_results")
-        # Warn user that option use_file_locks is deprecated.
-        use_file_locks = global_settings.GetField("use_file_locks")
-        if use_file_locks:
-            l = logger.GetLogger()
-            l.LogWarning(
-                "Option use_file_locks is deprecated, please remove it "
-                "from your experiment settings."
-            )
-        locks_dir = global_settings.GetField("locks_dir")
-        # If not specified, set the locks dir to the default locks dir in
-        # file_lock_machine.
-        if not locks_dir:
-            locks_dir = file_lock_machine.Machine.LOCKS_DIR
-        if not os.path.exists(locks_dir):
-            raise RuntimeError(
-                "Cannot access default lock directory. "
-                "Please run prodaccess or specify a local directory"
-            )
-        chrome_src = global_settings.GetField("chrome_src")
-        show_all_results = global_settings.GetField("show_all_results")
-        cwp_dso = global_settings.GetField("cwp_dso")
-        if cwp_dso and not cwp_dso in dso_list:
-            raise RuntimeError("The DSO specified is not supported")
-        ignore_min_max = global_settings.GetField("ignore_min_max")
-        dut_config = {
-            "enable_aslr": global_settings.GetField("enable_aslr"),
-            "intel_pstate": global_settings.GetField("intel_pstate"),
-            "cooldown_time": global_settings.GetField("cooldown_time"),
-            "cooldown_temp": global_settings.GetField("cooldown_temp"),
-            "governor": global_settings.GetField("governor"),
-            "cpu_usage": global_settings.GetField("cpu_usage"),
-            "cpu_freq_pct": global_settings.GetField("cpu_freq_pct"),
-            "turbostat": global_settings.GetField("turbostat"),
-            "top_interval": global_settings.GetField("top_interval"),
-        }
-        keep_stateful = global_settings.GetField("keep_stateful")
-
-        # Default cache hit conditions. The image checksum in the cache and the
-        # computed checksum of the image must match. Also a cache file must exist.
-        cache_conditions = [
-            CacheConditions.CACHE_FILE_EXISTS,
-            CacheConditions.CHECKSUMS_MATCH,
-        ]
-        if global_settings.GetField("rerun_if_failed"):
-            cache_conditions.append(CacheConditions.RUN_SUCCEEDED)
-        if global_settings.GetField("rerun") or global_settings.GetField(
-            "ignore_cache"
-        ):
-            cache_conditions.append(CacheConditions.FALSE)
-        if global_settings.GetField("same_machine"):
-            cache_conditions.append(CacheConditions.SAME_MACHINE_MATCH)
-        if global_settings.GetField("same_specs"):
-            cache_conditions.append(CacheConditions.MACHINES_MATCH)
-
-        # Construct benchmarks.
-        # Some fields are common with global settings. The values are
-        # inherited and/or merged with the global settings values.
-        benchmarks = []
-        all_benchmark_settings = experiment_file.GetSettings("benchmark")
-
-        # Check if there is duplicated benchmark name
-        benchmark_names = {}
-        # Check if in cwp_dso mode, all benchmarks should have same iterations
-        cwp_dso_iterations = 0
-
-        for benchmark_settings in all_benchmark_settings:
-            benchmark_name = benchmark_settings.name
-            test_name = benchmark_settings.GetField("test_name")
-            if not test_name:
-                test_name = benchmark_name
-            test_args = benchmark_settings.GetField("test_args")
-
-            # Rename benchmark name if 'story-filter' or 'story-tag-filter' specified
-            # in test_args. Make sure these two tags only appear once.
-            story_count = 0
-            for arg in test_args.split():
-                if "--story-filter=" in arg or "--story-tag-filter=" in arg:
-                    story_count += 1
-                    if story_count > 1:
-                        raise RuntimeError(
-                            "Only one story or story-tag filter allowed in "
-                            "a single benchmark run"
-                        )
-                    # Rename benchmark name with an extension of 'story'-option
-                    benchmark_name = "%s@@%s" % (
-                        benchmark_name,
-                        arg.split("=")[-1],
-                    )
-
-            # Check for duplicated benchmark name after renaming
-            if not benchmark_name in benchmark_names:
-                benchmark_names[benchmark_name] = True
-            else:
-                raise SyntaxError(
-                    "Duplicate benchmark name: '%s'." % benchmark_name
-                )
-
-            iterations = benchmark_settings.GetField("iterations")
-            if cwp_dso:
-                if cwp_dso_iterations not in (0, iterations):
-                    raise RuntimeError(
-                        "Iterations of each benchmark run are not the " "same"
-                    )
-                cwp_dso_iterations = iterations
-
-            suite = benchmark_settings.GetField("suite")
-            retries = benchmark_settings.GetField("retries")
-            run_local = benchmark_settings.GetField("run_local")
-            weight = benchmark_settings.GetField("weight")
-            if weight:
-                if not cwp_dso:
-                    raise RuntimeError(
-                        "Weight can only be set when DSO specified"
-                    )
-                if suite != "telemetry_Crosperf":
-                    raise RuntimeError(
-                        "CWP approximation weight only works with "
-                        "telemetry_Crosperf suite"
-                    )
-                if run_local:
-                    raise RuntimeError(
-                        "run_local must be set to False to use CWP "
-                        "approximation"
-                    )
-                if weight < 0:
-                    raise RuntimeError("Weight should be a float >=0")
-            elif cwp_dso:
-                raise RuntimeError(
-                    "With DSO specified, each benchmark should have a " "weight"
-                )
-
-            if suite == "telemetry_Crosperf":
-                if test_name == "all_crosbolt_perf":
-                    self.AppendBenchmarkSet(
-                        benchmarks,
-                        telemetry_crosbolt_perf_tests,
-                        test_args,
-                        iterations,
-                        rm_chroot_tmp,
-                        perf_args,
-                        "telemetry_Crosperf",
-                        show_all_results,
-                        retries,
-                        run_local,
-                        cwp_dso,
-                        weight,
-                    )
-                    self.AppendBenchmarkSet(
-                        benchmarks,
-                        crosbolt_perf_tests,
-                        "",
-                        iterations,
-                        rm_chroot_tmp,
-                        perf_args,
-                        "",
-                        show_all_results,
-                        retries,
-                        run_local=False,
-                        cwp_dso=cwp_dso,
-                        weight=weight,
-                    )
-                elif test_name == "all_toolchain_perf":
-                    self.AppendBenchmarkSet(
-                        benchmarks,
-                        telemetry_toolchain_perf_tests,
-                        test_args,
-                        iterations,
-                        rm_chroot_tmp,
-                        perf_args,
-                        suite,
-                        show_all_results,
-                        retries,
-                        run_local,
-                        cwp_dso,
-                        weight,
-                    )
-                    # Add non-telemetry toolchain-perf benchmarks:
-
-                    # TODO: crbug.com/1057755 Do not enable graphics_WebGLAquarium until
-                    # it gets fixed.
-                    #
-                    # benchmarks.append(
-                    #     Benchmark(
-                    #         'graphics_WebGLAquarium',
-                    #         'graphics_WebGLAquarium',
-                    #         '',
-                    #         iterations,
-                    #         rm_chroot_tmp,
-                    #         perf_args,
-                    #         'crosperf_Wrapper',  # Use client wrapper in Autotest
-                    #         show_all_results,
-                    #         retries,
-                    #         run_local=False,
-                    #         cwp_dso=cwp_dso,
-                    #         weight=weight))
-                else:
-                    benchmark = Benchmark(
-                        benchmark_name,
-                        test_name,
-                        test_args,
-                        iterations,
-                        rm_chroot_tmp,
-                        perf_args,
-                        suite,
-                        show_all_results,
-                        retries,
-                        run_local,
-                        cwp_dso,
-                        weight,
-                    )
-                    benchmarks.append(benchmark)
-            else:
-                if test_name == "all_graphics_perf":
-                    self.AppendBenchmarkSet(
-                        benchmarks,
-                        graphics_perf_tests,
-                        "",
-                        iterations,
-                        rm_chroot_tmp,
-                        perf_args,
-                        "",
-                        show_all_results,
-                        retries,
-                        run_local=False,
-                        cwp_dso=cwp_dso,
-                        weight=weight,
-                    )
-                else:
-                    # Add the single benchmark.
-                    benchmark = Benchmark(
-                        benchmark_name,
-                        test_name,
-                        test_args,
-                        iterations,
-                        rm_chroot_tmp,
-                        perf_args,
-                        suite,
-                        show_all_results,
-                        retries,
-                        run_local=False,
-                        cwp_dso=cwp_dso,
-                        weight=weight,
-                    )
-                    benchmarks.append(benchmark)
-
-        if not benchmarks:
-            raise RuntimeError("No benchmarks specified")
-
-        # Construct labels.
-        # Some fields are common with global settings. The values are
-        # inherited and/or merged with the global settings values.
-        labels = []
-        all_label_settings = experiment_file.GetSettings("label")
-        all_remote = list(remote)
-        for label_settings in all_label_settings:
-            label_name = label_settings.name
-            image = label_settings.GetField("chromeos_image")
-            build = label_settings.GetField("build")
-            autotest_path = label_settings.GetField("autotest_path")
-            debug_path = label_settings.GetField("debug_path")
-            chromeos_root = label_settings.GetField("chromeos_root")
-            my_remote = label_settings.GetField("remote")
-            compiler = label_settings.GetField("compiler")
-            new_remote = []
-            if my_remote:
-                for i in my_remote:
-                    c = re.sub("[\"']", "", i)
-                    new_remote.append(c)
-            my_remote = new_remote
-
-            if image:
-                if crosfleet:
-                    raise RuntimeError(
-                        "In crosfleet mode, local image should not be used."
-                    )
-                if build:
-                    raise RuntimeError(
-                        "Image path and build are provided at the same "
-                        "time, please use only one of them."
-                    )
-            else:
-                if not build:
-                    raise RuntimeError("Can not have empty 'build' field!")
-                image, autotest_path, debug_path = label_settings.GetXbuddyPath(
-                    build,
-                    autotest_path,
-                    debug_path,
-                    board,
-                    chromeos_root,
-                    log_level,
-                    download_debug,
-                )
-
-            cache_dir = label_settings.GetField("cache_dir")
-            chrome_src = label_settings.GetField("chrome_src")
-
-            # TODO(yunlian): We should consolidate code in machine_manager.py
-            # to derermine whether we are running from within google or not
-            if (
-                "corp.google.com" in socket.gethostname()
-                and not my_remote
-                and not crosfleet
-            ):
-                my_remote = self.GetDefaultRemotes(board)
-            if global_settings.GetField("same_machine") and len(my_remote) > 1:
-                raise RuntimeError(
-                    "Only one remote is allowed when same_machine "
-                    "is turned on"
-                )
-            all_remote += my_remote
-            image_args = label_settings.GetField("image_args")
-            if test_flag.GetTestMode():
-                # pylint: disable=too-many-function-args
-                label = MockLabel(
-                    label_name,
-                    build,
-                    image,
-                    autotest_path,
-                    debug_path,
-                    chromeos_root,
-                    board,
-                    my_remote,
-                    image_args,
-                    cache_dir,
-                    cache_only,
-                    log_level,
-                    compiler,
-                    crosfleet,
-                    chrome_src,
-                )
-            else:
-                label = Label(
-                    label_name,
-                    build,
-                    image,
-                    autotest_path,
-                    debug_path,
-                    chromeos_root,
-                    board,
-                    my_remote,
-                    image_args,
-                    cache_dir,
-                    cache_only,
-                    log_level,
-                    compiler,
-                    crosfleet,
-                    chrome_src,
-                )
-            labels.append(label)
-
-        if not labels:
-            raise RuntimeError("No labels specified")
-
-        email = global_settings.GetField("email")
-        all_remote += list(set(my_remote))
-        all_remote = list(set(all_remote))
-        if crosfleet:
-            for remote in all_remote:
-                self.CheckRemotesInCrosfleet(remote)
-        experiment = Experiment(
-            experiment_name,
-            all_remote,
-            working_directory,
-            chromeos_root,
-            cache_conditions,
-            labels,
-            benchmarks,
-            experiment_file.Canonicalize(),
-            email,
-            acquire_timeout,
-            log_dir,
-            log_level,
-            share_cache,
-            results_dir,
-            compress_results,
-            locks_dir,
-            cwp_dso,
-            ignore_min_max,
-            crosfleet,
-            dut_config,
-            keep_stateful,
-            no_lock=no_lock,
-        )
-
-        return experiment
-
-    def GetDefaultRemotes(self, board):
-        default_remotes_file = os.path.join(
-            os.path.dirname(__file__), "default_remotes"
-        )
-        try:
-            with open(default_remotes_file, encoding="utf-8") as f:
-                for line in f:
-                    key, v = line.split(":")
-                    if key.strip() == board:
-                        remotes = v.strip().split()
-                        if remotes:
-                            return remotes
-                        else:
-                            raise RuntimeError(
-                                f"There is no remote for {board}"
-                            )
-        except IOError:
-            # TODO: rethrow instead of throwing different exception.
-            raise RuntimeError(
-                f"IOError while reading file {default_remotes_file}"
-            )
-        else:
-            raise RuntimeError(f"There is no remote for {board}")
-
-    def CheckRemotesInCrosfleet(self, remote):
-        # TODO: (AI:zhizhouy) need to check whether a remote is a local or lab
-        # machine. If not lab machine, raise an error.
-        pass
-
-    def CheckCrosfleetTool(self, chromeos_root, log_level):
-        CROSFLEET_PATH = "crosfleet"
-        if os.path.exists(CROSFLEET_PATH):
-            return True
-        l = logger.GetLogger()
-        l.LogOutput("Crosfleet tool not installed, trying to install it.")
-        ce = command_executer.GetCommandExecuter(l, log_level=log_level)
-        setup_lab_tools = os.path.join(
-            chromeos_root, "chromeos-admin", "lab-tools", "setup_lab_tools"
-        )
-        cmd = "%s" % setup_lab_tools
-        status = ce.RunCommand(cmd)
-        if status != 0:
-            raise RuntimeError(
-                "Crosfleet tool not installed correctly, please try to "
-                "manually install it from %s" % setup_lab_tools
-            )
-        l.LogOutput(
-            "Crosfleet is installed at %s, please login before first use. "
-            'Login by running "crosfleet login" and follow instructions.'
-            % CROSFLEET_PATH
-        )
-        return False
diff --git a/crosperf/experiment_factory_unittest.py b/crosperf/experiment_factory_unittest.py
deleted file mode 100755
index 87e8c4f9..00000000
--- a/crosperf/experiment_factory_unittest.py
+++ /dev/null
@@ -1,578 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unit test for experiment_factory.py"""
-
-
-import io
-import os
-import socket
-import unittest
-import unittest.mock as mock
-
-import benchmark
-from cros_utils import command_executer
-from cros_utils.file_utils import FileUtils
-import experiment_factory
-from experiment_factory import ExperimentFactory
-from experiment_file import ExperimentFile
-from results_cache import CacheConditions
-import settings_factory
-import test_flag
-
-
-EXPERIMENT_FILE_1 = """
-  board: x86-alex
-  remote: chromeos-alex3
-  locks_dir: /tmp
-
-  benchmark: PageCycler {
-    iterations: 3
-  }
-
-  benchmark: webrtc {
-    iterations: 1
-    test_args: --story-filter=datachannel
-  }
-
-  image1 {
-    chromeos_image: /usr/local/google/cros_image1.bin
-  }
-
-  image2 {
-    chromeos_image: /usr/local/google/cros_image2.bin
-  }
-  """
-
-EXPERIMENT_FILE_2 = """
-  board: x86-alex
-  remote: chromeos-alex3
-  locks_dir: /tmp
-
-  cwp_dso: kallsyms
-
-  benchmark: Octane {
-    iterations: 1
-    suite: telemetry_Crosperf
-    run_local: False
-    weight: 0.8
-  }
-
-  benchmark: Kraken {
-    iterations: 1
-    suite: telemetry_Crosperf
-    run_local: False
-    weight: 0.2
-  }
-
-  image1 {
-    chromeos_image: /usr/local/google/cros_image1.bin
-  }
-  """
-
-# pylint: disable=too-many-function-args
-
-
-class ExperimentFactoryTest(unittest.TestCase):
-    """Class for running experiment factory unittests."""
-
-    def setUp(self):
-        self.append_benchmark_call_args = []
-
-    def testLoadExperimentFile1(self):
-        experiment_file = ExperimentFile(io.StringIO(EXPERIMENT_FILE_1))
-        exp = ExperimentFactory().GetExperiment(
-            experiment_file, working_directory="", log_dir=""
-        )
-        self.assertEqual(exp.remote, ["chromeos-alex3"])
-
-        self.assertEqual(len(exp.benchmarks), 2)
-        self.assertEqual(exp.benchmarks[0].name, "PageCycler")
-        self.assertEqual(exp.benchmarks[0].test_name, "PageCycler")
-        self.assertEqual(exp.benchmarks[0].iterations, 3)
-        self.assertEqual(exp.benchmarks[1].name, "webrtc@@datachannel")
-        self.assertEqual(exp.benchmarks[1].test_name, "webrtc")
-        self.assertEqual(exp.benchmarks[1].iterations, 1)
-
-        self.assertEqual(len(exp.labels), 2)
-        self.assertEqual(
-            exp.labels[0].chromeos_image, "/usr/local/google/cros_image1.bin"
-        )
-        self.assertEqual(exp.labels[0].board, "x86-alex")
-
-    def testLoadExperimentFile2CWP(self):
-        experiment_file = ExperimentFile(io.StringIO(EXPERIMENT_FILE_2))
-        exp = ExperimentFactory().GetExperiment(
-            experiment_file, working_directory="", log_dir=""
-        )
-        self.assertEqual(exp.cwp_dso, "kallsyms")
-        self.assertEqual(len(exp.benchmarks), 2)
-        self.assertEqual(exp.benchmarks[0].weight, 0.8)
-        self.assertEqual(exp.benchmarks[1].weight, 0.2)
-
-    def testDuplecateBenchmark(self):
-        mock_experiment_file = ExperimentFile(io.StringIO(EXPERIMENT_FILE_1))
-        mock_experiment_file.all_settings = []
-        benchmark_settings1 = settings_factory.BenchmarkSettings("name")
-        mock_experiment_file.all_settings.append(benchmark_settings1)
-        benchmark_settings2 = settings_factory.BenchmarkSettings("name")
-        mock_experiment_file.all_settings.append(benchmark_settings2)
-
-        with self.assertRaises(SyntaxError):
-            ef = ExperimentFactory()
-            ef.GetExperiment(mock_experiment_file, "", "")
-
-    def testCWPExceptions(self):
-        mock_experiment_file = ExperimentFile(io.StringIO(""))
-        mock_experiment_file.all_settings = []
-        global_settings = settings_factory.GlobalSettings("test_name")
-        global_settings.SetField("locks_dir", "/tmp")
-
-        # Test 1: DSO type not supported
-        global_settings.SetField("cwp_dso", "test")
-        self.assertEqual(global_settings.GetField("cwp_dso"), "test")
-        mock_experiment_file.global_settings = global_settings
-        with self.assertRaises(RuntimeError) as msg:
-            ef = ExperimentFactory()
-            ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertEqual(
-            "The DSO specified is not supported", str(msg.exception)
-        )
-
-        # Test 2: No weight after DSO specified
-        global_settings.SetField("cwp_dso", "kallsyms")
-        mock_experiment_file.global_settings = global_settings
-        benchmark_settings = settings_factory.BenchmarkSettings("name")
-        mock_experiment_file.all_settings.append(benchmark_settings)
-        with self.assertRaises(RuntimeError) as msg:
-            ef = ExperimentFactory()
-            ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertEqual(
-            "With DSO specified, each benchmark should have a weight",
-            str(msg.exception),
-        )
-
-        # Test 3: Weight is set, but no dso specified
-        global_settings.SetField("cwp_dso", "")
-        mock_experiment_file.global_settings = global_settings
-        benchmark_settings = settings_factory.BenchmarkSettings("name")
-        benchmark_settings.SetField("weight", "0.8")
-        mock_experiment_file.all_settings = []
-        mock_experiment_file.all_settings.append(benchmark_settings)
-        with self.assertRaises(RuntimeError) as msg:
-            ef = ExperimentFactory()
-            ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertEqual(
-            "Weight can only be set when DSO specified", str(msg.exception)
-        )
-
-        # Test 4: cwp_dso only works for telemetry_Crosperf benchmarks
-        global_settings.SetField("cwp_dso", "kallsyms")
-        mock_experiment_file.global_settings = global_settings
-        benchmark_settings = settings_factory.BenchmarkSettings("name")
-        benchmark_settings.SetField("weight", "0.8")
-        mock_experiment_file.all_settings = []
-        mock_experiment_file.all_settings.append(benchmark_settings)
-        with self.assertRaises(RuntimeError) as msg:
-            ef = ExperimentFactory()
-            ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertEqual(
-            "CWP approximation weight only works with "
-            "telemetry_Crosperf suite",
-            str(msg.exception),
-        )
-
-        # Test 5: cwp_dso does not work for local run
-        benchmark_settings = settings_factory.BenchmarkSettings("name")
-        benchmark_settings.SetField("weight", "0.8")
-        benchmark_settings.SetField("suite", "telemetry_Crosperf")
-        benchmark_settings.SetField("run_local", "True")
-        mock_experiment_file.all_settings = []
-        mock_experiment_file.all_settings.append(benchmark_settings)
-        with self.assertRaises(RuntimeError) as msg:
-            ef = ExperimentFactory()
-            ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertEqual(
-            "run_local must be set to False to use CWP approximation",
-            str(msg.exception),
-        )
-
-        # Test 6: weight should be float >=0
-        benchmark_settings = settings_factory.BenchmarkSettings("name")
-        benchmark_settings.SetField("weight", "-1.2")
-        benchmark_settings.SetField("suite", "telemetry_Crosperf")
-        benchmark_settings.SetField("run_local", "False")
-        mock_experiment_file.all_settings = []
-        mock_experiment_file.all_settings.append(benchmark_settings)
-        with self.assertRaises(RuntimeError) as msg:
-            ef = ExperimentFactory()
-            ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertEqual("Weight should be a float >=0", str(msg.exception))
-
-        # Test 7: more than one story tag in test_args
-        benchmark_settings = settings_factory.BenchmarkSettings("name")
-        benchmark_settings.SetField(
-            "test_args", "--story-filter=a --story-tag-filter=b"
-        )
-        benchmark_settings.SetField("weight", "1.2")
-        benchmark_settings.SetField("suite", "telemetry_Crosperf")
-        mock_experiment_file.all_settings = []
-        mock_experiment_file.all_settings.append(benchmark_settings)
-        with self.assertRaises(RuntimeError) as msg:
-            ef = ExperimentFactory()
-            ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertEqual(
-            "Only one story or story-tag filter allowed in a single "
-            "benchmark run",
-            str(msg.exception),
-        )
-
-        # Test 8: Iterations of each benchmark run are not same in cwp mode
-        mock_experiment_file.all_settings = []
-        benchmark_settings = settings_factory.BenchmarkSettings("name1")
-        benchmark_settings.SetField("iterations", "4")
-        benchmark_settings.SetField("weight", "1.2")
-        benchmark_settings.SetField("suite", "telemetry_Crosperf")
-        benchmark_settings.SetField("run_local", "False")
-        mock_experiment_file.all_settings.append(benchmark_settings)
-        benchmark_settings = settings_factory.BenchmarkSettings("name2")
-        benchmark_settings.SetField("iterations", "3")
-        benchmark_settings.SetField("weight", "1.2")
-        benchmark_settings.SetField("suite", "telemetry_Crosperf")
-        benchmark_settings.SetField("run_local", "False")
-        mock_experiment_file.all_settings.append(benchmark_settings)
-        with self.assertRaises(RuntimeError) as msg:
-            ef = ExperimentFactory()
-            ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertEqual(
-            "Iterations of each benchmark run are not the same",
-            str(msg.exception),
-        )
-
-    def test_append_benchmark_set(self):
-        ef = ExperimentFactory()
-
-        bench_list = []
-        ef.AppendBenchmarkSet(
-            bench_list,
-            experiment_factory.telemetry_crosbolt_perf_tests,
-            "",
-            1,
-            False,
-            "",
-            "telemetry_Crosperf",
-            False,
-            0,
-            False,
-            "",
-            0,
-        )
-        self.assertEqual(
-            len(bench_list),
-            len(experiment_factory.telemetry_crosbolt_perf_tests),
-        )
-        self.assertTrue(isinstance(bench_list[0], benchmark.Benchmark))
-
-        bench_list = []
-        ef.AppendBenchmarkSet(
-            bench_list,
-            experiment_factory.telemetry_toolchain_perf_tests,
-            "",
-            1,
-            False,
-            "",
-            "telemetry_Crosperf",
-            False,
-            0,
-            False,
-            "",
-            0,
-        )
-        self.assertEqual(
-            len(bench_list),
-            len(experiment_factory.telemetry_toolchain_perf_tests),
-        )
-        self.assertTrue(isinstance(bench_list[0], benchmark.Benchmark))
-
-        bench_list = []
-        ef.AppendBenchmarkSet(
-            bench_list,
-            experiment_factory.telemetry_toolchain_perf_tests,
-            "",
-            1,
-            False,
-            "",
-            "telemetry_Crosperf",
-            False,
-            0,
-            False,
-            "",
-            0,
-        )
-        self.assertEqual(
-            len(bench_list),
-            len(experiment_factory.telemetry_toolchain_perf_tests),
-        )
-        self.assertTrue(isinstance(bench_list[0], benchmark.Benchmark))
-
-    @mock.patch.object(socket, "gethostname")
-    def test_get_experiment(self, mock_socket):
-        test_flag.SetTestMode(False)
-        self.append_benchmark_call_args = []
-
-        def FakeAppendBenchmarkSet(
-            bench_list, set_list, args, iters, rm_ch, perf_args, suite, show_all
-        ):
-            "Helper function for test_get_experiment"
-            arg_list = [
-                bench_list,
-                set_list,
-                args,
-                iters,
-                rm_ch,
-                perf_args,
-                suite,
-                show_all,
-            ]
-            self.append_benchmark_call_args.append(arg_list)
-
-        def FakeGetDefaultRemotes(board):
-            if not board:
-                return []
-            return [
-                "fake_chromeos_machine1.cros",
-                "fake_chromeos_machine2.cros",
-            ]
-
-        def FakeGetXbuddyPath(
-            build, autotest_dir, debug_dir, board, chroot, log_level, perf_args
-        ):
-            autotest_path = autotest_dir
-            if not autotest_path:
-                autotest_path = "fake_autotest_path"
-            debug_path = debug_dir
-            if not debug_path and perf_args:
-                debug_path = "fake_debug_path"
-            if not build or not board or not chroot or not log_level:
-                return "", autotest_path, debug_path
-            return "fake_image_path", autotest_path, debug_path
-
-        ef = ExperimentFactory()
-        ef.AppendBenchmarkSet = FakeAppendBenchmarkSet
-        ef.GetDefaultRemotes = FakeGetDefaultRemotes
-
-        label_settings = settings_factory.LabelSettings("image_label")
-        benchmark_settings = settings_factory.BenchmarkSettings("bench_test")
-        global_settings = settings_factory.GlobalSettings("test_name")
-
-        label_settings.GetXbuddyPath = FakeGetXbuddyPath
-
-        mock_experiment_file = ExperimentFile(io.StringIO(""))
-        mock_experiment_file.all_settings = []
-
-        test_flag.SetTestMode(True)
-        # Basic test.
-        global_settings.SetField("name", "unittest_test")
-        global_settings.SetField("board", "lumpy")
-        global_settings.SetField("locks_dir", "/tmp")
-        global_settings.SetField("remote", "123.45.67.89 123.45.76.80")
-        benchmark_settings.SetField("test_name", "kraken")
-        benchmark_settings.SetField("suite", "telemetry_Crosperf")
-        benchmark_settings.SetField("iterations", 1)
-        label_settings.SetField(
-            "chromeos_image",
-            "chromeos/src/build/images/lumpy/latest/chromiumos_test_image.bin",
-        )
-        label_settings.SetField(
-            "chrome_src", "/usr/local/google/home/chrome-top"
-        )
-        label_settings.SetField("autotest_path", "/tmp/autotest")
-
-        mock_experiment_file.global_settings = global_settings
-        mock_experiment_file.all_settings.append(label_settings)
-        mock_experiment_file.all_settings.append(benchmark_settings)
-        mock_experiment_file.all_settings.append(global_settings)
-
-        mock_socket.return_value = ""
-
-        # First test. General test.
-        exp = ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertCountEqual(exp.remote, ["123.45.67.89", "123.45.76.80"])
-        self.assertEqual(exp.cache_conditions, [0, 2, 1])
-        self.assertEqual(exp.log_level, "average")
-
-        self.assertEqual(len(exp.benchmarks), 1)
-        self.assertEqual(exp.benchmarks[0].name, "bench_test")
-        self.assertEqual(exp.benchmarks[0].test_name, "kraken")
-        self.assertEqual(exp.benchmarks[0].iterations, 1)
-        self.assertEqual(exp.benchmarks[0].suite, "telemetry_Crosperf")
-        self.assertFalse(exp.benchmarks[0].show_all_results)
-
-        self.assertEqual(len(exp.labels), 1)
-        self.assertEqual(
-            exp.labels[0].chromeos_image,
-            "chromeos/src/build/images/lumpy/latest/"
-            "chromiumos_test_image.bin",
-        )
-        self.assertEqual(exp.labels[0].autotest_path, "/tmp/autotest")
-        self.assertEqual(exp.labels[0].board, "lumpy")
-        self.assertEqual(exp.machine_manager.keep_stateful, False)
-
-        # Second test: Remotes listed in labels.
-        test_flag.SetTestMode(True)
-        label_settings.SetField("remote", "chromeos1.cros chromeos2.cros")
-        # Also verify keep_stateful.
-        global_settings.SetField("keep_stateful", "true")
-        exp = ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertCountEqual(
-            exp.remote,
-            [
-                "123.45.67.89",
-                "123.45.76.80",
-                "chromeos1.cros",
-                "chromeos2.cros",
-            ],
-        )
-        # keep_stateful is propagated to machine_manager which flashes the
-        # images.
-        self.assertEqual(exp.machine_manager.keep_stateful, True)
-
-        # Third test: Automatic fixing of bad  logging_level param:
-        global_settings.SetField("logging_level", "really loud!")
-        exp = ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertEqual(exp.log_level, "verbose")
-
-        # Fourth test: Setting cache conditions; only 1 remote with "same_machine"
-        global_settings.SetField("rerun_if_failed", "true")
-        global_settings.SetField("rerun", "true")
-        global_settings.SetField("same_machine", "true")
-        global_settings.SetField("same_specs", "true")
-
-        self.assertRaises(
-            Exception, ef.GetExperiment, mock_experiment_file, "", ""
-        )
-        label_settings.SetField("remote", "")
-        global_settings.SetField("remote", "123.45.67.89")
-        exp = ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertEqual(
-            exp.cache_conditions,
-            [
-                CacheConditions.CACHE_FILE_EXISTS,
-                CacheConditions.CHECKSUMS_MATCH,
-                CacheConditions.RUN_SUCCEEDED,
-                CacheConditions.FALSE,
-                CacheConditions.SAME_MACHINE_MATCH,
-                CacheConditions.MACHINES_MATCH,
-            ],
-        )
-
-        # Check the alias option to ignore cache.
-        global_settings.SetField("rerun", "false")
-        global_settings.SetField("ignore_cache", "true")
-        exp = ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertEqual(
-            exp.cache_conditions,
-            [
-                CacheConditions.CACHE_FILE_EXISTS,
-                CacheConditions.CHECKSUMS_MATCH,
-                CacheConditions.RUN_SUCCEEDED,
-                CacheConditions.FALSE,
-                CacheConditions.SAME_MACHINE_MATCH,
-                CacheConditions.MACHINES_MATCH,
-            ],
-        )
-        # Check without cache use.
-        global_settings.SetField("rerun", "false")
-        global_settings.SetField("ignore_cache", "false")
-        exp = ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertEqual(
-            exp.cache_conditions,
-            [
-                CacheConditions.CACHE_FILE_EXISTS,
-                CacheConditions.CHECKSUMS_MATCH,
-                CacheConditions.RUN_SUCCEEDED,
-                CacheConditions.SAME_MACHINE_MATCH,
-                CacheConditions.MACHINES_MATCH,
-            ],
-        )
-
-        # Fifth Test: Adding a second label; calling GetXbuddyPath; omitting all
-        # remotes (Call GetDefaultRemotes).
-        mock_socket.return_value = "test.corp.google.com"
-        global_settings.SetField("remote", "")
-        global_settings.SetField("same_machine", "false")
-
-        label_settings_2 = settings_factory.LabelSettings(
-            "official_image_label"
-        )
-        label_settings_2.SetField("chromeos_root", "chromeos")
-        label_settings_2.SetField("build", "official-dev")
-        label_settings_2.SetField("autotest_path", "")
-        label_settings_2.GetXbuddyPath = FakeGetXbuddyPath
-
-        mock_experiment_file.all_settings.append(label_settings_2)
-        exp = ef.GetExperiment(mock_experiment_file, "", "")
-        self.assertEqual(len(exp.labels), 2)
-        self.assertEqual(exp.labels[1].chromeos_image, "fake_image_path")
-        self.assertEqual(exp.labels[1].autotest_path, "fake_autotest_path")
-        self.assertCountEqual(
-            exp.remote,
-            ["fake_chromeos_machine1.cros", "fake_chromeos_machine2.cros"],
-        )
-
-    def test_get_default_remotes(self):
-        board_list = [
-            "bob",
-            "chell",
-            "coral",
-            "elm",
-            "nautilus",
-            "snappy",
-        ]
-
-        ef = ExperimentFactory()
-        self.assertRaises(Exception, ef.GetDefaultRemotes, "bad-board")
-
-        # Verify that we have entries for every board
-        for b in board_list:
-            remotes = ef.GetDefaultRemotes(b)
-            self.assertGreaterEqual(len(remotes), 1)
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommand")
-    @mock.patch.object(os.path, "exists")
-    def test_check_crosfleet_tool(self, mock_exists, mock_runcmd):
-        ef = ExperimentFactory()
-        chromeos_root = "/tmp/chromeos"
-        log_level = "average"
-
-        mock_exists.return_value = True
-        ret = ef.CheckCrosfleetTool(chromeos_root, log_level)
-        self.assertTrue(ret)
-
-        mock_exists.return_value = False
-        mock_runcmd.return_value = 1
-        with self.assertRaises(RuntimeError) as err:
-            ef.CheckCrosfleetTool(chromeos_root, log_level)
-        self.assertEqual(mock_runcmd.call_count, 1)
-        self.assertEqual(
-            str(err.exception),
-            "Crosfleet tool not installed "
-            "correctly, please try to manually install it from "
-            "/tmp/chromeos/chromeos-admin/lab-tools/setup_lab_tools",
-        )
-
-        mock_runcmd.return_value = 0
-        mock_runcmd.call_count = 0
-        ret = ef.CheckCrosfleetTool(chromeos_root, log_level)
-        self.assertEqual(mock_runcmd.call_count, 1)
-        self.assertFalse(ret)
-
-
-if __name__ == "__main__":
-    FileUtils.Configure(True)
-    test_flag.SetTestMode(True)
-    unittest.main()
diff --git a/crosperf/experiment_file.py b/crosperf/experiment_file.py
deleted file mode 100644
index 70852a22..00000000
--- a/crosperf/experiment_file.py
+++ /dev/null
@@ -1,241 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2011 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""The experiment file module. It manages the input file of crosperf."""
-
-
-import os.path
-import re
-
-from settings_factory import SettingsFactory
-
-
-class ExperimentFile(object):
-    """Class for parsing the experiment file format.
-
-    The grammar for this format is:
-
-    experiment = { _FIELD_VALUE_RE | settings }
-    settings = _OPEN_SETTINGS_RE
-               { _FIELD_VALUE_RE }
-               _CLOSE_SETTINGS_RE
-
-    Where the regexes are terminals defined below. This results in an format
-    which looks something like:
-
-    field_name: value
-    settings_type: settings_name {
-      field_name: value
-      field_name: value
-    }
-    """
-
-    # Field regex, e.g. "iterations: 3"
-    _FIELD_VALUE_RE = re.compile(r"(\+)?\s*(\w+?)(?:\.(\S+))?\s*:\s*(.*)")
-    # Open settings regex, e.g. "label {"
-    _OPEN_SETTINGS_RE = re.compile(r"(?:([\w.-]+):)?\s*([\w.-]+)\s*{")
-    # Close settings regex.
-    _CLOSE_SETTINGS_RE = re.compile(r"}")
-
-    def __init__(self, experiment_file, overrides=None):
-        """Construct object from file-like experiment_file.
-
-        Args:
-          experiment_file: file-like object with text description of experiment.
-          overrides: A settings object that will override fields in other settings.
-
-        Raises:
-          Exception: if invalid build type or description is invalid.
-        """
-        self.all_settings = []
-        self.global_settings = SettingsFactory().GetSettings("global", "global")
-        self.all_settings.append(self.global_settings)
-
-        self._Parse(experiment_file)
-
-        for settings in self.all_settings:
-            settings.Inherit()
-            settings.Validate()
-            if overrides:
-                settings.Override(overrides)
-
-    def GetSettings(self, settings_type):
-        """Return nested fields from the experiment file."""
-        res = []
-        for settings in self.all_settings:
-            if settings.settings_type == settings_type:
-                res.append(settings)
-        return res
-
-    def GetGlobalSettings(self):
-        """Return the global fields from the experiment file."""
-        return self.global_settings
-
-    def _ParseField(self, reader):
-        """Parse a key/value field."""
-        line = reader.CurrentLine().strip()
-        match = ExperimentFile._FIELD_VALUE_RE.match(line)
-        append, name, _, text_value = match.groups()
-        return (name, text_value, append)
-
-    def _ParseSettings(self, reader):
-        """Parse a settings block."""
-        line = reader.CurrentLine().strip()
-        match = ExperimentFile._OPEN_SETTINGS_RE.match(line)
-        settings_type = match.group(1)
-        if settings_type is None:
-            settings_type = ""
-        settings_name = match.group(2)
-        settings = SettingsFactory().GetSettings(settings_name, settings_type)
-        settings.SetParentSettings(self.global_settings)
-
-        while reader.NextLine():
-            line = reader.CurrentLine().strip()
-
-            if not line:
-                continue
-
-            if ExperimentFile._FIELD_VALUE_RE.match(line):
-                field = self._ParseField(reader)
-                settings.SetField(field[0], field[1], field[2])
-            elif ExperimentFile._CLOSE_SETTINGS_RE.match(line):
-                return settings, settings_type
-
-        raise EOFError("Unexpected EOF while parsing settings block.")
-
-    def _Parse(self, experiment_file):
-        """Parse experiment file and create settings."""
-        reader = ExperimentFileReader(experiment_file)
-        settings_names = {}
-        try:
-            while reader.NextLine():
-                line = reader.CurrentLine().strip()
-
-                if not line:
-                    continue
-
-                if ExperimentFile._OPEN_SETTINGS_RE.match(line):
-                    new_settings, settings_type = self._ParseSettings(reader)
-                    # We will allow benchmarks with duplicated settings name for now.
-                    # Further decision will be made when parsing benchmark details in
-                    # ExperimentFactory.GetExperiment().
-                    if settings_type != "benchmark":
-                        if new_settings.name in settings_names:
-                            raise SyntaxError(
-                                "Duplicate settings name: '%s'."
-                                % new_settings.name
-                            )
-                        settings_names[new_settings.name] = True
-                    self.all_settings.append(new_settings)
-                elif ExperimentFile._FIELD_VALUE_RE.match(line):
-                    field = self._ParseField(reader)
-                    self.global_settings.SetField(field[0], field[1], field[2])
-                else:
-                    raise IOError("Unexpected line.")
-        except Exception as err:
-            raise RuntimeError(
-                "Line %d: %s\n==> %s"
-                % (reader.LineNo(), str(err), reader.CurrentLine(False))
-            )
-
-    def Canonicalize(self):
-        """Convert parsed experiment file back into an experiment file."""
-        res = ""
-        board = ""
-        for field_name in self.global_settings.fields:
-            field = self.global_settings.fields[field_name]
-            if field.assigned:
-                res += "%s: %s\n" % (field.name, field.GetString())
-            if field.name == "board":
-                board = field.GetString()
-        res += "\n"
-
-        for settings in self.all_settings:
-            if settings.settings_type != "global":
-                res += "%s: %s {\n" % (settings.settings_type, settings.name)
-                for field_name in settings.fields:
-                    field = settings.fields[field_name]
-                    if field.assigned:
-                        res += "\t%s: %s\n" % (field.name, field.GetString())
-                        if field.name == "chromeos_image":
-                            real_file = os.path.realpath(
-                                os.path.expanduser(field.GetString())
-                            )
-                            if real_file != field.GetString():
-                                res += "\t#actual_image: %s\n" % real_file
-                        if field.name == "build":
-                            chromeos_root_field = settings.fields[
-                                "chromeos_root"
-                            ]
-                            if chromeos_root_field:
-                                chromeos_root = chromeos_root_field.GetString()
-                            value = field.GetString()
-                            autotest_field = settings.fields["autotest_path"]
-                            autotest_path = ""
-                            if autotest_field.assigned:
-                                autotest_path = autotest_field.GetString()
-                            debug_field = settings.fields["debug_path"]
-                            debug_path = ""
-                            if debug_field.assigned:
-                                debug_path = autotest_field.GetString()
-                            # Do not download the debug symbols since this function is for
-                            # canonicalizing experiment file.
-                            downlad_debug = False
-                            (
-                                image_path,
-                                autotest_path,
-                                debug_path,
-                            ) = settings.GetXbuddyPath(
-                                value,
-                                autotest_path,
-                                debug_path,
-                                board,
-                                chromeos_root,
-                                "quiet",
-                                downlad_debug,
-                            )
-                            res += "\t#actual_image: %s\n" % image_path
-                            if not autotest_field.assigned:
-                                res += (
-                                    "\t#actual_autotest_path: %s\n"
-                                    % autotest_path
-                                )
-                            if not debug_field.assigned:
-                                res += "\t#actual_debug_path: %s\n" % debug_path
-
-                res += "}\n\n"
-
-        return res
-
-
-class ExperimentFileReader(object):
-    """Handle reading lines from an experiment file."""
-
-    def __init__(self, file_object):
-        self.file_object = file_object
-        self.current_line = None
-        self.current_line_no = 0
-
-    def CurrentLine(self, strip_comment=True):
-        """Return the next line from the file, without advancing the iterator."""
-        if strip_comment:
-            return self._StripComment(self.current_line)
-        return self.current_line
-
-    def NextLine(self, strip_comment=True):
-        """Advance the iterator and return the next line of the file."""
-        self.current_line_no += 1
-        self.current_line = self.file_object.readline()
-        return self.CurrentLine(strip_comment)
-
-    def _StripComment(self, line):
-        """Strip comments starting with # from a line."""
-        if "#" in line:
-            line = line[: line.find("#")] + line[-1]
-        return line
-
-    def LineNo(self):
-        """Return the current line number."""
-        return self.current_line_no
diff --git a/crosperf/experiment_file_unittest.py b/crosperf/experiment_file_unittest.py
deleted file mode 100755
index 5c09ee06..00000000
--- a/crosperf/experiment_file_unittest.py
+++ /dev/null
@@ -1,268 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-# Copyright 2011 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""The unittest of experiment_file."""
-
-import io
-import unittest
-
-from experiment_file import ExperimentFile
-
-
-EXPERIMENT_FILE_1 = """
-  board: x86-alex
-  remote: chromeos-alex3
-  perf_args: record -a -e cycles
-  benchmark: PageCycler {
-    iterations: 3
-  }
-
-  image1 {
-    chromeos_image: /usr/local/google/cros_image1.bin
-  }
-
-  image2 {
-    remote: chromeos-lumpy1
-    chromeos_image: /usr/local/google/cros_image2.bin
-  }
-  """
-
-EXPERIMENT_FILE_2 = """
-  board: x86-alex
-  remote: chromeos-alex3
-  iterations: 3
-
-  benchmark: PageCycler {
-  }
-
-  benchmark: AndroidBench {
-    iterations: 2
-  }
-
-  image1 {
-    chromeos_image:/usr/local/google/cros_image1.bin
-  }
-
-  image2 {
-    chromeos_image: /usr/local/google/cros_image2.bin
-  }
-  """
-
-EXPERIMENT_FILE_3 = """
-  board: x86-alex
-  remote: chromeos-alex3
-  iterations: 3
-
-  benchmark: PageCycler {
-  }
-
-  image1 {
-    chromeos_image:/usr/local/google/cros_image1.bin
-  }
-
-  image1 {
-    chromeos_image: /usr/local/google/cros_image2.bin
-  }
-  """
-
-EXPERIMENT_FILE_4 = """
-  board: x86-alex
-  remote: chromeos-alex3
-  iterations: 3
-
-  benchmark: webrtc {
-    test_args: --story-filter=datachannel
-  }
-
-  benchmark: webrtc {
-    test_args: --story-tag-filter=smoothness
-  }
-
-  image1 {
-    chromeos_image:/usr/local/google/cros_image1.bin
-  }
-  """
-
-DUT_CONFIG_EXPERIMENT_FILE_GOOD = """
-  board: kevin64
-  remote: chromeos-kevin.cros
-  turbostat: False
-  intel_pstate: no_hwp
-  cooldown_temp: 38
-  cooldown_time: 5
-  governor: powersave
-  cpu_usage: exclusive_cores
-  cpu_freq_pct: 50
-  top_interval: 5
-
-  benchmark: speedometer {
-    iterations: 3
-    suite: telemetry_Crosperf
-  }
-
-  image1 {
-    chromeos_image:/usr/local/google/cros_image1.bin
-  }
-  """
-
-DUT_CONFIG_EXPERIMENT_FILE_BAD_GOV = """
-  board: kevin64
-  remote: chromeos-kevin.cros
-  intel_pstate: active
-  governor: misspelled_governor
-
-  benchmark: speedometer2 {
-    iterations: 3
-    suite: telemetry_Crosperf
-  }
-  """
-
-DUT_CONFIG_EXPERIMENT_FILE_BAD_CPUUSE = """
-  board: kevin64
-  remote: chromeos-kevin.cros
-  turbostat: False
-  governor: ondemand
-  cpu_usage: unknown
-
-  benchmark: speedometer2 {
-    iterations: 3
-    suite: telemetry_Crosperf
-  }
-
-  image1 {
-    chromeos_image:/usr/local/google/cros_image1.bin
-  }
-  """
-
-OUTPUT_FILE = """board: x86-alex
-remote: chromeos-alex3
-perf_args: record -a -e cycles
-
-benchmark: PageCycler {
-\titerations: 3
-}
-
-label: image1 {
-\tchromeos_image: /usr/local/google/cros_image1.bin
-\tremote: chromeos-alex3
-}
-
-label: image2 {
-\tchromeos_image: /usr/local/google/cros_image2.bin
-\tremote: chromeos-lumpy1
-}\n\n"""
-
-
-class ExperimentFileTest(unittest.TestCase):
-    """The main class for Experiment File test."""
-
-    def testLoadExperimentFile1(self):
-        input_file = io.StringIO(EXPERIMENT_FILE_1)
-        experiment_file = ExperimentFile(input_file)
-        global_settings = experiment_file.GetGlobalSettings()
-        self.assertEqual(global_settings.GetField("remote"), ["chromeos-alex3"])
-        self.assertEqual(
-            global_settings.GetField("perf_args"), "record -a -e cycles"
-        )
-        benchmark_settings = experiment_file.GetSettings("benchmark")
-        self.assertEqual(len(benchmark_settings), 1)
-        self.assertEqual(benchmark_settings[0].name, "PageCycler")
-        self.assertEqual(benchmark_settings[0].GetField("iterations"), 3)
-
-        label_settings = experiment_file.GetSettings("label")
-        self.assertEqual(len(label_settings), 2)
-        self.assertEqual(label_settings[0].name, "image1")
-        self.assertEqual(
-            label_settings[0].GetField("chromeos_image"),
-            "/usr/local/google/cros_image1.bin",
-        )
-        self.assertEqual(
-            label_settings[1].GetField("remote"), ["chromeos-lumpy1"]
-        )
-        self.assertEqual(
-            label_settings[0].GetField("remote"), ["chromeos-alex3"]
-        )
-
-    def testOverrideSetting(self):
-        input_file = io.StringIO(EXPERIMENT_FILE_2)
-        experiment_file = ExperimentFile(input_file)
-        global_settings = experiment_file.GetGlobalSettings()
-        self.assertEqual(global_settings.GetField("remote"), ["chromeos-alex3"])
-
-        benchmark_settings = experiment_file.GetSettings("benchmark")
-        self.assertEqual(len(benchmark_settings), 2)
-        self.assertEqual(benchmark_settings[0].name, "PageCycler")
-        self.assertEqual(benchmark_settings[0].GetField("iterations"), 3)
-        self.assertEqual(benchmark_settings[1].name, "AndroidBench")
-        self.assertEqual(benchmark_settings[1].GetField("iterations"), 2)
-
-    def testDuplicateLabel(self):
-        input_file = io.StringIO(EXPERIMENT_FILE_3)
-        self.assertRaises(Exception, ExperimentFile, input_file)
-
-    def testDuplicateBenchmark(self):
-        input_file = io.StringIO(EXPERIMENT_FILE_4)
-        experiment_file = ExperimentFile(input_file)
-        benchmark_settings = experiment_file.GetSettings("benchmark")
-        self.assertEqual(benchmark_settings[0].name, "webrtc")
-        self.assertEqual(
-            benchmark_settings[0].GetField("test_args"),
-            "--story-filter=datachannel",
-        )
-        self.assertEqual(benchmark_settings[1].name, "webrtc")
-        self.assertEqual(
-            benchmark_settings[1].GetField("test_args"),
-            "--story-tag-filter=smoothness",
-        )
-
-    def testCanonicalize(self):
-        input_file = io.StringIO(EXPERIMENT_FILE_1)
-        experiment_file = ExperimentFile(input_file)
-        res = experiment_file.Canonicalize()
-        self.assertEqual(res, OUTPUT_FILE)
-
-    def testLoadDutConfigExperimentFile_Good(self):
-        input_file = io.StringIO(DUT_CONFIG_EXPERIMENT_FILE_GOOD)
-        experiment_file = ExperimentFile(input_file)
-        global_settings = experiment_file.GetGlobalSettings()
-        self.assertEqual(global_settings.GetField("turbostat"), False)
-        self.assertEqual(global_settings.GetField("intel_pstate"), "no_hwp")
-        self.assertEqual(global_settings.GetField("governor"), "powersave")
-        self.assertEqual(
-            global_settings.GetField("cpu_usage"), "exclusive_cores"
-        )
-        self.assertEqual(global_settings.GetField("cpu_freq_pct"), 50)
-        self.assertEqual(global_settings.GetField("cooldown_time"), 5)
-        self.assertEqual(global_settings.GetField("cooldown_temp"), 38)
-        self.assertEqual(global_settings.GetField("top_interval"), 5)
-
-    def testLoadDutConfigExperimentFile_WrongGovernor(self):
-        input_file = io.StringIO(DUT_CONFIG_EXPERIMENT_FILE_BAD_GOV)
-        with self.assertRaises(RuntimeError) as msg:
-            ExperimentFile(input_file)
-        self.assertRegex(str(msg.exception), "governor: misspelled_governor")
-        self.assertRegex(
-            str(msg.exception),
-            "Invalid enum value for field 'governor'."
-            r" Must be one of \(performance, powersave, userspace, ondemand,"
-            r" conservative, schedutils, sched, interactive\)",
-        )
-
-    def testLoadDutConfigExperimentFile_WrongCpuUsage(self):
-        input_file = io.StringIO(DUT_CONFIG_EXPERIMENT_FILE_BAD_CPUUSE)
-        with self.assertRaises(RuntimeError) as msg:
-            ExperimentFile(input_file)
-        self.assertRegex(str(msg.exception), "cpu_usage: unknown")
-        self.assertRegex(
-            str(msg.exception),
-            "Invalid enum value for field 'cpu_usage'."
-            r" Must be one of \(all, big_only, little_only, exclusive_cores\)",
-        )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/experiment_files/aes_perf.exp b/crosperf/experiment_files/aes_perf.exp
deleted file mode 100644
index 063c74be..00000000
--- a/crosperf/experiment_files/aes_perf.exp
+++ /dev/null
@@ -1,21 +0,0 @@
-# This experiment just runs a short autotest which measures the performance of
-# aes encryption.
-#
-# You should replace all the placeholders, marked by angle-brackets, with the
-# appropriate actual values.
-
-name: aes_example
-board: <your-board-goes-here>
-
-# Note:  You can specify multiple remotes, to run your tests in parallel on
-# multiple machines. e.g. "remote: test-machine-1.com test-machine2.come
-# test-machine3.com"
-remote: <your-remote-goes-here>
-
-benchmark: platform_AesThroughput {
-}
-
-# Replace the chromeos image below with the actual path to your test image.
-test_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/test-image/chromiumos_test_image.bin
-}
diff --git a/crosperf/experiment_files/bloat_perf.exp b/crosperf/experiment_files/bloat_perf.exp
deleted file mode 100644
index 14681778..00000000
--- a/crosperf/experiment_files/bloat_perf.exp
+++ /dev/null
@@ -1,25 +0,0 @@
-# This experiment just runs a short telemety autotest which measures
-# the performance of the page_cycler_v2.bloat test.
-#
-# You should replace all the placeholders, marked by angle-brackets, with the
-# appropriate actual values.
-
-name: bloat_perf_example
-board: <your-board-goes-here>
-
-# Note:  You can specify multiple remotes, to run your tests in parallel on
-# multiple machines. e.g. "remote: test-machine-1.com test-machine2.come
-# test-machine3.com"
-remote: <your-remote-goes-here>
-
-perf_args: record -e cycles
-
-benchmark: page_cycler_v2.bloat {
-	suite: telemetry_Crosperf
-	iterations:1
-}
-
-# Replace the chromeos image below with the actual path to your test image.
-test_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/test-image/chromiumos_test_image.bin
-}
diff --git a/crosperf/experiment_files/dut_config.exp b/crosperf/experiment_files/dut_config.exp
deleted file mode 100644
index fb81ba89..00000000
--- a/crosperf/experiment_files/dut_config.exp
+++ /dev/null
@@ -1,66 +0,0 @@
-# This experiment template shows how to run Telemetry tests (using autotest)
-# with explicitly specified DUT configurations.
-#
-# You should replace all the placeholders, marked by angle-brackets,
-# with the appropriate actual values.
-
-name: dut_config_telemetry_crosperf_example
-board: <your-board-goes-here>
-
-# Note:  You can specify multiple remotes, to run your tests in parallel on
-# multiple machines. e.g. "remote: test-machine-1.com test-machine2.come
-# test-machine3.com"
-remote: <your-remote-goes-here>
-
-# DUT configuration parameters. All are optional.
-#
-# Run turbostat process in background. Default: True.
-turbostat: <True|False>
-# Run top process in background with specified interval of sampling in
-# seconds, type float. 0 - don't run top.
-# Default: 0
-# Recommended values 1-5 (Lower number provides more accurate data).
-# NOTE: Running top with interval 1-5 sec has insignificant
-# performance impact (performance degradation does not exceed 0.3%).
-top_interval: <interval_in_seconds_float>
-# One of Intel Pstate modes defined in kernel command line:
-# active, passive, no_hwp.
-intel_pstate: <active|passive|no_hwp>
-# Wait until CPU cools down to a specified temperature
-# in Celsius or cooldown_time timeout reaches zero
-# (whichever happens first). Default: 40.
-cooldown_temp: <temperature-threshold-for-cooldown>
-# Timeout specified in minutes for CPU cooling down
-# to cooldown_temp temperature. Zero value disables cooldown.
-# Default: 0.
-cooldown_time: <time-to-cooldown-in-minutes>
-# CPU governor.
-# See: https://www.kernel.org/doc/Documentation/cpu-freq/governors.txt
-# for available values (they might differ for ARM and Intel).
-governor: <one-of-scaling_available_governors-values>
-# Restrict CPU usage to predefined "models":
-# all, big_only, little_only, exclusive_cores.
-cpu_usage: <usage-model>
-# Setup CPU frequency as percent of max_freq.
-# Default: 100
-cpu_freq_pct: <0-100>
-
-# The example below will run Telemetry toolchain performance benchmarks.
-# The exact list of benchmarks that will be run can be seen in
-# crosperf/experiment_factory.py
-benchmark: all_toolchain_perf {
-    suite: telemetry_Crosperf
-    run_local: False
-    iterations: 1
-}
-
-# NOTE: You must specify at least one image; you may specify more than one.
-# Replace <path-to-your-chroot-goes-here> and <board-goes-here> below.
-vanilla_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/vanilla-image/chromiumos_test_image.bin
-}
-
-# Replace the chromeos image below with the actual path to your test image.
-test_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/test-image/chromiumos_test_image.bin
-}
diff --git a/crosperf/experiment_files/enable_aslr.exp b/crosperf/experiment_files/enable_aslr.exp
deleted file mode 100644
index 5f8f654e..00000000
--- a/crosperf/experiment_files/enable_aslr.exp
+++ /dev/null
@@ -1,37 +0,0 @@
-# This example experiment file shows how to run a Telemetry test,
-# using autotest (via "suite: telemetry_Crosperf"), and also enable
-# ASLR. Note that ASLR is diabled by default
-# This turns on ASLR on the machine and runs the Telemetry's
-# "run_benchmark" for the specified test,
-#
-#
-# You should replace all the placeholders, marked by angle-brackets,
-# with the appropriate actual values.
-
-name: basic_telemetry_crosperf_example
-board: <your-board-goes-here>
-
-enable_aslr: True
-
-# Note:  You can specify multiple remotes, to run your tests in parallel on
-# multiple machines. e.g. "remote: test-machine-1.com test-machine2.come
-# test-machine3.com"
-remote: <your-remote-goes-here>
-
-# Replace "octane" below with the name of the Telemetry benchmark you 
-# want to run.
-benchmark: octane {
-    suite: telemetry_Crosperf
-    iterations: 1
-}
-
-# NOTE: You must specify at least one image; you may specify more than one.
-# Replace <path-to-your-chroot-goes-here> and <board-goes-here> below.
-vanilla_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/vanilla-image/chromiumos_test_image.bin
-}
-
-# Replace the chromeos image below with the actual path to your test image.
-test_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/test-image/chromiumos_test_image.bin
-}
diff --git a/crosperf/experiment_files/morejs_perf.exp b/crosperf/experiment_files/morejs_perf.exp
deleted file mode 100644
index ebc54753..00000000
--- a/crosperf/experiment_files/morejs_perf.exp
+++ /dev/null
@@ -1,25 +0,0 @@
-# This experiment just runs a short telemety autotest which measures
-# the performance of the page_cycler_v2.morejs test.
-#
-# You should replace all the placeholders, marked by angle-brackets, with the
-# appropriate actual values.
-
-name: morejs_perf_example
-board: <your-board-goes-here>
-
-# Note:  You can specify multiple remotes, to run your tests in parallel on
-# multiple machines. e.g. "remote: test-machine-1.com test-machine2.come
-# test-machine3.com"
-remote: <your-remote-goes-here>
-
-perf_args: record -e cycles
-
-benchmark: page_cycler_v2.morejs {
-	suite: telemetry_Crosperf
-	iterations: 1
-}
-
-# Replace the chromeos image below with the actual path to your test image.
-test_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/test-image/chromiumos_test_image.bin
-}
diff --git a/crosperf/experiment_files/non-telemetry-tests.exp b/crosperf/experiment_files/non-telemetry-tests.exp
deleted file mode 100644
index 0ad1fe5c..00000000
--- a/crosperf/experiment_files/non-telemetry-tests.exp
+++ /dev/null
@@ -1,31 +0,0 @@
-# This example experiment file showa how to run some basic non-Telemetry
-# autotest tests.
-#
-# You should replace all the placeholders, marked by angle-brackets,
-# with the appropriate actual values.
-
-name: non_telemetry_tests_example
-board: <your-board-goes-here>
-
-# Note:  You can specify multiple remotes, to run your tests in parallel on
-# multiple machines. e.g. "remote: test-machine-1.com test-machine2.come
-# test-machine3.com"
-remote: <your-remote-goes-here>
-
-benchmark: BootPerfServer {
-  test_name: BootPerfServer
-  iterations: 1
-}
-
-benchmark: bvt {
-  test_name: suite:bvt
-}
-
-benchmark: login_LoginSuccess {
-  test_name: login_LoginSuccess
-}
-
-# Replace the chromeos image below with the actual path to your test image.
-test_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/test-image/chromiumos_test_image.bin
-}
diff --git a/crosperf/experiment_files/official-image.exp b/crosperf/experiment_files/official-image.exp
deleted file mode 100644
index bce7d6a3..00000000
--- a/crosperf/experiment_files/official-image.exp
+++ /dev/null
@@ -1,41 +0,0 @@
-# This example experiment file shows how to run a basic test, using
-# official images.
-#
-# You should replace all the placeholders, marked by angle-brackets,
-# with the appropriate actual values.
-
-name: official_image_example
-
-board: <your-board-goes-here>
-
-# Note:  You can specify multiple remotes, to run your tests in parallel on
-# multiple machines. e.g. "remote: test-machine-1.com test-machine2.come
-# test-machine3.com"
-remote: <your-remote-goes-here>
-
-benchmark: canvasmark {
-  suite:telemetry_Crosperf
-  iterations: 1
-}
-
-
-# Replace <path-to-your-chroot-goes-here> with the actual directory path
-# to the top of your ChromimumOS chroot.
-first_official_image {
-  chromeos_root:<path-to-your-chroot-goes-here>
-  # Replace "latest-official" with the appropriate xbuddy version alias
-  # for the official image you want to use (see
-  # http://www.chromium.org/chromium-os/how-tos-and-troubleshooting/using-the-dev-server/xbuddy-for-devserver#TOC-XBuddy-Paths
-  # for xbuddy syntax).  
-  build: latest-official
-}
-
-second_official_image {
-  # Replace <path-to-your-chroot-goes-here> with actual path.
-  chromeos_root:<path-to-your-chroot-goes-here>
-  # Replace "lumpy-release/R35-5672.0.0" with the official image you want
-  # to use.
-  build:lumpy-release/R35-5672.0.0
-}
-
-
diff --git a/crosperf/experiment_files/page_cycler.exp b/crosperf/experiment_files/page_cycler.exp
deleted file mode 100644
index 6cb6166d..00000000
--- a/crosperf/experiment_files/page_cycler.exp
+++ /dev/null
@@ -1,28 +0,0 @@
-# This experiment file shows how to run all of the Telemetry
-# page_cycler tests.
-#
-# You should replace all the placeholders, marked by angle-brackets,
-# with the appropriate actual values.
-
-name: all_page_cyclers_example
-board: <your-board-goes-here>
-
-# Note: You can specify multiple remotes, to run your tests in
-# parallel on multiple machines. e.g. "remote: test-machine-1.com
-# test-machine2.come test-machine3.com"
-
-remote: <your-remote-goes-here>
-
-
-# NOTE: all_pagecyclers is a Crosperf alias that will cause all of the 
-# Telemetry page_cycler benchmark tests to be run. 
-benchmark: all_pagecyclers {
-	suite: telemetry_Crosperf
-	iterations: 2
-}
-
-# Replace the chromeos image below with the actual path to your test
-# image.
-test_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/test-image/chromiumos_test_image.bin
-}
diff --git a/crosperf/experiment_files/page_cycler_perf.exp b/crosperf/experiment_files/page_cycler_perf.exp
deleted file mode 100644
index cd661737..00000000
--- a/crosperf/experiment_files/page_cycler_perf.exp
+++ /dev/null
@@ -1,45 +0,0 @@
-# This experiment profiles some of the Telemetry page cycler tests,
-# uisng 'perf' on the remotes to get performance profiles.
-#
-# You should replace all the placeholders, marked by angle-brackets,
-# with the appropriate actual values.
-
-name: aes_example
-board: <your-board-goes-here>
-
-# Note:  You can specify multiple remotes, to run your tests in parallel on
-# multiple machines. e.g. "remote: test-machine-1.com test-machine2.come
-# test-machine3.com"
-remote: <your-remote-goes-here>
-
-perf_args: record -e cycles,instructions
-
-benchmark: page_cycler_v2.morejs {
-	suite: telemetry_Crosperf
-	iterations: 10
-}
-
-benchmark: page_cycler_v2.bloat {
-	suite: telemetry_Crosperf
-	iterations: 10
-}
-
-benchmark: page_cycler_v2.dhtml {
-	suite: telemetry_Crosperf
-	iterations: 10
-}
-
-benchmark: page_cycler_v2.intl_ar_fa_he {
-	suite: telemetry_Crosperf
-	iterations: 10
-}
-
-benchmark: page_cycler_v2.moz {
-	suite: telemetry_Crosperf
-	iterations: 10
-}
-
-# Replace the chromeos image below with the actual path to your test image.
-test_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/test-image/chromiumos_test_image.bin
-}
diff --git a/crosperf/experiment_files/telemetry-crosperf-suites.exp b/crosperf/experiment_files/telemetry-crosperf-suites.exp
deleted file mode 100644
index 2caa588d..00000000
--- a/crosperf/experiment_files/telemetry-crosperf-suites.exp
+++ /dev/null
@@ -1,54 +0,0 @@
-# This example experiment file shows how to invoke sets of tests (a
-# set is a group of tests that can be invoked by a single alias).
-# There are currently three sets defined for crosperf_Telemetry:
-# all_perfv2, all_pagecyclers, and all_toolchain_perf.
-#
-# You should replace all the placeholders, marked by angle-brackets,
-# with the appropriate actual values.
-
-
-name: telemetry_crosperf_suites_example
-board: <your-board-goes-here>
-
-# Note:  You can specify multiple remotes, to run your tests in parallel on
-# multiple machines. e.g. "remote: test-machine-1.com test-machine2.come
-# test-machine3.com"
-remote: <your-remote-goes-here>
-
-# The example below will run all the benchmarks in the perf_v2 suite.
-# The exact list of benchmarks that will be run can be seen in
-# crosperf/experiment_factory.py
-benchmark: all_perfv2 {
-  suite:telemetry_Crosperf
-  iterations: 2
-}
-
-# The example below will run all the Telemetry page_cycler benchmarks.
-# The exact list of benchmarks that will be run can be seen in
-# crosperf/experiment_factory.py
-benchmark: all_pagecyclers {
-  suite:telemetry_Crosperf
-  iterations: 1
-}
-
-# The example below will run all the Telemetry page_cycler benchmarks.
-# The exact list of benchmarks that will be run can be seen in
-# crosperf/experiment_factory.py
-benchmark: all_toolchain_perf {
-  suite:telemetry_Crosperf
-  iterations: 1
-}
-
-# Replace the chromeos image below with the actual path to your test image.
-test_image_1 {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/test-image/chromiumos_test_image.bin
-}
-
-# Replace the chromeos image below with the actual path to your second
-# test image (if desired).
-new_image {
-  chromeos_image:<path-to-your-other-chroot-goes-here>/src/build/images/<board-goes-here>/latest/chromiumos_test_image.bin
-}
-
-
-
diff --git a/crosperf/experiment_files/telemetry-crosperf-with-external-chrome-src.exp b/crosperf/experiment_files/telemetry-crosperf-with-external-chrome-src.exp
deleted file mode 100644
index 517c13f1..00000000
--- a/crosperf/experiment_files/telemetry-crosperf-with-external-chrome-src.exp
+++ /dev/null
@@ -1,31 +0,0 @@
-# This example experiment file showings how to specify an external
-# chrome source tree (rather than using the one inside the chroot).
-# The Telemetry tests will be run from the external Chrome source
-# tree.
-#
-# You should replace all the placeholders, marked by angle-brackets,
-# with the appropriate actual values.
-
-name: telemetry_crosperf_external_src_example
-
-board: <your-board-goes-here>
-
-# Note:  You can specify multiple remotes, to run your tests in parallel on
-# multiple machines. e.g. "remote: test-machine-1.com test-machine2.come
-# test-machine3.com"
-remote: <your-remote-goes-here>
-
-benchmark: octane {
-    suite: telemetry_Crosperf
-    iterations: 1
-}
-
-# Replace the chromeos image below with the actual path to your test imnage.
-test_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/test-image/chromiumos_test_image.bin
-  # Replace '/usr/local/google/chrome-top' with the path to the
-  # top of your Chrome source tree. From that directory
-  # "./src/tools/perf/run_benchmark" should be a valid file path.
-  chrome_src:/usr/local/google/chrome-top
-}
-
diff --git a/crosperf/experiment_files/telemetry-crosperf-with-profiler.exp b/crosperf/experiment_files/telemetry-crosperf-with-profiler.exp
deleted file mode 100644
index 4c2b88fc..00000000
--- a/crosperf/experiment_files/telemetry-crosperf-with-profiler.exp
+++ /dev/null
@@ -1,35 +0,0 @@
-# This example experiment file shows how to invoke the profiler (via
-# the perf_args above the benchmark).
-#
-# You should replace all the placeholders, marked by angle-brackets,
-# with the appropriate actual values.
-
-
-name: telemetry_crosperf_profiler_example
-
-board: <your-board-goes-here>
-
-# Note:  You can specify multiple remotes, to run your tests in parallel on
-# multiple machines. e.g. "remote: test-machine-1.com test-machine2.come
-# test-machine3.com"
-remote: <your-remote-goes-here>
-
-# Below is the line that causes the profiler to run.  Currently the
-# only profiler option is running 'perf' on the remote machine.  If
-# you want you can replace 'record' with 'stat'.  You would also need
-# to change the other args accordingly.  Crosperf automatically
-# inserts a '-a' if you use 'record' for you perf_args.  The results
-# of the perf run (perf.data and perf.report files) will be available
-# with the rest of the Crosperf results.
-perf_args: record -e cycles,instructions
-
-benchmark: page_cycler_v2.dhtml {
-    suite: telemetry_Crosperf
-    iterations: 1
-}
-
-# Replace the chromeos image below with the actual path to your test imnage.
-test_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/test-image/chromiumos_test_image.bin
-}
-
diff --git a/crosperf/experiment_files/telemetry-crosperf.exp b/crosperf/experiment_files/telemetry-crosperf.exp
deleted file mode 100644
index 111001d4..00000000
--- a/crosperf/experiment_files/telemetry-crosperf.exp
+++ /dev/null
@@ -1,32 +0,0 @@
-# This example experiment file shows how to run a Telemetry test,
-# using autotest (via "suite: telemetry_Crosperf").  This runs the
-# Telemetry's "run_benchmark" for the specified test.
-#
-# You should replace all the placeholders, marked by angle-brackets,
-# with the appropriate actual values.
-
-name: basic_telemetry_crosperf_example
-board: <your-board-goes-here>
-
-# Note:  You can specify multiple remotes, to run your tests in parallel on
-# multiple machines. e.g. "remote: test-machine-1.com test-machine2.come
-# test-machine3.com"
-remote: <your-remote-goes-here>
-
-# Replace "octane" below with the name of the Telemetry benchmark you 
-# want to run.
-benchmark: octane {
-    suite: telemetry_Crosperf
-    iterations: 1
-}
-
-# NOTE: You must specify at least one image; you may specify more than one.
-# Replace <path-to-your-chroot-goes-here> and <board-goes-here> below.
-vanilla_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/vanilla-image/chromiumos_test_image.bin
-}
-
-# Replace the chromeos image below with the actual path to your test image.
-test_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/test-image/chromiumos_test_image.bin
-}
diff --git a/crosperf/experiment_files/telemetry-without-autotest.exp b/crosperf/experiment_files/telemetry-without-autotest.exp
deleted file mode 100644
index ce3f207e..00000000
--- a/crosperf/experiment_files/telemetry-without-autotest.exp
+++ /dev/null
@@ -1,31 +0,0 @@
-# This example experiment file shows how to run a Telemetry test
-# directly, bypassing autotest.  This runs the "run_measurement"
-# script.  You need to supply both the name of the Telemetry test and
-# the page_set (via the test_args argument).
-#
-# You should replace all the placeholders, marked by angle-brackets,
-# with the appropriate actual values.
-
-name: telemetry_without_autotest_example
-board: <your-board-goes-here>
-
-# Note:  You can specify multiple remotes, to run your tests in parallel on
-# multiple machines. e.g. "remote: test-machine-1.com test-machine2.come
-# test-machine3.com"
-remote: <your-remote-goes-here>
-
-# Replace "page_cycler_dhtml" below with the name of the Telemetry test
-# that you want run_measurement to run.  Also replace the page set below
-# (in the test_args field) with the appropriate page set for your test.
-# N.B.  The key to running telemetry without autotest is the 'suite' field.
-# Make sure your suite is 'telemtry', NOT 'telemetry_Crosperf'.
-benchmark: page_cycler_dhtml {
-    suite: telemetry 
-    iterations: 1
-    test_args: ./page_sets/page_cycler/dhtml.json
-}
-
-# Replace the chromeos image below with the actual path to your test image.
-test_image {
-  chromeos_image:<path-to-your-chroot>/src/build/images/<board>/test-image/chromiumos_test_image.bin
-}
diff --git a/crosperf/experiment_files/telemetry_perf_perf b/crosperf/experiment_files/telemetry_perf_perf
deleted file mode 100755
index e46fdc2a..00000000
--- a/crosperf/experiment_files/telemetry_perf_perf
+++ /dev/null
@@ -1,76 +0,0 @@
-#!/bin/bash
-# Copyright 2016 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-#
-# Script for generating and running telemetry benchmarkes via crosperf with
-# different perf command lines in order to measure the impact of the perf
-# commands on performance. Crosperf cannot run the same benchmark multiple
-# times, so this script runs crosperf multpilpe times instead. Unfortunately,
-# this means you must compare the results yourself.
-#
-# Perf will run for the entire benchmark run, so results should be interpreted
-# in that context. i.e, if this shows a 3% overhead for a particular perf
-# command, that overhead would only be seen during the 2 seconds of measurement
-# during a ChromeOS Wide Profiling collection.
-set -e
-
-board=xxx #<you-board-here>
-remote=xxx #<your-remote-here>
-iterations=5
-chromeos_root=~/chromiumos
-chrome_src=~/chromium
-
-
-function GenerateExperiment() {
-  local perf_args="${1:+perf_args: $1}"
-  local track="$2"  # stable, beta, dev
-
-  cat <<_EOF
-$perf_args
-benchmark: page_cycler_v2.typical_25 {
-    suite: telemetry_Crosperf
-}
-
-$track {
-  build: latest-$track
-}
-_EOF
-}
-
-function RunExperiment() {
-  local name="$1"
-  local perf_command="$2"
-  GenerateExperiment "$perf_command" "stable" > /tmp/crosperf.exp
-  ./crosperf /tmp/crosperf.exp \
-    --name telemetry_perf_perf_${name} \
-    --board="${board}" \
-    --remote="${remote}" \
-    --iterations="${iterations}" \
-    --chromeos_root="${chromeos_root}" \
-    --chrome_src="${chrome_src}" \
-    --rerun=true \
-    --use_file_locks=true \
-    --locks_dir=/tmp/crosperf.locks
-}
-
-if [ "$board" = "xxx" -o "$remote" = "xxx" ]; then
-  echo "Please set board and remote at the top of this script before running."
-  exit -1
-fi
-
-
-# Note that "-a" is automatically inserted in the perf command line.
-
-# Control: No profiling.
-RunExperiment 'control' ''
-# This is our baseline standard 'cycles' perf command.
-RunExperiment 'cycles.flat' \
-  'record -e cycles -c 1000003'
-# Callgraph profiling.
-RunExperiment 'cycles.callgraph' \
-  'record -g -e cycles -c 4000037'
-# Memory bandwidth profiling. As a perf stat command, we expect imperceptible
-# overhead.
-RunExperiment 'memory.bandwidth' \
-  'stat -e cycles -e instructions -e uncore_imc/data_reads/ -e uncore_imc/data_writes/ -e cpu/event=0xD0,umask=0x11,name=MEM_UOPS_RETIRED-STLB_MISS_LOADS/ -e cpu/event=0xD0,umask=0x12,name=MEM_UOPS_RETIRED-STLB_MISS_STORES/'
diff --git a/crosperf/experiment_files/trybot-image.exp b/crosperf/experiment_files/trybot-image.exp
deleted file mode 100644
index a261e08c..00000000
--- a/crosperf/experiment_files/trybot-image.exp
+++ /dev/null
@@ -1,33 +0,0 @@
-# This example experiment shows how to run a basic test, using a
-# (previously made) trybot image.
-
-#
-# You should replace all the placeholders, marked by angle-brackets,
-# with the appropriate actual values.
-
-name: trybot_example
-board: <your-board-goes-here>
-
-# Note:  You can specify multiple remotes, to run your tests in parallel on
-# multiple machines. e.g. "remote: test-machine-1.com test-machine2.come
-# test-machine3.com"
-remote: <your-remote-goes-here>
-
-
-benchmark: canvasmark {
-  suite:telemetry_Crosperf
-  iterations: 1
-}
-
-
-# Replace <path-to-your-chroot-goes-here> with the actual directory path
-# to the top of your ChromimumOS chroot.
-trybot_image {
-  chromeos_root:<path-to-your-chroot-goes-here>
-  # Replace "trybot-lumpy-paladin/R34-5417.0.0-b1506" with the name of the
-  # trybot image that you wish to use.  You can find this by going to the
-  # trybot build log, going to the 'Report' stage, and looking for 'Build
-  # Artifacts' at the bottom. You can extract the trybot image name from that.
-  build:trybot-lumpy-paladin/R34-5417.0.0-b1506
-}
-
diff --git a/crosperf/experiment_runner.py b/crosperf/experiment_runner.py
deleted file mode 100644
index c41459a1..00000000
--- a/crosperf/experiment_runner.py
+++ /dev/null
@@ -1,412 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2011 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""The experiment runner module."""
-
-import getpass
-import os
-import shutil
-import time
-
-from cros_utils import command_executer
-from cros_utils import logger
-from cros_utils.email_sender import EmailSender
-from cros_utils.file_utils import FileUtils
-from experiment_status import ExperimentStatus
-import lock_machine
-from results_cache import CacheConditions
-from results_cache import ResultsCache
-from results_report import HTMLResultsReport
-from results_report import JSONResultsReport
-from results_report import TextResultsReport
-from schedv2 import Schedv2
-import test_flag
-
-import config
-
-
-def _WriteJSONReportToFile(experiment, results_dir, json_report):
-    """Writes a JSON report to a file in results_dir."""
-    has_llvm = any("llvm" in l.compiler for l in experiment.labels)
-    compiler_string = "llvm" if has_llvm else "gcc"
-    board = experiment.labels[0].board
-    filename = "report_%s_%s_%s.%s.json" % (
-        board,
-        json_report.date,
-        json_report.time.replace(":", "."),
-        compiler_string,
-    )
-    fullname = os.path.join(results_dir, filename)
-    report_text = json_report.GetReport()
-    with open(fullname, "w") as out_file:
-        out_file.write(report_text)
-
-
-class ExperimentRunner(object):
-    """ExperimentRunner Class."""
-
-    STATUS_TIME_DELAY = 30
-    THREAD_MONITOR_DELAY = 2
-
-    SUCCEEDED = 0
-    HAS_FAILURE = 1
-    ALL_FAILED = 2
-
-    def __init__(
-        self,
-        experiment,
-        json_report,
-        using_schedv2=False,
-        log=None,
-        cmd_exec=None,
-    ):
-        self._experiment = experiment
-        self.l = log or logger.GetLogger(experiment.log_dir)
-        self._ce = cmd_exec or command_executer.GetCommandExecuter(self.l)
-        self._terminated = False
-        self.json_report = json_report
-        self.locked_machines = []
-        if experiment.log_level != "verbose":
-            self.STATUS_TIME_DELAY = 10
-
-        # Setting this to True will use crosperf sched v2 (feature in progress).
-        self._using_schedv2 = using_schedv2
-
-    def _GetMachineList(self):
-        """Return a list of all requested machines.
-
-        Create a list of all the requested machines, both global requests and
-        label-specific requests, and return the list.
-        """
-        machines = self._experiment.remote
-        # All Label.remote is a sublist of experiment.remote.
-        for l in self._experiment.labels:
-            for r in l.remote:
-                assert r in machines
-        return machines
-
-    def _UpdateMachineList(self, locked_machines):
-        """Update machines lists to contain only locked machines.
-
-        Go through all the lists of requested machines, both global and
-        label-specific requests, and remove any machine that we were not
-        able to lock.
-
-        Args:
-          locked_machines: A list of the machines we successfully locked.
-        """
-        for m in self._experiment.remote:
-            if m not in locked_machines:
-                self._experiment.remote.remove(m)
-
-        for l in self._experiment.labels:
-            for m in l.remote:
-                if m not in locked_machines:
-                    l.remote.remove(m)
-
-    def _GetMachineType(self, lock_mgr, machine):
-        """Get where is the machine from.
-
-        Returns:
-          The location of the machine: local or crosfleet
-        """
-        # We assume that lab machine always starts with chromeos*, and local
-        # machines are ip address.
-        if "chromeos" in machine:
-            if lock_mgr.CheckMachineInCrosfleet(machine):
-                return "crosfleet"
-            else:
-                raise RuntimeError("Lab machine not in Crosfleet.")
-        return "local"
-
-    def _LockAllMachines(self, experiment):
-        """Attempt to globally lock all of the machines requested for run.
-
-        This method tries to lock all machines requested for this crosperf run
-        in three different modes automatically, to prevent any other crosperf runs
-        from being able to update/use the machines while this experiment is
-        running:
-          - Crosfleet machines: Use crosfleet lease-dut mechanism to lease
-          - Local machines: Use file lock mechanism to lock
-        """
-        if test_flag.GetTestMode():
-            self.locked_machines = self._GetMachineList()
-            experiment.locked_machines = self.locked_machines
-        else:
-            experiment.lock_mgr = lock_machine.LockManager(
-                self._GetMachineList(),
-                "",
-                experiment.labels[0].chromeos_root,
-                experiment.locks_dir,
-                log=self.l,
-            )
-            for m in experiment.lock_mgr.machines:
-                machine_type = self._GetMachineType(experiment.lock_mgr, m)
-                if machine_type == "local":
-                    experiment.lock_mgr.AddMachineToLocal(m)
-                elif machine_type == "crosfleet":
-                    experiment.lock_mgr.AddMachineToCrosfleet(m)
-            machine_states = experiment.lock_mgr.GetMachineStates("lock")
-            experiment.lock_mgr.CheckMachineLocks(machine_states, "lock")
-            self.locked_machines = experiment.lock_mgr.UpdateMachines(True)
-            experiment.locked_machines = self.locked_machines
-            self._UpdateMachineList(self.locked_machines)
-            experiment.machine_manager.RemoveNonLockedMachines(
-                self.locked_machines
-            )
-            if not self.locked_machines:
-                raise RuntimeError("Unable to lock any machines.")
-
-    def _ClearCacheEntries(self, experiment):
-        for br in experiment.benchmark_runs:
-            cache = ResultsCache()
-            cache.Init(
-                br.label.chromeos_image,
-                br.label.chromeos_root,
-                br.benchmark.test_name,
-                br.iteration,
-                br.test_args,
-                br.profiler_args,
-                br.machine_manager,
-                br.machine,
-                br.label.board,
-                br.cache_conditions,
-                br.logger(),
-                br.log_level,
-                br.label,
-                br.share_cache,
-                br.benchmark.suite,
-                br.benchmark.show_all_results,
-                br.benchmark.run_local,
-                br.benchmark.cwp_dso,
-            )
-            cache_dir = cache.GetCacheDirForWrite()
-            if os.path.exists(cache_dir):
-                self.l.LogOutput("Removing cache dir: %s" % cache_dir)
-                shutil.rmtree(cache_dir)
-
-    def _Run(self, experiment):
-        try:
-            # We should not lease machines if tests are launched via `crosfleet
-            # create-test`. This is because leasing DUT in crosfleet will create a
-            # no-op task on the DUT and new test created will be hanging there.
-            # TODO(zhizhouy): Need to check whether machine is ready or not before
-            # assigning a test to it.
-            if not experiment.no_lock and not experiment.crosfleet:
-                self._LockAllMachines(experiment)
-            # Calculate all checksums of avaiable/locked machines, to ensure same
-            # label has same machines for testing
-            experiment.SetCheckSums(forceSameImage=True)
-            if self._using_schedv2:
-                schedv2 = Schedv2(experiment)
-                experiment.set_schedv2(schedv2)
-            if CacheConditions.FALSE in experiment.cache_conditions:
-                self._ClearCacheEntries(experiment)
-            status = ExperimentStatus(experiment)
-            experiment.Run()
-            last_status_time = 0
-            last_status_string = ""
-            try:
-                if experiment.log_level != "verbose":
-                    self.l.LogStartDots()
-                while not experiment.IsComplete():
-                    if last_status_time + self.STATUS_TIME_DELAY < time.time():
-                        last_status_time = time.time()
-                        border = "=============================="
-                        if experiment.log_level == "verbose":
-                            self.l.LogOutput(border)
-                            self.l.LogOutput(status.GetProgressString())
-                            self.l.LogOutput(status.GetStatusString())
-                            self.l.LogOutput(border)
-                        else:
-                            current_status_string = status.GetStatusString()
-                            if current_status_string != last_status_string:
-                                self.l.LogEndDots()
-                                self.l.LogOutput(border)
-                                self.l.LogOutput(current_status_string)
-                                self.l.LogOutput(border)
-                                last_status_string = current_status_string
-                            else:
-                                self.l.LogAppendDot()
-                    time.sleep(self.THREAD_MONITOR_DELAY)
-            except KeyboardInterrupt:
-                self._terminated = True
-                self.l.LogError("Ctrl-c pressed. Cleaning up...")
-                experiment.Terminate()
-                raise
-            except SystemExit:
-                self._terminated = True
-                self.l.LogError("Unexpected exit. Cleaning up...")
-                experiment.Terminate()
-                raise
-        finally:
-            experiment.Cleanup()
-
-    def _PrintTable(self, experiment):
-        self.l.LogOutput(
-            TextResultsReport.FromExperiment(experiment).GetReport()
-        )
-
-    def _Email(self, experiment):
-        # Only email by default if a new run was completed.
-        send_mail = False
-        for benchmark_run in experiment.benchmark_runs:
-            if not benchmark_run.cache_hit:
-                send_mail = True
-                break
-        if (
-            not send_mail
-            and not experiment.email_to
-            or config.GetConfig("no_email")
-        ):
-            return
-
-        label_names = []
-        for label in experiment.labels:
-            label_names.append(label.name)
-        subject = "%s: %s" % (experiment.name, " vs. ".join(label_names))
-
-        text_report = TextResultsReport.FromExperiment(
-            experiment, True
-        ).GetReport()
-        text_report += (
-            "\nResults are stored in %s.\n" % experiment.results_directory
-        )
-        text_report = "<pre style='font-size: 13px'>%s</pre>" % text_report
-        html_report = HTMLResultsReport.FromExperiment(experiment).GetReport()
-        attachment = EmailSender.Attachment("report.html", html_report)
-        email_to = experiment.email_to or []
-        email_to.append(getpass.getuser())
-        EmailSender().SendEmail(
-            email_to,
-            subject,
-            text_report,
-            attachments=[attachment],
-            msg_type="html",
-        )
-
-    def _StoreResults(self, experiment):
-        if self._terminated:
-            return self.ALL_FAILED
-
-        results_directory = experiment.results_directory
-        FileUtils().RmDir(results_directory)
-        FileUtils().MkDirP(results_directory)
-        self.l.LogOutput("Storing experiment file in %s." % results_directory)
-        experiment_file_path = os.path.join(results_directory, "experiment.exp")
-        FileUtils().WriteFile(experiment_file_path, experiment.experiment_file)
-
-        all_failed = True
-
-        topstats_file = os.path.join(results_directory, "topstats.log")
-        self.l.LogOutput(
-            "Storing top statistics of each benchmark run into %s."
-            % topstats_file
-        )
-        # Track if any iterations for a given benchmark has passed for each
-        # label.
-        benchmarks_passes = {}
-        with open(topstats_file, "w") as top_fd:
-            for benchmark_run in experiment.benchmark_runs:
-                benchmarks_passes.setdefault(
-                    benchmark_run.label.name,
-                    {benchmark_run.benchmark.name: False},
-                )
-                if benchmark_run.result:
-                    if not benchmark_run.result.retval:
-                        all_failed = False
-                        benchmarks_passes[benchmark_run.label.name][
-                            benchmark_run.benchmark.name
-                        ] = True
-                    # Header with benchmark run name.
-                    top_fd.write("%s\n" % str(benchmark_run))
-                    # Formatted string with top statistics.
-                    top_fd.write(benchmark_run.result.FormatStringTopCommands())
-                    top_fd.write("\n\n")
-
-        if all_failed:
-            return self.ALL_FAILED
-        # Set has_passes if atleast one iteration of all benchmarks has passed
-        # for every label.
-        has_passes = True
-        for benchmarks in benchmarks_passes.values():
-            has_passes = has_passes and all(benchmarks.values())
-
-        self.l.LogOutput("Storing results of each benchmark run.")
-        for benchmark_run in experiment.benchmark_runs:
-            if benchmark_run.result:
-                benchmark_run_name = "".join(
-                    ch for ch in benchmark_run.name if ch.isalnum()
-                )
-                benchmark_run_path = os.path.join(
-                    results_directory, benchmark_run_name
-                )
-                if experiment.compress_results:
-                    benchmark_run.result.CompressResultsTo(benchmark_run_path)
-                else:
-                    benchmark_run.result.CopyResultsTo(benchmark_run_path)
-                # Don't remove benchmark tmp if it was a cache hit.
-                benchmark_run.result.CleanUp(
-                    benchmark_run.benchmark.rm_chroot_tmp
-                    and not benchmark_run.cache_hit
-                )
-
-        self.l.LogOutput("Storing results report in %s." % results_directory)
-        results_table_path = os.path.join(results_directory, "results.html")
-        report = HTMLResultsReport.FromExperiment(experiment).GetReport()
-        if self.json_report:
-            json_report = JSONResultsReport.FromExperiment(
-                experiment, json_args={"indent": 2}
-            )
-            _WriteJSONReportToFile(experiment, results_directory, json_report)
-
-        FileUtils().WriteFile(results_table_path, report)
-
-        self.l.LogOutput(
-            "Storing email message body in %s." % results_directory
-        )
-        msg_file_path = os.path.join(results_directory, "msg_body.html")
-        text_report = TextResultsReport.FromExperiment(
-            experiment, True
-        ).GetReport()
-        text_report += (
-            "\nResults are stored in %s.\n" % experiment.results_directory
-        )
-        msg_body = "<pre style='font-size: 13px'>%s</pre>" % text_report
-        FileUtils().WriteFile(msg_file_path, msg_body)
-
-        return self.SUCCEEDED if has_passes else self.HAS_FAILURE
-
-    def Run(self):
-        try:
-            self._Run(self._experiment)
-        finally:
-            # Always print the report at the end of the run.
-            self._PrintTable(self._experiment)
-            ret = self._StoreResults(self._experiment)
-            if ret != self.ALL_FAILED:
-                self._Email(self._experiment)
-        return ret
-
-
-class MockExperimentRunner(ExperimentRunner):
-    """Mocked ExperimentRunner for testing."""
-
-    def __init__(self, experiment, json_report):
-        super(MockExperimentRunner, self).__init__(experiment, json_report)
-
-    def _Run(self, experiment):
-        self.l.LogOutput(
-            "Would run the following experiment: '%s'." % experiment.name
-        )
-
-    def _PrintTable(self, experiment):
-        self.l.LogOutput("Would print the experiment table.")
-
-    def _Email(self, experiment):
-        self.l.LogOutput("Would send result email.")
-
-    def _StoreResults(self, experiment):
-        self.l.LogOutput("Would store the results.")
diff --git a/crosperf/experiment_runner_unittest.py b/crosperf/experiment_runner_unittest.py
deleted file mode 100755
index a9a12630..00000000
--- a/crosperf/experiment_runner_unittest.py
+++ /dev/null
@@ -1,567 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Copyright 2014 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Tests for the experiment runner module."""
-
-
-import getpass
-import io
-import os
-import time
-import unittest
-import unittest.mock as mock
-
-from cros_utils import command_executer
-from cros_utils.email_sender import EmailSender
-from cros_utils.file_utils import FileUtils
-from experiment_factory import ExperimentFactory
-from experiment_file import ExperimentFile
-import experiment_runner
-import experiment_status
-import machine_manager
-from results_cache import Result
-from results_report import HTMLResultsReport
-from results_report import TextResultsReport
-import test_flag
-
-import config
-
-
-EXPERIMENT_FILE_1 = """
-  board: parrot
-  remote: chromeos-parrot1.cros chromreos-parrot2.cros
-  locks_dir: /tmp
-
-  benchmark: kraken {
-    suite: telemetry_Crosperf
-    iterations: 3
-  }
-
-  image1 {
-    chromeos_root: /usr/local/google/chromeos
-    chromeos_image: /usr/local/google/chromeos/src/build/images/parrot/latest/cros_image1.bin
-  }
-
-  image2 {
-    chromeos_image: /usr/local/google/chromeos/src/build/imaages/parrot/latest/cros_image2.bin
-  }
-  """
-
-# pylint: disable=protected-access
-
-
-class FakeLogger(object):
-    """Fake logger for tests."""
-
-    def __init__(self):
-        self.LogOutputCount = 0
-        self.LogErrorCount = 0
-        self.output_msgs = []
-        self.error_msgs = []
-        self.dot_count = 0
-        self.LogStartDotsCount = 0
-        self.LogEndDotsCount = 0
-        self.LogAppendDotCount = 0
-
-    def LogOutput(self, msg):
-        self.LogOutputCount += 1
-        self.output_msgs.append(msg)
-
-    def LogError(self, msg):
-        self.LogErrorCount += 1
-        self.error_msgs.append(msg)
-
-    def LogStartDots(self):
-        self.LogStartDotsCount += 1
-        self.dot_count += 1
-
-    def LogAppendDot(self):
-        self.LogAppendDotCount += 1
-        self.dot_count += 1
-
-    def LogEndDots(self):
-        self.LogEndDotsCount += 1
-
-    def Reset(self):
-        self.LogOutputCount = 0
-        self.LogErrorCount = 0
-        self.output_msgs = []
-        self.error_msgs = []
-        self.dot_count = 0
-        self.LogStartDotsCount = 0
-        self.LogEndDotsCount = 0
-        self.LogAppendDotCount = 0
-
-
-class ExperimentRunnerTest(unittest.TestCase):
-    """Test for experiment runner class."""
-
-    run_count = 0
-    is_complete_count = 0
-    mock_logger = FakeLogger()
-    mock_cmd_exec = mock.Mock(spec=command_executer.CommandExecuter)
-
-    def make_fake_experiment(self):
-        test_flag.SetTestMode(True)
-        experiment_file = ExperimentFile(io.StringIO(EXPERIMENT_FILE_1))
-        experiment = ExperimentFactory().GetExperiment(
-            experiment_file, working_directory="", log_dir=""
-        )
-        return experiment
-
-    @mock.patch.object(machine_manager.MachineManager, "AddMachine")
-    @mock.patch.object(os.path, "isfile")
-
-    # pylint: disable=arguments-differ
-    def setUp(self, mock_isfile, _mock_addmachine):
-        mock_isfile.return_value = True
-        self.exp = self.make_fake_experiment()
-
-    def test_init(self):
-        er = experiment_runner.ExperimentRunner(
-            self.exp,
-            json_report=False,
-            using_schedv2=False,
-            log=self.mock_logger,
-            cmd_exec=self.mock_cmd_exec,
-        )
-        self.assertFalse(er._terminated)
-        self.assertEqual(er.STATUS_TIME_DELAY, 10)
-
-        self.exp.log_level = "verbose"
-        er = experiment_runner.ExperimentRunner(
-            self.exp,
-            json_report=False,
-            using_schedv2=False,
-            log=self.mock_logger,
-            cmd_exec=self.mock_cmd_exec,
-        )
-        self.assertEqual(er.STATUS_TIME_DELAY, 30)
-
-    @mock.patch.object(time, "time")
-    @mock.patch.object(time, "sleep")
-    @mock.patch.object(experiment_status.ExperimentStatus, "GetStatusString")
-    @mock.patch.object(experiment_status.ExperimentStatus, "GetProgressString")
-    def test_run(
-        self, mock_progress_string, mock_status_string, mock_sleep, mock_time
-    ):
-
-        self.run_count = 0
-        self.is_complete_count = 0
-        mock_sleep.return_value = None
-        # pylint: disable=range-builtin-not-iterating
-        mock_time.side_effect = range(1, 50, 1)
-
-        def reset():
-            self.run_count = 0
-            self.is_complete_count = 0
-
-        def FakeRun():
-            self.run_count += 1
-            return 0
-
-        def FakeIsComplete():
-            self.is_complete_count += 1
-            if self.is_complete_count < 6:
-                return False
-            else:
-                return True
-
-        self.mock_logger.Reset()
-        self.exp.Run = FakeRun
-        self.exp.IsComplete = FakeIsComplete
-
-        # Test 1: log_level == "quiet"
-        self.exp.log_level = "quiet"
-        er = experiment_runner.ExperimentRunner(
-            self.exp,
-            json_report=False,
-            using_schedv2=False,
-            log=self.mock_logger,
-            cmd_exec=self.mock_cmd_exec,
-        )
-        er.STATUS_TIME_DELAY = 2
-        mock_status_string.return_value = "Fake status string"
-        er._Run(self.exp)
-        self.assertEqual(self.run_count, 1)
-        self.assertTrue(self.is_complete_count > 0)
-        self.assertEqual(self.mock_logger.LogStartDotsCount, 1)
-        self.assertEqual(self.mock_logger.LogAppendDotCount, 1)
-        self.assertEqual(self.mock_logger.LogEndDotsCount, 1)
-        self.assertEqual(self.mock_logger.dot_count, 2)
-        self.assertEqual(mock_progress_string.call_count, 0)
-        self.assertEqual(mock_status_string.call_count, 2)
-        self.assertEqual(
-            self.mock_logger.output_msgs,
-            [
-                "==============================",
-                "Fake status string",
-                "==============================",
-            ],
-        )
-        self.assertEqual(len(self.mock_logger.error_msgs), 0)
-
-        # Test 2: log_level == "average"
-        self.mock_logger.Reset()
-        reset()
-        self.exp.log_level = "average"
-        mock_status_string.call_count = 0
-        er = experiment_runner.ExperimentRunner(
-            self.exp,
-            json_report=False,
-            using_schedv2=False,
-            log=self.mock_logger,
-            cmd_exec=self.mock_cmd_exec,
-        )
-        er.STATUS_TIME_DELAY = 2
-        mock_status_string.return_value = "Fake status string"
-        er._Run(self.exp)
-        self.assertEqual(self.run_count, 1)
-        self.assertTrue(self.is_complete_count > 0)
-        self.assertEqual(self.mock_logger.LogStartDotsCount, 1)
-        self.assertEqual(self.mock_logger.LogAppendDotCount, 1)
-        self.assertEqual(self.mock_logger.LogEndDotsCount, 1)
-        self.assertEqual(self.mock_logger.dot_count, 2)
-        self.assertEqual(mock_progress_string.call_count, 0)
-        self.assertEqual(mock_status_string.call_count, 2)
-        self.assertEqual(
-            self.mock_logger.output_msgs,
-            [
-                "==============================",
-                "Fake status string",
-                "==============================",
-            ],
-        )
-        self.assertEqual(len(self.mock_logger.error_msgs), 0)
-
-        # Test 3: log_level == "verbose"
-        self.mock_logger.Reset()
-        reset()
-        self.exp.log_level = "verbose"
-        mock_status_string.call_count = 0
-        er = experiment_runner.ExperimentRunner(
-            self.exp,
-            json_report=False,
-            using_schedv2=False,
-            log=self.mock_logger,
-            cmd_exec=self.mock_cmd_exec,
-        )
-        er.STATUS_TIME_DELAY = 2
-        mock_status_string.return_value = "Fake status string"
-        mock_progress_string.return_value = "Fake progress string"
-        er._Run(self.exp)
-        self.assertEqual(self.run_count, 1)
-        self.assertTrue(self.is_complete_count > 0)
-        self.assertEqual(self.mock_logger.LogStartDotsCount, 0)
-        self.assertEqual(self.mock_logger.LogAppendDotCount, 0)
-        self.assertEqual(self.mock_logger.LogEndDotsCount, 0)
-        self.assertEqual(self.mock_logger.dot_count, 0)
-        self.assertEqual(mock_progress_string.call_count, 2)
-        self.assertEqual(mock_status_string.call_count, 2)
-        self.assertEqual(
-            self.mock_logger.output_msgs,
-            [
-                "==============================",
-                "Fake progress string",
-                "Fake status string",
-                "==============================",
-                "==============================",
-                "Fake progress string",
-                "Fake status string",
-                "==============================",
-            ],
-        )
-        self.assertEqual(len(self.mock_logger.error_msgs), 0)
-
-    @mock.patch.object(TextResultsReport, "GetReport")
-    def test_print_table(self, mock_report):
-        self.mock_logger.Reset()
-        mock_report.return_value = "This is a fake experiment report."
-        er = experiment_runner.ExperimentRunner(
-            self.exp,
-            json_report=False,
-            using_schedv2=False,
-            log=self.mock_logger,
-            cmd_exec=self.mock_cmd_exec,
-        )
-        er._PrintTable(self.exp)
-        self.assertEqual(mock_report.call_count, 1)
-        self.assertEqual(
-            self.mock_logger.output_msgs, ["This is a fake experiment report."]
-        )
-
-    @mock.patch.object(HTMLResultsReport, "GetReport")
-    @mock.patch.object(TextResultsReport, "GetReport")
-    @mock.patch.object(EmailSender, "Attachment")
-    @mock.patch.object(EmailSender, "SendEmail")
-    @mock.patch.object(getpass, "getuser")
-    def test_email(
-        self,
-        mock_getuser,
-        mock_emailer,
-        mock_attachment,
-        mock_text_report,
-        mock_html_report,
-    ):
-
-        mock_getuser.return_value = "john.smith@google.com"
-        mock_text_report.return_value = "This is a fake text report."
-        mock_html_report.return_value = "This is a fake html report."
-
-        self.mock_logger.Reset()
-        config.AddConfig("no_email", True)
-        self.exp.email_to = ["jane.doe@google.com"]
-        er = experiment_runner.ExperimentRunner(
-            self.exp,
-            json_report=False,
-            using_schedv2=False,
-            log=self.mock_logger,
-            cmd_exec=self.mock_cmd_exec,
-        )
-        # Test 1. Config:no_email; exp.email_to set ==> no email sent
-        er._Email(self.exp)
-        self.assertEqual(mock_getuser.call_count, 0)
-        self.assertEqual(mock_emailer.call_count, 0)
-        self.assertEqual(mock_attachment.call_count, 0)
-        self.assertEqual(mock_text_report.call_count, 0)
-        self.assertEqual(mock_html_report.call_count, 0)
-
-        # Test 2. Config: email. exp.email_to set; cache hit.  => send email
-        self.mock_logger.Reset()
-        config.AddConfig("no_email", False)
-        for r in self.exp.benchmark_runs:
-            r.cache_hit = True
-        er._Email(self.exp)
-        self.assertEqual(mock_getuser.call_count, 1)
-        self.assertEqual(mock_emailer.call_count, 1)
-        self.assertEqual(mock_attachment.call_count, 1)
-        self.assertEqual(mock_text_report.call_count, 1)
-        self.assertEqual(mock_html_report.call_count, 1)
-        self.assertEqual(len(mock_emailer.call_args), 2)
-        self.assertEqual(
-            mock_emailer.call_args[0],
-            (
-                ["jane.doe@google.com", "john.smith@google.com"],
-                ": image1 vs. image2",
-                "<pre style='font-size: 13px'>This is a fake text "
-                "report.\nResults are stored in _results.\n</pre>",
-            ),
-        )
-        self.assertTrue(isinstance(mock_emailer.call_args[1], dict))
-        self.assertEqual(len(mock_emailer.call_args[1]), 2)
-        self.assertTrue("attachments" in mock_emailer.call_args[1].keys())
-        self.assertEqual(mock_emailer.call_args[1]["msg_type"], "html")
-
-        mock_attachment.assert_called_with(
-            "report.html", "This is a fake html report."
-        )
-
-        # Test 3. Config: email; exp.mail_to set; no cache hit.  => send email
-        self.mock_logger.Reset()
-        mock_getuser.reset_mock()
-        mock_emailer.reset_mock()
-        mock_attachment.reset_mock()
-        mock_text_report.reset_mock()
-        mock_html_report.reset_mock()
-        config.AddConfig("no_email", False)
-        for r in self.exp.benchmark_runs:
-            r.cache_hit = False
-        er._Email(self.exp)
-        self.assertEqual(mock_getuser.call_count, 1)
-        self.assertEqual(mock_emailer.call_count, 1)
-        self.assertEqual(mock_attachment.call_count, 1)
-        self.assertEqual(mock_text_report.call_count, 1)
-        self.assertEqual(mock_html_report.call_count, 1)
-        self.assertEqual(len(mock_emailer.call_args), 2)
-        self.assertEqual(
-            mock_emailer.call_args[0],
-            (
-                [
-                    "jane.doe@google.com",
-                    "john.smith@google.com",
-                    "john.smith@google.com",
-                ],
-                ": image1 vs. image2",
-                "<pre style='font-size: 13px'>This is a fake text "
-                "report.\nResults are stored in _results.\n</pre>",
-            ),
-        )
-        self.assertTrue(isinstance(mock_emailer.call_args[1], dict))
-        self.assertEqual(len(mock_emailer.call_args[1]), 2)
-        self.assertTrue("attachments" in mock_emailer.call_args[1].keys())
-        self.assertEqual(mock_emailer.call_args[1]["msg_type"], "html")
-
-        mock_attachment.assert_called_with(
-            "report.html", "This is a fake html report."
-        )
-
-        # Test 4. Config: email; exp.mail_to = None; no cache hit. => send email
-        self.mock_logger.Reset()
-        mock_getuser.reset_mock()
-        mock_emailer.reset_mock()
-        mock_attachment.reset_mock()
-        mock_text_report.reset_mock()
-        mock_html_report.reset_mock()
-        self.exp.email_to = []
-        er._Email(self.exp)
-        self.assertEqual(mock_getuser.call_count, 1)
-        self.assertEqual(mock_emailer.call_count, 1)
-        self.assertEqual(mock_attachment.call_count, 1)
-        self.assertEqual(mock_text_report.call_count, 1)
-        self.assertEqual(mock_html_report.call_count, 1)
-        self.assertEqual(len(mock_emailer.call_args), 2)
-        self.assertEqual(
-            mock_emailer.call_args[0],
-            (
-                ["john.smith@google.com"],
-                ": image1 vs. image2",
-                "<pre style='font-size: 13px'>This is a fake text "
-                "report.\nResults are stored in _results.\n</pre>",
-            ),
-        )
-        self.assertTrue(isinstance(mock_emailer.call_args[1], dict))
-        self.assertEqual(len(mock_emailer.call_args[1]), 2)
-        self.assertTrue("attachments" in mock_emailer.call_args[1].keys())
-        self.assertEqual(mock_emailer.call_args[1]["msg_type"], "html")
-
-        mock_attachment.assert_called_with(
-            "report.html", "This is a fake html report."
-        )
-
-        # Test 5. Config: email; exp.mail_to = None; cache hit => no email sent
-        self.mock_logger.Reset()
-        mock_getuser.reset_mock()
-        mock_emailer.reset_mock()
-        mock_attachment.reset_mock()
-        mock_text_report.reset_mock()
-        mock_html_report.reset_mock()
-        for r in self.exp.benchmark_runs:
-            r.cache_hit = True
-        er._Email(self.exp)
-        self.assertEqual(mock_getuser.call_count, 0)
-        self.assertEqual(mock_emailer.call_count, 0)
-        self.assertEqual(mock_attachment.call_count, 0)
-        self.assertEqual(mock_text_report.call_count, 0)
-        self.assertEqual(mock_html_report.call_count, 0)
-
-    @mock.patch.object(FileUtils, "RmDir")
-    @mock.patch.object(FileUtils, "MkDirP")
-    @mock.patch.object(FileUtils, "WriteFile")
-    @mock.patch.object(HTMLResultsReport, "FromExperiment")
-    @mock.patch.object(TextResultsReport, "FromExperiment")
-    @mock.patch.object(Result, "CompressResultsTo")
-    @mock.patch.object(Result, "CopyResultsTo")
-    @mock.patch.object(Result, "CleanUp")
-    @mock.patch.object(Result, "FormatStringTopCommands")
-    @mock.patch("builtins.open", new_callable=mock.mock_open)
-    def test_store_results(
-        self,
-        mock_open,
-        mock_top_commands,
-        mock_cleanup,
-        mock_copy,
-        mock_compress,
-        _mock_text_report,
-        mock_report,
-        mock_writefile,
-        mock_mkdir,
-        mock_rmdir,
-    ):
-
-        self.mock_logger.Reset()
-        self.exp.results_directory = "/usr/local/crosperf-results"
-        bench_run = self.exp.benchmark_runs[5]
-        bench_path = "/usr/local/crosperf-results/" + "".join(
-            ch for ch in bench_run.name if ch.isalnum()
-        )
-        self.assertEqual(len(self.exp.benchmark_runs), 6)
-
-        er = experiment_runner.ExperimentRunner(
-            self.exp,
-            json_report=False,
-            using_schedv2=False,
-            log=self.mock_logger,
-            cmd_exec=self.mock_cmd_exec,
-        )
-
-        # Test 1. Make sure nothing is done if _terminated is true.
-        er._terminated = True
-        er._StoreResults(self.exp)
-        self.assertEqual(mock_cleanup.call_count, 0)
-        self.assertEqual(mock_copy.call_count, 0)
-        self.assertEqual(mock_compress.call_count, 0)
-        self.assertEqual(mock_report.call_count, 0)
-        self.assertEqual(mock_writefile.call_count, 0)
-        self.assertEqual(mock_mkdir.call_count, 0)
-        self.assertEqual(mock_rmdir.call_count, 0)
-        self.assertEqual(self.mock_logger.LogOutputCount, 0)
-        self.assertEqual(mock_open.call_count, 0)
-        self.assertEqual(mock_top_commands.call_count, 0)
-
-        # Test 2. _terminated is false; everything works properly.
-        fake_result = Result(
-            self.mock_logger, self.exp.labels[0], "average", "daisy1"
-        )
-        for r in self.exp.benchmark_runs:
-            r.result = fake_result
-        er._terminated = False
-        self.exp.compress_results = False
-        er._StoreResults(self.exp)
-        self.assertEqual(mock_cleanup.call_count, 6)
-        mock_cleanup.assert_called_with(bench_run.benchmark.rm_chroot_tmp)
-        self.assertEqual(mock_copy.call_count, 6)
-        mock_copy.assert_called_with(bench_path)
-        self.assertEqual(mock_writefile.call_count, 3)
-        self.assertEqual(len(mock_writefile.call_args_list), 3)
-        first_args = mock_writefile.call_args_list[0]
-        second_args = mock_writefile.call_args_list[1]
-        self.assertEqual(
-            first_args[0][0], "/usr/local/crosperf-results/experiment.exp"
-        )
-        self.assertEqual(
-            second_args[0][0], "/usr/local/crosperf-results/results.html"
-        )
-        self.assertEqual(mock_mkdir.call_count, 1)
-        mock_mkdir.assert_called_with("/usr/local/crosperf-results")
-        self.assertEqual(mock_rmdir.call_count, 1)
-        mock_rmdir.assert_called_with("/usr/local/crosperf-results")
-        self.assertEqual(self.mock_logger.LogOutputCount, 5)
-        self.assertEqual(
-            self.mock_logger.output_msgs,
-            [
-                "Storing experiment file in /usr/local/crosperf-results.",
-                "Storing top statistics of each benchmark run into"
-                " /usr/local/crosperf-results/topstats.log.",
-                "Storing results of each benchmark run.",
-                "Storing results report in /usr/local/crosperf-results.",
-                "Storing email message body in /usr/local/crosperf-results.",
-            ],
-        )
-        self.assertEqual(mock_open.call_count, 1)
-        # Check write to a topstats.log file.
-        mock_open.assert_called_with(
-            "/usr/local/crosperf-results/topstats.log", "w"
-        )
-        mock_open().write.assert_called()
-
-        # Check top calls with no arguments.
-        topcalls = [mock.call()] * 6
-        self.assertEqual(mock_top_commands.call_args_list, topcalls)
-
-        # Test 3. Test compress_results.
-        self.exp.compress_results = True
-        mock_copy.call_count = 0
-        mock_compress.call_count = 0
-        er._StoreResults(self.exp)
-        self.assertEqual(mock_copy.call_count, 0)
-        mock_copy.assert_called_with(bench_path)
-        self.assertEqual(mock_compress.call_count, 6)
-        mock_compress.assert_called_with(bench_path)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/experiment_status.py b/crosperf/experiment_status.py
deleted file mode 100644
index fa6b1eec..00000000
--- a/crosperf/experiment_status.py
+++ /dev/null
@@ -1,167 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2011 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""The class to show the banner."""
-
-
-import collections
-import datetime
-import time
-
-
-class ExperimentStatus(object):
-    """The status class."""
-
-    def __init__(self, experiment):
-        self.experiment = experiment
-        self.num_total = len(self.experiment.benchmark_runs)
-        self.completed = 0
-        self.new_job_start_time = time.time()
-        self.log_level = experiment.log_level
-
-    def _GetProgressBar(self, num_complete, num_total):
-        ret = "Done: %s%%" % int(100.0 * num_complete / num_total)
-        bar_length = 50
-        done_char = ">"
-        undone_char = " "
-        num_complete_chars = bar_length * num_complete // num_total
-        num_undone_chars = bar_length - num_complete_chars
-        ret += " [%s%s]" % (
-            num_complete_chars * done_char,
-            num_undone_chars * undone_char,
-        )
-        return ret
-
-    def GetProgressString(self):
-        """Get the elapsed_time, ETA."""
-        current_time = time.time()
-        if self.experiment.start_time:
-            elapsed_time = current_time - self.experiment.start_time
-        else:
-            elapsed_time = 0
-        try:
-            if self.completed != self.experiment.num_complete:
-                self.completed = self.experiment.num_complete
-                self.new_job_start_time = current_time
-            time_completed_jobs = elapsed_time - (
-                current_time - self.new_job_start_time
-            )
-            # eta is calculated as:
-            #   ETA = (num_jobs_not_yet_started * estimated_time_per_job)
-            #          + time_left_for_current_job
-            #
-            #   where
-            #        num_jobs_not_yet_started = (num_total - num_complete - 1)
-            #
-            #        estimated_time_per_job = time_completed_jobs / num_run_complete
-            #
-            #        time_left_for_current_job = estimated_time_per_job -
-            #                                    time_spent_so_far_on_current_job
-            #
-            #  The biggest problem with this calculation is its assumption that
-            #  all jobs have roughly the same running time (blatantly false!).
-            #
-            #  ETA can come out negative if the time spent on the current job is
-            #  greater than the estimated time per job (e.g. you're running the
-            #  first long job, after a series of short jobs).  For now, if that
-            #  happens, we set the ETA to "Unknown."
-            #
-            eta_seconds = float(
-                self.num_total - self.experiment.num_complete - 1
-            ) * time_completed_jobs / self.experiment.num_run_complete + (
-                time_completed_jobs / self.experiment.num_run_complete
-                - (current_time - self.new_job_start_time)
-            )
-
-            eta_seconds = int(eta_seconds)
-            if eta_seconds > 0:
-                eta = datetime.timedelta(seconds=eta_seconds)
-            else:
-                eta = "Unknown"
-        except ZeroDivisionError:
-            eta = "Unknown"
-        strings = []
-        strings.append(
-            "Current time: %s Elapsed: %s ETA: %s"
-            % (
-                datetime.datetime.now(),
-                datetime.timedelta(seconds=int(elapsed_time)),
-                eta,
-            )
-        )
-        strings.append(
-            self._GetProgressBar(self.experiment.num_complete, self.num_total)
-        )
-        return "\n".join(strings)
-
-    def GetStatusString(self):
-        """Get the status string of all the benchmark_runs."""
-        status_bins = collections.defaultdict(list)
-        for benchmark_run in self.experiment.benchmark_runs:
-            status_bins[benchmark_run.timeline.GetLastEvent()].append(
-                benchmark_run
-            )
-
-        status_strings = []
-        for key, val in status_bins.items():
-            if key == "RUNNING":
-                get_description = self._GetNamesAndIterations
-            else:
-                get_description = self._GetCompactNamesAndIterations
-            status_strings.append("%s: %s" % (key, get_description(val)))
-
-        thread_status = ""
-        thread_status_format = "Thread Status: \n{}\n"
-        if (
-            self.experiment.schedv2() is None
-            and self.experiment.log_level == "verbose"
-        ):
-            # Add the machine manager status.
-            thread_status = thread_status_format.format(
-                self.experiment.machine_manager.AsString()
-            )
-        elif self.experiment.schedv2():
-            # In schedv2 mode, we always print out thread status.
-            thread_status = thread_status_format.format(
-                self.experiment.schedv2().threads_status_as_string()
-            )
-
-        result = "{}{}".format(thread_status, "\n".join(status_strings))
-
-        return result
-
-    def _GetNamesAndIterations(self, benchmark_runs):
-        strings = []
-        t = time.time()
-        for benchmark_run in benchmark_runs:
-            t_last = benchmark_run.timeline.GetLastEventTime()
-            elapsed = str(datetime.timedelta(seconds=int(t - t_last)))
-            strings.append("'{0}' {1}".format(benchmark_run.name, elapsed))
-        return " %s (%s)" % (len(strings), ", ".join(strings))
-
-    def _GetCompactNamesAndIterations(self, benchmark_runs):
-        grouped_benchmarks = collections.defaultdict(list)
-        for benchmark_run in benchmark_runs:
-            grouped_benchmarks[benchmark_run.label.name].append(benchmark_run)
-
-        output_segs = []
-        for label_name, label_runs in grouped_benchmarks.items():
-            strings = []
-            benchmark_iterations = collections.defaultdict(list)
-            for benchmark_run in label_runs:
-                assert benchmark_run.label.name == label_name
-                benchmark_name = benchmark_run.benchmark.name
-                benchmark_iterations[benchmark_name].append(
-                    benchmark_run.iteration
-                )
-            for key, val in benchmark_iterations.items():
-                val.sort()
-                iterations = ",".join(str(v) for v in val)
-                strings.append("{} [{}]".format(key, iterations))
-            output_segs.append(
-                "  " + label_name + ": " + ", ".join(strings) + "\n"
-            )
-
-        return " %s \n%s" % (len(benchmark_runs), "".join(output_segs))
diff --git a/crosperf/field.py b/crosperf/field.py
deleted file mode 100644
index 6b5ea110..00000000
--- a/crosperf/field.py
+++ /dev/null
@@ -1,167 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2011 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Module to represent a Field in an experiment file."""
-
-
-class Field(object):
-    """Class representing a Field in an experiment file."""
-
-    def __init__(self, name, required, default, inheritable, description):
-        self.name = name
-        self.required = required
-        self.assigned = False
-        self.default = default
-        self._value = default
-        self.inheritable = inheritable
-        self.description = description
-
-    def Set(self, value, parse=True):
-        if parse:
-            self._value = self._Parse(value)
-        else:
-            self._value = value
-        self.assigned = True
-
-    def Append(self, value):
-        self._value += self._Parse(value)
-        self.assigned = True
-
-    def _Parse(self, value):
-        return value
-
-    def Get(self):
-        return self._value
-
-    def GetString(self):
-        return str(self._value)
-
-
-class TextField(Field):
-    """Class of text field."""
-
-    def __init__(
-        self,
-        name,
-        required=False,
-        default="",
-        inheritable=False,
-        description="",
-    ):
-        super(TextField, self).__init__(
-            name, required, default, inheritable, description
-        )
-
-    def _Parse(self, value):
-        return str(value)
-
-
-class BooleanField(Field):
-    """Class of boolean field."""
-
-    def __init__(
-        self,
-        name,
-        required=False,
-        default=False,
-        inheritable=False,
-        description="",
-    ):
-        super(BooleanField, self).__init__(
-            name, required, default, inheritable, description
-        )
-
-    def _Parse(self, value):
-        if value.lower() == "true":
-            return True
-        elif value.lower() == "false":
-            return False
-        raise TypeError(
-            "Invalid value for '%s'. Must be true or false." % self.name
-        )
-
-
-class IntegerField(Field):
-    """Class of integer field."""
-
-    def __init__(
-        self, name, required=False, default=0, inheritable=False, description=""
-    ):
-        super(IntegerField, self).__init__(
-            name, required, default, inheritable, description
-        )
-
-    def _Parse(self, value):
-        return int(value)
-
-
-class FloatField(Field):
-    """Class of float field."""
-
-    def __init__(
-        self, name, required=False, default=0, inheritable=False, description=""
-    ):
-        super(FloatField, self).__init__(
-            name, required, default, inheritable, description
-        )
-
-    def _Parse(self, value):
-        return float(value)
-
-
-class ListField(Field):
-    """Class of list field."""
-
-    def __init__(
-        self,
-        name,
-        required=False,
-        default=None,
-        inheritable=False,
-        description="",
-    ):
-        super(ListField, self).__init__(
-            name, required, default, inheritable, description
-        )
-
-    def _Parse(self, value):
-        return value.split()
-
-    def GetString(self):
-        return " ".join(self._value)
-
-    def Append(self, value):
-        v = self._Parse(value)
-        if not self._value:
-            self._value = v
-        else:
-            self._value += v
-        self.assigned = True
-
-
-class EnumField(Field):
-    """Class of enum field."""
-
-    def __init__(
-        self,
-        name,
-        options,
-        required=False,
-        default="",
-        inheritable=False,
-        description="",
-    ):
-        super(EnumField, self).__init__(
-            name, required, default, inheritable, description
-        )
-        self.options = options
-
-    def _Parse(self, value):
-        if value not in self.options:
-            raise TypeError(
-                "Invalid enum value for field '%s'. Must be one of (%s)"
-                % (self.name, ", ".join(self.options))
-            )
-        return str(value)
diff --git a/crosperf/flag_test_unittest.py b/crosperf/flag_test_unittest.py
deleted file mode 100755
index 024849cb..00000000
--- a/crosperf/flag_test_unittest.py
+++ /dev/null
@@ -1,44 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2014 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""The unittest of flags."""
-
-
-import unittest
-
-import test_flag
-
-
-class FlagTestCase(unittest.TestCase):
-    """The unittest class."""
-
-    def test_test_flag(self):
-        # Verify that test_flag.is_test exists, that it is a list,
-        # and that it contains 1 element.
-        self.assertTrue(isinstance(test_flag.is_test, list))
-        self.assertEqual(len(test_flag.is_test), 1)
-
-        # Verify that the getting the flag works and that the flag
-        # contains False, its starting value.
-        save_flag = test_flag.GetTestMode()
-        self.assertFalse(save_flag)
-
-        # Verify that setting the flat to True, then getting it, works.
-        test_flag.SetTestMode(True)
-        self.assertTrue(test_flag.GetTestMode())
-
-        # Verify that setting the flag to False, then getting it, works.
-        test_flag.SetTestMode(save_flag)
-        self.assertFalse(test_flag.GetTestMode())
-
-        # Verify that test_flag.is_test still exists, that it still is a
-        # list, and that it still contains 1 element.
-        self.assertTrue(isinstance(test_flag.is_test, list))
-        self.assertEqual(len(test_flag.is_test), 1)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/generate_report.py b/crosperf/generate_report.py
deleted file mode 100755
index 55c13212..00000000
--- a/crosperf/generate_report.py
+++ /dev/null
@@ -1,306 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2016 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Given a specially-formatted JSON object, generates results report(s).
-
-The JSON object should look like:
-{"data": BenchmarkData, "platforms": BenchmarkPlatforms}
-
-BenchmarkPlatforms is a [str], each of which names a platform the benchmark
-  was run on (e.g. peppy, shamu, ...). Note that the order of this list is
-  related with the order of items in BenchmarkData.
-
-BenchmarkData is a {str: [PlatformData]}. The str is the name of the benchmark,
-and a PlatformData is a set of data for a given platform. There must be one
-PlatformData for each benchmark, for each element in BenchmarkPlatforms.
-
-A PlatformData is a [{str: float}], where each str names a metric we recorded,
-and the float is the value for that metric. Each element is considered to be
-the metrics collected from an independent run of this benchmark. NOTE: Each
-PlatformData is expected to have a "retval" key, with the return value of
-the benchmark. If the benchmark is successful, said return value should be 0.
-Otherwise, this will break some of our JSON functionality.
-
-Putting it all together, a JSON object will end up looking like:
-  { "platforms": ["peppy", "peppy-new-crosstool"],
-    "data": {
-      "bench_draw_line": [
-        [{"time (ms)": 1.321, "memory (mb)": 128.1, "retval": 0},
-         {"time (ms)": 1.920, "memory (mb)": 128.4, "retval": 0}],
-        [{"time (ms)": 1.221, "memory (mb)": 124.3, "retval": 0},
-         {"time (ms)": 1.423, "memory (mb)": 123.9, "retval": 0}]
-      ]
-    }
-  }
-
-Which says that we ran a benchmark on platforms named peppy, and
-  peppy-new-crosstool.
-We ran one benchmark, named bench_draw_line.
-It was run twice on each platform.
-Peppy's runs took 1.321ms and 1.920ms, while peppy-new-crosstool's took 1.221ms
-  and 1.423ms. None of the runs failed to complete.
-"""
-
-
-import argparse
-import functools
-import json
-import os
-import sys
-import traceback
-
-from results_report import BenchmarkResults
-from results_report import HTMLResultsReport
-from results_report import JSONResultsReport
-from results_report import TextResultsReport
-
-
-def CountBenchmarks(benchmark_runs):
-    """Counts the number of iterations for each benchmark in benchmark_runs."""
-
-    # Example input for benchmark_runs:
-    # {"bench": [[run1, run2, run3], [run1, run2, run3, run4]]}
-    def _MaxLen(results):
-        return 0 if not results else max(len(r) for r in results)
-
-    return [
-        (name, _MaxLen(results)) for name, results in benchmark_runs.items()
-    ]
-
-
-def CutResultsInPlace(results, max_keys=50, complain_on_update=True):
-    """Limits the given benchmark results to max_keys keys in-place.
-
-    This takes the `data` field from the benchmark input, and mutates each
-    benchmark run to contain `max_keys` elements (ignoring special elements, like
-    "retval"). At the moment, it just selects the first `max_keys` keyvals,
-    alphabetically.
-
-    If complain_on_update is true, this will print a message noting that a
-    truncation occurred.
-
-    This returns the `results` object that was passed in, for convenience.
-
-    e.g.
-    >>> benchmark_data = {
-    ...   "bench_draw_line": [
-    ...     [{"time (ms)": 1.321, "memory (mb)": 128.1, "retval": 0},
-    ...      {"time (ms)": 1.920, "memory (mb)": 128.4, "retval": 0}],
-    ...     [{"time (ms)": 1.221, "memory (mb)": 124.3, "retval": 0},
-    ...      {"time (ms)": 1.423, "memory (mb)": 123.9, "retval": 0}]
-    ...   ]
-    ... }
-    >>> CutResultsInPlace(benchmark_data, max_keys=1, complain_on_update=False)
-    {
-      'bench_draw_line': [
-        [{'memory (mb)': 128.1, 'retval': 0},
-         {'memory (mb)': 128.4, 'retval': 0}],
-        [{'memory (mb)': 124.3, 'retval': 0},
-         {'memory (mb)': 123.9, 'retval': 0}]
-      ]
-    }
-    """
-    actually_updated = False
-    for bench_results in results.values():
-        for platform_results in bench_results:
-            for i, result in enumerate(platform_results):
-                # Keep the keys that come earliest when sorted alphabetically.
-                # Forcing alphabetical order is arbitrary, but necessary; otherwise,
-                # the keyvals we'd emit would depend on our iteration order through a
-                # map.
-                removable_keys = sorted(k for k in result if k != "retval")
-                retained_keys = removable_keys[:max_keys]
-                platform_results[i] = {k: result[k] for k in retained_keys}
-                # retval needs to be passed through all of the time.
-                retval = result.get("retval")
-                if retval is not None:
-                    platform_results[i]["retval"] = retval
-                actually_updated = actually_updated or len(
-                    retained_keys
-                ) != len(removable_keys)
-
-    if actually_updated and complain_on_update:
-        print(
-            "Warning: Some benchmark keyvals have been truncated.",
-            file=sys.stderr,
-        )
-    return results
-
-
-def _PositiveInt(s):
-    i = int(s)
-    if i < 0:
-        raise argparse.ArgumentTypeError("%d is not a positive integer." % (i,))
-    return i
-
-
-def _AccumulateActions(args):
-    """Given program arguments, determines what actions we want to run.
-
-    Returns [(ResultsReportCtor, str)], where ResultsReportCtor can construct a
-    ResultsReport, and the str is the file extension for the given report.
-    """
-    results = []
-    # The order of these is arbitrary.
-    if args.json:
-        results.append((JSONResultsReport, "json"))
-    if args.text:
-        results.append((TextResultsReport, "txt"))
-    if args.email:
-        email_ctor = functools.partial(TextResultsReport, email=True)
-        results.append((email_ctor, "email"))
-    # We emit HTML if nothing else was specified.
-    if args.html or not results:
-        results.append((HTMLResultsReport, "html"))
-    return results
-
-
-# Note: get_contents is a function, because it may be expensive (generating some
-# HTML reports takes O(seconds) on my machine, depending on the size of the
-# input data).
-def WriteFile(output_prefix, extension, get_contents, overwrite, verbose):
-    """Writes `contents` to a file named "${output_prefix}.${extension}".
-
-    get_contents should be a zero-args function that returns a string (of the
-    contents to write).
-    If output_prefix == '-', this writes to stdout.
-    If overwrite is False, this will not overwrite files.
-    """
-    if output_prefix == "-":
-        if verbose:
-            print("Writing %s report to stdout" % (extension,), file=sys.stderr)
-        sys.stdout.write(get_contents())
-        return
-
-    file_name = "%s.%s" % (output_prefix, extension)
-    if not overwrite and os.path.exists(file_name):
-        raise IOError(
-            "Refusing to write %s -- it already exists" % (file_name,)
-        )
-
-    with open(file_name, "w") as out_file:
-        if verbose:
-            print(
-                "Writing %s report to %s" % (extension, file_name),
-                file=sys.stderr,
-            )
-        out_file.write(get_contents())
-
-
-def RunActions(actions, benchmark_results, output_prefix, overwrite, verbose):
-    """Runs `actions`, returning True if all succeeded."""
-    failed = False
-
-    report_ctor = None  # Make the linter happy
-    for report_ctor, extension in actions:
-        try:
-            get_contents = lambda: report_ctor(benchmark_results).GetReport()
-            WriteFile(
-                output_prefix, extension, get_contents, overwrite, verbose
-            )
-        except Exception:
-            # Complain and move along; we may have more actions that might complete
-            # successfully.
-            failed = True
-            traceback.print_exc()
-    return not failed
-
-
-def PickInputFile(input_name):
-    """Given program arguments, returns file to read for benchmark input."""
-    return sys.stdin if input_name == "-" else open(input_name)
-
-
-def _NoPerfReport(_label_name, _benchmark_name, _benchmark_iteration):
-    return {}
-
-
-def _ParseArgs(argv):
-    parser = argparse.ArgumentParser(
-        description="Turns JSON into results " "report(s)."
-    )
-    parser.add_argument(
-        "-v",
-        "--verbose",
-        action="store_true",
-        help="Be a tiny bit more verbose.",
-    )
-    parser.add_argument(
-        "-f",
-        "--force",
-        action="store_true",
-        help="Overwrite existing results files.",
-    )
-    parser.add_argument(
-        "-o",
-        "--output",
-        default="report",
-        type=str,
-        help="Prefix of the output filename (default: report). "
-        "- means stdout.",
-    )
-    parser.add_argument(
-        "-i",
-        "--input",
-        required=True,
-        type=str,
-        help="Where to read the JSON from. - means stdin.",
-    )
-    parser.add_argument(
-        "-l",
-        "--statistic-limit",
-        default=0,
-        type=_PositiveInt,
-        help="The maximum number of benchmark statistics to "
-        "display from a single run. 0 implies unlimited.",
-    )
-    parser.add_argument(
-        "--json", action="store_true", help="Output a JSON report."
-    )
-    parser.add_argument(
-        "--text", action="store_true", help="Output a text report."
-    )
-    parser.add_argument(
-        "--email",
-        action="store_true",
-        help="Output a text report suitable for email.",
-    )
-    parser.add_argument(
-        "--html",
-        action="store_true",
-        help="Output an HTML report (this is the default if no "
-        "other output format is specified).",
-    )
-    return parser.parse_args(argv)
-
-
-def Main(argv):
-    args = _ParseArgs(argv)
-    with PickInputFile(args.input) as in_file:
-        raw_results = json.load(in_file)
-
-    platform_names = raw_results["platforms"]
-    results = raw_results["data"]
-    if args.statistic_limit:
-        results = CutResultsInPlace(results, max_keys=args.statistic_limit)
-    benches = CountBenchmarks(results)
-    # In crosperf, a label is essentially a platform+configuration. So, a name of
-    # a label and a name of a platform are equivalent for our purposes.
-    bench_results = BenchmarkResults(
-        label_names=platform_names,
-        benchmark_names_and_iterations=benches,
-        run_keyvals=results,
-        read_perf_report=_NoPerfReport,
-    )
-    actions = _AccumulateActions(args)
-    ok = RunActions(
-        actions, bench_results, args.output, args.force, args.verbose
-    )
-    return 0 if ok else 1
-
-
-if __name__ == "__main__":
-    sys.exit(Main(sys.argv[1:]))
diff --git a/crosperf/generate_report_unittest.py b/crosperf/generate_report_unittest.py
deleted file mode 100755
index 86bbc164..00000000
--- a/crosperf/generate_report_unittest.py
+++ /dev/null
@@ -1,174 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2016 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Test for generate_report.py."""
-
-
-import copy
-import json
-import unittest
-import unittest.mock as mock
-
-import generate_report
-import results_report
-import test_flag
-
-
-# pylint: disable=deprecated-module
-try:
-    from StringIO import StringIO  # for Python 2
-except ImportError:
-    from io import StringIO  # for Python 3
-
-
-class _ContextualStringIO(StringIO):
-    """StringIO that can be used in `with` statements."""
-
-    def __init__(self, *args):
-        StringIO.__init__(self, *args)
-
-    def __enter__(self):
-        return self
-
-    def __exit__(self, _type, _value, _traceback):
-        pass
-
-
-class GenerateReportTests(unittest.TestCase):
-    """Tests for generate_report.py."""
-
-    def testCountBenchmarks(self):
-        runs = {
-            "foo": [[{}, {}, {}], [{}, {}, {}, {}]],
-            "bar": [],
-            "baz": [[], [{}], [{}, {}, {}]],
-        }
-        results = generate_report.CountBenchmarks(runs)
-        expected_results = [("foo", 4), ("bar", 0), ("baz", 3)]
-        self.assertCountEqual(expected_results, results)
-
-    def testCutResultsInPlace(self):
-        bench_data = {
-            "foo": [[{"a": 1, "b": 2, "c": 3}, {"a": 3, "b": 2.5, "c": 1}]],
-            "bar": [[{"d": 11, "e": 12, "f": 13}]],
-            "baz": [[{"g": 12, "h": 13}]],
-            "qux": [[{"i": 11}]],
-        }
-        original_bench_data = copy.deepcopy(bench_data)
-
-        max_keys = 2
-        results = generate_report.CutResultsInPlace(
-            bench_data, max_keys=max_keys, complain_on_update=False
-        )
-        # Cuts should be in-place.
-        self.assertIs(results, bench_data)
-        self.assertCountEqual(
-            list(original_bench_data.keys()), list(bench_data.keys())
-        )
-        for bench_name, original_runs in original_bench_data.items():
-            bench_runs = bench_data[bench_name]
-            self.assertEqual(len(original_runs), len(bench_runs))
-            # Order of these sub-lists shouldn't have changed.
-            for original_list, new_list in zip(original_runs, bench_runs):
-                self.assertEqual(len(original_list), len(new_list))
-                for original_keyvals, sub_keyvals in zip(
-                    original_list, new_list
-                ):
-                    # sub_keyvals must be a subset of original_keyvals
-                    self.assertDictContainsSubset(sub_keyvals, original_keyvals)
-
-    def testCutResultsInPlaceLeavesRetval(self):
-        bench_data = {
-            "foo": [[{"retval": 0, "a": 1}]],
-            "bar": [[{"retval": 1}]],
-            "baz": [[{"RETVAL": 1}]],
-        }
-        results = generate_report.CutResultsInPlace(
-            bench_data, max_keys=0, complain_on_update=False
-        )
-        # Just reach into results assuming we know it otherwise outputs things in
-        # the expected way. If it doesn't, testCutResultsInPlace should give an
-        # indication as to what, exactly, is broken.
-        self.assertEqual(list(results["foo"][0][0].items()), [("retval", 0)])
-        self.assertEqual(list(results["bar"][0][0].items()), [("retval", 1)])
-        self.assertEqual(list(results["baz"][0][0].items()), [])
-
-    def _RunMainWithInput(self, args, input_obj):
-        assert "-i" not in args
-        args += ["-i", "-"]
-        input_buf = _ContextualStringIO(json.dumps(input_obj))
-        with mock.patch(
-            "generate_report.PickInputFile", return_value=input_buf
-        ) as patched_pick:
-            result = generate_report.Main(args)
-            patched_pick.assert_called_once_with("-")
-            return result
-
-    @mock.patch("generate_report.RunActions")
-    def testMain(self, mock_run_actions):
-        # Email is left out because it's a bit more difficult to test, and it'll be
-        # mildly obvious if it's failing.
-        args = ["--json", "--html", "--text"]
-        return_code = self._RunMainWithInput(
-            args, {"platforms": [], "data": {}}
-        )
-        self.assertEqual(0, return_code)
-        self.assertEqual(mock_run_actions.call_count, 1)
-        ctors = [ctor for ctor, _ in mock_run_actions.call_args[0][0]]
-        self.assertEqual(
-            ctors,
-            [
-                results_report.JSONResultsReport,
-                results_report.TextResultsReport,
-                results_report.HTMLResultsReport,
-            ],
-        )
-
-    @mock.patch("generate_report.RunActions")
-    def testMainSelectsHTMLIfNoReportsGiven(self, mock_run_actions):
-        args = []
-        return_code = self._RunMainWithInput(
-            args, {"platforms": [], "data": {}}
-        )
-        self.assertEqual(0, return_code)
-        self.assertEqual(mock_run_actions.call_count, 1)
-        ctors = [ctor for ctor, _ in mock_run_actions.call_args[0][0]]
-        self.assertEqual(ctors, [results_report.HTMLResultsReport])
-
-    # We only mock print_exc so we don't have exception info printed to stdout.
-    @mock.patch("generate_report.WriteFile", side_effect=ValueError("Oh noo"))
-    @mock.patch("traceback.print_exc")
-    def testRunActionsRunsAllActionsRegardlessOfExceptions(
-        self, mock_print_exc, mock_write_file
-    ):
-        actions = [
-            (None, "json"),
-            (None, "html"),
-            (None, "text"),
-            (None, "email"),
-        ]
-        output_prefix = "-"
-        ok = generate_report.RunActions(
-            actions, {}, output_prefix, overwrite=False, verbose=False
-        )
-        self.assertFalse(ok)
-        self.assertEqual(mock_write_file.call_count, len(actions))
-        self.assertEqual(mock_print_exc.call_count, len(actions))
-
-    @mock.patch("generate_report.WriteFile")
-    def testRunActionsReturnsTrueIfAllActionsSucceed(self, mock_write_file):
-        actions = [(None, "json"), (None, "html"), (None, "text")]
-        output_prefix = "-"
-        ok = generate_report.RunActions(
-            actions, {}, output_prefix, overwrite=False, verbose=False
-        )
-        self.assertEqual(mock_write_file.call_count, len(actions))
-        self.assertTrue(ok)
-
-
-if __name__ == "__main__":
-    test_flag.SetTestMode(True)
-    unittest.main()
diff --git a/crosperf/help.py b/crosperf/help.py
deleted file mode 100644
index db95fc6c..00000000
--- a/crosperf/help.py
+++ /dev/null
@@ -1,126 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2011 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Module to print help message."""
-
-
-import sys
-import textwrap
-
-from settings_factory import BenchmarkSettings
-from settings_factory import GlobalSettings
-from settings_factory import LabelSettings
-
-
-class Help(object):
-    """The help class."""
-
-    def GetUsage(self):
-        return """%s [OPTIONS] EXPERIMENT_FILE""" % (sys.argv[0])
-
-    def _WrapLine(self, line):
-        return "\n".join(textwrap.wrap(line, 80))
-
-    def _GetFieldDescriptions(self, fields):
-        res = ""
-        for field_name in fields:
-            field = fields[field_name]
-            res += "Field:\t\t%s\n" % field.name
-            res += self._WrapLine("Description:\t%s" % field.description) + "\n"
-            res += "Type:\t\t%s\n" % type(field).__name__.replace("Field", "")
-            res += "Required:\t%s\n" % field.required
-            if field.default:
-                res += "Default:\t%s\n" % field.default
-            res += "\n"
-        return res
-
-    def GetHelp(self):
-        global_fields = self._GetFieldDescriptions(GlobalSettings("").fields)
-        benchmark_fields = self._GetFieldDescriptions(
-            BenchmarkSettings("").fields
-        )
-        label_fields = self._GetFieldDescriptions(LabelSettings("").fields)
-
-        return """%s is a script for running performance experiments on
-ChromeOS. It allows one to run ChromeOS Autotest benchmarks over
-several images and compare the results to determine whether there
-is a performance difference.
-
-Comparing several images using %s is referred to as running an
-"experiment". An "experiment file" is a configuration file which holds
-all the information that describes the experiment and how it should be
-run. An example of a simple experiment file is below:
-
---------------------------------- test.exp ---------------------------------
-name: my_experiment
-board: x86-alex
-remote: chromeos2-row1-rack4-host7.cros 172.18.122.132
-
-benchmark: page_cycler_v2.morejs {
-  suite: telemetry_Crosperf
-  iterations: 3
-}
-
-my_first_image {
-  chromeos_image: /usr/local/chromeos-1/chromiumos_image.bin
-}
-
-my_second_image {
-  chromeos_image:  /usr/local/chromeos-2/chromiumos_image.bin
-}
-----------------------------------------------------------------------------
-
-This experiment file names the experiment "my_experiment". It will be
-run on the board x86-alex. Benchmarks will be run using two remote
-devices, one is a device specified by a hostname and the other is a
-device specified by it's IP address. Benchmarks will be run in
-parallel across these devices.  There is currently no way to specify
-which benchmark will run on each device.
-
-We define one "benchmark" that will be run, page_cycler_v2.morejs. This
-benchmark has two "fields", one which specifies that this benchmark is
-part of the telemetry_Crosperf suite (this is the common way to run
-most Telemetry benchmarks), and the other which specifies how many
-iterations it will run for.
-
-We specify one or more "labels" or images which will be compared. The
-page_cycler_v2.morejs benchmark will be run on each of these images 3
-times and a result table will be output which compares them for all
-the images specified.
-
-The full list of fields that can be specified in the experiment file
-are as follows:
-=================
-Global Fields
-=================
-%s
-=================
-Benchmark Fields
-=================
-%s
-=================
-Label Fields
-=================
-%s
-
-Note that global fields are overidden by label or benchmark fields, if
-they can be specified in both places. Fields that are specified as
-arguments override fields specified in experiment files.
-
-%s is invoked by passing it a path to an experiment file,
-as well as any options (in addition to those specified in the
-experiment file).  Crosperf runs the experiment and caches the results
-(or reads the previously cached experiment results out of the cache),
-generates and displays a report based on the run, and emails the
-report to the user.  If the results were all read out of the cache,
-then by default no email is generated.
-""" % (
-            sys.argv[0],
-            sys.argv[0],
-            global_fields,
-            benchmark_fields,
-            label_fields,
-            sys.argv[0],
-        )
diff --git a/crosperf/image_checksummer.py b/crosperf/image_checksummer.py
deleted file mode 100644
index 87664e9d..00000000
--- a/crosperf/image_checksummer.py
+++ /dev/null
@@ -1,84 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2011 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Compute image checksum."""
-
-
-import os
-import threading
-
-from cros_utils import logger
-from cros_utils.file_utils import FileUtils
-
-
-class ImageChecksummer(object):
-    """Compute image checksum."""
-
-    class PerImageChecksummer(object):
-        """Compute checksum for an image."""
-
-        def __init__(self, label, log_level):
-            self._lock = threading.Lock()
-            self.label = label
-            self._checksum = None
-            self.log_level = log_level
-
-        def Checksum(self):
-            with self._lock:
-                if not self._checksum:
-                    logger.GetLogger().LogOutput(
-                        "Acquiring checksum for '%s'." % self.label.name
-                    )
-                    self._checksum = None
-                    if self.label.image_type != "local":
-                        raise RuntimeError(
-                            "Called Checksum on non-local image!"
-                        )
-                    if self.label.chromeos_image:
-                        if os.path.exists(self.label.chromeos_image):
-                            self._checksum = FileUtils().Md5File(
-                                self.label.chromeos_image,
-                                log_level=self.log_level,
-                            )
-                            logger.GetLogger().LogOutput(
-                                "Computed checksum is " ": %s" % self._checksum
-                            )
-                    if not self._checksum:
-                        raise RuntimeError("Checksum computing error.")
-                    logger.GetLogger().LogOutput(
-                        "Checksum is: %s" % self._checksum
-                    )
-                return self._checksum
-
-    _instance = None
-    _lock = threading.Lock()
-    _per_image_checksummers = {}
-
-    def __new__(cls, *args, **kwargs):
-        with cls._lock:
-            if not cls._instance:
-                cls._instance = super(ImageChecksummer, cls).__new__(
-                    cls, *args, **kwargs
-                )
-            return cls._instance
-
-    def Checksum(self, label, log_level):
-        if label.image_type != "local":
-            raise RuntimeError("Attempt to call Checksum on non-local image.")
-        with self._lock:
-            if label.name not in self._per_image_checksummers:
-                self._per_image_checksummers[
-                    label.name
-                ] = ImageChecksummer.PerImageChecksummer(label, log_level)
-            checksummer = self._per_image_checksummers[label.name]
-
-        try:
-            return checksummer.Checksum()
-        except:
-            logger.GetLogger().LogError(
-                "Could not compute checksum of image in label"
-                " '%s'." % label.name
-            )
-            raise
diff --git a/crosperf/label.py b/crosperf/label.py
deleted file mode 100644
index 9aeff562..00000000
--- a/crosperf/label.py
+++ /dev/null
@@ -1,203 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""The label of benchamrks."""
-
-
-import hashlib
-import os
-
-from cros_utils import misc
-from cros_utils.file_utils import FileUtils
-from image_checksummer import ImageChecksummer
-
-
-class Label(object):
-    """The label class."""
-
-    def __init__(
-        self,
-        name,
-        build,
-        chromeos_image,
-        autotest_path,
-        debug_path,
-        chromeos_root,
-        board,
-        remote,
-        image_args,
-        cache_dir,
-        cache_only,
-        log_level,
-        compiler,
-        crosfleet=False,
-        chrome_src=None,
-    ):
-
-        self.image_type = self._GetImageType(chromeos_image)
-
-        # Expand ~
-        chromeos_root = os.path.expanduser(chromeos_root)
-        if self.image_type == "local":
-            chromeos_image = os.path.expanduser(chromeos_image)
-
-        self.name = name
-        self.build = build
-        self.chromeos_image = chromeos_image
-        self.autotest_path = autotest_path
-        self.debug_path = debug_path
-        self.board = board
-        self.remote = remote
-        self.image_args = image_args
-        self.cache_dir = cache_dir
-        self.cache_only = cache_only
-        self.log_level = log_level
-        self.chrome_version = ""
-        self.compiler = compiler
-        self.crosfleet = crosfleet
-
-        if not chromeos_root:
-            if self.image_type == "local":
-                chromeos_root = FileUtils().ChromeOSRootFromImage(
-                    chromeos_image
-                )
-            if not chromeos_root:
-                raise RuntimeError(
-                    "No ChromeOS root given for label '%s' and could "
-                    "not determine one from image path: '%s'."
-                    % (name, chromeos_image)
-                )
-        else:
-            chromeos_root = FileUtils().CanonicalizeChromeOSRoot(chromeos_root)
-            if not chromeos_root:
-                raise RuntimeError(
-                    "Invalid ChromeOS root given for label '%s': '%s'."
-                    % (name, chromeos_root)
-                )
-
-        self.chromeos_root = chromeos_root
-        if not chrome_src:
-            # Old and new chroots may have different chrome src locations.
-            # The path also depends on the chrome build flags.
-            # Give priority to chrome-src-internal.
-            chrome_src_rel_paths = [
-                ".cache/distfiles/target/chrome-src-internal",
-                ".cache/distfiles/chrome-src-internal",
-                ".cache/distfiles/target/chrome-src",
-                ".cache/distfiles/chrome-src",
-            ]
-            for chrome_src_rel_path in chrome_src_rel_paths:
-                chrome_src_abs_path = os.path.join(
-                    self.chromeos_root, chrome_src_rel_path
-                )
-                if os.path.exists(chrome_src_abs_path):
-                    chrome_src = chrome_src_abs_path
-                    break
-            if not chrome_src:
-                raise RuntimeError(
-                    "Can not find location of Chrome sources.\n"
-                    f"Checked paths: {chrome_src_rel_paths}"
-                )
-        else:
-            chrome_src = misc.CanonicalizePath(chrome_src)
-            # Make sure the path exists.
-            if not os.path.exists(chrome_src):
-                raise RuntimeError(
-                    "Invalid Chrome src given for label '%s': '%s'."
-                    % (name, chrome_src)
-                )
-        self.chrome_src = chrome_src
-
-        self._SetupChecksum()
-
-    def _SetupChecksum(self):
-        """Compute label checksum only once."""
-
-        self.checksum = None
-        if self.image_type == "local":
-            self.checksum = ImageChecksummer().Checksum(self, self.log_level)
-        elif self.image_type == "trybot":
-            self.checksum = hashlib.md5(
-                self.chromeos_image.encode("utf-8")
-            ).hexdigest()
-
-    def _GetImageType(self, chromeos_image):
-        image_type = None
-        if chromeos_image.find("xbuddy://") < 0:
-            image_type = "local"
-        elif chromeos_image.find("trybot") >= 0:
-            image_type = "trybot"
-        else:
-            image_type = "official"
-        return image_type
-
-    def __hash__(self):
-        """Label objects are used in a map, so provide "hash" and "equal"."""
-
-        return hash(self.name)
-
-    def __eq__(self, other):
-        """Label objects are used in a map, so provide "hash" and "equal"."""
-
-        return isinstance(other, Label) and other.name == self.name
-
-    def __str__(self):
-        """For better debugging."""
-
-        return 'label[name="{}"]'.format(self.name)
-
-
-class MockLabel(object):
-    """The mock label class."""
-
-    def __init__(
-        self,
-        name,
-        build,
-        chromeos_image,
-        autotest_path,
-        debug_path,
-        chromeos_root,
-        board,
-        remote,
-        image_args,
-        cache_dir,
-        cache_only,
-        log_level,
-        compiler,
-        crosfleet=False,
-        chrome_src=None,
-    ):
-        self.name = name
-        self.build = build
-        self.chromeos_image = chromeos_image
-        self.autotest_path = autotest_path
-        self.debug_path = debug_path
-        self.board = board
-        self.remote = remote
-        self.cache_dir = cache_dir
-        self.cache_only = cache_only
-        if not chromeos_root:
-            self.chromeos_root = "/tmp/chromeos_root"
-        else:
-            self.chromeos_root = chromeos_root
-        self.image_args = image_args
-        self.chrome_src = chrome_src
-        self.image_type = self._GetImageType(chromeos_image)
-        self.checksum = ""
-        self.log_level = log_level
-        self.compiler = compiler
-        self.crosfleet = crosfleet
-        self.chrome_version = "Fake Chrome Version 50"
-
-    def _GetImageType(self, chromeos_image):
-        image_type = None
-        if chromeos_image.find("xbuddy://") < 0:
-            image_type = "local"
-        elif chromeos_image.find("trybot") >= 0:
-            image_type = "trybot"
-        else:
-            image_type = "official"
-        return image_type
diff --git a/crosperf/machine_image_manager.py b/crosperf/machine_image_manager.py
deleted file mode 100644
index 74379bff..00000000
--- a/crosperf/machine_image_manager.py
+++ /dev/null
@@ -1,315 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2015 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""MachineImageManager allocates images to duts."""
-
-
-import functools
-
-
-class MachineImageManager(object):
-    """Management of allocating images to duts.
-
-    * Data structure we have -
-
-      duts_ - list of duts, for each duts, we assume the following 2 properties
-      exist - label_ (the current label the duts_ carries or None, if it has an
-      alien image) and name (a string)
-
-      labels_ - a list of labels, for each label, we assume these properties
-      exist - remote (a set/vector/list of dut names (not dut object), to each
-      of which this image is compatible), remote could be none, which means
-      universal compatible.
-
-      label_duts_ - for each label, we maintain a list of duts, onto which the
-      label is imaged. Note this is an array of lists. Each element of each list
-      is an integer which is dut oridnal. We access this array using label
-      ordinal.
-
-      allocate_log_ - a list of allocation record. For example, if we allocate
-      l1 to d1, then l2 to d2, then allocate_log_ would be [(1, 1), (2, 2)].
-      This is used for debug/log, etc. All tuples in the list are integer pairs
-      (label_ordinal, dut_ordinal).
-
-      n_duts_ - number of duts.
-
-      n_labels_ - number of labels.
-
-      dut_name_ordinal_ - mapping from dut name (a string) to an integer,
-      starting from 0. So that duts_[dut_name_ordinal_[a_dut.name]]= a_dut.
-
-    * Problem abstraction -
-
-      Assume we have the following matrix - label X machine (row X col). A 'X'
-      in (i, j) in the matrix means machine and lable is not compatible, or that
-      we cannot image li to Mj.
-
-           M1   M2   M3
-      L1    X
-
-      L2             X
-
-      L3    X    X
-
-      Now that we'll try to find a way to fill Ys in the matrix so that -
-
-        a) - each row at least get a Y, this ensures that each label get imaged
-        at least once, an apparent prerequiste.
-
-        b) - each column get at most N Ys. This make sure we can successfully
-        finish all tests by re-image each machine at most N times. That being
-        said, we could *OPTIONALLY* reimage some machines more than N times to
-        *accelerate* the test speed.
-
-      How to choose initial N for b) -
-      If number of duts (nd) is equal to or more than that of labels (nl), we
-      start from N == 1. Else we start from N = nl - nd + 1.
-
-      We will begin the search with pre-defined N, if we fail to find such a
-      solution for such N, we increase N by 1 and continue the search till we
-      get N == nl, at this case we fails.
-
-      Such a solution ensures minimal number of reimages.
-
-    * Solution representation
-
-      The solution will be placed inside the matrix, like below
-
-           M1   M2   M3   M4
-      L1    X    X    Y
-
-      L2    Y         X
-
-      L3    X    Y    X
-
-    * Allocation algorithm
-
-      When Mj asks for a image, we check column j, pick the first cell that
-      contains a 'Y', and mark the cell '_'. If no such 'Y' exists (like M4 in
-      the above solution matrix), we just pick an image that the minimal reimage
-      number.
-
-      After allocate for M3
-           M1   M2   M3   M4
-      L1    X    X    _
-
-      L2    Y         X
-
-      L3    X    Y    X
-
-      After allocate for M4
-           M1   M2   M3   M4
-      L1    X    X    _
-
-      L2    Y         X    _
-
-      L3    X    Y    X
-
-      After allocate for M2
-           M1   M2   M3   M4
-      L1    X    X    _
-
-      L2    Y         X    _
-
-      L3    X    _    X
-
-      After allocate for M1
-           M1   M2   M3   M4
-      L1    X    X    _
-
-      L2    _         X    _
-
-      L3    X    _    X
-
-      After allocate for M2
-           M1   M2   M3   M4
-      L1    X    X    _
-
-      L2    _    _    X    _
-
-      L3    X    _    X
-
-      If we try to allocate for M1 or M2 or M3 again, we get None.
-
-    * Special / common case to handle seperately
-
-      We have only 1 dut or if we have only 1 label, that's simple enough.
-    """
-
-    def __init__(self, labels, duts):
-        self.labels_ = labels
-        self.duts_ = duts
-        self.n_labels_ = len(labels)
-        self.n_duts_ = len(duts)
-        self.dut_name_ordinal_ = dict()
-        for idx, dut in enumerate(self.duts_):
-            self.dut_name_ordinal_[dut.name] = idx
-
-        # Generate initial matrix containg 'X' or ' '.
-        self.matrix_ = [
-            ["X" if l.remote else " " for _ in range(self.n_duts_)]
-            for l in self.labels_
-        ]
-        for ol, l in enumerate(self.labels_):
-            if l.remote:
-                for r in l.remote:
-                    self.matrix_[ol][self.dut_name_ordinal_[r]] = " "
-
-        self.label_duts_ = [[] for _ in range(self.n_labels_)]
-        self.allocate_log_ = []
-
-    def compute_initial_allocation(self):
-        """Compute the initial label-dut allocation.
-
-        This method finds the most efficient way that every label gets imaged at
-        least once.
-
-        Returns:
-          False, only if not all labels could be imaged to a certain machine,
-          otherwise True.
-        """
-
-        if self.n_duts_ == 1:
-            for i, v in self.matrix_vertical_generator(0):
-                if v != "X":
-                    self.matrix_[i][0] = "Y"
-            return
-
-        if self.n_labels_ == 1:
-            for j, v in self.matrix_horizontal_generator(0):
-                if v != "X":
-                    self.matrix_[0][j] = "Y"
-            return
-
-        if self.n_duts_ >= self.n_labels_:
-            n = 1
-        else:
-            n = self.n_labels_ - self.n_duts_ + 1
-        while n <= self.n_labels_:
-            if self._compute_initial_allocation_internal(0, n):
-                break
-            n += 1
-
-        return n <= self.n_labels_
-
-    def _record_allocate_log(self, label_i, dut_j):
-        self.allocate_log_.append((label_i, dut_j))
-        self.label_duts_[label_i].append(dut_j)
-
-    def allocate(self, dut, schedv2=None):
-        """Allocate a label for dut.
-
-        Args:
-          dut: the dut that asks for a new image.
-          schedv2: the scheduling instance, we need the benchmark run
-                   information with schedv2 for a better allocation.
-
-        Returns:
-          a label to image onto the dut or None if no more available images for
-          the dut.
-        """
-        j = self.dut_name_ordinal_[dut.name]
-        # 'can_' prefix means candidate label's.
-        can_reimage_number = 999
-        can_i = 999
-        can_label = None
-        can_pending_br_num = 0
-        for i, v in self.matrix_vertical_generator(j):
-            label = self.labels_[i]
-
-            # 2 optimizations here regarding allocating label to dut.
-            # Note schedv2 might be None in case we do not need this
-            # optimization or we are in testing mode.
-            if schedv2 is not None:
-                pending_br_num = len(schedv2.get_label_map()[label])
-                if pending_br_num == 0:
-                    # (A) - we have finished all br of this label,
-                    # apparently, we do not want to reimaeg dut to
-                    # this label.
-                    continue
-            else:
-                # In case we do not have a schedv2 instance, mark
-                # pending_br_num as 0, so pending_br_num >=
-                # can_pending_br_num is always True.
-                pending_br_num = 0
-
-            # For this time being, I just comment this out until we have a
-            # better estimation how long each benchmarkrun takes.
-            # if (pending_br_num <= 5 and
-            #     len(self.label_duts_[i]) >= 1):
-            #     # (B) this is heuristic - if there are just a few test cases
-            #     # (say <5) left undone for this label, and there is at least
-            #     # 1 other machine working on this lable, we probably not want
-            #     # to bother to reimage this dut to help with these 5 test
-            #     # cases
-            #     continue
-
-            if v == "Y":
-                self.matrix_[i][j] = "_"
-                self._record_allocate_log(i, j)
-                return label
-            if v == " ":
-                label_reimage_number = len(self.label_duts_[i])
-                if (can_label is None) or (
-                    label_reimage_number < can_reimage_number
-                    or (
-                        label_reimage_number == can_reimage_number
-                        and pending_br_num >= can_pending_br_num
-                    )
-                ):
-                    can_reimage_number = label_reimage_number
-                    can_i = i
-                    can_label = label
-                    can_pending_br_num = pending_br_num
-
-        # All labels are marked either '_' (already taken) or 'X' (not
-        # compatible), so return None to notify machine thread to quit.
-        if can_label is None:
-            return None
-
-        # At this point, we don't find any 'Y' for the machine, so we go the
-        # 'min' approach.
-        self.matrix_[can_i][j] = "_"
-        self._record_allocate_log(can_i, j)
-        return can_label
-
-    def matrix_vertical_generator(self, col):
-        """Iterate matrix vertically at column 'col'.
-
-        Yield row number i and value at matrix_[i][col].
-        """
-        for i, _ in enumerate(self.labels_):
-            yield i, self.matrix_[i][col]
-
-    def matrix_horizontal_generator(self, row):
-        """Iterate matrix horizontally at row 'row'.
-
-        Yield col number j and value at matrix_[row][j].
-        """
-        for j, _ in enumerate(self.duts_):
-            yield j, self.matrix_[row][j]
-
-    def _compute_initial_allocation_internal(self, level, N):
-        """Search matrix for d with N."""
-
-        if level == self.n_labels_:
-            return True
-
-        for j, v in self.matrix_horizontal_generator(level):
-            if v == " ":
-                # Before we put a 'Y', we check how many Y column 'j' has.
-                # Note y[0] is row idx, y[1] is the cell value.
-                ny = functools.reduce(
-                    lambda x, y: x + 1 if (y[1] == "Y") else x,
-                    self.matrix_vertical_generator(j),
-                    0,
-                )
-                if ny < N:
-                    self.matrix_[level][j] = "Y"
-                    if self._compute_initial_allocation_internal(level + 1, N):
-                        return True
-                    self.matrix_[level][j] = " "
-
-        return False
diff --git a/crosperf/machine_image_manager_unittest.py b/crosperf/machine_image_manager_unittest.py
deleted file mode 100755
index 1ea63b1c..00000000
--- a/crosperf/machine_image_manager_unittest.py
+++ /dev/null
@@ -1,339 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2015 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unit tests for the MachineImageManager class."""
-
-
-import random
-import unittest
-
-from machine_image_manager import MachineImageManager
-
-
-class MockLabel(object):
-    """Class for generating a mock Label."""
-
-    def __init__(self, name, remotes=None):
-        self.name = name
-        self.remote = remotes
-
-    def __hash__(self):
-        """Provide hash function for label.
-
-        This is required because Label object is used inside a dict as key.
-        """
-        return hash(self.name)
-
-    def __eq__(self, other):
-        """Provide eq function for label.
-
-        This is required because Label object is used inside a dict as key.
-        """
-        return isinstance(other, MockLabel) and other.name == self.name
-
-
-class MockDut(object):
-    """Class for creating a mock Device-Under-Test (DUT)."""
-
-    def __init__(self, name, label=None):
-        self.name = name
-        self.label_ = label
-
-
-class MachineImageManagerTester(unittest.TestCase):
-    """Class for testing MachineImageManager."""
-
-    def gen_duts_by_name(self, *names):
-        duts = []
-        for n in names:
-            duts.append(MockDut(n))
-        return duts
-
-    def create_labels_and_duts_from_pattern(self, pattern):
-        labels = []
-        duts = []
-        for i, r in enumerate(pattern):
-            l = MockLabel("l{}".format(i), [])
-            for j, v in enumerate(r.split()):
-                if v == ".":
-                    l.remote.append("m{}".format(j))
-                if i == 0:
-                    duts.append(MockDut("m{}".format(j)))
-            labels.append(l)
-        return labels, duts
-
-    def check_matrix_against_pattern(self, matrix, pattern):
-        for i, s in enumerate(pattern):
-            for j, v in enumerate(s.split()):
-                self.assertTrue(
-                    v == "." and matrix[i][j] == " " or v == matrix[i][j]
-                )
-
-    def pattern_based_test(self, inp, output):
-        labels, duts = self.create_labels_and_duts_from_pattern(inp)
-        mim = MachineImageManager(labels, duts)
-        self.assertTrue(mim.compute_initial_allocation())
-        self.check_matrix_against_pattern(mim.matrix_, output)
-        return mim
-
-    def test_single_dut(self):
-        labels = [MockLabel("l1"), MockLabel("l2"), MockLabel("l3")]
-        dut = MockDut("m1")
-        mim = MachineImageManager(labels, [dut])
-        mim.compute_initial_allocation()
-        self.assertTrue(mim.matrix_ == [["Y"], ["Y"], ["Y"]])
-
-    def test_single_label(self):
-        labels = [MockLabel("l1")]
-        duts = self.gen_duts_by_name("m1", "m2", "m3")
-        mim = MachineImageManager(labels, duts)
-        mim.compute_initial_allocation()
-        self.assertTrue(mim.matrix_ == [["Y", "Y", "Y"]])
-
-    def test_case1(self):
-        labels = [
-            MockLabel("l1", ["m1", "m2"]),
-            MockLabel("l2", ["m2", "m3"]),
-            MockLabel("l3", ["m1"]),
-        ]
-        duts = [MockDut("m1"), MockDut("m2"), MockDut("m3")]
-        mim = MachineImageManager(labels, duts)
-        self.assertTrue(
-            mim.matrix_ == [[" ", " ", "X"], ["X", " ", " "], [" ", "X", "X"]]
-        )
-        mim.compute_initial_allocation()
-        self.assertTrue(
-            mim.matrix_ == [[" ", "Y", "X"], ["X", " ", "Y"], ["Y", "X", "X"]]
-        )
-
-    def test_case2(self):
-        labels = [
-            MockLabel("l1", ["m1", "m2"]),
-            MockLabel("l2", ["m2", "m3"]),
-            MockLabel("l3", ["m1"]),
-        ]
-        duts = [MockDut("m1"), MockDut("m2"), MockDut("m3")]
-        mim = MachineImageManager(labels, duts)
-        self.assertTrue(
-            mim.matrix_ == [[" ", " ", "X"], ["X", " ", " "], [" ", "X", "X"]]
-        )
-        mim.compute_initial_allocation()
-        self.assertTrue(
-            mim.matrix_ == [[" ", "Y", "X"], ["X", " ", "Y"], ["Y", "X", "X"]]
-        )
-
-    def test_case3(self):
-        labels = [
-            MockLabel("l1", ["m1", "m2"]),
-            MockLabel("l2", ["m2", "m3"]),
-            MockLabel("l3", ["m1"]),
-        ]
-        duts = [MockDut("m1", labels[0]), MockDut("m2"), MockDut("m3")]
-        mim = MachineImageManager(labels, duts)
-        mim.compute_initial_allocation()
-        self.assertTrue(
-            mim.matrix_ == [[" ", "Y", "X"], ["X", " ", "Y"], ["Y", "X", "X"]]
-        )
-
-    def test_case4(self):
-        labels = [
-            MockLabel("l1", ["m1", "m2"]),
-            MockLabel("l2", ["m2", "m3"]),
-            MockLabel("l3", ["m1"]),
-        ]
-        duts = [MockDut("m1"), MockDut("m2", labels[0]), MockDut("m3")]
-        mim = MachineImageManager(labels, duts)
-        mim.compute_initial_allocation()
-        self.assertTrue(
-            mim.matrix_ == [[" ", "Y", "X"], ["X", " ", "Y"], ["Y", "X", "X"]]
-        )
-
-    def test_case5(self):
-        labels = [
-            MockLabel("l1", ["m3"]),
-            MockLabel("l2", ["m3"]),
-            MockLabel("l3", ["m1"]),
-        ]
-        duts = self.gen_duts_by_name("m1", "m2", "m3")
-        mim = MachineImageManager(labels, duts)
-        self.assertTrue(mim.compute_initial_allocation())
-        self.assertTrue(
-            mim.matrix_ == [["X", "X", "Y"], ["X", "X", "Y"], ["Y", "X", "X"]]
-        )
-
-    def test_2x2_with_allocation(self):
-        labels = [MockLabel("l0"), MockLabel("l1")]
-        duts = [MockDut("m0"), MockDut("m1")]
-        mim = MachineImageManager(labels, duts)
-        self.assertTrue(mim.compute_initial_allocation())
-        self.assertTrue(mim.allocate(duts[0]) == labels[0])
-        self.assertTrue(mim.allocate(duts[0]) == labels[1])
-        self.assertTrue(mim.allocate(duts[0]) is None)
-        self.assertTrue(mim.matrix_[0][0] == "_")
-        self.assertTrue(mim.matrix_[1][0] == "_")
-        self.assertTrue(mim.allocate(duts[1]) == labels[1])
-
-    def test_10x10_general(self):
-        """Gen 10x10 matrix."""
-        n = 10
-        labels = []
-        duts = []
-        for i in range(n):
-            labels.append(MockLabel("l{}".format(i)))
-            duts.append(MockDut("m{}".format(i)))
-        mim = MachineImageManager(labels, duts)
-        self.assertTrue(mim.compute_initial_allocation())
-        for i in range(n):
-            for j in range(n):
-                if i == j:
-                    self.assertTrue(mim.matrix_[i][j] == "Y")
-                else:
-                    self.assertTrue(mim.matrix_[i][j] == " ")
-        self.assertTrue(mim.allocate(duts[3]).name == "l3")
-
-    def test_random_generated(self):
-        n = 10
-        labels = []
-        duts = []
-        for i in range(10):
-            # generate 3-5 machines that is compatible with this label
-            l = MockLabel("l{}".format(i), [])
-            r = random.random()
-            for _ in range(4):
-                t = int(r * 10) % n
-                r *= 10
-                l.remote.append("m{}".format(t))
-            labels.append(l)
-            duts.append(MockDut("m{}".format(i)))
-        mim = MachineImageManager(labels, duts)
-        self.assertTrue(mim.compute_initial_allocation())
-
-    def test_10x10_fully_random(self):
-        inp = [
-            "X  .  .  .  X  X  .  X  X  .",
-            "X  X  .  X  .  X  .  X  X  .",
-            "X  X  X  .  .  X  .  X  .  X",
-            "X  .  X  X  .  .  X  X  .  X",
-            "X  X  X  X  .  .  .  X  .  .",
-            "X  X  .  X  .  X  .  .  X  .",
-            ".  X  .  X  .  X  X  X  .  .",
-            ".  X  .  X  X  .  X  X  .  .",
-            "X  X  .  .  .  X  X  X  .  .",
-            ".  X  X  X  X  .  .  .  .  X",
-        ]
-        output = [
-            "X  Y  .  .  X  X  .  X  X  .",
-            "X  X  Y  X  .  X  .  X  X  .",
-            "X  X  X  Y  .  X  .  X  .  X",
-            "X  .  X  X  Y  .  X  X  .  X",
-            "X  X  X  X  .  Y  .  X  .  .",
-            "X  X  .  X  .  X  Y  .  X  .",
-            "Y  X  .  X  .  X  X  X  .  .",
-            ".  X  .  X  X  .  X  X  Y  .",
-            "X  X  .  .  .  X  X  X  .  Y",
-            ".  X  X  X  X  .  .  Y  .  X",
-        ]
-        self.pattern_based_test(inp, output)
-
-    def test_10x10_fully_random2(self):
-        inp = [
-            "X  .  X  .  .  X  .  X  X  X",
-            "X  X  X  X  X  X  .  .  X  .",
-            "X  .  X  X  X  X  X  .  .  X",
-            "X  X  X  .  X  .  X  X  .  .",
-            ".  X  .  X  .  X  X  X  X  X",
-            "X  X  X  X  X  X  X  .  .  X",
-            "X  .  X  X  X  X  X  .  .  X",
-            "X  X  X  .  X  X  X  X  .  .",
-            "X  X  X  .  .  .  X  X  X  X",
-            ".  X  X  .  X  X  X  .  X  X",
-        ]
-        output = [
-            "X  .  X  Y  .  X  .  X  X  X",
-            "X  X  X  X  X  X  Y  .  X  .",
-            "X  Y  X  X  X  X  X  .  .  X",
-            "X  X  X  .  X  Y  X  X  .  .",
-            ".  X  Y  X  .  X  X  X  X  X",
-            "X  X  X  X  X  X  X  Y  .  X",
-            "X  .  X  X  X  X  X  .  Y  X",
-            "X  X  X  .  X  X  X  X  .  Y",
-            "X  X  X  .  Y  .  X  X  X  X",
-            "Y  X  X  .  X  X  X  .  X  X",
-        ]
-        self.pattern_based_test(inp, output)
-
-    def test_3x4_with_allocation(self):
-        inp = ["X  X  .  .", ".  .  X  .", "X  .  X  ."]
-        output = ["X  X  Y  .", "Y  .  X  .", "X  Y  X  ."]
-        mim = self.pattern_based_test(inp, output)
-        self.assertTrue(mim.allocate(mim.duts_[2]) == mim.labels_[0])
-        self.assertTrue(mim.allocate(mim.duts_[3]) == mim.labels_[2])
-        self.assertTrue(mim.allocate(mim.duts_[0]) == mim.labels_[1])
-        self.assertTrue(mim.allocate(mim.duts_[1]) == mim.labels_[2])
-        self.assertTrue(mim.allocate(mim.duts_[3]) == mim.labels_[1])
-        self.assertTrue(mim.allocate(mim.duts_[3]) == mim.labels_[0])
-        self.assertTrue(mim.allocate(mim.duts_[3]) is None)
-        self.assertTrue(mim.allocate(mim.duts_[2]) is None)
-        self.assertTrue(mim.allocate(mim.duts_[1]) == mim.labels_[1])
-        self.assertTrue(mim.allocate(mim.duts_[1]) is None)
-        self.assertTrue(mim.allocate(mim.duts_[0]) is None)
-        self.assertTrue(mim.label_duts_[0] == [2, 3])
-        self.assertTrue(mim.label_duts_[1] == [0, 3, 1])
-        self.assertTrue(mim.label_duts_[2] == [3, 1])
-        self.assertListEqual(
-            mim.allocate_log_,
-            [(0, 2), (2, 3), (1, 0), (2, 1), (1, 3), (0, 3), (1, 1)],
-        )
-
-    def test_cornercase_1(self):
-        """This corner case is brought up by Caroline.
-
-        The description is -
-
-        If you have multiple labels and multiple machines, (so we don't
-        automatically fall into the 1 dut or 1 label case), but all of the
-        labels specify the same 1 remote, then instead of assigning the same
-        machine to all the labels, your algorithm fails to assign any...
-
-        So first step is to create an initial matrix like below, l0, l1 and l2
-        all specify the same 1 remote - m0.
-
-             m0    m1    m2
-        l0   .     X     X
-
-        l1   .     X     X
-
-        l2   .     X     X
-
-        The search process will be like this -
-        a) try to find a solution with at most 1 'Y's per column (but ensure at
-        least 1 Y per row), fail
-        b) try to find a solution with at most 2 'Y's per column (but ensure at
-        least 1 Y per row), fail
-        c) try to find a solution with at most 3 'Y's per column (but ensure at
-        least 1 Y per row), succeed, so we end up having this solution
-
-            m0    m1    m2
-        l0   Y     X     X
-
-        l1   Y     X     X
-
-        l2   Y     X     X
-        """
-
-        inp = [".  X  X", ".  X  X", ".  X  X"]
-        output = ["Y  X  X", "Y  X  X", "Y  X  X"]
-        mim = self.pattern_based_test(inp, output)
-        self.assertTrue(mim.allocate(mim.duts_[1]) is None)
-        self.assertTrue(mim.allocate(mim.duts_[2]) is None)
-        self.assertTrue(mim.allocate(mim.duts_[0]) == mim.labels_[0])
-        self.assertTrue(mim.allocate(mim.duts_[0]) == mim.labels_[1])
-        self.assertTrue(mim.allocate(mim.duts_[0]) == mim.labels_[2])
-        self.assertTrue(mim.allocate(mim.duts_[0]) is None)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/machine_manager.py b/crosperf/machine_manager.py
deleted file mode 100644
index 17db64f5..00000000
--- a/crosperf/machine_manager.py
+++ /dev/null
@@ -1,809 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Machine Manager module."""
-
-
-import collections
-import hashlib
-import math
-import os.path
-import re
-import sys
-import threading
-import time
-
-from cros_utils import command_executer
-from cros_utils import logger
-import file_lock_machine
-import image_chromeos
-import test_flag
-
-
-CHECKSUM_FILE = "/usr/local/osimage_checksum_file"
-
-
-class BadChecksum(Exception):
-    """Raised if all machines for a label don't have the same checksum."""
-
-
-class BadChecksumString(Exception):
-    """Raised if all machines for a label don't have the same checksum string."""
-
-
-class MissingLocksDirectory(Exception):
-    """Raised when cannot find/access the machine locks directory."""
-
-
-class CrosCommandError(Exception):
-    """Raised when an error occurs running command on DUT."""
-
-
-class CrosMachine(object):
-    """The machine class."""
-
-    def __init__(self, name, chromeos_root, log_level, cmd_exec=None):
-        self.name = name
-        self.image = None
-        # We relate a dut with a label if we reimage the dut using label or we
-        # detect at the very beginning that the dut is running this label.
-        self.label = None
-        self.checksum = None
-        self.locked = False
-        self.released_time = time.time()
-        self.test_run = None
-        self.chromeos_root = chromeos_root
-        self.log_level = log_level
-        self.cpuinfo = None
-        self.machine_id = None
-        self.checksum_string = None
-        self.meminfo = None
-        self.phys_kbytes = None
-        self.cooldown_wait_time = 0
-        self.ce = cmd_exec or command_executer.GetCommandExecuter(
-            log_level=self.log_level
-        )
-        self.SetUpChecksumInfo()
-
-    def SetUpChecksumInfo(self):
-        if not self.IsReachable():
-            self.machine_checksum = None
-            return
-        self._GetMemoryInfo()
-        self._GetCPUInfo()
-        self._ComputeMachineChecksumString()
-        self._GetMachineID()
-        self.machine_checksum = self._GetMD5Checksum(self.checksum_string)
-        self.machine_id_checksum = self._GetMD5Checksum(self.machine_id)
-
-    def IsReachable(self):
-        command = "ls"
-        ret = self.ce.CrosRunCommand(
-            command, machine=self.name, chromeos_root=self.chromeos_root
-        )
-        if ret:
-            return False
-        return True
-
-    def AddCooldownWaitTime(self, wait_time):
-        self.cooldown_wait_time += wait_time
-
-    def GetCooldownWaitTime(self):
-        return self.cooldown_wait_time
-
-    def _ParseMemoryInfo(self):
-        line = self.meminfo.splitlines()[0]
-        usable_kbytes = int(line.split()[1])
-        # This code is from src/third_party/test/files/client/bin/base_utils.py
-        # usable_kbytes is system's usable DRAM in kbytes,
-        #   as reported by memtotal() from device /proc/meminfo memtotal
-        #   after Linux deducts 1.5% to 9.5% for system table overhead
-        # Undo the unknown actual deduction by rounding up
-        #   to next small multiple of a big power-of-two
-        #   eg  12GB - 5.1% gets rounded back up to 12GB
-        mindeduct = 0.005  # 0.5 percent
-        maxdeduct = 0.095  # 9.5 percent
-        # deduction range 1.5% .. 9.5% supports physical mem sizes
-        #    6GB .. 12GB in steps of .5GB
-        #   12GB .. 24GB in steps of 1 GB
-        #   24GB .. 48GB in steps of 2 GB ...
-        # Finer granularity in physical mem sizes would require
-        #   tighter spread between min and max possible deductions
-
-        # increase mem size by at least min deduction, without rounding
-        min_kbytes = int(usable_kbytes / (1.0 - mindeduct))
-        # increase mem size further by 2**n rounding, by 0..roundKb or more
-        round_kbytes = int(usable_kbytes / (1.0 - maxdeduct)) - min_kbytes
-        # find least binary roundup 2**n that covers worst-cast roundKb
-        mod2n = 1 << int(math.ceil(math.log(round_kbytes, 2)))
-        # have round_kbytes <= mod2n < round_kbytes*2
-        # round min_kbytes up to next multiple of mod2n
-        phys_kbytes = min_kbytes + mod2n - 1
-        phys_kbytes -= phys_kbytes % mod2n  # clear low bits
-        self.phys_kbytes = phys_kbytes
-
-    def _GetMemoryInfo(self):
-        # TODO yunlian: when the machine in rebooting, it will not return
-        # meminfo, the assert does not catch it either
-        command = "cat /proc/meminfo"
-        ret, self.meminfo, _ = self.ce.CrosRunCommandWOutput(
-            command, machine=self.name, chromeos_root=self.chromeos_root
-        )
-        assert ret == 0, "Could not get meminfo from machine: %s" % self.name
-        if ret == 0:
-            self._ParseMemoryInfo()
-
-    def _GetCPUInfo(self):
-        command = "cat /proc/cpuinfo"
-        ret, self.cpuinfo, _ = self.ce.CrosRunCommandWOutput(
-            command, machine=self.name, chromeos_root=self.chromeos_root
-        )
-        assert ret == 0, "Could not get cpuinfo from machine: %s" % self.name
-
-    def _ComputeMachineChecksumString(self):
-        self.checksum_string = ""
-        # Some lines from cpuinfo have to be excluded because they are not
-        # persistent across DUTs.
-        # MHz, BogoMIPS are dynamically changing values.
-        # core id, apicid are identifiers assigned on startup
-        # and may differ on the same type of machine.
-        exclude_lines_list = [
-            "MHz",
-            "BogoMIPS",
-            "bogomips",
-            "core id",
-            "apicid",
-        ]
-        for line in self.cpuinfo.splitlines():
-            if not any(e in line for e in exclude_lines_list):
-                self.checksum_string += line
-        self.checksum_string += " " + str(self.phys_kbytes)
-
-    def _GetMD5Checksum(self, ss):
-        if ss:
-            return hashlib.md5(ss.encode("utf-8")).hexdigest()
-        return ""
-
-    def _GetMachineID(self):
-        command = "dump_vpd_log --full --stdout"
-        _, if_out, _ = self.ce.CrosRunCommandWOutput(
-            command, machine=self.name, chromeos_root=self.chromeos_root
-        )
-        b = if_out.splitlines()
-        a = [l for l in b if "Product" in l]
-        if a:
-            self.machine_id = a[0]
-            return
-        command = "ifconfig"
-        _, if_out, _ = self.ce.CrosRunCommandWOutput(
-            command, machine=self.name, chromeos_root=self.chromeos_root
-        )
-        b = if_out.splitlines()
-        a = [l for l in b if "HWaddr" in l]
-        if a:
-            self.machine_id = "_".join(a)
-            return
-        a = [l for l in b if "ether" in l]
-        if a:
-            self.machine_id = "_".join(a)
-            return
-        assert 0, "Could not get machine_id from machine: %s" % self.name
-
-    def __str__(self):
-        l = []
-        l.append(self.name)
-        l.append(str(self.image))
-        l.append(str(self.checksum))
-        l.append(str(self.locked))
-        l.append(str(self.released_time))
-        return ", ".join(l)
-
-
-class MachineManager(object):
-    """Lock, image and unlock machines locally for benchmark runs.
-
-    This class contains methods and calls to lock, unlock and image
-    machines and distribute machines to each benchmark run.  The assumption is
-    that all of the machines for the experiment have been globally locked
-    in the ExperimentRunner, but the machines still need to be locally
-    locked/unlocked (allocated to benchmark runs) to prevent multiple benchmark
-    runs within the same experiment from trying to use the same machine at the
-    same time.
-    """
-
-    def __init__(
-        self,
-        chromeos_root,
-        acquire_timeout,
-        log_level,
-        locks_dir,
-        cmd_exec=None,
-        lgr=None,
-        keep_stateful: bool = False,
-    ):
-        self._lock = threading.RLock()
-        self._all_machines = []
-        self._machines = []
-        self.image_lock = threading.Lock()
-        self.num_reimages = 0
-        self.chromeos_root = None
-        self.machine_checksum = {}
-        self.machine_checksum_string = {}
-        self.acquire_timeout = acquire_timeout
-        self.log_level = log_level
-        self.locks_dir = locks_dir
-        self.keep_stateful = keep_stateful
-        self.ce = cmd_exec or command_executer.GetCommandExecuter(
-            log_level=self.log_level
-        )
-        self.logger = lgr or logger.GetLogger()
-
-        if self.locks_dir and not os.path.isdir(self.locks_dir):
-            raise MissingLocksDirectory(
-                "Cannot access locks directory: %s" % self.locks_dir
-            )
-
-        self._initialized_machines = []
-        self.chromeos_root = chromeos_root
-
-    def RemoveNonLockedMachines(self, locked_machines):
-        for m in self._all_machines:
-            if m.name not in locked_machines:
-                self._all_machines.remove(m)
-
-        for m in self._machines:
-            if m.name not in locked_machines:
-                self._machines.remove(m)
-
-    def GetChromeVersion(self, machine):
-        """Get the version of Chrome running on the DUT."""
-
-        cmd = "/opt/google/chrome/chrome --version"
-        ret, version, _ = self.ce.CrosRunCommandWOutput(
-            cmd, machine=machine.name, chromeos_root=self.chromeos_root
-        )
-        if ret != 0:
-            raise CrosCommandError(
-                "Couldn't get Chrome version from %s." % machine.name
-            )
-
-        if ret != 0:
-            version = ""
-        return version.rstrip()
-
-    def ImageMachine(self, machine, label):
-        checksum = label.checksum
-
-        if checksum and (machine.checksum == checksum):
-            return
-        chromeos_root = label.chromeos_root
-        if not chromeos_root:
-            chromeos_root = self.chromeos_root
-        image_chromeos_args = [
-            image_chromeos.__file__,
-            "--no_lock",
-            f"--chromeos_root={chromeos_root}",
-            f"--image={label.chromeos_image}",
-            f"--image_args={label.image_args}",
-            f"--remote={machine.name}",
-            f"--logging_level={self.log_level}",
-        ]
-        if label.board:
-            image_chromeos_args.append(f"--board={label.board}")
-        if self.keep_stateful:
-            image_chromeos_args.append("--keep_stateful")
-
-        # Currently can't image two machines at once.
-        # So have to serialized on this lock.
-        save_ce_log_level = self.ce.log_level
-        if self.log_level != "verbose":
-            self.ce.log_level = "average"
-
-        with self.image_lock:
-            if self.log_level != "verbose":
-                self.logger.LogOutput("Pushing image onto machine.")
-                self.logger.LogOutput(
-                    "Running image_chromeos.DoImage with %s"
-                    % " ".join(image_chromeos_args)
-                )
-            retval = 0
-            if not test_flag.GetTestMode():
-                retval = image_chromeos.DoImage(image_chromeos_args)
-            if retval:
-                cmd = "reboot && exit"
-                if self.log_level != "verbose":
-                    self.logger.LogOutput("reboot & exit.")
-                self.ce.CrosRunCommand(
-                    cmd, machine=machine.name, chromeos_root=self.chromeos_root
-                )
-                time.sleep(60)
-                if self.log_level != "verbose":
-                    self.logger.LogOutput("Pushing image onto machine.")
-                    self.logger.LogOutput(
-                        "Running image_chromeos.DoImage with %s"
-                        % " ".join(image_chromeos_args)
-                    )
-                retval = image_chromeos.DoImage(image_chromeos_args)
-            if retval:
-                raise RuntimeError(
-                    "Could not image machine: '%s'." % machine.name
-                )
-
-            self.num_reimages += 1
-            machine.checksum = checksum
-            machine.image = label.chromeos_image
-            machine.label = label
-
-        if not label.chrome_version:
-            label.chrome_version = self.GetChromeVersion(machine)
-
-        self.ce.log_level = save_ce_log_level
-        return retval
-
-    def ComputeCommonCheckSum(self, label):
-        # Since this is used for cache lookups before the machines have been
-        # compared/verified, check here to make sure they all have the same
-        # checksum (otherwise the cache lookup may not be valid).
-        base = None
-        for machine in self.GetMachines(label):
-            # Make sure the machine's checksums are calculated.
-            if not machine.machine_checksum:
-                machine.SetUpChecksumInfo()
-            # Use the first machine as the basis for comparison.
-            if not base:
-                base = machine
-            # Make sure this machine's checksum matches our 'common' checksum.
-            if base.machine_checksum != machine.machine_checksum:
-                # Found a difference. Fatal error.
-                # Extract non-matching part and report it.
-                for mismatch_index in range(len(base.checksum_string)):
-                    if (
-                        mismatch_index >= len(machine.checksum_string)
-                        or base.checksum_string[mismatch_index]
-                        != machine.checksum_string[mismatch_index]
-                    ):
-                        break
-                # We want to show some context after the mismatch.
-                end_ind = mismatch_index + 8
-                # Print a mismatching string.
-                raise BadChecksum(
-                    "Machine checksums do not match!\n"
-                    "Diff:\n"
-                    f"{base.name}: {base.checksum_string[:end_ind]}\n"
-                    f"{machine.name}: {machine.checksum_string[:end_ind]}\n"
-                    "\nCheck for matching /proc/cpuinfo and /proc/meminfo on DUTs.\n"
-                )
-        self.machine_checksum[label.name] = base.machine_checksum
-
-    def ComputeCommonCheckSumString(self, label):
-        # The assumption is that this function is only called AFTER
-        # ComputeCommonCheckSum, so there is no need to verify the machines
-        # are the same here.  If this is ever changed, this function should be
-        # modified to verify that all the machines for a given label are the
-        # same.
-        for machine in self.GetMachines(label):
-            if machine.checksum_string:
-                self.machine_checksum_string[
-                    label.name
-                ] = machine.checksum_string
-                break
-
-    def _TryToLockMachine(self, cros_machine):
-        with self._lock:
-            assert cros_machine, "Machine can't be None"
-            for m in self._machines:
-                if m.name == cros_machine.name:
-                    return
-            locked = True
-            if self.locks_dir:
-                locked = file_lock_machine.Machine(
-                    cros_machine.name, self.locks_dir
-                ).Lock(True, sys.argv[0])
-            if locked:
-                self._machines.append(cros_machine)
-                command = "cat %s" % CHECKSUM_FILE
-                ret, out, _ = self.ce.CrosRunCommandWOutput(
-                    command,
-                    chromeos_root=self.chromeos_root,
-                    machine=cros_machine.name,
-                )
-                if ret == 0:
-                    cros_machine.checksum = out.strip()
-            elif self.locks_dir:
-                self.logger.LogOutput("Couldn't lock: %s" % cros_machine.name)
-
-    # This is called from single threaded mode.
-    def AddMachine(self, machine_name):
-        with self._lock:
-            for m in self._all_machines:
-                assert m.name != machine_name, (
-                    "Tried to double-add %s" % machine_name
-                )
-
-            if self.log_level != "verbose":
-                self.logger.LogOutput(
-                    "Setting up remote access to %s" % machine_name
-                )
-                self.logger.LogOutput(
-                    "Checking machine characteristics for %s" % machine_name
-                )
-            cm = CrosMachine(machine_name, self.chromeos_root, self.log_level)
-            if cm.machine_checksum:
-                self._all_machines.append(cm)
-
-    def RemoveMachine(self, machine_name):
-        with self._lock:
-            self._machines = [
-                m for m in self._machines if m.name != machine_name
-            ]
-            if self.locks_dir:
-                res = file_lock_machine.Machine(
-                    machine_name, self.locks_dir
-                ).Unlock(True)
-                if not res:
-                    self.logger.LogError(
-                        "Could not unlock machine: '%s'." % machine_name
-                    )
-
-    def ForceSameImageToAllMachines(self, label):
-        machines = self.GetMachines(label)
-        for m in machines:
-            self.ImageMachine(m, label)
-            m.SetUpChecksumInfo()
-
-    def AcquireMachine(self, label):
-        image_checksum = label.checksum
-        machines = self.GetMachines(label)
-        check_interval_time = 120
-        with self._lock:
-            # Lazily external lock machines
-            while self.acquire_timeout >= 0:
-                for m in machines:
-                    new_machine = m not in self._all_machines
-                    self._TryToLockMachine(m)
-                    if new_machine:
-                        m.released_time = time.time()
-                if self.GetAvailableMachines(label):
-                    break
-                sleep_time = max(
-                    1, min(self.acquire_timeout, check_interval_time)
-                )
-                time.sleep(sleep_time)
-                self.acquire_timeout -= sleep_time
-
-            if self.acquire_timeout < 0:
-                self.logger.LogFatal(
-                    "Could not acquire any of the "
-                    "following machines: '%s'"
-                    % ", ".join(machine.name for machine in machines)
-                )
-
-            ###      for m in self._machines:
-            ###        if (m.locked and time.time() - m.released_time < 10 and
-            ###            m.checksum == image_checksum):
-            ###          return None
-            unlocked_machines = [
-                machine
-                for machine in self.GetAvailableMachines(label)
-                if not machine.locked
-            ]
-            for m in unlocked_machines:
-                if image_checksum and m.checksum == image_checksum:
-                    m.locked = True
-                    m.test_run = threading.current_thread()
-                    return m
-            for m in unlocked_machines:
-                if not m.checksum:
-                    m.locked = True
-                    m.test_run = threading.current_thread()
-                    return m
-            # This logic ensures that threads waiting on a machine will get a machine
-            # with a checksum equal to their image over other threads. This saves time
-            # when crosperf initially assigns the machines to threads by minimizing
-            # the number of re-images.
-            # TODO(asharif): If we centralize the thread-scheduler, we wont need this
-            # code and can implement minimal reimaging code more cleanly.
-            for m in unlocked_machines:
-                if time.time() - m.released_time > 15:
-                    # The release time gap is too large, so it is probably in the start
-                    # stage, we need to reset the released_time.
-                    m.released_time = time.time()
-                elif time.time() - m.released_time > 8:
-                    m.locked = True
-                    m.test_run = threading.current_thread()
-                    return m
-        return None
-
-    def GetAvailableMachines(self, label=None):
-        if not label:
-            return self._machines
-        return [m for m in self._machines if m.name in label.remote]
-
-    def GetMachines(self, label=None):
-        if not label:
-            return self._all_machines
-        return [m for m in self._all_machines if m.name in label.remote]
-
-    def ReleaseMachine(self, machine):
-        with self._lock:
-            for m in self._machines:
-                if machine.name == m.name:
-                    assert m.locked, "Tried to double-release %s" % m.name
-                    m.released_time = time.time()
-                    m.locked = False
-                    m.status = "Available"
-                    break
-
-    def Cleanup(self):
-        with self._lock:
-            # Unlock all machines (via file lock)
-            for m in self._machines:
-                res = file_lock_machine.Machine(m.name, self.locks_dir).Unlock(
-                    True
-                )
-
-                if not res:
-                    self.logger.LogError(
-                        "Could not unlock machine: '%s'." % m.name
-                    )
-
-    def __str__(self):
-        with self._lock:
-            l = ["MachineManager Status:"] + [str(m) for m in self._machines]
-            return "\n".join(l)
-
-    def AsString(self):
-        with self._lock:
-            stringify_fmt = "%-30s %-10s %-4s %-25s %-32s"
-            header = stringify_fmt % (
-                "Machine",
-                "Thread",
-                "Lock",
-                "Status",
-                "Checksum",
-            )
-            table = [header]
-            for m in self._machines:
-                if m.test_run:
-                    test_name = m.test_run.name
-                    test_status = m.test_run.timeline.GetLastEvent()
-                else:
-                    test_name = ""
-                    test_status = ""
-
-                try:
-                    machine_string = stringify_fmt % (
-                        m.name,
-                        test_name,
-                        m.locked,
-                        test_status,
-                        m.checksum,
-                    )
-                except ValueError:
-                    machine_string = ""
-                table.append(machine_string)
-            return "Machine Status:\n%s" % "\n".join(table)
-
-    def GetAllCPUInfo(self, labels):
-        """Get cpuinfo for labels, merge them if their cpuinfo are the same."""
-        dic = collections.defaultdict(list)
-        for label in labels:
-            for machine in self._all_machines:
-                if machine.name in label.remote:
-                    dic[machine.cpuinfo].append(label.name)
-                    break
-        output_segs = []
-        for key, v in dic.items():
-            output = " ".join(v)
-            output += "\n-------------------\n"
-            output += key
-            output += "\n\n\n"
-            output_segs.append(output)
-        return "".join(output_segs)
-
-    def GetAllMachines(self):
-        return self._all_machines
-
-
-class MockCrosMachine(CrosMachine):
-    """Mock cros machine class."""
-
-    # pylint: disable=super-init-not-called
-
-    MEMINFO_STRING = """MemTotal:        3990332 kB
-MemFree:         2608396 kB
-Buffers:          147168 kB
-Cached:           811560 kB
-SwapCached:            0 kB
-Active:           503480 kB
-Inactive:         628572 kB
-Active(anon):     174532 kB
-Inactive(anon):    88576 kB
-Active(file):     328948 kB
-Inactive(file):   539996 kB
-Unevictable:           0 kB
-Mlocked:               0 kB
-SwapTotal:       5845212 kB
-SwapFree:        5845212 kB
-Dirty:              9384 kB
-Writeback:             0 kB
-AnonPages:        173408 kB
-Mapped:           146268 kB
-Shmem:             89676 kB
-Slab:             188260 kB
-SReclaimable:     169208 kB
-SUnreclaim:        19052 kB
-KernelStack:        2032 kB
-PageTables:         7120 kB
-NFS_Unstable:          0 kB
-Bounce:                0 kB
-WritebackTmp:          0 kB
-CommitLimit:     7840376 kB
-Committed_AS:    1082032 kB
-VmallocTotal:   34359738367 kB
-VmallocUsed:      364980 kB
-VmallocChunk:   34359369407 kB
-DirectMap4k:       45824 kB
-DirectMap2M:     4096000 kB
-"""
-
-    CPUINFO_STRING = """processor: 0
-vendor_id: GenuineIntel
-cpu family: 6
-model: 42
-model name: Intel(R) Celeron(R) CPU 867 @ 1.30GHz
-stepping: 7
-microcode: 0x25
-cpu MHz: 1300.000
-cache size: 2048 KB
-physical id: 0
-siblings: 2
-core id: 0
-cpu cores: 2
-apicid: 0
-initial apicid: 0
-fpu: yes
-fpu_exception: yes
-cpuid level: 13
-wp: yes
-flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer xsave lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid
-bogomips: 2594.17
-clflush size: 64
-cache_alignment: 64
-address sizes: 36 bits physical, 48 bits virtual
-power management:
-
-processor: 1
-vendor_id: GenuineIntel
-cpu family: 6
-model: 42
-model name: Intel(R) Celeron(R) CPU 867 @ 1.30GHz
-stepping: 7
-microcode: 0x25
-cpu MHz: 1300.000
-cache size: 2048 KB
-physical id: 0
-siblings: 2
-core id: 1
-cpu cores: 2
-apicid: 2
-initial apicid: 2
-fpu: yes
-fpu_exception: yes
-cpuid level: 13
-wp: yes
-flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer xsave lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid
-bogomips: 2594.17
-clflush size: 64
-cache_alignment: 64
-address sizes: 36 bits physical, 48 bits virtual
-power management:
-"""
-
-    def __init__(self, name, chromeos_root, log_level):
-        self.name = name
-        self.image = None
-        self.checksum = None
-        self.locked = False
-        self.released_time = time.time()
-        self.test_run = None
-        self.chromeos_root = chromeos_root
-        self.checksum_string = re.sub(r"\d", "", name)
-        # In test, we assume "lumpy1", "lumpy2" are the same machine.
-        self.machine_checksum = self._GetMD5Checksum(self.checksum_string)
-        self.log_level = log_level
-        self.label = None
-        self.cooldown_wait_time = 0
-        self.ce = command_executer.GetCommandExecuter(log_level=self.log_level)
-        self._GetCPUInfo()
-
-    def IsReachable(self):
-        return True
-
-    def _GetMemoryInfo(self):
-        self.meminfo = self.MEMINFO_STRING
-        self._ParseMemoryInfo()
-
-    def _GetCPUInfo(self):
-        self.cpuinfo = self.CPUINFO_STRING
-
-
-class MockMachineManager(MachineManager):
-    """Mock machine manager class."""
-
-    def __init__(
-        self,
-        chromeos_root,
-        acquire_timeout,
-        log_level,
-        locks_dir,
-        keep_stateful: bool = False,
-    ):
-        super(MockMachineManager, self).__init__(
-            chromeos_root,
-            acquire_timeout,
-            log_level,
-            locks_dir,
-            keep_stateful=keep_stateful,
-        )
-
-    def _TryToLockMachine(self, cros_machine):
-        self._machines.append(cros_machine)
-        cros_machine.checksum = ""
-
-    def AddMachine(self, machine_name):
-        with self._lock:
-            for m in self._all_machines:
-                assert m.name != machine_name, (
-                    "Tried to double-add %s" % machine_name
-                )
-            cm = MockCrosMachine(
-                machine_name, self.chromeos_root, self.log_level
-            )
-            assert cm.machine_checksum, (
-                "Could not find checksum for machine %s" % machine_name
-            )
-            # In Original MachineManager, the test is 'if cm.machine_checksum:' - if a
-            # machine is unreachable, then its machine_checksum is None. Here we
-            # cannot do this, because machine_checksum is always faked, so we directly
-            # test cm.IsReachable, which is properly mocked.
-            if cm.IsReachable():
-                self._all_machines.append(cm)
-
-    def GetChromeVersion(self, machine):
-        return "Mock Chrome Version R50"
-
-    def AcquireMachine(self, label):
-        for machine in self._all_machines:
-            if not machine.locked:
-                machine.locked = True
-                return machine
-        return None
-
-    def ImageMachine(self, machine, label):
-        if machine or label:
-            return 0
-        return 1
-
-    def ReleaseMachine(self, machine):
-        machine.locked = False
-
-    def GetMachines(self, label=None):
-        return self._all_machines
-
-    def GetAvailableMachines(self, label=None):
-        return self._all_machines
-
-    def ForceSameImageToAllMachines(self, label=None):
-        return 0
-
-    def ComputeCommonCheckSum(self, label=None):
-        common_checksum = 12345
-        for machine in self.GetMachines(label):
-            machine.machine_checksum = common_checksum
-        self.machine_checksum[label.name] = common_checksum
-
-    def GetAllMachines(self):
-        return self._all_machines
diff --git a/crosperf/machine_manager_unittest.py b/crosperf/machine_manager_unittest.py
deleted file mode 100755
index 6324a227..00000000
--- a/crosperf/machine_manager_unittest.py
+++ /dev/null
@@ -1,961 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-# Copyright 2012 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unittest for machine_manager."""
-
-
-import hashlib
-import os.path
-import time
-import unittest
-import unittest.mock as mock
-
-from benchmark import Benchmark
-from benchmark_run import MockBenchmarkRun
-from cros_utils import command_executer
-from cros_utils import logger
-import image_checksummer
-import label
-import machine_manager
-import test_flag
-
-
-# pylint: disable=protected-access
-
-
-class MyMachineManager(machine_manager.MachineManager):
-    """Machine manager for test."""
-
-    def __init__(self, chromeos_root):
-        super(MyMachineManager, self).__init__(chromeos_root, 0, "average", "")
-
-    def _TryToLockMachine(self, cros_machine):
-        self._machines.append(cros_machine)
-        cros_machine.checksum = ""
-
-    def AddMachine(self, machine_name):
-        with self._lock:
-            for m in self._all_machines:
-                assert m.name != machine_name, (
-                    "Tried to double-add %s" % machine_name
-                )
-            cm = machine_manager.MockCrosMachine(
-                machine_name, self.chromeos_root, "average"
-            )
-            assert cm.machine_checksum, (
-                "Could not find checksum for machine %s" % machine_name
-            )
-            self._all_machines.append(cm)
-
-
-CHROMEOS_ROOT = "/tmp/chromeos-root"
-MACHINE_NAMES = ["lumpy1", "lumpy2", "lumpy3", "daisy1", "daisy2"]
-LABEL_LUMPY = label.MockLabel(
-    "lumpy",
-    "build",
-    "lumpy_chromeos_image",
-    "autotest_dir",
-    "debug_dir",
-    CHROMEOS_ROOT,
-    "lumpy",
-    ["lumpy1", "lumpy2", "lumpy3", "lumpy4"],
-    "",
-    "",
-    False,
-    "average",
-    "gcc",
-    False,
-    None,
-)
-LABEL_MIX = label.MockLabel(
-    "mix",
-    "build",
-    "chromeos_image",
-    "autotest_dir",
-    "debug_dir",
-    CHROMEOS_ROOT,
-    "mix",
-    ["daisy1", "daisy2", "lumpy3", "lumpy4"],
-    "",
-    "",
-    False,
-    "average",
-    "gcc",
-    False,
-    None,
-)
-
-
-class MachineManagerTest(unittest.TestCase):
-    """Test for machine manager class."""
-
-    msgs = []
-    image_log = []
-    log_fatal_msgs = []
-    fake_logger_count = 0
-    fake_logger_msgs = []
-
-    mock_cmd_exec = mock.Mock(spec=command_executer.CommandExecuter)
-
-    mock_logger = mock.Mock(spec=logger.Logger)
-
-    mock_lumpy1 = mock.Mock(spec=machine_manager.CrosMachine)
-    mock_lumpy2 = mock.Mock(spec=machine_manager.CrosMachine)
-    mock_lumpy3 = mock.Mock(spec=machine_manager.CrosMachine)
-    mock_lumpy4 = mock.Mock(spec=machine_manager.CrosMachine)
-    mock_daisy1 = mock.Mock(spec=machine_manager.CrosMachine)
-    mock_daisy2 = mock.Mock(spec=machine_manager.CrosMachine)
-
-    @mock.patch.object(os.path, "isdir")
-
-    # pylint: disable=arguments-differ
-    def setUp(self, mock_isdir):
-
-        mock_isdir.return_value = True
-        self.mm = machine_manager.MachineManager(
-            "/usr/local/chromeos",
-            0,
-            "average",
-            None,
-            self.mock_cmd_exec,
-            self.mock_logger,
-        )
-
-        self.mock_lumpy1.name = "lumpy1"
-        self.mock_lumpy2.name = "lumpy2"
-        self.mock_lumpy3.name = "lumpy3"
-        self.mock_lumpy4.name = "lumpy4"
-        self.mock_daisy1.name = "daisy1"
-        self.mock_daisy2.name = "daisy2"
-        self.mock_lumpy1.machine_checksum = "lumpy123"
-        self.mock_lumpy2.machine_checksum = "lumpy123"
-        self.mock_lumpy3.machine_checksum = "lumpy123"
-        self.mock_lumpy4.machine_checksum = "lumpy123"
-        self.mock_daisy1.machine_checksum = "daisy12"
-        self.mock_daisy2.machine_checksum = "daisy12"
-        self.mock_lumpy1.checksum_string = "lumpy_checksum_str"
-        self.mock_lumpy2.checksum_string = "lumpy_checksum_str"
-        self.mock_lumpy3.checksum_string = "lumpy_checksum_str"
-        self.mock_lumpy4.checksum_string = "lumpy_checksum_str"
-        self.mock_daisy1.checksum_string = "daisy_checksum_str"
-        self.mock_daisy2.checksum_string = "daisy_checksum_str"
-        self.mock_lumpy1.cpuinfo = "lumpy_cpu_info"
-        self.mock_lumpy2.cpuinfo = "lumpy_cpu_info"
-        self.mock_lumpy3.cpuinfo = "lumpy_cpu_info"
-        self.mock_lumpy4.cpuinfo = "lumpy_cpu_info"
-        self.mock_daisy1.cpuinfo = "daisy_cpu_info"
-        self.mock_daisy2.cpuinfo = "daisy_cpu_info"
-        self.mm._all_machines.append(self.mock_daisy1)
-        self.mm._all_machines.append(self.mock_daisy2)
-        self.mm._all_machines.append(self.mock_lumpy1)
-        self.mm._all_machines.append(self.mock_lumpy2)
-        self.mm._all_machines.append(self.mock_lumpy3)
-
-    def testGetMachines(self):
-        manager = MyMachineManager(CHROMEOS_ROOT)
-        for m in MACHINE_NAMES:
-            manager.AddMachine(m)
-        names = [m.name for m in manager.GetMachines(LABEL_LUMPY)]
-        self.assertEqual(names, ["lumpy1", "lumpy2", "lumpy3"])
-
-    def testGetAvailableMachines(self):
-        manager = MyMachineManager(CHROMEOS_ROOT)
-        for m in MACHINE_NAMES:
-            manager.AddMachine(m)
-        for m in manager._all_machines:
-            if int(m.name[-1]) % 2:
-                manager._TryToLockMachine(m)
-        names = [m.name for m in manager.GetAvailableMachines(LABEL_LUMPY)]
-        self.assertEqual(names, ["lumpy1", "lumpy3"])
-
-    @mock.patch.object(time, "sleep")
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommand")
-    @mock.patch.object(command_executer.CommandExecuter, "CrosRunCommand")
-    @mock.patch.object(image_checksummer.ImageChecksummer, "Checksum")
-    def test_image_machine(
-        self, mock_checksummer, mock_run_croscmd, mock_run_cmd, mock_sleep
-    ):
-        def FakeMD5Checksum(_input_str):
-            return "machine_fake_md5_checksum"
-
-        self.fake_logger_count = 0
-        self.fake_logger_msgs = []
-
-        def FakeLogOutput(msg):
-            self.fake_logger_count += 1
-            self.fake_logger_msgs.append(msg)
-
-        def ResetValues():
-            self.fake_logger_count = 0
-            self.fake_logger_msgs = []
-            mock_run_cmd.reset_mock()
-            mock_run_croscmd.reset_mock()
-            mock_checksummer.reset_mock()
-            mock_sleep.reset_mock()
-            machine.checksum = "fake_md5_checksum"
-            self.mm.checksum = None
-            self.mm.num_reimages = 0
-
-        self.mock_cmd_exec.CrosRunCommand = mock_run_croscmd
-        self.mock_cmd_exec.RunCommand = mock_run_cmd
-
-        self.mm.logger.LogOutput = FakeLogOutput
-        machine = self.mock_lumpy1
-        machine._GetMD5Checksum = FakeMD5Checksum
-        machine.checksum = "fake_md5_checksum"
-        mock_checksummer.return_value = "fake_md5_checksum"
-        self.mock_cmd_exec.log_level = "verbose"
-
-        test_flag.SetTestMode(True)
-        # Test 1: label.image_type == "local"
-        LABEL_LUMPY.image_type = "local"
-        self.mm.ImageMachine(machine, LABEL_LUMPY)
-        self.assertEqual(mock_run_cmd.call_count, 0)
-        self.assertEqual(mock_run_croscmd.call_count, 0)
-
-        # Test 2: label.image_type == "trybot"
-        ResetValues()
-        LABEL_LUMPY.image_type = "trybot"
-        mock_run_cmd.return_value = 0
-        self.mm.ImageMachine(machine, LABEL_LUMPY)
-        self.assertEqual(mock_run_croscmd.call_count, 0)
-        self.assertEqual(mock_checksummer.call_count, 0)
-
-        # Test 3: label.image_type is neither local nor trybot; retval from
-        # RunCommand is 1, i.e. image_chromeos fails...
-        ResetValues()
-        LABEL_LUMPY.image_type = "other"
-        mock_run_cmd.return_value = 1
-        try:
-            self.mm.ImageMachine(machine, LABEL_LUMPY)
-        except RuntimeError:
-            self.assertEqual(mock_checksummer.call_count, 0)
-            self.assertEqual(mock_run_cmd.call_count, 2)
-            self.assertEqual(mock_run_croscmd.call_count, 1)
-            self.assertEqual(mock_sleep.call_count, 1)
-            image_call_args_str = mock_run_cmd.call_args[0][0]
-            image_call_args = image_call_args_str.split(" ")
-            self.assertEqual(image_call_args[0], "python")
-            self.assertEqual(
-                image_call_args[1].split("/")[-1], "image_chromeos.pyc"
-            )
-            image_call_args = image_call_args[2:]
-            self.assertEqual(
-                image_call_args,
-                [
-                    "--chromeos_root=/tmp/chromeos-root",
-                    "--image=lumpy_chromeos_image",
-                    "--image_args=",
-                    "--remote=lumpy1",
-                    "--logging_level=average",
-                    "--board=lumpy",
-                ],
-            )
-            self.assertEqual(mock_run_croscmd.call_args[0][0], "reboot && exit")
-
-        # Test 4: Everything works properly. Trybot image type.
-        ResetValues()
-        LABEL_LUMPY.image_type = "trybot"
-        mock_run_cmd.return_value = 0
-        self.mm.ImageMachine(machine, LABEL_LUMPY)
-        self.assertEqual(mock_checksummer.call_count, 0)
-        self.assertEqual(mock_run_croscmd.call_count, 0)
-        self.assertEqual(mock_sleep.call_count, 0)
-
-    def test_compute_common_checksum(self):
-        self.mm.machine_checksum = {}
-        self.mm.ComputeCommonCheckSum(LABEL_LUMPY)
-        self.assertEqual(self.mm.machine_checksum["lumpy"], "lumpy123")
-        self.assertEqual(len(self.mm.machine_checksum), 1)
-
-        self.mm.machine_checksum = {}
-        self.assertRaisesRegex(
-            machine_manager.BadChecksum,
-            r"daisy.*\n.*lumpy",
-            self.mm.ComputeCommonCheckSum,
-            LABEL_MIX,
-        )
-
-    def test_compute_common_checksum_string(self):
-        self.mm.machine_checksum_string = {}
-        self.mm.ComputeCommonCheckSumString(LABEL_LUMPY)
-        self.assertEqual(len(self.mm.machine_checksum_string), 1)
-        self.assertEqual(
-            self.mm.machine_checksum_string["lumpy"], "lumpy_checksum_str"
-        )
-
-        self.mm.machine_checksum_string = {}
-        self.mm.ComputeCommonCheckSumString(LABEL_MIX)
-        self.assertEqual(len(self.mm.machine_checksum_string), 1)
-        self.assertEqual(
-            self.mm.machine_checksum_string["mix"], "daisy_checksum_str"
-        )
-
-    @mock.patch.object(
-        command_executer.CommandExecuter, "CrosRunCommandWOutput"
-    )
-    def test_try_to_lock_machine(self, mock_cros_runcmd):
-        mock_cros_runcmd.return_value = [0, "false_lock_checksum", ""]
-        self.mock_cmd_exec.CrosRunCommandWOutput = mock_cros_runcmd
-        self.mm._machines = []
-        self.mm._TryToLockMachine(self.mock_lumpy1)
-        self.assertEqual(len(self.mm._machines), 1)
-        self.assertEqual(self.mm._machines[0], self.mock_lumpy1)
-        self.assertEqual(self.mock_lumpy1.checksum, "false_lock_checksum")
-        self.assertEqual(mock_cros_runcmd.call_count, 1)
-        cmd_str = mock_cros_runcmd.call_args[0][0]
-        self.assertEqual(cmd_str, "cat /usr/local/osimage_checksum_file")
-        args_dict = mock_cros_runcmd.call_args[1]
-        self.assertEqual(len(args_dict), 2)
-        self.assertEqual(args_dict["machine"], self.mock_lumpy1.name)
-        self.assertEqual(args_dict["chromeos_root"], "/usr/local/chromeos")
-
-    @mock.patch.object(machine_manager, "CrosMachine")
-    def test_add_machine(self, mock_machine):
-
-        mock_machine.machine_checksum = "daisy123"
-        self.assertEqual(len(self.mm._all_machines), 5)
-        self.mm.AddMachine("daisy3")
-        self.assertEqual(len(self.mm._all_machines), 6)
-
-        self.assertRaises(Exception, self.mm.AddMachine, "lumpy1")
-
-    def test_remove_machine(self):
-        self.mm._machines = self.mm._all_machines
-        self.assertTrue(self.mock_lumpy2 in self.mm._machines)
-        self.mm.RemoveMachine(self.mock_lumpy2.name)
-        self.assertFalse(self.mock_lumpy2 in self.mm._machines)
-
-    def test_force_same_image_to_all_machines(self):
-        self.image_log = []
-
-        def FakeImageMachine(machine, label_arg):
-            image = label_arg.chromeos_image
-            self.image_log.append("Pushed %s onto %s" % (image, machine.name))
-
-        def FakeSetUpChecksumInfo():
-            pass
-
-        self.mm.ImageMachine = FakeImageMachine
-        self.mock_lumpy1.SetUpChecksumInfo = FakeSetUpChecksumInfo
-        self.mock_lumpy2.SetUpChecksumInfo = FakeSetUpChecksumInfo
-        self.mock_lumpy3.SetUpChecksumInfo = FakeSetUpChecksumInfo
-
-        self.mm.ForceSameImageToAllMachines(LABEL_LUMPY)
-        self.assertEqual(len(self.image_log), 3)
-        self.assertEqual(
-            self.image_log[0], "Pushed lumpy_chromeos_image onto lumpy1"
-        )
-        self.assertEqual(
-            self.image_log[1], "Pushed lumpy_chromeos_image onto lumpy2"
-        )
-        self.assertEqual(
-            self.image_log[2], "Pushed lumpy_chromeos_image onto lumpy3"
-        )
-
-    @mock.patch.object(image_checksummer.ImageChecksummer, "Checksum")
-    @mock.patch.object(hashlib, "md5")
-    def test_acquire_machine(self, mock_md5, mock_checksum):
-
-        self.msgs = []
-        self.log_fatal_msgs = []
-
-        def FakeLock(machine):
-            self.msgs.append("Tried to lock %s" % machine.name)
-
-        def FakeLogFatal(msg):
-            self.log_fatal_msgs.append(msg)
-
-        self.mm._TryToLockMachine = FakeLock
-        self.mm.logger.LogFatal = FakeLogFatal
-
-        mock_md5.return_value = "123456"
-        mock_checksum.return_value = "fake_md5_checksum"
-
-        self.mm._machines = self.mm._all_machines
-        self.mock_lumpy1.locked = True
-        self.mock_lumpy2.locked = True
-        self.mock_lumpy3.locked = False
-        self.mock_lumpy3.checksum = "fake_md5_checksum"
-        self.mock_daisy1.locked = True
-        self.mock_daisy2.locked = False
-        self.mock_daisy2.checksum = "fake_md5_checksum"
-
-        self.mock_lumpy1.released_time = time.time()
-        self.mock_lumpy2.released_time = time.time()
-        self.mock_lumpy3.released_time = time.time()
-        self.mock_daisy1.released_time = time.time()
-        self.mock_daisy2.released_time = time.time()
-
-        # Test 1. Basic test. Acquire lumpy3.
-        self.mm.AcquireMachine(LABEL_LUMPY)
-        m = self.mock_lumpy1
-        self.assertEqual(m, self.mock_lumpy1)
-        self.assertTrue(self.mock_lumpy1.locked)
-        self.assertEqual(mock_md5.call_count, 0)
-        self.assertEqual(
-            self.msgs,
-            [
-                "Tried to lock lumpy1",
-                "Tried to lock lumpy2",
-                "Tried to lock lumpy3",
-            ],
-        )
-
-        # Test the second return statment (machine is unlocked, has no checksum)
-        save_locked = self.mock_lumpy1.locked
-        self.mock_lumpy1.locked = False
-        self.mock_lumpy1.checksum = None
-        m = self.mm.AcquireMachine(LABEL_LUMPY)
-        self.assertEqual(m, self.mock_lumpy1)
-        self.assertTrue(self.mock_lumpy1.locked)
-
-        # Test the third return statement:
-        #   - machine is unlocked
-        #   - checksums don't match
-        #   - current time minus release time is > 20.
-        self.mock_lumpy1.locked = False
-        self.mock_lumpy1.checksum = "123"
-        self.mock_lumpy1.released_time = time.time() - 8
-        m = self.mm.AcquireMachine(LABEL_LUMPY)
-        self.assertEqual(m, self.mock_lumpy1)
-        self.assertTrue(self.mock_lumpy1.locked)
-
-        # Test all machines are already locked.
-        m = self.mm.AcquireMachine(LABEL_LUMPY)
-        self.assertIsNone(m)
-
-        # Restore values of mock_lumpy1, so other tests succeed.
-        self.mock_lumpy1.locked = save_locked
-        self.mock_lumpy1.checksum = "123"
-
-    def test_get_available_machines(self):
-        self.mm._machines = self.mm._all_machines
-
-        machine_list = self.mm.GetAvailableMachines()
-        self.assertEqual(machine_list, self.mm._all_machines)
-
-        machine_list = self.mm.GetAvailableMachines(LABEL_MIX)
-        self.assertEqual(
-            machine_list, [self.mock_daisy1, self.mock_daisy2, self.mock_lumpy3]
-        )
-
-        machine_list = self.mm.GetAvailableMachines(LABEL_LUMPY)
-        self.assertEqual(
-            machine_list, [self.mock_lumpy1, self.mock_lumpy2, self.mock_lumpy3]
-        )
-
-    def test_get_machines(self):
-        machine_list = self.mm.GetMachines()
-        self.assertEqual(machine_list, self.mm._all_machines)
-
-        machine_list = self.mm.GetMachines(LABEL_MIX)
-        self.assertEqual(
-            machine_list, [self.mock_daisy1, self.mock_daisy2, self.mock_lumpy3]
-        )
-
-        machine_list = self.mm.GetMachines(LABEL_LUMPY)
-        self.assertEqual(
-            machine_list, [self.mock_lumpy1, self.mock_lumpy2, self.mock_lumpy3]
-        )
-
-    def test_release_machines(self):
-
-        self.mm._machines = [self.mock_lumpy1, self.mock_daisy2]
-
-        self.mock_lumpy1.locked = True
-        self.mock_daisy2.locked = True
-
-        self.assertTrue(self.mock_lumpy1.locked)
-        self.mm.ReleaseMachine(self.mock_lumpy1)
-        self.assertFalse(self.mock_lumpy1.locked)
-        self.assertEqual(self.mock_lumpy1.status, "Available")
-
-        self.assertTrue(self.mock_daisy2.locked)
-        self.mm.ReleaseMachine(self.mock_daisy2)
-        self.assertFalse(self.mock_daisy2.locked)
-        self.assertEqual(self.mock_daisy2.status, "Available")
-
-        # Test double-relase...
-        self.assertRaises(
-            AssertionError, self.mm.ReleaseMachine, self.mock_lumpy1
-        )
-
-    def test_cleanup(self):
-        self.mock_logger.reset_mock()
-        self.mm.Cleanup()
-        self.assertEqual(self.mock_logger.call_count, 0)
-
-    OUTPUT_STR = (
-        "Machine Status:\nMachine                        Thread     "
-        "Lock Status                    Checksum"
-        "                        \nlumpy1                         test "
-        "run   True PENDING                   123"
-        "                             \nlumpy2                         "
-        "test run   False PENDING                   123"
-        "                             \nlumpy3                         "
-        "test run   False PENDING                   123"
-        "                             \ndaisy1                         "
-        "test run   False PENDING                   678"
-        "                             \ndaisy2                         "
-        "test run   True PENDING                   678"
-        "                             "
-    )
-
-    def test_as_string(self):
-
-        mock_logger = mock.Mock(spec=logger.Logger)
-
-        bench = Benchmark(
-            "page_cycler_v2.netsim.top_10",  # name
-            "page_cycler_v2.netsim.top_10",  # test_name
-            "",  # test_args
-            1,  # iteratins
-            False,  # rm_chroot_tmp
-            "",  # perf_args
-            suite="telemetry_Crosperf",
-        )  # suite
-
-        test_run = MockBenchmarkRun(
-            "test run",
-            bench,
-            LABEL_LUMPY,
-            1,
-            [],
-            self.mm,
-            mock_logger,
-            "verbose",
-            "",
-            {},
-        )
-
-        self.mm._machines = [
-            self.mock_lumpy1,
-            self.mock_lumpy2,
-            self.mock_lumpy3,
-            self.mock_daisy1,
-            self.mock_daisy2,
-        ]
-
-        self.mock_lumpy1.test_run = test_run
-        self.mock_lumpy2.test_run = test_run
-        self.mock_lumpy3.test_run = test_run
-        self.mock_daisy1.test_run = test_run
-        self.mock_daisy2.test_run = test_run
-
-        self.mock_lumpy1.locked = True
-        self.mock_lumpy2.locked = False
-        self.mock_lumpy3.locked = False
-        self.mock_daisy1.locked = False
-        self.mock_daisy2.locked = True
-
-        self.mock_lumpy1.checksum = "123"
-        self.mock_lumpy2.checksum = "123"
-        self.mock_lumpy3.checksum = "123"
-        self.mock_daisy1.checksum = "678"
-        self.mock_daisy2.checksum = "678"
-
-        output = self.mm.AsString()
-        self.assertEqual(output, self.OUTPUT_STR)
-
-    def test_get_all_cpu_info(self):
-        info = self.mm.GetAllCPUInfo([LABEL_LUMPY, LABEL_MIX])
-        self.assertEqual(
-            info,
-            "lumpy\n-------------------\nlumpy_cpu_info\n\n\nmix\n-"
-            "------------------\ndaisy_cpu_info\n\n\n",
-        )
-
-
-MEMINFO_STRING = """MemTotal:        3990332 kB
-MemFree:         2608396 kB
-Buffers:          147168 kB
-Cached:           811560 kB
-SwapCached:            0 kB
-Active:           503480 kB
-Inactive:         628572 kB
-Active(anon):     174532 kB
-Inactive(anon):    88576 kB
-Active(file):     328948 kB
-Inactive(file):   539996 kB
-Unevictable:           0 kB
-Mlocked:               0 kB
-SwapTotal:       5845212 kB
-SwapFree:        5845212 kB
-Dirty:              9384 kB
-Writeback:             0 kB
-AnonPages:        173408 kB
-Mapped:           146268 kB
-Shmem:             89676 kB
-Slab:             188260 kB
-SReclaimable:     169208 kB
-SUnreclaim:        19052 kB
-KernelStack:        2032 kB
-PageTables:         7120 kB
-NFS_Unstable:          0 kB
-Bounce:                0 kB
-WritebackTmp:          0 kB
-CommitLimit:     7840376 kB
-Committed_AS:    1082032 kB
-VmallocTotal:   34359738367 kB
-VmallocUsed:      364980 kB
-VmallocChunk:   34359369407 kB
-DirectMap4k:       45824 kB
-DirectMap2M:     4096000 kB
-"""
-
-CPUINFO_STRING = """processor: 0
-vendor_id: GenuineIntel
-cpu family: 6
-model: 42
-model name: Intel(R) Celeron(R) CPU 867 @ 1.30GHz
-stepping: 7
-microcode: 0x25
-cpu MHz: 1300.000
-cache size: 2048 KB
-physical id: 0
-siblings: 2
-core id: 0
-cpu cores: 2
-apicid: 0
-initial apicid: 0
-fpu: yes
-fpu_exception: yes
-cpuid level: 13
-wp: yes
-flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer xsave lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid
-bogomips: 2594.17
-clflush size: 64
-cache_alignment: 64
-address sizes: 36 bits physical, 48 bits virtual
-power management:
-
-processor: 1
-vendor_id: GenuineIntel
-cpu family: 6
-model: 42
-model name: Intel(R) Celeron(R) CPU 867 @ 1.30GHz
-stepping: 7
-microcode: 0x25
-cpu MHz: 1300.000
-cache size: 2048 KB
-physical id: 0
-siblings: 2
-core id: 1
-cpu cores: 2
-apicid: 2
-initial apicid: 2
-fpu: yes
-fpu_exception: yes
-cpuid level: 13
-wp: yes
-flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer xsave lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid
-bogomips: 2594.17
-clflush size: 64
-cache_alignment: 64
-address sizes: 36 bits physical, 48 bits virtual
-power management:
-"""
-
-CHECKSUM_STRING = (
-    "processor: 0vendor_id: GenuineIntelcpu family: 6model: "
-    "42model name: Intel(R) Celeron(R) CPU 867 @ "
-    "1.30GHzstepping: 7microcode: 0x25cache size: 2048 "
-    "KBphysical id: 0siblings: 2cpu cores: 2"
-    "fpu: yesfpu_exception: yescpuid level: "
-    "13wp: yesflags: fpu vme de pse tsc msr pae mce cx8 apic sep"
-    " mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse "
-    "sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc "
-    "arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc "
-    "aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 "
-    "ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt "
-    "tsc_deadline_timer xsave lahf_lm arat epb xsaveopt pln pts "
-    "dts tpr_shadow vnmi flexpriority ept vpidclflush size: "
-    "64cache_alignment: 64address sizes: 36 bits physical, 48 "
-    "bits virtualpower management:processor: 1vendor_id: "
-    "GenuineIntelcpu family: 6model: 42model name: Intel(R) "
-    "Celeron(R) CPU 867 @ 1.30GHzstepping: 7microcode: 0x25cache"
-    " size: 2048 KBphysical id: 0siblings: 2cpu cores:"
-    " 2fpu: yesfpu_exception: yescpuid"
-    " level: 13wp: yesflags: fpu vme de pse tsc msr pae mce cx8 "
-    "apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx "
-    "fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm "
-    "constant_tsc arch_perfmon pebs bts rep_good nopl xtopology "
-    "nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl "
-    "vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic "
-    "popcnt tsc_deadline_timer xsave lahf_lm arat epb xsaveopt "
-    "pln pts dts tpr_shadow vnmi flexpriority ept vpidclflush "
-    "size: 64cache_alignment: 64address sizes: 36 bits physical,"
-    " 48 bits virtualpower management: 4194304"
-)
-
-DUMP_VPD_STRING = """
-"PBA_SN"="Pba.txt"
-"Product_S/N"="HT4L91SC300208"
-"serial_number"="HT4L91SC300208Z"
-"System_UUID"="12153006-1755-4f66-b410-c43758a71127"
-"shipping_country"="US"
-"initial_locale"="en-US"
-"keyboard_layout"="xkb:us::eng"
-"initial_timezone"="America/Los_Angeles"
-"MACAddress"=""
-"System_UUID"="29dd9c61-7fa1-4c83-b89a-502e7eb08afe"
-"ubind_attribute"="0c433ce7585f486730b682bb05626a12ce2d896e9b57665387f8ce2ccfdcc56d2e2f1483"
-"gbind_attribute"="7e9a851324088e269319347c6abb8d1572ec31022fa07e28998229afe8acb45c35a89b9d"
-"ActivateDate"="2013-38"
-"""
-
-IFCONFIG_STRING = """
-eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
-        inet 172.17.129.247  netmask 255.255.254.0  broadcast 172.17.129.255
-        inet6 2620:0:1000:3002:143:fed4:3ff6:279d  prefixlen 64  scopeid 0x0<global>
-        inet6 2620:0:1000:3002:4459:1399:1f02:9e4c  prefixlen 64  scopeid 0x0<global>
-        inet6 2620:0:1000:3002:d9e4:87b:d4ec:9a0e  prefixlen 64  scopeid 0x0<global>
-        inet6 2620:0:1000:3002:7d45:23f1:ea8a:9604  prefixlen 64  scopeid 0x0<global>
-        inet6 2620:0:1000:3002:250:b6ff:fe63:db65  prefixlen 64  scopeid 0x0<global>
-        inet6 fe80::250:b6ff:fe63:db65  prefixlen 64  scopeid 0x20<link>
-        ether 00:50:b6:63:db:65  txqueuelen 1000  (Ethernet)
-        RX packets 9817166  bytes 10865181708 (10.1 GiB)
-        RX errors 194  dropped 0  overruns 0  frame 194
-        TX packets 0  bytes 2265811903 (2.1 GiB)
-        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
-
-eth1: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
-        ether e8:03:9a:9c:50:3d  txqueuelen 1000  (Ethernet)
-        RX packets 0  bytes 0 (0.0 B)
-        RX errors 0  dropped 0  overruns 0  frame 0
-        TX packets 0  bytes 0 (0.0 B)
-        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
-
-lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 16436
-        inet 127.0.0.1  netmask 255.0.0.0
-        inet6 ::1  prefixlen 128  scopeid 0x10<host>
-        loop  txqueuelen 0  (Local Loopback)
-        RX packets 981004  bytes 1127468524 (1.0 GiB)
-        RX errors 0  dropped 0  overruns 0  frame 0
-        TX packets 981004  bytes 1127468524 (1.0 GiB)
-        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
-
-wlan0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
-        ether 44:6d:57:20:4a:c5  txqueuelen 1000  (Ethernet)
-        RX packets 0  bytes 0 (0.0 B)
-        RX errors 0  dropped 0  overruns 0  frame 0
-        TX packets 0  bytes 0 (0.0 B)
-        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
-"""
-
-
-class CrosMachineTest(unittest.TestCase):
-    """Test for CrosMachine class."""
-
-    mock_cmd_exec = mock.Mock(spec=command_executer.CommandExecuter)
-
-    @mock.patch.object(machine_manager.CrosMachine, "SetUpChecksumInfo")
-    def test_init(self, mock_setup):
-
-        cm = machine_manager.CrosMachine(
-            "daisy.cros", "/usr/local/chromeos", "average", self.mock_cmd_exec
-        )
-        self.assertEqual(mock_setup.call_count, 1)
-        self.assertEqual(cm.chromeos_root, "/usr/local/chromeos")
-        self.assertEqual(cm.log_level, "average")
-
-    @mock.patch.object(machine_manager.CrosMachine, "IsReachable")
-    @mock.patch.object(machine_manager.CrosMachine, "_GetMemoryInfo")
-    @mock.patch.object(machine_manager.CrosMachine, "_GetCPUInfo")
-    @mock.patch.object(
-        machine_manager.CrosMachine, "_ComputeMachineChecksumString"
-    )
-    @mock.patch.object(machine_manager.CrosMachine, "_GetMachineID")
-    @mock.patch.object(machine_manager.CrosMachine, "_GetMD5Checksum")
-    def test_setup_checksum_info(
-        self,
-        mock_md5sum,
-        mock_machineid,
-        mock_checkstring,
-        mock_cpuinfo,
-        mock_meminfo,
-        mock_isreachable,
-    ):
-
-        # Test 1. Machine is not reachable; SetUpChecksumInfo is called via
-        # __init__.
-        mock_isreachable.return_value = False
-        mock_md5sum.return_value = "md5_checksum"
-        cm = machine_manager.CrosMachine(
-            "daisy.cros", "/usr/local/chromeos", "average", self.mock_cmd_exec
-        )
-        cm.checksum_string = "This is a checksum string."
-        cm.machine_id = "machine_id1"
-        self.assertEqual(mock_isreachable.call_count, 1)
-        self.assertIsNone(cm.machine_checksum)
-        self.assertEqual(mock_meminfo.call_count, 0)
-
-        # Test 2. Machine is reachable. Call explicitly.
-        mock_isreachable.return_value = True
-        cm.checksum_string = "This is a checksum string."
-        cm.machine_id = "machine_id1"
-        cm.SetUpChecksumInfo()
-        self.assertEqual(mock_isreachable.call_count, 2)
-        self.assertEqual(mock_meminfo.call_count, 1)
-        self.assertEqual(mock_cpuinfo.call_count, 1)
-        self.assertEqual(mock_checkstring.call_count, 1)
-        self.assertEqual(mock_machineid.call_count, 1)
-        self.assertEqual(mock_md5sum.call_count, 2)
-        self.assertEqual(cm.machine_checksum, "md5_checksum")
-        self.assertEqual(cm.machine_id_checksum, "md5_checksum")
-        self.assertEqual(
-            mock_md5sum.call_args_list[0][0][0], "This is a checksum string."
-        )
-        self.assertEqual(mock_md5sum.call_args_list[1][0][0], "machine_id1")
-
-    @mock.patch.object(command_executer.CommandExecuter, "CrosRunCommand")
-    @mock.patch.object(machine_manager.CrosMachine, "SetUpChecksumInfo")
-    def test_is_reachable(self, mock_setup, mock_run_cmd):
-
-        cm = machine_manager.CrosMachine(
-            "daisy.cros", "/usr/local/chromeos", "average", self.mock_cmd_exec
-        )
-        self.mock_cmd_exec.CrosRunCommand = mock_run_cmd
-
-        # Test 1. CrosRunCommand returns 1 (fail)
-        mock_run_cmd.return_value = 1
-        result = cm.IsReachable()
-        self.assertFalse(result)
-        self.assertEqual(mock_setup.call_count, 1)
-        self.assertEqual(mock_run_cmd.call_count, 1)
-
-        # Test 2. CrosRunCommand returns 0 (success)
-        mock_run_cmd.return_value = 0
-        result = cm.IsReachable()
-        self.assertTrue(result)
-        self.assertEqual(mock_run_cmd.call_count, 2)
-        first_args = mock_run_cmd.call_args_list[0]
-        second_args = mock_run_cmd.call_args_list[1]
-        self.assertEqual(first_args[0], second_args[0])
-        self.assertEqual(first_args[1], second_args[1])
-        self.assertEqual(len(first_args[0]), 1)
-        self.assertEqual(len(first_args[1]), 2)
-        self.assertEqual(first_args[0][0], "ls")
-        args_dict = first_args[1]
-        self.assertEqual(args_dict["machine"], "daisy.cros")
-        self.assertEqual(args_dict["chromeos_root"], "/usr/local/chromeos")
-
-    @mock.patch.object(machine_manager.CrosMachine, "SetUpChecksumInfo")
-    def test_parse_memory_info(self, _mock_setup):
-        cm = machine_manager.CrosMachine(
-            "daisy.cros", "/usr/local/chromeos", "average", self.mock_cmd_exec
-        )
-        cm.meminfo = MEMINFO_STRING
-        cm._ParseMemoryInfo()
-        self.assertEqual(cm.phys_kbytes, 4194304)
-
-    @mock.patch.object(
-        command_executer.CommandExecuter, "CrosRunCommandWOutput"
-    )
-    @mock.patch.object(machine_manager.CrosMachine, "SetUpChecksumInfo")
-    def test_get_memory_info(self, _mock_setup, mock_run_cmd):
-        cm = machine_manager.CrosMachine(
-            "daisy.cros", "/usr/local/chromeos", "average", self.mock_cmd_exec
-        )
-        self.mock_cmd_exec.CrosRunCommandWOutput = mock_run_cmd
-        mock_run_cmd.return_value = [0, MEMINFO_STRING, ""]
-        cm._GetMemoryInfo()
-        self.assertEqual(mock_run_cmd.call_count, 1)
-        call_args = mock_run_cmd.call_args_list[0]
-        self.assertEqual(call_args[0][0], "cat /proc/meminfo")
-        args_dict = call_args[1]
-        self.assertEqual(args_dict["machine"], "daisy.cros")
-        self.assertEqual(args_dict["chromeos_root"], "/usr/local/chromeos")
-        self.assertEqual(cm.meminfo, MEMINFO_STRING)
-        self.assertEqual(cm.phys_kbytes, 4194304)
-
-        mock_run_cmd.return_value = [1, MEMINFO_STRING, ""]
-        self.assertRaises(Exception, cm._GetMemoryInfo)
-
-    @mock.patch.object(
-        command_executer.CommandExecuter, "CrosRunCommandWOutput"
-    )
-    @mock.patch.object(machine_manager.CrosMachine, "SetUpChecksumInfo")
-    def test_get_cpu_info(self, _mock_setup, mock_run_cmd):
-        cm = machine_manager.CrosMachine(
-            "daisy.cros", "/usr/local/chromeos", "average", self.mock_cmd_exec
-        )
-        self.mock_cmd_exec.CrosRunCommandWOutput = mock_run_cmd
-        mock_run_cmd.return_value = [0, CPUINFO_STRING, ""]
-        cm._GetCPUInfo()
-        self.assertEqual(mock_run_cmd.call_count, 1)
-        call_args = mock_run_cmd.call_args_list[0]
-        self.assertEqual(call_args[0][0], "cat /proc/cpuinfo")
-        args_dict = call_args[1]
-        self.assertEqual(args_dict["machine"], "daisy.cros")
-        self.assertEqual(args_dict["chromeos_root"], "/usr/local/chromeos")
-        self.assertEqual(cm.cpuinfo, CPUINFO_STRING)
-
-    @mock.patch.object(machine_manager.CrosMachine, "SetUpChecksumInfo")
-    def test_compute_machine_checksum_string(self, _mock_setup):
-        cm = machine_manager.CrosMachine(
-            "daisy.cros", "/usr/local/chromeos", "average", self.mock_cmd_exec
-        )
-        cm.cpuinfo = CPUINFO_STRING
-        cm.meminfo = MEMINFO_STRING
-        cm._ParseMemoryInfo()
-        cm._ComputeMachineChecksumString()
-        self.assertEqual(cm.checksum_string, CHECKSUM_STRING)
-
-    @mock.patch.object(machine_manager.CrosMachine, "SetUpChecksumInfo")
-    def test_get_md5_checksum(self, _mock_setup):
-        cm = machine_manager.CrosMachine(
-            "daisy.cros", "/usr/local/chromeos", "average", self.mock_cmd_exec
-        )
-        temp_str = "abcde"
-        checksum_str = cm._GetMD5Checksum(temp_str)
-        self.assertEqual(checksum_str, "ab56b4d92b40713acc5af89985d4b786")
-
-        temp_str = ""
-        checksum_str = cm._GetMD5Checksum(temp_str)
-        self.assertEqual(checksum_str, "")
-
-    @mock.patch.object(
-        command_executer.CommandExecuter, "CrosRunCommandWOutput"
-    )
-    @mock.patch.object(machine_manager.CrosMachine, "SetUpChecksumInfo")
-    def test_get_machine_id(self, _mock_setup, mock_run_cmd):
-        cm = machine_manager.CrosMachine(
-            "daisy.cros", "/usr/local/chromeos", "average", self.mock_cmd_exec
-        )
-        self.mock_cmd_exec.CrosRunCommandWOutput = mock_run_cmd
-        mock_run_cmd.return_value = [0, DUMP_VPD_STRING, ""]
-
-        cm._GetMachineID()
-        self.assertEqual(cm.machine_id, '"Product_S/N"="HT4L91SC300208"')
-
-        mock_run_cmd.return_value = [0, IFCONFIG_STRING, ""]
-        cm._GetMachineID()
-        self.assertEqual(
-            cm.machine_id,
-            "        ether 00:50:b6:63:db:65  txqueuelen 1000  (Ethernet)_        "
-            "ether e8:03:9a:9c:50:3d  txqueuelen 1000  (Ethernet)_        ether "
-            "44:6d:57:20:4a:c5  txqueuelen 1000  (Ethernet)",
-        )
-
-        mock_run_cmd.return_value = [0, "invalid hardware config", ""]
-        self.assertRaises(Exception, cm._GetMachineID)
-
-    def test_add_cooldown_waittime(self):
-        cm = machine_manager.CrosMachine(
-            "1.2.3.4.cros", "/usr/local/chromeos", "average"
-        )
-        self.assertEqual(cm.GetCooldownWaitTime(), 0)
-        cm.AddCooldownWaitTime(250)
-        self.assertEqual(cm.GetCooldownWaitTime(), 250)
-        cm.AddCooldownWaitTime(1)
-        self.assertEqual(cm.GetCooldownWaitTime(), 251)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/mock_instance.py b/crosperf/mock_instance.py
deleted file mode 100644
index 4a3f9a72..00000000
--- a/crosperf/mock_instance.py
+++ /dev/null
@@ -1,171 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""This contains some mock instances for testing."""
-
-
-from benchmark import Benchmark
-from label import MockLabel
-
-
-perf_args = "record -a -e cycles"
-label1 = MockLabel(
-    "test1",
-    "build1",
-    "image1",
-    "autotest_dir",
-    "debug_dir",
-    "/tmp/test_benchmark_run",
-    "x86-alex",
-    "chromeos-alex1",
-    image_args="",
-    cache_dir="",
-    cache_only=False,
-    log_level="average",
-    compiler="gcc",
-    crosfleet=False,
-    chrome_src=None,
-)
-
-label2 = MockLabel(
-    "test2",
-    "build2",
-    "image2",
-    "autotest_dir",
-    "debug_dir",
-    "/tmp/test_benchmark_run_2",
-    "x86-alex",
-    "chromeos-alex2",
-    image_args="",
-    cache_dir="",
-    cache_only=False,
-    log_level="average",
-    compiler="gcc",
-    crosfleet=False,
-    chrome_src=None,
-)
-
-benchmark1 = Benchmark(
-    "benchmark1",
-    "autotest_name_1",
-    "autotest_args",
-    2,
-    "",
-    perf_args,
-    "telemetry_Crosperf",
-    "",
-)
-
-benchmark2 = Benchmark(
-    "benchmark2",
-    "autotest_name_2",
-    "autotest_args",
-    2,
-    "",
-    perf_args,
-    "telemetry_Crosperf",
-    "",
-)
-
-keyval = {}
-keyval[0] = {
-    "": "PASS",
-    "milliseconds_1": "1",
-    "milliseconds_2": "8",
-    "milliseconds_3": "9.2",
-    "test{1}": "2",
-    "test{2}": "4",
-    "ms_1": "2.1",
-    "total": "5",
-    "bool": "True",
-}
-
-keyval[1] = {
-    "": "PASS",
-    "milliseconds_1": "3",
-    "milliseconds_2": "5",
-    "ms_1": "2.2",
-    "total": "6",
-    "test{1}": "3",
-    "test{2}": "4",
-    "bool": "FALSE",
-}
-
-keyval[2] = {
-    "": "PASS",
-    "milliseconds_4": "30",
-    "milliseconds_5": "50",
-    "ms_1": "2.23",
-    "total": "6",
-    "test{1}": "5",
-    "test{2}": "4",
-    "bool": "FALSE",
-}
-
-keyval[3] = {
-    "": "PASS",
-    "milliseconds_1": "3",
-    "milliseconds_6": "7",
-    "ms_1": "2.3",
-    "total": "7",
-    "test{1}": "2",
-    "test{2}": "6",
-    "bool": "FALSE",
-}
-
-keyval[4] = {
-    "": "PASS",
-    "milliseconds_1": "3",
-    "milliseconds_8": "6",
-    "ms_1": "2.3",
-    "total": "7",
-    "test{1}": "2",
-    "test{2}": "6",
-    "bool": "TRUE",
-}
-
-keyval[5] = {
-    "": "PASS",
-    "milliseconds_1": "3",
-    "milliseconds_8": "6",
-    "ms_1": "2.2",
-    "total": "7",
-    "test{1}": "2",
-    "test{2}": "2",
-    "bool": "TRUE",
-}
-
-keyval[6] = {
-    "": "PASS",
-    "milliseconds_1": "3",
-    "milliseconds_8": "6",
-    "ms_1": "2",
-    "total": "7",
-    "test{1}": "2",
-    "test{2}": "4",
-    "bool": "TRUE",
-}
-
-keyval[7] = {
-    "": "PASS",
-    "milliseconds_1": "3",
-    "milliseconds_8": "6",
-    "ms_1": "1",
-    "total": "7",
-    "test{1}": "1",
-    "test{2}": "6",
-    "bool": "TRUE",
-}
-
-keyval[8] = {
-    "": "PASS",
-    "milliseconds_1": "3",
-    "milliseconds_8": "6",
-    "ms_1": "3.3",
-    "total": "7",
-    "test{1}": "2",
-    "test{2}": "8",
-    "bool": "TRUE",
-}
diff --git a/crosperf/perf_files/perf.data.report.0 b/crosperf/perf_files/perf.data.report.0
deleted file mode 100644
index 910fdc44..00000000
--- a/crosperf/perf_files/perf.data.report.0
+++ /dev/null
@@ -1,734 +0,0 @@
-# To display the perf.data header info, please use --header/--header-only options.
-#
-# NOTE: this file has been manually cut into arbitrary tiny pieces. The original
-# was > 100,000 lines, and took Python a few seconds to run through. This one
-# takes almost no time, and should work just as well.
-#
-# Samples: 292K of event 'cycles'
-# Event count (approx.): 106521626675
-#
-# Overhead       Samples          Command                   Shared Object                  Symbol
-# ........  ............  ...............  ..............................  ......................
-#
-     0.66%          3539          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a1f1c9
-     0.61%          1703           chrome  [kernel.kallsyms]               [k] 0xffffffffa4eca110
-     0.50%          1402           chrome  [kernel.kallsyms]               [k] 0xffffffffa4beea47
-     0.48%          1297           chrome  perf-24199.map                  [.] 0x0000115bb6c35d7a
-     0.47%          1286           chrome  perf-24199.map                  [.] 0x0000115bb7ba9b54
-     0.42%          1149            lsusb  lsusb                           [.] 0x0000000000010e60
-     0.37%          1029           chrome  chrome                          [.] 0x0000000000e45a2b
-     0.37%           991           chrome  perf-24199.map                  [.] 0x0000115bb6c35d72
-     0.28%           762           chrome  perf-24199.map                  [.] 0x0000115bb6c35d76
-     0.27%           735           chrome  perf-24199.map                  [.] 0x0000115bb6aa463a
-     0.22%           608           chrome  perf-24199.map                  [.] 0x0000115bb7ba9ebf
-     0.17%           468           chrome  perf-24199.map                  [.] 0x0000115bb6a7afc3
-     0.17%           503           chrome  [kernel.kallsyms]               [k] 0xffffffffa4bf4c3d
-     0.17%           450           chrome  perf-24199.map                  [.] 0x0000115bb6af7457
-     0.16%           444           chrome  perf-24199.map                  [.] 0x0000115bb7c6edd1
-     0.16%           438           chrome  perf-24199.map                  [.] 0x0000115bb7c6f93d
-     0.15%           420           chrome  perf-24199.map                  [.] 0x0000115bb6af744b
-     0.15%           414           chrome  perf-24199.map                  [.] 0x0000115bb7c6fa42
-     0.15%           405           chrome  perf-24199.map                  [.] 0x0000115bb6af7430
-     0.15%           398           chrome  perf-24199.map                  [.] 0x0000115bb6af7421
-     0.15%           396           chrome  perf-24199.map                  [.] 0x0000115bb6af7438
-     0.15%           396           chrome  perf-24199.map                  [.] 0x0000115bb6af742b
-     0.14%           437           chrome  chrome                          [.] 0x0000000005d10b64
-     0.14%           385           chrome  perf-24199.map                  [.] 0x0000115bb7c6f9e5
-     0.14%           371           chrome  perf-24199.map                  [.] 0x0000115bb6af7418
-     0.14%           369           chrome  perf-24199.map                  [.] 0x0000115bb6af73f9
-     0.14%           369           chrome  perf-24199.map                  [.] 0x0000115bb5d21648
-     0.13%           363           chrome  perf-24199.map                  [.] 0x0000115bb6af7428
-     0.13%           358           chrome  perf-24199.map                  [.] 0x0000115bb6b80e03
-     0.13%           343           chrome  perf-24199.map                  [.] 0x0000115bb6af73fc
-     0.13%           344           chrome  chrome                          [.] 0x0000000000e55b20
-     0.12%           338           chrome  chrome                          [.] 0x00000000011d1cb0
-     0.12%           317           chrome  perf-24199.map                  [.] 0x0000115bb6aa469c
-     0.11%           311           chrome  perf-24199.map                  [.] 0x0000115bb6af73f6
-     0.11%           315           chrome  chrome                          [.] 0x0000000000e48e65
-     0.11%           310           chrome  perf-24199.map                  [.] 0x0000115bb6af73dc
-     0.11%           309           chrome  perf-24199.map                  [.] 0x0000115bb6af73cc
-     0.11%           303           chrome  perf-24199.map                  [.] 0x0000115bb5d21662
-     0.11%           302           chrome  perf-24199.map                  [.] 0x0000115bb5d29f6a
-     0.11%           295           chrome  perf-24199.map                  [.] 0x0000115bb6af7382
-     0.11%           295           chrome  perf-24199.map                  [.] 0x0000115bb6c35d1d
-     0.11%           294           chrome  perf-24199.map                  [.] 0x0000115bb6c35d99
-     0.11%           293           chrome  perf-24199.map                  [.] 0x0000115bb6c35cec
-     0.11%           292           chrome  perf-24199.map                  [.] 0x0000115bb6af73bc
-     0.10%           285           chrome  chrome                          [.] 0x0000000000e46990
-     0.10%           283           chrome  perf-24199.map                  [.] 0x0000115bb6af7465
-     0.10%           282           chrome  perf-24199.map                  [.] 0x0000115bb6aa4699
-     0.10%           276           chrome  perf-24199.map                  [.] 0x0000115bb6c35d2e
-     0.10%           274           chrome  perf-24199.map                  [.] 0x0000115bb6c35d6e
-     0.10%           273           chrome  perf-24199.map                  [.] 0x0000115bb6af73f0
-     0.10%           268           chrome  perf-24199.map                  [.] 0x0000115bb7ba9ecb
-     0.10%           266           chrome  perf-24199.map                  [.] 0x0000115bb6af73a1
-     0.10%           262           chrome  perf-24199.map                  [.] 0x0000115bb6c35d57
-     0.09%           286           chrome  [kernel.kallsyms]               [k] 0xffffffffa4bef022
-     0.09%           256           chrome  chrome                          [.] 0x0000000000e6fa2b
-     0.09%           249           chrome  perf-24199.map                  [.] 0x0000115bb6c35d47
-     0.09%           248           chrome  perf-24199.map                  [.] 0x0000115bb6af73e6
-     0.09%           247           chrome  perf-24199.map                  [.] 0x0000115bb6c35d8d
-     0.09%           240           chrome  perf-24199.map                  [.] 0x0000115bb6a7b6e7
-     0.09%           240           chrome  perf-24199.map                  [.] 0x0000115bb6c35d81
-     0.09%           233           chrome  perf-24199.map                  [.] 0x0000115bb7ba9e8c
-     0.09%           233           chrome  perf-24199.map                  [.] 0x0000115bb6c35d02
-     0.08%           230           chrome  perf-24199.map                  [.] 0x0000115bb5d09f68
-     0.08%           228           chrome  chrome                          [.] 0x0000000000e45adc
-     0.08%           232          swapper  [kernel.kallsyms]               [k] 0xffffffffa4dccf94
-     0.08%           222           chrome  perf-24199.map                  [.] 0x0000115bb7bed938
-     0.08%           222           chrome  perf-24199.map                  [.] 0x0000115bb5d0a372
-     0.08%           338           python  [kernel.kallsyms]               [k] 0xffffffffa4eca110
-     0.08%           218           chrome  perf-24199.map                  [.] 0x0000115bb7ba9b5d
-     0.08%           215           chrome  perf-24199.map                  [.] 0x0000115bb7ba9ea8
-     0.08%           246           python  [kernel.kallsyms]               [k] 0xffffffffa4ad6f19
-     0.08%           216          swapper  [kernel.kallsyms]               [k] 0xffffffffa4dccfa1
-     0.08%           206            lsusb  lsusb                           [.] 0x0000000000010e63
-     0.08%           207           chrome  chrome                          [.] 0x0000000000e4596c
-     0.07%           204           chrome  perf-24199.map                  [.] 0x0000115bb5d29dd4
-     0.07%           202           chrome  perf-24199.map                  [.] 0x0000115bb6b25330
-     0.07%           199           chrome  perf-24199.map                  [.] 0x0000115bb6b25338
-     0.07%           198           chrome  perf-24199.map                  [.] 0x0000115bb5d1726d
-     0.07%           194           chrome  perf-24199.map                  [.] 0x0000115bb6a7b07c
-     0.07%           214           chrome  chrome                          [.] 0x0000000005d10e5e
-     0.07%           187           chrome  perf-24199.map                  [.] 0x0000115bb7ba9b69
-     0.07%           188           chrome  perf-24199.map                  [.] 0x0000115bb5d1728e
-     0.07%           187           chrome  perf-24199.map                  [.] 0x0000115bb6b80dfe
-     0.07%           179           chrome  perf-24199.map                  [.] 0x0000115bb7bed940
-     0.07%           179           chrome  perf-24199.map                  [.] 0x0000115bb5d0a36e
-     0.06%           176           chrome  chrome                          [.] 0x0000000000e75fe4
-     0.06%           181           chrome  chrome                          [.] 0x00000000023fd480
-     0.06%           172           chrome  perf-24199.map                  [.] 0x0000115bb6af73e9
-     0.06%           170           chrome  perf-24199.map                  [.] 0x0000115bb6a7b6fe
-     0.06%           177          swapper  [kernel.kallsyms]               [k] 0xffffffffa4dccf9b
-     0.06%           168           chrome  chrome                          [.] 0x0000000000e45aff
-     0.06%           166           chrome  perf-24199.map                  [.] 0x0000115bb6b25340
-     0.06%           175           chrome  [kernel.kallsyms]               [k] 0xffffffffa4ac31c3
-     0.06%           163           chrome  chrome                          [.] 0x0000000000e4fcb8
-     0.06%           160           chrome  perf-24199.map                  [.] 0x0000115bb6a7afbb
-     0.06%           160           chrome  chrome                          [.] 0x0000000000e54d5c
-     0.06%           156           chrome  perf-24199.map                  [.] 0x0000115bb6a7af9f
-     0.06%           157           chrome  perf-24199.map                  [.] 0x0000115bb5d29daf
-     0.06%           156           chrome  perf-24199.map                  [.] 0x0000115bb5d21656
-     0.06%           172           chrome  chrome                          [.] 0x0000000005d10b5b
-     0.06%           156           chrome  perf-24199.map                  [.] 0x0000115bb6aa4662
-     0.06%           155           chrome  perf-24199.map                  [.] 0x0000115bb7bed932
-     0.06%           155           chrome  perf-24199.map                  [.] 0x0000115bb6b82327
-     0.05%           149           chrome  perf-24199.map                  [.] 0x0000115bb7ba9ede
-     0.05%           146           chrome  perf-24199.map                  [.] 0x0000115bb6aa45f8
-     0.05%           145           chrome  perf-24199.map                  [.] 0x0000115bb6aa460e
-     0.05%           153           chrome  chrome                          [.] 0x0000000000cb7030
-     0.05%           142           chrome  perf-24199.map                  [.] 0x0000115bb7ba9b18
-     0.05%           143           chrome  chrome                          [.] 0x0000000000f13e9c
-     0.05%           143           chrome  perf-24199.map                  [.] 0x0000115bb6b2530a
-     0.05%           141           chrome  chrome                          [.] 0x0000000000e18c45
-     0.05%           138           chrome  perf-24199.map                  [.] 0x0000115bb6ca5090
-     0.05%           211           python  [kernel.kallsyms]               [k] 0xffffffffa4ae14fd
-     0.05%           137           chrome  perf-24199.map                  [.] 0x0000115bb6aa4692
-     0.05%           137           chrome  perf-24199.map                  [.] 0x0000115bb6aa4626
-     0.05%           136           chrome  perf-24199.map                  [.] 0x0000115bb7ba9ed2
-     0.05%           196           python  [kernel.kallsyms]               [k] 0xffffffffa4beeac5
-     0.05%           133           chrome  perf-24199.map                  [.] 0x0000115bb6ca5109
-     0.05%           132           chrome  perf-24199.map                  [.] 0x0000115bb7ba9b42
-     0.05%           132           chrome  perf-24199.map                  [.] 0x0000115bb6b8230f
-     0.05%           132           chrome  perf-24199.map                  [.] 0x0000115bb5d215e5
-     0.05%           131           chrome  perf-24199.map                  [.] 0x0000115bb7c6fa0a
-     0.05%           149           chrome  libpthread-2.19.so              [.] 0x000000000000b471
-     0.05%           130           chrome  perf-24199.map                  [.] 0x0000115bb6aa4678
-     0.05%           133           chrome  libc-2.19.so                    [.] 0x0000000000088b72
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6c4c692
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6a7b4bc
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb7bba146
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb7ba9e83
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb7ba9dde
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6c4c713
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6a7b197
-     0.01%            16           keygen  libfreebl3.so                   [.] 0x000000000005bc62
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6ae766b
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6c4c6ef
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6c4e9ef
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6c4c0ba
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6a78053
-     0.01%            16           chrome  chrome                          [.] 0x0000000000e73bb0
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6c36bee
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6c3979b
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6c4e93b
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6af73bf
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6b814a7
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6a7b6cd
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6af73c5
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6b8147d
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6b8216b
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6b80dc6
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6ba1724
-     0.01%            16           chrome  chrome                          [.] 0x000000000254788e
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb6b81ed9
-     0.01%            16           chrome  perf-24199.map                  [.] 0x0000115bb5d27a01
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a09503
-     0.00%             1        rcu_sched  [kernel.kallsyms]               [k] 0xffffffffa4a63d39
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a65090
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4dcd1c3
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a544b0
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a54f5b
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4ec8bec
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4c532e4
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4a00e4c
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4a63e67
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4ec855a
-     0.00%             1             mtpd  [kernel.kallsyms]               [k] 0xffffffffa4a0cb13
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4a00dee
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a5d3a2
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a66eba
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4bea29e
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a545c4
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4a62fcf
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4cc8948
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4ec9b33
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4ec8911
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4a64bf8
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a00e4c
-     0.00%             1        rcu_sched  [kernel.kallsyms]               [k] 0xffffffffa4a63d0c
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4bea29a
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a75623
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4a5d435
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4a546cf
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4bec12d
-     0.00%             1        rcu_sched  [kernel.kallsyms]               [k] 0xffffffffa4a66db1
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4ec855b
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a6394d
-     0.00%             1      dbus-daemon  [kernel.kallsyms]               [k] 0xffffffffa4ded832
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a638c4
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a1fc16
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a75810
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a92368
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a23893
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4a00e17
-     0.00%             1        rcu_sched  [kernel.kallsyms]               [k] 0xffffffffa4a679aa
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a6e743
-     0.00%             1            disks                                 [.] 0x00000000000e9fc7
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a55032
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a58dc9
-     0.00%             1        rcu_sched  [kernel.kallsyms]               [k] 0xffffffffa4a6646c
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a65163
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4ec84f8
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4a54e31
-     0.00%             1        rcu_sched  [kernel.kallsyms]               [k] 0xffffffffa4a63e17
-     0.00%             1           chrome  [kernel.kallsyms]               [k] 0xffffffffa4ec8435
-     0.00%             1        rcu_sched  [kernel.kallsyms]               [k] 0xffffffffa4bf4d14
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a3909e
-     0.00%             1      dbus-daemon  [kernel.kallsyms]               [k] 0xffffffffa4bef0f0
-     0.00%             1            disks                                 [.] 0x0000000000082e08
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4dce5a5
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a66a5c
-     0.00%             1            rsync  [kernel.kallsyms]               [k] 0xffffffffa4bbaa3c
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a1feea
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a5de3a
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a38e6b
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a2cb16
-     0.00%             1          swapper  [kernel.kallsyms]               [k] 0xffffffffa4a7a32f
-     0.00%             5             perf  [kernel.kallsyms]               [k] 0xffffffffa4a13e63
-
-
-# Samples: 266K of event 'instructions'
-# Event count (approx.): 154627854320
-#
-# Overhead       Samples          Command               Shared Object                  Symbol
-# ........  ............  ...............  ..........................  ......................
-#
-     1.65%          2882           chrome  perf-24199.map              [.] 0x0000115bb6c35d7a
-     0.67%           987           chrome  perf-24199.map              [.] 0x0000115bb7ba9b54
-     0.51%           663           chrome  perf-24199.map              [.] 0x0000115bb6af7457
-     0.45%           592           chrome  perf-24199.map              [.] 0x0000115bb6af744b
-     0.45%           660           chrome  perf-24199.map              [.] 0x0000115bb7ba9ebf
-     0.44%           581           chrome  perf-24199.map              [.] 0x0000115bb6af7438
-     0.39%           576           chrome  perf-24199.map              [.] 0x0000115bb7c6f9e5
-     0.37%           488           chrome  perf-24199.map              [.] 0x0000115bb6af7430
-     0.34%           499           chrome  perf-24199.map              [.] 0x0000115bb7c6f93d
-     0.33%           575           chrome  perf-24199.map              [.] 0x0000115bb6c35d81
-     0.33%           573           chrome  perf-24199.map              [.] 0x0000115bb6c35d99
-     0.32%           420           chrome  perf-24199.map              [.] 0x0000115bb6af742b
-     0.30%           391           chrome  perf-24199.map              [.] 0x0000115bb6af7465
-     0.29%           503           chrome  perf-24199.map              [.] 0x0000115bb6c35d76
-     0.29%           377           chrome  perf-24199.map              [.] 0x0000115bb6af73f0
-     0.28%           492           chrome  perf-24199.map              [.] 0x0000115bb6a7afc3
-     0.28%           373           chrome  perf-24199.map              [.] 0x0000115bb6af7428
-     0.27%           361           chrome  perf-24199.map              [.] 0x0000115bb6af7382
-     0.27%           361           chrome  perf-24199.map              [.] 0x0000115bb6af73f9
-     0.27%           360           chrome  perf-24199.map              [.] 0x0000115bb6af73a1
-     0.27%           464           chrome  perf-24199.map              [.] 0x0000115bb6c35d8d
-     0.24%           318           chrome  perf-24199.map              [.] 0x0000115bb6af7421
-     0.24%           425           chrome  perf-24199.map              [.] 0x0000115bb6b80e03
-     0.24%           314           chrome  perf-24199.map              [.] 0x0000115bb6af73f6
-     0.24%           345           chrome  perf-24199.map              [.] 0x0000115bb7c6fa42
-     0.23%           338           chrome  perf-24199.map              [.] 0x0000115bb7c6edd1
-     0.21%           315           chrome  perf-24199.map              [.] 0x0000115bb7ba9ecb
-     0.21%           279           chrome  perf-24199.map              [.] 0x0000115bb6af7418
-     0.21%           277           chrome  perf-24199.map              [.] 0x0000115bb6af73bc
-     0.21%           304           chrome  perf-24199.map              [.] 0x0000115bb7ba9ea8
-     0.20%           534           chrome  perf-24199.map              [.] 0x0000115bb5d21648
-     0.18%           238           chrome  perf-24199.map              [.] 0x0000115bb6af73e6
-     0.17%           227           chrome  perf-24199.map              [.] 0x0000115bb6af73fc
-     0.17%           241           chrome  perf-24199.map              [.] 0x0000115bb7ba9b5d
-     0.16%           240           chrome  perf-24199.map              [.] 0x0000115bb7ba9b10
-     0.16%           285           chrome  perf-24199.map              [.] 0x0000115bb6a7b07c
-     0.15%           205           chrome  perf-24199.map              [.] 0x0000115bb6af73dc
-     0.15%           290           chrome  perf-24199.map              [.] 0x0000115bb6aa460e
-     0.15%           223           chrome  perf-24199.map              [.] 0x0000115bb7ba9b69
-     0.15%           194           chrome  perf-24199.map              [.] 0x0000115bb6af73cc
-     0.15%           191           chrome  perf-24199.map              [.] 0x0000115bb6af974c
-     0.14%           185           chrome  perf-24199.map              [.] 0x0000115bb6af7461
-     0.14%           204           chrome  perf-24199.map              [.] 0x0000115bb7bed940
-     0.14%           368           chrome  perf-24199.map              [.] 0x0000115bb5d21662
-     0.14%           230           chrome  perf-24199.map              [.] 0x0000115bb6a7b412
-     0.14%           199           chrome  perf-24199.map              [.] 0x0000115bb7ba9ed2
-     0.13%           197           chrome  perf-24199.map              [.] 0x0000115bb7ba9ede
-     0.13%           255           chrome  perf-24199.map              [.] 0x0000115bb6aa463a
-     0.13%           191           chrome  perf-24199.map              [.] 0x0000115bb7ba9e8c
-     0.13%           187           chrome  perf-24199.map              [.] 0x0000115bb7c6f9cb
-     0.13%           242           chrome  perf-24199.map              [.] 0x0000115bb6aa4678
-     0.13%           165           chrome  perf-24199.map              [.] 0x0000115bb6af975e
-     0.13%           222           chrome  perf-24199.map              [.] 0x0000115bb6b80dfe
-     0.12%           163           chrome  perf-24199.map              [.] 0x0000115bb6af9746
-     0.12%           234           chrome  perf-24199.map              [.] 0x0000115bb6aa4692
-     0.12%           178           chrome  perf-24199.map              [.] 0x0000115bb7c6ed55
-     0.12%           157           chrome  perf-24199.map              [.] 0x0000115bb6af96f4
-     0.12%           154           chrome  perf-24199.map              [.] 0x0000115bb6af9737
-     0.12%           173           chrome  perf-24199.map              [.] 0x0000115bb7c6fa73
-     0.12%           171           chrome  perf-24199.map              [.] 0x0000115bb7c6fa5e
-     0.12%           200           chrome  perf-24199.map              [.] 0x0000115bb6a7afbb
-     0.12%           199           chrome  perf-24199.map              [.] 0x0000115bb6a7b6fe
-     0.12%           169           chrome  perf-24199.map              [.] 0x0000115bb7c6f8f2
-     0.11%           148           chrome  perf-24199.map              [.] 0x0000115bb6af737e
-     0.11%           205           chrome  libc-2.19.so                [.] 0x0000000000088b72
-     0.11%           212           chrome  perf-24199.map              [.] 0x0000115bb6aa469c
-     0.11%           160           chrome  perf-24199.map              [.] 0x0000115bb7c6f8d0
-     0.11%           204           chrome  perf-24199.map              [.] 0x0000115bb6aa4626
-     0.11%           160           chrome  perf-24199.map              [.] 0x0000115bb7bed932
-     0.11%           154           chrome  perf-24199.map              [.] 0x0000115bb7ba9b18
-     0.11%           137           chrome  perf-24199.map              [.] 0x0000115bb6af972f
-     0.10%           153           chrome  perf-24199.map              [.] 0x0000115bb7c6f9f9
-     0.10%           136           chrome  perf-24199.map              [.] 0x0000115bb6af7394
-     0.10%           238           chrome  chrome                      [.] 0x0000000000e45adc
-     0.10%           131           chrome  perf-24199.map              [.] 0x0000115bb6af977d
-     0.10%           146           chrome  perf-24199.map              [.] 0x0000115bb7c6f907
-     0.10%           171           chrome  perf-24199.map              [.] 0x0000115bb6a7b6e7
-     0.10%           144           chrome  perf-24199.map              [.] 0x0000115bb7c6f9e1
-     0.10%           128           chrome  perf-24199.map              [.] 0x0000115bb6af9732
-     0.10%           256           chrome  perf-24199.map              [.] 0x0000115bb5d21656
-     0.10%           142           chrome  perf-24199.map              [.] 0x0000115bb7c6f9b1
-     0.10%           181           chrome  perf-24199.map              [.] 0x0000115bb6aa464e
-     0.10%           147           chrome  perf-24199.map              [.] 0x0000115bb7bf5700
-     0.10%           181           chrome  perf-24199.map              [.] 0x0000115bb6aa4662
-     0.09%           161           chrome  perf-24199.map              [.] 0x0000115bb6a7af9f
-     0.09%           136           chrome  perf-24199.map              [.] 0x0000115bb7c6f216
-     0.09%           159           chrome  perf-24199.map              [.] 0x0000115bb6a7b377
-     0.09%           228           chrome  perf-24199.map              [.] 0x0000115bb5d0a372
-     0.09%           118           chrome  perf-24199.map              [.] 0x0000115bb6af9769
-     0.09%           117           chrome  perf-24199.map              [.] 0x0000115bb6af96f2
-     0.09%           336           chrome  perf-24199.map              [.] 0x0000115bb5d1726d
-     0.09%           193           chrome  chrome                      [.] 0x0000000000e76562
-     0.09%           117           chrome  perf-24199.map              [.] 0x0000115bb6add6ed
-     0.09%           219           chrome  chrome                      [.] 0x0000000000e45a2b
-     0.09%           148           chrome  perf-24199.map              [.] 0x0000115bb6a7afc9
-     0.08%           111           chrome  perf-24199.map              [.] 0x0000115bb6af972a
-     0.08%           158           chrome  perf-24199.map              [.] 0x0000115bb6aa45f8
-     0.08%           145           chrome  perf-24199.map              [.] 0x0000115bb6a7afed
-     0.08%           112           chrome  perf-24199.map              [.] 0x0000115bb6af73b8
-     0.08%           107           chrome  perf-24199.map              [.] 0x0000115bb6af9707
-     0.08%           118           chrome  perf-24199.map              [.] 0x0000115bb7ba9b3a
-     0.08%           138           chrome  perf-24199.map              [.] 0x0000115bb6a785c9
-     0.08%           117           chrome  perf-24199.map              [.] 0x0000115bb7c6ed3e
-     0.08%           142           chrome  perf-24199.map              [.] 0x0000115bb6b81e10
-     0.08%           106           chrome  perf-24199.map              [.] 0x0000115bb6af73e3
-     0.08%           154           chrome  chrome                      [.] 0x0000000000eb5472
-     0.08%           116           chrome  perf-24199.map              [.] 0x0000115bb7bed9b6
-     0.08%           287           chrome  perf-24199.map              [.] 0x0000115bb5d29f6a
-     0.08%           199           chrome  chrome                      [.] 0x0000000000e6fa2b
-     0.08%           218           chrome  chrome                      [.] 0x0000000000e55b20
-     0.08%           110           chrome  perf-24199.map              [.] 0x0000115bb7c6f925
-     0.07%           112           chrome  perf-24199.map              [.] 0x0000115bb7bb53c2
-     0.07%           107           chrome  perf-24199.map              [.] 0x0000115bb7c6f92d
-     0.07%           155           chrome  perf-24199.map              [.] 0x0000115bb5d2640e
-     0.07%           127           chrome  perf-24199.map              [.] 0x0000115bb6c35d72
-     0.07%           124           chrome  perf-24199.map              [.] 0x0000115bb6a78284
-     0.07%           107           chrome  perf-24199.map              [.] 0x0000115bb7bed990
-     0.07%           421           python  [kernel.kallsyms]           [k] 0xffffffffa4ae14fd
-     0.07%            93           chrome  perf-24199.map              [.] 0x0000115bb6af73e9
-     0.07%           104           chrome  perf-24199.map              [.] 0x0000115bb7bed938
-     0.07%           158           chrome  perf-24199.map              [.] 0x0000115bb6b57933
-     0.07%           100           chrome  perf-24199.map              [.] 0x0000115bb7c6f96b
-     0.07%           123           chrome  perf-24199.map              [.] 0x0000115bb6b8230f
-     0.07%           101           chrome  perf-24199.map              [.] 0x0000115bb7bed9bf
-     0.07%           102           chrome  perf-24199.map              [.] 0x0000115bb7bb53df
-     0.07%           144           chrome  perf-24199.map              [.] 0x0000115bb6b2530a
-     0.07%            91           chrome  perf-24199.map              [.] 0x0000115bb6add73f
-     0.07%            89           chrome  perf-24199.map              [.] 0x0000115bb6add762
-     0.07%            98           chrome  perf-24199.map              [.] 0x0000115bb7bed926
-     0.06%            85           chrome  perf-24199.map              [.] 0x0000115bb6af96a7
-     0.06%           129           chrome  perf-24199.map              [.] 0x0000115bb6aa4699
-     0.06%           112           chrome  perf-24199.map              [.] 0x0000115bb6c35d47
-     0.06%           295           chrome  chrome                      [.] 0x00000000011d1cb0
-     0.06%           114           chrome  perf-24199.map              [.] 0x0000115bb6b822b1
-     0.06%            94           chrome  perf-24199.map              [.] 0x0000115bb7bed99d
-     0.06%            94           chrome  perf-24199.map              [.] 0x0000115bb7bb53f2
-     0.06%            92           chrome  perf-24199.map              [.] 0x0000115bb7ba9b86
-     0.06%            92           chrome  perf-24199.map              [.] 0x0000115bb7ba9b29
-     0.06%            88           chrome  perf-24199.map              [.] 0x0000115bb6e14f87
-     0.06%            80           chrome  perf-24199.map              [.] 0x0000115bb6af9722
-     0.06%           109           chrome  perf-24199.map              [.] 0x0000115bb6b8238a
-     0.06%            93           chrome  perf-24199.map              [.] 0x0000115bb7ba71a3
-     0.06%            80           chrome  perf-24199.map              [.] 0x0000115bb6af747f
-     0.06%           107           chrome  perf-24199.map              [.] 0x0000115bb6b80e22
-     0.06%           104           chrome  perf-24199.map              [.] 0x0000115bb6a72466
-     0.06%            78           chrome  perf-24199.map              [.] 0x0000115bb6add757
-     0.06%            80           chrome  perf-24199.map              [.] 0x0000115bb6add745
-     0.06%           102           chrome  perf-24199.map              [.] 0x0000115bb6c35cec
-     0.06%           202           chrome  chrome                      [.] 0x0000000000f13e9c
-     0.06%           166           chrome  chrome                      [.] 0x0000000000e46989
-     0.06%           318           python  [kernel.kallsyms]           [k] 0xffffffffa4ad6f19
-     0.06%            83           chrome  perf-24199.map              [.] 0x0000115bb7c6f97d
-     0.06%            77           chrome  perf-24199.map              [.] 0x0000115bb6add730
-     0.06%            82           chrome  perf-24199.map              [.] 0x0000115bb7c6f992
-     0.06%           132           chrome  perf-24199.map              [.] 0x0000115bb5d417f2
-     0.06%            76           chrome  perf-24199.map              [.] 0x0000115bb6add728
-     0.06%            72           chrome  perf-24199.map              [.] 0x0000115bb6af9702
-     0.06%            94           chrome  perf-24199.map              [.] 0x0000115bb6c4ea18
-     0.06%           321           chrome  [kernel.kallsyms]           [k] 0xffffffffa4bf4c3d
-     0.06%           139           chrome  perf-24199.map              [.] 0x0000115bb5d21666
-     0.06%            72           chrome  perf-24199.map              [.] 0x0000115bb6af9684
-     0.06%            78           chrome  perf-24199.map              [.] 0x0000115bb6add6a0
-     0.05%            72           chrome  perf-24199.map              [.] 0x0000115bb6af73c8
-     0.05%           137           chrome  perf-24199.map              [.] 0x0000115bb5d0a36e
-     0.05%            97           chrome  perf-24199.map              [.] 0x0000115bb6b82327
-     0.05%            70           chrome  perf-24199.map              [.] 0x0000115bb6b06516
-     0.05%           137           chrome  chrome                      [.] 0x0000000000e6fa1c
-     0.05%           132           chrome  perf-24199.map              [.] 0x0000115bb5d21629
-     0.05%           130           chrome  chrome                      [.] 0x0000000000e54d5c
-     0.05%           122           chrome  chrome                      [.] 0x0000000000e48e5f
-     0.05%           839            lsusb  lsusb                       [.] 0x0000000000010e60
-     0.05%           133           chrome  perf-24199.map              [.] 0x0000115bb5d215dd
-     0.05%           130           chrome  perf-24199.map              [.] 0x0000115bb5d215c9
-     0.05%           130           chrome  perf-24199.map              [.] 0x0000115bb78d3895
-     0.05%            76           chrome  perf-24199.map              [.] 0x0000115bb7c6f174
-     0.01%            46           chrome  chrome                      [.] 0x0000000005d109a9
-     0.01%            15           chrome  perf-24199.map              [.] 0x0000115bb6aa5665
-     0.01%            17           chrome  chrome                      [.] 0x0000000000ec6b13
-     0.01%            18           chrome  perf-24199.map              [.] 0x0000115bb5d417ea
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6c3581e
-     0.01%            10           chrome  perf-24199.map              [.] 0x0000115bb6af94fb
-     0.01%            29           chrome  libc-2.19.so                [.] 0x000000000009a8d5
-     0.01%            25           chrome  chrome                      [.] 0x0000000000e57849
-     0.01%            40           chrome  chrome                      [.] 0x0000000005d1101d
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb6e1502e
-     0.01%            20           chrome  perf-24199.map              [.] 0x0000115bb7c9f11d
-     0.01%            18           chrome  perf-24199.map              [.] 0x0000115bb6b577c8
-     0.01%            30           chrome  [kernel.kallsyms]           [k] 0xffffffffa4acff4a
-     0.01%            38           python  libpython2.7.so.1.0         [.] 0x000000000011ad14
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6b8221e
-     0.01%            59           chrome  i965_dri.so                 [.] 0x0000000000483802
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6a7d197
-     0.01%            19           chrome  perf-24199.map              [.] 0x0000115bb6b51f6a
-     0.01%            31           chrome  libpthread-2.19.so          [.] 0x0000000000009e71
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6c4eac4
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb7bea5c2
-     0.01%            38           chrome  chrome                      [.] 0x0000000000d3e821
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb7c78e2c
-     0.01%            22           chrome  perf-24199.map              [.] 0x0000115bb5d67f0f
-     0.01%            30           chrome  chrome                      [.] 0x0000000000ec4b08
-     0.01%            15           chrome  perf-24199.map              [.] 0x0000115bb5d2793b
-     0.01%            28           chrome  chrome                      [.] 0x0000000000f38669
-     0.01%            43           chrome  chrome                      [.] 0x0000000001bfb240
-     0.01%            20           chrome  perf-24199.map              [.] 0x0000115bb5d09f8e
-     0.01%            30           chrome  perf-24199.map              [.] 0x0000115bb5e3f9b2
-     0.01%            18           chrome  perf-24199.map              [.] 0x0000115bb5d072d7
-     0.01%            32           chrome  ld-2.19.so                  [.] 0x000000000000bcc7
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6a786b6
-     0.01%            36           chrome  chrome                      [.] 0x00000000024366f0
-     0.01%            17           chrome  perf-24199.map              [.] 0x0000115bb6b5a6bf
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6a78024
-     0.01%            16             sshd  sshd                        [.] 0x0000000000075c37
-     0.01%            10           chrome  perf-24199.map              [.] 0x0000115bb6adec72
-     0.01%            15           chrome  perf-24199.map              [.] 0x0000115bb6ba0a78
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6a7b373
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6c3256e
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb6e14f74
-     0.01%            10           chrome  perf-24199.map              [.] 0x0000115bb6af95ac
-     0.01%            26           python  libpython2.7.so.1.0         [.] 0x0000000000095f32
-     0.01%            19           chrome  perf-24199.map              [.] 0x0000115bb5d215e1
-     0.01%            10           chrome  perf-24199.map              [.] 0x0000115bb6adeb55
-     0.01%            16           chrome  perf-24199.map              [.] 0x0000115bb6b2c7db
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb7ba9e3d
-     0.01%            10           chrome  perf-24199.map              [.] 0x0000115bb6af5a62
-     0.01%            25           chrome  chrome                      [.] 0x0000000000e1654e
-     0.01%            24           chrome  perf-24199.map              [.] 0x0000115bb6ca75f2
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb7c6ef16
-     0.01%            17           chrome  chrome                      [.] 0x0000000000ed5a9a
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6b810f4
-     0.01%            30           chrome  chrome                      [.] 0x0000000000e543e9
-     0.01%            17           chrome  perf-24199.map              [.] 0x0000115bb6b5a5e2
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6ab0afa
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb7c6f24e
-     0.01%            16           chrome  perf-24199.map              [.] 0x0000115bb6b2ca81
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6aa471b
-     0.01%            37           chrome  chrome                      [.] 0x0000000000eb4d31
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb6e1f80f
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb7c6f5d9
-     0.01%            31           chrome  chrome                      [.] 0x0000000000e18aa9
-     0.01%            18           chrome  chrome                      [.] 0x0000000000e4907d
-     0.01%            58           chrome  i965_dri.so                 [.] 0x0000000000483806
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6a7b69b
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6b81edd
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb7c6f5b3
-     0.01%            41           chrome  chrome                      [.] 0x0000000000d1e411
-     0.01%            17           chrome  perf-24199.map              [.] 0x0000115bb7b0a72f
-     0.01%            23             lsof  [kernel.kallsyms]           [k] 0xffffffffa4bf4c3d
-     0.01%            16           chrome  chrome                      [.] 0x0000000000e7656e
-     0.01%            16           chrome  perf-24199.map              [.] 0x0000115bb6b24dbd
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6aa5672
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb7ba9d29
-     0.01%            16           chrome  perf-24199.map              [.] 0x0000115bb6cb5bbd
-     0.01%            16           chrome  perf-24199.map              [.] 0x0000115bb6b2ca2a
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb6a72710
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6c35d84
-     0.01%            19           chrome  chrome                      [.] 0x0000000000e45ac6
-     0.01%            38           python  libpython2.7.so.1.0         [.] 0x00000000000a8c9e
-     0.01%            16           chrome  perf-24199.map              [.] 0x0000115bb6b2c989
-     0.01%            16           chrome  perf-24199.map              [.] 0x0000115bb6b2ca71
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6aa5666
-     0.01%            20           chrome  perf-24199.map              [.] 0x0000115bb5d2165e
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6aa4729
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6b80feb
-     0.01%            27           python  libpython2.7.so.1.0         [.] 0x00000000000a8bf8
-     0.01%            23           chrome  chrome                      [.] 0x0000000000eea37a
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb7c6f25f
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb6e1fc56
-     0.01%            16           chrome  perf-24199.map              [.] 0x0000115bb6b315b4
-     0.01%            24           chrome  chrome                      [.] 0x0000000000f16081
-     0.01%            26           chrome  [kernel.kallsyms]           [k] 0xffffffffa4ad9a67
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb5d2177a
-     0.01%            30           chrome  libc-2.19.so                [.] 0x000000000009a991
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6a7b687
-     0.01%            32           chrome  chrome                      [.] 0x0000000000f9f3c0
-     0.01%            25           chrome  chrome                      [.] 0x0000000000f13e73
-     0.01%            17           chrome  chrome                      [.] 0x0000000000e48e41
-     0.01%            25           chrome  chrome                      [.] 0x0000000000f19e4e
-     0.01%            17           chrome  perf-24199.map              [.] 0x0000115bb6b557dd
-     0.01%            28           chrome  chrome                      [.] 0x0000000000e18b99
-     0.01%            17           chrome  perf-24199.map              [.] 0x0000115bb7c9f0c6
-     0.01%            30           python  libpython2.7.so.1.0         [.] 0x00000000000e8dc7
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6ad9163
-     0.01%            17           chrome  perf-24199.map              [.] 0x0000115bb6b5c652
-     0.01%            18           chrome  chrome                      [.] 0x0000000000e45abc
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb6a7d0d7
-     0.01%            37           chrome  chrome                      [.] 0x0000000000ce74f6
-     0.01%            15           chrome  chrome                      [.] 0x0000000000f13df8
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6bc7d01
-     0.01%            20           chrome  perf-24199.map              [.] 0x0000115bb5d265fa
-     0.01%            38           chrome  chrome                      [.] 0x00000000011dc830
-     0.01%            27           chrome  perf-24199.map              [.] 0x0000115bb5d17263
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb6c36bc0
-     0.01%            24           chrome  chrome                      [.] 0x0000000000e18b9d
-     0.01%            10           chrome  perf-24199.map              [.] 0x0000115bb6ad4877
-     0.01%            27           chrome  chrome                      [.] 0x0000000000f15f92
-     0.01%            31           chrome  chrome                      [.] 0x0000000000cf4525
-     0.01%            10           chrome  perf-24199.map              [.] 0x0000115bb6aded45
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb6c36bee
-     0.01%            10           chrome  perf-24199.map              [.] 0x0000115bb6af5ac8
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6aac55f
-     0.01%            16           chrome  perf-24199.map              [.] 0x0000115bb5d07a9c
-     0.01%            15           chrome  chrome                      [.] 0x0000000000e520df
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6b80f05
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6ac669f
-     0.01%            29           chrome  libc-2.19.so                [.] 0x000000000008e2bb
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6b81f43
-     0.01%            32           chrome  ld-2.19.so                  [.] 0x000000000000bca3
-     0.01%            23           chrome  perf-24199.map              [.] 0x0000115bb6ca738d
-     0.01%            10           chrome  perf-24199.map              [.] 0x0000115bb6e1fb74
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6abcae7
-     0.01%            33           chrome  chrome                      [.] 0x0000000000e10fd9
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb7beaa06
-     0.01%            10           chrome  perf-24199.map              [.] 0x0000115bb6e150b1
-     0.01%            27           chrome  perf-24199.map              [.] 0x0000115bb7e1e828
-     0.01%            23           chrome  chrome                      [.] 0x0000000000f1608a
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb6a7b4f3
-     0.01%            18           chrome  perf-24199.map              [.] 0x0000115bb6b57760
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb7bf5036
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6b814d1
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6ba4ea1
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6aaca3e
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb7bf5678
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6a7d202
-     0.01%            24           chrome  ld-2.19.so                  [.] 0x000000000000967a
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb706289d
-     0.01%            16           chrome  perf-24199.map              [.] 0x0000115bb6b252a0
-     0.01%            10           chrome  perf-24199.map              [.] 0x0000115bb6e1fbb7
-     0.01%            23           chrome  chrome                      [.] 0x0000000000f15f6a
-     0.01%            27           chrome  chrome                      [.] 0x0000000000f19e57
-     0.01%            20           chrome  chrome                      [.] 0x0000000000e5752d
-     0.01%            14           chrome  perf-24199.map              [.] 0x0000115bb6a9f27e
-     0.01%            24               ps  [kernel.kallsyms]           [k] 0xffffffffa4bee3ef
-     0.01%            18           chrome  chrome                      [.] 0x0000000000ed5ad3
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb6c4e9e3
-     0.01%            16           chrome  perf-24199.map              [.] 0x0000115bb6b5a2e0
-     0.01%            25           chrome  chrome                      [.] 0x0000000000eb696b
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6b8213f
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb6c35d37
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb6a7b399
-     0.01%            21           chrome  chrome                      [.] 0x0000000000e4722f
-     0.01%            20           chrome  chrome                      [.] 0x0000000000dbec48
-     0.01%            15           chrome  perf-24199.map              [.] 0x0000115bb6b358f7
-     0.01%            13           chrome  perf-24199.map              [.] 0x0000115bb6b8215a
-     0.01%            21           chrome  libc-2.19.so                [.] 0x000000000008d855
-     0.01%            10           chrome  perf-24199.map              [.] 0x0000115bb6af5abf
-     0.01%            36           chrome  chrome                      [.] 0x0000000005d10df6
-     0.01%            16           chrome  perf-24199.map              [.] 0x0000115bb6b5c716
-     0.01%            18           chrome  chrome                      [.] 0x0000000000e45c20
-     0.01%            26           chrome  chrome                      [.] 0x0000000000f1606e
-     0.01%            22           chrome  chrome                      [.] 0x0000000000f1419e
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb7c8ad93
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb6a784f8
-     0.01%            17           chrome  perf-24199.map              [.] 0x0000115bb7cdc597
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb6b810d3
-     0.01%            10           chrome  perf-24199.map              [.] 0x0000115bb7ba9bae
-     0.01%            19           chrome  chrome                      [.] 0x0000000000e48dd8
-     0.01%            19           chrome  chrome                      [.] 0x0000000000eb4de8
-     0.01%            10           chrome  perf-24199.map              [.] 0x0000115bb6e41577
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb6b7d7f6
-     0.01%            12           chrome  perf-24199.map              [.] 0x0000115bb6b794d1
-     0.01%             9           chrome  perf-24199.map              [.] 0x0000115bb6af95d5
-     0.01%            17           chrome  chrome                      [.] 0x0000000000e6fa00
-     0.01%            11           chrome  perf-24199.map              [.] 0x0000115bb7c8b961
-     0.01%            15           chrome  perf-24199.map              [.] 0x0000115bb6b315c5
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a549b6
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a923d2
-     0.00%             1           python  [kernel.kallsyms]           [k] 0xffffffffa4b03afa
-     0.00%             1             tcsd  [kernel.kallsyms]           [k] 0xffffffffa4a5c294
-     0.00%             1     kworker/0:1H  [kernel.kallsyms]           [k] 0xffffffffa4cef732
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a00c88
-     0.00%             1      kworker/u:5  [kernel.kallsyms]           [k] 0xffffffffa4a8ac8a
-     0.00%             1             lsof  [kernel.kallsyms]           [k] 0xffffffffa4a51a55
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a3a27c
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a58dff
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a66016
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a92359
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a93347
-     0.00%             1               sh  libc-2.19.so                [.] 0x0000000000082a06
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a38e4d
-     0.00%             1           powerd                             [.] 0x00000000000a0930
-     0.00%             1           python  [kernel.kallsyms]           [k] 0xffffffffa4ec8d72
-     0.00%             1           powerd  [kernel.kallsyms]           [k] 0xffffffffa4a408c8
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4ab35a2
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a5fda6
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a5fed2
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a543e8
-     0.00%             1           dhcpcd                             [.] 0x00000000000280a3
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a54db9
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a00e21
-     0.00%             1           python  [kernel.kallsyms]           [k] 0xffffffffa4a65ea8
-     0.00%             1           python  [kernel.kallsyms]           [k] 0xffffffffa4adde99
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a5655b
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a40550
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a54a89
-     0.00%             1           python  libpython2.7.so.1.0         [.] 0x000000000008f1be
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a64ade
-     0.00%             1           python  [kernel.kallsyms]           [k] 0xffffffffa4bbaa30
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a00c06
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a66bfe
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a63bf9
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a5fe01
-     0.00%             1             tcsd  [kernel.kallsyms]           [k] 0xffffffffa4a5f308
-     0.00%             1           chrome  [kernel.kallsyms]           [k] 0xffffffffa4be56c1
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a40596
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a9381f
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a757c5
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a65ea3
-     0.00%             1         rsyslogd  [kernel.kallsyms]           [k] 0xffffffffa4bee6b7
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a2d72a
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a63d00
-     0.00%             1           chrome  chrome                      [.] 0x0000000000cd3879
-     0.00%             1           python  libc-2.19.so                [.] 0x00000000000e9fc7
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a58e03
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a65f0b
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4ec89d8
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4ec90d6
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a548c8
-     0.00%             1           python  [kernel.kallsyms]           [k] 0xffffffffa4a38453
-     0.00%             1           python  [kernel.kallsyms]           [k] 0xffffffffa4a66335
-     0.00%             1           chrome  chrome                      [.] 0x0000000000cc75d8
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a2058f
-     0.00%             1           chrome  [kernel.kallsyms]           [k] 0xffffffffa4e0f293
-     0.00%             1      kworker/u:0  [ath9k]                     [k] 0x0000000000008126
-     0.00%             1      jbd2/sda1-8  [kernel.kallsyms]           [k] 0xffffffffa4a08ab7
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a2388e
-     0.00%             1           powerd                             [.] 0x000000000007dca0
-     0.00%             1           python  [kernel.kallsyms]           [k] 0xffffffffa4a2c97c
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a0659b
-     0.00%             1           chrome  chrome                      [.] 0x0000000000d0e3bd
-     0.00%             1      cryptohomed                             [.] 0x000000000001dcd0
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4bec0c0
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a62145
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4bf3318
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a5fd33
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a2d71a
-     0.00%             1      kworker/u:0  [kernel.kallsyms]           [k] 0xffffffffa4ad29f3
-     0.00%             1             tcsd  [kernel.kallsyms]           [k] 0xffffffffa4a67997
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a548a8
-     0.00%             1           python  libc-2.19.so                [.] 0x0000000000082bb3
-     0.00%             1           dhcpcd                             [.] 0x0000000000024f56
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4bf334e
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a65e88
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a41578
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a65a87
-     0.00%             1           python  [kernel.kallsyms]           [k] 0xffffffffa4a66d95
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4ec89cf
-     0.00%             1           powerd  [kernel.kallsyms]           [k] 0xffffffffa4c6a22e
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a91b84
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a4073d
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a408c2
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a66a84
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a61b9f
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4ec8c7f
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a40a1d
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4ec8b7c
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a6646c
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a3a6d4
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a41623
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a75d98
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4bf32fd
-     0.00%             1           chrome  chrome                      [.] 0x00000000022ad6a0
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a62152
-     0.00%             1      dbus-daemon  [kernel.kallsyms]           [k] 0xffffffffa4b0d38b
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a58d66
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4ecb3ed
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a54eca
-     0.00%             1  periodic_schedu  [kernel.kallsyms]           [k] 0xffffffffa4a92b1a
-     0.00%             1  periodic_schedu  [kernel.kallsyms]           [k] 0xffffffffa4bec2e9
-     0.00%             1               sh  [kernel.kallsyms]           [k] 0xffffffffa4a92a06
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a65b67
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a66ce5
-     0.00%             1           python  [kernel.kallsyms]           [k] 0xffffffffa4b06121
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a54418
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a65cc5
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a547ee
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a64c99
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a5adda
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a923ac
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a91ec6
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a63269
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a62f53
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a621ff
-     0.00%             1           chrome  [kernel.kallsyms]           [k] 0xffffffffa4a65d8b
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a5de41
-     0.00%             1           chrome  [kernel.kallsyms]           [k] 0xffffffffa4a56f3d
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4be56c6
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a5d828
-     0.00%             1        rcu_sched  [kernel.kallsyms]           [k] 0xffffffffa4a65ccf
-     0.00%             1               sh  [kernel.kallsyms]           [k] 0xffffffffa4b06cb5
-     0.00%             1             perf  [kernel.kallsyms]           [k] 0xffffffffa4ab0a38
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a1ff32
-     0.00%             1          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a7aac4
-     0.00%             8          swapper  [kernel.kallsyms]           [k] 0xffffffffa4a0ee03
-     0.00%             4             perf  [kernel.kallsyms]           [k] 0xffffffffa4a0ee03
-
-#
-# (For a higher level overview, try: perf report --sort comm,dso)
-#
diff --git a/crosperf/results_cache.py b/crosperf/results_cache.py
deleted file mode 100644
index b08fde48..00000000
--- a/crosperf/results_cache.py
+++ /dev/null
@@ -1,1683 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Module to deal with result cache."""
-
-
-import collections
-import glob
-import hashlib
-import heapq
-import json
-import os
-import pickle
-import re
-import tempfile
-
-from cros_utils import command_executer
-from cros_utils import misc
-from image_checksummer import ImageChecksummer
-import results_report
-import test_flag
-
-
-SCRATCH_DIR = os.path.expanduser("~/cros_scratch")
-RESULTS_FILE = "results.pickle"
-MACHINE_FILE = "machine.txt"
-AUTOTEST_TARBALL = "autotest.tbz2"
-RESULTS_TARBALL = "results.tbz2"
-PERF_RESULTS_FILE = "perf-results.txt"
-CACHE_KEYS_FILE = "cache_keys.txt"
-
-
-class PidVerificationError(Exception):
-    """Error of perf PID verification in per-process mode."""
-
-
-class PerfDataReadError(Exception):
-    """Error of reading a perf.data header."""
-
-
-class Result(object):
-    """Class for holding the results of a single test run.
-
-    This class manages what exactly is stored inside the cache without knowing
-    what the key of the cache is. For runs with perf, it stores perf.data,
-    perf.report, etc. The key generation is handled by the ResultsCache class.
-    """
-
-    def __init__(self, logger, label, log_level, machine, cmd_exec=None):
-        self.chromeos_root = label.chromeos_root
-        self._logger = logger
-        self.ce = cmd_exec or command_executer.GetCommandExecuter(
-            self._logger, log_level=log_level
-        )
-        self.temp_dir = None
-        self.label = label
-        self.results_dir = None
-        self.log_level = log_level
-        self.machine = machine
-        self.perf_data_files = []
-        self.perf_report_files = []
-        self.results_file = []
-        self.turbostat_log_file = ""
-        self.cpustats_log_file = ""
-        self.cpuinfo_file = ""
-        self.top_log_file = ""
-        self.wait_time_log_file = ""
-        self.chrome_version = ""
-        self.err = None
-        self.chroot_results_dir = ""
-        self.test_name = ""
-        self.keyvals = None
-        self.board = None
-        self.suite = None
-        self.cwp_dso = ""
-        self.retval = None
-        self.out = None
-        self.top_cmds = []
-
-    def GetTopCmds(self):
-        """Get the list of top commands consuming CPU on the machine."""
-        return self.top_cmds
-
-    def FormatStringTopCommands(self):
-        """Get formatted string of top commands.
-
-        Get the formatted string with top commands consuming CPU on DUT machine.
-        Number of "non-chrome" processes in the list is limited to 5.
-        """
-        format_list = [
-            "Top commands with highest CPU usage:",
-            # Header.
-            "%20s %9s %6s   %s" % ("COMMAND", "AVG CPU%", "COUNT", "HIGHEST 5"),
-            "-" * 50,
-        ]
-        if self.top_cmds:
-            # After switching to top processes we have to expand the list since there
-            # will be a lot of 'chrome' processes (up to 10, sometimes more) in the
-            # top.
-            # Let's limit the list size by the number of non-chrome processes.
-            limit_of_non_chrome_procs = 5
-            num_of_non_chrome_procs = 0
-            for topcmd in self.top_cmds:
-                print_line = "%20s %9.2f %6s   %s" % (
-                    topcmd["cmd"],
-                    topcmd["cpu_use_avg"],
-                    topcmd["count"],
-                    topcmd["top5_cpu_use"],
-                )
-                format_list.append(print_line)
-                if not topcmd["cmd"].startswith("chrome"):
-                    num_of_non_chrome_procs += 1
-                    if num_of_non_chrome_procs >= limit_of_non_chrome_procs:
-                        break
-        else:
-            format_list.append("[NO DATA FROM THE TOP LOG]")
-        format_list.append("-" * 50)
-        return "\n".join(format_list)
-
-    def CopyFilesTo(self, dest_dir, files_to_copy):
-        file_index = 0
-        for file_to_copy in files_to_copy:
-            if not os.path.isdir(dest_dir):
-                command = "mkdir -p %s" % dest_dir
-                self.ce.RunCommand(command)
-            dest_file = os.path.join(
-                dest_dir,
-                ("%s.%s" % (os.path.basename(file_to_copy), file_index)),
-            )
-            ret = self.ce.CopyFiles(file_to_copy, dest_file, recursive=False)
-            if ret:
-                raise IOError("Could not copy results file: %s" % file_to_copy)
-            file_index += 1
-
-    def CopyResultsTo(self, dest_dir):
-        self.CopyFilesTo(dest_dir, self.results_file)
-        self.CopyFilesTo(dest_dir, self.perf_data_files)
-        self.CopyFilesTo(dest_dir, self.perf_report_files)
-        extra_files = []
-        if self.top_log_file:
-            extra_files.append(self.top_log_file)
-        if self.cpuinfo_file:
-            extra_files.append(self.cpuinfo_file)
-        if extra_files:
-            self.CopyFilesTo(dest_dir, extra_files)
-        if self.results_file or self.perf_data_files or self.perf_report_files:
-            self._logger.LogOutput("Results files stored in %s." % dest_dir)
-
-    def CompressResultsTo(self, dest_dir):
-        tarball = os.path.join(self.results_dir, RESULTS_TARBALL)
-        # Test_that runs hold all output under TEST_NAME_HASHTAG/results/,
-        # while tast runs hold output under TEST_NAME/.
-        # Both ensure to be unique.
-        result_dir_name = self.test_name if self.suite == "tast" else "results"
-        results_dir = self.FindFilesInResultsDir(
-            "-name %s" % result_dir_name
-        ).split("\n")[0]
-
-        if not results_dir:
-            self._logger.LogOutput(
-                "WARNING: No results dir matching %r found" % result_dir_name
-            )
-            return
-
-        self.CreateTarball(results_dir, tarball)
-        self.CopyFilesTo(dest_dir, [tarball])
-        if results_dir:
-            self._logger.LogOutput(
-                "Results files compressed into %s." % dest_dir
-            )
-
-    def GetNewKeyvals(self, keyvals_dict):
-        # Initialize 'units' dictionary.
-        units_dict = {}
-        for k in keyvals_dict:
-            units_dict[k] = ""
-        results_files = self.GetDataMeasurementsFiles()
-        for f in results_files:
-            # Make sure we can find the results file
-            if os.path.exists(f):
-                data_filename = f
-            else:
-                # Otherwise get the base filename and create the correct
-                # path for it.
-                _, f_base = misc.GetRoot(f)
-                data_filename = misc.GetOutsideChrootPath(
-                    self.chromeos_root,
-                    os.path.join("/tmp", self.temp_dir, f_base),
-                )
-            if data_filename.find(".json") > 0:
-                raw_dict = dict()
-                if os.path.exists(data_filename):
-                    with open(
-                        data_filename, "r", encoding="utf-8"
-                    ) as data_file:
-                        raw_dict = json.load(data_file)
-
-                if "charts" in raw_dict:
-                    raw_dict = raw_dict["charts"]
-                for k1 in raw_dict:
-                    field_dict = raw_dict[k1]
-                    for k2 in field_dict:
-                        result_dict = field_dict[k2]
-                        key = k1 + "__" + k2
-                        if "value" in result_dict:
-                            keyvals_dict[key] = result_dict["value"]
-                        elif "values" in result_dict:
-                            values = result_dict["values"]
-                            if (
-                                "type" in result_dict
-                                and result_dict["type"]
-                                == "list_of_scalar_values"
-                                and values
-                                and values != "null"
-                            ):
-                                keyvals_dict[key] = sum(values) / float(
-                                    len(values)
-                                )
-                            else:
-                                keyvals_dict[key] = values
-                        units_dict[key] = result_dict["units"]
-            else:
-                if os.path.exists(data_filename):
-                    with open(
-                        data_filename, "r", encoding="utf-8"
-                    ) as data_file:
-                        lines = data_file.readlines()
-                        for line in lines:
-                            tmp_dict = json.loads(line)
-                            graph_name = tmp_dict["graph"]
-                            graph_str = (
-                                (graph_name + "__") if graph_name else ""
-                            )
-                            key = graph_str + tmp_dict["description"]
-                            keyvals_dict[key] = tmp_dict["value"]
-                            units_dict[key] = tmp_dict["units"]
-
-        return keyvals_dict, units_dict
-
-    def AppendTelemetryUnits(self, keyvals_dict, units_dict):
-        """keyvals_dict is the dict of key-value used to generate Crosperf reports.
-
-        units_dict is a dictionary of the units for the return values in
-        keyvals_dict.  We need to associate the units with the return values,
-        for Telemetry tests, so that we can include the units in the reports.
-        This function takes each value in keyvals_dict, finds the corresponding
-        unit in the units_dict, and replaces the old value with a list of the
-        old value and the units.  This later gets properly parsed in the
-        ResultOrganizer class, for generating the reports.
-        """
-
-        results_dict = {}
-        for k in keyvals_dict:
-            # We don't want these lines in our reports; they add no useful data.
-            if not k or k == "telemetry_Crosperf":
-                continue
-            val = keyvals_dict[k]
-            units = units_dict[k]
-            new_val = [val, units]
-            results_dict[k] = new_val
-        return results_dict
-
-    def GetKeyvals(self):
-        results_in_chroot = misc.GetOutsideChrootPath(
-            self.chromeos_root, "/tmp"
-        )
-        if not self.temp_dir:
-            self.temp_dir = tempfile.mkdtemp(dir=results_in_chroot)
-            command = f"cp -r {self.results_dir}/* {self.temp_dir}"
-            self.ce.RunCommand(command, print_to_console=False)
-
-        tmp_dir_in_chroot = misc.GetInsideChrootPath(
-            self.chromeos_root, self.temp_dir
-        )
-        command = "./generate_test_report --no-color --csv %s" % (
-            tmp_dir_in_chroot
-        )
-        _, out, _ = self.ce.ChrootRunCommandWOutput(
-            self.chromeos_root, command, print_to_console=False
-        )
-        keyvals_dict = {}
-        for line in out.splitlines():
-            tokens = re.split("=|,", line)
-            key = tokens[-2]
-            if key.startswith(tmp_dir_in_chroot):
-                key = key[len(tmp_dir_in_chroot) + 1 :]
-            value = tokens[-1]
-            keyvals_dict[key] = value
-
-        # Check to see if there is a perf_measurements file and get the
-        # data from it if so.
-        keyvals_dict, units_dict = self.GetNewKeyvals(keyvals_dict)
-        if self.suite == "telemetry_Crosperf":
-            # For telemtry_Crosperf results, append the units to the return
-            # results, for use in generating the reports.
-            keyvals_dict = self.AppendTelemetryUnits(keyvals_dict, units_dict)
-        return keyvals_dict
-
-    def GetSamples(self):
-        actual_samples = 0
-        for perf_data_file in self.perf_data_files:
-            chroot_perf_data_file = misc.GetInsideChrootPath(
-                self.chromeos_root, perf_data_file
-            )
-            perf_path = misc.GetOutsideChrootPath(
-                self.chromeos_root, "/usr/bin/perf"
-            )
-            perf_file = "/usr/sbin/perf"
-            if os.path.exists(perf_path):
-                perf_file = "/usr/bin/perf"
-
-            # For each perf.data, we want to collect sample count for specific DSO.
-            # We specify exact match for known DSO type, and every sample for `all`.
-            exact_match = ""
-            if self.cwp_dso == "all":
-                exact_match = ""
-            elif self.cwp_dso == "chrome":
-                exact_match = "chrome"
-            elif self.cwp_dso == "kallsyms":
-                exact_match = "[kernel.kallsyms]"
-            else:
-                # This will need to be updated once there are more DSO types supported,
-                # if user want an exact match for the field they want.
-                exact_match = self.cwp_dso
-
-            command = (
-                f"{perf_file} report -n -s dso -i "
-                f"{chroot_perf_data_file} 2> /dev/null"
-            )
-            _, result, _ = self.ce.ChrootRunCommandWOutput(
-                self.chromeos_root, command
-            )
-            # Accumulate the sample count for all matched fields.
-            # Each line looks like this:
-            #     45.42%        237210  chrome
-            # And we want the second number which is the sample count.
-            samples = 0
-            try:
-                for line in result.split("\n"):
-                    attr = line.split()
-                    if len(attr) == 3 and "%" in attr[0]:
-                        if exact_match and exact_match != attr[2]:
-                            continue
-                        samples += int(attr[1])
-            except:
-                raise RuntimeError("Cannot parse perf dso result")
-
-            actual_samples += samples
-
-            # Remove idle cycles from the accumulated sample count.
-            perf_report_file = f"{perf_data_file}.report"
-            if not os.path.exists(perf_report_file):
-                raise RuntimeError(
-                    f"Missing perf report file: {perf_report_file}"
-                )
-
-            idle_functions = {
-                "[kernel.kallsyms]": (
-                    "intel_idle",
-                    "arch_cpu_idle",
-                    "intel_idle",
-                    "cpu_startup_entry",
-                    "default_idle",
-                    "cpu_idle_loop",
-                    "do_idle",
-                    "cpuidle_enter_state",
-                ),
-            }
-            idle_samples = 0
-
-            with open(perf_report_file, encoding="utf-8") as f:
-                try:
-                    for line in f:
-                        line = line.strip()
-                        if not line or line[0] == "#":
-                            continue
-                        # Each line has the following fields,
-                        # pylint: disable=line-too-long
-                        # Overhead       Samples  Command          Shared Object         Symbol
-                        # pylint: disable=line-too-long
-                        # 1.48%          60       swapper          [kernel.kallsyms]     [k] intel_idle
-                        # pylint: disable=line-too-long
-                        # 0.00%          1        shill            libshill-net.so       [.] std::__1::vector<unsigned char, std::__1::allocator<unsigned char> >::vector<unsigned char const*>
-                        _, samples, _, dso, _, function = line.split(None, 5)
-
-                        if (
-                            dso in idle_functions
-                            and function in idle_functions[dso]
-                        ):
-                            if self.log_level != "verbose":
-                                self._logger.LogOutput(
-                                    "Removing %s samples from %s in %s"
-                                    % (samples, function, dso)
-                                )
-                            idle_samples += int(samples)
-                except:
-                    raise RuntimeError("Cannot parse perf report")
-            actual_samples -= idle_samples
-        return [actual_samples, "samples"]
-
-    def GetResultsDir(self):
-        if self.suite == "tast":
-            mo = re.search(r"Writing results to (\S+)", self.out)
-        else:
-            mo = re.search(r"Results placed in (\S+)", self.out)
-        if mo:
-            result = mo.group(1)
-            return result
-        raise RuntimeError("Could not find results directory.")
-
-    def FindFilesInResultsDir(self, find_args):
-        if not self.results_dir:
-            return ""
-
-        command = "find %s %s" % (self.results_dir, find_args)
-        ret, out, _ = self.ce.RunCommandWOutput(command, print_to_console=False)
-        if ret:
-            raise RuntimeError("Could not run find command!")
-        return out
-
-    def GetResultsFile(self):
-        if self.suite == "telemetry_Crosperf":
-            return self.FindFilesInResultsDir(
-                "-name histograms.json"
-            ).splitlines()
-        return self.FindFilesInResultsDir(
-            "-name results-chart.json"
-        ).splitlines()
-
-    def GetPerfDataFiles(self):
-        return self.FindFilesInResultsDir("-name perf.data").splitlines()
-
-    def GetPerfReportFiles(self):
-        return self.FindFilesInResultsDir("-name perf.data.report").splitlines()
-
-    def GetDataMeasurementsFiles(self):
-        result = self.FindFilesInResultsDir(
-            "-name perf_measurements"
-        ).splitlines()
-        if not result:
-            if self.suite == "telemetry_Crosperf":
-                result = self.FindFilesInResultsDir(
-                    "-name histograms.json"
-                ).splitlines()
-            else:
-                result = self.FindFilesInResultsDir(
-                    "-name results-chart.json"
-                ).splitlines()
-        return result
-
-    def GetTurbostatFile(self):
-        """Get turbostat log path string."""
-        return self.FindFilesInResultsDir("-name turbostat.log").split("\n")[0]
-
-    def GetCpustatsFile(self):
-        """Get cpustats log path string."""
-        return self.FindFilesInResultsDir("-name cpustats.log").split("\n")[0]
-
-    def GetCpuinfoFile(self):
-        """Get cpustats log path string."""
-        return self.FindFilesInResultsDir("-name cpuinfo.log").split("\n")[0]
-
-    def GetTopFile(self):
-        """Get cpustats log path string."""
-        return self.FindFilesInResultsDir("-name top.log").split("\n")[0]
-
-    def GetWaitTimeFile(self):
-        """Get wait time log path string."""
-        return self.FindFilesInResultsDir("-name wait_time.log").split("\n")[0]
-
-    def _CheckDebugPath(self, option, path):
-        out_chroot_path = misc.GetOutsideChrootPath(self.chromeos_root, path)
-        if os.path.exists(out_chroot_path):
-            if option == "kallsyms":
-                path = os.path.join(path, "System.map-*")
-            return "--" + option + " " + path
-        else:
-            print(
-                "** WARNING **: --%s option not applied, %s does not exist"
-                % (option, out_chroot_path)
-            )
-            return ""
-
-    def GeneratePerfReportFiles(self):
-        perf_report_files = []
-        for perf_data_file in self.perf_data_files:
-            # Generate a perf.report and store it side-by-side with the perf.data
-            # file.
-            chroot_perf_data_file = misc.GetInsideChrootPath(
-                self.chromeos_root, perf_data_file
-            )
-            perf_report_file = "%s.report" % perf_data_file
-            if os.path.exists(perf_report_file):
-                raise RuntimeError(
-                    "Perf report file already exists: %s" % perf_report_file
-                )
-            chroot_perf_report_file = misc.GetInsideChrootPath(
-                self.chromeos_root, perf_report_file
-            )
-            perf_path = misc.GetOutsideChrootPath(
-                self.chromeos_root, "/usr/bin/perf"
-            )
-
-            perf_file = "/usr/sbin/perf"
-            if os.path.exists(perf_path):
-                perf_file = "/usr/bin/perf"
-
-            debug_path = self.label.debug_path
-
-            if debug_path:
-                symfs = "--symfs " + debug_path
-                vmlinux = "--vmlinux " + os.path.join(
-                    debug_path, "usr", "lib", "debug", "boot", "vmlinux"
-                )
-                kallsyms = ""
-                print(
-                    "** WARNING **: --kallsyms option not applied, no System.map-* "
-                    "for downloaded image."
-                )
-            else:
-                if self.label.image_type != "local":
-                    print(
-                        "** WARNING **: Using local debug info in /build, this may "
-                        "not match the downloaded image."
-                    )
-                build_path = os.path.join("/build", self.board)
-                symfs = self._CheckDebugPath("symfs", build_path)
-                vmlinux_path = os.path.join(
-                    build_path, "usr/lib/debug/boot/vmlinux"
-                )
-                vmlinux = self._CheckDebugPath("vmlinux", vmlinux_path)
-                kallsyms_path = os.path.join(build_path, "boot")
-                kallsyms = self._CheckDebugPath("kallsyms", kallsyms_path)
-
-            command = "%s report -n %s %s %s -i %s --stdio > %s" % (
-                perf_file,
-                symfs,
-                vmlinux,
-                kallsyms,
-                chroot_perf_data_file,
-                chroot_perf_report_file,
-            )
-            if self.log_level != "verbose":
-                self._logger.LogOutput(
-                    "Generating perf report...\nCMD: %s" % command
-                )
-            exit_code = self.ce.ChrootRunCommand(self.chromeos_root, command)
-            if exit_code == 0:
-                if self.log_level != "verbose":
-                    self._logger.LogOutput(
-                        "Perf report generated successfully."
-                    )
-            else:
-                raise RuntimeError(
-                    "Perf report not generated correctly. CMD: %s" % command
-                )
-
-            # Add a keyval to the dictionary for the events captured.
-            perf_report_files.append(
-                misc.GetOutsideChrootPath(
-                    self.chromeos_root, chroot_perf_report_file
-                )
-            )
-        return perf_report_files
-
-    def GatherPerfResults(self):
-        report_id = 0
-        for perf_report_file in self.perf_report_files:
-            with open(perf_report_file, "r", encoding="utf-8") as f:
-                report_contents = f.read()
-                for group in re.findall(
-                    r"Events: (\S+) (\S+)", report_contents
-                ):
-                    num_events = group[0]
-                    event_name = group[1]
-                    key = "perf_%s_%s" % (report_id, event_name)
-                    value = str(misc.UnitToNumber(num_events))
-                    self.keyvals[key] = value
-
-    def PopulateFromRun(self, out, err, retval, test, suite, cwp_dso):
-        self.board = self.label.board
-        self.out = out
-        self.err = err
-        self.retval = retval
-        self.test_name = test
-        self.suite = suite
-        self.cwp_dso = cwp_dso
-        self.chroot_results_dir = self.GetResultsDir()
-        self.results_dir = misc.GetOutsideChrootPath(
-            self.chromeos_root, self.chroot_results_dir
-        )
-        self.results_file = self.GetResultsFile()
-        self.perf_data_files = self.GetPerfDataFiles()
-        # Include all perf.report data in table.
-        self.perf_report_files = self.GeneratePerfReportFiles()
-        self.turbostat_log_file = self.GetTurbostatFile()
-        self.cpustats_log_file = self.GetCpustatsFile()
-        self.cpuinfo_file = self.GetCpuinfoFile()
-        self.top_log_file = self.GetTopFile()
-        self.wait_time_log_file = self.GetWaitTimeFile()
-        # TODO(asharif): Do something similar with perf stat.
-
-        # Grab keyvals from the directory.
-        self.ProcessResults()
-
-    def ProcessChartResults(self):
-        # Open and parse the json results file generated by telemetry/test_that.
-        if not self.results_file:
-            raise IOError("No results file found.")
-        filename = self.results_file[0]
-        if not filename.endswith(".json"):
-            raise IOError(
-                "Attempt to call json on non-json file: %s" % filename
-            )
-        if not os.path.exists(filename):
-            raise IOError("%s does not exist" % filename)
-
-        keyvals = {}
-        with open(filename, "r", encoding="utf-8") as f:
-            raw_dict = json.load(f)
-            if "charts" in raw_dict:
-                raw_dict = raw_dict["charts"]
-            for k, field_dict in raw_dict.items():
-                for item in field_dict:
-                    keyname = k + "__" + item
-                    value_dict = field_dict[item]
-                    if "value" in value_dict:
-                        result = value_dict["value"]
-                    elif "values" in value_dict:
-                        values = value_dict["values"]
-                        if not values:
-                            continue
-                        if (
-                            "type" in value_dict
-                            and value_dict["type"] == "list_of_scalar_values"
-                            and values != "null"
-                        ):
-                            result = sum(values) / float(len(values))
-                        else:
-                            result = values
-                    else:
-                        continue
-                    units = value_dict["units"]
-                    new_value = [result, units]
-                    keyvals[keyname] = new_value
-        return keyvals
-
-    def ProcessTurbostatResults(self):
-        """Given turbostat_log_file non-null parse cpu stats from file.
-
-        Returns:
-          Dictionary of 'cpufreq', 'cputemp' where each
-          includes dictionary 'all': [list_of_values]
-
-        Example of the output of turbostat_log.
-        ----------------------
-        CPU     Avg_MHz Busy%   Bzy_MHz TSC_MHz IRQ     CoreTmp
-        -       329     12.13   2723    2393    10975   77
-        0       336     12.41   2715    2393    6328    77
-        2       323     11.86   2731    2393    4647    69
-        CPU     Avg_MHz Busy%   Bzy_MHz TSC_MHz IRQ     CoreTmp
-        -       1940    67.46   2884    2393    39920   83
-        0       1827    63.70   2877    2393    21184   83
-        """
-        cpustats = {}
-        read_data = ""
-        with open(self.turbostat_log_file, encoding="utf-8") as f:
-            read_data = f.readlines()
-
-        if not read_data:
-            self._logger.LogOutput("WARNING: Turbostat output file is empty.")
-            return {}
-
-        # First line always contains the header.
-        stats = read_data[0].split()
-
-        # Mandatory parameters.
-        if "CPU" not in stats:
-            self._logger.LogOutput(
-                "WARNING: Missing data for CPU# in Turbostat output."
-            )
-            return {}
-        if "Bzy_MHz" not in stats:
-            self._logger.LogOutput(
-                "WARNING: Missing data for Bzy_MHz in Turbostat output."
-            )
-            return {}
-        cpu_index = stats.index("CPU")
-        cpufreq_index = stats.index("Bzy_MHz")
-        cpufreq = cpustats.setdefault("cpufreq", {"all": []})
-
-        # Optional parameters.
-        cputemp_index = -1
-        if "CoreTmp" in stats:
-            cputemp_index = stats.index("CoreTmp")
-            cputemp = cpustats.setdefault("cputemp", {"all": []})
-
-        # Parse data starting from the second line ignoring repeating headers.
-        for st in read_data[1:]:
-            # Data represented by int or float separated by spaces.
-            numbers = st.split()
-            if not all(
-                word.replace(".", "", 1).isdigit() for word in numbers[1:]
-            ):
-                # Skip the line if data mismatch.
-                continue
-            if numbers[cpu_index] != "-":
-                # Ignore Core-specific statistics which starts with Core number.
-                # Combined statistics for all core has "-" CPU identifier.
-                continue
-
-            cpufreq["all"].append(int(numbers[cpufreq_index]))
-            if cputemp_index != -1:
-                cputemp["all"].append(int(numbers[cputemp_index]))
-        return cpustats
-
-    def ProcessTopResults(self):
-        """Given self.top_log_file process top log data.
-
-        Returns:
-          List of dictionaries with the following keyvals:
-           'cmd': command name (string),
-           'cpu_use_avg': average cpu usage (float),
-           'count': number of occurrences (int),
-           'top5_cpu_use': up to 5 highest cpu usages (descending list of floats)
-
-        Example of the top log:
-          PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
-         4102 chronos   12  -8 3454472 238300 118188 R  41.8   6.1   0:08.37 chrome
-          375 root       0 -20       0      0      0 S   5.9   0.0   0:00.17 kworker
-          617 syslog    20   0   25332   8372   7888 S   5.9   0.2   0:00.77 systemd
-
-          PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
-         5745 chronos   20   0 5438580 139328  67988 R 122.8   3.6   0:04.26 chrome
-          912 root     -51   0       0      0      0 S   2.0   0.0   0:01.04 irq/cro
-          121 root      20   0       0      0      0 S   1.0   0.0   0:00.45 spi5
-        """
-        all_data = ""
-        with open(self.top_log_file, encoding="utf-8") as f:
-            all_data = f.read()
-
-        if not all_data:
-            self._logger.LogOutput("WARNING: Top log file is empty.")
-            return []
-
-        top_line_regex = re.compile(
-            r"""
-        ^\s*(?P<pid>\d+)\s+         # Group 1: PID
-        \S+\s+\S+\s+-?\d+\s+        # Ignore: user, prio, nice
-        \d+\s+\d+\s+\d+\s+          # Ignore: virt/res/shared mem
-        \S+\s+                      # Ignore: state
-        (?P<cpu_use>\d+\.\d+)\s+    # Group 2: CPU usage
-        \d+\.\d+\s+\d+:\d+\.\d+\s+  # Ignore: mem usage, time
-        (?P<cmd>\S+)$               # Group 3: command
-        """,
-            re.VERBOSE,
-        )
-        # Page represents top log data per one measurement within time interval
-        # 'top_interval'.
-        # Pages separated by empty line.
-        pages = all_data.split("\n\n")
-        # Snapshots are structured representation of the pages.
-        snapshots = []
-        for page in pages:
-            if not page:
-                continue
-
-            # Snapshot list will contain all processes (command duplicates are
-            # allowed).
-            snapshot = []
-            for line in page.splitlines():
-                match = top_line_regex.match(line)
-                if match:
-                    # Top line is valid, collect data.
-                    process = {
-                        # NOTE: One command may be represented by multiple processes.
-                        "cmd": match.group("cmd"),
-                        "pid": match.group("pid"),
-                        "cpu_use": float(match.group("cpu_use")),
-                    }
-
-                    # Filter out processes with 0 CPU usage and top command.
-                    if process["cpu_use"] > 0 and process["cmd"] != "top":
-                        snapshot.append(process)
-
-            # If page contained meaningful data add snapshot to the list.
-            if snapshot:
-                snapshots.append(snapshot)
-
-        # Define threshold of CPU usage when Chrome is busy, i.e. benchmark is
-        # running.
-        # Ideally it should be 100% but it will be hardly reachable with 1 core.
-        # Statistics on DUT with 2-6 cores shows that chrome load of 100%, 95% and
-        # 90% equally occurs in 72-74% of all top log snapshots.
-        # Further decreasing of load threshold leads to a shifting percent of
-        # "high load" snapshots which might include snapshots when benchmark is
-        # not running.
-        # On 1-core DUT 90% chrome cpu load occurs in 55%, 95% in 33% and 100% in 2%
-        # of snapshots accordingly.
-        # Threshold of "high load" is reduced to 70% (from 90) when we switched to
-        # topstats per process. From experiment data the rest 20% are distributed
-        # among other chrome processes.
-        CHROME_HIGH_CPU_LOAD = 70
-        # Number of snapshots where chrome is heavily used.
-        high_load_snapshots = 0
-        # Total CPU use per process in ALL active snapshots.
-        cmd_total_cpu_use = collections.defaultdict(float)
-        # Top CPU usages per command.
-        cmd_top5_cpu_use = collections.defaultdict(list)
-        # List of Top Commands to be returned.
-        topcmds = []
-
-        for snapshot_processes in snapshots:
-            # CPU usage per command, per PID in one snapshot.
-            cmd_cpu_use_per_snapshot = collections.defaultdict(dict)
-            for process in snapshot_processes:
-                cmd = process["cmd"]
-                cpu_use = process["cpu_use"]
-                pid = process["pid"]
-                cmd_cpu_use_per_snapshot[cmd][pid] = cpu_use
-
-            # Chrome processes, pid: cpu_usage.
-            chrome_processes = cmd_cpu_use_per_snapshot.get("chrome", {})
-            chrome_cpu_use_list = chrome_processes.values()
-
-            if (
-                chrome_cpu_use_list
-                and max(chrome_cpu_use_list) > CHROME_HIGH_CPU_LOAD
-            ):
-                # CPU usage of any of the "chrome" processes exceeds "High load"
-                # threshold which means DUT is busy running a benchmark.
-                high_load_snapshots += 1
-                for cmd, cpu_use_per_pid in cmd_cpu_use_per_snapshot.items():
-                    for pid, cpu_use in cpu_use_per_pid.items():
-                        # Append PID to the name of the command.
-                        cmd_with_pid = cmd + "-" + pid
-                        cmd_total_cpu_use[cmd_with_pid] += cpu_use
-
-                        # Add cpu_use into command top cpu usages, sorted in descending
-                        # order.
-                        heapq.heappush(
-                            cmd_top5_cpu_use[cmd_with_pid], round(cpu_use, 1)
-                        )
-
-        for consumer, usage in sorted(
-            cmd_total_cpu_use.items(), key=lambda x: x[1], reverse=True
-        ):
-            # Iterate through commands by descending order of total CPU usage.
-            topcmd = {
-                "cmd": consumer,
-                "cpu_use_avg": usage / high_load_snapshots,
-                "count": len(cmd_top5_cpu_use[consumer]),
-                "top5_cpu_use": heapq.nlargest(5, cmd_top5_cpu_use[consumer]),
-            }
-            topcmds.append(topcmd)
-
-        return topcmds
-
-    def ProcessCpustatsResults(self):
-        """Given cpustats_log_file non-null parse cpu data from file.
-
-        Returns:
-          Dictionary of 'cpufreq', 'cputemp' where each
-          includes dictionary of parameter: [list_of_values]
-
-        Example of cpustats.log output.
-        ----------------------
-        /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_cur_freq 1512000
-        /sys/devices/system/cpu/cpu2/cpufreq/cpuinfo_cur_freq 2016000
-        little-cpu 41234
-        big-cpu 51234
-
-        If cores share the same policy their frequencies may always match
-        on some devices.
-        To make report concise we should eliminate redundancy in the output.
-        Function removes cpuN data if it duplicates data from other cores.
-        """
-
-        cpustats = {}
-        read_data = ""
-        with open(self.cpustats_log_file, encoding="utf-8") as f:
-            read_data = f.readlines()
-
-        if not read_data:
-            self._logger.LogOutput("WARNING: Cpustats output file is empty.")
-            return {}
-
-        cpufreq_regex = re.compile(r"^[/\S]+/(cpu\d+)/[/\S]+\s+(\d+)$")
-        cputemp_regex = re.compile(r"^([^/\s]+)\s+(\d+)$")
-
-        for st in read_data:
-            match = cpufreq_regex.match(st)
-            if match:
-                cpu = match.group(1)
-                # CPU frequency comes in kHz.
-                freq_khz = int(match.group(2))
-                freq_mhz = freq_khz / 1000
-                # cpufreq represents a dictionary with CPU frequency-related
-                # data from cpustats.log.
-                cpufreq = cpustats.setdefault("cpufreq", {})
-                cpu_n_freq = cpufreq.setdefault(cpu, [])
-                cpu_n_freq.append(freq_mhz)
-            else:
-                match = cputemp_regex.match(st)
-                if match:
-                    therm_type = match.group(1)
-                    # The value is int, uCelsius unit.
-                    temp_uc = float(match.group(2))
-                    # Round to XX.X float.
-                    temp_c = round(temp_uc / 1000, 1)
-                    # cputemp represents a dictionary with temperature measurements
-                    # from cpustats.log.
-                    cputemp = cpustats.setdefault("cputemp", {})
-                    therm_type = cputemp.setdefault(therm_type, [])
-                    therm_type.append(temp_c)
-
-        # Remove duplicate statistics from cpustats.
-        pruned_stats = {}
-        for cpukey, cpuparam in cpustats.items():
-            # Copy 'cpufreq' and 'cputemp'.
-            pruned_params = pruned_stats.setdefault(cpukey, {})
-            for paramkey, paramvalue in sorted(cpuparam.items()):
-                # paramvalue is list of all measured data.
-                if paramvalue not in pruned_params.values():
-                    pruned_params[paramkey] = paramvalue
-
-        return pruned_stats
-
-    def ProcessHistogramsResults(self):
-        # Open and parse the json results file generated by telemetry/test_that.
-        if not self.results_file:
-            raise IOError("No results file found.")
-        filename = self.results_file[0]
-        if not filename.endswith(".json"):
-            raise IOError(
-                "Attempt to call json on non-json file: %s" % filename
-            )
-        if not os.path.exists(filename):
-            raise IOError("%s does not exist" % filename)
-
-        keyvals = {}
-        with open(filename, encoding="utf-8") as f:
-            histograms = json.load(f)
-            value_map = {}
-            # Gets generic set values.
-            for obj in histograms:
-                if "type" in obj and obj["type"] == "GenericSet":
-                    value_map[obj["guid"]] = obj["values"]
-
-            for obj in histograms:
-                if "name" not in obj or "sampleValues" not in obj:
-                    continue
-                metric_name = obj["name"]
-                vals = obj["sampleValues"]
-                if isinstance(vals, list):
-                    # Remove None elements from the list
-                    vals = [val for val in vals if val is not None]
-                    if vals:
-                        result = float(sum(vals)) / len(vals)
-                    else:
-                        result = 0
-                else:
-                    result = vals
-                unit = obj["unit"]
-                diagnostics = obj["diagnostics"]
-                # for summaries of benchmarks
-                key = metric_name
-                if key not in keyvals:
-                    keyvals[key] = [[result], unit]
-                else:
-                    keyvals[key][0].append(result)
-                # TODO: do we need summaries of stories?
-                # for summaries of story tags
-                if "storyTags" in diagnostics:
-                    guid = diagnostics["storyTags"]
-                    if guid not in value_map:
-                        raise RuntimeError(
-                            "Unrecognized storyTags in %s " % (obj)
-                        )
-                    for story_tag in value_map[guid]:
-                        key = metric_name + "__" + story_tag
-                        if key not in keyvals:
-                            keyvals[key] = [[result], unit]
-                        else:
-                            keyvals[key][0].append(result)
-        # calculate summary
-        for key in keyvals:
-            vals = keyvals[key][0]
-            unit = keyvals[key][1]
-            result = float(sum(vals)) / len(vals)
-            keyvals[key] = [result, unit]
-        return keyvals
-
-    def ReadPidFromPerfData(self):
-        """Read PIDs from perf.data files.
-
-        Extract PID from perf.data if "perf record" was running per process,
-        i.e. with "-p <PID>" and no "-a".
-
-        Returns:
-          pids: list of PIDs.
-
-        Raises:
-          PerfDataReadError when perf.data header reading fails.
-        """
-        cmd = ["/usr/bin/perf", "report", "--header-only", "-i"]
-        pids = []
-
-        for perf_data_path in self.perf_data_files:
-            perf_data_path_in_chroot = misc.GetInsideChrootPath(
-                self.chromeos_root, perf_data_path
-            )
-            path_str = " ".join(cmd + [perf_data_path_in_chroot])
-            status, output, _ = self.ce.ChrootRunCommandWOutput(
-                self.chromeos_root, path_str
-            )
-            if status:
-                # Error of reading a perf.data profile is fatal.
-                raise PerfDataReadError(
-                    f"Failed to read perf.data profile: {path_str}"
-                )
-
-            # Pattern to search a line with "perf record" command line:
-            # # cmdline : /usr/bin/perf record -e instructions -p 123"
-            cmdline_regex = re.compile(
-                r"^\#\scmdline\s:\s+(?P<cmd>.*perf\s+record\s+.*)$"
-            )
-            # Pattern to search PID in a command line.
-            pid_regex = re.compile(r"^.*\s-p\s(?P<pid>\d+)\s*.*$")
-            for line in output.splitlines():
-                cmd_match = cmdline_regex.match(line)
-                if cmd_match:
-                    # Found a perf command line.
-                    cmdline = cmd_match.group("cmd")
-                    # '-a' is a system-wide mode argument.
-                    if "-a" not in cmdline.split():
-                        # It can be that perf was attached to PID and was still running in
-                        # system-wide mode.
-                        # We filter out this case here since it's not per-process.
-                        pid_match = pid_regex.match(cmdline)
-                        if pid_match:
-                            pids.append(pid_match.group("pid"))
-                    # Stop the search and move to the next perf.data file.
-                    break
-            else:
-                # cmdline wasn't found in the header. It's a fatal error.
-                raise PerfDataReadError(
-                    f"Perf command line is not found in {path_str}"
-                )
-        return pids
-
-    def VerifyPerfDataPID(self):
-        """Verify PIDs in per-process perf.data profiles.
-
-        Check that at list one top process is profiled if perf was running in
-        per-process mode.
-
-        Raises:
-          PidVerificationError if PID verification of per-process perf.data profiles
-          fail.
-        """
-        perf_data_pids = self.ReadPidFromPerfData()
-        if not perf_data_pids:
-            # In system-wide mode there are no PIDs.
-            self._logger.LogOutput("System-wide perf mode. Skip verification.")
-            return
-
-        # PIDs will be present only in per-process profiles.
-        # In this case we need to verify that profiles are collected on the
-        # hottest processes.
-        top_processes = [top_cmd["cmd"] for top_cmd in self.top_cmds]
-        # top_process structure: <cmd>-<pid>
-        top_pids = [top_process.split("-")[-1] for top_process in top_processes]
-        for top_pid in top_pids:
-            if top_pid in perf_data_pids:
-                self._logger.LogOutput(
-                    "PID verification passed! "
-                    f"Top process {top_pid} is profiled."
-                )
-                return
-        raise PidVerificationError(
-            f"top processes {top_processes} are missing in perf.data traces with"
-            f" PID: {perf_data_pids}."
-        )
-
-    def ProcessResults(self, use_cache=False):
-        # Note that this function doesn't know anything about whether there is a
-        # cache hit or miss. It should process results agnostic of the cache hit
-        # state.
-        if (
-            self.results_file
-            and self.suite == "telemetry_Crosperf"
-            and "histograms.json" in self.results_file[0]
-        ):
-            self.keyvals = self.ProcessHistogramsResults()
-        elif (
-            self.results_file
-            and self.suite != "telemetry_Crosperf"
-            and "results-chart.json" in self.results_file[0]
-        ):
-            self.keyvals = self.ProcessChartResults()
-        else:
-            if not use_cache:
-                print(
-                    "\n ** WARNING **: Had to use deprecated output-method to "
-                    "collect results.\n"
-                )
-            self.keyvals = self.GetKeyvals()
-        self.keyvals["retval"] = self.retval
-        # If we are in CWP approximation mode, we want to collect DSO samples
-        # for each perf.data file
-        if self.cwp_dso and self.retval == 0:
-            self.keyvals["samples"] = self.GetSamples()
-            # If the samples count collected from perf file is 0, we will treat
-            # it as a failed run.
-            if self.keyvals["samples"][0] == 0:
-                del self.keyvals["samples"]
-                self.keyvals["retval"] = 1
-        # Generate report from all perf.data files.
-        # Now parse all perf report files and include them in keyvals.
-        self.GatherPerfResults()
-
-        cpustats = {}
-        # Turbostat output has higher priority of processing.
-        if self.turbostat_log_file:
-            cpustats = self.ProcessTurbostatResults()
-        # Process cpustats output only if turbostat has no data.
-        if not cpustats and self.cpustats_log_file:
-            cpustats = self.ProcessCpustatsResults()
-        if self.top_log_file:
-            self.top_cmds = self.ProcessTopResults()
-        # Verify that PID in non system-wide perf.data and top_cmds are matching.
-        if self.perf_data_files and self.top_cmds:
-            self.VerifyPerfDataPID()
-        if self.wait_time_log_file:
-            with open(self.wait_time_log_file, encoding="utf-8") as f:
-                wait_time = f.readline().strip()
-                try:
-                    wait_time = float(wait_time)
-                except ValueError:
-                    raise ValueError("Wait time in log file is not a number.")
-            # This is for accumulating wait time for telemtry_Crosperf runs only,
-            # for test_that runs, please refer to suite_runner.
-            self.machine.AddCooldownWaitTime(wait_time)
-
-        for param_key, param in cpustats.items():
-            for param_type, param_values in param.items():
-                val_avg = sum(param_values) / len(param_values)
-                val_min = min(param_values)
-                val_max = max(param_values)
-                # Average data is always included.
-                self.keyvals["_".join([param_key, param_type, "avg"])] = val_avg
-                # Insert min/max results only if they deviate
-                # from average.
-                if val_min != val_avg:
-                    self.keyvals[
-                        "_".join([param_key, param_type, "min"])
-                    ] = val_min
-                if val_max != val_avg:
-                    self.keyvals[
-                        "_".join([param_key, param_type, "max"])
-                    ] = val_max
-
-    def GetChromeVersionFromCache(self, cache_dir):
-        # Read chrome_version from keys file, if present.
-        chrome_version = ""
-        keys_file = os.path.join(cache_dir, CACHE_KEYS_FILE)
-        if os.path.exists(keys_file):
-            with open(keys_file, "r", encoding="utf-8") as f:
-                lines = f.readlines()
-                for l in lines:
-                    if l.startswith("Google Chrome "):
-                        chrome_version = l
-                        if chrome_version.endswith("\n"):
-                            chrome_version = chrome_version[:-1]
-                        break
-        return chrome_version
-
-    def PopulateFromCacheDir(self, cache_dir, test, suite, cwp_dso):
-        self.test_name = test
-        self.suite = suite
-        self.cwp_dso = cwp_dso
-        # Read in everything from the cache directory.
-        with open(os.path.join(cache_dir, RESULTS_FILE), "rb") as f:
-            self.out = pickle.load(f)
-            self.err = pickle.load(f)
-            self.retval = pickle.load(f)
-
-        # Untar the tarball to a temporary directory
-        self.temp_dir = tempfile.mkdtemp(
-            dir=misc.GetOutsideChrootPath(self.chromeos_root, "/tmp")
-        )
-
-        command = "cd %s && tar xf %s" % (
-            self.temp_dir,
-            os.path.join(cache_dir, AUTOTEST_TARBALL),
-        )
-        ret = self.ce.RunCommand(command, print_to_console=False)
-        if ret:
-            raise RuntimeError("Could not untar cached tarball")
-        self.results_dir = self.temp_dir
-        self.results_file = self.GetDataMeasurementsFiles()
-        self.perf_data_files = self.GetPerfDataFiles()
-        self.perf_report_files = self.GetPerfReportFiles()
-        self.chrome_version = self.GetChromeVersionFromCache(cache_dir)
-        self.ProcessResults(use_cache=True)
-
-    def CleanUp(self, rm_chroot_tmp):
-        if (
-            rm_chroot_tmp
-            and self.results_dir
-            and self.results_dir != self.temp_dir
-        ):
-            dirname, basename = misc.GetRoot(self.results_dir)
-            if basename.find("test_that_results_") != -1:
-                command = "rm -rf %s" % self.results_dir
-            else:
-                command = "rm -rf %s" % dirname
-            self.ce.RunCommand(command)
-        if self.temp_dir:
-            command = "rm -rf %s" % self.temp_dir
-            self.ce.RunCommand(command)
-
-    def CreateTarball(self, results_dir, tarball):
-        if not results_dir.strip():
-            raise ValueError(
-                "Refusing to `tar` an empty results_dir: %r" % results_dir
-            )
-
-        ret = self.ce.RunCommand(
-            "cd %s && "
-            "tar "
-            "--exclude=var/spool "
-            "--exclude=var/log "
-            "-cjf %s ." % (results_dir, tarball)
-        )
-        if ret:
-            raise RuntimeError("Couldn't compress test output directory.")
-
-    def StoreToCacheDir(self, cache_dir, machine_manager, key_list):
-        # Create the dir if it doesn't exist.
-        temp_dir = tempfile.mkdtemp()
-
-        # Store to the temp directory.
-        with open(os.path.join(temp_dir, RESULTS_FILE), "wb") as f:
-            pickle.dump(self.out, f)
-            pickle.dump(self.err, f)
-            pickle.dump(self.retval, f)
-
-        if not test_flag.GetTestMode():
-            with open(
-                os.path.join(temp_dir, CACHE_KEYS_FILE), "w", encoding="utf-8"
-            ) as f:
-                f.write("%s\n" % self.label.name)
-                f.write("%s\n" % self.label.chrome_version)
-                f.write("%s\n" % self.machine.checksum_string)
-                for k in key_list:
-                    f.write(k)
-                    f.write("\n")
-
-        if self.results_dir:
-            tarball = os.path.join(temp_dir, AUTOTEST_TARBALL)
-            self.CreateTarball(self.results_dir, tarball)
-
-        # Store machine info.
-        # TODO(asharif): Make machine_manager a singleton, and don't pass it into
-        # this function.
-        with open(
-            os.path.join(temp_dir, MACHINE_FILE), "w", encoding="utf-8"
-        ) as f:
-            f.write(machine_manager.machine_checksum_string[self.label.name])
-
-        if os.path.exists(cache_dir):
-            command = f"rm -rf {cache_dir}"
-            self.ce.RunCommand(command)
-
-        parent_dir = os.path.dirname(cache_dir)
-        command = f"mkdir -p {parent_dir} && "
-        command += f"chmod g+x {temp_dir} && "
-        command += f"mv {temp_dir} {cache_dir}"
-        ret = self.ce.RunCommand(command)
-        if ret:
-            command = f"rm -rf {temp_dir}"
-            self.ce.RunCommand(command)
-            raise RuntimeError(
-                "Could not move dir %s to dir %s" % (temp_dir, cache_dir)
-            )
-
-    @classmethod
-    def CreateFromRun(
-        cls,
-        logger,
-        log_level,
-        label,
-        machine,
-        out,
-        err,
-        retval,
-        test,
-        suite="telemetry_Crosperf",
-        cwp_dso="",
-    ):
-        if suite == "telemetry":
-            result = TelemetryResult(logger, label, log_level, machine)
-        else:
-            result = cls(logger, label, log_level, machine)
-        result.PopulateFromRun(out, err, retval, test, suite, cwp_dso)
-        return result
-
-    @classmethod
-    def CreateFromCacheHit(
-        cls,
-        logger,
-        log_level,
-        label,
-        machine,
-        cache_dir,
-        test,
-        suite="telemetry_Crosperf",
-        cwp_dso="",
-    ):
-        if suite == "telemetry":
-            result = TelemetryResult(logger, label, log_level, machine)
-        else:
-            result = cls(logger, label, log_level, machine)
-        try:
-            result.PopulateFromCacheDir(cache_dir, test, suite, cwp_dso)
-
-        except RuntimeError as e:
-            logger.LogError("Exception while using cache: %s" % e)
-            return None
-        return result
-
-
-class TelemetryResult(Result):
-    """Class to hold the results of a single Telemetry run."""
-
-    def PopulateFromRun(self, out, err, retval, test, suite, cwp_dso):
-        self.out = out
-        self.err = err
-        self.retval = retval
-
-        self.ProcessResults()
-
-    # pylint: disable=arguments-differ
-    def ProcessResults(self):
-        # The output is:
-        # url,average_commit_time (ms),...
-        # www.google.com,33.4,21.2,...
-        # We need to convert to this format:
-        # {"www.google.com:average_commit_time (ms)": "33.4",
-        #  "www.google.com:...": "21.2"}
-        # Added note:  Occasionally the output comes back
-        # with "JSON.stringify(window.automation.GetResults())" on
-        # the first line, and then the rest of the output as
-        # described above.
-
-        lines = self.out.splitlines()
-        self.keyvals = {}
-
-        if lines:
-            if lines[0].startswith("JSON.stringify"):
-                lines = lines[1:]
-
-        if not lines:
-            return
-        labels = lines[0].split(",")
-        for line in lines[1:]:
-            fields = line.split(",")
-            if len(fields) != len(labels):
-                continue
-            for i in range(1, len(labels)):
-                key = "%s %s" % (fields[0], labels[i])
-                value = fields[i]
-                self.keyvals[key] = value
-        self.keyvals["retval"] = self.retval
-
-    def PopulateFromCacheDir(self, cache_dir, test, suite, cwp_dso):
-        self.test_name = test
-        self.suite = suite
-        self.cwp_dso = cwp_dso
-        with open(os.path.join(cache_dir, RESULTS_FILE), "rb") as f:
-            self.out = pickle.load(f)
-            self.err = pickle.load(f)
-            self.retval = pickle.load(f)
-
-        self.chrome_version = super(
-            TelemetryResult, self
-        ).GetChromeVersionFromCache(cache_dir)
-        self.ProcessResults()
-
-
-class CacheConditions(object):
-    """Various Cache condition values, for export."""
-
-    # Cache hit only if the result file exists.
-    CACHE_FILE_EXISTS = 0
-
-    # Cache hit if the checksum of cpuinfo and totalmem of
-    # the cached result and the new run match.
-    MACHINES_MATCH = 1
-
-    # Cache hit if the image checksum of the cached result and the new run match.
-    CHECKSUMS_MATCH = 2
-
-    # Cache hit only if the cached result was successful
-    RUN_SUCCEEDED = 3
-
-    # Never a cache hit.
-    FALSE = 4
-
-    # Cache hit if the image path matches the cached image path.
-    IMAGE_PATH_MATCH = 5
-
-    # Cache hit if the uuid of hard disk mataches the cached one
-
-    SAME_MACHINE_MATCH = 6
-
-
-class ResultsCache(object):
-    """Class to handle the cache for storing/retrieving test run results.
-
-    This class manages the key of the cached runs without worrying about what
-    is exactly stored (value). The value generation is handled by the Results
-    class.
-    """
-
-    CACHE_VERSION = 6
-
-    def __init__(self):
-        # Proper initialization happens in the Init function below.
-        self.chromeos_image = None
-        self.chromeos_root = None
-        self.test_name = None
-        self.iteration = None
-        self.test_args = None
-        self.profiler_args = None
-        self.board = None
-        self.cache_conditions = None
-        self.machine_manager = None
-        self.machine = None
-        self._logger = None
-        self.ce = None
-        self.label = None
-        self.share_cache = None
-        self.suite = None
-        self.log_level = None
-        self.show_all = None
-        self.run_local = None
-        self.cwp_dso = None
-
-    def Init(
-        self,
-        chromeos_image,
-        chromeos_root,
-        test_name,
-        iteration,
-        test_args,
-        profiler_args,
-        machine_manager,
-        machine,
-        board,
-        cache_conditions,
-        logger_to_use,
-        log_level,
-        label,
-        share_cache,
-        suite,
-        show_all_results,
-        run_local,
-        cwp_dso,
-    ):
-        self.chromeos_image = chromeos_image
-        self.chromeos_root = chromeos_root
-        self.test_name = test_name
-        self.iteration = iteration
-        self.test_args = test_args
-        self.profiler_args = profiler_args
-        self.board = board
-        self.cache_conditions = cache_conditions
-        self.machine_manager = machine_manager
-        self.machine = machine
-        self._logger = logger_to_use
-        self.ce = command_executer.GetCommandExecuter(
-            self._logger, log_level=log_level
-        )
-        self.label = label
-        self.share_cache = share_cache
-        self.suite = suite
-        self.log_level = log_level
-        self.show_all = show_all_results
-        self.run_local = run_local
-        self.cwp_dso = cwp_dso
-
-    def GetCacheDirForRead(self):
-        matching_dirs = []
-        for glob_path in self.FormCacheDir(self.GetCacheKeyList(True)):
-            matching_dirs += glob.glob(glob_path)
-
-        if matching_dirs:
-            # Cache file found.
-            return matching_dirs[0]
-        return None
-
-    def GetCacheDirForWrite(self, get_keylist=False):
-        cache_path = self.FormCacheDir(self.GetCacheKeyList(False))[0]
-        if get_keylist:
-            args_str = "%s_%s_%s" % (
-                self.test_args,
-                self.profiler_args,
-                self.run_local,
-            )
-            version, image = results_report.ParseChromeosImage(
-                self.label.chromeos_image
-            )
-            keylist = [
-                version,
-                image,
-                self.label.board,
-                self.machine.name,
-                self.test_name,
-                str(self.iteration),
-                args_str,
-            ]
-            return cache_path, keylist
-        return cache_path
-
-    def FormCacheDir(self, list_of_strings):
-        cache_key = " ".join(list_of_strings)
-        cache_dir = misc.GetFilenameFromString(cache_key)
-        if self.label.cache_dir:
-            cache_home = os.path.abspath(
-                os.path.expanduser(self.label.cache_dir)
-            )
-            cache_path = [os.path.join(cache_home, cache_dir)]
-        else:
-            cache_path = [os.path.join(SCRATCH_DIR, cache_dir)]
-
-        if self.share_cache:
-            for path in [x.strip() for x in self.share_cache.split(",")]:
-                if os.path.exists(path):
-                    cache_path.append(os.path.join(path, cache_dir))
-                else:
-                    self._logger.LogFatal(
-                        "Unable to find shared cache: %s" % path
-                    )
-
-        return cache_path
-
-    def GetCacheKeyList(self, read):
-        if read and CacheConditions.MACHINES_MATCH not in self.cache_conditions:
-            machine_checksum = "*"
-        else:
-            machine_checksum = self.machine_manager.machine_checksum[
-                self.label.name
-            ]
-        if (
-            read
-            and CacheConditions.CHECKSUMS_MATCH not in self.cache_conditions
-        ):
-            checksum = "*"
-        elif self.label.image_type == "trybot":
-            checksum = hashlib.md5(
-                self.label.chromeos_image.encode("utf-8")
-            ).hexdigest()
-        elif self.label.image_type == "official":
-            checksum = "*"
-        else:
-            checksum = ImageChecksummer().Checksum(self.label, self.log_level)
-
-        if (
-            read
-            and CacheConditions.IMAGE_PATH_MATCH not in self.cache_conditions
-        ):
-            image_path_checksum = "*"
-        else:
-            image_path_checksum = hashlib.md5(
-                self.chromeos_image.encode("utf-8")
-            ).hexdigest()
-
-        machine_id_checksum = ""
-        if (
-            read
-            and CacheConditions.SAME_MACHINE_MATCH not in self.cache_conditions
-        ):
-            machine_id_checksum = "*"
-        else:
-            if self.machine and self.machine.name in self.label.remote:
-                machine_id_checksum = self.machine.machine_id_checksum
-            else:
-                for machine in self.machine_manager.GetMachines(self.label):
-                    if machine.name == self.label.remote[0]:
-                        machine_id_checksum = machine.machine_id_checksum
-                        break
-
-        temp_test_args = "%s %s %s" % (
-            self.test_args,
-            self.profiler_args,
-            self.run_local,
-        )
-        test_args_checksum = hashlib.md5(
-            temp_test_args.encode("utf-8")
-        ).hexdigest()
-        return (
-            image_path_checksum,
-            self.test_name,
-            str(self.iteration),
-            test_args_checksum,
-            checksum,
-            machine_checksum,
-            machine_id_checksum,
-            str(self.CACHE_VERSION),
-        )
-
-    def ReadResult(self):
-        if CacheConditions.FALSE in self.cache_conditions:
-            cache_dir = self.GetCacheDirForWrite()
-            command = "rm -rf %s" % (cache_dir,)
-            self.ce.RunCommand(command)
-            return None
-        cache_dir = self.GetCacheDirForRead()
-
-        if not cache_dir:
-            return None
-
-        if not os.path.isdir(cache_dir):
-            return None
-
-        if self.log_level == "verbose":
-            self._logger.LogOutput(
-                "Trying to read from cache dir: %s" % cache_dir
-            )
-        result = Result.CreateFromCacheHit(
-            self._logger,
-            self.log_level,
-            self.label,
-            self.machine,
-            cache_dir,
-            self.test_name,
-            self.suite,
-            self.cwp_dso,
-        )
-        if not result:
-            return None
-
-        if (
-            result.retval == 0
-            or CacheConditions.RUN_SUCCEEDED not in self.cache_conditions
-        ):
-            return result
-
-        return None
-
-    def StoreResult(self, result):
-        cache_dir, keylist = self.GetCacheDirForWrite(get_keylist=True)
-        result.StoreToCacheDir(cache_dir, self.machine_manager, keylist)
-
-
-class MockResultsCache(ResultsCache):
-    """Class for mock testing, corresponding to ResultsCache class."""
-
-    # FIXME: pylint complains about this mock init method, we should probably
-    # replace all Mock classes in Crosperf with simple Mock.mock().
-    # pylint: disable=arguments-differ
-    def Init(self, *args):
-        pass
-
-    def ReadResult(self):
-        return None
-
-    def StoreResult(self, result):
-        pass
-
-
-class MockResult(Result):
-    """Class for mock testing, corresponding to Result class."""
-
-    def PopulateFromRun(self, out, err, retval, test, suite, cwp_dso):
-        self.out = out
-        self.err = err
-        self.retval = retval
diff --git a/crosperf/results_cache_unittest.py b/crosperf/results_cache_unittest.py
deleted file mode 100755
index 06a8b94d..00000000
--- a/crosperf/results_cache_unittest.py
+++ /dev/null
@@ -1,2362 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-# Copyright 2011 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Module of result cache unittest."""
-
-
-import io
-import os
-import pickle
-import re
-import shutil
-import tempfile
-import unittest
-import unittest.mock as mock
-
-from cros_utils import command_executer
-from cros_utils import logger
-from cros_utils import misc
-import image_checksummer
-from label import MockLabel
-import machine_manager
-from results_cache import CacheConditions
-from results_cache import PerfDataReadError
-from results_cache import PidVerificationError
-from results_cache import Result
-from results_cache import ResultsCache
-from results_cache import TelemetryResult
-import test_flag
-
-
-# The following hardcoded string has blocked words replaced, and thus
-# is not representative of a true crosperf output.
-# pylint: disable=line-too-long
-OUTPUT = """CMD (True): ./test_that.sh\
- --remote=172.17.128.241  --board=lumpy   LibCBench
-CMD (None): cd /usr/local/google/home/yunlian/gd/src/build/images/lumpy/latest/../../../../..; cros_sdk  -- ./in_chroot_cmd6X7Cxu.sh
-Identity added: /tmp/test_that.PO1234567/autotest_key (/tmp/test_that.PO1234567/autotest_key)
-INFO    : Using emerged autotests already installed at /build/lumpy/usr/local/autotest.
-
-INFO    : Running the following control files 1 times:
-INFO    :  * 'client/site_tests/platform_LibCBench/control'
-
-INFO    : Running client test client/site_tests/platform_LibCBench/control
-./server/autoserv -m 172.17.128.241 --ssh-port 22 -c client/site_tests/platform_LibCBench/control -r /tmp/test_that.PO1234567/platform_LibCBench --test-retry=0 --args
-ERROR:root:import statsd failed, no stats will be reported.
-14:20:22 INFO | Results placed in /tmp/test_that.PO1234567/platform_LibCBench
-14:20:22 INFO | Processing control file
-14:20:23 INFO | Starting main ssh connection '/usr/bin/ssh -a -x -N -o ControlMain=yes -o ControlPath=/tmp/_autotmp_VIIP67ssh-main/socket -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o ConnectTimeout=30 -o ServerAliveInterval=180 -o ServerAliveCountMax=3 -o ConnectionAttempts=4 -o Protocol=2 -l root -p 22 172.17.128.241'
-14:20:23 ERROR| [stderr] Warning: Permanently added '172.17.128.241' (RSA) to the list of known hosts.
-14:20:23 INFO | INFO\t----\t----\tkernel=3.8.11\tlocaltime=May 22 14:20:23\ttimestamp=1369257623
-14:20:23 INFO | Installing autotest on 172.17.128.241
-14:20:23 INFO | Using installation dir /usr/local/autotest
-14:20:23 WARNI| No job_repo_url for <remote host: 172.17.128.241>
-14:20:23 INFO | Could not install autotest using the packaging system: No repos to install an autotest client from. Trying other methods
-14:20:23 INFO | Installation of autotest completed
-14:20:24 WARNI| No job_repo_url for <remote host: 172.17.128.241>
-14:20:24 INFO | Executing /usr/local/autotest/bin/autotest /usr/local/autotest/control phase 0
-14:20:24 INFO | Entered autotestd_monitor.
-14:20:24 INFO | Finished launching tail subprocesses.
-14:20:24 INFO | Finished waiting on autotestd to start.
-14:20:26 INFO | START\t----\t----\ttimestamp=1369257625\tlocaltime=May 22 14:20:25
-14:20:26 INFO | \tSTART\tplatform_LibCBench\tplatform_LibCBench\ttimestamp=1369257625\tlocaltime=May 22 14:20:25
-14:20:30 INFO | \t\tGOOD\tplatform_LibCBench\tplatform_LibCBench\ttimestamp=1369257630\tlocaltime=May 22 14:20:30\tcompleted successfully
-14:20:30 INFO | \tEND GOOD\tplatform_LibCBench\tplatform_LibCBench\ttimestamp=1369257630\tlocaltime=May 22 14:20:30
-14:20:31 INFO | END GOOD\t----\t----\ttimestamp=1369257630\tlocaltime=May 22 14:20:30
-14:20:31 INFO | Got lock of exit_code_file.
-14:20:31 INFO | Released lock of exit_code_file and closed it.
-OUTPUT: ==============================
-OUTPUT: Current time: 2013-05-22 14:20:32.818831 Elapsed: 0:01:30 ETA: Unknown
-Done: 0% [                                                  ]
-OUTPUT: Thread Status:
-RUNNING:  1 ('ttt: LibCBench (1)' 0:01:21)
-Machine Status:
-Machine                        Thread     Lock Status                    Checksum
-172.17.128.241                 ttt: LibCBench (1) True RUNNING                   3ba9f2ecbb222f20887daea5583d86ba
-
-OUTPUT: ==============================
-14:20:33 INFO | Killing child processes.
-14:20:33 INFO | Client complete
-14:20:33 INFO | Finished processing control file
-14:20:33 INFO | Starting main ssh connection '/usr/bin/ssh -a -x -N -o ControlMain=yes -o ControlPath=/tmp/_autotmp_aVJUgmssh-main/socket -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o ConnectTimeout=30 -o ServerAliveInterval=180 -o ServerAliveCountMax=3 -o ConnectionAttempts=4 -o Protocol=2 -l root -p 22 172.17.128.241'
-14:20:33 ERROR| [stderr] Warning: Permanently added '172.17.128.241' (RSA) to the list of known hosts.
-
-INFO    : Test results:
--------------------------------------------------------------------
-platform_LibCBench                                      [  PASSED  ]
-platform_LibCBench/platform_LibCBench                   [  PASSED  ]
-platform_LibCBench/platform_LibCBench                     b_malloc_big1__0_                                     0.00375231466667
-platform_LibCBench/platform_LibCBench                     b_malloc_big2__0_                                     0.002951359
-platform_LibCBench/platform_LibCBench                     b_malloc_bubble__0_                                   0.015066374
-platform_LibCBench/platform_LibCBench                     b_malloc_sparse__0_                                   0.015053784
-platform_LibCBench/platform_LibCBench                     b_malloc_thread_local__0_                             0.01138439
-platform_LibCBench/platform_LibCBench                     b_malloc_thread_stress__0_                            0.0367894733333
-platform_LibCBench/platform_LibCBench                     b_malloc_tiny1__0_                                    0.000768474333333
-platform_LibCBench/platform_LibCBench                     b_malloc_tiny2__0_                                    0.000581407333333
-platform_LibCBench/platform_LibCBench                     b_pthread_create_serial1__0_                          0.0291785246667
-platform_LibCBench/platform_LibCBench                     b_pthread_createjoin_serial1__0_                      0.031907936
-platform_LibCBench/platform_LibCBench                     b_pthread_createjoin_serial2__0_                      0.043485347
-platform_LibCBench/platform_LibCBench                     b_pthread_uselesslock__0_                             0.0294113346667
-platform_LibCBench/platform_LibCBench                     b_regex_compile____a_b_c__d_b__                       0.00529833933333
-platform_LibCBench/platform_LibCBench                     b_regex_search____a_b_c__d_b__                        0.00165455066667
-platform_LibCBench/platform_LibCBench                     b_regex_search___a_25_b__                             0.0496191923333
-platform_LibCBench/platform_LibCBench                     b_stdio_putcgetc__0_                                  0.100005711667
-platform_LibCBench/platform_LibCBench                     b_stdio_putcgetc_unlocked__0_                         0.0371443833333
-platform_LibCBench/platform_LibCBench                     b_string_memset__0_                                   0.00275405066667
-platform_LibCBench/platform_LibCBench                     b_string_strchr__0_                                   0.00456903
-platform_LibCBench/platform_LibCBench                     b_string_strlen__0_                                   0.044893587
-platform_LibCBench/platform_LibCBench                     b_string_strstr___aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaac__ 0.118360778
-platform_LibCBench/platform_LibCBench                     b_string_strstr___aaaaaaaaaaaaaaaaaaaaaaaaac__        0.068957325
-platform_LibCBench/platform_LibCBench                     b_string_strstr___aaaaaaaaaaaaaacccccccccccc__        0.0135694476667
-platform_LibCBench/platform_LibCBench                     b_string_strstr___abcdefghijklmnopqrstuvwxyz__        0.0134553343333
-platform_LibCBench/platform_LibCBench                     b_string_strstr___azbycxdwevfugthsirjqkplomn__        0.0133123556667
-platform_LibCBench/platform_LibCBench                     b_utf8_bigbuf__0_                                     0.0473772253333
-platform_LibCBench/platform_LibCBench                     b_utf8_onebyone__0_                                   0.130938538333
--------------------------------------------------------------------
-Total PASS: 2/2 (100%)
-
-INFO    : Elapsed time: 0m16s
-"""
-
-error = """
-ERROR: Identity added: /tmp/test_that.Z4Ld/autotest_key (/tmp/test_that.Z4Ld/autotest_key)
-INFO    : Using emerged autotests already installed at /build/lumpy/usr/local/autotest.
-INFO    : Running the following control files 1 times:
-INFO    :  * 'client/site_tests/platform_LibCBench/control'
-INFO    : Running client test client/site_tests/platform_LibCBench/control
-INFO    : Test results:
-INFO    : Elapsed time: 0m18s
-"""
-
-keyvals = {
-    "": "PASS",
-    "b_stdio_putcgetc__0_": "0.100005711667",
-    "b_string_strstr___azbycxdwevfugthsirjqkplomn__": "0.0133123556667",
-    "b_malloc_thread_local__0_": "0.01138439",
-    "b_string_strlen__0_": "0.044893587",
-    "b_malloc_sparse__0_": "0.015053784",
-    "b_string_memset__0_": "0.00275405066667",
-    "platform_LibCBench": "PASS",
-    "b_pthread_uselesslock__0_": "0.0294113346667",
-    "b_string_strchr__0_": "0.00456903",
-    "b_pthread_create_serial1__0_": "0.0291785246667",
-    "b_string_strstr___aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaac__": "0.118360778",
-    "b_string_strstr___aaaaaaaaaaaaaacccccccccccc__": "0.0135694476667",
-    "b_pthread_createjoin_serial1__0_": "0.031907936",
-    "b_malloc_thread_stress__0_": "0.0367894733333",
-    "b_regex_search____a_b_c__d_b__": "0.00165455066667",
-    "b_malloc_bubble__0_": "0.015066374",
-    "b_malloc_big2__0_": "0.002951359",
-    "b_stdio_putcgetc_unlocked__0_": "0.0371443833333",
-    "b_pthread_createjoin_serial2__0_": "0.043485347",
-    "b_regex_search___a_25_b__": "0.0496191923333",
-    "b_utf8_bigbuf__0_": "0.0473772253333",
-    "b_malloc_big1__0_": "0.00375231466667",
-    "b_regex_compile____a_b_c__d_b__": "0.00529833933333",
-    "b_string_strstr___aaaaaaaaaaaaaaaaaaaaaaaaac__": "0.068957325",
-    "b_malloc_tiny2__0_": "0.000581407333333",
-    "b_utf8_onebyone__0_": "0.130938538333",
-    "b_malloc_tiny1__0_": "0.000768474333333",
-    "b_string_strstr___abcdefghijklmnopqrstuvwxyz__": "0.0134553343333",
-}
-
-PERF_DATA_HEADER = """
-# ========
-# captured on    : Thu Jan 01 00:00:00 1980
-# header version : 1
-# data offset    : 536
-# data size      : 737678672
-# feat offset    : 737679208
-# hostname : localhost
-# os release : 5.4.61
-# perf version :
-# arch : aarch64
-# nrcpus online : 8
-# nrcpus avail : 8
-# total memory : 5911496 kB
-# cmdline : /usr/bin/perf record -e instructions -p {pid}
-# event : name = instructions, , id = ( 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193 ), type = 8, size = 112
-# event : name = placeholder:u, , id = ( 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204 ), type = 1, size = 112, config = 0x9
-# CPU_TOPOLOGY info available, use -I to display
-# pmu mappings: software = 1, uprobe = 6, cs_etm = 8, breakpoint = 5, tracepoint = 2, armv8_pmuv3 = 7
-# contains AUX area data (e.g. instruction trace)
-# time of first sample : 0.000000
-# time of last sample : 0.000000
-# sample duration :      0.000 ms
-# missing features: TRACING_DATA CPUDESC CPUID NUMA_TOPOLOGY BRANCH_STACK GROUP_DESC STAT CACHE MEM_TOPOLOGY CLOCKID DIR_FORMAT
-# ========
-#
-"""
-
-TURBOSTAT_LOG_OUTPUT = """CPU     Avg_MHz Busy%   Bzy_MHz TSC_MHz IRQ     CoreTmp
--       329     12.13   2723    2393    10975   77
-0       336     12.41   2715    2393    6328    77
-2       323     11.86   2731    2393    4647    69
-CPU     Avg_MHz Busy%   Bzy_MHz TSC_MHz IRQ     CoreTmp
--       1940    67.46   2884    2393    39920   83
-0       1827    63.70   2877    2393    21184   83
-2       2053    71.22   2891    2393    18736   67
-CPU     Avg_MHz Busy%   Bzy_MHz TSC_MHz IRQ     CoreTmp
--       1927    66.02   2927    2393    48946   84
-0       1880    64.47   2925    2393    24457   84
-2       1973    67.57   2928    2393    24489   69
-CPU     Avg_MHz Busy%   Bzy_MHz TSC_MHz IRQ     CoreTmp
--       1899    64.84   2937    2393    42540   72
-0       2135    72.82   2940    2393    23615   65
-2       1663    56.85   2934    2393    18925   72
-CPU     Avg_MHz Busy%   Bzy_MHz TSC_MHz IRQ     CoreTmp
--       1908    65.24   2932    2393    43172   75
-0       1876    64.25   2928    2393    20743   75
-2       1939    66.24   2936    2393    22429   69
-CPU     Avg_MHz Busy%   Bzy_MHz TSC_MHz IRQ     CoreTmp
--       1553    53.12   2933    2393    35488   46
-0       1484    50.80   2929    2393    18246   46
-2       1623    55.44   2936    2393    17242   45
-CPU     Avg_MHz Busy%   Bzy_MHz TSC_MHz IRQ     CoreTmp
--       843     29.83   2832    2393    28161   47
-0       827     29.35   2826    2393    16093   47
-2       858     30.31   2838    2393    12068   46
-"""
-TURBOSTAT_DATA = {
-    "cpufreq": {"all": [2723, 2884, 2927, 2937, 2932, 2933, 2832]},
-    "cputemp": {"all": [77, 83, 84, 72, 75, 46, 47]},
-}
-
-TOP_LOG = """
-  PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
- 4102 chronos   12  -8 3454472 238300 118188 R  41.8   6.1   0:08.37 chrome
- 4204 chronos   12  -8 2492716 205728 179016 S  11.8   5.3   0:03.89 chrome
- 4890 root      20   0    3396   2064   1596 R  11.8   0.1   0:00.03 top
-  375 root       0 -20       0      0      0 S   5.9   0.0   0:00.17 kworker/u13
-  617 syslog    20   0   25332   8372   7888 S   5.9   0.2   0:00.77 sys-journal
-
-  PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
- 5745 chronos   20   0 5438580 139328  67988 R 122.8   3.6   0:04.26 chrome
-  912 root     -51   0       0      0      0 S   2.0   0.0   0:01.04 irq/cros-ec
-  121 root      20   0       0      0      0 S   1.0   0.0   0:00.45 spi5
- 4811 root      20   0    6808   4084   3492 S   1.0   0.1   0:00.02 sshd
- 4890 root      20   0    3364   2148   1596 R   1.0   0.1   0:00.36 top
- 5205 chronos   12  -8 3673780 240928 130864 S   1.0   6.2   0:07.30 chrome
-
-
-  PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
- 5745 chronos   20   0 5434484 139432  63892 R 107.9   3.6   0:05.35 chrome
- 5713 chronos   20   0 5178652 103120  50372 S  17.8   2.6   0:01.13 chrome
-    7 root      20   0       0      0      0 S   1.0   0.0   0:00.73 rcu_preempt
-  855 root      20   0       0      0      0 S   1.0   0.0   0:00.01 kworker/4:2
-"""
-TOP_DATA = [
-    {
-        "cmd": "chrome-5745",
-        "cpu_use_avg": 115.35,
-        "count": 2,
-        "top5_cpu_use": [122.8, 107.9],
-    },
-    {
-        "cmd": "chrome-5713",
-        "cpu_use_avg": 8.9,
-        "count": 1,
-        "top5_cpu_use": [17.8],
-    },
-    {
-        "cmd": "irq/cros-ec-912",
-        "cpu_use_avg": 1.0,
-        "count": 1,
-        "top5_cpu_use": [2.0],
-    },
-    {
-        "cmd": "chrome-5205",
-        "cpu_use_avg": 0.5,
-        "count": 1,
-        "top5_cpu_use": [1.0],
-    },
-    {
-        "cmd": "spi5-121",
-        "cpu_use_avg": 0.5,
-        "count": 1,
-        "top5_cpu_use": [1.0],
-    },
-    {
-        "cmd": "sshd-4811",
-        "cpu_use_avg": 0.5,
-        "count": 1,
-        "top5_cpu_use": [1.0],
-    },
-    {
-        "cmd": "rcu_preempt-7",
-        "cpu_use_avg": 0.5,
-        "count": 1,
-        "top5_cpu_use": [1.0],
-    },
-    {
-        "cmd": "kworker/4:2-855",
-        "cpu_use_avg": 0.5,
-        "count": 1,
-        "top5_cpu_use": [1.0],
-    },
-]
-TOP_OUTPUT = """             COMMAND     AVG CPU%  SEEN   HIGHEST 5
-              chrome   128.250000     6   [122.8, 107.9, 17.8, 5.0, 2.0]
-     irq/230-cros-ec     1.000000     1   [2.0]
-                sshd     0.500000     1   [1.0]
-     irq/231-cros-ec     0.500000     1   [1.0]
-                spi5     0.500000     1   [1.0]
-         rcu_preempt     0.500000     1   [1.0]
-         kworker/4:2     0.500000     1   [1.0]
-"""
-
-CPUSTATS_UNIQ_OUTPUT = """
-/sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_cur_freq 1512000
-/sys/devices/system/cpu/cpu1/cpufreq/cpuinfo_cur_freq 1512000
-/sys/devices/system/cpu/cpu3/cpufreq/cpuinfo_cur_freq 2016000
-soc-thermal 44444
-little-cpu 41234
-big-cpu 51234
-/sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_cur_freq 1500000
-/sys/devices/system/cpu/cpu1/cpufreq/cpuinfo_cur_freq 1600000
-/sys/devices/system/cpu/cpu3/cpufreq/cpuinfo_cur_freq 2012000
-soc-thermal 45456
-little-cpu 42555
-big-cpu 61724
-"""
-CPUSTATS_UNIQ_DATA = {
-    "cpufreq": {
-        "cpu0": [1512, 1500],
-        "cpu1": [1512, 1600],
-        "cpu3": [2016, 2012],
-    },
-    "cputemp": {
-        "soc-thermal": [44.4, 45.5],
-        "little-cpu": [41.2, 42.6],
-        "big-cpu": [51.2, 61.7],
-    },
-}
-CPUSTATS_DUPL_OUTPUT = """
-/sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_cur_freq 1512000
-/sys/devices/system/cpu/cpu1/cpufreq/cpuinfo_cur_freq 1512000
-/sys/devices/system/cpu/cpu2/cpufreq/cpuinfo_cur_freq 1512000
-/sys/devices/system/cpu/cpu3/cpufreq/cpuinfo_cur_freq 2016000
-/sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_cur_freq 1500000
-/sys/devices/system/cpu/cpu1/cpufreq/cpuinfo_cur_freq 1500000
-/sys/devices/system/cpu/cpu2/cpufreq/cpuinfo_cur_freq 1500000
-/sys/devices/system/cpu/cpu3/cpufreq/cpuinfo_cur_freq 2016000
-/sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_cur_freq 1614000
-/sys/devices/system/cpu/cpu1/cpufreq/cpuinfo_cur_freq 1614000
-/sys/devices/system/cpu/cpu2/cpufreq/cpuinfo_cur_freq 1614000
-/sys/devices/system/cpu/cpu3/cpufreq/cpuinfo_cur_freq 1982000
-"""
-CPUSTATS_DUPL_DATA = {
-    "cpufreq": {"cpu0": [1512, 1500, 1614], "cpu3": [2016, 2016, 1982]},
-}
-
-TMP_DIR1 = "/tmp/tmpAbcXyz"
-
-HISTOGRAMSET = """
-[
-    {
-        "values": [
-            "cache_temperature_cold",
-            "typical",
-            "cache_temperature:cold"
-        ],
-        "guid": "db6d463b-7c07-4873-b839-db0652ccb97e",
-        "type": "GenericSet"
-    },
-    {
-        "values": [
-            "cache_temperature_warm",
-            "typical",
-            "cache_temperature:warm"
-        ],
-        "guid": "a270eb9d-3bb0-472a-951d-74ac3398b718",
-        "type": "GenericSet"
-    },
-    {
-        "sampleValues": [
-            1111.672
-        ],
-        "name": "timeToFirstContentfulPaint",
-        "diagnostics": {
-            "storyTags": "a270eb9d-3bb0-472a-951d-74ac3398b718"
-        },
-        "unit": "ms_smallerIsBetter"
-    },
-    {
-        "sampleValues": [
-            1146.459
-        ],
-        "name": "timeToFirstContentfulPaint",
-        "diagnostics": {
-            "storyTags": "db6d463b-7c07-4873-b839-db0652ccb97e"
-        },
-        "unit": "ms_smallerIsBetter"
-    },
-    {
-        "sampleValues": [
-            888.328
-        ],
-        "name": "timeToFirstContentfulPaint",
-        "diagnostics": {
-            "storyTags": "a270eb9d-3bb0-472a-951d-74ac3398b718"
-        },
-        "unit": "ms_smallerIsBetter"
-    },
-    {
-        "sampleValues": [
-            853.541
-        ],
-        "name": "timeToFirstContentfulPaint",
-        "diagnostics": {
-            "storyTags": "db6d463b-7c07-4873-b839-db0652ccb97e"
-        },
-        "unit": "ms_smallerIsBetter"
-    },
-    {
-        "sampleValues": [
-            400.000
-        ],
-        "name": "timeToFirstContentfulPaint",
-        "diagnostics": {
-            "storyTags": "a270eb9d-3bb0-472a-951d-74ac3398b718"
-        },
-        "unit": "ms_smallerIsBetter"
-    }
-
-]
-"""
-
-# pylint: enable=line-too-long
-
-
-class MockResult(Result):
-    """Mock result class."""
-
-    def __init__(self, mylogger, label, logging_level, machine):
-        super(MockResult, self).__init__(
-            mylogger, label, logging_level, machine
-        )
-
-    def FindFilesInResultsDir(self, find_args):
-        return ""
-
-    # pylint: disable=arguments-differ
-    def GetKeyvals(self, temp=False):
-        if temp:
-            pass
-        return keyvals
-
-
-class RegexMatcher:
-    """A regex matcher, for passing to mocks."""
-
-    def __init__(self, regex):
-        self._regex = re.compile(regex)
-
-    def __eq__(self, string):
-        return self._regex.search(string) is not None
-
-
-class ResultTest(unittest.TestCase):
-    """Result test class."""
-
-    def __init__(self, *args, **kwargs):
-        super(ResultTest, self).__init__(*args, **kwargs)
-        self.callFakeProcessResults = False
-        self.fakeCacheReturnResult = None
-        self.callGetResultsDir = False
-        self.callProcessResults = False
-        self.callGetPerfReportFiles = False
-        self.kv_dict = None
-        self.tmpdir = ""
-        self.callGetNewKeyvals = False
-        self.callGetResultsFile = False
-        self.callGetPerfDataFiles = False
-        self.callGetTurbostatFile = False
-        self.callGetCpustatsFile = False
-        self.callGetTopFile = False
-        self.callGetCpuinfoFile = False
-        self.callGetWaitTimeFile = False
-        self.args = None
-        self.callGatherPerfResults = False
-        self.mock_logger = mock.Mock(spec=logger.Logger)
-        self.mock_cmd_exec = mock.Mock(spec=command_executer.CommandExecuter)
-        self.mock_label = MockLabel(
-            "mock_label",
-            "build",
-            "chromeos_image",
-            "autotest_dir",
-            "debug_dir",
-            "/tmp",
-            "lumpy",
-            "remote",
-            "image_args",
-            "cache_dir",
-            "average",
-            "gcc",
-            False,
-            None,
-        )
-
-    @mock.patch.object(os.path, "exists")
-    def testCreateFromRun(self, mock_path_exists):
-        mock_path_exists.side_effect = lambda x: x != "/etc/cros_chroot_version"
-        result = MockResult.CreateFromRun(
-            logger.GetLogger(),
-            "average",
-            self.mock_label,
-            "remote1",
-            OUTPUT,
-            error,
-            0,
-            True,
-        )
-        self.assertEqual(result.keyvals, keyvals)
-        self.assertEqual(
-            result.chroot_results_dir,
-            "/tmp/test_that.PO1234567/platform_LibCBench",
-        )
-        self.assertEqual(
-            result.results_dir,
-            RegexMatcher("/tmp/.*tmp/test_that.PO1234567/platform_LibCBench"),
-        )
-        self.assertEqual(result.retval, 0)
-
-    def setUp(self):
-        self.result = Result(
-            self.mock_logger, self.mock_label, "average", self.mock_cmd_exec
-        )
-        self.result.chromeos_root = "/tmp/chromeos"
-        self.orig_exists = os.path.exists
-
-    @mock.patch.object(os.path, "isdir")
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommand")
-    @mock.patch.object(command_executer.CommandExecuter, "CopyFiles")
-    def test_copy_files_to(self, mock_copyfiles, mock_runcmd, mock_isdir):
-        files = ["src_file_1", "src_file_2", "src_file_3"]
-        dest_dir = "/tmp/test"
-        self.mock_cmd_exec.RunCommand = mock_runcmd
-        self.mock_cmd_exec.CopyFiles = mock_copyfiles
-
-        mock_copyfiles.return_value = 0
-
-        # test 1. dest_dir exists; CopyFiles returns 0.
-        mock_isdir.return_value = True
-        self.result.CopyFilesTo(dest_dir, files)
-        self.assertEqual(mock_runcmd.call_count, 0)
-        self.assertEqual(mock_copyfiles.call_count, 3)
-        first_args = mock_copyfiles.call_args_list[0][0]
-        second_args = mock_copyfiles.call_args_list[1][0]
-        third_args = mock_copyfiles.call_args_list[2][0]
-        self.assertEqual(first_args, ("src_file_1", "/tmp/test/src_file_1.0"))
-        self.assertEqual(second_args, ("src_file_2", "/tmp/test/src_file_2.1"))
-        self.assertEqual(third_args, ("src_file_3", "/tmp/test/src_file_3.2"))
-
-        mock_runcmd.reset_mock()
-        mock_copyfiles.reset_mock()
-        # test 2. dest_dir does not exist; CopyFiles returns 0.
-        mock_isdir.return_value = False
-        self.result.CopyFilesTo(dest_dir, files)
-        self.assertEqual(mock_runcmd.call_count, 3)
-        self.assertEqual(mock_copyfiles.call_count, 3)
-        self.assertEqual(
-            mock_runcmd.call_args_list[0], mock_runcmd.call_args_list[1]
-        )
-        self.assertEqual(
-            mock_runcmd.call_args_list[0], mock_runcmd.call_args_list[2]
-        )
-        self.assertEqual(
-            mock_runcmd.call_args_list[0][0], ("mkdir -p /tmp/test",)
-        )
-
-        # test 3. CopyFiles returns 1 (fails).
-        mock_copyfiles.return_value = 1
-        self.assertRaises(Exception, self.result.CopyFilesTo, dest_dir, files)
-
-    @mock.patch.object(Result, "CopyFilesTo")
-    def test_copy_results_to(self, mockCopyFilesTo):
-        results_file = [
-            "/tmp/result.json.0",
-            "/tmp/result.json.1",
-            "/tmp/result.json.2",
-        ]
-        perf_data_files = [
-            "/tmp/perf.data.0",
-            "/tmp/perf.data.1",
-            "/tmp/perf.data.2",
-        ]
-        perf_report_files = [
-            "/tmp/perf.report.0",
-            "/tmp/perf.report.1",
-            "/tmp/perf.report.2",
-        ]
-
-        self.result.results_file = results_file
-        self.result.perf_data_files = perf_data_files
-        self.result.perf_report_files = perf_report_files
-
-        self.result.CopyFilesTo = mockCopyFilesTo
-        self.result.CopyResultsTo("/tmp/results/")
-        self.assertEqual(mockCopyFilesTo.call_count, 3)
-        self.assertEqual(len(mockCopyFilesTo.call_args_list), 3)
-        self.assertEqual(
-            mockCopyFilesTo.call_args_list[0][0],
-            ("/tmp/results/", results_file),
-        )
-        self.assertEqual(
-            mockCopyFilesTo.call_args_list[1][0],
-            ("/tmp/results/", perf_data_files),
-        )
-        self.assertEqual(
-            mockCopyFilesTo.call_args_list[2][0],
-            ("/tmp/results/", perf_report_files),
-        )
-
-    def test_get_new_keyvals(self):
-        kv_dict = {}
-
-        def FakeGetDataMeasurementsFiles():
-            filename = os.path.join(os.getcwd(), "unittest_keyval_file.txt")
-            return [filename]
-
-        self.result.GetDataMeasurementsFiles = FakeGetDataMeasurementsFiles
-        kv_dict2, udict = self.result.GetNewKeyvals(kv_dict)
-        self.assertEqual(
-            kv_dict2,
-            {
-                "Box2D__Box2D": 4775,
-                "Mandreel__Mandreel": 6620,
-                "Gameboy__Gameboy": 9901,
-                "Crypto__Crypto": 8737,
-                "telemetry_page_measurement_results__num_errored": 0,
-                "telemetry_page_measurement_results__num_failed": 0,
-                "PdfJS__PdfJS": 6455,
-                "Total__Score": 7918,
-                "EarleyBoyer__EarleyBoyer": 14340,
-                "MandreelLatency__MandreelLatency": 5188,
-                "CodeLoad__CodeLoad": 6271,
-                "DeltaBlue__DeltaBlue": 14401,
-                "Typescript__Typescript": 9815,
-                "SplayLatency__SplayLatency": 7653,
-                "zlib__zlib": 16094,
-                "Richards__Richards": 10358,
-                "RegExp__RegExp": 1765,
-                "NavierStokes__NavierStokes": 9815,
-                "Splay__Splay": 4425,
-                "RayTrace__RayTrace": 16600,
-            },
-        )
-        self.assertEqual(
-            udict,
-            {
-                "Box2D__Box2D": "score",
-                "Mandreel__Mandreel": "score",
-                "Gameboy__Gameboy": "score",
-                "Crypto__Crypto": "score",
-                "telemetry_page_measurement_results__num_errored": "count",
-                "telemetry_page_measurement_results__num_failed": "count",
-                "PdfJS__PdfJS": "score",
-                "Total__Score": "score",
-                "EarleyBoyer__EarleyBoyer": "score",
-                "MandreelLatency__MandreelLatency": "score",
-                "CodeLoad__CodeLoad": "score",
-                "DeltaBlue__DeltaBlue": "score",
-                "Typescript__Typescript": "score",
-                "SplayLatency__SplayLatency": "score",
-                "zlib__zlib": "score",
-                "Richards__Richards": "score",
-                "RegExp__RegExp": "score",
-                "NavierStokes__NavierStokes": "score",
-                "Splay__Splay": "score",
-                "RayTrace__RayTrace": "score",
-            },
-        )
-
-    def test_append_telemetry_units(self):
-        kv_dict = {
-            "Box2D__Box2D": 4775,
-            "Mandreel__Mandreel": 6620,
-            "Gameboy__Gameboy": 9901,
-            "Crypto__Crypto": 8737,
-            "PdfJS__PdfJS": 6455,
-            "Total__Score": 7918,
-            "EarleyBoyer__EarleyBoyer": 14340,
-            "MandreelLatency__MandreelLatency": 5188,
-            "CodeLoad__CodeLoad": 6271,
-            "DeltaBlue__DeltaBlue": 14401,
-            "Typescript__Typescript": 9815,
-            "SplayLatency__SplayLatency": 7653,
-            "zlib__zlib": 16094,
-            "Richards__Richards": 10358,
-            "RegExp__RegExp": 1765,
-            "NavierStokes__NavierStokes": 9815,
-            "Splay__Splay": 4425,
-            "RayTrace__RayTrace": 16600,
-        }
-        units_dict = {
-            "Box2D__Box2D": "score",
-            "Mandreel__Mandreel": "score",
-            "Gameboy__Gameboy": "score",
-            "Crypto__Crypto": "score",
-            "PdfJS__PdfJS": "score",
-            "Total__Score": "score",
-            "EarleyBoyer__EarleyBoyer": "score",
-            "MandreelLatency__MandreelLatency": "score",
-            "CodeLoad__CodeLoad": "score",
-            "DeltaBlue__DeltaBlue": "score",
-            "Typescript__Typescript": "score",
-            "SplayLatency__SplayLatency": "score",
-            "zlib__zlib": "score",
-            "Richards__Richards": "score",
-            "RegExp__RegExp": "score",
-            "NavierStokes__NavierStokes": "score",
-            "Splay__Splay": "score",
-            "RayTrace__RayTrace": "score",
-        }
-
-        results_dict = self.result.AppendTelemetryUnits(kv_dict, units_dict)
-        self.assertEqual(
-            results_dict,
-            {
-                "Box2D__Box2D": [4775, "score"],
-                "Splay__Splay": [4425, "score"],
-                "Gameboy__Gameboy": [9901, "score"],
-                "Crypto__Crypto": [8737, "score"],
-                "PdfJS__PdfJS": [6455, "score"],
-                "Total__Score": [7918, "score"],
-                "EarleyBoyer__EarleyBoyer": [14340, "score"],
-                "MandreelLatency__MandreelLatency": [5188, "score"],
-                "DeltaBlue__DeltaBlue": [14401, "score"],
-                "SplayLatency__SplayLatency": [7653, "score"],
-                "Mandreel__Mandreel": [6620, "score"],
-                "Richards__Richards": [10358, "score"],
-                "zlib__zlib": [16094, "score"],
-                "CodeLoad__CodeLoad": [6271, "score"],
-                "Typescript__Typescript": [9815, "score"],
-                "RegExp__RegExp": [1765, "score"],
-                "RayTrace__RayTrace": [16600, "score"],
-                "NavierStokes__NavierStokes": [9815, "score"],
-            },
-        )
-
-    @mock.patch.object(misc, "GetInsideChrootPath")
-    @mock.patch.object(tempfile, "mkdtemp")
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommand")
-    @mock.patch.object(
-        command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-    )
-    def test_get_keyvals(
-        self, mock_chrootruncmd, mock_runcmd, mock_mkdtemp, mock_getpath
-    ):
-        self.kv_dict = {}
-        self.callGetNewKeyvals = False
-
-        def reset():
-            self.kv_dict = {}
-            self.callGetNewKeyvals = False
-            mock_chrootruncmd.reset_mock()
-            mock_runcmd.reset_mock()
-            mock_mkdtemp.reset_mock()
-            mock_getpath.reset_mock()
-
-        def FakeGetNewKeyvals(kv_dict):
-            self.kv_dict = kv_dict
-            self.callGetNewKeyvals = True
-            return_kvdict = {"first_time": 680, "Total": 10}
-            return_udict = {"first_time": "ms", "Total": "score"}
-            return return_kvdict, return_udict
-
-        mock_mkdtemp.return_value = TMP_DIR1
-        mock_chrootruncmd.return_value = [
-            "",
-            ("%s,PASS\n%s/telemetry_Crosperf,PASS\n") % (TMP_DIR1, TMP_DIR1),
-            "",
-        ]
-        mock_getpath.return_value = TMP_DIR1
-        self.result.ce.ChrootRunCommandWOutput = mock_chrootruncmd
-        self.result.ce.RunCommand = mock_runcmd
-        self.result.GetNewKeyvals = FakeGetNewKeyvals
-        self.result.suite = "telemetry_Crosperf"
-        self.result.results_dir = "/tmp/test_that_resultsNmq"
-
-        # Test 1. no self.temp_dir.
-        res = self.result.GetKeyvals()
-        self.assertTrue(self.callGetNewKeyvals)
-        self.assertEqual(
-            self.kv_dict, {"": "PASS", "telemetry_Crosperf": "PASS"}
-        )
-        self.assertEqual(mock_runcmd.call_count, 1)
-        self.assertEqual(
-            mock_runcmd.call_args_list[0][0],
-            ("cp -r /tmp/test_that_resultsNmq/* %s" % TMP_DIR1,),
-        )
-        self.assertEqual(mock_chrootruncmd.call_count, 1)
-        self.assertEqual(
-            mock_chrootruncmd.call_args_list[0][0],
-            (
-                self.result.chromeos_root,
-                ("./generate_test_report --no-color --csv %s") % TMP_DIR1,
-            ),
-        )
-        self.assertEqual(mock_getpath.call_count, 1)
-        self.assertEqual(mock_mkdtemp.call_count, 1)
-        self.assertEqual(
-            res, {"Total": [10, "score"], "first_time": [680, "ms"]}
-        )
-
-        # Test 2. self.temp_dir
-        reset()
-        mock_chrootruncmd.return_value = [
-            "",
-            (
-                "/tmp/tmpJCajRG,PASS\n/tmp/tmpJCajRG/"
-                "telemetry_Crosperf,PASS\n"
-            ),
-            "",
-        ]
-        mock_getpath.return_value = "/tmp/tmpJCajRG"
-        self.result.temp_dir = "/tmp/tmpJCajRG"
-        res = self.result.GetKeyvals()
-        self.assertEqual(mock_runcmd.call_count, 0)
-        self.assertEqual(mock_mkdtemp.call_count, 0)
-        self.assertEqual(mock_chrootruncmd.call_count, 1)
-        self.assertTrue(self.callGetNewKeyvals)
-        self.assertEqual(
-            self.kv_dict, {"": "PASS", "telemetry_Crosperf": "PASS"}
-        )
-        self.assertEqual(
-            res, {"Total": [10, "score"], "first_time": [680, "ms"]}
-        )
-
-        # Test 3. suite != telemetry_Crosperf.  Normally this would be for
-        # running non-Telemetry autotests, such as BootPerfServer.  In this test
-        # case, the keyvals we have set up were returned from a Telemetry test
-        # run; so this pass is basically testing that we don't append the units
-        # to the test results (which we do for Telemetry autotest runs).
-        reset()
-        self.result.suite = ""
-        res = self.result.GetKeyvals()
-        self.assertEqual(res, {"Total": 10, "first_time": 680})
-
-    @mock.patch.object(misc, "GetInsideChrootPath")
-    @mock.patch.object(
-        command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-    )
-    @mock.patch.object(os.path, "exists")
-    def test_get_samples(
-        self, mock_exists, mock_get_total_samples, mock_getpath
-    ):
-        self.result.perf_data_files = ["/tmp/results/perf.data"]
-        self.result.board = "samus"
-        self.result.cwp_dso = "kallsyms"
-        mock_getpath.return_value = "/usr/chromeos/chroot/tmp/results/perf.data"
-        mock_get_total_samples.return_value = [
-            "",
-            (
-                "45.42%        53721   chrome \n"
-                "10.01%        12345   [kernel.kallsyms] \n"
-                "1.42%         1234    ssh "
-            ),
-            "",
-        ]
-        mock_exists.return_value = True
-
-        # mock_open does not seem to support iteration.
-        # pylint: disable=line-too-long
-        content = """1.63%            66  dav1d-tile       chrome                [.] decode_coefs
-    1.48%            60  swapper          [kernel.kallsyms]     [k] intel_idle
-    1.16%            47  dav1d-tile       chrome                [.] decode_sb"""
-
-        with mock.patch("builtins.open", return_value=io.StringIO(content)):
-            samples = self.result.GetSamples()
-        self.assertEqual(samples, [12345 - 60, "samples"])
-
-    def test_get_results_dir(self):
-        self.result.out = ""
-        self.assertRaises(Exception, self.result.GetResultsDir)
-
-        self.result.out = OUTPUT
-        resdir = self.result.GetResultsDir()
-        self.assertEqual(resdir, "/tmp/test_that.PO1234567/platform_LibCBench")
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommandGeneric")
-    def test_find_files_in_results_dir(self, mock_runcmd):
-        self.result.results_dir = None
-        res = self.result.FindFilesInResultsDir("-name perf.data")
-        self.assertEqual(res, "")
-
-        self.result.ce.RunCommand = mock_runcmd
-        self.result.results_dir = "/tmp/test_results"
-        mock_runcmd.return_value = [0, "/tmp/test_results/perf.data", ""]
-        res = self.result.FindFilesInResultsDir("-name perf.data")
-        self.assertEqual(mock_runcmd.call_count, 1)
-        self.assertEqual(
-            mock_runcmd.call_args_list[0][0],
-            ("find /tmp/test_results -name perf.data",),
-        )
-        self.assertEqual(res, "/tmp/test_results/perf.data")
-
-        mock_runcmd.reset_mock()
-        mock_runcmd.return_value = [1, "", ""]
-        self.assertRaises(
-            Exception, self.result.FindFilesInResultsDir, "-name perf.data"
-        )
-
-    @mock.patch.object(Result, "FindFilesInResultsDir")
-    def test_get_perf_data_files(self, mock_findfiles):
-        self.args = None
-
-        mock_findfiles.return_value = "line1\nline1\n"
-        self.result.FindFilesInResultsDir = mock_findfiles
-        res = self.result.GetPerfDataFiles()
-        self.assertEqual(res, ["line1", "line1"])
-        self.assertEqual(
-            mock_findfiles.call_args_list[0][0], ("-name perf.data",)
-        )
-
-    def test_get_perf_report_files(self):
-        self.args = None
-
-        def FakeFindFiles(find_args):
-            self.args = find_args
-            return "line1\nline1\n"
-
-        self.result.FindFilesInResultsDir = FakeFindFiles
-        res = self.result.GetPerfReportFiles()
-        self.assertEqual(res, ["line1", "line1"])
-        self.assertEqual(self.args, "-name perf.data.report")
-
-    def test_get_data_measurement_files(self):
-        self.args = None
-
-        def FakeFindFiles(find_args):
-            self.args = find_args
-            return "line1\nline1\n"
-
-        self.result.FindFilesInResultsDir = FakeFindFiles
-        res = self.result.GetDataMeasurementsFiles()
-        self.assertEqual(res, ["line1", "line1"])
-        self.assertEqual(self.args, "-name perf_measurements")
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommandWOutput")
-    def test_get_turbostat_file_finds_single_log(self, mock_runcmd):
-        """Expected behavior when a single log file found."""
-        self.result.results_dir = "/tmp/test_results"
-        self.result.ce.RunCommandWOutput = mock_runcmd
-        mock_runcmd.return_value = (0, "some/long/path/turbostat.log", "")
-        found_single_log = self.result.GetTurbostatFile()
-        self.assertEqual(found_single_log, "some/long/path/turbostat.log")
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommandWOutput")
-    def test_get_turbostat_file_finds_multiple_logs(self, mock_runcmd):
-        """Error case when multiple files found."""
-        self.result.results_dir = "/tmp/test_results"
-        self.result.ce.RunCommandWOutput = mock_runcmd
-        mock_runcmd.return_value = (
-            0,
-            "some/long/path/turbostat.log\nturbostat.log",
-            "",
-        )
-        found_first_logs = self.result.GetTurbostatFile()
-        self.assertEqual(found_first_logs, "some/long/path/turbostat.log")
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommandWOutput")
-    def test_get_turbostat_file_finds_no_logs(self, mock_runcmd):
-        """Error case when no log file found."""
-        self.result.results_dir = "/tmp/test_results"
-        self.result.ce.RunCommandWOutput = mock_runcmd
-        mock_runcmd.return_value = (0, "", "")
-        found_no_logs = self.result.GetTurbostatFile()
-        self.assertEqual(found_no_logs, "")
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommandWOutput")
-    def test_get_turbostat_file_with_failing_find(self, mock_runcmd):
-        """Error case when file search returns an error."""
-        self.result.results_dir = "/tmp/test_results"
-        mock_runcmd.return_value = (-1, "", "error")
-        with self.assertRaises(RuntimeError):
-            self.result.GetTurbostatFile()
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommandWOutput")
-    def test_get_top_file_finds_single_log(self, mock_runcmd):
-        """Expected behavior when a single top log file found."""
-        self.result.results_dir = "/tmp/test_results"
-        self.result.ce.RunCommandWOutput = mock_runcmd
-        mock_runcmd.return_value = (0, "some/long/path/top.log", "")
-        found_single_log = self.result.GetTopFile()
-        self.assertEqual(found_single_log, "some/long/path/top.log")
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommandWOutput")
-    def test_get_top_file_finds_multiple_logs(self, mock_runcmd):
-        """The case when multiple top files found."""
-        self.result.results_dir = "/tmp/test_results"
-        self.result.ce.RunCommandWOutput = mock_runcmd
-        mock_runcmd.return_value = (0, "some/long/path/top.log\ntop.log", "")
-        found_first_logs = self.result.GetTopFile()
-        self.assertEqual(found_first_logs, "some/long/path/top.log")
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommandWOutput")
-    def test_get_top_file_finds_no_logs(self, mock_runcmd):
-        """Error case when no log file found."""
-        self.result.results_dir = "/tmp/test_results"
-        self.result.ce.RunCommandWOutput = mock_runcmd
-        mock_runcmd.return_value = (0, "", "")
-        found_no_logs = self.result.GetTopFile()
-        self.assertEqual(found_no_logs, "")
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommandWOutput")
-    def test_get_cpuinfo_file_finds_single_log(self, mock_runcmd):
-        """Expected behavior when a single cpuinfo file found."""
-        self.result.results_dir = "/tmp/test_results"
-        self.result.ce.RunCommandWOutput = mock_runcmd
-        mock_runcmd.return_value = (0, "some/long/path/cpuinfo.log", "")
-        found_single_log = self.result.GetCpuinfoFile()
-        self.assertEqual(found_single_log, "some/long/path/cpuinfo.log")
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommandWOutput")
-    def test_get_cpustats_file_finds_single_log(self, mock_runcmd):
-        """Expected behavior when a single log file found."""
-        self.result.results_dir = "/tmp/test_results"
-        self.result.ce.RunCommandWOutput = mock_runcmd
-        mock_runcmd.return_value = (0, "some/long/path/cpustats.log", "")
-        found_single_log = self.result.GetCpustatsFile()
-        self.assertEqual(found_single_log, "some/long/path/cpustats.log")
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommandWOutput")
-    def test_get_cpustats_file_finds_multiple_logs(self, mock_runcmd):
-        """The case when multiple files found."""
-        self.result.results_dir = "/tmp/test_results"
-        self.result.ce.RunCommandWOutput = mock_runcmd
-        mock_runcmd.return_value = (
-            0,
-            "some/long/path/cpustats.log\ncpustats.log",
-            "",
-        )
-        found_first_logs = self.result.GetCpustatsFile()
-        self.assertEqual(found_first_logs, "some/long/path/cpustats.log")
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommandWOutput")
-    def test_get_cpustats_file_finds_no_logs(self, mock_runcmd):
-        """Error case when no log file found."""
-        self.result.results_dir = "/tmp/test_results"
-        self.result.ce.RunCommandWOutput = mock_runcmd
-        mock_runcmd.return_value = (0, "", "")
-        found_no_logs = self.result.GetCpustatsFile()
-        self.assertEqual(found_no_logs, "")
-
-    def test_verify_perf_data_pid_ok(self):
-        """Verify perf PID which is present in TOP_DATA."""
-        self.result.top_cmds = TOP_DATA
-        # pid is present in TOP_DATA.
-        with mock.patch.object(
-            Result, "ReadPidFromPerfData", return_value=["5713"]
-        ):
-            self.result.VerifyPerfDataPID()
-
-    def test_verify_perf_data_pid_fail(self):
-        """Test perf PID missing in top raises the error."""
-        self.result.top_cmds = TOP_DATA
-        # pid is not in the list of top processes.
-        with mock.patch.object(
-            Result, "ReadPidFromPerfData", return_value=["9999"]
-        ):
-            with self.assertRaises(PidVerificationError):
-                self.result.VerifyPerfDataPID()
-
-    @mock.patch.object(
-        command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-    )
-    def test_read_pid_from_perf_data_ok(self, mock_runcmd):
-        """Test perf header parser, normal flow."""
-        self.result.ce.ChrootRunCommandWOutput = mock_runcmd
-        self.result.perf_data_files = [
-            "/tmp/chromeos/chroot/tmp/results/perf.data"
-        ]
-        exp_pid = "12345"
-        mock_runcmd.return_value = (0, PERF_DATA_HEADER.format(pid=exp_pid), "")
-        pids = self.result.ReadPidFromPerfData()
-        self.assertEqual(pids, [exp_pid])
-
-    @mock.patch.object(
-        command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-    )
-    def test_read_pid_from_perf_data_mult_profiles(self, mock_runcmd):
-        """Test multiple perf.data files with PID."""
-        self.result.ce.ChrootRunCommandWOutput = mock_runcmd
-        # self.result.chromeos_root = '/tmp/chromeos'
-        self.result.perf_data_files = [
-            "/tmp/chromeos/chroot/tmp/results/perf.data.0",
-            "/tmp/chromeos/chroot/tmp/results/perf.data.1",
-        ]
-        # There is '-p <pid>' in command line but it's still system-wide: '-a'.
-        cmd_line = "# cmdline : /usr/bin/perf record -e instructions -p {pid}"
-        exp_perf_pids = ["1111", "2222"]
-        mock_runcmd.side_effect = [
-            (0, cmd_line.format(pid=exp_perf_pids[0]), ""),
-            (0, cmd_line.format(pid=exp_perf_pids[1]), ""),
-        ]
-        pids = self.result.ReadPidFromPerfData()
-        self.assertEqual(pids, exp_perf_pids)
-
-    @mock.patch.object(
-        command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-    )
-    def test_read_pid_from_perf_data_no_pid(self, mock_runcmd):
-        """Test perf.data without PID."""
-        self.result.ce.ChrootRunCommandWOutput = mock_runcmd
-        self.result.perf_data_files = [
-            "/tmp/chromeos/chroot/tmp/results/perf.data"
-        ]
-        cmd_line = "# cmdline : /usr/bin/perf record -e instructions"
-        mock_runcmd.return_value = (0, cmd_line, "")
-        pids = self.result.ReadPidFromPerfData()
-        # pids is empty.
-        self.assertEqual(pids, [])
-
-    @mock.patch.object(
-        command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-    )
-    def test_read_pid_from_perf_data_system_wide(self, mock_runcmd):
-        """Test reading from system-wide profile with PID."""
-        self.result.ce.ChrootRunCommandWOutput = mock_runcmd
-        self.result.perf_data_files = [
-            "/tmp/chromeos/chroot/tmp/results/perf.data"
-        ]
-        # There is '-p <pid>' in command line but it's still system-wide: '-a'.
-        cmd_line = "# cmdline : /usr/bin/perf record -e instructions -a -p 1234"
-        mock_runcmd.return_value = (0, cmd_line, "")
-        pids = self.result.ReadPidFromPerfData()
-        # pids should be empty since it's not a per-process profiling.
-        self.assertEqual(pids, [])
-
-    @mock.patch.object(
-        command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-    )
-    def test_read_pid_from_perf_data_read_fail(self, mock_runcmd):
-        """Failure to read perf.data raises the error."""
-        self.result.ce.ChrootRunCommandWOutput = mock_runcmd
-        self.result.perf_data_files = [
-            "/tmp/chromeos/chroot/tmp/results/perf.data"
-        ]
-        # Error status of the profile read.
-        mock_runcmd.return_value = (1, "", "")
-        with self.assertRaises(PerfDataReadError):
-            self.result.ReadPidFromPerfData()
-
-    @mock.patch.object(
-        command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-    )
-    def test_read_pid_from_perf_data_fail(self, mock_runcmd):
-        """Failure to find cmdline in perf.data header raises the error."""
-        self.result.ce.ChrootRunCommandWOutput = mock_runcmd
-        self.result.perf_data_files = [
-            "/tmp/chromeos/chroot/tmp/results/perf.data"
-        ]
-        # Empty output.
-        mock_runcmd.return_value = (0, "", "")
-        with self.assertRaises(PerfDataReadError):
-            self.result.ReadPidFromPerfData()
-
-    def test_process_turbostat_results_with_valid_data(self):
-        """Normal case when log exists and contains valid data."""
-        self.result.turbostat_log_file = "/tmp/somelogfile.log"
-        with mock.patch(
-            "builtins.open",
-            mock.mock_open(read_data=TURBOSTAT_LOG_OUTPUT),
-        ) as mo:
-            cpustats = self.result.ProcessTurbostatResults()
-            # Check that the log got opened and data were read/parsed.
-            calls = [mock.call("/tmp/somelogfile.log", encoding="utf-8")]
-            mo.assert_has_calls(calls)
-            self.assertEqual(cpustats, TURBOSTAT_DATA)
-
-    def test_process_turbostat_results_from_empty_file(self):
-        """Error case when log exists but file is empty."""
-        self.result.turbostat_log_file = "/tmp/emptylogfile.log"
-        with mock.patch("builtins.open", mock.mock_open(read_data="")) as mo:
-            cpustats = self.result.ProcessTurbostatResults()
-            # Check that the log got opened and parsed successfully and empty
-            # data returned.
-            calls = [mock.call("/tmp/emptylogfile.log", encoding="utf-8")]
-            mo.assert_has_calls(calls)
-            self.assertEqual(cpustats, {})
-
-    def test_process_turbostat_results_when_file_doesnt_exist(self):
-        """Error case when file does not exist."""
-        nonexistinglog = "/tmp/1"
-        while os.path.exists(nonexistinglog):
-            # Extend file path if it happens to exist.
-            nonexistinglog = os.path.join(nonexistinglog, "1")
-        self.result.turbostat_log_file = nonexistinglog
-        # Allow the tested function to call a 'real' open and hopefully crash.
-        with self.assertRaises(IOError):
-            self.result.ProcessTurbostatResults()
-
-    def test_process_cpustats_results_with_uniq_data(self):
-        """Process cpustats log which has freq unique to each core.
-
-        Testing normal case when frequency data vary between
-        different cores.
-        Expecting that data for all cores will be present in
-        returned cpustats.
-        """
-        self.result.cpustats_log_file = "/tmp/somelogfile.log"
-        with mock.patch(
-            "builtins.open",
-            mock.mock_open(read_data=CPUSTATS_UNIQ_OUTPUT),
-        ) as mo:
-            cpustats = self.result.ProcessCpustatsResults()
-            # Check that the log got opened and data were read/parsed.
-            calls = [mock.call("/tmp/somelogfile.log", encoding="utf-8")]
-            mo.assert_has_calls(calls)
-            self.assertEqual(cpustats, CPUSTATS_UNIQ_DATA)
-
-    def test_process_cpustats_results_with_dupl_data(self):
-        """Process cpustats log where cores have duplicate freq.
-
-        Testing normal case when frequency data on some cores
-        are duplicated.
-        Expecting that duplicated data is discarded in
-        returned cpustats.
-        """
-        self.result.cpustats_log_file = "/tmp/somelogfile.log"
-        with mock.patch(
-            "builtins.open",
-            mock.mock_open(read_data=CPUSTATS_DUPL_OUTPUT),
-        ) as mo:
-            cpustats = self.result.ProcessCpustatsResults()
-            # Check that the log got opened and data were read/parsed.
-            calls = [mock.call("/tmp/somelogfile.log", encoding="utf-8")]
-            mo.assert_has_calls(calls)
-            self.assertEqual(cpustats, CPUSTATS_DUPL_DATA)
-
-    def test_process_cpustats_results_from_empty_file(self):
-        """Error case when log exists but file is empty."""
-        self.result.cpustats_log_file = "/tmp/emptylogfile.log"
-        with mock.patch("builtins.open", mock.mock_open(read_data="")) as mo:
-            cpustats = self.result.ProcessCpustatsResults()
-            # Check that the log got opened and parsed successfully and empty
-            # data returned.
-            calls = [mock.call("/tmp/emptylogfile.log", encoding="utf-8")]
-            mo.assert_has_calls(calls)
-            self.assertEqual(cpustats, {})
-
-    def test_process_top_results_with_valid_data(self):
-        """Process top log with valid data."""
-
-        self.result.top_log_file = "/tmp/fakelogfile.log"
-        with mock.patch(
-            "builtins.open", mock.mock_open(read_data=TOP_LOG)
-        ) as mo:
-            topproc = self.result.ProcessTopResults()
-            # Check that the log got opened and data were read/parsed.
-            calls = [mock.call("/tmp/fakelogfile.log", encoding="utf-8")]
-            mo.assert_has_calls(calls)
-            self.assertEqual(topproc, TOP_DATA)
-
-    def test_process_top_results_from_empty_file(self):
-        """Error case when log exists but file is empty."""
-        self.result.top_log_file = "/tmp/emptylogfile.log"
-        with mock.patch("builtins.open", mock.mock_open(read_data="")) as mo:
-            topcalls = self.result.ProcessTopResults()
-            # Check that the log got opened and parsed successfully and empty
-            # data returned.
-            calls = [mock.call("/tmp/emptylogfile.log", encoding="utf-8")]
-            mo.assert_has_calls(calls)
-            self.assertEqual(topcalls, [])
-
-    def test_format_string_top_cmds(self):
-        """Test formatted string with top commands."""
-        self.result.top_cmds = [
-            {
-                "cmd": "chrome-111",
-                "cpu_use_avg": 119.753453465,
-                "count": 44444,
-                "top5_cpu_use": [222.8, 217.9, 217.8, 191.0, 189.9],
-            },
-            {
-                "cmd": "chrome-222",
-                "cpu_use_avg": 100,
-                "count": 33333,
-                "top5_cpu_use": [200.0, 195.0, 190.0, 185.0, 180.0],
-            },
-            {
-                "cmd": "irq/230-cros-ec",
-                "cpu_use_avg": 10.000000000000001,
-                "count": 1000,
-                "top5_cpu_use": [11.5, 11.4, 11.3, 11.2, 11.1],
-            },
-            {
-                "cmd": "powerd",
-                "cpu_use_avg": 2.0,
-                "count": 2,
-                "top5_cpu_use": [3.0, 1.0],
-            },
-            {
-                "cmd": "cmd3",
-                "cpu_use_avg": 1.0,
-                "count": 1,
-                "top5_cpu_use": [1.0],
-            },
-            {
-                "cmd": "cmd4",
-                "cpu_use_avg": 1.0,
-                "count": 1,
-                "top5_cpu_use": [1.0],
-            },
-            {
-                "cmd": "cmd5",
-                "cpu_use_avg": 1.0,
-                "count": 1,
-                "top5_cpu_use": [1.0],
-            },
-            {
-                "cmd": "cmd6_not_for_print",
-                "cpu_avg": 1.0,
-                "count": 1,
-                "top5": [1.0],
-            },
-        ]
-        form_str = self.result.FormatStringTopCommands()
-        self.assertEqual(
-            form_str,
-            "\n".join(
-                [
-                    "Top commands with highest CPU usage:",
-                    "             COMMAND  AVG CPU%  COUNT   HIGHEST 5",
-                    "-" * 50,
-                    "          chrome-111    119.75  44444   "
-                    "[222.8, 217.9, 217.8, 191.0, 189.9]",
-                    "          chrome-222    100.00  33333   "
-                    "[200.0, 195.0, 190.0, 185.0, 180.0]",
-                    "     irq/230-cros-ec     10.00   1000   "
-                    "[11.5, 11.4, 11.3, 11.2, 11.1]",
-                    "              powerd      2.00      2   [3.0, 1.0]",
-                    "                cmd3      1.00      1   [1.0]",
-                    "                cmd4      1.00      1   [1.0]",
-                    "                cmd5      1.00      1   [1.0]",
-                    "-" * 50,
-                ]
-            ),
-        )
-
-    def test_format_string_top_calls_no_data(self):
-        """Test formatted string of top with no data."""
-        self.result.top_cmds = []
-        form_str = self.result.FormatStringTopCommands()
-        self.assertEqual(
-            form_str,
-            "\n".join(
-                [
-                    "Top commands with highest CPU usage:",
-                    "             COMMAND  AVG CPU%  COUNT   HIGHEST 5",
-                    "-" * 50,
-                    "[NO DATA FROM THE TOP LOG]",
-                    "-" * 50,
-                ]
-            ),
-        )
-
-    @mock.patch.object(os.path, "exists")
-    @mock.patch.object(misc, "GetInsideChrootPath")
-    @mock.patch.object(command_executer.CommandExecuter, "ChrootRunCommand")
-    def test_generate_perf_report_files(
-        self, mock_chrootruncmd, mock_getpath, mock_pathexists
-    ):
-        mock_pathexists.side_effect = (
-            lambda x: self.orig_exists(x)
-            if x != "/etc/cros_chroot_version"
-            else False
-        )
-        fake_file = "/tmp/results/perf.data.report"
-        self.result.perf_data_files = ["/tmp/results/perf.data"]
-        self.result.board = "lumpy"
-        mock_getpath.return_value = fake_file
-        self.result.ce.ChrootRunCommand = mock_chrootruncmd
-        mock_chrootruncmd.return_value = 0
-        # Debug path not found
-        self.result.label.debug_path = ""
-        tmp = self.result.GeneratePerfReportFiles()
-        self.assertEqual(len(tmp), 1)
-        self.assertEqual(tmp[0], RegexMatcher("/tmp/chromeos.*%s" % fake_file))
-        self.assertEqual(
-            mock_chrootruncmd.call_args_list[0][0],
-            (
-                self.result.chromeos_root,
-                ("/usr/sbin/perf report -n    " "-i %s --stdio > %s")
-                % (fake_file, fake_file),
-            ),
-        )
-
-    @mock.patch.object(os.path, "exists")
-    @mock.patch.object(misc, "GetInsideChrootPath")
-    @mock.patch.object(command_executer.CommandExecuter, "ChrootRunCommand")
-    def test_generate_perf_report_files_debug(
-        self, mock_chrootruncmd, mock_getpath, mock_pathexists
-    ):
-        mock_pathexists.side_effect = (
-            lambda x: self.orig_exists(x)
-            if x != "/etc/cros_chroot_version"
-            else False
-        )
-        fake_file = "/tmp/results/perf.data.report"
-        self.result.perf_data_files = ["/tmp/results/perf.data"]
-        self.result.board = "lumpy"
-        mock_getpath.return_value = fake_file
-        self.result.ce.ChrootRunCommand = mock_chrootruncmd
-        mock_chrootruncmd.return_value = 0
-        # Debug path found
-        self.result.label.debug_path = "/tmp/debug"
-        tmp = self.result.GeneratePerfReportFiles()
-        self.assertEqual(len(tmp), 1)
-        self.assertEqual(tmp[0], RegexMatcher("/tmp/chromeos.*%s" % fake_file))
-        self.assertEqual(
-            mock_chrootruncmd.call_args_list[0][0],
-            (
-                self.result.chromeos_root,
-                (
-                    "/usr/sbin/perf report -n --symfs /tmp/debug "
-                    "--vmlinux /tmp/debug/usr/lib/debug/boot/vmlinux  "
-                    "-i %s --stdio > %s"
-                )
-                % (fake_file, fake_file),
-            ),
-        )
-
-    @mock.patch.object(misc, "GetOutsideChrootPath")
-    def test_populate_from_run(self, mock_getpath):
-        def FakeGetResultsDir():
-            self.callGetResultsDir = True
-            return "/tmp/results_dir"
-
-        def FakeGetResultsFile():
-            self.callGetResultsFile = True
-            return []
-
-        def FakeGetPerfDataFiles():
-            self.callGetPerfDataFiles = True
-            return []
-
-        def FakeGetPerfReportFiles():
-            self.callGetPerfReportFiles = True
-            return []
-
-        def FakeGetTurbostatFile():
-            self.callGetTurbostatFile = True
-            return []
-
-        def FakeGetCpustatsFile():
-            self.callGetCpustatsFile = True
-            return []
-
-        def FakeGetTopFile():
-            self.callGetTopFile = True
-            return []
-
-        def FakeGetCpuinfoFile():
-            self.callGetCpuinfoFile = True
-            return []
-
-        def FakeGetWaitTimeFile():
-            self.callGetWaitTimeFile = True
-            return []
-
-        def FakeProcessResults(show_results=False):
-            if show_results:
-                pass
-            self.callProcessResults = True
-
-        if mock_getpath:
-            pass
-        mock.get_path = "/tmp/chromeos/tmp/results_dir"
-
-        self.callGetResultsDir = False
-        self.callGetResultsFile = False
-        self.callGetPerfDataFiles = False
-        self.callGetPerfReportFiles = False
-        self.callGetTurbostatFile = False
-        self.callGetCpustatsFile = False
-        self.callGetTopFile = False
-        self.callGetCpuinfoFile = False
-        self.callGetWaitTimeFile = False
-        self.callProcessResults = False
-
-        self.result.GetResultsDir = FakeGetResultsDir
-        self.result.GetResultsFile = FakeGetResultsFile
-        self.result.GetPerfDataFiles = FakeGetPerfDataFiles
-        self.result.GeneratePerfReportFiles = FakeGetPerfReportFiles
-        self.result.GetTurbostatFile = FakeGetTurbostatFile
-        self.result.GetCpustatsFile = FakeGetCpustatsFile
-        self.result.GetTopFile = FakeGetTopFile
-        self.result.GetCpuinfoFile = FakeGetCpuinfoFile
-        self.result.GetWaitTimeFile = FakeGetWaitTimeFile
-        self.result.ProcessResults = FakeProcessResults
-
-        self.result.PopulateFromRun(
-            OUTPUT, "", 0, "test", "telemetry_Crosperf", "chrome"
-        )
-        self.assertTrue(self.callGetResultsDir)
-        self.assertTrue(self.callGetResultsFile)
-        self.assertTrue(self.callGetPerfDataFiles)
-        self.assertTrue(self.callGetPerfReportFiles)
-        self.assertTrue(self.callGetTurbostatFile)
-        self.assertTrue(self.callGetCpustatsFile)
-        self.assertTrue(self.callGetTopFile)
-        self.assertTrue(self.callGetCpuinfoFile)
-        self.assertTrue(self.callGetWaitTimeFile)
-        self.assertTrue(self.callProcessResults)
-
-    def FakeGetKeyvals(self, show_all=False):
-        if show_all:
-            return {"first_time": 680, "Total": 10}
-        else:
-            return {"Total": 10}
-
-    def test_process_results(self):
-        def FakeGatherPerfResults():
-            self.callGatherPerfResults = True
-
-        def FakeGetSamples():
-            return (1, "samples")
-
-        # Test 1
-        self.callGatherPerfResults = False
-
-        self.result.GetKeyvals = self.FakeGetKeyvals
-        self.result.GatherPerfResults = FakeGatherPerfResults
-
-        self.result.retval = 0
-        self.result.ProcessResults()
-        self.assertTrue(self.callGatherPerfResults)
-        self.assertEqual(len(self.result.keyvals), 2)
-        self.assertEqual(self.result.keyvals, {"Total": 10, "retval": 0})
-
-        # Test 2
-        self.result.retval = 1
-        self.result.ProcessResults()
-        self.assertEqual(len(self.result.keyvals), 2)
-        self.assertEqual(self.result.keyvals, {"Total": 10, "retval": 1})
-
-        # Test 3
-        self.result.cwp_dso = "chrome"
-        self.result.retval = 0
-        self.result.GetSamples = FakeGetSamples
-        self.result.ProcessResults()
-        self.assertEqual(len(self.result.keyvals), 3)
-        self.assertEqual(
-            self.result.keyvals,
-            {"Total": 10, "samples": (1, "samples"), "retval": 0},
-        )
-
-        # Test 4. Parse output of benchmarks with multiple sotries in histogram
-        # format
-        self.result.suite = "telemetry_Crosperf"
-        self.result.results_file = [tempfile.mkdtemp() + "/histograms.json"]
-        with open(self.result.results_file[0], "w", encoding="utf-8") as f:
-            f.write(HISTOGRAMSET)
-        self.result.ProcessResults()
-        shutil.rmtree(os.path.dirname(self.result.results_file[0]))
-        # Verify the summary for the story is correct
-        self.assertEqual(
-            self.result.keyvals["timeToFirstContentfulPaint__typical"],
-            [880.000, "ms_smallerIsBetter"],
-        )
-        # Veirfy the summary for a certain stroy tag is correct
-        self.assertEqual(
-            self.result.keyvals[
-                "timeToFirstContentfulPaint__cache_temperature:cold"
-            ],
-            [1000.000, "ms_smallerIsBetter"],
-        )
-        self.assertEqual(
-            self.result.keyvals[
-                "timeToFirstContentfulPaint__cache_temperature:warm"
-            ],
-            [800.000, "ms_smallerIsBetter"],
-        )
-
-    @mock.patch.object(Result, "ProcessCpustatsResults")
-    @mock.patch.object(Result, "ProcessTurbostatResults")
-    def test_process_results_with_turbostat_log(
-        self, mock_proc_turbo, mock_proc_cpustats
-    ):
-        self.result.GetKeyvals = self.FakeGetKeyvals
-
-        self.result.retval = 0
-        self.result.turbostat_log_file = "/tmp/turbostat.log"
-        mock_proc_turbo.return_value = {
-            "cpufreq": {"all": [1, 2, 3]},
-            "cputemp": {"all": [5.0, 6.0, 7.0]},
-        }
-        self.result.ProcessResults()
-        mock_proc_turbo.assert_has_calls([mock.call()])
-        mock_proc_cpustats.assert_not_called()
-        self.assertEqual(len(self.result.keyvals), 8)
-        self.assertEqual(
-            self.result.keyvals,
-            {
-                "Total": 10,
-                "cpufreq_all_avg": 2,
-                "cpufreq_all_max": 3,
-                "cpufreq_all_min": 1,
-                "cputemp_all_avg": 6.0,
-                "cputemp_all_min": 5.0,
-                "cputemp_all_max": 7.0,
-                "retval": 0,
-            },
-        )
-
-    @mock.patch.object(Result, "ProcessCpustatsResults")
-    @mock.patch.object(Result, "ProcessTurbostatResults")
-    def test_process_results_with_cpustats_log(
-        self, mock_proc_turbo, mock_proc_cpustats
-    ):
-        self.result.GetKeyvals = self.FakeGetKeyvals
-
-        self.result.retval = 0
-        self.result.cpustats_log_file = "/tmp/cpustats.log"
-        mock_proc_cpustats.return_value = {
-            "cpufreq": {"cpu0": [100, 100, 100], "cpu1": [4, 5, 6]},
-            "cputemp": {
-                "little": [20.2, 20.2, 20.2],
-                "big": [55.2, 66.1, 77.3],
-            },
-        }
-        self.result.ProcessResults()
-        mock_proc_turbo.assert_not_called()
-        mock_proc_cpustats.assert_has_calls([mock.call()])
-        self.assertEqual(len(self.result.keyvals), 10)
-        self.assertEqual(
-            self.result.keyvals,
-            {
-                "Total": 10,
-                "cpufreq_cpu0_avg": 100,
-                "cpufreq_cpu1_avg": 5,
-                "cpufreq_cpu1_max": 6,
-                "cpufreq_cpu1_min": 4,
-                "cputemp_big_avg": 66.2,
-                "cputemp_big_max": 77.3,
-                "cputemp_big_min": 55.2,
-                "cputemp_little_avg": 20.2,
-                "retval": 0,
-            },
-        )
-
-    @mock.patch.object(Result, "ProcessCpustatsResults")
-    @mock.patch.object(Result, "ProcessTurbostatResults")
-    def test_process_results_with_turbostat_and_cpustats_logs(
-        self, mock_proc_turbo, mock_proc_cpustats
-    ):
-        self.result.GetKeyvals = self.FakeGetKeyvals
-
-        self.result.retval = 0
-        self.result.turbostat_log_file = "/tmp/turbostat.log"
-        self.result.cpustats_log_file = "/tmp/cpustats.log"
-        mock_proc_turbo.return_value = {
-            "cpufreq": {"all": [1, 2, 3]},
-            "cputemp": {"all": [5.0, 6.0, 7.0]},
-        }
-        self.result.ProcessResults()
-        mock_proc_turbo.assert_has_calls([mock.call()])
-        mock_proc_cpustats.assert_not_called()
-        self.assertEqual(len(self.result.keyvals), 8)
-        self.assertEqual(
-            self.result.keyvals,
-            {
-                "Total": 10,
-                "cpufreq_all_avg": 2,
-                "cpufreq_all_max": 3,
-                "cpufreq_all_min": 1,
-                "cputemp_all_avg": 6.0,
-                "cputemp_all_min": 5.0,
-                "cputemp_all_max": 7.0,
-                "retval": 0,
-            },
-        )
-
-    @mock.patch.object(Result, "ProcessCpustatsResults")
-    @mock.patch.object(Result, "ProcessTurbostatResults")
-    def test_process_results_without_cpu_data(
-        self, mock_proc_turbo, mock_proc_cpustats
-    ):
-        self.result.GetKeyvals = self.FakeGetKeyvals
-
-        self.result.retval = 0
-        self.result.turbostat_log_file = ""
-        self.result.cpustats_log_file = ""
-        self.result.ProcessResults()
-        mock_proc_turbo.assert_not_called()
-        mock_proc_cpustats.assert_not_called()
-        self.assertEqual(len(self.result.keyvals), 2)
-        self.assertEqual(self.result.keyvals, {"Total": 10, "retval": 0})
-
-    @mock.patch.object(misc, "GetInsideChrootPath")
-    @mock.patch.object(
-        command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-    )
-    def test_populate_from_cache_dir(self, mock_runchrootcmd, mock_getpath):
-        # pylint: disable=redefined-builtin
-        def FakeMkdtemp(dir=None):
-            if dir:
-                pass
-            return self.tmpdir
-
-        def FakeGetSamples():
-            return [1, "samples"]
-
-        current_path = os.getcwd()
-        cache_dir = os.path.join(current_path, "test_cache/test_input")
-        self.result.ce = command_executer.GetCommandExecuter(
-            log_level="average"
-        )
-        self.result.ce.ChrootRunCommandWOutput = mock_runchrootcmd
-        mock_runchrootcmd.return_value = [
-            "",
-            ("%s,PASS\n%s/\telemetry_Crosperf,PASS\n") % (TMP_DIR1, TMP_DIR1),
-            "",
-        ]
-        mock_getpath.return_value = TMP_DIR1
-        self.tmpdir = tempfile.mkdtemp()
-        save_real_mkdtemp = tempfile.mkdtemp
-        tempfile.mkdtemp = FakeMkdtemp
-
-        self.result.PopulateFromCacheDir(
-            cache_dir, "sunspider", "telemetry_Crosperf", ""
-        )
-        self.assertEqual(
-            self.result.keyvals,
-            {
-                "Total__Total": [444.0, "ms"],
-                "regexp-dna__regexp-dna": [16.2, "ms"],
-                "telemetry_page_measurement_results__num_failed": [
-                    0,
-                    "count",
-                ],
-                "telemetry_page_measurement_results__num_errored": [
-                    0,
-                    "count",
-                ],
-                "string-fasta__string-fasta": [23.2, "ms"],
-                "crypto-sha1__crypto-sha1": [11.6, "ms"],
-                "bitops-3bit-bits-in-byte__bitops-3bit-bits-in-byte": [
-                    3.2,
-                    "ms",
-                ],
-                "access-nsieve__access-nsieve": [7.9, "ms"],
-                "bitops-nsieve-bits__bitops-nsieve-bits": [9.4, "ms"],
-                "string-validate-input__string-validate-input": [19.3, "ms"],
-                "3d-raytrace__3d-raytrace": [24.7, "ms"],
-                "3d-cube__3d-cube": [28.0, "ms"],
-                "string-unpack-code__string-unpack-code": [46.7, "ms"],
-                "date-format-tofte__date-format-tofte": [26.3, "ms"],
-                "math-partial-sums__math-partial-sums": [22.0, "ms"],
-                "\telemetry_Crosperf": ["PASS", ""],
-                "crypto-aes__crypto-aes": [15.2, "ms"],
-                "bitops-bitwise-and__bitops-bitwise-and": [8.4, "ms"],
-                "crypto-md5__crypto-md5": [10.5, "ms"],
-                "string-tagcloud__string-tagcloud": [52.8, "ms"],
-                "access-nbody__access-nbody": [8.5, "ms"],
-                "retval": 0,
-                "math-spectral-norm__math-spectral-norm": [6.6, "ms"],
-                "math-cordic__math-cordic": [8.7, "ms"],
-                "access-binary-trees__access-binary-trees": [4.5, "ms"],
-                "controlflow-recursive__controlflow-recursive": [4.4, "ms"],
-                "access-fannkuch__access-fannkuch": [17.8, "ms"],
-                "string-base64__string-base64": [16.0, "ms"],
-                "date-format-xparb__date-format-xparb": [20.9, "ms"],
-                "3d-morph__3d-morph": [22.1, "ms"],
-                "bitops-bits-in-byte__bitops-bits-in-byte": [9.1, "ms"],
-            },
-        )
-
-        self.result.GetSamples = FakeGetSamples
-        self.result.PopulateFromCacheDir(
-            cache_dir, "sunspider", "telemetry_Crosperf", "chrome"
-        )
-        self.assertEqual(
-            self.result.keyvals,
-            {
-                "Total__Total": [444.0, "ms"],
-                "regexp-dna__regexp-dna": [16.2, "ms"],
-                "telemetry_page_measurement_results__num_failed": [
-                    0,
-                    "count",
-                ],
-                "telemetry_page_measurement_results__num_errored": [
-                    0,
-                    "count",
-                ],
-                "string-fasta__string-fasta": [23.2, "ms"],
-                "crypto-sha1__crypto-sha1": [11.6, "ms"],
-                "bitops-3bit-bits-in-byte__bitops-3bit-bits-in-byte": [
-                    3.2,
-                    "ms",
-                ],
-                "access-nsieve__access-nsieve": [7.9, "ms"],
-                "bitops-nsieve-bits__bitops-nsieve-bits": [9.4, "ms"],
-                "string-validate-input__string-validate-input": [19.3, "ms"],
-                "3d-raytrace__3d-raytrace": [24.7, "ms"],
-                "3d-cube__3d-cube": [28.0, "ms"],
-                "string-unpack-code__string-unpack-code": [46.7, "ms"],
-                "date-format-tofte__date-format-tofte": [26.3, "ms"],
-                "math-partial-sums__math-partial-sums": [22.0, "ms"],
-                "\telemetry_Crosperf": ["PASS", ""],
-                "crypto-aes__crypto-aes": [15.2, "ms"],
-                "bitops-bitwise-and__bitops-bitwise-and": [8.4, "ms"],
-                "crypto-md5__crypto-md5": [10.5, "ms"],
-                "string-tagcloud__string-tagcloud": [52.8, "ms"],
-                "access-nbody__access-nbody": [8.5, "ms"],
-                "retval": 0,
-                "math-spectral-norm__math-spectral-norm": [6.6, "ms"],
-                "math-cordic__math-cordic": [8.7, "ms"],
-                "access-binary-trees__access-binary-trees": [4.5, "ms"],
-                "controlflow-recursive__controlflow-recursive": [4.4, "ms"],
-                "access-fannkuch__access-fannkuch": [17.8, "ms"],
-                "string-base64__string-base64": [16.0, "ms"],
-                "date-format-xparb__date-format-xparb": [20.9, "ms"],
-                "3d-morph__3d-morph": [22.1, "ms"],
-                "bitops-bits-in-byte__bitops-bits-in-byte": [9.1, "ms"],
-                "samples": [1, "samples"],
-            },
-        )
-
-        # Clean up after test.
-        tempfile.mkdtemp = save_real_mkdtemp
-        command = "rm -Rf %s" % self.tmpdir
-        self.result.ce.RunCommand(command)
-
-    @mock.patch.object(misc, "GetRoot")
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommand")
-    def test_cleanup(self, mock_runcmd, mock_getroot):
-        # Test 1. 'rm_chroot_tmp' is True; self.results_dir exists;
-        # self.temp_dir exists; results_dir name contains 'test_that_results_'.
-        mock_getroot.return_value = [
-            "/tmp/tmp_AbcXyz",
-            "test_that_results_fake",
-        ]
-        self.result.ce.RunCommand = mock_runcmd
-        self.result.results_dir = "test_results_dir"
-        self.result.temp_dir = "testtemp_dir"
-        self.result.CleanUp(True)
-        self.assertEqual(mock_getroot.call_count, 1)
-        self.assertEqual(mock_runcmd.call_count, 2)
-        self.assertEqual(
-            mock_runcmd.call_args_list[0][0], ("rm -rf test_results_dir",)
-        )
-        self.assertEqual(
-            mock_runcmd.call_args_list[1][0], ("rm -rf testtemp_dir",)
-        )
-
-        # Test 2. Same, except ath results_dir name does not contain
-        # 'test_that_results_'
-        mock_getroot.reset_mock()
-        mock_runcmd.reset_mock()
-        mock_getroot.return_value = ["/tmp/tmp_AbcXyz", "other_results_fake"]
-        self.result.ce.RunCommand = mock_runcmd
-        self.result.results_dir = "test_results_dir"
-        self.result.temp_dir = "testtemp_dir"
-        self.result.CleanUp(True)
-        self.assertEqual(mock_getroot.call_count, 1)
-        self.assertEqual(mock_runcmd.call_count, 2)
-        self.assertEqual(
-            mock_runcmd.call_args_list[0][0], ("rm -rf /tmp/tmp_AbcXyz",)
-        )
-        self.assertEqual(
-            mock_runcmd.call_args_list[1][0], ("rm -rf testtemp_dir",)
-        )
-
-        # Test 3. mock_getroot returns nothing; 'rm_chroot_tmp' is False.
-        mock_getroot.reset_mock()
-        mock_runcmd.reset_mock()
-        self.result.CleanUp(False)
-        self.assertEqual(mock_getroot.call_count, 0)
-        self.assertEqual(mock_runcmd.call_count, 1)
-        self.assertEqual(
-            mock_runcmd.call_args_list[0][0], ("rm -rf testtemp_dir",)
-        )
-
-        # Test 4. 'rm_chroot_tmp' is True, but result_dir & temp_dir are None.
-        mock_getroot.reset_mock()
-        mock_runcmd.reset_mock()
-        self.result.results_dir = None
-        self.result.temp_dir = None
-        self.result.CleanUp(True)
-        self.assertEqual(mock_getroot.call_count, 0)
-        self.assertEqual(mock_runcmd.call_count, 0)
-
-    @mock.patch.object(misc, "GetInsideChrootPath")
-    @mock.patch.object(command_executer.CommandExecuter, "ChrootRunCommand")
-    def test_store_to_cache_dir(self, mock_chrootruncmd, mock_getpath):
-        def FakeMkdtemp(directory=""):
-            if directory:
-                pass
-            return self.tmpdir
-
-        if mock_chrootruncmd or mock_getpath:
-            pass
-        current_path = os.getcwd()
-        cache_dir = os.path.join(current_path, "test_cache/test_output")
-
-        self.result.ce = command_executer.GetCommandExecuter(
-            log_level="average"
-        )
-        self.result.out = OUTPUT
-        self.result.err = error
-        self.result.retval = 0
-        self.tmpdir = tempfile.mkdtemp()
-        if not os.path.exists(self.tmpdir):
-            os.makedirs(self.tmpdir)
-        self.result.results_dir = os.path.join(os.getcwd(), "test_cache")
-        save_real_mkdtemp = tempfile.mkdtemp
-        tempfile.mkdtemp = FakeMkdtemp
-
-        mock_mm = machine_manager.MockMachineManager(
-            "/tmp/chromeos_root", 0, "average", ""
-        )
-        mock_mm.machine_checksum_string[
-            "mock_label"
-        ] = "fake_machine_checksum123"
-
-        mock_keylist = ["key1", "key2", "key3"]
-        test_flag.SetTestMode(True)
-        self.result.StoreToCacheDir(cache_dir, mock_mm, mock_keylist)
-
-        # Check that the correct things were written to the 'cache'.
-        test_dir = os.path.join(os.getcwd(), "test_cache/test_output")
-        base_dir = os.path.join(os.getcwd(), "test_cache/compare_output")
-        self.assertTrue(os.path.exists(os.path.join(test_dir, "autotest.tbz2")))
-        self.assertTrue(os.path.exists(os.path.join(test_dir, "machine.txt")))
-        self.assertTrue(
-            os.path.exists(os.path.join(test_dir, "results.pickle"))
-        )
-
-        f1 = os.path.join(test_dir, "machine.txt")
-        f2 = os.path.join(base_dir, "machine.txt")
-        cmd = "diff %s %s" % (f1, f2)
-        [_, out, _] = self.result.ce.RunCommandWOutput(cmd)
-        self.assertEqual(len(out), 0)
-
-        f1 = os.path.join(test_dir, "results.pickle")
-        f2 = os.path.join(base_dir, "results.pickle")
-        with open(f1, "rb") as f:
-            f1_obj = pickle.load(f)
-        with open(f2, "rb") as f:
-            f2_obj = pickle.load(f)
-        self.assertEqual(f1_obj, f2_obj)
-
-        # Clean up after test.
-        tempfile.mkdtemp = save_real_mkdtemp
-        command = "rm %s/*" % test_dir
-        self.result.ce.RunCommand(command)
-
-
-TELEMETRY_RESULT_KEYVALS = {
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "math-cordic (ms)": "11.4",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "access-nbody (ms)": "6.9",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "access-fannkuch (ms)": "26.3",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "math-spectral-norm (ms)": "6.3",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "bitops-nsieve-bits (ms)": "9.3",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "math-partial-sums (ms)": "32.8",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "regexp-dna (ms)": "16.1",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "3d-cube (ms)": "42.7",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "crypto-md5 (ms)": "10.8",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "crypto-sha1 (ms)": "12.4",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "string-tagcloud (ms)": "47.2",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "string-fasta (ms)": "36.3",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "access-binary-trees (ms)": "7.3",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "date-format-xparb (ms)": "138.1",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "crypto-aes (ms)": "19.2",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "Total (ms)": "656.5",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "string-base64 (ms)": "17.5",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "string-validate-input (ms)": "24.8",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "3d-raytrace (ms)": "28.7",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "controlflow-recursive (ms)": "5.3",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "bitops-bits-in-byte (ms)": "9.8",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "3d-morph (ms)": "50.2",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "bitops-bitwise-and (ms)": "8.8",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "access-nsieve (ms)": "8.6",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "date-format-tofte (ms)": "31.2",
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "bitops-3bit-bits-in-byte (ms)": "3.5",
-    "retval": 0,
-    "http://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html "
-    "string-unpack-code (ms)": "45.0",
-}
-
-PURE_TELEMETRY_OUTPUT = """
-page_name,3d-cube (ms),3d-morph (ms),3d-raytrace (ms),Total (ms),access-binary-trees (ms),access-fannkuch (ms),access-nbody (ms),access-nsieve (ms),bitops-3bit-bits-in-byte (ms),bitops-bits-in-byte (ms),bitops-bitwise-and (ms),bitops-nsieve-bits (ms),controlflow-recursive (ms),crypto-aes (ms),crypto-md5 (ms),crypto-sha1 (ms),date-format-tofte (ms),date-format-xparb (ms),math-cordic (ms),math-partial-sums (ms),math-spectral-norm (ms),regexp-dna (ms),string-base64 (ms),string-fasta (ms),string-tagcloud (ms),string-unpack-code (ms),string-validate-input (ms)\r\nhttp://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html,42.7,50.2,28.7,656.5,7.3,26.3,6.9,8.6,3.5,9.8,8.8,9.3,5.3,19.2,10.8,12.4,31.2,138.1,11.4,32.8,6.3,16.1,17.5,36.3,47.2,45.0,24.8\r
-"""
-
-
-class TelemetryResultTest(unittest.TestCase):
-    """Telemetry result test."""
-
-    def __init__(self, *args, **kwargs):
-        super(TelemetryResultTest, self).__init__(*args, **kwargs)
-        self.callFakeProcessResults = False
-        self.result = None
-        self.mock_logger = mock.Mock(spec=logger.Logger)
-        self.mock_cmd_exec = mock.Mock(spec=command_executer.CommandExecuter)
-        self.mock_label = MockLabel(
-            "mock_label",
-            "build",
-            "chromeos_image",
-            "autotest_dir",
-            "debug_dir",
-            "/tmp",
-            "lumpy",
-            "remote",
-            "image_args",
-            "cache_dir",
-            "average",
-            "gcc",
-            False,
-            None,
-        )
-        self.mock_machine = machine_manager.MockCrosMachine(
-            "falco.cros", "/tmp/chromeos", "average"
-        )
-
-    def test_populate_from_run(self):
-        def FakeProcessResults():
-            self.callFakeProcessResults = True
-
-        self.callFakeProcessResults = False
-        self.result = TelemetryResult(
-            self.mock_logger, self.mock_label, "average", self.mock_cmd_exec
-        )
-        self.result.ProcessResults = FakeProcessResults
-        self.result.PopulateFromRun(
-            OUTPUT, error, 3, "fake_test", "telemetry_Crosperf", ""
-        )
-        self.assertTrue(self.callFakeProcessResults)
-        self.assertEqual(self.result.out, OUTPUT)
-        self.assertEqual(self.result.err, error)
-        self.assertEqual(self.result.retval, 3)
-
-    def test_populate_from_cache_dir_and_process_results(self):
-        self.result = TelemetryResult(
-            self.mock_logger, self.mock_label, "average", self.mock_machine
-        )
-        current_path = os.getcwd()
-        cache_dir = os.path.join(
-            current_path, "test_cache/test_puretelemetry_input"
-        )
-        self.result.PopulateFromCacheDir(cache_dir, "", "", "")
-        self.assertEqual(self.result.out.strip(), PURE_TELEMETRY_OUTPUT.strip())
-        self.assertEqual(self.result.err, "")
-        self.assertEqual(self.result.retval, 0)
-        self.assertEqual(self.result.keyvals, TELEMETRY_RESULT_KEYVALS)
-
-
-class ResultsCacheTest(unittest.TestCase):
-    """Resultcache test class."""
-
-    def __init__(self, *args, **kwargs):
-        super(ResultsCacheTest, self).__init__(*args, **kwargs)
-        self.fakeCacheReturnResult = None
-        self.mock_logger = mock.Mock(spec=logger.Logger)
-        self.mock_label = MockLabel(
-            "mock_label",
-            "build",
-            "chromeos_image",
-            "autotest_dir",
-            "debug_dir",
-            "/tmp",
-            "lumpy",
-            "remote",
-            "image_args",
-            "cache_dir",
-            "average",
-            "gcc",
-            False,
-            None,
-        )
-
-    def setUp(self):
-        self.results_cache = ResultsCache()
-
-        mock_machine = machine_manager.MockCrosMachine(
-            "falco.cros", "/tmp/chromeos", "average"
-        )
-
-        mock_mm = machine_manager.MockMachineManager(
-            "/tmp/chromeos_root", 0, "average", ""
-        )
-        mock_mm.machine_checksum_string[
-            "mock_label"
-        ] = "fake_machine_checksum123"
-
-        self.results_cache.Init(
-            self.mock_label.chromeos_image,
-            self.mock_label.chromeos_root,
-            "sunspider",
-            1,  # benchmark_run.iteration,
-            "",  # benchmark_run.test_args,
-            "",  # benchmark_run.profiler_args,
-            mock_mm,
-            mock_machine,
-            self.mock_label.board,
-            [
-                CacheConditions.CACHE_FILE_EXISTS,
-                CacheConditions.CHECKSUMS_MATCH,
-            ],
-            self.mock_logger,
-            "average",
-            self.mock_label,
-            "",  # benchmark_run.share_cache
-            "telemetry_Crosperf",
-            True,  # benchmark_run.show_all_results
-            False,  # benchmark_run.run_local
-            "",
-        )  # benchmark_run.cwp_dso
-
-    @mock.patch.object(image_checksummer.ImageChecksummer, "Checksum")
-    def test_get_cache_dir_for_write(self, mock_checksum):
-        def FakeGetMachines(label):
-            if label:
-                pass
-            m1 = machine_manager.MockCrosMachine(
-                "lumpy1.cros", self.results_cache.chromeos_root, "average"
-            )
-            m2 = machine_manager.MockCrosMachine(
-                "lumpy2.cros", self.results_cache.chromeos_root, "average"
-            )
-            return [m1, m2]
-
-        mock_checksum.return_value = "FakeImageChecksumabc123"
-        self.results_cache.machine_manager.GetMachines = FakeGetMachines
-        self.results_cache.machine_manager.machine_checksum[
-            "mock_label"
-        ] = "FakeMachineChecksumabc987"
-        # Based on the label, benchmark and machines, get the directory in which
-        # to store the cache information for this test run.
-        result_path = self.results_cache.GetCacheDirForWrite()
-        # Verify that the returned directory is correct (since the label
-        # contained a cache_dir, named 'cache_dir', that's what is expected in
-        # the result, rather than '~/cros_scratch').
-        comp_path = os.path.join(
-            os.getcwd(),
-            "cache_dir/54524606abaae4fdf7b02f49f7ae7127_"
-            "sunspider_1_fda29412ceccb72977516c4785d08e2c_"
-            "FakeImageChecksumabc123_FakeMachineChecksum"
-            "abc987__6",
-        )
-        self.assertEqual(result_path, comp_path)
-
-    def test_form_cache_dir(self):
-        # This is very similar to the previous test (FormCacheDir is called
-        # from GetCacheDirForWrite).
-        cache_key_list = (
-            "54524606abaae4fdf7b02f49f7ae7127",
-            "sunspider",
-            "1",
-            "7215ee9c7d9dc229d2921a40e899ec5f",
-            "FakeImageChecksumabc123",
-            "*",
-            "*",
-            "6",
-        )
-        path = self.results_cache.FormCacheDir(cache_key_list)
-        self.assertEqual(len(path), 1)
-        path1 = path[0]
-        test_dirname = (
-            "54524606abaae4fdf7b02f49f7ae7127_sunspider_1_7215ee9"
-            "c7d9dc229d2921a40e899ec5f_FakeImageChecksumabc123_*_*_6"
-        )
-        comp_path = os.path.join(os.getcwd(), "cache_dir", test_dirname)
-        self.assertEqual(path1, comp_path)
-
-    @mock.patch.object(image_checksummer.ImageChecksummer, "Checksum")
-    def test_get_cache_key_list(self, mock_checksum):
-        # This tests the mechanism that generates the various pieces of the
-        # cache directory name, based on various conditions.
-
-        def FakeGetMachines(label):
-            if label:
-                pass
-            m1 = machine_manager.MockCrosMachine(
-                "lumpy1.cros", self.results_cache.chromeos_root, "average"
-            )
-            m2 = machine_manager.MockCrosMachine(
-                "lumpy2.cros", self.results_cache.chromeos_root, "average"
-            )
-            return [m1, m2]
-
-        mock_checksum.return_value = "FakeImageChecksumabc123"
-        self.results_cache.machine_manager.GetMachines = FakeGetMachines
-        self.results_cache.machine_manager.machine_checksum[
-            "mock_label"
-        ] = "FakeMachineChecksumabc987"
-
-        # Test 1. Generating cache name for reading (not writing).
-        key_list = self.results_cache.GetCacheKeyList(True)
-        self.assertEqual(key_list[0], "*")  # Machine checksum value, for read.
-        self.assertEqual(key_list[1], "sunspider")
-        self.assertEqual(key_list[2], "1")
-        self.assertEqual(key_list[3], "fda29412ceccb72977516c4785d08e2c")
-        self.assertEqual(key_list[4], "FakeImageChecksumabc123")
-        self.assertEqual(key_list[5], "*")
-        self.assertEqual(key_list[6], "*")
-        self.assertEqual(key_list[7], "6")
-
-        # Test 2. Generating cache name for writing, with local image type.
-        key_list = self.results_cache.GetCacheKeyList(False)
-        self.assertEqual(key_list[0], "54524606abaae4fdf7b02f49f7ae7127")
-        self.assertEqual(key_list[1], "sunspider")
-        self.assertEqual(key_list[2], "1")
-        self.assertEqual(key_list[3], "fda29412ceccb72977516c4785d08e2c")
-        self.assertEqual(key_list[4], "FakeImageChecksumabc123")
-        self.assertEqual(key_list[5], "FakeMachineChecksumabc987")
-        self.assertEqual(key_list[6], "")
-        self.assertEqual(key_list[7], "6")
-
-        # Test 3. Generating cache name for writing, with trybot image type.
-        self.results_cache.label.image_type = "trybot"
-        key_list = self.results_cache.GetCacheKeyList(False)
-        self.assertEqual(key_list[0], "54524606abaae4fdf7b02f49f7ae7127")
-        self.assertEqual(key_list[3], "fda29412ceccb72977516c4785d08e2c")
-        self.assertEqual(key_list[4], "54524606abaae4fdf7b02f49f7ae7127")
-        self.assertEqual(key_list[5], "FakeMachineChecksumabc987")
-
-        # Test 4. Generating cache name for writing, with official image type.
-        self.results_cache.label.image_type = "official"
-        key_list = self.results_cache.GetCacheKeyList(False)
-        self.assertEqual(key_list[0], "54524606abaae4fdf7b02f49f7ae7127")
-        self.assertEqual(key_list[1], "sunspider")
-        self.assertEqual(key_list[2], "1")
-        self.assertEqual(key_list[3], "fda29412ceccb72977516c4785d08e2c")
-        self.assertEqual(key_list[4], "*")
-        self.assertEqual(key_list[5], "FakeMachineChecksumabc987")
-        self.assertEqual(key_list[6], "")
-        self.assertEqual(key_list[7], "6")
-
-        # Test 5. Generating cache name for writing, with local image type, and
-        # specifying that the image path must match the cached image path.
-        self.results_cache.label.image_type = "local"
-        self.results_cache.cache_conditions.append(
-            CacheConditions.IMAGE_PATH_MATCH
-        )
-        key_list = self.results_cache.GetCacheKeyList(False)
-        self.assertEqual(key_list[0], "54524606abaae4fdf7b02f49f7ae7127")
-        self.assertEqual(key_list[3], "fda29412ceccb72977516c4785d08e2c")
-        self.assertEqual(key_list[4], "FakeImageChecksumabc123")
-        self.assertEqual(key_list[5], "FakeMachineChecksumabc987")
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommand")
-    @mock.patch.object(os.path, "isdir")
-    @mock.patch.object(Result, "CreateFromCacheHit")
-    def test_read_result(self, mock_create, mock_isdir, mock_runcmd):
-        self.fakeCacheReturnResult = None
-
-        def FakeGetCacheDirForRead():
-            return self.fakeCacheReturnResult
-
-        def FakeGetCacheDirForWrite():
-            return self.fakeCacheReturnResult
-
-        mock_cmd_exec = mock.Mock(spec=command_executer.CommandExecuter)
-        fake_result = Result(
-            self.mock_logger, self.mock_label, "average", mock_cmd_exec
-        )
-        fake_result.retval = 0
-
-        # Set up results_cache _GetCacheDirFor{Read,Write} to return
-        # self.fakeCacheReturnResult, which is initially None (see above).
-        # So initially, no cache dir is returned.
-        self.results_cache.GetCacheDirForRead = FakeGetCacheDirForRead
-        self.results_cache.GetCacheDirForWrite = FakeGetCacheDirForWrite
-
-        mock_isdir.return_value = True
-        save_cc = [
-            CacheConditions.CACHE_FILE_EXISTS,
-            CacheConditions.CHECKSUMS_MATCH,
-        ]
-        self.results_cache.cache_conditions.append(CacheConditions.FALSE)
-
-        # Test 1. CacheCondition.FALSE, which means do not read from the cache.
-        # (force re-running of test).  Result should be None.
-        res = self.results_cache.ReadResult()
-        self.assertIsNone(res)
-        self.assertEqual(mock_runcmd.call_count, 1)
-
-        # Test 2. Remove CacheCondition.FALSE. Result should still be None,
-        # because GetCacheDirForRead is returning None at the moment.
-        mock_runcmd.reset_mock()
-        self.results_cache.cache_conditions = save_cc
-        res = self.results_cache.ReadResult()
-        self.assertIsNone(res)
-        self.assertEqual(mock_runcmd.call_count, 0)
-
-        # Test 3. Now set up cache dir to be returned by GetCacheDirForRead.
-        # Since cache_dir is found, will call Result.CreateFromCacheHit, which
-        # which will actually all our mock_create and should return fake_result.
-        self.fakeCacheReturnResult = "fake/cache/dir"
-        mock_create.return_value = fake_result
-        res = self.results_cache.ReadResult()
-        self.assertEqual(mock_runcmd.call_count, 0)
-        self.assertEqual(res, fake_result)
-
-        # Test 4. os.path.isdir(cache_dir) will now return false, so result
-        # should be None again (no cache found).
-        mock_isdir.return_value = False
-        res = self.results_cache.ReadResult()
-        self.assertEqual(mock_runcmd.call_count, 0)
-        self.assertIsNone(res)
-
-        # Test 5. os.path.isdir returns true, but mock_create now returns None
-        # (the call to CreateFromCacheHit returns None), so overal result is
-        # None.
-        mock_isdir.return_value = True
-        mock_create.return_value = None
-        res = self.results_cache.ReadResult()
-        self.assertEqual(mock_runcmd.call_count, 0)
-        self.assertIsNone(res)
-
-        # Test 6. Everything works 'as expected', result should be fake_result.
-        mock_create.return_value = fake_result
-        res = self.results_cache.ReadResult()
-        self.assertEqual(mock_runcmd.call_count, 0)
-        self.assertEqual(res, fake_result)
-
-        # Test 7. The run failed; result should be None.
-        mock_create.return_value = fake_result
-        fake_result.retval = 1
-        self.results_cache.cache_conditions.append(
-            CacheConditions.RUN_SUCCEEDED
-        )
-        res = self.results_cache.ReadResult()
-        self.assertEqual(mock_runcmd.call_count, 0)
-        self.assertIsNone(res)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/results_organizer.py b/crosperf/results_organizer.py
deleted file mode 100644
index 354e002d..00000000
--- a/crosperf/results_organizer.py
+++ /dev/null
@@ -1,232 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Parse data from benchmark_runs for tabulator."""
-
-
-import errno
-import json
-import os
-import re
-import sys
-
-from cros_utils import misc
-
-
-_TELEMETRY_RESULT_DEFAULTS_FILE = "default-telemetry-results.json"
-_DUP_KEY_REGEX = re.compile(r"(\w+)\{(\d+)\}")
-
-
-def _AdjustIteration(benchmarks, max_dup, bench):
-    """Adjust the interation numbers if they have keys like ABCD{i}."""
-    for benchmark in benchmarks:
-        if benchmark.name != bench or benchmark.iteration_adjusted:
-            continue
-        benchmark.iteration_adjusted = True
-        benchmark.iterations *= max_dup + 1
-
-
-def _GetMaxDup(data):
-    """Find the maximum i inside ABCD{i}.
-
-    data should be a [[[Key]]], where Key is a string that may look like
-    ABCD{i}.
-    """
-    max_dup = 0
-    for label in data:
-        for run in label:
-            for key in run:
-                match = _DUP_KEY_REGEX.match(key)
-                if match:
-                    max_dup = max(max_dup, int(match.group(2)))
-    return max_dup
-
-
-def _Repeat(func, times):
-    """Returns the result of running func() n times."""
-    return [func() for _ in range(times)]
-
-
-def _DictWithReturnValues(retval, pass_fail):
-    """Create a new dictionary pre-populated with success/fail values."""
-    new_dict = {}
-    # Note: 0 is a valid retval; test to make sure it's not None.
-    if retval is not None:
-        new_dict["retval"] = retval
-    if pass_fail:
-        new_dict[""] = pass_fail
-    return new_dict
-
-
-def _GetNonDupLabel(max_dup, runs):
-    """Create new list for the runs of the same label.
-
-    Specifically, this will split out keys like foo{0}, foo{1} from one run into
-    their own runs. For example, given a run like:
-      {"foo": 1, "bar{0}": 2, "baz": 3, "qux{1}": 4, "pirate{0}": 5}
-
-    You'll get:
-      [{"foo": 1, "baz": 3}, {"bar": 2, "pirate": 5}, {"qux": 4}]
-
-    Hands back the lists of transformed runs, all concatenated together.
-    """
-    new_runs = []
-    for run in runs:
-        run_retval = run.get("retval", None)
-        run_pass_fail = run.get("", None)
-        new_run = {}
-        # pylint: disable=cell-var-from-loop
-        added_runs = _Repeat(
-            lambda: _DictWithReturnValues(run_retval, run_pass_fail), max_dup
-        )
-        for key, value in run.items():
-            match = _DUP_KEY_REGEX.match(key)
-            if not match:
-                new_run[key] = value
-            else:
-                new_key, index_str = match.groups()
-                added_runs[int(index_str) - 1][new_key] = str(value)
-        new_runs.append(new_run)
-        new_runs += added_runs
-    return new_runs
-
-
-def _DuplicatePass(result, benchmarks):
-    """Properly expands keys like `foo{1}` in `result`."""
-    for bench, data in result.items():
-        max_dup = _GetMaxDup(data)
-        # If there's nothing to expand, there's nothing to do.
-        if not max_dup:
-            continue
-        for i, runs in enumerate(data):
-            data[i] = _GetNonDupLabel(max_dup, runs)
-        _AdjustIteration(benchmarks, max_dup, bench)
-
-
-def _ReadSummaryFile(filename):
-    """Reads the summary file at filename."""
-    dirname, _ = misc.GetRoot(filename)
-    fullname = os.path.join(dirname, _TELEMETRY_RESULT_DEFAULTS_FILE)
-    try:
-        # Slurp the summary file into a dictionary. The keys in the dictionary are
-        # the benchmark names. The value for a key is a list containing the names
-        # of all the result fields that should be returned in a 'default' report.
-        with open(fullname) as in_file:
-            return json.load(in_file)
-    except IOError as e:
-        # ENOENT means "no such file or directory"
-        if e.errno == errno.ENOENT:
-            return {}
-        raise
-
-
-def _MakeOrganizeResultOutline(benchmark_runs, labels):
-    """Creates the "outline" of the OrganizeResults result for a set of runs.
-
-    Report generation returns lists of different sizes, depending on the input
-    data. Depending on the order in which we iterate through said input data, we
-    may populate the Nth index of a list, then the N-1st, then the N+Mth, ...
-
-    It's cleaner to figure out the "skeleton"/"outline" ahead of time, so we don't
-    have to worry about resizing while computing results.
-    """
-    # Count how many iterations exist for each benchmark run.
-    # We can't simply count up, since we may be given an incomplete set of
-    # iterations (e.g. [r.iteration for r in benchmark_runs] == [1, 3])
-    iteration_count = {}
-    for run in benchmark_runs:
-        name = run.benchmark.name
-        old_iterations = iteration_count.get(name, -1)
-        # N.B. run.iteration starts at 1, not 0.
-        iteration_count[name] = max(old_iterations, run.iteration)
-
-    # Result structure: {benchmark_name: [[{key: val}]]}
-    result = {}
-    for run in benchmark_runs:
-        name = run.benchmark.name
-        num_iterations = iteration_count[name]
-        # default param makes cros lint be quiet about defining num_iterations in a
-        # loop.
-        make_dicts = lambda n=num_iterations: _Repeat(dict, n)
-        result[name] = _Repeat(make_dicts, len(labels))
-    return result
-
-
-def OrganizeResults(benchmark_runs, labels, benchmarks=None, json_report=False):
-    """Create a dict from benchmark_runs.
-
-    The structure of the output dict is as follows:
-    {"benchmark_1":[
-      [{"key1":"v1", "key2":"v2"},{"key1":"v1", "key2","v2"}]
-      #one label
-      []
-      #the other label
-      ]
-     "benchmark_2":
-      [
-      ]}.
-    """
-    result = _MakeOrganizeResultOutline(benchmark_runs, labels)
-    label_names = [label.name for label in labels]
-    label_indices = {name: i for i, name in enumerate(label_names)}
-    summary_file = _ReadSummaryFile(sys.argv[0])
-
-    if benchmarks is None:
-        benchmarks = []
-
-    for benchmark_run in benchmark_runs:
-        if not benchmark_run.result:
-            continue
-        benchmark = benchmark_run.benchmark
-        label_index = label_indices[benchmark_run.label.name]
-        cur_label_list = result[benchmark.name][label_index]
-        cur_dict = cur_label_list[benchmark_run.iteration - 1]
-
-        show_all_results = json_report or benchmark.show_all_results
-        if not show_all_results:
-            summary_list = summary_file.get(benchmark.name)
-            if summary_list:
-                for key in benchmark_run.result.keyvals.keys():
-                    if any(
-                        key.startswith(added_key)
-                        for added_key in ["retval", "cpufreq", "cputemp"]
-                    ):
-                        summary_list.append(key)
-            else:
-                # Did not find test_name in json file; show everything.
-                show_all_results = True
-        if benchmark_run.result.cwp_dso:
-            # If we are in cwp approximation mode, we only care about samples
-            if "samples" in benchmark_run.result.keyvals:
-                cur_dict["samples"] = benchmark_run.result.keyvals["samples"]
-            cur_dict["retval"] = benchmark_run.result.keyvals["retval"]
-            for key, value in benchmark_run.result.keyvals.items():
-                if any(
-                    key.startswith(cpustat_keyword)
-                    for cpustat_keyword in ["cpufreq", "cputemp"]
-                ):
-                    cur_dict[key] = value
-        else:
-            for test_key in benchmark_run.result.keyvals:
-                if show_all_results or test_key in summary_list:
-                    cur_dict[test_key] = benchmark_run.result.keyvals[test_key]
-        # Occasionally Telemetry tests will not fail but they will not return a
-        # result, either.  Look for those cases, and force them to be a fail.
-        # (This can happen if, for example, the test has been disabled.)
-        if len(cur_dict) == 1 and cur_dict["retval"] == 0:
-            cur_dict["retval"] = 1
-            benchmark_run.result.keyvals["retval"] = 1
-            # TODO: This output should be sent via logger.
-            print(
-                "WARNING: Test '%s' appears to have succeeded but returned"
-                " no results." % benchmark.name,
-                file=sys.stderr,
-            )
-        if json_report and benchmark_run.machine:
-            cur_dict["machine"] = benchmark_run.machine.name
-            cur_dict["machine_checksum"] = benchmark_run.machine.checksum
-            cur_dict["machine_string"] = benchmark_run.machine.checksum_string
-    _DuplicatePass(result, benchmarks)
-    return result
diff --git a/crosperf/results_organizer_unittest.py b/crosperf/results_organizer_unittest.py
deleted file mode 100755
index f3db266d..00000000
--- a/crosperf/results_organizer_unittest.py
+++ /dev/null
@@ -1,163 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Testing of ResultsOrganizer
-
-   We create some labels, benchmark_runs and then create a ResultsOrganizer,
-   after that, we compare the result of ResultOrganizer.
-"""
-
-
-import unittest
-
-from benchmark_run import BenchmarkRun
-import mock_instance
-from results_cache import Result
-from results_organizer import OrganizeResults
-
-
-result = {
-    "benchmark1": [
-        [
-            {
-                "": "PASS",
-                "bool": "True",
-                "milliseconds_1": "1",
-                "milliseconds_2": "8",
-                "milliseconds_3": "9.2",
-                "ms_1": "2.1",
-                "total": "5",
-            },
-            {"": "PASS", "test": "2"},
-            {"": "PASS", "test": "4"},
-            {
-                "": "PASS",
-                "bool": "FALSE",
-                "milliseconds_1": "3",
-                "milliseconds_2": "5",
-                "ms_1": "2.2",
-                "total": "6",
-            },
-            {"": "PASS", "test": "3"},
-            {"": "PASS", "test": "4"},
-        ],
-        [
-            {
-                "": "PASS",
-                "bool": "FALSE",
-                "milliseconds_4": "30",
-                "milliseconds_5": "50",
-                "ms_1": "2.23",
-                "total": "6",
-            },
-            {"": "PASS", "test": "5"},
-            {"": "PASS", "test": "4"},
-            {
-                "": "PASS",
-                "bool": "FALSE",
-                "milliseconds_1": "3",
-                "milliseconds_6": "7",
-                "ms_1": "2.3",
-                "total": "7",
-            },
-            {"": "PASS", "test": "2"},
-            {"": "PASS", "test": "6"},
-        ],
-    ],
-    "benchmark2": [
-        [
-            {
-                "": "PASS",
-                "bool": "TRUE",
-                "milliseconds_1": "3",
-                "milliseconds_8": "6",
-                "ms_1": "2.3",
-                "total": "7",
-            },
-            {"": "PASS", "test": "2"},
-            {"": "PASS", "test": "6"},
-            {
-                "": "PASS",
-                "bool": "TRUE",
-                "milliseconds_1": "3",
-                "milliseconds_8": "6",
-                "ms_1": "2.2",
-                "total": "7",
-            },
-            {"": "PASS", "test": "2"},
-            {"": "PASS", "test": "2"},
-        ],
-        [
-            {
-                "": "PASS",
-                "bool": "TRUE",
-                "milliseconds_1": "3",
-                "milliseconds_8": "6",
-                "ms_1": "2",
-                "total": "7",
-            },
-            {"": "PASS", "test": "2"},
-            {"": "PASS", "test": "4"},
-            {
-                "": "PASS",
-                "bool": "TRUE",
-                "milliseconds_1": "3",
-                "milliseconds_8": "6",
-                "ms_1": "1",
-                "total": "7",
-            },
-            {"": "PASS", "test": "1"},
-            {"": "PASS", "test": "6"},
-        ],
-    ],
-}  # yapf: disable
-
-
-class ResultOrganizerTest(unittest.TestCase):
-    """Test result organizer."""
-
-    def testResultOrganizer(self):
-        labels = [mock_instance.label1, mock_instance.label2]
-        benchmarks = [mock_instance.benchmark1, mock_instance.benchmark2]
-        benchmark_runs = [None] * 8
-        benchmark_runs[0] = BenchmarkRun(
-            "b1", benchmarks[0], labels[0], 1, "", "", "", "average", "", {}
-        )
-        benchmark_runs[1] = BenchmarkRun(
-            "b2", benchmarks[0], labels[0], 2, "", "", "", "average", "", {}
-        )
-        benchmark_runs[2] = BenchmarkRun(
-            "b3", benchmarks[0], labels[1], 1, "", "", "", "average", "", {}
-        )
-        benchmark_runs[3] = BenchmarkRun(
-            "b4", benchmarks[0], labels[1], 2, "", "", "", "average", "", {}
-        )
-        benchmark_runs[4] = BenchmarkRun(
-            "b5", benchmarks[1], labels[0], 1, "", "", "", "average", "", {}
-        )
-        benchmark_runs[5] = BenchmarkRun(
-            "b6", benchmarks[1], labels[0], 2, "", "", "", "average", "", {}
-        )
-        benchmark_runs[6] = BenchmarkRun(
-            "b7", benchmarks[1], labels[1], 1, "", "", "", "average", "", {}
-        )
-        benchmark_runs[7] = BenchmarkRun(
-            "b8", benchmarks[1], labels[1], 2, "", "", "", "average", "", {}
-        )
-
-        i = 0
-        for b in benchmark_runs:
-            b.result = Result("", b.label, "average", "machine")
-            b.result.keyvals = mock_instance.keyval[i]
-            i += 1
-
-        organized = OrganizeResults(benchmark_runs, labels, benchmarks)
-        self.assertEqual(organized, result)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/results_report.py b/crosperf/results_report.py
deleted file mode 100644
index 045e623b..00000000
--- a/crosperf/results_report.py
+++ /dev/null
@@ -1,917 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""A module to handle the report format."""
-
-import datetime
-import functools
-import itertools
-import json
-import os
-import re
-import time
-
-from column_chart import ColumnChart
-from cros_utils.tabulator import AmeanResult
-from cros_utils.tabulator import Cell
-from cros_utils.tabulator import CoeffVarFormat
-from cros_utils.tabulator import CoeffVarResult
-from cros_utils.tabulator import Column
-from cros_utils.tabulator import Format
-from cros_utils.tabulator import GmeanRatioResult
-from cros_utils.tabulator import IterationResult
-from cros_utils.tabulator import LiteralResult
-from cros_utils.tabulator import MaxResult
-from cros_utils.tabulator import MinResult
-from cros_utils.tabulator import PValueFormat
-from cros_utils.tabulator import PValueResult
-from cros_utils.tabulator import RatioFormat
-from cros_utils.tabulator import RawResult
-from cros_utils.tabulator import SamplesTableGenerator
-from cros_utils.tabulator import StdResult
-from cros_utils.tabulator import TableFormatter
-from cros_utils.tabulator import TableGenerator
-from cros_utils.tabulator import TablePrinter
-from results_organizer import OrganizeResults
-import results_report_templates as templates
-from update_telemetry_defaults import TelemetryDefaults
-
-
-def ParseChromeosImage(chromeos_image):
-    """Parse the chromeos_image string for the image and version.
-
-  The chromeos_image string will probably be in one of two formats:
-  1: <path-to-chroot>/src/build/images/<board>/<ChromeOS-version>.<datetime>/ \
-     chromiumos_test_image.bin
-  2: <path-to-chroot>/chroot/tmp/<buildbot-build>/<ChromeOS-version>/ \
-      chromiumos_test_image.bin
-
-  We parse these strings to find the 'chromeos_version' to store in the
-  json archive (without the .datatime bit in the first case); and also
-  the 'chromeos_image', which would be all of the first case, but only the
-  part after '/chroot/tmp' in the second case.
-
-  Args:
-    chromeos_image: string containing the path to the chromeos_image that
-    crosperf used for the test.
-
-  Returns:
-    version, image: The results of parsing the input string, as explained
-    above.
-  """
-    # Find the Chromeos Version, e.g. R45-2345.0.0.....
-    # chromeos_image should have been something like:
-    # <path>/<board-trybot-release>/<chromeos-version>/chromiumos_test_image.bin"
-    if chromeos_image.endswith("/chromiumos_test_image.bin"):
-        full_version = chromeos_image.split("/")[-2]
-        # Strip the date and time off of local builds (which have the format
-        # "R43-2345.0.0.date-and-time").
-        version, _ = os.path.splitext(full_version)
-    else:
-        version = ""
-
-    # Find the chromeos image.  If it's somewhere in .../chroot/tmp/..., then
-    # it's an official image that got downloaded, so chop off the download path
-    # to make the official image name more clear.
-    official_image_path = "/chroot/tmp"
-    if official_image_path in chromeos_image:
-        image = chromeos_image.split(official_image_path, 1)[1]
-    else:
-        image = chromeos_image
-    return version, image
-
-
-def _AppendUntilLengthIs(gen, the_list, target_len):
-    """Appends to `list` until `list` is `target_len` elements long.
-
-    Uses `gen` to generate elements.
-    """
-    the_list.extend(gen() for _ in range(target_len - len(the_list)))
-    return the_list
-
-
-def _FilterPerfReport(event_threshold, report):
-    """Filters out entries with `< event_threshold` percent in a perf report."""
-
-    def filter_dict(m):
-        return {
-            fn_name: pct for fn_name, pct in m.items() if pct >= event_threshold
-        }
-
-    return {event: filter_dict(m) for event, m in report.items()}
-
-
-class _PerfTable(object):
-    """Generates dicts from a perf table.
-
-    Dicts look like:
-    {'benchmark_name': {'perf_event_name': [LabelData]}}
-    where LabelData is a list of perf dicts, each perf dict coming from the same
-    label.
-    Each perf dict looks like {'function_name': 0.10, ...} (where 0.10 is the
-    percentage of time spent in function_name).
-    """
-
-    def __init__(
-        self,
-        benchmark_names_and_iterations,
-        label_names,
-        read_perf_report,
-        event_threshold=None,
-    ):
-        """Constructor.
-
-        read_perf_report is a function that takes a label name, benchmark name, and
-        benchmark iteration, and returns a dictionary describing the perf output for
-        that given run.
-        """
-        self.event_threshold = event_threshold
-        self._label_indices = {name: i for i, name in enumerate(label_names)}
-        self.perf_data = {}
-        for label in label_names:
-            for bench_name, bench_iterations in benchmark_names_and_iterations:
-                for i in range(bench_iterations):
-                    report = read_perf_report(label, bench_name, i)
-                    self._ProcessPerfReport(report, label, bench_name, i)
-
-    def _ProcessPerfReport(self, perf_report, label, benchmark_name, iteration):
-        """Add the data from one run to the dict."""
-        perf_of_run = perf_report
-        if self.event_threshold is not None:
-            perf_of_run = _FilterPerfReport(self.event_threshold, perf_report)
-        if benchmark_name not in self.perf_data:
-            self.perf_data[benchmark_name] = {
-                event: [] for event in perf_of_run
-            }
-        ben_data = self.perf_data[benchmark_name]
-        label_index = self._label_indices[label]
-        for event in ben_data:
-            _AppendUntilLengthIs(list, ben_data[event], label_index + 1)
-            data_for_label = ben_data[event][label_index]
-            _AppendUntilLengthIs(dict, data_for_label, iteration + 1)
-            data_for_label[iteration] = (
-                perf_of_run[event] if perf_of_run else {}
-            )
-
-
-def _GetResultsTableHeader(ben_name, iterations):
-    benchmark_info = "Benchmark:  {0};  Iterations: {1}".format(
-        ben_name, iterations
-    )
-    cell = Cell()
-    cell.string_value = benchmark_info
-    cell.header = True
-    return [[cell]]
-
-
-def _GetDSOHeader(cwp_dso):
-    info = "CWP_DSO: %s" % cwp_dso
-    cell = Cell()
-    cell.string_value = info
-    cell.header = False
-    return [[cell]]
-
-
-def _ParseColumn(columns, iteration):
-    new_column = []
-    for column in columns:
-        if column.result.__class__.__name__ != "RawResult":
-            new_column.append(column)
-        else:
-            new_column.extend(
-                Column(LiteralResult(i), Format(), str(i + 1))
-                for i in range(iteration)
-            )
-    return new_column
-
-
-def _GetTables(benchmark_results, columns, table_type):
-    iter_counts = benchmark_results.iter_counts
-    result = benchmark_results.run_keyvals
-    tables = []
-    for bench_name, runs in result.items():
-        iterations = iter_counts[bench_name]
-        ben_table = _GetResultsTableHeader(bench_name, iterations)
-
-        all_runs_empty = all(not dict for label in runs for dict in label)
-        if all_runs_empty:
-            cell = Cell()
-            cell.string_value = (
-                "This benchmark contains no result."
-                " Is the benchmark name valid?"
-            )
-            cell_table = [[cell]]
-        else:
-            table = TableGenerator(
-                runs, benchmark_results.label_names
-            ).GetTable()
-            parsed_columns = _ParseColumn(columns, iterations)
-            tf = TableFormatter(table, parsed_columns)
-            cell_table = tf.GetCellTable(table_type)
-        tables.append(ben_table)
-        tables.append(cell_table)
-    return tables
-
-
-def _GetPerfTables(benchmark_results, columns, table_type):
-    p_table = _PerfTable(
-        benchmark_results.benchmark_names_and_iterations,
-        benchmark_results.label_names,
-        benchmark_results.read_perf_report,
-    )
-
-    tables = []
-    for benchmark in p_table.perf_data:
-        iterations = benchmark_results.iter_counts[benchmark]
-        ben_table = _GetResultsTableHeader(benchmark, iterations)
-        tables.append(ben_table)
-        benchmark_data = p_table.perf_data[benchmark]
-        table = []
-        for event in benchmark_data:
-            tg = TableGenerator(
-                benchmark_data[event],
-                benchmark_results.label_names,
-                sort=TableGenerator.SORT_BY_VALUES_DESC,
-            )
-            table = tg.GetTable(ResultsReport.PERF_ROWS)
-            parsed_columns = _ParseColumn(columns, iterations)
-            tf = TableFormatter(table, parsed_columns)
-            tf.GenerateCellTable(table_type)
-            tf.AddColumnName()
-            tf.AddLabelName()
-            tf.AddHeader(str(event))
-            table = tf.GetCellTable(table_type, headers=False)
-            tables.append(table)
-    return tables
-
-
-def _GetSamplesTables(benchmark_results, columns, table_type):
-    tables = []
-    dso_header_table = _GetDSOHeader(benchmark_results.cwp_dso)
-    tables.append(dso_header_table)
-    (table, new_keyvals, iter_counts) = SamplesTableGenerator(
-        benchmark_results.run_keyvals,
-        benchmark_results.label_names,
-        benchmark_results.iter_counts,
-        benchmark_results.weights,
-    ).GetTable()
-    parsed_columns = _ParseColumn(columns, 1)
-    tf = TableFormatter(table, parsed_columns, samples_table=True)
-    cell_table = tf.GetCellTable(table_type)
-    tables.append(cell_table)
-    return (tables, new_keyvals, iter_counts)
-
-
-class ResultsReport(object):
-    """Class to handle the report format."""
-
-    MAX_COLOR_CODE = 255
-    PERF_ROWS = 5
-
-    def __init__(self, results):
-        self.benchmark_results = results
-
-    def _GetTablesWithColumns(self, columns, table_type, summary_type):
-        if summary_type == "perf":
-            get_tables = _GetPerfTables
-        elif summary_type == "samples":
-            get_tables = _GetSamplesTables
-        else:
-            get_tables = _GetTables
-        ret = get_tables(self.benchmark_results, columns, table_type)
-        # If we are generating a samples summary table, the return value of
-        # get_tables will be a tuple, and we will update the benchmark_results for
-        # composite benchmark so that full table can use it.
-        if isinstance(ret, tuple):
-            self.benchmark_results.run_keyvals = ret[1]
-            self.benchmark_results.iter_counts = ret[2]
-            ret = ret[0]
-        return ret
-
-    def GetFullTables(self, perf=False):
-        ignore_min_max = self.benchmark_results.ignore_min_max
-        columns = [
-            Column(RawResult(), Format()),
-            Column(MinResult(), Format()),
-            Column(MaxResult(), Format()),
-            Column(AmeanResult(ignore_min_max), Format()),
-            Column(StdResult(ignore_min_max), Format(), "StdDev"),
-            Column(
-                CoeffVarResult(ignore_min_max), CoeffVarFormat(), "StdDev/Mean"
-            ),
-            Column(
-                GmeanRatioResult(ignore_min_max), RatioFormat(), "GmeanSpeedup"
-            ),
-            Column(PValueResult(ignore_min_max), PValueFormat(), "p-value"),
-        ]
-        return self._GetTablesWithColumns(columns, "full", perf)
-
-    def GetSummaryTables(self, summary_type=""):
-        ignore_min_max = self.benchmark_results.ignore_min_max
-        columns = []
-        if summary_type == "samples":
-            columns += [
-                Column(IterationResult(), Format(), "Iterations [Pass:Fail]")
-            ]
-        columns += [
-            Column(
-                AmeanResult(ignore_min_max),
-                Format(),
-                "Weighted Samples Amean" if summary_type == "samples" else "",
-            ),
-            Column(StdResult(ignore_min_max), Format(), "StdDev"),
-            Column(
-                CoeffVarResult(ignore_min_max), CoeffVarFormat(), "StdDev/Mean"
-            ),
-            Column(
-                GmeanRatioResult(ignore_min_max), RatioFormat(), "GmeanSpeedup"
-            ),
-            Column(PValueResult(ignore_min_max), PValueFormat(), "p-value"),
-        ]
-        return self._GetTablesWithColumns(columns, "summary", summary_type)
-
-
-def _PrintTable(tables, out_to):
-    # tables may be None.
-    if not tables:
-        return ""
-
-    if out_to == "HTML":
-        out_type = TablePrinter.HTML
-    elif out_to == "PLAIN":
-        out_type = TablePrinter.PLAIN
-    elif out_to == "CONSOLE":
-        out_type = TablePrinter.CONSOLE
-    elif out_to == "TSV":
-        out_type = TablePrinter.TSV
-    elif out_to == "EMAIL":
-        out_type = TablePrinter.EMAIL
-    else:
-        raise ValueError("Invalid out_to value: %s" % (out_to,))
-
-    printers = (TablePrinter(table, out_type) for table in tables)
-    return "".join(printer.Print() for printer in printers)
-
-
-class TextResultsReport(ResultsReport):
-    """Class to generate text result report."""
-
-    H1_STR = "==========================================="
-    H2_STR = "-------------------------------------------"
-
-    def __init__(self, results, email=False, experiment=None):
-        super(TextResultsReport, self).__init__(results)
-        self.email = email
-        self.experiment = experiment
-
-    @staticmethod
-    def _MakeTitle(title):
-        header_line = TextResultsReport.H1_STR
-        # '' at the end gives one newline.
-        return "\n".join([header_line, title, header_line, ""])
-
-    @staticmethod
-    def _MakeSection(title, body):
-        header_line = TextResultsReport.H2_STR
-        # '\n' at the end gives us two newlines.
-        return "\n".join([header_line, title, header_line, body, "\n"])
-
-    @staticmethod
-    def FromExperiment(experiment, email=False):
-        results = BenchmarkResults.FromExperiment(experiment)
-        return TextResultsReport(results, email, experiment)
-
-    def GetStatusTable(self):
-        """Generate the status table by the tabulator."""
-        table = [["", ""]]
-        columns = [
-            Column(LiteralResult(iteration=0), Format(), "Status"),
-            Column(LiteralResult(iteration=1), Format(), "Failing Reason"),
-        ]
-
-        for benchmark_run in self.experiment.benchmark_runs:
-            status = [
-                benchmark_run.name,
-                [
-                    benchmark_run.timeline.GetLastEvent(),
-                    benchmark_run.failure_reason,
-                ],
-            ]
-            table.append(status)
-        cell_table = TableFormatter(table, columns).GetCellTable("status")
-        return [cell_table]
-
-    def GetTotalWaitCooldownTime(self):
-        """Get cooldown wait time in seconds from experiment benchmark runs.
-
-        Returns:
-          Dictionary {'dut': int(wait_time_in_seconds)}
-        """
-        waittime_dict = {}
-        for dut in self.experiment.machine_manager.GetMachines():
-            waittime_dict[dut.name] = dut.GetCooldownWaitTime()
-        return waittime_dict
-
-    def GetReport(self):
-        """Generate the report for email and console."""
-        output_type = "EMAIL" if self.email else "CONSOLE"
-        experiment = self.experiment
-
-        sections = []
-        if experiment is not None:
-            title_contents = "Results report for '%s'" % (experiment.name,)
-        else:
-            title_contents = "Results report"
-        sections.append(self._MakeTitle(title_contents))
-
-        if not self.benchmark_results.cwp_dso:
-            summary_table = _PrintTable(self.GetSummaryTables(), output_type)
-        else:
-            summary_table = _PrintTable(
-                self.GetSummaryTables(summary_type="samples"), output_type
-            )
-        sections.append(self._MakeSection("Summary", summary_table))
-
-        if experiment is not None:
-            table = _PrintTable(self.GetStatusTable(), output_type)
-            sections.append(self._MakeSection("Benchmark Run Status", table))
-
-        if not self.benchmark_results.cwp_dso:
-            perf_table = _PrintTable(
-                self.GetSummaryTables(summary_type="perf"), output_type
-            )
-            sections.append(self._MakeSection("Perf Data", perf_table))
-
-        if experiment is not None:
-            experiment_file = experiment.experiment_file
-            sections.append(
-                self._MakeSection("Experiment File", experiment_file)
-            )
-
-            cpu_info = experiment.machine_manager.GetAllCPUInfo(
-                experiment.labels
-            )
-            sections.append(self._MakeSection("CPUInfo", cpu_info))
-
-            totaltime = (
-                (time.time() - experiment.start_time)
-                if experiment.start_time
-                else 0
-            )
-            totaltime_str = "Total experiment time:\n%d min" % (totaltime // 60)
-            cooldown_waittime_list = ["Cooldown wait time:"]
-            # When running experiment on multiple DUTs cooldown wait time may vary
-            # on different devices. In addition its combined time may exceed total
-            # experiment time which will look weird but it is reasonable.
-            # For this matter print cooldown time per DUT.
-            for dut, waittime in sorted(
-                self.GetTotalWaitCooldownTime().items()
-            ):
-                cooldown_waittime_list.append(
-                    "DUT %s: %d min" % (dut, waittime // 60)
-                )
-            cooldown_waittime_str = "\n".join(cooldown_waittime_list)
-            sections.append(
-                self._MakeSection(
-                    "Duration",
-                    "\n\n".join([totaltime_str, cooldown_waittime_str]),
-                )
-            )
-
-        return "\n".join(sections)
-
-
-def _GetHTMLCharts(label_names, test_results):
-    charts = []
-    for item, runs in test_results.items():
-        # Fun fact: label_names is actually *entirely* useless as a param, since we
-        # never add headers. We still need to pass it anyway.
-        table = TableGenerator(runs, label_names).GetTable()
-        columns = [
-            Column(AmeanResult(), Format()),
-            Column(MinResult(), Format()),
-            Column(MaxResult(), Format()),
-        ]
-        tf = TableFormatter(table, columns)
-        data_table = tf.GetCellTable("full", headers=False)
-
-        for cur_row_data in data_table:
-            test_key = cur_row_data[0].string_value
-            title = "{0}: {1}".format(item, test_key.replace("/", ""))
-            chart = ColumnChart(title, 300, 200)
-            chart.AddColumn("Label", "string")
-            chart.AddColumn("Average", "number")
-            chart.AddColumn("Min", "number")
-            chart.AddColumn("Max", "number")
-            chart.AddSeries("Min", "line", "black")
-            chart.AddSeries("Max", "line", "black")
-            cur_index = 1
-            for label in label_names:
-                chart.AddRow(
-                    [
-                        label,
-                        cur_row_data[cur_index].value,
-                        cur_row_data[cur_index + 1].value,
-                        cur_row_data[cur_index + 2].value,
-                    ]
-                )
-                if isinstance(cur_row_data[cur_index].value, str):
-                    chart = None
-                    break
-                cur_index += 3
-            if chart:
-                charts.append(chart)
-    return charts
-
-
-class HTMLResultsReport(ResultsReport):
-    """Class to generate html result report."""
-
-    def __init__(self, benchmark_results, experiment=None):
-        super(HTMLResultsReport, self).__init__(benchmark_results)
-        self.experiment = experiment
-
-    @staticmethod
-    def FromExperiment(experiment):
-        return HTMLResultsReport(
-            BenchmarkResults.FromExperiment(experiment), experiment=experiment
-        )
-
-    def GetReport(self):
-        label_names = self.benchmark_results.label_names
-        test_results = self.benchmark_results.run_keyvals
-        charts = _GetHTMLCharts(label_names, test_results)
-        chart_javascript = "".join(chart.GetJavascript() for chart in charts)
-        chart_divs = "".join(chart.GetDiv() for chart in charts)
-
-        if not self.benchmark_results.cwp_dso:
-            summary_table = self.GetSummaryTables()
-            perf_table = self.GetSummaryTables(summary_type="perf")
-        else:
-            summary_table = self.GetSummaryTables(summary_type="samples")
-            perf_table = None
-        full_table = self.GetFullTables()
-
-        experiment_file = ""
-        if self.experiment is not None:
-            experiment_file = self.experiment.experiment_file
-        # Use kwargs for code readability, and so that testing is a bit easier.
-        return templates.GenerateHTMLPage(
-            perf_table=perf_table,
-            chart_js=chart_javascript,
-            summary_table=summary_table,
-            print_table=_PrintTable,
-            chart_divs=chart_divs,
-            full_table=full_table,
-            experiment_file=experiment_file,
-        )
-
-
-def ParseStandardPerfReport(report_data):
-    """Parses the output of `perf report`.
-
-    It'll parse the following:
-    {{garbage}}
-    # Samples: 1234M of event 'foo'
-
-    1.23% command shared_object location function::name
-
-    1.22% command shared_object location function2::name
-
-    # Samples: 999K of event 'bar'
-
-    0.23% command shared_object location function3::name
-    {{etc.}}
-
-    Into:
-      {'foo': {'function::name': 1.23, 'function2::name': 1.22},
-       'bar': {'function3::name': 0.23, etc.}}
-    """
-    # This function fails silently on its if it's handed a string (as opposed to a
-    # list of lines). So, auto-split if we do happen to get a string.
-    if isinstance(report_data, str):
-        report_data = report_data.splitlines()
-    # When switching to python3 catch the case when bytes are passed.
-    elif isinstance(report_data, bytes):
-        raise TypeError()
-
-    # Samples: N{K,M,G} of event 'event-name'
-    samples_regex = re.compile(r"#\s+Samples: \d+\S? of event '([^']+)'")
-
-    # We expect lines like:
-    # N.NN%  command  samples  shared_object  [location] symbol
-    #
-    # Note that we're looking at stripped lines, so there is no space at the
-    # start.
-    perf_regex = re.compile(
-        r"^(\d+(?:.\d*)?)%"  # N.NN%
-        r"\s*\d+"  # samples count (ignored)
-        r"\s*\S+"  # command (ignored)
-        r"\s*\S+"  # shared_object (ignored)
-        r"\s*\[.\]"  # location (ignored)
-        r"\s*(\S.+)"  # function
-    )
-
-    stripped_lines = (l.strip() for l in report_data)
-    nonempty_lines = (l for l in stripped_lines if l)
-    # Ignore all lines before we see samples_regex
-    interesting_lines = itertools.dropwhile(
-        lambda x: not samples_regex.match(x), nonempty_lines
-    )
-
-    first_sample_line = next(interesting_lines, None)
-    # Went through the entire file without finding a 'samples' header. Quit.
-    if first_sample_line is None:
-        return {}
-
-    sample_name = samples_regex.match(first_sample_line).group(1)
-    current_result = {}
-    results = {sample_name: current_result}
-    for line in interesting_lines:
-        samples_match = samples_regex.match(line)
-        if samples_match:
-            sample_name = samples_match.group(1)
-            current_result = {}
-            results[sample_name] = current_result
-            continue
-
-        match = perf_regex.match(line)
-        if not match:
-            continue
-        percentage_str, func_name = match.groups()
-        try:
-            percentage = float(percentage_str)
-        except ValueError:
-            # Couldn't parse it; try to be "resilient".
-            continue
-        current_result[func_name] = percentage
-    return results
-
-
-def _ReadExperimentPerfReport(
-    results_directory, label_name, benchmark_name, benchmark_iteration
-):
-    """Reads a perf report for the given benchmark. Returns {} on failure.
-
-    The result should be a map of maps; it should look like:
-    {perf_event_name: {function_name: pct_time_spent}}, e.g.
-    {'cpu_cycles': {'_malloc': 10.0, '_free': 0.3, ...}}
-    """
-    raw_dir_name = label_name + benchmark_name + str(benchmark_iteration + 1)
-    dir_name = "".join(c for c in raw_dir_name if c.isalnum())
-    file_name = os.path.join(results_directory, dir_name, "perf.data.report.0")
-    try:
-        with open(file_name) as in_file:
-            return ParseStandardPerfReport(in_file)
-    except IOError:
-        # Yes, we swallow any IO-related errors.
-        return {}
-
-
-# Split out so that testing (specifically: mocking) is easier
-def _ExperimentToKeyvals(experiment, for_json_report):
-    """Converts an experiment to keyvals."""
-    return OrganizeResults(
-        experiment.benchmark_runs,
-        experiment.labels,
-        json_report=for_json_report,
-    )
-
-
-class BenchmarkResults(object):
-    """The minimum set of fields that any ResultsReport will take."""
-
-    def __init__(
-        self,
-        label_names,
-        benchmark_names_and_iterations,
-        run_keyvals,
-        ignore_min_max=False,
-        read_perf_report=None,
-        cwp_dso=None,
-        weights=None,
-    ):
-        if read_perf_report is None:
-
-            def _NoPerfReport(*_args, **_kwargs):
-                return {}
-
-            read_perf_report = _NoPerfReport
-
-        self.label_names = label_names
-        self.benchmark_names_and_iterations = benchmark_names_and_iterations
-        self.iter_counts = dict(benchmark_names_and_iterations)
-        self.run_keyvals = run_keyvals
-        self.ignore_min_max = ignore_min_max
-        self.read_perf_report = read_perf_report
-        self.cwp_dso = cwp_dso
-        self.weights = dict(weights) if weights else None
-
-    @staticmethod
-    def FromExperiment(experiment, for_json_report=False):
-        label_names = [label.name for label in experiment.labels]
-        benchmark_names_and_iterations = [
-            (benchmark.name, benchmark.iterations)
-            for benchmark in experiment.benchmarks
-        ]
-        run_keyvals = _ExperimentToKeyvals(experiment, for_json_report)
-        ignore_min_max = experiment.ignore_min_max
-        read_perf_report = functools.partial(
-            _ReadExperimentPerfReport, experiment.results_directory
-        )
-        cwp_dso = experiment.cwp_dso
-        weights = [
-            (benchmark.name, benchmark.weight)
-            for benchmark in experiment.benchmarks
-        ]
-        return BenchmarkResults(
-            label_names,
-            benchmark_names_and_iterations,
-            run_keyvals,
-            ignore_min_max,
-            read_perf_report,
-            cwp_dso,
-            weights,
-        )
-
-
-def _GetElemByName(name, from_list):
-    """Gets an element from the given list by its name field.
-
-    Raises an error if it doesn't find exactly one match.
-    """
-    elems = [e for e in from_list if e.name == name]
-    if len(elems) != 1:
-        raise ValueError(
-            "Expected 1 item named %s, found %d" % (name, len(elems))
-        )
-    return elems[0]
-
-
-def _Unlist(l):
-    """If l is a list, extracts the first element of l. Otherwise, returns l."""
-    return l[0] if isinstance(l, list) else l
-
-
-class JSONResultsReport(ResultsReport):
-    """Class that generates JSON reports for experiments."""
-
-    def __init__(
-        self,
-        benchmark_results,
-        benchmark_date=None,
-        benchmark_time=None,
-        experiment=None,
-        json_args=None,
-    ):
-        """Construct a JSONResultsReport.
-
-        json_args is the dict of arguments we pass to json.dumps in GetReport().
-        """
-        super(JSONResultsReport, self).__init__(benchmark_results)
-
-        defaults = TelemetryDefaults()
-        defaults.ReadDefaultsFile()
-        summary_field_defaults = defaults.GetDefault()
-        if summary_field_defaults is None:
-            summary_field_defaults = {}
-        self.summary_field_defaults = summary_field_defaults
-
-        if json_args is None:
-            json_args = {}
-        self.json_args = json_args
-
-        self.experiment = experiment
-        if not benchmark_date:
-            timestamp = datetime.datetime.strftime(
-                datetime.datetime.now(), "%Y-%m-%d %H:%M:%S"
-            )
-            benchmark_date, benchmark_time = timestamp.split(" ")
-        self.date = benchmark_date
-        self.time = benchmark_time
-
-    @staticmethod
-    def FromExperiment(
-        experiment, benchmark_date=None, benchmark_time=None, json_args=None
-    ):
-        benchmark_results = BenchmarkResults.FromExperiment(
-            experiment, for_json_report=True
-        )
-        return JSONResultsReport(
-            benchmark_results,
-            benchmark_date,
-            benchmark_time,
-            experiment,
-            json_args,
-        )
-
-    def GetReportObjectIgnoringExperiment(self):
-        """Gets the JSON report object specifically for the output data.
-
-        Ignores any experiment-specific fields (e.g. board, machine checksum, ...).
-        """
-        benchmark_results = self.benchmark_results
-        label_names = benchmark_results.label_names
-        summary_field_defaults = self.summary_field_defaults
-        final_results = []
-        for test, test_results in benchmark_results.run_keyvals.items():
-            for label_name, label_results in zip(label_names, test_results):
-                for iter_results in label_results:
-                    passed = iter_results.get("retval") == 0
-                    json_results = {
-                        "date": self.date,
-                        "time": self.time,
-                        "label": label_name,
-                        "test_name": test,
-                        "pass": passed,
-                    }
-                    final_results.append(json_results)
-
-                    if not passed:
-                        continue
-
-                    # Get overall results.
-                    summary_fields = summary_field_defaults.get(test)
-                    if summary_fields is not None:
-                        value = []
-                        json_results["overall_result"] = value
-                        for f in summary_fields:
-                            v = iter_results.get(f)
-                            if v is None:
-                                continue
-                            # New telemetry results format: sometimes we get a list of lists
-                            # now.
-                            v = _Unlist(_Unlist(v))
-                            value.append((f, float(v)))
-
-                    # Get detailed results.
-                    detail_results = {}
-                    json_results["detailed_results"] = detail_results
-                    for k, v in iter_results.items():
-                        if (
-                            k == "retval"
-                            or k == "PASS"
-                            or k == ["PASS"]
-                            or v == "PASS"
-                        ):
-                            continue
-
-                        v = _Unlist(v)
-                        if "machine" in k:
-                            json_results[k] = v
-                        elif v is not None:
-                            if isinstance(v, list):
-                                detail_results[k] = [float(d) for d in v]
-                            else:
-                                detail_results[k] = float(v)
-        return final_results
-
-    def GetReportObject(self):
-        """Generate the JSON report, returning it as a python object."""
-        report_list = self.GetReportObjectIgnoringExperiment()
-        if self.experiment is not None:
-            self._AddExperimentSpecificFields(report_list)
-        return report_list
-
-    def _AddExperimentSpecificFields(self, report_list):
-        """Add experiment-specific data to the JSON report."""
-        board = self.experiment.labels[0].board
-        manager = self.experiment.machine_manager
-        for report in report_list:
-            label_name = report["label"]
-            label = _GetElemByName(label_name, self.experiment.labels)
-
-            img_path = os.path.realpath(
-                os.path.expanduser(label.chromeos_image)
-            )
-            ver, img = ParseChromeosImage(img_path)
-
-            report.update(
-                {
-                    "board": board,
-                    "chromeos_image": img,
-                    "chromeos_version": ver,
-                    "chrome_version": label.chrome_version,
-                    "compiler": label.compiler,
-                }
-            )
-
-            if not report["pass"]:
-                continue
-            if "machine_checksum" not in report:
-                report["machine_checksum"] = manager.machine_checksum[
-                    label_name
-                ]
-            if "machine_string" not in report:
-                report["machine_string"] = manager.machine_checksum_string[
-                    label_name
-                ]
-
-    def GetReport(self):
-        """Dump the results of self.GetReportObject() to a string as JSON."""
-        # This exists for consistency with the other GetReport methods.
-        # Specifically, they all return strings, so it's a bit awkward if the JSON
-        # results reporter returns an object.
-        return json.dumps(self.GetReportObject(), **self.json_args)
diff --git a/crosperf/results_report_templates.py b/crosperf/results_report_templates.py
deleted file mode 100644
index 3ef9e74a..00000000
--- a/crosperf/results_report_templates.py
+++ /dev/null
@@ -1,226 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2016 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Text templates used by various parts of results_report."""
-
-import html
-from string import Template
-
-
-_TabMenuTemplate = Template(
-    """
-<div class='tab-menu'>
-  <a href="javascript:switchTab('$table_name', 'html')">HTML</a>
-  <a href="javascript:switchTab('$table_name', 'text')">Text</a>
-  <a href="javascript:switchTab('$table_name', 'tsv')">TSV</a>
-</div>"""
-)
-
-
-def _GetTabMenuHTML(table_name):
-    # N.B. cgi.escape does some very basic HTML escaping. Nothing more.
-    escaped = html.escape(table_name)
-    return _TabMenuTemplate.substitute(table_name=escaped)
-
-
-_ExperimentFileHTML = """
-<div class='results-section'>
-  <div class='results-section-title'>Experiment File</div>
-  <div class='results-section-content'>
-    <pre>%s</pre>
-</div>
-"""
-
-
-def _GetExperimentFileHTML(experiment_file_text):
-    if not experiment_file_text:
-        return ""
-    return _ExperimentFileHTML % (
-        html.escape(experiment_file_text, quote=False),
-    )
-
-
-_ResultsSectionHTML = Template(
-    """
-<div class='results-section'>
-  <div class='results-section-title'>$sect_name</div>
-  <div class='results-section-content'>
-    <div id='${short_name}-html'>$html_table</div>
-    <div id='${short_name}-text'><pre>$text_table</pre></div>
-    <div id='${short_name}-tsv'><pre>$tsv_table</pre></div>
-  </div>
-  $tab_menu
-</div>
-"""
-)
-
-
-def _GetResultsSectionHTML(print_table, table_name, data):
-    first_word = table_name.strip().split()[0]
-    short_name = first_word.lower()
-    return _ResultsSectionHTML.substitute(
-        sect_name=table_name,
-        html_table=print_table(data, "HTML"),
-        text_table=print_table(data, "PLAIN"),
-        tsv_table=print_table(data, "TSV"),
-        tab_menu=_GetTabMenuHTML(short_name),
-        short_name=short_name,
-    )
-
-
-_MainHTML = Template(
-    """
-<html>
-<head>
-  <style type="text/css">
-    body {
-      font-family: "Lucida Sans Unicode", "Lucida Grande", Sans-Serif;
-      font-size: 12px;
-    }
-
-    pre {
-      margin: 10px;
-      color: #039;
-      font-size: 14px;
-    }
-
-    .chart {
-      display: inline;
-    }
-
-    .hidden {
-      visibility: hidden;
-    }
-
-    .results-section {
-      border: 1px solid #b9c9fe;
-      margin: 10px;
-    }
-
-    .results-section-title {
-      background-color: #b9c9fe;
-      color: #039;
-      padding: 7px;
-      font-size: 14px;
-      width: 200px;
-    }
-
-    .results-section-content {
-      margin: 10px;
-      padding: 10px;
-      overflow:auto;
-    }
-
-    #box-table-a {
-      font-size: 12px;
-      width: 480px;
-      text-align: left;
-      border-collapse: collapse;
-    }
-
-    #box-table-a th {
-      padding: 6px;
-      background: #b9c9fe;
-      border-right: 1px solid #fff;
-      border-bottom: 1px solid #fff;
-      color: #039;
-      text-align: center;
-    }
-
-    #box-table-a td {
-      padding: 4px;
-      background: #e8edff;
-      border-bottom: 1px solid #fff;
-      border-right: 1px solid #fff;
-      color: #669;
-      border-top: 1px solid transparent;
-    }
-
-    #box-table-a tr:hover td {
-      background: #d0dafd;
-      color: #339;
-    }
-
-  </style>
-  <script type='text/javascript' src='https://www.google.com/jsapi'></script>
-  <script type='text/javascript'>
-    google.load('visualization', '1', {packages:['corechart']});
-    google.setOnLoadCallback(init);
-    function init() {
-      switchTab('summary', 'html');
-      ${perf_init};
-      switchTab('full', 'html');
-      drawTable();
-    }
-    function drawTable() {
-      ${chart_js};
-    }
-    function switchTab(table, tab) {
-      document.getElementById(table + '-html').style.display = 'none';
-      document.getElementById(table + '-text').style.display = 'none';
-      document.getElementById(table + '-tsv').style.display = 'none';
-      document.getElementById(table + '-' + tab).style.display = 'block';
-    }
-  </script>
-</head>
-
-<body>
-  $summary_table
-  $perf_html
-  <div class='results-section'>
-    <div class='results-section-title'>Charts</div>
-    <div class='results-section-content'>$chart_divs</div>
-  </div>
-  $full_table
-  $experiment_file
-</body>
-</html>
-"""
-)
-
-
-# It's a bit ugly that we take some HTML things, and some non-HTML things, but I
-# need to balance prettiness with time spent making things pretty.
-def GenerateHTMLPage(
-    perf_table,
-    chart_js,
-    summary_table,
-    print_table,
-    chart_divs,
-    full_table,
-    experiment_file,
-):
-    """Generates a crosperf HTML page from the given arguments.
-
-    print_table is a two-arg function called like: print_table(t, f)
-      t is one of [summary_table, print_table, full_table]; it's the table we want
-        to format.
-      f is one of ['TSV', 'HTML', 'PLAIN']; it's the type of format we want.
-    """
-    summary_table_html = _GetResultsSectionHTML(
-        print_table, "Summary Table", summary_table
-    )
-    if perf_table:
-        perf_html = _GetResultsSectionHTML(
-            print_table, "Perf Table", perf_table
-        )
-        perf_init = "switchTab('perf', 'html')"
-    else:
-        perf_html = ""
-        perf_init = ""
-
-    full_table_html = _GetResultsSectionHTML(
-        print_table, "Full Table", full_table
-    )
-    experiment_file_html = _GetExperimentFileHTML(experiment_file)
-    return _MainHTML.substitute(
-        perf_init=perf_init,
-        chart_js=chart_js,
-        summary_table=summary_table_html,
-        perf_html=perf_html,
-        chart_divs=chart_divs,
-        full_table=full_table_html,
-        experiment_file=experiment_file_html,
-    )
diff --git a/crosperf/results_report_unittest.py b/crosperf/results_report_unittest.py
deleted file mode 100755
index 4ce654d0..00000000
--- a/crosperf/results_report_unittest.py
+++ /dev/null
@@ -1,502 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Copyright 2016 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unittest for the results reporter."""
-
-
-import collections
-import io
-import os
-import unittest
-import unittest.mock as mock
-
-from benchmark_run import MockBenchmarkRun
-from cros_utils import logger
-from experiment_factory import ExperimentFactory
-from experiment_file import ExperimentFile
-from machine_manager import MockCrosMachine
-from machine_manager import MockMachineManager
-from results_cache import MockResult
-from results_report import BenchmarkResults
-from results_report import HTMLResultsReport
-from results_report import JSONResultsReport
-from results_report import ParseChromeosImage
-from results_report import ParseStandardPerfReport
-from results_report import TextResultsReport
-import test_flag
-
-
-class FreeFunctionsTest(unittest.TestCase):
-    """Tests for any free functions in results_report."""
-
-    def testParseChromeosImage(self):
-        # N.B. the cases with blank versions aren't explicitly supported by
-        # ParseChromeosImage. I'm not sure if they need to be supported, but the
-        # goal of this was to capture existing functionality as much as possible.
-        base_case = (
-            "/my/chroot/src/build/images/x86-generic/R01-1.0.date-time"
-            "/chromiumos_test_image.bin"
-        )
-        self.assertEqual(ParseChromeosImage(base_case), ("R01-1.0", base_case))
-
-        dir_base_case = os.path.dirname(base_case)
-        self.assertEqual(ParseChromeosImage(dir_base_case), ("", dir_base_case))
-
-        buildbot_case = (
-            "/my/chroot/chroot/tmp/buildbot-build/R02-1.0.date-time"
-            "/chromiumos_test_image.bin"
-        )
-        buildbot_img = buildbot_case.split("/chroot/tmp")[1]
-
-        self.assertEqual(
-            ParseChromeosImage(buildbot_case), ("R02-1.0", buildbot_img)
-        )
-        self.assertEqual(
-            ParseChromeosImage(os.path.dirname(buildbot_case)),
-            ("", os.path.dirname(buildbot_img)),
-        )
-
-        # Ensure we do something reasonable when giving paths that don't quite
-        # match the expected pattern.
-        fun_case = "/chromiumos_test_image.bin"
-        self.assertEqual(ParseChromeosImage(fun_case), ("", fun_case))
-
-        fun_case2 = "chromiumos_test_image.bin"
-        self.assertEqual(ParseChromeosImage(fun_case2), ("", fun_case2))
-
-
-# There are many ways for this to be done better, but the linter complains
-# about all of them (that I can think of, at least).
-_fake_path_number = [0]
-
-
-def FakePath(ext):
-    """Makes a unique path that shouldn't exist on the host system.
-
-    Each call returns a different path, so if said path finds its way into an
-    error message, it may be easier to track it to its source.
-    """
-    _fake_path_number[0] += 1
-    prefix = "/tmp/should/not/exist/%d/" % (_fake_path_number[0],)
-    return os.path.join(prefix, ext)
-
-
-def MakeMockExperiment(compiler="gcc"):
-    """Mocks an experiment using the given compiler."""
-    mock_experiment_file = io.StringIO(
-        """
-      board: x86-alex
-      remote: 127.0.0.1
-      locks_dir: /tmp
-      perf_args: record -a -e cycles
-      benchmark: PageCycler {
-        iterations: 3
-      }
-
-      image1 {
-        chromeos_image: %s
-      }
-
-      image2 {
-        remote: 127.0.0.2
-        chromeos_image: %s
-      }
-      """
-        % (FakePath("cros_image1.bin"), FakePath("cros_image2.bin"))
-    )
-    efile = ExperimentFile(mock_experiment_file)
-    experiment = ExperimentFactory().GetExperiment(
-        efile, FakePath("working_directory"), FakePath("log_dir")
-    )
-    for label in experiment.labels:
-        label.compiler = compiler
-    return experiment
-
-
-def _InjectSuccesses(experiment, how_many, keyvals, for_benchmark=0):
-    """Injects successful experiment runs (for each label) into the experiment."""
-    # Defensive copy of keyvals, so if it's modified, we'll know.
-    keyvals = dict(keyvals)
-    num_configs = len(experiment.benchmarks) * len(experiment.labels)
-    num_runs = len(experiment.benchmark_runs) // num_configs
-
-    # TODO(gbiv): Centralize the mocking of these, maybe? (It's also done in
-    # benchmark_run_unittest)
-    bench = experiment.benchmarks[for_benchmark]
-    cache_conditions = []
-    log_level = "average"
-    share_cache = ""
-    locks_dir = ""
-    log = logger.GetLogger()
-    machine_manager = MockMachineManager(
-        FakePath("chromeos_root"), 0, log_level, locks_dir
-    )
-    machine_manager.AddMachine("testing_machine")
-    machine = next(
-        m for m in machine_manager.GetMachines() if m.name == "testing_machine"
-    )
-
-    def MakeSuccessfulRun(n, label):
-        run = MockBenchmarkRun(
-            "mock_success%d" % (n,),
-            bench,
-            label,
-            1 + n + num_runs,
-            cache_conditions,
-            machine_manager,
-            log,
-            log_level,
-            share_cache,
-            {},
-        )
-        mock_result = MockResult(log, label, log_level, machine)
-        mock_result.keyvals = keyvals
-        run.result = mock_result
-        return run
-
-    for label in experiment.labels:
-        experiment.benchmark_runs.extend(
-            MakeSuccessfulRun(n, label) for n in range(how_many)
-        )
-    return experiment
-
-
-class TextResultsReportTest(unittest.TestCase):
-    """Tests that the output of a text report contains the things we pass in.
-
-    At the moment, this doesn't care deeply about the format in which said
-    things are displayed. It just cares that they're present.
-    """
-
-    def _checkReport(self, mock_getcooldown, email):
-        num_success = 2
-        success_keyvals = {"retval": 0, "machine": "some bot", "a_float": 3.96}
-        experiment = _InjectSuccesses(
-            MakeMockExperiment(), num_success, success_keyvals
-        )
-        SECONDS_IN_MIN = 60
-        mock_getcooldown.return_value = {
-            experiment.remote[0]: 12 * SECONDS_IN_MIN,
-            experiment.remote[1]: 8 * SECONDS_IN_MIN,
-        }
-
-        text_report = TextResultsReport.FromExperiment(
-            experiment, email=email
-        ).GetReport()
-        self.assertIn(str(success_keyvals["a_float"]), text_report)
-        self.assertIn(success_keyvals["machine"], text_report)
-        self.assertIn(MockCrosMachine.CPUINFO_STRING, text_report)
-        self.assertIn("\nDuration\n", text_report)
-        self.assertIn("Total experiment time:\n", text_report)
-        self.assertIn("Cooldown wait time:\n", text_report)
-        self.assertIn(
-            "DUT %s: %d min" % (experiment.remote[0], 12), text_report
-        )
-        self.assertIn("DUT %s: %d min" % (experiment.remote[1], 8), text_report)
-        return text_report
-
-    @mock.patch.object(TextResultsReport, "GetTotalWaitCooldownTime")
-    def testOutput(self, mock_getcooldown):
-        email_report = self._checkReport(mock_getcooldown, email=True)
-        text_report = self._checkReport(mock_getcooldown, email=False)
-
-        # Ensure that the reports somehow different. Otherwise, having the
-        # distinction is useless.
-        self.assertNotEqual(email_report, text_report)
-
-    def test_get_totalwait_cooldowntime(self):
-        experiment = MakeMockExperiment()
-        cros_machines = experiment.machine_manager.GetMachines()
-        cros_machines[0].AddCooldownWaitTime(120)
-        cros_machines[1].AddCooldownWaitTime(240)
-        text_results = TextResultsReport.FromExperiment(experiment, email=False)
-        total = text_results.GetTotalWaitCooldownTime()
-        self.assertEqual(total[experiment.remote[0]], 120)
-        self.assertEqual(total[experiment.remote[1]], 240)
-
-
-class HTMLResultsReportTest(unittest.TestCase):
-    """Tests that the output of a HTML report contains the things we pass in.
-
-    At the moment, this doesn't care deeply about the format in which said
-    things are displayed. It just cares that they're present.
-    """
-
-    _TestOutput = collections.namedtuple(
-        "TestOutput",
-        [
-            "summary_table",
-            "perf_html",
-            "chart_js",
-            "charts",
-            "full_table",
-            "experiment_file",
-        ],
-    )
-
-    @staticmethod
-    def _GetTestOutput(
-        perf_table,
-        chart_js,
-        summary_table,
-        print_table,
-        chart_divs,
-        full_table,
-        experiment_file,
-    ):
-        # N.B. Currently we don't check chart_js; it's just passed through because
-        # cros lint complains otherwise.
-        summary_table = print_table(summary_table, "HTML")
-        perf_html = print_table(perf_table, "HTML")
-        full_table = print_table(full_table, "HTML")
-        return HTMLResultsReportTest._TestOutput(
-            summary_table=summary_table,
-            perf_html=perf_html,
-            chart_js=chart_js,
-            charts=chart_divs,
-            full_table=full_table,
-            experiment_file=experiment_file,
-        )
-
-    def _GetOutput(self, experiment=None, benchmark_results=None):
-        with mock.patch("results_report_templates.GenerateHTMLPage") as standin:
-            if experiment is not None:
-                HTMLResultsReport.FromExperiment(experiment).GetReport()
-            else:
-                HTMLResultsReport(benchmark_results).GetReport()
-            mod_mock = standin
-        self.assertEqual(mod_mock.call_count, 1)
-        # call_args[0] is positional args, call_args[1] is kwargs.
-        self.assertEqual(mod_mock.call_args[0], tuple())
-        fmt_args = mod_mock.call_args[1]
-        return self._GetTestOutput(**fmt_args)
-
-    def testNoSuccessOutput(self):
-        output = self._GetOutput(MakeMockExperiment())
-        self.assertIn("no result", output.summary_table)
-        self.assertIn("no result", output.full_table)
-        self.assertEqual(output.charts, "")
-        self.assertNotEqual(output.experiment_file, "")
-
-    def testSuccessfulOutput(self):
-        num_success = 2
-        success_keyvals = {"retval": 0, "a_float": 3.96}
-        output = self._GetOutput(
-            _InjectSuccesses(MakeMockExperiment(), num_success, success_keyvals)
-        )
-
-        self.assertNotIn("no result", output.summary_table)
-        # self.assertIn(success_keyvals['machine'], output.summary_table)
-        self.assertIn("a_float", output.summary_table)
-        self.assertIn(str(success_keyvals["a_float"]), output.summary_table)
-        self.assertIn("a_float", output.full_table)
-        # The _ in a_float is filtered out when we're generating HTML.
-        self.assertIn("afloat", output.charts)
-        # And make sure we have our experiment file...
-        self.assertNotEqual(output.experiment_file, "")
-
-    def testBenchmarkResultFailure(self):
-        labels = ["label1"]
-        benchmark_names_and_iterations = [("bench1", 1)]
-        benchmark_keyvals = {"bench1": [[]]}
-        results = BenchmarkResults(
-            labels, benchmark_names_and_iterations, benchmark_keyvals
-        )
-        output = self._GetOutput(benchmark_results=results)
-        self.assertIn("no result", output.summary_table)
-        self.assertEqual(output.charts, "")
-        self.assertEqual(output.experiment_file, "")
-
-    def testBenchmarkResultSuccess(self):
-        labels = ["label1"]
-        benchmark_names_and_iterations = [("bench1", 1)]
-        benchmark_keyvals = {"bench1": [[{"retval": 1, "foo": 2.0}]]}
-        results = BenchmarkResults(
-            labels, benchmark_names_and_iterations, benchmark_keyvals
-        )
-        output = self._GetOutput(benchmark_results=results)
-        self.assertNotIn("no result", output.summary_table)
-        self.assertIn("bench1", output.summary_table)
-        self.assertIn("bench1", output.full_table)
-        self.assertNotEqual(output.charts, "")
-        self.assertEqual(output.experiment_file, "")
-
-
-class JSONResultsReportTest(unittest.TestCase):
-    """Tests JSONResultsReport."""
-
-    REQUIRED_REPORT_KEYS = ("date", "time", "label", "test_name", "pass")
-    EXPERIMENT_REPORT_KEYS = (
-        "board",
-        "chromeos_image",
-        "chromeos_version",
-        "chrome_version",
-        "compiler",
-    )
-
-    @staticmethod
-    def _GetRequiredKeys(is_experiment):
-        required_keys = JSONResultsReportTest.REQUIRED_REPORT_KEYS
-        if is_experiment:
-            required_keys += JSONResultsReportTest.EXPERIMENT_REPORT_KEYS
-        return required_keys
-
-    def _CheckRequiredKeys(self, test_output, is_experiment):
-        required_keys = self._GetRequiredKeys(is_experiment)
-        for output in test_output:
-            for key in required_keys:
-                self.assertIn(key, output)
-
-    def testAllFailedJSONReportOutput(self):
-        experiment = MakeMockExperiment()
-        results = JSONResultsReport.FromExperiment(experiment).GetReportObject()
-        self._CheckRequiredKeys(results, is_experiment=True)
-        # Nothing succeeded; we don't send anything more than what's required.
-        required_keys = self._GetRequiredKeys(is_experiment=True)
-        for result in results:
-            self.assertCountEqual(result.keys(), required_keys)
-
-    def testJSONReportOutputWithSuccesses(self):
-        success_keyvals = {
-            "retval": 0,
-            "a_float": "2.3",
-            "many_floats": [["1.0", "2.0"], ["3.0"]],
-            "machine": "i'm a pirate",
-        }
-
-        # 2 is arbitrary.
-        num_success = 2
-        experiment = _InjectSuccesses(
-            MakeMockExperiment(), num_success, success_keyvals
-        )
-        results = JSONResultsReport.FromExperiment(experiment).GetReportObject()
-        self._CheckRequiredKeys(results, is_experiment=True)
-
-        num_passes = num_success * len(experiment.labels)
-        non_failures = [r for r in results if r["pass"]]
-        self.assertEqual(num_passes, len(non_failures))
-
-        # TODO(gbiv): ...Is the 3.0 *actually* meant to be dropped?
-        expected_detailed = {"a_float": 2.3, "many_floats": [1.0, 2.0]}
-        for pass_ in non_failures:
-            self.assertIn("detailed_results", pass_)
-            self.assertDictEqual(expected_detailed, pass_["detailed_results"])
-            self.assertIn("machine", pass_)
-            self.assertEqual(success_keyvals["machine"], pass_["machine"])
-
-    def testFailedJSONReportOutputWithoutExperiment(self):
-        labels = ["label1"]
-        # yapf:disable
-        benchmark_names_and_iterations = [
-            ("bench1", 1),
-            ("bench2", 2),
-            ("bench3", 1),
-            ("bench4", 0),
-        ]
-        # yapf:enable
-
-        benchmark_keyvals = {
-            "bench1": [[{"retval": 1, "foo": 2.0}]],
-            "bench2": [[{"retval": 1, "foo": 4.0}, {"retval": -1, "bar": 999}]],
-            # lack of retval is considered a failure.
-            "bench3": [[{}]],
-            "bench4": [[]],
-        }
-        bench_results = BenchmarkResults(
-            labels, benchmark_names_and_iterations, benchmark_keyvals
-        )
-        results = JSONResultsReport(bench_results).GetReportObject()
-        self._CheckRequiredKeys(results, is_experiment=False)
-        self.assertFalse(any(r["pass"] for r in results))
-
-    def testJSONGetReportObeysJSONSettings(self):
-        labels = ["label1"]
-        benchmark_names_and_iterations = [("bench1", 1)]
-        # These can be anything, really. So long as they're distinctive.
-        separators = (",\t\n\t", ":\t\n\t")
-        benchmark_keyvals = {"bench1": [[{"retval": 0, "foo": 2.0}]]}
-        bench_results = BenchmarkResults(
-            labels, benchmark_names_and_iterations, benchmark_keyvals
-        )
-        reporter = JSONResultsReport(
-            bench_results, json_args={"separators": separators}
-        )
-        result_str = reporter.GetReport()
-        self.assertIn(separators[0], result_str)
-        self.assertIn(separators[1], result_str)
-
-    def testSuccessfulJSONReportOutputWithoutExperiment(self):
-        labels = ["label1"]
-        benchmark_names_and_iterations = [("bench1", 1), ("bench2", 2)]
-        benchmark_keyvals = {
-            "bench1": [[{"retval": 0, "foo": 2.0}]],
-            "bench2": [[{"retval": 0, "foo": 4.0}, {"retval": 0, "bar": 999}]],
-        }
-        bench_results = BenchmarkResults(
-            labels, benchmark_names_and_iterations, benchmark_keyvals
-        )
-        results = JSONResultsReport(bench_results).GetReportObject()
-        self._CheckRequiredKeys(results, is_experiment=False)
-        self.assertTrue(all(r["pass"] for r in results))
-        # Enforce that the results have *some* deterministic order.
-        keyfn = lambda r: (
-            r["test_name"],
-            r["detailed_results"].get("foo", 5.0),
-        )
-        sorted_results = sorted(results, key=keyfn)
-        detailed_results = [r["detailed_results"] for r in sorted_results]
-        bench1, bench2_foo, bench2_bar = detailed_results
-        self.assertEqual(bench1["foo"], 2.0)
-        self.assertEqual(bench2_foo["foo"], 4.0)
-        self.assertEqual(bench2_bar["bar"], 999)
-        self.assertNotIn("bar", bench1)
-        self.assertNotIn("bar", bench2_foo)
-        self.assertNotIn("foo", bench2_bar)
-
-
-class PerfReportParserTest(unittest.TestCase):
-    """Tests for the perf report parser in results_report."""
-
-    @staticmethod
-    def _ReadRealPerfReport():
-        my_dir = os.path.dirname(os.path.realpath(__file__))
-        with open(os.path.join(my_dir, "perf_files/perf.data.report.0")) as f:
-            return f.read()
-
-    def testParserParsesRealWorldPerfReport(self):
-        report = ParseStandardPerfReport(self._ReadRealPerfReport())
-        self.assertCountEqual(["cycles", "instructions"], list(report.keys()))
-
-        # Arbitrarily selected known percentages from the perf report.
-        known_cycles_percentages = {
-            "0xffffffffa4a1f1c9": 0.66,
-            "0x0000115bb7ba9b54": 0.47,
-            "0x0000000000082e08": 0.00,
-            "0xffffffffa4a13e63": 0.00,
-        }
-        report_cycles = report["cycles"]
-        self.assertEqual(len(report_cycles), 214)
-        for k, v in known_cycles_percentages.items():
-            self.assertIn(k, report_cycles)
-            self.assertEqual(v, report_cycles[k])
-
-        known_instrunctions_percentages = {
-            "0x0000115bb6c35d7a": 1.65,
-            "0x0000115bb7ba9b54": 0.67,
-            "0x0000000000024f56": 0.00,
-            "0xffffffffa4a0ee03": 0.00,
-        }
-        report_instructions = report["instructions"]
-        self.assertEqual(len(report_instructions), 492)
-        for k, v in known_instrunctions_percentages.items():
-            self.assertIn(k, report_instructions)
-            self.assertEqual(v, report_instructions[k])
-
-
-if __name__ == "__main__":
-    test_flag.SetTestMode(True)
-    unittest.main()
diff --git a/crosperf/run_tests.sh b/crosperf/run_tests.sh
deleted file mode 100755
index b3d4d1e2..00000000
--- a/crosperf/run_tests.sh
+++ /dev/null
@@ -1,6 +0,0 @@
-#!/bin/bash
-#
-# Copyright 2011 Google LLC
-# Author: raymes@google.com (Raymes Khoury)
-
-../run_tests_for.py .
diff --git a/crosperf/schedv2.py b/crosperf/schedv2.py
deleted file mode 100644
index 828b8b81..00000000
--- a/crosperf/schedv2.py
+++ /dev/null
@@ -1,479 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2015 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Module to optimize the scheduling of benchmark_run tasks."""
-
-
-from collections import defaultdict
-import sys
-from threading import Lock
-from threading import Thread
-import traceback
-
-from cros_utils import command_executer
-from cros_utils import logger
-from machine_image_manager import MachineImageManager
-import test_flag
-
-
-class DutWorker(Thread):
-    """Working thread for a dut."""
-
-    def __init__(self, dut, sched):
-        super(DutWorker, self).__init__(name="DutWorker-{}".format(dut.name))
-        self._dut = dut
-        self._sched = sched
-        self._stat_num_br_run = 0
-        self._stat_num_reimage = 0
-        self._stat_annotation = ""
-        self._logger = logger.GetLogger(self._sched.get_experiment().log_dir)
-        self.daemon = True
-        self._terminated = False
-        self._active_br = None
-        # Race condition accessing _active_br between _execute_benchmark_run and
-        # _terminate, so lock it up.
-        self._active_br_lock = Lock()
-
-    def terminate(self):
-        self._terminated = True
-        with self._active_br_lock:
-            if self._active_br is not None:
-                # BenchmarkRun.Terminate() terminates any running testcase via
-                # suite_runner.Terminate and updates timeline.
-                self._active_br.Terminate()
-
-    def run(self):
-        """Do the "run-test->(optionally reimage)->run-test" chore.
-
-        Note - 'br' below means 'benchmark_run'.
-        """
-
-        # Firstly, handle benchmarkruns that have cache hit.
-        br = self._sched.get_cached_benchmark_run()
-        while br:
-            try:
-                self._stat_annotation = "finishing cached {}".format(br)
-                br.run()
-            except RuntimeError:
-                traceback.print_exc(file=sys.stdout)
-            br = self._sched.get_cached_benchmark_run()
-
-        # Secondly, handle benchmarkruns that needs to be run on dut.
-        self._setup_dut_label()
-        try:
-            self._logger.LogOutput("{} started.".format(self))
-            while not self._terminated:
-                br = self._sched.get_benchmark_run(self._dut)
-                if br is None:
-                    # No br left for this label. Considering reimaging.
-                    label = self._sched.allocate_label(self._dut)
-                    if label is None:
-                        # No br even for other labels. We are done.
-                        self._logger.LogOutput(
-                            "ImageManager found no label "
-                            "for dut, stopping working "
-                            "thread {}.".format(self)
-                        )
-                        break
-                    if self._reimage(label):
-                        # Reimage to run other br fails, dut is doomed, stop
-                        # this thread.
-                        self._logger.LogWarning(
-                            "Re-image failed, dut "
-                            "in an unstable state, stopping "
-                            "working thread {}.".format(self)
-                        )
-                        break
-                else:
-                    # Execute the br.
-                    self._execute_benchmark_run(br)
-        finally:
-            self._stat_annotation = "finished"
-            # Thread finishes. Notify scheduler that I'm done.
-            self._sched.dut_worker_finished(self)
-
-    def _reimage(self, label):
-        """Reimage image to label.
-
-        Args:
-          label: the label to remimage onto dut.
-
-        Returns:
-          0 if successful, otherwise 1.
-        """
-
-        # Termination could happen anywhere, check it.
-        if self._terminated:
-            return 1
-
-        if self._sched.get_experiment().crosfleet:
-            self._logger.LogOutput(
-                "Crosfleet mode, do not image before testing."
-            )
-            self._dut.label = label
-            return 0
-
-        self._logger.LogOutput("Reimaging {} using {}".format(self, label))
-        self._stat_num_reimage += 1
-        self._stat_annotation = 'reimaging using "{}"'.format(label.name)
-        try:
-            # Note, only 1 reimage at any given time, this is guaranteed in
-            # ImageMachine, so no sync needed below.
-            retval = self._sched.get_experiment().machine_manager.ImageMachine(
-                self._dut, label
-            )
-
-            if retval:
-                return 1
-        except RuntimeError:
-            return 1
-
-        self._dut.label = label
-        return 0
-
-    def _execute_benchmark_run(self, br):
-        """Execute a single benchmark_run.
-
-        Note - this function never throws exceptions.
-        """
-
-        # Termination could happen anywhere, check it.
-        if self._terminated:
-            return
-
-        self._logger.LogOutput("{} started working on {}".format(self, br))
-        self._stat_num_br_run += 1
-        self._stat_annotation = "executing {}".format(br)
-        # benchmark_run.run does not throws, but just play it safe here.
-        try:
-            assert br.owner_thread is None
-            br.owner_thread = self
-            with self._active_br_lock:
-                self._active_br = br
-            br.run()
-        finally:
-            self._sched.get_experiment().BenchmarkRunFinished(br)
-            with self._active_br_lock:
-                self._active_br = None
-
-    def _setup_dut_label(self):
-        """Try to match dut image with a certain experiment label.
-
-        If such match is found, we just skip doing reimage and jump to execute
-        some benchmark_runs.
-        """
-
-        checksum_file = "/usr/local/osimage_checksum_file"
-        try:
-            (
-                rv,
-                checksum,
-                _,
-            ) = command_executer.GetCommandExecuter().CrosRunCommandWOutput(
-                "cat " + checksum_file,
-                chromeos_root=self._sched.get_labels(0).chromeos_root,
-                machine=self._dut.name,
-                print_to_console=False,
-            )
-            if rv == 0:
-                checksum = checksum.strip()
-                for l in self._sched.get_labels():
-                    if l.checksum == checksum:
-                        self._logger.LogOutput(
-                            "Dut '{}' is pre-installed with '{}'".format(
-                                self._dut.name, l
-                            )
-                        )
-                        self._dut.label = l
-                        return
-        except RuntimeError:
-            traceback.print_exc(file=sys.stdout)
-            self._dut.label = None
-
-    def __str__(self):
-        return 'DutWorker[dut="{}", label="{}"]'.format(
-            self._dut.name, self._dut.label.name if self._dut.label else "None"
-        )
-
-    def dut(self):
-        return self._dut
-
-    def status_str(self):
-        """Report thread status."""
-
-        return (
-            'Worker thread "{}", label="{}", benchmark_run={}, '
-            "reimage={}, now {}".format(
-                self._dut.name,
-                "None" if self._dut.label is None else self._dut.label.name,
-                self._stat_num_br_run,
-                self._stat_num_reimage,
-                self._stat_annotation,
-            )
-        )
-
-
-class BenchmarkRunCacheReader(Thread):
-    """The thread to read cache for a list of benchmark_runs.
-
-    On creation, each instance of this class is given a br_list, which is a
-    subset of experiment._benchmark_runs.
-    """
-
-    def __init__(self, schedv2, br_list):
-        super(BenchmarkRunCacheReader, self).__init__()
-        self._schedv2 = schedv2
-        self._br_list = br_list
-        self._logger = self._schedv2.get_logger()
-
-    def run(self):
-        for br in self._br_list:
-            try:
-                br.ReadCache()
-                if br.cache_hit:
-                    self._logger.LogOutput("Cache hit - {}".format(br))
-                    with self._schedv2.lock_on("_cached_br_list"):
-                        self._schedv2.get_cached_run_list().append(br)
-                else:
-                    self._logger.LogOutput("Cache not hit - {}".format(br))
-            except RuntimeError:
-                traceback.print_exc(file=sys.stderr)
-
-
-class Schedv2(object):
-    """New scheduler for crosperf."""
-
-    def __init__(self, experiment):
-        self._experiment = experiment
-        self._logger = logger.GetLogger(experiment.log_dir)
-
-        # Create shortcuts to nested data structure. "_duts" points to a list of
-        # locked machines. _labels points to a list of all labels.
-        self._duts = self._experiment.machine_manager.GetMachines()
-        self._labels = self._experiment.labels
-
-        # Bookkeeping for synchronization.
-        self._workers_lock = Lock()
-        # pylint: disable=unnecessary-lambda
-        self._lock_map = defaultdict(lambda: Lock())
-
-        # Test mode flag
-        self._in_test_mode = test_flag.GetTestMode()
-
-        # Read benchmarkrun cache.
-        self._read_br_cache()
-
-        # Mapping from label to a list of benchmark_runs.
-        self._label_brl_map = dict((l, []) for l in self._labels)
-        for br in self._experiment.benchmark_runs:
-            assert br.label in self._label_brl_map
-            # Only put no-cache-hit br into the map.
-            if br not in self._cached_br_list:
-                self._label_brl_map[br.label].append(br)
-
-        # Use machine image manager to calculate initial label allocation.
-        self._mim = MachineImageManager(self._labels, self._duts)
-        self._mim.compute_initial_allocation()
-
-        # Create worker thread, 1 per dut.
-        self._active_workers = [DutWorker(dut, self) for dut in self._duts]
-        self._finished_workers = []
-
-        # Termination flag.
-        self._terminated = False
-
-    def run_sched(self):
-        """Start all dut worker threads and return immediately."""
-
-        for w in self._active_workers:
-            w.start()
-
-    def _read_br_cache(self):
-        """Use multi-threading to read cache for all benchmarkruns.
-
-        We do this by firstly creating a few threads, and then assign each
-        thread a segment of all brs. Each thread will check cache status for
-        each br and put those with cache into '_cached_br_list'.
-        """
-
-        self._cached_br_list = []
-        n_benchmarkruns = len(self._experiment.benchmark_runs)
-        if n_benchmarkruns <= 4:
-            # Use single thread to read cache.
-            self._logger.LogOutput(
-                (
-                    "Starting to read cache status for " "{} benchmark runs ..."
-                ).format(n_benchmarkruns)
-            )
-            BenchmarkRunCacheReader(self, self._experiment.benchmark_runs).run()
-            return
-
-        # Split benchmarkruns set into segments. Each segment will be handled by
-        # a thread. Note, we use (x+3)/4 to mimic math.ceil(x/4).
-        n_threads = max(2, min(20, (n_benchmarkruns + 3) // 4))
-        self._logger.LogOutput(
-            (
-                "Starting {} threads to read cache status for "
-                "{} benchmark runs ..."
-            ).format(n_threads, n_benchmarkruns)
-        )
-        benchmarkruns_per_thread = (
-            n_benchmarkruns + n_threads - 1
-        ) // n_threads
-        benchmarkrun_segments = []
-        for i in range(n_threads - 1):
-            start = i * benchmarkruns_per_thread
-            end = (i + 1) * benchmarkruns_per_thread
-            benchmarkrun_segments.append(
-                self._experiment.benchmark_runs[start:end]
-            )
-        benchmarkrun_segments.append(
-            self._experiment.benchmark_runs[
-                (n_threads - 1) * benchmarkruns_per_thread :
-            ]
-        )
-
-        # Assert: aggregation of benchmarkrun_segments equals to benchmark_runs.
-        assert sum(len(x) for x in benchmarkrun_segments) == n_benchmarkruns
-
-        # Create and start all readers.
-        cache_readers = [
-            BenchmarkRunCacheReader(self, x) for x in benchmarkrun_segments
-        ]
-
-        for x in cache_readers:
-            x.start()
-
-        # Wait till all readers finish.
-        for x in cache_readers:
-            x.join()
-
-        # Summarize.
-        self._logger.LogOutput(
-            "Total {} cache hit out of {} benchmark_runs.".format(
-                len(self._cached_br_list), n_benchmarkruns
-            )
-        )
-
-    def get_cached_run_list(self):
-        return self._cached_br_list
-
-    def get_label_map(self):
-        return self._label_brl_map
-
-    def get_experiment(self):
-        return self._experiment
-
-    def get_labels(self, i=None):
-        if i is None:
-            return self._labels
-        return self._labels[i]
-
-    def get_logger(self):
-        return self._logger
-
-    def get_cached_benchmark_run(self):
-        """Get a benchmark_run with 'cache hit'.
-
-        Returns:
-          The benchmark that has cache hit, if any. Otherwise none.
-        """
-
-        with self.lock_on("_cached_br_list"):
-            if self._cached_br_list:
-                return self._cached_br_list.pop()
-            return None
-
-    def get_benchmark_run(self, dut):
-        """Get a benchmark_run (br) object for a certain dut.
-
-        Args:
-          dut: the dut for which a br is returned.
-
-        Returns:
-          A br with its label matching that of the dut. If no such br could be
-          found, return None (this usually means a reimage is required for the
-          dut).
-        """
-
-        # If terminated, stop providing any br.
-        if self._terminated:
-            return None
-
-        # If dut bears an unrecognized label, return None.
-        if dut.label is None:
-            return None
-
-        # If br list for the dut's label is empty (that means all brs for this
-        # label have been done), return None.
-        with self.lock_on(dut.label):
-            brl = self._label_brl_map[dut.label]
-            if not brl:
-                return None
-            # Return the first br.
-            return brl.pop(0)
-
-    def allocate_label(self, dut):
-        """Allocate a label to a dut.
-
-            The work is delegated to MachineImageManager.
-
-            The dut_worker calling this method is responsible for reimage the dut to
-            this label.
-
-        Args:
-          dut: the new label that is to be reimaged onto the dut.
-
-        Returns:
-          The label or None.
-        """
-
-        if self._terminated:
-            return None
-
-        return self._mim.allocate(dut, self)
-
-    def dut_worker_finished(self, dut_worker):
-        """Notify schedv2 that the dut_worker thread finished.
-
-        Args:
-          dut_worker: the thread that is about to end.
-        """
-
-        self._logger.LogOutput("{} finished.".format(dut_worker))
-        with self._workers_lock:
-            self._active_workers.remove(dut_worker)
-            self._finished_workers.append(dut_worker)
-
-    def is_complete(self):
-        return len(self._active_workers) == 0
-
-    def lock_on(self, my_object):
-        return self._lock_map[my_object]
-
-    def terminate(self):
-        """Mark flag so we stop providing br/reimages.
-
-        Also terminate each DutWorker, so they refuse to execute br or reimage.
-        """
-
-        self._terminated = True
-        for dut_worker in self._active_workers:
-            dut_worker.terminate()
-
-    def threads_status_as_string(self):
-        """Report the dut worker threads status."""
-
-        status = "{} active threads, {} finished threads.\n".format(
-            len(self._active_workers), len(self._finished_workers)
-        )
-        status += "  Active threads:"
-        for dw in self._active_workers:
-            status += "\n    " + dw.status_str()
-        if self._finished_workers:
-            status += "\n  Finished threads:"
-            for dw in self._finished_workers:
-                status += "\n    " + dw.status_str()
-        return status
diff --git a/crosperf/schedv2_unittest.py b/crosperf/schedv2_unittest.py
deleted file mode 100755
index db5f5feb..00000000
--- a/crosperf/schedv2_unittest.py
+++ /dev/null
@@ -1,249 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-# Copyright 2015 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""This contains the unit tests for the new Crosperf task scheduler."""
-
-
-import functools
-import io
-import unittest
-import unittest.mock as mock
-
-import benchmark_run
-from cros_utils.command_executer import CommandExecuter
-from experiment_factory import ExperimentFactory
-from experiment_file import ExperimentFile
-from experiment_runner_unittest import FakeLogger
-from schedv2 import Schedv2
-import test_flag
-
-
-EXPERIMENT_FILE_1 = """\
-board: daisy
-remote: chromeos-daisy1.cros chromeos-daisy2.cros
-locks_dir: /tmp
-
-benchmark: kraken {
-  suite: telemetry_Crosperf
-  iterations: 3
-}
-
-image1 {
-  chromeos_image: /chromeos/src/build/images/daisy/latest/cros_image1.bin
-  remote: chromeos-daisy3.cros
-}
-
-image2 {
-  chromeos_image: /chromeos/src/build/imaages/daisy/latest/cros_image2.bin
-  remote: chromeos-daisy4.cros chromeos-daisy5.cros
-}
-"""
-
-EXPERIMENT_FILE_WITH_FORMAT = """\
-board: daisy
-remote: chromeos-daisy1.cros chromeos-daisy2.cros
-locks_dir: /tmp
-
-benchmark: kraken {{
-  suite: telemetry_Crosperf
-  iterations: {kraken_iterations}
-}}
-
-image1 {{
-  chromeos_image: /chromeos/src/build/images/daisy/latest/cros_image1.bin
-  remote: chromeos-daisy3.cros
-}}
-
-image2 {{
-  chromeos_image: /chromeos/src/build/imaages/daisy/latest/cros_image2.bin
-  remote: chromeos-daisy4.cros chromeos-daisy5.cros
-}}
-"""
-
-
-class Schedv2Test(unittest.TestCase):
-    """Class for setting up and running the unit tests."""
-
-    def setUp(self):
-        self.exp = None
-
-    mock_logger = FakeLogger()
-    mock_cmd_exec = mock.Mock(spec=CommandExecuter)
-
-    @mock.patch(
-        "benchmark_run.BenchmarkRun", new=benchmark_run.MockBenchmarkRun
-    )
-    def _make_fake_experiment(self, expstr):
-        """Create fake experiment from string.
-
-        Note - we mock out BenchmarkRun in this step.
-        """
-        experiment_file = ExperimentFile(io.StringIO(expstr))
-        experiment = ExperimentFactory().GetExperiment(
-            experiment_file, working_directory="", log_dir=""
-        )
-        return experiment
-
-    def test_remote(self):
-        """Test that remotes in labels are aggregated into experiment.remote."""
-
-        self.exp = self._make_fake_experiment(EXPERIMENT_FILE_1)
-        self.exp.log_level = "verbose"
-        my_schedv2 = Schedv2(self.exp)
-        self.assertFalse(my_schedv2.is_complete())
-        self.assertIn("chromeos-daisy1.cros", self.exp.remote)
-        self.assertIn("chromeos-daisy2.cros", self.exp.remote)
-        self.assertIn("chromeos-daisy3.cros", self.exp.remote)
-        self.assertIn("chromeos-daisy4.cros", self.exp.remote)
-        self.assertIn("chromeos-daisy5.cros", self.exp.remote)
-
-    def test_unreachable_remote(self):
-        """Test unreachable remotes are removed from experiment and label."""
-
-        def MockIsReachable(cm):
-            return (
-                cm.name != "chromeos-daisy3.cros"
-                and cm.name != "chromeos-daisy5.cros"
-            )
-
-        with mock.patch(
-            "machine_manager.MockCrosMachine.IsReachable", new=MockIsReachable
-        ):
-            self.exp = self._make_fake_experiment(EXPERIMENT_FILE_1)
-            self.assertIn("chromeos-daisy1.cros", self.exp.remote)
-            self.assertIn("chromeos-daisy2.cros", self.exp.remote)
-            self.assertNotIn("chromeos-daisy3.cros", self.exp.remote)
-            self.assertIn("chromeos-daisy4.cros", self.exp.remote)
-            self.assertNotIn("chromeos-daisy5.cros", self.exp.remote)
-
-            for l in self.exp.labels:
-                if l.name == "image2":
-                    self.assertNotIn("chromeos-daisy5.cros", l.remote)
-                    self.assertIn("chromeos-daisy4.cros", l.remote)
-                elif l.name == "image1":
-                    self.assertNotIn("chromeos-daisy3.cros", l.remote)
-
-    @mock.patch("schedv2.BenchmarkRunCacheReader")
-    def test_BenchmarkRunCacheReader_1(self, reader):
-        """Test benchmarkrun set is split into 5 segments."""
-
-        self.exp = self._make_fake_experiment(
-            EXPERIMENT_FILE_WITH_FORMAT.format(kraken_iterations=9)
-        )
-        my_schedv2 = Schedv2(self.exp)
-        self.assertFalse(my_schedv2.is_complete())
-        # We have 9 * 2 == 18 brs, we use 5 threads, each reading 4, 4, 4,
-        # 4, 2 brs respectively.
-        # Assert that BenchmarkRunCacheReader() is called 5 times.
-        self.assertEqual(reader.call_count, 5)
-        # reader.call_args_list[n] - nth call.
-        # reader.call_args_list[n][0] - positioned args in nth call.
-        # reader.call_args_list[n][0][1] - the 2nd arg in nth call,
-        # that is 'br_list' in 'schedv2.BenchmarkRunCacheReader'.
-        self.assertEqual(len(reader.call_args_list[0][0][1]), 4)
-        self.assertEqual(len(reader.call_args_list[1][0][1]), 4)
-        self.assertEqual(len(reader.call_args_list[2][0][1]), 4)
-        self.assertEqual(len(reader.call_args_list[3][0][1]), 4)
-        self.assertEqual(len(reader.call_args_list[4][0][1]), 2)
-
-    @mock.patch("schedv2.BenchmarkRunCacheReader")
-    def test_BenchmarkRunCacheReader_2(self, reader):
-        """Test benchmarkrun set is split into 4 segments."""
-
-        self.exp = self._make_fake_experiment(
-            EXPERIMENT_FILE_WITH_FORMAT.format(kraken_iterations=8)
-        )
-        my_schedv2 = Schedv2(self.exp)
-        self.assertFalse(my_schedv2.is_complete())
-        # We have 8 * 2 == 16 brs, we use 4 threads, each reading 4 brs.
-        self.assertEqual(reader.call_count, 4)
-        self.assertEqual(len(reader.call_args_list[0][0][1]), 4)
-        self.assertEqual(len(reader.call_args_list[1][0][1]), 4)
-        self.assertEqual(len(reader.call_args_list[2][0][1]), 4)
-        self.assertEqual(len(reader.call_args_list[3][0][1]), 4)
-
-    @mock.patch("schedv2.BenchmarkRunCacheReader")
-    def test_BenchmarkRunCacheReader_3(self, reader):
-        """Test benchmarkrun set is split into 2 segments."""
-
-        self.exp = self._make_fake_experiment(
-            EXPERIMENT_FILE_WITH_FORMAT.format(kraken_iterations=3)
-        )
-        my_schedv2 = Schedv2(self.exp)
-        self.assertFalse(my_schedv2.is_complete())
-        # We have 3 * 2 == 6 brs, we use 2 threads.
-        self.assertEqual(reader.call_count, 2)
-        self.assertEqual(len(reader.call_args_list[0][0][1]), 3)
-        self.assertEqual(len(reader.call_args_list[1][0][1]), 3)
-
-    @mock.patch("schedv2.BenchmarkRunCacheReader")
-    def test_BenchmarkRunCacheReader_4(self, reader):
-        """Test benchmarkrun set is not splitted."""
-
-        self.exp = self._make_fake_experiment(
-            EXPERIMENT_FILE_WITH_FORMAT.format(kraken_iterations=1)
-        )
-        my_schedv2 = Schedv2(self.exp)
-        self.assertFalse(my_schedv2.is_complete())
-        # We have 1 * 2 == 2 br, so only 1 instance.
-        self.assertEqual(reader.call_count, 1)
-        self.assertEqual(len(reader.call_args_list[0][0][1]), 2)
-
-    def test_cachehit(self):
-        """Test cache-hit and none-cache-hit brs are properly organized."""
-
-        def MockReadCache(br):
-            br.cache_hit = br.label.name == "image2"
-
-        with mock.patch(
-            "benchmark_run.MockBenchmarkRun.ReadCache", new=MockReadCache
-        ):
-            # We have 2 * 30 brs, half of which are put into _cached_br_list.
-            self.exp = self._make_fake_experiment(
-                EXPERIMENT_FILE_WITH_FORMAT.format(kraken_iterations=30)
-            )
-            my_schedv2 = Schedv2(self.exp)
-            self.assertEqual(len(my_schedv2.get_cached_run_list()), 30)
-            # The non-cache-hit brs are put into Schedv2._label_brl_map.
-            self.assertEqual(
-                functools.reduce(
-                    lambda a, x: a + len(x[1]),
-                    my_schedv2.get_label_map().items(),
-                    0,
-                ),
-                30,
-            )
-
-    def test_nocachehit(self):
-        """Test no cache-hit."""
-
-        def MockReadCache(br):
-            br.cache_hit = False
-
-        with mock.patch(
-            "benchmark_run.MockBenchmarkRun.ReadCache", new=MockReadCache
-        ):
-            # We have 2 * 30 brs, none of which are put into _cached_br_list.
-            self.exp = self._make_fake_experiment(
-                EXPERIMENT_FILE_WITH_FORMAT.format(kraken_iterations=30)
-            )
-            my_schedv2 = Schedv2(self.exp)
-            self.assertEqual(len(my_schedv2.get_cached_run_list()), 0)
-            # The non-cache-hit brs are put into Schedv2._label_brl_map.
-            self.assertEqual(
-                functools.reduce(
-                    lambda a, x: a + len(x[1]),
-                    my_schedv2.get_label_map().items(),
-                    0,
-                ),
-                60,
-            )
-
-
-if __name__ == "__main__":
-    test_flag.SetTestMode(True)
-    unittest.main()
diff --git a/crosperf/settings.py b/crosperf/settings.py
deleted file mode 100644
index 5a983b32..00000000
--- a/crosperf/settings.py
+++ /dev/null
@@ -1,111 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Module to get the settings from experiment file."""
-
-
-from cros_utils import logger
-from cros_utils import misc
-from download_images import ImageDownloader
-
-
-class Settings(object):
-    """Class representing settings (a set of fields) from an experiment file."""
-
-    def __init__(self, name, settings_type):
-        self.name = name
-        self.settings_type = settings_type
-        self.fields = {}
-        self.parent = None
-
-    def SetParentSettings(self, settings):
-        """Set the parent settings which these settings can inherit from."""
-        self.parent = settings
-
-    def AddField(self, field):
-        name = field.name
-        if name in self.fields:
-            raise SyntaxError("Field %s defined previously." % name)
-        self.fields[name] = field
-
-    def SetField(self, name, value, append=False):
-        if name not in self.fields:
-            raise SyntaxError(
-                "'%s' is not a valid field in '%s' settings"
-                % (name, self.settings_type)
-            )
-        if append:
-            self.fields[name].Append(value)
-        else:
-            self.fields[name].Set(value)
-
-    def GetField(self, name):
-        """Get the value of a field with a given name."""
-        if name not in self.fields:
-            raise SyntaxError(
-                "Field '%s' not a valid field in '%s' settings."
-                % (name, self.name)
-            )
-        field = self.fields[name]
-        if not field.assigned and field.required:
-            raise SyntaxError(
-                "Required field '%s' not defined in '%s' settings."
-                % (name, self.name)
-            )
-        return self.fields[name].Get()
-
-    def Inherit(self):
-        """Inherit any unset values from the parent settings."""
-        for name in self.fields:
-            if (
-                not self.fields[name].assigned
-                and self.parent
-                and name in self.parent.fields
-                and self.parent.fields[name].assigned
-            ):
-                self.fields[name].Set(self.parent.GetField(name), parse=False)
-
-    def Override(self, settings):
-        """Override settings with settings from a different object."""
-        for name in settings.fields:
-            if name in self.fields and settings.fields[name].assigned:
-                self.fields[name].Set(settings.GetField(name), parse=False)
-
-    def Validate(self):
-        """Check that all required fields have been set."""
-        for name in self.fields:
-            if not self.fields[name].assigned and self.fields[name].required:
-                raise SyntaxError("Field %s is invalid." % name)
-
-    def GetXbuddyPath(
-        self,
-        path_str,
-        autotest_path,
-        debug_path,
-        board,
-        chromeos_root,
-        log_level,
-        download_debug,
-    ):
-        prefix = "remote"
-        l = logger.GetLogger()
-        if (
-            path_str.find("trybot") < 0
-            and path_str.find("toolchain") < 0
-            and path_str.find(board) < 0
-            and path_str.find(board.replace("_", "-"))
-        ):
-            xbuddy_path = "%s/%s/%s" % (prefix, board, path_str)
-        else:
-            xbuddy_path = "%s/%s" % (prefix, path_str)
-        image_downloader = ImageDownloader(l, log_level)
-        # Returns three variables: image, autotest_path, debug_path
-        return image_downloader.Run(
-            misc.CanonicalizePath(chromeos_root),
-            xbuddy_path,
-            autotest_path,
-            debug_path,
-            download_debug,
-        )
diff --git a/crosperf/settings_factory.py b/crosperf/settings_factory.py
deleted file mode 100644
index b34f0b16..00000000
--- a/crosperf/settings_factory.py
+++ /dev/null
@@ -1,589 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Setting files for global, benchmark and labels."""
-
-
-from field import BooleanField
-from field import EnumField
-from field import FloatField
-from field import IntegerField
-from field import ListField
-from field import TextField
-from settings import Settings
-
-
-class BenchmarkSettings(Settings):
-    """Settings used to configure individual benchmarks."""
-
-    def __init__(self, name):
-        super(BenchmarkSettings, self).__init__(name, "benchmark")
-        self.AddField(
-            TextField(
-                "test_name",
-                description="The name of the test to run. "
-                "Defaults to the name of the benchmark.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "test_args",
-                description="Arguments to be passed to the " "test.",
-            )
-        )
-        self.AddField(
-            IntegerField(
-                "iterations",
-                required=False,
-                default=0,
-                description="Number of iterations to run the test. "
-                "If not set, will run each benchmark test the optimum number of "
-                "times to get a stable result.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "suite",
-                default="test_that",
-                description="The type of the benchmark.",
-            )
-        )
-        self.AddField(
-            IntegerField(
-                "retries",
-                default=0,
-                description="Number of times to retry a " "benchmark run.",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "run_local",
-                description="Run benchmark harness on the DUT. "
-                "Currently only compatible with the suite: "
-                "telemetry_Crosperf.",
-                required=False,
-                default=True,
-            )
-        )
-        self.AddField(
-            FloatField(
-                "weight",
-                default=0.0,
-                description="Weight of the benchmark for CWP approximation",
-            )
-        )
-
-
-class LabelSettings(Settings):
-    """Settings for each label."""
-
-    def __init__(self, name):
-        super(LabelSettings, self).__init__(name, "label")
-        self.AddField(
-            TextField(
-                "chromeos_image",
-                required=False,
-                description="The path to the image to run tests "
-                "on, for local/custom-built images. See the "
-                "'build' option for official or trybot images.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "autotest_path",
-                required=False,
-                description="Autotest directory path relative to chroot which "
-                "has autotest files for the image to run tests requiring autotest "
-                "files.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "debug_path",
-                required=False,
-                description="Debug info directory relative to chroot which has "
-                "symbols and vmlinux that can be used by perf tool.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "chromeos_root",
-                description="The path to a chromeos checkout which "
-                "contains a src/scripts directory. Defaults to "
-                "the chromeos checkout which contains the "
-                "chromeos_image.",
-            )
-        )
-        self.AddField(
-            ListField(
-                "remote",
-                description="A comma-separated list of IPs of chromeos"
-                "devices to run experiments on.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "image_args",
-                required=False,
-                default="",
-                description="Extra arguments to pass to " "image_chromeos.py.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "cache_dir",
-                default="",
-                description="The cache dir for this image.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "compiler",
-                default="gcc",
-                description="The compiler used to build the "
-                "ChromeOS image (gcc or llvm).",
-            )
-        )
-        self.AddField(
-            TextField(
-                "chrome_src",
-                description="The path to the source of chrome. "
-                "This is used to run telemetry benchmarks. "
-                "The default one is the src inside chroot.",
-                required=False,
-                default="",
-            )
-        )
-        self.AddField(
-            TextField(
-                "build",
-                description="The xbuddy specification for an "
-                "official or trybot image to use for tests. "
-                "'/remote' is assumed, and the board is given "
-                "elsewhere, so omit the '/remote/<board>/' xbuddy "
-                "prefix.",
-                required=False,
-                default="",
-            )
-        )
-
-
-class GlobalSettings(Settings):
-    """Settings that apply per-experiment."""
-
-    def __init__(self, name):
-        super(GlobalSettings, self).__init__(name, "global")
-        self.AddField(
-            TextField(
-                "name",
-                description="The name of the experiment. Just an "
-                "identifier.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "board",
-                description="The target board for running "
-                "experiments on, e.g. x86-alex.",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "crosfleet",
-                description="Whether to run experiments via crosfleet.",
-                default=False,
-            )
-        )
-        self.AddField(
-            ListField(
-                "remote",
-                description="A comma-separated list of IPs of "
-                "chromeos devices to run experiments on.",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "rerun_if_failed",
-                description="Whether to re-run failed test runs or not.",
-                default=False,
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "rm_chroot_tmp",
-                default=False,
-                description="Whether to remove the test_that "
-                "result in the chroot.",
-            )
-        )
-        self.AddField(
-            ListField(
-                "email",
-                description="Space-separated list of email "
-                "addresses to send email to.",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "rerun",
-                description="Whether to ignore the cache and "
-                "for tests to be re-run.",
-                default=False,
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "ignore_cache",
-                description='Alias of "rerun" to ignore cache.',
-                default=False,
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "same_specs",
-                default=True,
-                description="Ensure cached runs are run on the "
-                "same kind of devices which are specified as a "
-                "remote.",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "same_machine",
-                default=False,
-                description="Ensure cached runs are run on the same remote.",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "use_file_locks",
-                default=False,
-                description="DEPRECATED: Whether to use the file locks "
-                "or AFE server lock mechanism.",
-            )
-        )
-        self.AddField(
-            IntegerField(
-                "iterations",
-                required=False,
-                default=0,
-                description="Number of iterations to run all tests. "
-                "If not set, will run each benchmark test the optimum number of "
-                "times to get a stable result.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "chromeos_root",
-                description="The path to a chromeos checkout which "
-                "contains a src/scripts directory. Defaults to "
-                "the chromeos checkout which contains the "
-                "chromeos_image.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "logging_level",
-                default="average",
-                description="The level of logging desired. "
-                "Options are 'quiet', 'average', and 'verbose'.",
-            )
-        )
-        self.AddField(
-            IntegerField(
-                "acquire_timeout",
-                default=0,
-                description="Number of seconds to wait for "
-                "machine before exit if all the machines in "
-                "the experiment file are busy. Default is 0.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "perf_args",
-                default="",
-                description="The optional profile command. It "
-                "enables perf commands to record perforamance "
-                "related counters. It must start with perf "
-                "command record or stat followed by arguments.",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "download_debug",
-                default=True,
-                description="Download compressed debug symbols alongwith "
-                "image. This can provide more info matching symbols for"
-                "profiles, but takes larger space. By default, download"
-                "it only when perf_args is specified.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "cache_dir",
-                default="",
-                description="The abs path of cache dir. "
-                "Default is /home/$(whoami)/cros_scratch.",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "cache_only",
-                default=False,
-                description="Whether to use only cached "
-                "results (do not rerun failed tests).",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "no_email",
-                default=False,
-                description="Whether to disable the email to "
-                "user after crosperf finishes.",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "json_report",
-                default=False,
-                description="Whether to generate a json version "
-                "of the report, for archiving.",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "show_all_results",
-                default=False,
-                description="When running Telemetry tests, "
-                "whether to all the results, instead of just "
-                "the default (summary) results.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "share_cache",
-                default="",
-                description="Path to alternate cache whose data "
-                "you want to use. It accepts multiple directories "
-                'separated by a ",".',
-            )
-        )
-        self.AddField(
-            TextField("results_dir", default="", description="The results dir.")
-        )
-        self.AddField(
-            BooleanField(
-                "compress_results",
-                default=True,
-                description="Whether to compress all test results other than "
-                "reports into a tarball to save disk space.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "locks_dir",
-                default="",
-                description="An alternate directory to use for "
-                "storing/checking machine file locks for local machines. "
-                "By default the file locks directory is "
-                "/google/data/rw/users/mo/mobiletc-prebuild/locks.\n"
-                "WARNING: If you use your own locks directory, "
-                "there is no guarantee that someone else might not "
-                "hold a lock on the same machine in a different "
-                "locks directory.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "chrome_src",
-                description="The path to the source of chrome. "
-                "This is used to run telemetry benchmarks. "
-                "The default one is the src inside chroot.",
-                required=False,
-                default="",
-            )
-        )
-        self.AddField(
-            IntegerField(
-                "retries",
-                default=0,
-                description="Number of times to retry a " "benchmark run.",
-            )
-        )
-        self.AddField(
-            TextField(
-                "cwp_dso",
-                description="The DSO type that we want to use for "
-                "CWP approximation. This is used to run telemetry "
-                "benchmarks. Valid DSO types can be found from dso_list "
-                "in experiment_factory.py. The default value is set to "
-                "be empty.",
-                required=False,
-                default="",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "enable_aslr",
-                description="Enable ASLR on the machine to run the "
-                "benchmarks. ASLR is disabled by default",
-                required=False,
-                default=False,
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "ignore_min_max",
-                description="When doing math for the raw results, "
-                "ignore min and max values to reduce noise.",
-                required=False,
-                default=False,
-            )
-        )
-        self.AddField(
-            TextField(
-                "intel_pstate",
-                description="Intel Pstate mode.\n"
-                'Supported modes: "active", "passive", "no_hwp".\n'
-                'Default is "no_hwp" which disables hardware pstates to avoid '
-                "noise in benchmarks.",
-                required=False,
-                default="no_hwp",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "turbostat",
-                description="Run turbostat process in the background"
-                " of a benchmark. Enabled by default.",
-                required=False,
-                default=True,
-            )
-        )
-        self.AddField(
-            FloatField(
-                "top_interval",
-                description="Run top command in the background of a benchmark with"
-                " interval of sampling specified in seconds.\n"
-                "Recommended values 1-5. Lower number provides more accurate"
-                " data.\n"
-                "With 0 - do not run top.\n"
-                "NOTE: Running top with interval 1-5 sec has insignificant"
-                " performance impact (performance degradation does not exceed"
-                " 0.3%%, measured on x86_64, ARM32, and ARM64). "
-                "The default value is 1.",
-                required=False,
-                default=1,
-            )
-        )
-        self.AddField(
-            IntegerField(
-                "cooldown_temp",
-                required=False,
-                default=40,
-                description="Wait until CPU temperature goes down below"
-                " specified temperature in Celsius"
-                " prior starting a benchmark. "
-                "By default the value is set to 40 degrees.",
-            )
-        )
-        self.AddField(
-            IntegerField(
-                "cooldown_time",
-                required=False,
-                default=10,
-                description="Wait specified time in minutes allowing"
-                " CPU to cool down. Zero value disables cooldown. "
-                "The default value is 10 minutes.",
-            )
-        )
-        self.AddField(
-            EnumField(
-                "governor",
-                options=[
-                    "performance",
-                    "powersave",
-                    "userspace",
-                    "ondemand",
-                    "conservative",
-                    "schedutils",
-                    "sched",
-                    "interactive",
-                ],
-                default="performance",
-                required=False,
-                description="Setup CPU governor for all cores.\n"
-                "For more details refer to:\n"
-                "https://www.kernel.org/doc/Documentation/cpu-freq/governors.txt. "
-                'Default is "performance" governor.',
-            )
-        )
-        self.AddField(
-            EnumField(
-                "cpu_usage",
-                options=[
-                    "all",
-                    "big_only",
-                    "little_only",
-                    "exclusive_cores",
-                ],
-                default="all",
-                required=False,
-                description="Restrict usage of CPUs to decrease CPU interference.\n"
-                '"all" - no restrictions;\n'
-                '"big-only", "little-only" - enable only big/little cores,'
-                " applicable only on ARM;\n"
-                '"exclusive-cores" - (for future use)'
-                " isolate cores for exclusive use of benchmark processes. "
-                "By default use all CPUs.",
-            )
-        )
-        self.AddField(
-            IntegerField(
-                "cpu_freq_pct",
-                required=False,
-                default=95,
-                description="Setup CPU frequency to a supported value less than"
-                " or equal to a percent of max_freq. "
-                "CPU frequency is reduced to 95%% by default to reduce thermal "
-                "throttling.",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "no_lock",
-                default=False,
-                description="Do not attempt to lock the DUT."
-                " Useful when lock is held externally, say with crosfleet.",
-            )
-        )
-        self.AddField(
-            BooleanField(
-                "keep_stateful",
-                default=False,
-                description="When flashing a ChromeOS image keep the stateful"
-                " partition, i.e. don't use --clobber-stateful. This option"
-                " is useful to keep ssh keys, wi-fi settings and so on.",
-            )
-        )
-
-
-class SettingsFactory(object):
-    """Factory class for building different types of Settings objects.
-
-    This factory is currently hardcoded to produce settings for ChromeOS
-    experiment files. The idea is that in the future, other types
-    of settings could be produced.
-    """
-
-    def GetSettings(self, name, settings_type):
-        if settings_type == "label" or not settings_type:
-            return LabelSettings(name)
-        if settings_type == "global":
-            return GlobalSettings(name)
-        if settings_type == "benchmark":
-            return BenchmarkSettings(name)
-
-        raise TypeError("Invalid settings type: '%s'." % settings_type)
diff --git a/crosperf/settings_factory_unittest.py b/crosperf/settings_factory_unittest.py
deleted file mode 100755
index a6771c03..00000000
--- a/crosperf/settings_factory_unittest.py
+++ /dev/null
@@ -1,122 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Copyright 2017 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unittest for crosperf."""
-
-
-import unittest
-
-import settings_factory
-
-
-class BenchmarkSettingsTest(unittest.TestCase):
-    """Class to test benchmark settings."""
-
-    def test_init(self):
-        res = settings_factory.BenchmarkSettings("b_settings")
-        self.assertIsNotNone(res)
-        self.assertEqual(len(res.fields), 7)
-        self.assertEqual(res.GetField("test_name"), "")
-        self.assertEqual(res.GetField("test_args"), "")
-        self.assertEqual(res.GetField("iterations"), 0)
-        self.assertEqual(res.GetField("suite"), "test_that")
-
-
-class LabelSettingsTest(unittest.TestCase):
-    """Class to test label settings."""
-
-    def test_init(self):
-        res = settings_factory.LabelSettings("l_settings")
-        self.assertIsNotNone(res)
-        self.assertEqual(len(res.fields), 10)
-        self.assertEqual(res.GetField("chromeos_image"), "")
-        self.assertEqual(res.GetField("autotest_path"), "")
-        self.assertEqual(res.GetField("chromeos_root"), "")
-        self.assertEqual(res.GetField("remote"), None)
-        self.assertEqual(res.GetField("image_args"), "")
-        self.assertEqual(res.GetField("cache_dir"), "")
-        self.assertEqual(res.GetField("chrome_src"), "")
-        self.assertEqual(res.GetField("build"), "")
-
-
-class GlobalSettingsTest(unittest.TestCase):
-    """Class to test global settings."""
-
-    def test_init(self):
-        res = settings_factory.GlobalSettings("g_settings")
-        self.assertIsNotNone(res)
-        self.assertEqual(len(res.fields), 42)
-        self.assertEqual(res.GetField("name"), "")
-        self.assertEqual(res.GetField("board"), "")
-        self.assertEqual(res.GetField("crosfleet"), False)
-        self.assertEqual(res.GetField("remote"), None)
-        self.assertEqual(res.GetField("rerun_if_failed"), False)
-        self.assertEqual(res.GetField("rm_chroot_tmp"), False)
-        self.assertEqual(res.GetField("email"), None)
-        self.assertEqual(res.GetField("rerun"), False)
-        self.assertEqual(res.GetField("ignore_cache"), False)
-        self.assertEqual(res.GetField("same_specs"), True)
-        self.assertEqual(res.GetField("same_machine"), False)
-        self.assertEqual(res.GetField("iterations"), 0)
-        self.assertEqual(res.GetField("chromeos_root"), "")
-        self.assertEqual(res.GetField("logging_level"), "average")
-        self.assertEqual(res.GetField("acquire_timeout"), 0)
-        self.assertEqual(res.GetField("perf_args"), "")
-        self.assertEqual(res.GetField("download_debug"), True)
-        self.assertEqual(res.GetField("cache_dir"), "")
-        self.assertEqual(res.GetField("cache_only"), False)
-        self.assertEqual(res.GetField("no_email"), False)
-        self.assertEqual(res.GetField("show_all_results"), False)
-        self.assertEqual(res.GetField("share_cache"), "")
-        self.assertEqual(res.GetField("results_dir"), "")
-        self.assertEqual(res.GetField("compress_results"), True)
-        self.assertEqual(res.GetField("chrome_src"), "")
-        self.assertEqual(res.GetField("cwp_dso"), "")
-        self.assertEqual(res.GetField("enable_aslr"), False)
-        self.assertEqual(res.GetField("ignore_min_max"), False)
-        self.assertEqual(res.GetField("intel_pstate"), "no_hwp")
-        self.assertEqual(res.GetField("turbostat"), True)
-        self.assertEqual(res.GetField("top_interval"), 1)
-        self.assertEqual(res.GetField("cooldown_time"), 10)
-        self.assertEqual(res.GetField("cooldown_temp"), 40)
-        self.assertEqual(res.GetField("governor"), "performance")
-        self.assertEqual(res.GetField("cpu_usage"), "all")
-        self.assertEqual(res.GetField("cpu_freq_pct"), 95)
-
-
-class SettingsFactoryTest(unittest.TestCase):
-    """Class to test SettingsFactory."""
-
-    def test_get_settings(self):
-        self.assertRaises(
-            Exception,
-            settings_factory.SettingsFactory.GetSettings,
-            "global",
-            "bad_type",
-        )
-
-        l_settings = settings_factory.SettingsFactory().GetSettings(
-            "label", "label"
-        )
-        self.assertIsInstance(l_settings, settings_factory.LabelSettings)
-        self.assertEqual(len(l_settings.fields), 10)
-
-        b_settings = settings_factory.SettingsFactory().GetSettings(
-            "benchmark", "benchmark"
-        )
-        self.assertIsInstance(b_settings, settings_factory.BenchmarkSettings)
-        self.assertEqual(len(b_settings.fields), 7)
-
-        g_settings = settings_factory.SettingsFactory().GetSettings(
-            "global", "global"
-        )
-        self.assertIsInstance(g_settings, settings_factory.GlobalSettings)
-        self.assertEqual(len(g_settings.fields), 42)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/settings_unittest.py b/crosperf/settings_unittest.py
deleted file mode 100755
index ab31e18f..00000000
--- a/crosperf/settings_unittest.py
+++ /dev/null
@@ -1,293 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""unittest for settings."""
-
-
-import unittest
-import unittest.mock as mock
-
-from cros_utils import logger
-import download_images
-from field import IntegerField
-from field import ListField
-import settings
-import settings_factory
-
-
-class TestSettings(unittest.TestCase):
-    """setting test class."""
-
-    def setUp(self):
-        self.settings = settings.Settings("global_name", "global")
-
-    def test_init(self):
-        self.assertEqual(self.settings.name, "global_name")
-        self.assertEqual(self.settings.settings_type, "global")
-        self.assertIsNone(self.settings.parent)
-
-    def test_set_parent_settings(self):
-        self.assertIsNone(self.settings.parent)
-        settings_parent = {"fake_parent_entry": 0}
-        self.settings.SetParentSettings(settings_parent)
-        self.assertIsNotNone(self.settings.parent)
-        self.assertTrue(isinstance(self.settings.parent, dict))
-        self.assertEqual(self.settings.parent, settings_parent)
-
-    def test_add_field(self):
-        self.assertEqual(self.settings.fields, {})
-        self.settings.AddField(
-            IntegerField(
-                "iterations",
-                default=1,
-                required=False,
-                description="Number of iterations to " "run the test.",
-            )
-        )
-        self.assertEqual(len(self.settings.fields), 1)
-        # Adding the same field twice raises an exception.
-        self.assertRaises(
-            Exception,
-            self.settings.AddField,
-            (
-                IntegerField(
-                    "iterations",
-                    default=1,
-                    required=False,
-                    description="Number of iterations to run " "the test.",
-                )
-            ),
-        )
-        res = self.settings.fields["iterations"]
-        self.assertIsInstance(res, IntegerField)
-        self.assertEqual(res.Get(), 1)
-
-    def test_set_field(self):
-        self.assertEqual(self.settings.fields, {})
-        self.settings.AddField(
-            IntegerField(
-                "iterations",
-                default=1,
-                required=False,
-                description="Number of iterations to run the " "test.",
-            )
-        )
-        res = self.settings.fields["iterations"]
-        self.assertEqual(res.Get(), 1)
-
-        self.settings.SetField("iterations", 10)
-        res = self.settings.fields["iterations"]
-        self.assertEqual(res.Get(), 10)
-
-        # Setting a field that's not there raises an exception.
-        self.assertRaises(
-            Exception, self.settings.SetField, "remote", "lumpy1.cros"
-        )
-
-        self.settings.AddField(
-            ListField(
-                "remote",
-                default=[],
-                description="A comma-separated list of ip's of "
-                "chromeos devices to run "
-                "experiments on.",
-            )
-        )
-        self.assertTrue(isinstance(self.settings.fields, dict))
-        self.assertEqual(len(self.settings.fields), 2)
-        res = self.settings.fields["remote"]
-        self.assertEqual(res.Get(), [])
-        self.settings.SetField("remote", "lumpy1.cros", append=True)
-        self.settings.SetField("remote", "lumpy2.cros", append=True)
-        res = self.settings.fields["remote"]
-        self.assertEqual(res.Get(), ["lumpy1.cros", "lumpy2.cros"])
-
-    def test_get_field(self):
-        # Getting a field that's not there raises an exception.
-        self.assertRaises(Exception, self.settings.GetField, "iterations")
-
-        # Getting a required field that hasn't been assigned raises an exception.
-        self.settings.AddField(
-            IntegerField(
-                "iterations",
-                required=True,
-                description="Number of iterations to " "run the test.",
-            )
-        )
-        self.assertIsNotNone(self.settings.fields["iterations"])
-        self.assertRaises(Exception, self.settings.GetField, "iterations")
-
-        # Set the value, then get it.
-        self.settings.SetField("iterations", 5)
-        res = self.settings.GetField("iterations")
-        self.assertEqual(res, 5)
-
-    def test_inherit(self):
-        parent_settings = settings_factory.SettingsFactory().GetSettings(
-            "global", "global"
-        )
-        label_settings = settings_factory.SettingsFactory().GetSettings(
-            "label", "label"
-        )
-        self.assertEqual(parent_settings.GetField("chromeos_root"), "")
-        self.assertEqual(label_settings.GetField("chromeos_root"), "")
-        self.assertIsNone(label_settings.parent)
-
-        parent_settings.SetField("chromeos_root", "/tmp/chromeos")
-        label_settings.SetParentSettings(parent_settings)
-        self.assertEqual(
-            parent_settings.GetField("chromeos_root"), "/tmp/chromeos"
-        )
-        self.assertEqual(label_settings.GetField("chromeos_root"), "")
-        label_settings.Inherit()
-        self.assertEqual(
-            label_settings.GetField("chromeos_root"), "/tmp/chromeos"
-        )
-
-    def test_override(self):
-        self.settings.AddField(
-            ListField(
-                "email",
-                default=[],
-                description="Space-seperated"
-                "list of email addresses to send "
-                "email to.",
-            )
-        )
-
-        global_settings = settings_factory.SettingsFactory().GetSettings(
-            "global", "global"
-        )
-
-        global_settings.SetField("email", "john.doe@google.com", append=True)
-        global_settings.SetField("email", "jane.smith@google.com", append=True)
-
-        res = self.settings.GetField("email")
-        self.assertEqual(res, [])
-
-        self.settings.Override(global_settings)
-        res = self.settings.GetField("email")
-        self.assertEqual(res, ["john.doe@google.com", "jane.smith@google.com"])
-
-    def test_validate(self):
-
-        self.settings.AddField(
-            IntegerField(
-                "iterations",
-                required=True,
-                description="Number of iterations " "to run the test.",
-            )
-        )
-        self.settings.AddField(
-            ListField(
-                "remote",
-                default=[],
-                required=True,
-                description="A comma-separated list "
-                "of ip's of chromeos "
-                "devices to run experiments on.",
-            )
-        )
-        self.settings.AddField(
-            ListField(
-                "email",
-                default=[],
-                description="Space-seperated"
-                "list of email addresses to "
-                "send email to.",
-            )
-        )
-
-        # 'required' fields have not been assigned; should raise an exception.
-        self.assertRaises(Exception, self.settings.Validate)
-        self.settings.SetField("iterations", 2)
-        self.settings.SetField("remote", "x86-alex.cros", append=True)
-        # Should run without exception now.
-        self.settings.Validate()
-
-    @mock.patch.object(logger, "GetLogger")
-    @mock.patch.object(download_images.ImageDownloader, "Run")
-    @mock.patch.object(download_images, "ImageDownloader")
-    def test_get_xbuddy_path(self, mock_downloader, mock_run, mock_logger):
-
-        mock_run.return_value = "fake_xbuddy_translation"
-        mock_downloader.Run = mock_run
-        board = "lumpy"
-        chromeos_root = "/tmp/chromeos"
-        log_level = "average"
-
-        trybot_str = "trybot-lumpy-paladin/R34-5417.0.0-b1506"
-        official_str = "lumpy-release/R34-5417.0.0"
-        xbuddy_str = "latest-dev"
-        autotest_path = ""
-        debug_path = ""
-        download_debug = False
-
-        self.settings.GetXbuddyPath(
-            trybot_str,
-            autotest_path,
-            debug_path,
-            board,
-            chromeos_root,
-            log_level,
-            download_debug,
-        )
-        self.assertEqual(mock_run.call_count, 1)
-        self.assertEqual(
-            mock_run.call_args_list[0][0],
-            (
-                "/tmp/chromeos",
-                "remote/trybot-lumpy-paladin/R34-5417.0.0-b1506",
-                "",
-                "",
-                False,
-            ),
-        )
-
-        mock_run.reset_mock()
-        self.settings.GetXbuddyPath(
-            official_str,
-            autotest_path,
-            debug_path,
-            board,
-            chromeos_root,
-            log_level,
-            download_debug,
-        )
-        self.assertEqual(mock_run.call_count, 1)
-        self.assertEqual(
-            mock_run.call_args_list[0][0],
-            (
-                "/tmp/chromeos",
-                "remote/lumpy-release/R34-5417.0.0",
-                "",
-                "",
-                False,
-            ),
-        )
-
-        mock_run.reset_mock()
-        self.settings.GetXbuddyPath(
-            xbuddy_str,
-            autotest_path,
-            debug_path,
-            board,
-            chromeos_root,
-            log_level,
-            download_debug,
-        )
-        self.assertEqual(mock_run.call_count, 1)
-        self.assertEqual(
-            mock_run.call_args_list[0][0],
-            ("/tmp/chromeos", "remote/lumpy/latest-dev", "", "", False),
-        )
-
-        if mock_logger:
-            return
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/suite_runner.py b/crosperf/suite_runner.py
deleted file mode 100644
index f5566f51..00000000
--- a/crosperf/suite_runner.py
+++ /dev/null
@@ -1,437 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2013 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""SuiteRunner defines the interface from crosperf to test script."""
-
-
-import contextlib
-import json
-import os
-from pathlib import Path
-import pipes
-import random
-import shlex
-import subprocess
-import time
-
-from cros_utils import command_executer
-from cros_utils import misc
-
-
-# sshwatcher path, relative to ChromiumOS source root.
-SSHWATCHER = "src/platform/dev/contrib/sshwatcher/sshwatcher.go"
-TEST_THAT_PATH = "/usr/bin/test_that"
-TAST_PATH = "/usr/bin/tast"
-CROSFLEET_PATH = "crosfleet"
-GS_UTIL = "src/chromium/depot_tools/gsutil.py"
-AUTOTEST_DIR = "/mnt/host/source/src/third_party/autotest/files"
-CHROME_MOUNT_DIR = "/tmp/chrome_root"
-
-
-def GetProfilerArgs(profiler_args):
-    # Remove "--" from in front of profiler args.
-    args_list = shlex.split(profiler_args)
-    new_list = []
-    for arg in args_list:
-        if arg[0:2] == "--":
-            arg = arg[2:]
-        new_list.append(arg)
-    args_list = new_list
-
-    # Remove "perf_options=" from middle of profiler args.
-    new_list = []
-    for arg in args_list:
-        idx = arg.find("perf_options=")
-        if idx != -1:
-            prefix = arg[0:idx]
-            suffix = arg[idx + len("perf_options=") + 1 : -1]
-            new_arg = prefix + "'" + suffix + "'"
-            new_list.append(new_arg)
-        else:
-            new_list.append(arg)
-    args_list = new_list
-
-    return " ".join(args_list)
-
-
-def GetDutConfigArgs(dut_config):
-    return f"dut_config={pipes.quote(json.dumps(dut_config))}"
-
-
-@contextlib.contextmanager
-def ssh_tunnel(sshwatcher: "os.PathLike", machinename: str) -> str:
-    """Context manager that forwards a TCP port over SSH while active.
-
-    This class is used to set up port forwarding before entering the
-    chroot, so that the forwarded port can be used from inside
-    the chroot.
-
-    Args:
-        sshwatcher: Path to sshwatcher.go
-        machinename: Hostname of the machine to connect to.
-
-    Returns:
-        host:port string that can be passed to tast
-    """
-    # We have to tell sshwatcher which port we want to use.
-    # We pick a port that is likely to be available.
-    port = random.randrange(4096, 32768)
-    cmd = ["go", "run", str(sshwatcher), machinename, str(port)]
-    # Pylint wants us to use subprocess.Popen as a context manager,
-    # but we don't, so that we can ask sshwatcher to terminate and
-    # limit the time we wait for it to do so.
-    # pylint: disable=consider-using-with
-    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE)
-    try:
-        # sshwatcher takes a few seconds before it binds to the port,
-        # presumably due to SSH handshaking taking a while.
-        # Give it 12 seconds before we ask the client to connect.
-        time.sleep(12)
-        yield f"localhost:{port}"
-    finally:
-        proc.terminate()
-        proc.wait(timeout=5)
-
-
-class SuiteRunner(object):
-    """This defines the interface from crosperf to test script."""
-
-    def __init__(
-        self,
-        dut_config,
-        logger_to_use=None,
-        log_level="verbose",
-        cmd_exec=None,
-        cmd_term=None,
-    ):
-        self.logger = logger_to_use
-        self.log_level = log_level
-        self._ce = cmd_exec or command_executer.GetCommandExecuter(
-            self.logger, log_level=self.log_level
-        )
-        # DUT command executer.
-        # Will be initialized and used within Run.
-        self._ct = cmd_term or command_executer.CommandTerminator()
-        self.dut_config = dut_config
-
-    def Run(self, cros_machine, label, benchmark, test_args, profiler_args):
-        machine_name = cros_machine.name
-        for i in range(0, benchmark.retries + 1):
-            if label.crosfleet:
-                ret_tup = self.Crosfleet_Run(
-                    label, benchmark, test_args, profiler_args
-                )
-            else:
-                if benchmark.suite == "tast":
-                    with ssh_tunnel(
-                        Path(label.chromeos_root, SSHWATCHER), machine_name
-                    ) as hostport:
-                        ret_tup = self.Tast_Run(hostport, label, benchmark)
-                else:
-                    ret_tup = self.Test_That_Run(
-                        machine_name, label, benchmark, test_args, profiler_args
-                    )
-            if ret_tup[0] != 0:
-                self.logger.LogOutput(
-                    "benchmark %s failed. Retries left: %s"
-                    % (benchmark.name, benchmark.retries - i)
-                )
-            elif i > 0:
-                self.logger.LogOutput(
-                    "benchmark %s succeded after %s retries"
-                    % (benchmark.name, i)
-                )
-                break
-            else:
-                self.logger.LogOutput(
-                    "benchmark %s succeded on first try" % benchmark.name
-                )
-                break
-        return ret_tup
-
-    def RemoveTelemetryTempFile(self, machine, chromeos_root):
-        filename = "telemetry@%s" % machine
-        fullname = misc.GetOutsideChrootPath(
-            chromeos_root, os.path.join("/tmp", filename)
-        )
-        if os.path.exists(fullname):
-            os.remove(fullname)
-
-    def GenTestArgs(self, benchmark, test_args, profiler_args):
-        args_list = []
-
-        if benchmark.suite != "telemetry_Crosperf" and profiler_args:
-            self.logger.LogFatal(
-                "Tests other than telemetry_Crosperf do not "
-                "support profiler."
-            )
-
-        if test_args:
-            # Strip double quotes off args (so we can wrap them in single
-            # quotes, to pass through to Telemetry).
-            if test_args[0] == '"' and test_args[-1] == '"':
-                test_args = test_args[1:-1]
-            args_list.append("test_args='%s'" % test_args)
-
-        args_list.append(GetDutConfigArgs(self.dut_config))
-
-        if not (
-            benchmark.suite == "telemetry_Crosperf"
-            or benchmark.suite == "crosperf_Wrapper"
-        ):
-            self.logger.LogWarning(
-                "Please make sure the server test has stage for "
-                "device setup.\n"
-            )
-        else:
-            args_list.append("test=%s" % benchmark.test_name)
-            if benchmark.suite == "telemetry_Crosperf":
-                args_list.append("run_local=%s" % benchmark.run_local)
-                args_list.append(GetProfilerArgs(profiler_args))
-
-        return args_list
-
-    # TODO(zhizhouy): Currently do not support passing arguments or running
-    # customized tast tests, as we do not have such requirements.
-    def Tast_Run(self, machine, label, benchmark):
-        # Remove existing tast results
-        command = "rm -rf /usr/local/autotest/results/*"
-        self._ce.CrosRunCommand(
-            command, machine=machine, chromeos_root=label.chromeos_root
-        )
-
-        command = " ".join(
-            [TAST_PATH, "run", "-build=False", machine, benchmark.test_name]
-        )
-
-        if self.log_level != "verbose":
-            self.logger.LogOutput("Running test.")
-            self.logger.LogOutput("CMD: %s" % command)
-
-        return self._ce.ChrootRunCommandWOutput(
-            label.chromeos_root, command, command_terminator=self._ct
-        )
-
-    def Test_That_Run(
-        self, machine, label, benchmark, test_args, profiler_args
-    ):
-        """Run the test_that test.."""
-
-        # Remove existing test_that results
-        command = "rm -rf /usr/local/autotest/results/*"
-        self._ce.CrosRunCommand(
-            command, machine=machine, chromeos_root=label.chromeos_root
-        )
-
-        if benchmark.suite == "telemetry_Crosperf":
-            if not os.path.isdir(label.chrome_src):
-                self.logger.LogFatal(
-                    "Cannot find chrome src dir to "
-                    "run telemetry: %s" % label.chrome_src
-                )
-            # Check for and remove temporary file that may have been left by
-            # previous telemetry runs (and which might prevent this run from
-            # working).
-            self.RemoveTelemetryTempFile(machine, label.chromeos_root)
-
-        # --autotest_dir specifies which autotest directory to use.
-        autotest_dir_arg = "--autotest_dir=%s" % (
-            label.autotest_path if label.autotest_path else AUTOTEST_DIR
-        )
-
-        # --fast avoids unnecessary copies of syslogs.
-        fast_arg = "--fast"
-        board_arg = "--board=%s" % label.board
-
-        args_list = self.GenTestArgs(benchmark, test_args, profiler_args)
-        args_arg = "--args=%s" % pipes.quote(" ".join(args_list))
-
-        command = " ".join(
-            [
-                TEST_THAT_PATH,
-                autotest_dir_arg,
-                fast_arg,
-                board_arg,
-                args_arg,
-                machine,
-                benchmark.suite
-                if (
-                    benchmark.suite == "telemetry_Crosperf"
-                    or benchmark.suite == "crosperf_Wrapper"
-                )
-                else benchmark.test_name,
-            ]
-        )
-
-        # Use --no-ns-pid so that cros_sdk does not create a different
-        # process namespace and we can kill process created easily by their
-        # process group.
-        chrome_root_options = (
-            f"--no-ns-pid "
-            f"--chrome_root={label.chrome_src} --chrome_root_mount={CHROME_MOUNT_DIR} "
-            f'FEATURES="-usersandbox" '
-            f"CHROME_ROOT={CHROME_MOUNT_DIR}"
-        )
-
-        if self.log_level != "verbose":
-            self.logger.LogOutput("Running test.")
-            self.logger.LogOutput("CMD: %s" % command)
-
-        return self._ce.ChrootRunCommandWOutput(
-            label.chromeos_root,
-            command,
-            command_terminator=self._ct,
-            cros_sdk_options=chrome_root_options,
-        )
-
-    def DownloadResult(self, label, task_id):
-        gsutil_cmd = os.path.join(label.chromeos_root, GS_UTIL)
-        result_dir = "gs://chromeos-autotest-results/swarming-%s" % task_id
-        download_path = misc.GetOutsideChrootPath(label.chromeos_root, "/tmp")
-        ls_command = "%s ls %s" % (
-            gsutil_cmd,
-            os.path.join(result_dir, "autoserv_test"),
-        )
-        cp_command = "%s -mq cp -r %s %s" % (
-            gsutil_cmd,
-            result_dir,
-            download_path,
-        )
-
-        # Server sometimes will not be able to generate the result directory right
-        # after the test. Will try to access this gs location every 60s for
-        # RETRY_LIMIT mins.
-        t = 0
-        RETRY_LIMIT = 10
-        while t < RETRY_LIMIT:
-            t += 1
-            status = self._ce.RunCommand(ls_command, print_to_console=False)
-            if status == 0:
-                break
-            if t < RETRY_LIMIT:
-                self.logger.LogOutput(
-                    "Result directory not generated yet, "
-                    "retry (%d) in 60s." % t
-                )
-                time.sleep(60)
-            else:
-                self.logger.LogOutput(
-                    "No result directory for task %s" % task_id
-                )
-                return status
-
-        # Wait for 60s to make sure server finished writing to gs location.
-        time.sleep(60)
-
-        status = self._ce.RunCommand(cp_command)
-        if status != 0:
-            self.logger.LogOutput(
-                "Cannot download results from task %s" % task_id
-            )
-        else:
-            self.logger.LogOutput("Result downloaded for task %s" % task_id)
-        return status
-
-    def Crosfleet_Run(self, label, benchmark, test_args, profiler_args):
-        """Run the test via crosfleet.."""
-        options = []
-        if label.board:
-            options.append("-board=%s" % label.board)
-        if label.build:
-            options.append("-image=%s" % label.build)
-        # TODO: now only put toolchain pool here, user need to be able to specify
-        # which pool to use. Need to request feature to not use this option at all.
-        options.append("-pool=toolchain")
-
-        args_list = self.GenTestArgs(benchmark, test_args, profiler_args)
-        options.append("-test-args=%s" % pipes.quote(" ".join(args_list)))
-
-        dimensions = []
-        for dut in label.remote:
-            dimensions.append("-dim dut_name:%s" % dut.rstrip(".cros"))
-
-        command = ("%s create-test %s %s %s") % (
-            CROSFLEET_PATH,
-            " ".join(dimensions),
-            " ".join(options),
-            benchmark.suite
-            if (
-                benchmark.suite == "telemetry_Crosperf"
-                or benchmark.suite == "crosperf_Wrapper"
-            )
-            else benchmark.test_name,
-        )
-
-        if self.log_level != "verbose":
-            self.logger.LogOutput("Starting crosfleet test.")
-            self.logger.LogOutput("CMD: %s" % command)
-        ret_tup = self._ce.RunCommandWOutput(
-            command, command_terminator=self._ct
-        )
-
-        if ret_tup[0] != 0:
-            self.logger.LogOutput("Crosfleet test not created successfully.")
-            return ret_tup
-
-        # Std output of the command will look like:
-        # Created request at https://ci.chromium.org/../cros_test_platform/b12345
-        # We want to parse it and get the id number of the task, which is the
-        # number in the very end of the link address.
-        task_id = ret_tup[1].strip().split("b")[-1]
-
-        command = "crosfleet wait-task %s" % task_id
-        if self.log_level != "verbose":
-            self.logger.LogOutput("Waiting for crosfleet test to finish.")
-            self.logger.LogOutput("CMD: %s" % command)
-
-        ret_tup = self._ce.RunCommandWOutput(
-            command, command_terminator=self._ct
-        )
-
-        # The output of `wait-task` command will be a combination of verbose and a
-        # json format result in the end. The json result looks like this:
-        # {"task-result":
-        #   {"name":"Test Platform Invocation",
-        #    "state":"", "failure":false, "success":true,
-        #    "task-run-id":"12345",
-        #    "task-run-url":"https://ci.chromium.org/.../cros_test_platform/b12345",
-        #    "task-logs-url":""
-        #    },
-        #  "stdout":"",
-        #  "child-results":
-        #    [{"name":"graphics_WebGLAquarium",
-        #      "state":"", "failure":false, "success":true, "task-run-id":"",
-        #      "task-run-url":"https://chromeos-swarming.appspot.com/task?id=1234",
-        #      "task-logs-url":"https://stainless.corp.google.com/1234/"}
-        #    ]
-        # }
-        # We need the task id of the child-results to download result.
-        output = json.loads(ret_tup[1].split("\n")[-1])
-        output = output["child-results"][0]
-        if output["success"]:
-            task_id = output["task-run-url"].split("=")[-1]
-            if self.DownloadResult(label, task_id) == 0:
-                result_dir = "\nResults placed in tmp/swarming-%s\n" % task_id
-                return (ret_tup[0], result_dir, ret_tup[2])
-        return ret_tup
-
-    def CommandTerminator(self):
-        return self._ct
-
-    def Terminate(self):
-        self._ct.Terminate()
-
-
-class MockSuiteRunner(object):
-    """Mock suite runner for test."""
-
-    def __init__(self):
-        self._true = True
-
-    def Run(self, *_args):
-        if self._true:
-            return [0, "", ""]
-        else:
-            return [0, "", ""]
diff --git a/crosperf/suite_runner_unittest.py b/crosperf/suite_runner_unittest.py
deleted file mode 100755
index c936a074..00000000
--- a/crosperf/suite_runner_unittest.py
+++ /dev/null
@@ -1,411 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Copyright 2014 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unittest for suite_runner."""
-
-
-import contextlib
-import json
-import unittest
-import unittest.mock as mock
-
-from benchmark import Benchmark
-from cros_utils import command_executer
-from cros_utils import logger
-import label
-from machine_manager import MockCrosMachine
-import suite_runner
-
-
-class SuiteRunnerTest(unittest.TestCase):
-    """Class of SuiteRunner test."""
-
-    mock_json = mock.Mock(spec=json)
-    mock_cmd_exec = mock.Mock(spec=command_executer.CommandExecuter)
-    mock_cmd_term = mock.Mock(spec=command_executer.CommandTerminator)
-    mock_logger = mock.Mock(spec=logger.Logger)
-    mock_label = label.MockLabel(
-        "lumpy",
-        "build",
-        "lumpy_chromeos_image",
-        "",
-        "",
-        "/tmp/chromeos",
-        "lumpy",
-        ["lumpy1.cros", "lumpy.cros2"],
-        "",
-        "",
-        False,
-        "average",
-        "gcc",
-        False,
-        "",
-    )
-    telemetry_crosperf_bench = Benchmark(
-        "b1_test",  # name
-        "octane",  # test_name
-        "",  # test_args
-        3,  # iterations
-        False,  # rm_chroot_tmp
-        "record -e cycles",  # perf_args
-        "telemetry_Crosperf",  # suite
-        True,
-    )  # show_all_results
-
-    crosperf_wrapper_bench = Benchmark(
-        "b2_test",  # name
-        "webgl",  # test_name
-        "",  # test_args
-        3,  # iterations
-        False,  # rm_chroot_tmp
-        "",  # perf_args
-        "crosperf_Wrapper",
-    )  # suite
-
-    tast_bench = Benchmark(
-        "b3_test",  # name
-        "platform.ReportDiskUsage",  # test_name
-        "",  # test_args
-        1,  # iterations
-        False,  # rm_chroot_tmp
-        "",  # perf_args
-        "tast",
-    )  # suite
-
-    def __init__(self, *args, **kwargs):
-        super(SuiteRunnerTest, self).__init__(*args, **kwargs)
-        self.crosfleet_run_args = []
-        self.test_that_args = []
-        self.tast_args = []
-        self.call_crosfleet_run = False
-        self.call_test_that_run = False
-        self.call_tast_run = False
-
-    def setUp(self):
-        self.runner = suite_runner.SuiteRunner(
-            {},
-            self.mock_logger,
-            "verbose",
-            self.mock_cmd_exec,
-            self.mock_cmd_term,
-        )
-
-    def test_get_profiler_args(self):
-        input_str = (
-            "--profiler=custom_perf --profiler_args='perf_options"
-            '="record -a -e cycles,instructions"\''
-        )
-        output_str = (
-            "profiler=custom_perf profiler_args='record -a -e "
-            "cycles,instructions'"
-        )
-        res = suite_runner.GetProfilerArgs(input_str)
-        self.assertEqual(res, output_str)
-
-    def test_get_dut_config_args(self):
-        dut_config = {"enable_aslr": False, "top_interval": 1.0}
-        output_str = (
-            "dut_config="
-            "'"
-            '{"enable_aslr": '
-            'false, "top_interval": 1.0}'
-            "'"
-            ""
-        )
-        res = suite_runner.GetDutConfigArgs(dut_config)
-        self.assertEqual(res, output_str)
-
-    @mock.patch("suite_runner.ssh_tunnel")
-    def test_run(self, ssh_tunnel):
-        @contextlib.contextmanager
-        def mock_ssh_tunnel(_watcher, _host):
-            yield "fakelocalhost:1234"
-
-        ssh_tunnel.side_effect = mock_ssh_tunnel
-
-        def reset():
-            self.test_that_args = []
-            self.crosfleet_run_args = []
-            self.tast_args = []
-            self.call_test_that_run = False
-            self.call_crosfleet_run = False
-            self.call_tast_run = False
-
-        def FakeCrosfleetRun(test_label, benchmark, test_args, profiler_args):
-            self.crosfleet_run_args = [
-                test_label,
-                benchmark,
-                test_args,
-                profiler_args,
-            ]
-            self.call_crosfleet_run = True
-            return "Ran FakeCrosfleetRun"
-
-        def FakeTestThatRun(
-            machine, test_label, benchmark, test_args, profiler_args
-        ):
-            self.test_that_args = [
-                machine,
-                test_label,
-                benchmark,
-                test_args,
-                profiler_args,
-            ]
-            self.call_test_that_run = True
-            return "Ran FakeTestThatRun"
-
-        def FakeTastRun(machine, test_label, benchmark):
-            self.tast_args = [machine, test_label, benchmark]
-            self.call_tast_run = True
-            return "Ran FakeTastRun"
-
-        self.runner.Crosfleet_Run = FakeCrosfleetRun
-        self.runner.Test_That_Run = FakeTestThatRun
-        self.runner.Tast_Run = FakeTastRun
-
-        self.runner.dut_config["enable_aslr"] = False
-        self.runner.dut_config["cooldown_time"] = 0
-        self.runner.dut_config["governor"] = "fake_governor"
-        self.runner.dut_config["cpu_freq_pct"] = 65
-        self.runner.dut_config["intel_pstate"] = "no_hwp"
-        machine = "fake_machine"
-        cros_machine = MockCrosMachine(
-            machine, self.mock_label.chromeos_root, self.mock_logger
-        )
-        test_args = ""
-        profiler_args = ""
-
-        # Test crosfleet run for telemetry_Crosperf and crosperf_Wrapper benchmarks.
-        self.mock_label.crosfleet = True
-        reset()
-        self.runner.Run(
-            cros_machine,
-            self.mock_label,
-            self.crosperf_wrapper_bench,
-            test_args,
-            profiler_args,
-        )
-        self.assertTrue(self.call_crosfleet_run)
-        self.assertFalse(self.call_test_that_run)
-        self.assertEqual(
-            self.crosfleet_run_args,
-            [self.mock_label, self.crosperf_wrapper_bench, "", ""],
-        )
-
-        reset()
-        self.runner.Run(
-            cros_machine,
-            self.mock_label,
-            self.telemetry_crosperf_bench,
-            test_args,
-            profiler_args,
-        )
-        self.assertTrue(self.call_crosfleet_run)
-        self.assertFalse(self.call_test_that_run)
-        self.assertEqual(
-            self.crosfleet_run_args,
-            [self.mock_label, self.telemetry_crosperf_bench, "", ""],
-        )
-
-        # Test test_that run for telemetry_Crosperf and crosperf_Wrapper benchmarks.
-        self.mock_label.crosfleet = False
-        reset()
-        self.runner.Run(
-            cros_machine,
-            self.mock_label,
-            self.crosperf_wrapper_bench,
-            test_args,
-            profiler_args,
-        )
-        self.assertTrue(self.call_test_that_run)
-        self.assertFalse(self.call_crosfleet_run)
-        self.assertEqual(
-            self.test_that_args,
-            [
-                "fake_machine",
-                self.mock_label,
-                self.crosperf_wrapper_bench,
-                "",
-                "",
-            ],
-        )
-
-        reset()
-        self.runner.Run(
-            cros_machine,
-            self.mock_label,
-            self.telemetry_crosperf_bench,
-            test_args,
-            profiler_args,
-        )
-        self.assertTrue(self.call_test_that_run)
-        self.assertFalse(self.call_crosfleet_run)
-        self.assertEqual(
-            self.test_that_args,
-            [
-                "fake_machine",
-                self.mock_label,
-                self.telemetry_crosperf_bench,
-                "",
-                "",
-            ],
-        )
-
-        # Test tast run for tast benchmarks.
-        reset()
-        self.runner.Run(cros_machine, self.mock_label, self.tast_bench, "", "")
-        self.assertTrue(self.call_tast_run)
-        self.assertFalse(self.call_test_that_run)
-        self.assertFalse(self.call_crosfleet_run)
-        self.assertEqual(
-            self.tast_args,
-            ["fakelocalhost:1234", self.mock_label, self.tast_bench],
-        )
-
-    def test_gen_test_args(self):
-        test_args = "--iterations=2"
-        perf_args = "record -a -e cycles"
-
-        # Test crosperf_Wrapper benchmarks arg list generation
-        args_list = [
-            "test_args='--iterations=2'",
-            "dut_config='{}'",
-            "test=webgl",
-        ]
-        res = self.runner.GenTestArgs(
-            self.crosperf_wrapper_bench, test_args, ""
-        )
-        self.assertCountEqual(res, args_list)
-
-        # Test telemetry_Crosperf benchmarks arg list generation
-        args_list = [
-            "test_args='--iterations=2'",
-            "dut_config='{}'",
-            "test=octane",
-            "run_local=False",
-        ]
-        args_list.append(suite_runner.GetProfilerArgs(perf_args))
-        res = self.runner.GenTestArgs(
-            self.telemetry_crosperf_bench, test_args, perf_args
-        )
-        self.assertCountEqual(res, args_list)
-
-    @mock.patch.object(command_executer.CommandExecuter, "CrosRunCommand")
-    @mock.patch.object(
-        command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-    )
-    def test_tast_run(self, mock_chroot_runcmd, mock_cros_runcmd):
-        mock_chroot_runcmd.return_value = 0
-        self.mock_cmd_exec.ChrootRunCommandWOutput = mock_chroot_runcmd
-        self.mock_cmd_exec.CrosRunCommand = mock_cros_runcmd
-        res = self.runner.Tast_Run(
-            "lumpy1.cros", self.mock_label, self.tast_bench
-        )
-        self.assertEqual(mock_cros_runcmd.call_count, 1)
-        self.assertEqual(mock_chroot_runcmd.call_count, 1)
-        self.assertEqual(res, 0)
-        self.assertEqual(
-            mock_cros_runcmd.call_args_list[0][0],
-            ("rm -rf /usr/local/autotest/results/*",),
-        )
-        args_list = mock_chroot_runcmd.call_args_list[0][0]
-        args_dict = mock_chroot_runcmd.call_args_list[0][1]
-        self.assertEqual(len(args_list), 2)
-        self.assertEqual(args_dict["command_terminator"], self.mock_cmd_term)
-
-    @mock.patch.object(command_executer.CommandExecuter, "CrosRunCommand")
-    @mock.patch.object(
-        command_executer.CommandExecuter, "ChrootRunCommandWOutput"
-    )
-    @mock.patch.object(logger.Logger, "LogFatal")
-    def test_test_that_run(
-        self, mock_log_fatal, mock_chroot_runcmd, mock_cros_runcmd
-    ):
-        mock_log_fatal.side_effect = SystemExit()
-        self.runner.logger.LogFatal = mock_log_fatal
-        # Test crosperf_Wrapper benchmarks cannot take perf_args
-        raised_exception = False
-        try:
-            self.runner.Test_That_Run(
-                "lumpy1.cros",
-                self.mock_label,
-                self.crosperf_wrapper_bench,
-                "",
-                "record -a -e cycles",
-            )
-        except SystemExit:
-            raised_exception = True
-        self.assertTrue(raised_exception)
-
-        mock_chroot_runcmd.return_value = 0
-        self.mock_cmd_exec.ChrootRunCommandWOutput = mock_chroot_runcmd
-        self.mock_cmd_exec.CrosRunCommand = mock_cros_runcmd
-        res = self.runner.Test_That_Run(
-            "lumpy1.cros",
-            self.mock_label,
-            self.crosperf_wrapper_bench,
-            "--iterations=2",
-            "",
-        )
-        self.assertEqual(mock_cros_runcmd.call_count, 1)
-        self.assertEqual(mock_chroot_runcmd.call_count, 1)
-        self.assertEqual(res, 0)
-        self.assertEqual(
-            mock_cros_runcmd.call_args_list[0][0],
-            ("rm -rf /usr/local/autotest/results/*",),
-        )
-        args_list = mock_chroot_runcmd.call_args_list[0][0]
-        args_dict = mock_chroot_runcmd.call_args_list[0][1]
-        self.assertEqual(len(args_list), 2)
-        self.assertEqual(args_dict["command_terminator"], self.mock_cmd_term)
-
-    @mock.patch.object(command_executer.CommandExecuter, "RunCommandWOutput")
-    @mock.patch.object(json, "loads")
-    def test_crosfleet_run_client(self, mock_json_loads, mock_runcmd):
-        def FakeDownloadResult(l, task_id):
-            if l and task_id:
-                self.assertEqual(task_id, "12345")
-                return 0
-
-        mock_runcmd.return_value = (
-            0,
-            "Created Swarming task https://swarming/task/b12345",
-            "",
-        )
-        self.mock_cmd_exec.RunCommandWOutput = mock_runcmd
-
-        mock_json_loads.return_value = {
-            "child-results": [
-                {
-                    "success": True,
-                    "task-run-url": "https://swarming/task?id=12345",
-                }
-            ]
-        }
-        self.mock_json.loads = mock_json_loads
-
-        self.mock_label.crosfleet = True
-        self.runner.DownloadResult = FakeDownloadResult
-        res = self.runner.Crosfleet_Run(
-            self.mock_label, self.crosperf_wrapper_bench, "", ""
-        )
-        ret_tup = (0, "\nResults placed in tmp/swarming-12345\n", "")
-        self.assertEqual(res, ret_tup)
-        self.assertEqual(mock_runcmd.call_count, 2)
-
-        args_list = mock_runcmd.call_args_list[0][0]
-        args_dict = mock_runcmd.call_args_list[0][1]
-        self.assertEqual(len(args_list), 1)
-        self.assertEqual(args_dict["command_terminator"], self.mock_cmd_term)
-
-        args_list = mock_runcmd.call_args_list[1][0]
-        self.assertEqual(args_list[0], ("crosfleet wait-task 12345"))
-        self.assertEqual(args_dict["command_terminator"], self.mock_cmd_term)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/crosperf/test_cache/compare_output/autotest.tbz2 b/crosperf/test_cache/compare_output/autotest.tbz2
deleted file mode 100644
index 066dd9ac..00000000
Binary files a/crosperf/test_cache/compare_output/autotest.tbz2 and /dev/null differ
diff --git a/crosperf/test_cache/compare_output/machine.txt b/crosperf/test_cache/compare_output/machine.txt
deleted file mode 100644
index a82af3aa..00000000
--- a/crosperf/test_cache/compare_output/machine.txt
+++ /dev/null
@@ -1 +0,0 @@
-fake_machine_checksum123
\ No newline at end of file
diff --git a/crosperf/test_cache/compare_output/results.pickle b/crosperf/test_cache/compare_output/results.pickle
deleted file mode 100644
index 587863c5..00000000
Binary files a/crosperf/test_cache/compare_output/results.pickle and /dev/null differ
diff --git a/crosperf/test_cache/test_input/autotest.tbz2 b/crosperf/test_cache/test_input/autotest.tbz2
deleted file mode 100644
index 6ddbc6bf..00000000
Binary files a/crosperf/test_cache/test_input/autotest.tbz2 and /dev/null differ
diff --git a/crosperf/test_cache/test_input/machine.txt b/crosperf/test_cache/test_input/machine.txt
deleted file mode 100644
index 9bd78434..00000000
--- a/crosperf/test_cache/test_input/machine.txt
+++ /dev/null
@@ -1 +0,0 @@
-processor	: 0vendor_id	: GenuineIntelcpu family	: 6model		: 42model name	: Intel(R) Celeron(R) CPU 867 @ 1.30GHzstepping	: 7microcode	: 0x25cache size	: 2048 KBphysical id	: 0siblings	: 2core id		: 0cpu cores	: 2apicid		: 0initial apicid	: 0fpu		: yesfpu_exception	: yescpuid level	: 13wp		: yesflags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer xsave lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpidclflush size	: 64cache_alignment	: 64address sizes	: 36 bits physical, 48 bits virtualpower management:processor	: 1vendor_id	: GenuineIntelcpu family	: 6model		: 42model name	: Intel(R) Celeron(R) CPU 867 @ 1.30GHzstepping	: 7microcode	: 0x25cache size	: 2048 KBphysical id	: 0siblings	: 2core id		: 1cpu cores	: 2apicid		: 2initial apicid	: 2fpu		: yesfpu_exception	: yescpuid level	: 13wp		: yesflags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer xsave lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpidclflush size	: 64cache_alignment	: 64address sizes	: 36 bits physical, 48 bits virtualpower management: 4194304
\ No newline at end of file
diff --git a/crosperf/test_cache/test_input/results.pickle b/crosperf/test_cache/test_input/results.pickle
deleted file mode 100644
index 33ba6ab7..00000000
--- a/crosperf/test_cache/test_input/results.pickle
+++ /dev/null
@@ -1,6 +0,0 @@
-S"11:22:08 INFO | Running autotest_quickmerge step.\n11:22:08 INFO | quickmerge| 11:22:08: INFO: RunCommand: sudo -- /usr/bin/python2.7 /mnt/host/source/chromite/bin/autotest_quickmerge '--board=lumpy'\n11:22:08 INFO | quickmerge| 11:22:08: INFO: RunCommand: find /build/lumpy/usr/local/build/autotest/ -path /build/lumpy/usr/local/build/autotest/ExternalSource -prune -o -path /build/lumpy/usr/local/build/autotest/logs -prune -o -path /build/lumpy/usr/local/build/autotest/results -prune -o -path /build/lumpy/usr/local/build/autotest/site-packages -prune -o -printf '%T@\\n'\n11:22:22 INFO | quickmerge| 11:22:22: INFO: RunCommand: find /mnt/host/source/src/third_party/autotest/files/ -path /mnt/host/source/src/third_party/autotest/files/ExternalSource -prune -o -path /mnt/host/source/src/third_party/autotest/files/logs -prune -o -path /mnt/host/source/src/third_party/autotest/files/results -prune -o -path /mnt/host/source/src/third_party/autotest/files/site-packages -prune -o -printf '%T@\\n'\n11:22:32 INFO | quickmerge| 11:22:32: INFO: The sysroot appears to be newer than the source tree, doing nothing and exiting now.\n11:22:32 INFO | Re-running test_that script in /build/lumpy/usr/local/build/autotest copy of autotest.\n11:22:33 INFO | Began logging to /tmp/test_that_results_zZZfQa\nAdding labels [u'cros-version:ad_hoc_build', u'board:lumpy'] to host chromeos2-row2-rack4-host11.cros\n13:22:33 INFO | Fetching suite for job named telemetry_Crosperf...\n13:22:43 INFO | Scheduling suite for job named telemetry_Crosperf...\n13:22:43 INFO | ... scheduled 1 job(s).\n13:22:43 INFO | autoserv| DEBUG:root:import statsd failed, no stats will be reported.\n13:22:43 INFO | autoserv| Results placed in /tmp/test_that_results_zZZfQa/results-1-telemetry_Crosperf\n13:22:43 INFO | autoserv| Logged pid 25397 to /tmp/test_that_results_zZZfQa/results-1-telemetry_Crosperf/.autoserv_execute\n13:22:43 INFO | autoserv| I am PID 25397\n13:22:43 INFO | autoserv| Not checking if job_repo_url contains autotest packages on ['chromeos2-row2-rack4-host11.cros']\n13:22:43 INFO | autoserv| Processing control file\n13:22:44 INFO | autoserv| START\ttelemetry_Crosperf\ttelemetry_Crosperf\ttimestamp=1401301364\tlocaltime=May 28 11:22:44\n13:22:44 INFO | autoserv| Starting master ssh connection '/usr/bin/ssh -a -x -N -o ControlMaster=yes -o ControlPath=/tmp/_autotmp_HsB3vQssh-master/socket -o StrictHostKeyChecking=no -o UserKnownHostsFile=/tmp/tmpxFy6lj -o BatchMode=yes -o ConnectTimeout=30 -o ServerAliveInterval=300 -l root -p 22 chromeos2-row2-rack4-host11.cros'\n13:22:45 INFO | autoserv| Starting master ssh connection '/usr/bin/ssh -a -x   -N -o ControlMaster=yes -o ControlPath=/tmp/_autotmp_YTu9wYssh-master/socket -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o ConnectTimeout=30 -o ServerAliveInterval=180 -o ServerAliveCountMax=3 -o ConnectionAttempts=4 -o Protocol=2 -l root -p 22 chromeos2-row2-rack4-host11.cros'\n13:22:45 INFO | autoserv| Installing autotest on chromeos2-row2-rack4-host11.cros\n13:22:45 INFO | autoserv| Using installation dir /tmp/sysinfo/autoserv-MxOMOw\n13:22:46 INFO | autoserv| No job_repo_url for <remote host: chromeos2-row2-rack4-host11.cros>\n13:22:46 INFO | autoserv| Could not install autotest using the packaging system: No repos to install an autotest client from. Trying other methods\n13:22:47 INFO | autoserv| Installation of autotest completed\n13:22:47 INFO | autoserv| Installing updated global_config.ini.\n13:22:48 INFO | autoserv| No job_repo_url for <remote host: chromeos2-row2-rack4-host11.cros>\n13:22:48 INFO | autoserv| Executing /tmp/sysinfo/autoserv-MxOMOw/bin/autotest /tmp/sysinfo/autoserv-MxOMOw/control phase 0\n13:22:48 INFO | autoserv| Entered autotestd_monitor.\n13:22:48 INFO | autoserv| Finished launching tail subprocesses.\n13:22:48 INFO | autoserv| Finished waiting on autotestd to start.\n13:22:48 INFO | autoserv| START\t----\t----\ttimestamp=1401301368\tlocaltime=May 28 11:22:48\n13:22:48 INFO | autoserv| GOOD\t----\tsysinfo.before\ttimestamp=1401301368\tlocaltime=May 28 11:22:48\n13:22:48 INFO | autoserv| END GOOD\t----\t----\ttimestamp=1401301368\tlocaltime=May 28 11:22:48\n13:22:48 INFO | autoserv| Got lock of exit_code_file.\n13:22:48 INFO | autoserv| Released lock of exit_code_file and closed it.\n13:22:50 INFO | autoserv| Killing child processes.\n13:22:50 INFO | autoserv| Client complete\n13:22:52 INFO | autoserv| No job_repo_url for <remote host: chromeos2-row2-rack4-host11.cros>\n13:22:52 INFO | autoserv| Executing /tmp/sysinfo/autoserv-MxOMOw/bin/autotest /tmp/sysinfo/autoserv-MxOMOw/control phase 0\n13:22:53 INFO | autoserv| Entered autotestd_monitor.\n13:22:53 INFO | autoserv| Finished launching tail subprocesses.\n13:22:53 INFO | autoserv| Finished waiting on autotestd to start.\n13:22:53 INFO | autoserv| START\t----\t----\ttimestamp=1401301373\tlocaltime=May 28 11:22:53\n13:22:53 INFO | autoserv| GOOD\t----\tsysinfo.iteration.before\ttimestamp=1401301373\tlocaltime=May 28 11:22:53\n13:22:53 INFO | autoserv| END GOOD\t----\t----\ttimestamp=1401301373\tlocaltime=May 28 11:22:53\n13:22:53 INFO | autoserv| Got lock of exit_code_file.\n13:22:53 INFO | autoserv| Released lock of exit_code_file and closed it.\n13:22:55 INFO | autoserv| Killing child processes.\n13:22:55 INFO | autoserv| Client complete\n13:22:55 INFO | autoserv| Using Chrome source tree at /tmp/chrome_root\n13:22:55 INFO | autoserv| CMD: /tmp/chrome_root/src/tools/perf/run_benchmark --browser=cros-chrome --remote=chromeos2-row2-rack4-host11.cros  sunspider\n13:23:35 INFO | autoserv| Telemetry completed with exit code: 0.\n13:23:35 INFO | autoserv| stdout:Pages: [http___www.webkit.org_perf_sunspider-1.0.2_sunspider-1.0.2_driver.html]\n13:23:35 INFO | autoserv| RESULT 3d-cube: 3d-cube= [28,28,28,28,31,26,28,28,28,27] ms\n13:23:35 INFO | autoserv| Avg 3d-cube: 28.000000ms\n13:23:35 INFO | autoserv| Sd  3d-cube: 1.247219ms\n13:23:35 INFO | autoserv| RESULT 3d-morph: 3d-morph= [23,22,22,22,22,22,22,22,22,22] ms\n13:23:35 INFO | autoserv| Avg 3d-morph: 22.100000ms\n13:23:35 INFO | autoserv| Sd  3d-morph: 0.316228ms\n13:23:35 INFO | autoserv| RESULT 3d-raytrace: 3d-raytrace= [26,23,24,25,25,25,26,24,24,25] ms\n13:23:35 INFO | autoserv| Avg 3d-raytrace: 24.700000ms\n13:23:35 INFO | autoserv| Sd  3d-raytrace: 0.948683ms\n13:23:35 INFO | autoserv| *RESULT Total: Total= [443,440,440,447,451,435,441,449,449,445] ms\n13:23:35 INFO | autoserv| Avg Total: 444.000000ms\n13:23:35 INFO | autoserv| Sd  Total: 5.077182ms\n13:23:35 INFO | autoserv| RESULT access-binary-trees: access-binary-trees= [4,3,5,6,5,5,3,5,5,4] ms\n13:23:35 INFO | autoserv| Avg access-binary-trees: 4.500000ms\n13:23:35 INFO | autoserv| Sd  access-binary-trees: 0.971825ms\n13:23:35 INFO | autoserv| RESULT access-fannkuch: access-fannkuch= [19,18,17,18,17,18,18,18,17,18] ms\n13:23:35 INFO | autoserv| Avg access-fannkuch: 17.800000ms\n13:23:35 INFO | autoserv| Sd  access-fannkuch: 0.632456ms\n13:23:35 INFO | autoserv| RESULT access-nbody: access-nbody= [7,9,8,7,12,8,9,10,8,7] ms\n13:23:35 INFO | autoserv| Avg access-nbody: 8.500000ms\n13:23:35 INFO | autoserv| Sd  access-nbody: 1.581139ms\n13:23:35 INFO | autoserv| RESULT access-nsieve: access-nsieve= [9,8,8,8,8,7,8,7,8,8] ms\n13:23:35 INFO | autoserv| Avg access-nsieve: 7.900000ms\n13:23:35 INFO | autoserv| Sd  access-nsieve: 0.567646ms\n13:23:35 INFO | autoserv| RESULT bitops-3bit-bits-in-byte: bitops-3bit-bits-in-byte= [3,3,3,3,3,3,3,4,4,3] ms\n13:23:35 INFO | autoserv| Avg bitops-3bit-bits-in-byte: 3.200000ms\n13:23:35 INFO | autoserv| Sd  bitops-3bit-bits-in-byte: 0.421637ms\n13:23:35 INFO | autoserv| RESULT bitops-bits-in-byte: bitops-bits-in-byte= [9,9,9,9,9,9,9,9,9,10] ms\n13:23:35 INFO | autoserv| Avg bitops-bits-in-byte: 9.100000ms\n13:23:35 INFO | autoserv| Sd  bitops-bits-in-byte: 0.316228ms\n13:23:35 INFO | autoserv| RESULT bitops-bitwise-and: bitops-bitwise-and= [8,8,7,9,8,9,8,8,9,10] ms\n13:23:35 INFO | autoserv| Avg bitops-bitwise-and: 8.400000ms\n13:23:35 INFO | autoserv| Sd  bitops-bitwise-and: 0.843274ms\n13:23:35 INFO | autoserv| RESULT bitops-nsieve-bits: bitops-nsieve-bits= [9,9,9,9,9,9,9,11,11,9] ms\n13:23:35 INFO | autoserv| Avg bitops-nsieve-bits: 9.400000ms\n13:23:35 INFO | autoserv| Sd  bitops-nsieve-bits: 0.843274ms\n13:23:35 INFO | autoserv| RESULT controlflow-recursive: controlflow-recursive= [5,5,5,4,4,4,5,4,4,4] ms\n13:23:35 INFO | autoserv| Avg controlflow-recursive: 4.400000ms\n13:23:35 INFO | autoserv| Sd  controlflow-recursive: 0.516398ms\n13:23:35 INFO | autoserv| RESULT crypto-aes: crypto-aes= [14,16,15,16,15,14,17,14,15,16] ms\n13:23:35 INFO | autoserv| Avg crypto-aes: 15.200000ms\n13:23:35 INFO | autoserv| Sd  crypto-aes: 1.032796ms\n13:23:35 INFO | autoserv| RESULT crypto-md5: crypto-md5= [10,11,11,11,10,10,11,10,10,11] ms\n13:23:35 INFO | autoserv| Avg crypto-md5: 10.500000ms\n13:23:35 INFO | autoserv| Sd  crypto-md5: 0.527046ms\n13:23:35 INFO | autoserv| RESULT crypto-sha1: crypto-sha1= [11,11,12,12,12,12,12,10,13,11] ms\n13:23:35 INFO | autoserv| Avg crypto-sha1: 11.600000ms\n13:23:35 INFO | autoserv| Sd  crypto-sha1: 0.843274ms\n13:23:35 INFO | autoserv| RESULT date-format-tofte: date-format-tofte= [28,25,25,26,26,27,26,28,27,25] ms\n13:23:35 INFO | autoserv| Avg date-format-tofte: 26.300000ms\n13:23:35 INFO | autoserv| Sd  date-format-tofte: 1.159502ms\n13:23:35 INFO | autoserv| RESULT date-format-xparb: date-format-xparb= [21,22,21,21,21,20,20,20,21,22] ms\n13:23:35 INFO | autoserv| Avg date-format-xparb: 20.900000ms\n13:23:35 INFO | autoserv| Sd  date-format-xparb: 0.737865ms\n13:23:35 INFO | autoserv| RESULT math-cordic: math-cordic= [8,8,8,9,9,9,9,9,9,9] ms\n13:23:35 INFO | autoserv| Avg math-cordic: 8.700000ms\n13:23:35 INFO | autoserv| Sd  math-cordic: 0.483046ms\n13:23:35 INFO | autoserv| RESULT math-partial-sums: math-partial-sums= [22,22,22,21,23,20,20,23,25,22] ms\n13:23:35 INFO | autoserv| Avg math-partial-sums: 22.000000ms\n13:23:35 INFO | autoserv| Sd  math-partial-sums: 1.490712ms\n13:23:35 INFO | autoserv| RESULT math-spectral-norm: math-spectral-norm= [6,7,6,7,7,6,7,6,7,7] ms\n13:23:35 INFO | autoserv| Avg math-spectral-norm: 6.600000ms\n13:23:35 INFO | autoserv| Sd  math-spectral-norm: 0.516398ms\n13:23:35 INFO | autoserv| RESULT regexp-dna: regexp-dna= [16,16,17,16,16,16,16,16,17,16] ms\n13:23:35 INFO | autoserv| Avg regexp-dna: 16.200000ms\n13:23:35 INFO | autoserv| Sd  regexp-dna: 0.421637ms\n13:23:35 INFO | autoserv| RESULT string-base64: string-base64= [17,16,16,16,17,16,16,16,14,16] ms\n13:23:35 INFO | autoserv| Avg string-base64: 16.000000ms\n13:23:35 INFO | autoserv| Sd  string-base64: 0.816497ms\n13:23:35 INFO | autoserv| RESULT string-fasta: string-fasta= [23,22,23,24,23,23,23,25,23,23] ms\n13:23:35 INFO | autoserv| Avg string-fasta: 23.200000ms\n13:23:35 INFO | autoserv| Sd  string-fasta: 0.788811ms\n13:23:35 INFO | autoserv| RESULT string-tagcloud: string-tagcloud= [53,52,54,53,53,52,51,54,53,53] ms\n13:23:35 INFO | autoserv| Avg string-tagcloud: 52.800000ms\n13:23:35 INFO | autoserv| Sd  string-tagcloud: 0.918937ms\n13:23:35 INFO | autoserv| RESULT string-unpack-code: string-unpack-code= [46,47,46,48,47,46,46,47,47,47] ms\n13:23:35 INFO | autoserv| Avg string-unpack-code: 46.700000ms\n13:23:35 INFO | autoserv| Sd  string-unpack-code: 0.674949ms\n13:23:35 INFO | autoserv| RESULT string-validate-input: string-validate-input= [18,20,19,19,19,19,19,21,19,20] ms\n13:23:35 INFO | autoserv| Avg string-validate-input: 19.300000ms\n13:23:35 INFO | autoserv| Sd  string-validate-input: 0.823273ms\n13:23:35 INFO | autoserv| RESULT telemetry_page_measurement_results: num_failed= 0 count\n13:23:35 INFO | autoserv| RESULT telemetry_page_measurement_results: num_errored= 0 count\n13:23:35 INFO | autoserv| \n13:23:35 INFO | autoserv| View result at file:///tmp/chrome_root/src/tools/perf/results.html\n13:23:35 INFO | autoserv| \n13:23:35 INFO | autoserv| stderr:\n13:23:35 INFO | autoserv| No job_repo_url for <remote host: chromeos2-row2-rack4-host11.cros>\n13:23:35 INFO | autoserv| Executing /tmp/sysinfo/autoserv-MxOMOw/bin/autotest /tmp/sysinfo/autoserv-MxOMOw/control phase 0\n13:23:36 INFO | autoserv| Entered autotestd_monitor.\n13:23:36 INFO | autoserv| Finished launching tail subprocesses.\n13:23:36 INFO | autoserv| Finished waiting on autotestd to start.\n13:23:37 INFO | autoserv| START\t----\t----\ttimestamp=1401301417\tlocaltime=May 28 11:23:37\n13:23:37 INFO | autoserv| GOOD\t----\tsysinfo.iteration.after\ttimestamp=1401301417\tlocaltime=May 28 11:23:37\n13:23:37 INFO | autoserv| END GOOD\t----\t----\ttimestamp=1401301417\tlocaltime=May 28 11:23:37\n13:23:37 INFO | autoserv| Got lock of exit_code_file.\n13:23:37 INFO | autoserv| Released lock of exit_code_file and closed it.\n13:23:39 INFO | autoserv| Killing child processes.\n13:23:39 INFO | autoserv| Client complete\n13:23:39 INFO | autoserv| No job_repo_url for <remote host: chromeos2-row2-rack4-host11.cros>\n13:23:40 INFO | autoserv| Executing /tmp/sysinfo/autoserv-MxOMOw/bin/autotest /tmp/sysinfo/autoserv-MxOMOw/control phase 0\n13:23:40 INFO | autoserv| Entered autotestd_monitor.\n13:23:40 INFO | autoserv| Finished launching tail subprocesses.\n13:23:40 INFO | autoserv| Finished waiting on autotestd to start.\n13:23:40 INFO | autoserv| START\t----\t----\ttimestamp=1401301420\tlocaltime=May 28 11:23:40\n13:23:40 INFO | autoserv| GOOD\t----\tsysinfo.after\ttimestamp=1401301420\tlocaltime=May 28 11:23:40\n13:23:40 INFO | autoserv| END GOOD\t----\t----\ttimestamp=1401301420\tlocaltime=May 28 11:23:40\n13:23:40 INFO | autoserv| Got lock of exit_code_file.\n13:23:40 INFO | autoserv| Released lock of exit_code_file and closed it.\n13:23:42 INFO | autoserv| Killing child processes.\n13:23:42 INFO | autoserv| Client complete\n13:23:44 INFO | autoserv| GOOD\ttelemetry_Crosperf\ttelemetry_Crosperf\ttimestamp=1401301424\tlocaltime=May 28 11:23:44\tcompleted successfully\n13:23:44 INFO | autoserv| END GOOD\ttelemetry_Crosperf\ttelemetry_Crosperf\ttimestamp=1401301424\tlocaltime=May 28 11:23:44\n13:23:44 INFO | autoserv| Finished processing control file\n13:23:44 INFO | autoserv| Starting master ssh connection '/usr/bin/ssh -a -x -N -o ControlMaster=yes -o ControlPath=/tmp/_autotmp_UyjlWMssh-master/socket -o StrictHostKeyChecking=no -o UserKnownHostsFile=/tmp/tmpCvMigR -o BatchMode=yes -o ConnectTimeout=30 -o ServerAliveInterval=300 -l root -p 22 chromeos2-row2-rack4-host11.cros'\n13:23:45 INFO | autoserv| Starting master ssh connection '/usr/bin/ssh -a -x   -N -o ControlMaster=yes -o ControlPath=/tmp/_autotmp_w_KGTassh-master/socket -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o ConnectTimeout=30 -o ServerAliveInterval=180 -o ServerAliveCountMax=3 -o ConnectionAttempts=4 -o Protocol=2 -l root -p 22 chromeos2-row2-rack4-host11.cros'\n-----------------------------------------------------------------------------------------\n/tmp/test_that_results_zZZfQa/results-1-telemetry_Crosperf                    [  PASSED  ]\n/tmp/test_that_results_zZZfQa/results-1-telemetry_Crosperf/telemetry_Crosperf [  PASSED  ]\n-----------------------------------------------------------------------------------------\nTotal PASS: 2/2 (100%)\n\n13:23:47 INFO | Finished running tests. Results can be found in /tmp/test_that_results_zZZfQa\n"
-p0
-.S'INFO:root:Identity added: /tmp/test_that_results_PPRMIh/testing_rsa (/tmp/test_that_results_PPRMIh/testing_rsa)\nINFO:root:Identity added: /tmp/test_that_results_zZZfQa/testing_rsa (/tmp/test_that_results_zZZfQa/testing_rsa)\n'
-p0
-.I0
-.
\ No newline at end of file
diff --git a/crosperf/test_cache/test_puretelemetry_input/machine.txt b/crosperf/test_cache/test_puretelemetry_input/machine.txt
deleted file mode 100644
index 9bd78434..00000000
--- a/crosperf/test_cache/test_puretelemetry_input/machine.txt
+++ /dev/null
@@ -1 +0,0 @@
-processor	: 0vendor_id	: GenuineIntelcpu family	: 6model		: 42model name	: Intel(R) Celeron(R) CPU 867 @ 1.30GHzstepping	: 7microcode	: 0x25cache size	: 2048 KBphysical id	: 0siblings	: 2core id		: 0cpu cores	: 2apicid		: 0initial apicid	: 0fpu		: yesfpu_exception	: yescpuid level	: 13wp		: yesflags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer xsave lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpidclflush size	: 64cache_alignment	: 64address sizes	: 36 bits physical, 48 bits virtualpower management:processor	: 1vendor_id	: GenuineIntelcpu family	: 6model		: 42model name	: Intel(R) Celeron(R) CPU 867 @ 1.30GHzstepping	: 7microcode	: 0x25cache size	: 2048 KBphysical id	: 0siblings	: 2core id		: 1cpu cores	: 2apicid		: 2initial apicid	: 2fpu		: yesfpu_exception	: yescpuid level	: 13wp		: yesflags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer xsave lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpidclflush size	: 64cache_alignment	: 64address sizes	: 36 bits physical, 48 bits virtualpower management: 4194304
\ No newline at end of file
diff --git a/crosperf/test_cache/test_puretelemetry_input/results.pickle b/crosperf/test_cache/test_puretelemetry_input/results.pickle
deleted file mode 100644
index 497d1cf3..00000000
--- a/crosperf/test_cache/test_puretelemetry_input/results.pickle
+++ /dev/null
@@ -1,6 +0,0 @@
-S'page_name,3d-cube (ms),3d-morph (ms),3d-raytrace (ms),Total (ms),access-binary-trees (ms),access-fannkuch (ms),access-nbody (ms),access-nsieve (ms),bitops-3bit-bits-in-byte (ms),bitops-bits-in-byte (ms),bitops-bitwise-and (ms),bitops-nsieve-bits (ms),controlflow-recursive (ms),crypto-aes (ms),crypto-md5 (ms),crypto-sha1 (ms),date-format-tofte (ms),date-format-xparb (ms),math-cordic (ms),math-partial-sums (ms),math-spectral-norm (ms),regexp-dna (ms),string-base64 (ms),string-fasta (ms),string-tagcloud (ms),string-unpack-code (ms),string-validate-input (ms)\r\nhttp://www.webkit.org/perf/sunspider-1.0.2/sunspider-1.0.2/driver.html,42.7,50.2,28.7,656.5,7.3,26.3,6.9,8.6,3.5,9.8,8.8,9.3,5.3,19.2,10.8,12.4,31.2,138.1,11.4,32.8,6.3,16.1,17.5,36.3,47.2,45.0,24.8\r\n'
-p0
-.S''
-p0
-.I0
-.
\ No newline at end of file
diff --git a/crosperf/test_flag.py b/crosperf/test_flag.py
deleted file mode 100644
index 17c17a3d..00000000
--- a/crosperf/test_flag.py
+++ /dev/null
@@ -1,16 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2011 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""A global variable for testing."""
-
-is_test = [False]
-
-
-def SetTestMode(flag):
-    is_test[0] = flag
-
-
-def GetTestMode():
-    return is_test[0]
diff --git a/crosperf/translate_xbuddy.py b/crosperf/translate_xbuddy.py
deleted file mode 100755
index e6a53a94..00000000
--- a/crosperf/translate_xbuddy.py
+++ /dev/null
@@ -1,42 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Module to translate the xbuddy config."""
-
-
-import os
-import sys
-
-
-if "/mnt/host/source/src/third_party/toolchain-utils/crosperf" in sys.path:
-    dev_path = os.path.expanduser("~/trunk/chromite/lib/xbuddy")
-    sys.path.append(dev_path)
-else:
-    print(
-        "This script can only be run from inside a ChromeOS chroot.  Please "
-        "enter your chroot, go to ~/src/third_party/toolchain-utils/crosperf"
-        " and try again."
-    )
-    sys.exit(0)
-
-# pylint: disable=import-error,wrong-import-position
-import xbuddy
-
-
-def Main(xbuddy_string):
-    if not os.path.exists("./xbuddy_config.ini"):
-        config_path = os.path.expanduser(
-            "~/trunk/chromite/lib/xbuddy/xbuddy_config.ini"
-        )
-        os.symlink(config_path, "./xbuddy_config.ini")
-    x = xbuddy.XBuddy(manage_builds=False, static_dir="/tmp/devserver/static")
-    build_id = x.Translate(os.path.split(xbuddy_string))
-    return build_id
-
-
-if __name__ == "__main__":
-    print(Main(sys.argv[1]))
-    sys.exit(0)
diff --git a/crosperf/unittest_keyval_file.txt b/crosperf/unittest_keyval_file.txt
deleted file mode 100644
index cc76398e..00000000
--- a/crosperf/unittest_keyval_file.txt
+++ /dev/null
@@ -1,20 +0,0 @@
-{"description": "Box2D", "graph": "Box2D", "higher_is_better": true, "units": "score", "value": 4775}
-{"description": "CodeLoad", "graph": "CodeLoad", "higher_is_better": true, "units": "score", "value": 6271}
-{"description": "Crypto", "graph": "Crypto", "higher_is_better": true, "units": "score", "value": 8737}
-{"description": "DeltaBlue", "graph": "DeltaBlue", "higher_is_better": true, "units": "score", "value": 14401}
-{"description": "EarleyBoyer", "graph": "EarleyBoyer", "higher_is_better": true, "units": "score", "value": 14340}
-{"description": "Gameboy", "graph": "Gameboy", "higher_is_better": true, "units": "score", "value": 9901}
-{"description": "Mandreel", "graph": "Mandreel", "higher_is_better": true, "units": "score", "value": 6620}
-{"description": "MandreelLatency", "graph": "MandreelLatency", "higher_is_better": true, "units": "score", "value": 5188}
-{"description": "NavierStokes", "graph": "NavierStokes", "higher_is_better": true, "units": "score", "value": 9815}
-{"description": "PdfJS", "graph": "PdfJS", "higher_is_better": true, "units": "score", "value": 6455}
-{"description": "RayTrace", "graph": "RayTrace", "higher_is_better": true, "units": "score", "value": 16600}
-{"description": "RegExp", "graph": "RegExp", "higher_is_better": true, "units": "score", "value": 1765}
-{"description": "Richards", "graph": "Richards", "higher_is_better": true, "units": "score", "value": 10358}
-{"description": "Splay", "graph": "Splay", "higher_is_better": true, "units": "score", "value": 4425}
-{"description": "SplayLatency", "graph": "SplayLatency", "higher_is_better": true, "units": "score", "value": 7653}
-{"description": "Typescript", "graph": "Typescript", "higher_is_better": true, "units": "score", "value": 9815}
-{"description": "zlib", "graph": "zlib", "higher_is_better": true, "units": "score", "value": 16094}
-{"description": "Score", "graph": "Total", "higher_is_better": true, "units": "score", "value": 7918}
-{"description": "num_failed", "graph": "telemetry_page_measurement_results", "higher_is_better": true, "units": "count", "value": 0}
-{"description": "num_errored", "graph": "telemetry_page_measurement_results", "higher_is_better": true, "units": "count", "value": 0}
diff --git a/cwp/cr-os/README.md b/cwp/cr_os/README.md
similarity index 100%
rename from cwp/cr-os/README.md
rename to cwp/cr_os/README.md
diff --git a/cwp/cr-os/fetch_gn_descs.py b/cwp/cr_os/fetch_gn_descs.py
old mode 100755
new mode 100644
similarity index 98%
rename from cwp/cr-os/fetch_gn_descs.py
rename to cwp/cr_os/fetch_gn_descs.py
index 50b555ad..d30067f0
--- a/cwp/cr-os/fetch_gn_descs.py
+++ b/cwp/cr_os/fetch_gn_descs.py
@@ -1,5 +1,3 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
 # Copyright 2020 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -205,7 +203,3 @@ def main(args):
     with open(results_intermed, "w", encoding="utf-8") as f:
         json.dump(results, f)
     os.rename(results_intermed, results_file)
-
-
-if __name__ == "__main__":
-    sys.exit(main(sys.argv[1:]))
diff --git a/cwp/cr-os/fetch_gn_descs_test.py b/cwp/cr_os/fetch_gn_descs_test.py
old mode 100755
new mode 100644
similarity index 97%
rename from cwp/cr-os/fetch_gn_descs_test.py
rename to cwp/cr_os/fetch_gn_descs_test.py
index 8a88fe3e..468d0b12
--- a/cwp/cr-os/fetch_gn_descs_test.py
+++ b/cwp/cr_os/fetch_gn_descs_test.py
@@ -10,7 +10,7 @@
 import io
 import unittest
 
-import fetch_gn_descs
+from cwp.cr_os import fetch_gn_descs
 
 # pylint: disable=protected-access
 
@@ -117,7 +117,3 @@ class Test(unittest.TestCase):
         warnings, desc_json = fetch_gn_descs._parse_gn_desc_output(gn_desc)
         self.assertEqual(warnings, "")
         self.assertEqual(desc_json, {"bar": "baz"})
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/debug_info_test/allowlist.py b/debug_info_test/allowlist.py
deleted file mode 100644
index 70bb776a..00000000
--- a/debug_info_test/allowlist.py
+++ /dev/null
@@ -1,67 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2018 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Allowlist functions."""
-
-
-import glob
-import os
-import re
-
-
-# Matching a string of length m in an NFA of size n is O(mn^2), but the
-# performance also depends largely on the implementation. It appears to be fast
-# enough according to the tests.
-#
-# The performance bottleneck of this script is readelf. Unless this becomes
-# slower than readelf, don't waste time here.
-def is_allowlisted(list_name, pattern):
-    """Check whether the given pattern is specified in the allowlist.
-
-    Args:
-      list_name: name of the allowlist.
-      pattern: the target string.
-
-    Returns:
-      True if matched otherwise False.
-    """
-    return pattern and allowlists[list_name].match(pattern)
-
-
-def prepare_allowlist(patterns):
-    """Join and compile the re patterns.
-
-    Args:
-      patterns: regex patterns.
-
-    Returns:
-      A compiled re object.
-    """
-    return re.compile("|".join(patterns))
-
-
-def load_allowlists(dirname):
-    """Load allowlists under dirname.
-
-    An allowlist ends with .allowlist.
-
-    Args:
-      dirname: path to the dir.
-
-    Returns:
-      A dictionary of 'filename' -> allowlist matcher.
-    """
-    wlist = {}
-    for fn in glob.glob(os.path.join(dirname, "*.allowlist")):
-        key = os.path.splitext(os.path.basename(fn))[0]
-        with open(fn, "r", encoding="utf-8") as f:
-            patterns = f.read().splitlines()
-            patterns = [l for l in patterns if l != ""]
-            patterns = [l for l in patterns if l[0] != "#"]
-        wlist[key] = prepare_allowlist(patterns)
-    return wlist
-
-
-allowlists = load_allowlists(os.path.dirname(__file__))
diff --git a/debug_info_test/check_cus.py b/debug_info_test/check_cus.py
deleted file mode 100644
index dbf22d08..00000000
--- a/debug_info_test/check_cus.py
+++ /dev/null
@@ -1,78 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2018 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""check compile units."""
-
-
-import os
-import subprocess
-
-import check_ngcc
-
-
-cu_checks = [check_ngcc.not_by_gcc]
-
-
-def check_compile_unit(dso_path, producer, comp_path):
-    """check all compiler flags used to build the compile unit.
-
-    Args:
-      dso_path: path to the elf/dso.
-      producer: DW_AT_producer contains the compiler command line.
-      comp_path: DW_AT_comp_dir + DW_AT_name.
-
-    Returns:
-      A set of failed tests.
-    """
-    failed = set()
-    for c in cu_checks:
-        if not c(dso_path, producer, comp_path):
-            failed.add(c.__module__)
-
-    return failed
-
-
-def check_compile_units(dso_path):
-    """check all compile units in the given dso.
-
-    Args:
-      dso_path: path to the dso.
-
-    Returns:
-      True if everything looks fine otherwise False.
-    """
-
-    failed = set()
-    producer = ""
-    comp_path = ""
-
-    readelf = subprocess.Popen(
-        ["llvm-dwarfdump", "--recurse-depth=0", dso_path],
-        stdout=subprocess.PIPE,
-        stderr=open(os.devnull, "w"),
-        encoding="utf-8",
-    )
-    for l in readelf.stdout:
-        if "DW_TAG_compile_unit" in l:
-            if producer:
-                failed = failed.union(
-                    check_compile_unit(dso_path, producer, comp_path)
-                )
-            producer = ""
-            comp_path = ""
-        elif "DW_AT_producer" in l:
-            producer = l
-        elif "DW_AT_name" in l:
-            comp_path = os.path.join(comp_path, l.split(":")[-1].strip())
-        elif "DW_AT_comp_dir" in l:
-            comp_path = os.path.join(l.split(":")[-1].strip(), comp_path)
-    if producer:
-        failed = failed.union(check_compile_unit(dso_path, producer, comp_path))
-
-    if failed:
-        print("%s failed check: %s" % (dso_path, " ".join(failed)))
-        return False
-
-    return True
diff --git a/debug_info_test/check_exist.py b/debug_info_test/check_exist.py
deleted file mode 100644
index 863c591f..00000000
--- a/debug_info_test/check_exist.py
+++ /dev/null
@@ -1,102 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2018 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""check whether intended components exists in the given dso."""
-
-
-import os
-import subprocess
-
-from allowlist import is_allowlisted
-
-
-def check_debug_info(dso_path, readelf_content):
-    """Check whether debug info section exists in the elf file.
-
-    Args:
-      dso_path: path to the dso.
-      readelf_content: debug info dumped by command readelf.
-
-    Returns:
-      True if debug info section exists, otherwise False.
-    """
-
-    # Return True if it is allowlisted
-    if is_allowlisted("exist_debug_info", dso_path):
-        return True
-
-    for l in readelf_content:
-        if "debug_info" in l:
-            return True
-    return False
-
-
-def check_producer(dso_path, readelf_content):
-    """Check whether DW_AT_producer exists in each compile unit.
-
-    Args:
-      dso_path: path to the dso.
-      readelf_content: debug info dumped by command readelf.
-
-    Returns:
-      True if DW_AT_producer exists in each compile unit, otherwise False.
-      Notice: If no compile unit in DSO, also return True.
-    """
-
-    # Return True if it is allowlisted
-    if is_allowlisted("exist_producer", dso_path):
-        return True
-
-    # Indicate if there is a producer under each cu
-    cur_producer = False
-
-    first_cu = True
-    producer_exist = True
-
-    for l in readelf_content:
-        if "DW_TAG_compile_unit" in l:
-            if not first_cu and not cur_producer:
-                producer_exist = False
-                break
-            first_cu = False
-            cur_producer = False
-        elif "DW_AT_producer" in l:
-            cur_producer = True
-
-    # Check whether last producer of compile unit exists in the elf,
-    # also return True if no cu in the DSO.
-    if not first_cu and not cur_producer:
-        producer_exist = False
-
-    return producer_exist
-
-
-def check_exist_all(dso_path):
-    """check whether intended components exists in the given dso.
-
-    Args:
-      dso_path: path to the dso.
-
-    Returns:
-      True if everything looks fine otherwise False.
-    """
-
-    readelf = subprocess.Popen(
-        ["llvm-dwarfdump", "--recurse-depth=0", dso_path],
-        stdout=subprocess.PIPE,
-        stderr=open(os.devnull, "w"),
-        encoding="utf-8",
-    )
-    readelf_content = list(readelf.stdout)
-
-    exist_checks = [check_debug_info, check_producer]
-
-    for e in exist_checks:
-        if not e(dso_path, readelf_content):
-            check_failed = e.__module__ + ": " + e.__name__
-            print("%s failed check: %s" % (dso_path, check_failed))
-            return False
-
-    return True
diff --git a/debug_info_test/check_icf.py b/debug_info_test/check_icf.py
deleted file mode 100644
index a717d81e..00000000
--- a/debug_info_test/check_icf.py
+++ /dev/null
@@ -1,54 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2018 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""check whether chrome was built with identical code folding."""
-
-
-import os
-import re
-import subprocess
-
-
-def check_identical_code_folding(dso_path):
-    """check whether chrome was built with identical code folding.
-
-    Args:
-      dso_path: path to the dso.
-
-    Returns:
-      False if the dso is chrome and it was not built with icf,
-      True otherwise.
-    """
-
-    if not dso_path.endswith("/chrome.debug"):
-        return True
-
-    # Run 'nm' on the chrome binary and read the output.
-    nm = subprocess.Popen(
-        ["nm", dso_path],
-        stdout=subprocess.PIPE,
-        stderr=open(os.devnull, "w"),
-        encoding="utf-8",
-    )
-    nm_output, _ = nm.communicate()
-
-    # Search for addresses of text symbols.
-    text_addresses = re.findall("^[0-9a-f]+[ ]+[tT] ", nm_output, re.MULTILINE)
-
-    # Calculate number of text symbols in chrome binary.
-    num_text_addresses = len(text_addresses)
-
-    # Calculate number of unique text symbols in chrome binary.
-    num_unique_text_addresses = len(set(text_addresses))
-
-    # Check that the number of duplicate symbols is at least 10,000.
-    #   - https://crbug.com/813272#c18
-    if num_text_addresses - num_unique_text_addresses >= 10000:
-        return True
-
-    print("%s was not built with ICF" % dso_path)
-    print("    num_text_addresses = %d" % num_text_addresses)
-    print("    num_unique_text_addresses = %d" % num_unique_text_addresses)
-    return False
diff --git a/debug_info_test/check_ngcc.py b/debug_info_test/check_ngcc.py
deleted file mode 100644
index bbb58741..00000000
--- a/debug_info_test/check_ngcc.py
+++ /dev/null
@@ -1,29 +0,0 @@
-# -*- coding: utf-8 -*-
-# Copyright 2018 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Check whether the compile unit is not built by gcc."""
-
-
-from allowlist import is_allowlisted
-
-
-def not_by_gcc(dso_path, producer, comp_path):
-    """Check whether the compile unit is not built by gcc.
-
-    Args:
-      dso_path: path to the elf/dso.
-      producer: DW_AT_producer contains the compiler command line.
-      comp_path: DW_AT_comp_dir + DW_AT_name.
-
-    Returns:
-      False if compiled by gcc otherwise True.
-    """
-    if is_allowlisted("ngcc_comp_path", comp_path):
-        return True
-
-    if is_allowlisted("ngcc_dso_path", dso_path):
-        return True
-
-    return "GNU C" not in producer
diff --git a/debug_info_test/debug_info_test.py b/debug_info_test/debug_info_test.py
deleted file mode 100755
index c324bf4c..00000000
--- a/debug_info_test/debug_info_test.py
+++ /dev/null
@@ -1,70 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2018 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Test for debug info."""
-
-
-import os
-import subprocess
-import sys
-
-import check_cus
-import check_exist
-import check_icf
-
-
-elf_checks = [
-    check_exist.check_exist_all,
-    check_cus.check_compile_units,
-    check_icf.check_identical_code_folding,
-]
-
-
-def scanelf(root):
-    """Find ELFs in root.
-
-    Args:
-      root: root dir to start with the search.
-
-    Returns:
-      Filenames of ELFs in root.
-    """
-    p = subprocess.Popen(
-        ["scanelf", "-y", "-B", "-F", "%F", "-R", root],
-        stdout=subprocess.PIPE,
-        encoding="utf-8",
-    )
-    return [l.strip() for l in p.stdout]
-
-
-def Main(argv):
-    if len(argv) < 2:
-        print("usage: %s [file|dir]")
-        return 1
-
-    files = []
-    cand = argv[1]
-    if os.path.isfile(cand):
-        files = [cand]
-    elif os.path.isdir(cand):
-        files = scanelf(cand)
-    else:
-        print("usage: %s [file|dir]")
-        return 1
-
-    failed = False
-    for f in files:
-        for c in elf_checks:
-            if not c(f):
-                failed = True
-
-    if failed:
-        return 1
-    return 0
-
-
-if __name__ == "__main__":
-    sys.exit(Main(sys.argv))
diff --git a/debug_info_test/exist_debug_info.allowlist b/debug_info_test/exist_debug_info.allowlist
deleted file mode 100644
index e0076fd6..00000000
--- a/debug_info_test/exist_debug_info.allowlist
+++ /dev/null
@@ -1,13 +0,0 @@
-# To hide existing failures that some DSOs may have no debug info.
-.*/usr/bin/memdiskfind\.debug
-.*/usr/bin/isohybrid\.debug
-.*/usr/bin/gethostip\.debug
-.*/usr/lib.*/libevent-.*\.so.*\.debug
-.*/usr/lib.*/libcares\.so.*\.debug
-.*/usr/lib64/libdcerpc-samr\.so.*\.debug
-.*/usr/lib64/libGLESv2\.so.*\.debug
-.*/usr/lib64/python2.7/site-packages/selenium/webdriver/firefox/.*/x_ignore_nofocus\.so\.debug
-.*/lib.*/libiptc\.so.*\.debug
-.*/autotest/.*\.debug
-.*/opt/intel/fw_parser\.debug
-# todos:
diff --git a/debug_info_test/exist_producer.allowlist b/debug_info_test/exist_producer.allowlist
deleted file mode 100644
index ee75de72..00000000
--- a/debug_info_test/exist_producer.allowlist
+++ /dev/null
@@ -1,8 +0,0 @@
-# To hide existing failures that producer not in certain compiler units.
-.*/opt/google/chrome/libosmesa\.so\.debug
-.*/opt/google/chrome/chrome-sandbox\.debug
-.*/opt/google/chrome/chrome\.debug
-.*/opt/google/chrome/libosmesa\.so\.debug
-.*/opt/google/chrome/nacl_helper\.debug
-.*/usr/local/chromedriver/chromedriver\.debug
-# todos:
diff --git a/debug_info_test/ngcc_comp_path.allowlist b/debug_info_test/ngcc_comp_path.allowlist
deleted file mode 100644
index 45c5b4a2..00000000
--- a/debug_info_test/ngcc_comp_path.allowlist
+++ /dev/null
@@ -1,24 +0,0 @@
-# CrOS packages are compiled in /tmp/$board/portage/${CATEGORY}/${P}.
-# They can be matched by .*/portage/${CATEGORY}/${PN}-.*
-.*/portage/chromeos-base/ec-utils-.*
-.*/portage/dev-libs/elfutils-.*
-.*/portage/dev-libs/libusb-.*
-.*/portage/dev-util/perf-.*
-.*/portage/media-libs/arc-cros-gralloc-.*
-.*/portage/media-video/yavta-.*
-.*/portage/sys-apps/cavium-n3fips-driver-.*
-.*/portage/sys-apps/cavium-n3fips-tools-.*
-.*/portage/sys-apps/busybox-.*
-.*/portage/sys-apps/snaggletooth-drivers-.*
-.*/portage/sys-boot/syslinux-.*
-.*/portage/sys-kernel/chromeos-kernel-.*
-.*/portage/sys-kernel/fzm-kmod.*
-.*/portage/sys-kernel/kernel-.*
-.*/portage/sys-kernel/.*-kernel.*
-.*/portage/sys-kernel/ti-uio-module-drv-.*
-.*/portage/sys-libs/gcc-libs-.*
-# glibc and libgcc are built in different ways.
-# and libstdc++.
-.*/glibc-.*/
-.*/libgcc/.*
-.*/libstdc\+\+-.*
diff --git a/debug_info_test/ngcc_dso_path.allowlist b/debug_info_test/ngcc_dso_path.allowlist
deleted file mode 100644
index 858465e0..00000000
--- a/debug_info_test/ngcc_dso_path.allowlist
+++ /dev/null
@@ -1,23 +0,0 @@
-# DSOs specified here are not CrOS packages compiled within CrOS SDK.
-# CrOS packages should be allowlisted in *_comp_path.allowlist
-# modules we don't care:
-.*/binutils/.*
-.*/binutils-bin/.*
-.*/boot/u-boot\.debug
-.*/boot/vmlinux\.debug
-.*/uboot/u-boot\.debug
-.*/firmware/u-boot\.debug
-.*/libpepflashplayer\.so\.debug
-.*/opt/punybench/bin/.*
-.*/opt/scribe/grpc/_cython/cygrpc\.so\.debug
-.*/telemetry_dep/.*
-.*/unixbench/.*
-.*/usr/bin/core_collector32\.debug
-.*/usr/bin/kubelet\.debug
-.*/usr/lib/libatomic.so.*\.debug
-.*/usr/lib/librk_aiq\.so\.debug
-.*/usr/local/build/autotest/client/site_tests/factory_Leds/src/ec_ctl\.debug
-.*/opt/google/containers/android/.*
-.*/libmali\.so.*\.debug
-.*/pyelftools/examples/sample_exe64\.elf\.debug
-# todos:
diff --git a/go/android/adb_marlin b/go/android/adb_marlin
deleted file mode 100755
index 476e6603..00000000
--- a/go/android/adb_marlin
+++ /dev/null
@@ -1,8 +0,0 @@
-#!/bin/bash
-
-# This wrapper runs adb with the serial number of the marlin device.
-# Replace XXXXXXXX with the actual serial number of the device.
-# This is just an example. Create one such wrapper for each Android
-# device used for running Go tests.
-
-exec adb -s XXXXXXXX "$@"
diff --git a/go/android/adb_marlin32 b/go/android/adb_marlin32
deleted file mode 120000
index 9cdd321b..00000000
--- a/go/android/adb_marlin32
+++ /dev/null
@@ -1 +0,0 @@
-adb_marlin
\ No newline at end of file
diff --git a/go/android/build_go b/go/android/build_go
deleted file mode 100755
index ecb3bee0..00000000
--- a/go/android/build_go
+++ /dev/null
@@ -1,26 +0,0 @@
-#!/bin/bash
-set -e -o pipefail
-
-# This script builds the go cross compilers for Android targets.
-#
-# Usage: build_go
-#
-# It assumes that the "arm-linux-androideabi" toolchain is already installed.
-# It assumes that the "aarch64-linux-android" toolchain is already installed.
-
-if [[ ! -e "make.bash" && -e "src/make.bash" ]]
-then
-	cd src
-fi
-
-# Build the Go toolchain for arm devices.
-GOOS="android" GOARCH="arm" CGO_ENABLED="1" \
-	CC_FOR_TARGET="arm-linux-androideabi-clang" \
-	CXX_FOR_TARGET="arm-linux-androideabi-clang++" \
-	./make.bash --no-clean
-
-# Build the Go toolchain for arm64 devices.
-GOOS="android" GOARCH="arm64" CGO_ENABLED="1" \
-	CC_FOR_TARGET="aarch64-linux-android-clang" \
-	CXX_FOR_TARGET="aarch64-linux-android-clang++" \
-	./make.bash --no-clean
diff --git a/go/android/go_marlin b/go/android/go_marlin
deleted file mode 100755
index bfb564f9..00000000
--- a/go/android/go_marlin
+++ /dev/null
@@ -1,11 +0,0 @@
-#!/bin/bash
-
-# Invoke the Go cross compiler for marlin.
-# Uses ../go_target to add PIE flags.
-#
-# This is just an example for an arm64 device.
-
-GOOS="android" GOARCH="arm64" CGO_ENABLED="1" \
-	CC="aarch64-linux-android-clang" \
-	CXX="aarch64-linux-android-clang++" \
-	exec go_target "$@"
diff --git a/go/android/go_marlin32 b/go/android/go_marlin32
deleted file mode 100755
index d02dadc9..00000000
--- a/go/android/go_marlin32
+++ /dev/null
@@ -1,11 +0,0 @@
-#!/bin/bash
-
-# Invoke the Go cross compiler for marlin32.
-# Uses ../go_target to add PIE flags.
-#
-# This is just an example for an arm device.
-
-GOOS="android" GOARCH="arm" CGO_ENABLED="1" \
-	CC="arm-linux-androideabi-clang" \
-	CXX="arm-linux-androideabi-clang++" \
-	exec go_target "$@"
diff --git a/go/android/go_marlin32_exec b/go/android/go_marlin32_exec
deleted file mode 100755
index ed3fdf42..00000000
--- a/go/android/go_marlin32_exec
+++ /dev/null
@@ -1,8 +0,0 @@
-#!/bin/bash
-
-# Copy and remotely execute a binary on the marlin32 device.
-#
-# For this to work, the corresponding adb_marlin32 wrapper
-# must exist to tell adb the serial number of the device.
-
-GOOS="android" GOARCH="arm" exec go_target_exec marlin32 "$@"
diff --git a/go/android/go_marlin_exec b/go/android/go_marlin_exec
deleted file mode 100755
index 9f4c06d4..00000000
--- a/go/android/go_marlin_exec
+++ /dev/null
@@ -1,8 +0,0 @@
-#!/bin/bash
-
-# Copy and remotely execute a binary on the marlin device.
-#
-# For this to work, the corresponding adb_marlin wrapper
-# must exist to tell adb the serial number of the device.
-
-GOOS="android" GOARCH="arm64" exec go_target_exec marlin "$@"
diff --git a/go/android/target_cp b/go/android/target_cp
deleted file mode 100755
index f6cd5cbe..00000000
--- a/go/android/target_cp
+++ /dev/null
@@ -1,28 +0,0 @@
-#!/bin/bash
-set -e -o pipefail
-
-# Copy a file or directory to the target Android device.
-#
-# Usage: target_cp <src> <target>:<dest>
-
-src="$1"
-shift
-
-targetdest="$1"
-shift
-
-target="${targetdest%:*}"
-dest="${targetdest#*:}"
-
-if [[ -z "${src}" || -z "${target}" || -z "${dest}" || "${targetdest}" != "${target}:${dest}" || -n "$*" ]]
-then
-	echo "Usage: target_cp <src> <target>:<dest>"
-	exit 1
-fi
-
-if [[ -d ${src} ]]
-then
-	adb_${target} push ${src} ${dest}/${src##*/} >/dev/null
-else
-	adb_${target} push ${src} ${dest} >/dev/null
-fi
diff --git a/go/android/target_sh b/go/android/target_sh
deleted file mode 100755
index 241843e8..00000000
--- a/go/android/target_sh
+++ /dev/null
@@ -1,13 +0,0 @@
-#!/bin/bash
-set -e -o pipefail
-
-# Run a command on the target Android device.
-#
-# Usage: target_sh <cmd> <args>...
-
-target="$1"
-shift
-
-exitcode="$(target_tmpdir)/exitcode"
-adb_${target} shell "$*; echo -n \$? > ${exitcode}" | sed -e 's:\r$::' -u
-exit $(adb_${target} shell "cat ${exitcode}")
diff --git a/go/android/target_tmpdir b/go/android/target_tmpdir
deleted file mode 100755
index b5953696..00000000
--- a/go/android/target_tmpdir
+++ /dev/null
@@ -1,5 +0,0 @@
-#!/bin/bash
-
-# Temporary directory to be used on Android devices.
-
-echo "/data/local/tmp"
diff --git a/go/chromeos/build_go b/go/chromeos/build_go
deleted file mode 100755
index 0dec7dfd..00000000
--- a/go/chromeos/build_go
+++ /dev/null
@@ -1,46 +0,0 @@
-#!/bin/bash
-set -e -o pipefail
-
-# This script builds the go cross compilers for ChromeOS targets.
-#
-# Usage: build_go
-#
-# It assumes that the "x86_64-cros-linux-gnu" toolchain is already installed.
-# It assumes that the "armv7a-cros-linux-gnueabihf" toolchain is
-# already installed.
-# It assumes that the "aarch64-cros-linux-gnu" toolchain is already installed.
-
-if [[ ! -e "make.bash" && -e "src/make.bash" ]]
-then
-	cd src
-fi
-
-# Build the Go toolchain for amd64 targets.
-GOOS="linux" GOARCH="amd64" CGO_ENABLED="1" \
-	CC_FOR_TARGET="x86_64-cros-linux-gnu-clang" \
-	CXX_FOR_TARGET="x86_64-cros-linux-gnu-clang++" \
-	./make.bash --no-clean
-GOOS="linux" GOARCH="amd64" CGO_ENABLED="1" \
-	CC="x86_64-cros-linux-gnu-clang" \
-	CXX="x86_64-cros-linux-gnu-clang++" \
-	../bin/go install -v -buildmode=pie std
-
-# Build the Go toolchain for arm targets.
-GOOS="linux" GOARCH="arm" CGO_ENABLED="1" \
-	CC_FOR_TARGET="armv7a-cros-linux-gnueabihf-clang" \
-	CXX_FOR_TARGET="armv7a-cros-linux-gnueabihf-clang++" \
-	./make.bash --no-clean
-GOOS="linux" GOARCH="arm" CGO_ENABLED="1" \
-	CC="armv7a-cros-linux-gnueabihf-clang" \
-	CXX="armv7a-cros-linux-gnueabihf-clang++" \
-	../bin/go install -v -buildmode=pie std
-
-# Build the Go toolchain for arm64 targets.
-GOOS="linux" GOARCH="arm64" CGO_ENABLED="1" \
-	CC_FOR_TARGET="aarch64-cros-linux-gnu-clang" \
-	CXX_FOR_TARGET="aarch64-cros-linux-gnu-clang++" \
-	./make.bash --no-clean
-GOOS="linux" GOARCH="arm64" CGO_ENABLED="1" \
-	CC="aarch64-cros-linux-gnu-clang" \
-	CXX="aarch64-cros-linux-gnu-clang++" \
-	../bin/go install -v -buildmode=pie std
diff --git a/go/chromeos/push_glibc b/go/chromeos/push_glibc
deleted file mode 100755
index 8211d9d5..00000000
--- a/go/chromeos/push_glibc
+++ /dev/null
@@ -1,56 +0,0 @@
-#!/bin/bash
-set -e -o pipefail
-
-# This script copies a locally built GLIBC to a remote device.
-#
-# Usage: push_glibc <target>...
-#
-# This script works with 64-bit (amd64 or arm64) ChromeOS targets.
-# It copies both 32-bit and 64-bit glibc loaders onto the device.
-# This allows loading and running both 32-bit and 64-bit binaries
-# on the same device.
-
-for target in "$@"
-do
-	echo -n "pushing glibc to ${target} ... "
-	case "$(ssh -i ${HOME}/.ssh/testing_rsa ${target} uname -m)" in
-		x86_64)
-			glibc="/usr/x86_64-cros-linux-gnu/lib64"
-			loader="ld-linux-x86-64.so.2"
-			glibc32="/usr/i686-pc-linux-gnu/lib"
-			loader32="ld-linux.so.2"
-			;;
-		aarch64)
-			glibc="/usr/aarch64-cros-linux-gnu/lib64"
-			loader="ld-linux-aarch64.so.1"
-			glibc32="/usr/armv7a-cros-linux-gnueabihf/lib"
-			loader32="ld-linux-armhf.so.3"
-			;;
-		*)
-			echo "unknown arch"
-			continue
-			;;
-	esac
-
-	target_sh ${target} "rm -rf /tmp/glibc"
-	target_sh ${target} "mkdir -p /tmp/glibc"
-	target_cp "${glibc}" ${target}:/tmp/glibc
-
-	target_sh ${target} "rm -rf /tmp/glibc32"
-	target_sh ${target} "mkdir -p /tmp/glibc32"
-	target_cp "${glibc32}" ${target}:/tmp/glibc32
-
-	echo "#!/bin/bash" > /tmp/ld.so
-	echo "LD_LIBRARY_PATH=/tmp/glibc/${glibc##*/} exec /tmp/glibc/${glibc##*/}/${loader} \"\$@\"" >> /tmp/ld.so
-	chmod +x /tmp/ld.so
-	target_cp /tmp/ld.so ${target}:/tmp/glibc
-	rm /tmp/ld.so
-
-	echo "#!/bin/bash" > /tmp/ld.so
-	echo "LD_LIBRARY_PATH=/tmp/glibc32/${glibc32##*/} exec /tmp/glibc32/${glibc32##*/}/${loader32} \"\$@\"" >> /tmp/ld.so
-	chmod +x /tmp/ld.so
-	target_cp /tmp/ld.so ${target}:/tmp/glibc32
-	rm /tmp/ld.so
-
-	echo "done"
-done
diff --git a/go/chromeos/setup_chromeos_testing.py b/go/chromeos/setup_chromeos_testing.py
deleted file mode 100755
index 39530bde..00000000
--- a/go/chromeos/setup_chromeos_testing.py
+++ /dev/null
@@ -1,282 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Generate board-specific scripts for Go compiler testing."""
-
-
-import argparse
-import getpass
-import os
-import sys
-
-from cros_utils import command_executer
-
-SUCCESS = 0
-DEBUG = False
-
-ARCH_DATA = {"x86_64": "amd64", "arm32": "arm", "arm64": "arm64"}
-
-CROS_TOOLCHAIN_DATA = {
-    "x86_64": "x86_64-cros-linux-gnu",
-    "arm32": "armv7a-cros-linux-gnueabihf",
-    "arm64": "aarch64-cros-linux-gnu",
-}
-
-GLIBC_DATA = {"x86_64": "glibc", "arm32": "glibc32", "arm64": "glibc"}
-
-CONFIG_TEMPLATE = """
-Host %s
-    HostName %s
-    User root
-    UserKnownHostsFile /dev/null
-    BatchMode yes
-    CheckHostIP no
-    StrictHostKeyChecking no
-    IdentityFile %%d/.ssh/testing_rsa
-"""
-
-BASE_TEMPLATE = """#!/bin/bash
-
-# Invoke the Go cross compiler for %s.
-# Uses ../go_target to add PIE flags.
-#
-# This is just an example for an %s board.
-
-GOOS="linux" GOARCH="%s" CGO_ENABLED="1" \\
-        GOROOT="/usr/lib/go/%s" \\
-        CC="%s-clang" \\
-        CXX="%s-clang++" \\
-        exec go_target "$@"
-"""
-
-EXEC_TEMPLATE = """#!/bin/bash
-
-# Copy and remotely execute a binary on the %s device.
-#
-# For this to work, the corresponding entry must exist in
-# ~/.ssh/config and the device must already be setup for
-# password-less authentication. See setup instructions at
-# http://go/chromeos-toolchain-team/go-toolchain
-
-GOOS="linux" GOARCH="%s" \\
-      GOLOADER="/tmp/%s/ld.so" \\
-      exec go_target_exec %s "$@"
-"""
-
-
-def log(msg):
-
-    if DEBUG:
-        print(msg)
-
-
-def WriteFile(file_content, file_name):
-    with open(file_name, "w", encoding="utf-8") as out_file:
-        out_file.write(file_content)
-
-
-def GenerateGoHelperScripts(ce, arm_board, x86_board, chromeos_root):
-    keys = ["x86_64", "arm32", "arm64"]
-    names = {
-        "x86_64": x86_board,
-        "arm64": arm_board,
-        "arm32": ("%s32" % arm_board),
-    }
-
-    toolchain_dir = os.path.join(
-        chromeos_root, "src", "third_party", "toolchain-utils", "go", "chromeos"
-    )
-    for k in keys:
-        name = names[k]
-        arch = ARCH_DATA[k]
-        toolchain = CROS_TOOLCHAIN_DATA[k]
-        glibc = GLIBC_DATA[k]
-
-        base_file = os.path.join(toolchain_dir, ("go_%s" % name))
-        base_file_content = BASE_TEMPLATE % (
-            name,
-            arch,
-            arch,
-            toolchain,
-            toolchain,
-            toolchain,
-        )
-        WriteFile(base_file_content, base_file)
-        cmd = "chmod 755 %s" % base_file
-        ce.RunCommand(cmd)
-
-        exec_file = os.path.join(toolchain_dir, ("go_%s_exec" % name))
-        exec_file_content = EXEC_TEMPLATE % (name, arch, glibc, name)
-        WriteFile(exec_file_content, exec_file)
-        cmd = "chmod 755 %s" % exec_file
-        ce.RunCommand(cmd)
-
-    return 0
-
-
-def UpdateChrootSshConfig(
-    ce, arm_board, arm_dut, x86_board, x86_dut, chromeos_root
-):
-    log("Entering UpdateChrootSshConfig")
-    # Copy testing_rsa to .ssh and set file protections properly.
-    user = getpass.getuser()
-    ssh_dir = os.path.join(chromeos_root, "chroot", "home", user, ".ssh")
-    dest_file = os.path.join(ssh_dir, "testing_rsa")
-    src_file = os.path.join(
-        chromeos_root, "src", "scripts", "mod_for_test_scripts", "testing_rsa"
-    )
-    if not os.path.exists(dest_file):
-        if os.path.exists(src_file):
-            cmd = "cp %s %s" % (src_file, dest_file)
-            ret = ce.RunCommand(cmd)
-            if ret != SUCCESS:
-                print('Error executing "%s". Exiting now...' % cmd)
-                sys.exit(1)
-            cmd = "chmod 600 %s" % dest_file
-            ret = ce.RunCommand(cmd)
-            if ret != SUCCESS:
-                print(
-                    "Error executing %s; may need to re-run this manually."
-                    % cmd
-                )
-        else:
-            print(
-                "Cannot find %s; you will need to update testing_rsa by hand."
-                % src_file
-            )
-    else:
-        log("testing_rsa exists already.")
-
-    # Save ~/.ssh/config file, if not already done.
-    config_file = os.path.expanduser("~/.ssh/config")
-    saved_config_file = os.path.join(
-        os.path.expanduser("~/.ssh"), "config.save.go-scripts"
-    )
-    if not os.path.exists(saved_config_file):
-        cmd = "cp %s %s" % (config_file, saved_config_file)
-        ret = ce.RunCommand(cmd)
-        if ret != SUCCESS:
-            print("Error making save copy of ~/.ssh/config. Exiting...")
-            sys.exit(1)
-
-    # Update ~/.ssh/config file
-    log("Reading ssh config file")
-    with open(config_file, "r") as input_file:
-        config_lines = input_file.read()
-
-    x86_host_config = CONFIG_TEMPLATE % (x86_board, x86_dut)
-    arm_names = "%s %s32" % (arm_board, arm_board)
-    arm_host_config = CONFIG_TEMPLATE % (arm_names, arm_dut)
-
-    config_lines += x86_host_config
-    config_lines += arm_host_config
-
-    log("Writing ~/.ssh/config")
-    WriteFile(config_lines, config_file)
-
-    return 0
-
-
-def CleanUp(ce, x86_board, arm_board, chromeos_root):
-    # Find and remove go helper scripts
-    keys = ["x86_64", "arm32", "arm64"]
-    names = {
-        "x86_64": x86_board,
-        "arm64": arm_board,
-        "arm32": ("%s32" % arm_board),
-    }
-
-    toolchain_dir = os.path.join(
-        chromeos_root, "src", "third_party", "toolchain-utils", "go", "chromeos"
-    )
-    for k in keys:
-        name = names[k]
-        base_file = os.path.join(toolchain_dir, ("go_%s" % name))
-        exec_file = os.path.join(toolchain_dir, ("go_%s_exec" % name))
-        cmd = "rm -f %s; rm -f %s" % (base_file, exec_file)
-        ce.RunCommand(cmd)
-
-    # Restore saved config_file
-    config_file = os.path.expanduser("~/.ssh/config")
-    saved_config_file = os.path.join(
-        os.path.expanduser("~/.ssh"), "config.save.go-scripts"
-    )
-    if not os.path.exists(saved_config_file):
-        print(
-            "Could not find file: %s; unable to restore ~/.ssh/config ."
-            % saved_config_file
-        )
-    else:
-        cmd = "mv %s %s" % (saved_config_file, config_file)
-        ce.RunCommand(cmd)
-
-    return 0
-
-
-def Main(argv):
-    # pylint: disable=global-statement
-    global DEBUG
-
-    parser = argparse.ArgumentParser()
-    parser.add_argument("-a", "--arm64_board", dest="arm_board", required=True)
-    parser.add_argument(
-        "-b", "--x86_64_board", dest="x86_64_board", required=True
-    )
-    parser.add_argument(
-        "-c", "--chromeos_root", dest="chromeos_root", required=True
-    )
-    parser.add_argument("-x", "--x86_64_dut", dest="x86_64_dut", required=True)
-    parser.add_argument("-y", "--arm64_dut", dest="arm_dut", required=True)
-    parser.add_argument(
-        "-z", "--cleanup", dest="cleanup", default=False, action="store_true"
-    )
-    parser.add_argument(
-        "-v", "--verbose", dest="verbose", default=False, action="store_true"
-    )
-
-    options = parser.parse_args(argv[1:])
-
-    if options.verbose:
-        DEBUG = True
-
-    if not os.path.exists(options.chromeos_root):
-        print("Invalid ChromeOS Root: %s" % options.chromeos_root)
-
-    ce = command_executer.GetCommandExecuter()
-    all_good = True
-    for m in (options.x86_64_dut, options.arm_dut):
-        cmd = "ping -c 3 %s > /dev/null" % m
-        ret = ce.RunCommand(cmd)
-        if ret != SUCCESS:
-            print("Machine %s is currently not responding to ping." % m)
-            all_good = False
-
-    if not all_good:
-        return 1
-
-    if not options.cleanup:
-        UpdateChrootSshConfig(
-            ce,
-            options.arm_board,
-            options.arm_dut,
-            options.x86_64_board,
-            options.x86_64_dut,
-            options.chromeos_root,
-        )
-        GenerateGoHelperScripts(
-            ce, options.arm_board, options.x86_64_board, options.chromeos_root
-        )
-    else:
-        CleanUp(
-            ce, options.x86_64_board, options.arm_board, options.chromeos_root
-        )
-
-    return 0
-
-
-if __name__ == "__main__":
-    val = Main(sys.argv)
-    sys.exit(val)
diff --git a/go/chromeos/target_cp b/go/chromeos/target_cp
deleted file mode 100755
index 10f4bf72..00000000
--- a/go/chromeos/target_cp
+++ /dev/null
@@ -1,28 +0,0 @@
-#!/bin/bash
-set -e -o pipefail
-
-# Copy a file or directory to the target ChromeOS device.
-#
-# Usage: target_cp <src> <target>:<dest>
-
-src="$1"
-shift
-
-targetdest="$1"
-shift
-
-target="${targetdest%:*}"
-dest="${targetdest#*:}"
-
-if [[ -z "${src}" || -z "${target}" || -z "${dest}" || "${targetdest}" != "${target}:${dest}" || -n "$*" ]]
-then
-	echo "Usage: target_cp <src> <target>:<dest>"
-	exit 1
-fi
-
-if [[ -d ${src} ]]
-then
-	tar -C $(dirname ${src}) -zcf - $(basename ${src}) | ssh -i ${HOME}/.ssh/testing_rsa ${target} "tar -C ${dest} -zxf -"
-else
-	scp -i ${HOME}/.ssh/testing_rsa -q ${src} ${target}:${dest}
-fi
diff --git a/go/chromeos/target_sh b/go/chromeos/target_sh
deleted file mode 100755
index 4c56252e..00000000
--- a/go/chromeos/target_sh
+++ /dev/null
@@ -1,11 +0,0 @@
-#!/bin/bash
-set -e -o pipefail
-
-# Run a command on the target ChromeOS device.
-#
-# Usage: target_sh <cmd> <args>...
-
-target="$1"
-shift
-
-ssh -i ${HOME}/.ssh/testing_rsa ${target} "$*"
diff --git a/go/chromeos/target_tmpdir b/go/chromeos/target_tmpdir
deleted file mode 100755
index 382a0334..00000000
--- a/go/chromeos/target_tmpdir
+++ /dev/null
@@ -1,5 +0,0 @@
-#!/bin/bash
-
-# Temporary directory to be used on ChromeOS devices.
-
-echo "/tmp"
diff --git a/go/go_local b/go/go_local
deleted file mode 100755
index cb2a4dc1..00000000
--- a/go/go_local
+++ /dev/null
@@ -1,8 +0,0 @@
-#!/bin/bash
-
-# Invoke the Go compiler for localhost.
-
-GOOS="linux" GOARCH="amd64" CGO_ENABLED="1" \
-	CC="clang" \
-	CXX="clang++" \
-	exec go "$@"
diff --git a/go/go_target b/go/go_target
deleted file mode 100755
index 8943d813..00000000
--- a/go/go_target
+++ /dev/null
@@ -1,67 +0,0 @@
-#!/bin/bash
-set -e -o pipefail
-
-# This script wraps the go cross compilers.
-#
-# It ensures that Go binaries are linked with an external linker
-# by default (cross clang). Appropriate flags are added to build a
-# position independent executable (PIE) for ASLR.
-# "export GOPIE=0" to temporarily disable this behavior.
-
-function pie_enabled()
-	{
-	[[ "${GOPIE}" != "0" ]]
-	}
-
-function has_ldflags()
-	{
-	# Check if any linker flags are present in argv.
-	for arg in "$@"
-	do
-		case "${arg}" in
-			-ldflags | -ldflags=*) return 0 ;;
-			-linkmode | -linkmode=*) return 0 ;;
-			-buildmode | -buildmode=*) return 0 ;;
-			-installsuffix | -installsuffix=*) return 0 ;;
-			-extld | -extld=*) return 0 ;;
-			-extldflags | -extldflags=*) return 0 ;;
-		esac
-	done
-	return 1
-	}
-
-pie_flags=()
-if pie_enabled && ! has_ldflags "$@"
-then
-	case "$1" in
-		build | install | run | test)
-			# Add "-buildmode=pie" to "go build|install|run|test" commands.
-			pie_flags=( "$1" )
-			shift
-			[[ "${GOOS}" == "android" ]] || pie_flags+=( "-buildmode=pie" )
-			;;
-		tool)
-			case "$2" in
-				asm)
-					# Handle direct assembler invocations ("go tool asm <args>").
-					pie_flags=( "$1" "$2" "-shared" )
-					shift 2
-					;;
-				compile)
-					# Handle direct compiler invocations ("go tool compile <args>").
-					pie_flags=( "$1" "$2" "-shared" )
-					shift 2
-					[[ "${GOOS}" == "android" ]] || pie_flags+=( "-installsuffix=shared" )
-					;;
-				link)
-					# Handle direct linker invocations ("go tool link <args>").
-					pie_flags=( "$1" "$2" "-extld" "${CC}" "-buildmode=pie" )
-					shift 2
-					[[ "${GOOS}" == "android" ]] || pie_flags+=( "-installsuffix=shared" )
-					;;
-			esac
-			;;
-	esac
-fi
-
-exec go "${pie_flags[@]}" "$@"
diff --git a/go/go_target_exec b/go/go_target_exec
deleted file mode 100755
index 0a44b4cf..00000000
--- a/go/go_target_exec
+++ /dev/null
@@ -1,41 +0,0 @@
-#!/bin/bash
-set -e -o pipefail
-
-# This wrapper copies an executable to a target device and executes it there.
-#
-# Usage: go_target_exec <target> <binary> <args>...
-#
-# This script can work with both ChromeOS/Android devices.
-#
-# It uses "target_tmpdir" to get the path to the temporary directory on the device.
-# It uses "target_cp" to copy the binary to the temporary directory on the device.
-# It uses "target_sh" to execute the binary remotely and get the output/exitcode.
-
-target="$1"
-shift
-
-binary="$1"
-shift
-
-# Get path to temporary directory on device and copy the binary over.
-tmpdir="$(target_tmpdir)"
-target_cp ${binary} ${target}:${tmpdir}/a.out
-
-# If current directory is inside GOROOT, then execute the binary in the
-# corresponding directory inside GOROOT on the device.
-targetdir="${tmpdir}"
-goroot="$(go_${target} env GOROOT)"
-if [[ "${PWD}" == ${goroot}/src/* ]]
-then
-	targetdir="${tmpdir}/goroot/src/${PWD#${goroot}/src/}"
-fi
-
-# Set GOROOT, and forward some environment variables to the remote shell.
-vars="GOROOT=${tmpdir}/goroot"
-vars+="${GOOS:+ GOOS=${GOOS}}"
-vars+="${GOARCH:+ GOARCH=${GOARCH}}"
-vars+="${GOMAXPROCS:+ GOMAXPROCS=${GOMAXPROCS}}"
-vars+="${GOTRACEBACK:+ GOTRACEBACK=${GOTRACEBACK}}"
-
-# Remotely execute the binary using ssh (for ChromeOS) or adb (for Android).
-target_sh ${target} "cd ${targetdir} && ${vars} ${GOLOADER} ${tmpdir}/a.out $*"
diff --git a/go/patch/go-1.10.2/go0.patch b/go/patch/go-1.10.2/go0.patch
deleted file mode 100644
index c539865e..00000000
--- a/go/patch/go-1.10.2/go0.patch
+++ /dev/null
@@ -1,45 +0,0 @@
-testenv: look for "go" executable in path.
-
---- src/go/build/deps_test.go
-+++ src/go/build/deps_test.go
-@@ -182,17 +182,17 @@ var pkgDeps = map[string][]string{
- 	"runtime/debug":  {"L2", "fmt", "io/ioutil", "os", "time"},
- 	"runtime/pprof":  {"L2", "compress/gzip", "context", "encoding/binary", "fmt", "io/ioutil", "os", "text/tabwriter", "time"},
- 	"runtime/trace":  {"L0"},
- 	"text/tabwriter": {"L2"},
- 
- 	"testing":          {"L2", "flag", "fmt", "internal/race", "os", "runtime/debug", "runtime/pprof", "runtime/trace", "time"},
- 	"testing/iotest":   {"L2", "log"},
- 	"testing/quick":    {"L2", "flag", "fmt", "reflect", "time"},
--	"internal/testenv": {"L2", "OS", "flag", "testing", "syscall"},
-+	"internal/testenv": {"L2", "OS", "os/exec", "flag", "testing", "syscall"},
- 
- 	// L4 is defined as L3+fmt+log+time, because in general once
- 	// you're using L3 packages, use of fmt, log, or time is not a big deal.
- 	"L4": {
- 		"L3",
- 		"fmt",
- 		"log",
- 		"time",
---- src/internal/testenv/testenv.go
-+++ src/internal/testenv/testenv.go
-@@ -43,16 +43,19 @@ func HasGoBuild() bool {
- 	switch runtime.GOOS {
- 	case "android", "nacl":
- 		return false
- 	case "darwin":
- 		if strings.HasPrefix(runtime.GOARCH, "arm") {
- 			return false
- 		}
- 	}
-+	if _, err := exec.LookPath("go"); err != nil {
-+		return false
-+	}
- 	return true
- }
- 
- // MustHaveGoBuild checks that the current system can build programs with ``go build''
- // and then run them with os.StartProcess or exec.Command.
- // If not, MustHaveGoBuild calls t.Skip with an explanation.
- func MustHaveGoBuild(t testing.TB) {
- 	if os.Getenv("GO_GCFLAGS") != "" {
diff --git a/go/patch/go-1.10.2/go1.patch b/go/patch/go-1.10.2/go1.patch
deleted file mode 100644
index e32268ac..00000000
--- a/go/patch/go-1.10.2/go1.patch
+++ /dev/null
@@ -1,67 +0,0 @@
-test: enable some tests on android/arm64.
-
---- test/chanlinear.go
-+++ test/chanlinear.go
-@@ -1,9 +1,9 @@
--// +build darwin linux
-+// +build darwin linux android
- // run
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Test that dequeueing from a pending channel doesn't
- // take linear time.
---- test/fixedbugs/bug385_64.go
-+++ test/fixedbugs/bug385_64.go
-@@ -1,9 +1,9 @@
--// +build amd64
-+// +build amd64 arm64
- // errorcheck
- 
- // Copyright 2011 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Issue 2444
- // Issue 4666: issue with arrays of exactly 4GB.
---- test/fixedbugs/issue10607.go
-+++ test/fixedbugs/issue10607.go
-@@ -1,9 +1,9 @@
--// +build linux,!ppc64
-+// +build linux,!ppc64 android
- // run
- 
- // Copyright 2015 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Test that a -B option is passed through when using both internal
- // and external linking mode.
---- test/maplinear.go
-+++ test/maplinear.go
-@@ -1,9 +1,9 @@
--// +build darwin linux
-+// +build darwin linux android
- // run
- 
- // Copyright 2013 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Test that maps don't go quadratic for NaNs and other values.
- 
---- test/recover4.go
-+++ test/recover4.go
-@@ -1,9 +1,9 @@
--// +build linux darwin
-+// +build linux android darwin
- // run
- 
- // Copyright 2015 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Test that if a slice access causes a fault, a deferred func
- // sees the most recent value of the variables it accesses.
diff --git a/go/patch/go-1.10.2/go2.patch b/go/patch/go-1.10.2/go2.patch
deleted file mode 100644
index 20f04791..00000000
--- a/go/patch/go-1.10.2/go2.patch
+++ /dev/null
@@ -1,359 +0,0 @@
-test: add -target flag.
-
---- test/run.go
-+++ test/run.go
-@@ -34,19 +34,19 @@ import (
- 
- var (
- 	verbose        = flag.Bool("v", false, "verbose. if set, parallelism is set to 1.")
- 	keep           = flag.Bool("k", false, "keep. keep temporary directory.")
- 	numParallel    = flag.Int("n", runtime.NumCPU(), "number of parallel tests to run")
- 	summary        = flag.Bool("summary", false, "show summary of results")
- 	showSkips      = flag.Bool("show_skips", false, "show skipped tests")
- 	runSkips       = flag.Bool("run_skips", false, "run skipped tests (ignore skip and build tags)")
--	linkshared     = flag.Bool("linkshared", false, "")
- 	updateErrors   = flag.Bool("update_errors", false, "update error messages in test file based on compiler output")
- 	runoutputLimit = flag.Int("l", defaultRunOutputLimit(), "number of parallel runoutput tests to run")
-+	target         = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
- 
- 	shard  = flag.Int("shard", 0, "shard index to run. Only applicable if -shards is non-zero.")
- 	shards = flag.Int("shards", 0, "number of shards. If 0, all tests are run. This is used by the continuous build.")
- )
- 
- var (
- 	goos, goarch string
- 
-@@ -189,48 +189,49 @@ func goFiles(dir string) []string {
- 	}
- 	sort.Strings(names)
- 	return names
- }
- 
- type runCmd func(...string) ([]byte, error)
- 
- func compileFile(runcmd runCmd, longname string, flags []string) (out []byte, err error) {
--	cmd := []string{"go", "tool", "compile", "-e"}
-+	cmd := []string{findGoCmd(), "tool", "compile", "-e"}
- 	cmd = append(cmd, flags...)
--	if *linkshared {
--		cmd = append(cmd, "-dynlink", "-installsuffix=dynlink")
--	}
- 	cmd = append(cmd, longname)
- 	return runcmd(cmd...)
- }
- 
- func compileInDir(runcmd runCmd, dir string, flags []string, names ...string) (out []byte, err error) {
--	cmd := []string{"go", "tool", "compile", "-e", "-D", ".", "-I", "."}
-+	cmd := []string{findGoCmd(), "tool", "compile", "-e", "-D", ".", "-I", "."}
- 	cmd = append(cmd, flags...)
--	if *linkshared {
--		cmd = append(cmd, "-dynlink", "-installsuffix=dynlink")
--	}
- 	for _, name := range names {
- 		cmd = append(cmd, filepath.Join(dir, name))
- 	}
- 	return runcmd(cmd...)
- }
- 
- func linkFile(runcmd runCmd, goname string) (err error) {
- 	pfile := strings.Replace(goname, ".go", ".o", -1)
--	cmd := []string{"go", "tool", "link", "-w", "-o", "a.exe", "-L", "."}
--	if *linkshared {
--		cmd = append(cmd, "-linkshared", "-installsuffix=dynlink")
--	}
--	cmd = append(cmd, pfile)
--	_, err = runcmd(cmd...)
-+	_, err = runcmd(findGoCmd(), "tool", "link", "-w", "-o", "a.exe", "-L", ".", pfile)
- 	return
- }
- 
-+func goRun(runcmd runCmd, flags []string, goname string, args ...string) (out []byte, err error) {
-+	cmd := []string{findGoCmd(), "run", goGcflags()}
-+	if len(findExecCmd()) > 0 {
-+		cmd = append(cmd, "-exec")
-+		cmd = append(cmd, findExecCmd()...)
-+	}
-+	cmd = append(cmd, flags...)
-+	cmd = append(cmd, goname)
-+	cmd = append(cmd, args...)
-+	return runcmd(cmd...)
-+}
-+
- // skipError describes why a test was skipped.
- type skipError string
- 
- func (s skipError) Error() string { return string(s) }
- 
- func check(err error) {
- 	if err != nil {
- 		log.Fatal(err)
-@@ -590,18 +591,17 @@ func (t *test) run() {
- 
- 	long := filepath.Join(cwd, t.goFileName())
- 	switch action {
- 	default:
- 		t.err = fmt.Errorf("unimplemented action %q", action)
- 
- 	case "errorcheck":
- 		// TODO(gri) remove need for -C (disable printing of columns in error messages)
--		cmdline := []string{"go", "tool", "compile", "-C", "-e", "-o", "a.o"}
--		// No need to add -dynlink even if linkshared if we're just checking for errors...
-+		cmdline := []string{findGoCmd(), "tool", "compile", "-C", "-e", "-o", "a.o"}
- 		cmdline = append(cmdline, flags...)
- 		cmdline = append(cmdline, long)
- 		out, err := runcmd(cmdline...)
- 		if wantError {
- 			if err == nil {
- 				t.err = fmt.Errorf("compilation succeeded unexpectedly\n%s", out)
- 				return
- 			}
-@@ -704,17 +704,17 @@ func (t *test) run() {
- 				}
- 				if strings.Replace(string(out), "\r\n", "\n", -1) != t.expectedOutput() {
- 					t.err = fmt.Errorf("incorrect output\n%s", out)
- 				}
- 			}
- 		}
- 
- 	case "build":
--		_, err := runcmd("go", "build", goGcflags(), "-o", "a.exe", long)
-+		_, err := runcmd(findGoCmd(), "build", goGcflags(), "-o", "a.exe", long)
- 		if err != nil {
- 			t.err = err
- 		}
- 
- 	case "builddir":
- 		// Build an executable from all the .go and .s files in a subdirectory.
- 		useTmp = true
- 		longdir := filepath.Join(cwd, t.goDirName())
-@@ -730,177 +730,132 @@ func (t *test) run() {
- 			case ".go":
- 				gos = append(gos, file)
- 			case ".s":
- 				asms = append(asms, file)
- 			}
- 
- 		}
- 		var objs []string
--		cmd := []string{"go", "tool", "compile", "-e", "-D", ".", "-I", ".", "-o", "go.o"}
-+		cmd := []string{findGoCmd(), "tool", "compile", "-e", "-D", ".", "-I", ".", "-o", "go.o"}
- 		if len(asms) > 0 {
- 			cmd = append(cmd, "-asmhdr", "go_asm.h")
- 		}
- 		for _, file := range gos {
- 			cmd = append(cmd, filepath.Join(longdir, file.Name()))
- 		}
- 		_, err := runcmd(cmd...)
- 		if err != nil {
- 			t.err = err
- 			break
- 		}
- 		objs = append(objs, "go.o")
- 		if len(asms) > 0 {
--			cmd = []string{"go", "tool", "asm", "-e", "-I", ".", "-o", "asm.o"}
-+			cmd = []string{findGoCmd(), "tool", "asm", "-e", "-I", ".", "-o", "asm.o"}
- 			for _, file := range asms {
- 				cmd = append(cmd, filepath.Join(longdir, file.Name()))
- 			}
- 			_, err = runcmd(cmd...)
- 			if err != nil {
- 				t.err = err
- 				break
- 			}
- 			objs = append(objs, "asm.o")
- 		}
--		cmd = []string{"go", "tool", "pack", "c", "all.a"}
-+		cmd = []string{findGoCmd(), "tool", "pack", "c", "all.a"}
- 		cmd = append(cmd, objs...)
- 		_, err = runcmd(cmd...)
- 		if err != nil {
- 			t.err = err
- 			break
- 		}
--		cmd = []string{"go", "tool", "link", "all.a"}
-+		cmd = []string{findGoCmd(), "tool", "link", "all.a"}
- 		_, err = runcmd(cmd...)
- 		if err != nil {
- 			t.err = err
- 			break
- 		}
- 
- 	case "buildrun": // build binary, then run binary, instead of go run. Useful for timeout tests where failure mode is infinite loop.
- 		// TODO: not supported on NaCl
- 		useTmp = true
--		cmd := []string{"go", "build", goGcflags(), "-o", "a.exe"}
--		if *linkshared {
--			cmd = append(cmd, "-linkshared")
--		}
-+		cmd := []string{findGoCmd(), "build", goGcflags(), "-o", "a.exe"}
- 		longdirgofile := filepath.Join(filepath.Join(cwd, t.dir), t.gofile)
- 		cmd = append(cmd, flags...)
- 		cmd = append(cmd, longdirgofile)
- 		out, err := runcmd(cmd...)
- 		if err != nil {
- 			t.err = err
- 			return
- 		}
--		cmd = []string{"./a.exe"}
-+		cmd = []string{}
-+		if len(findExecCmd()) > 0 {
-+			cmd = append(cmd, findExecCmd()...)
-+		}
-+		cmd = append(cmd, "./a.exe")
- 		out, err = runcmd(append(cmd, args...)...)
- 		if err != nil {
- 			t.err = err
- 			return
- 		}
- 
- 		if strings.Replace(string(out), "\r\n", "\n", -1) != t.expectedOutput() {
- 			t.err = fmt.Errorf("incorrect output\n%s", out)
- 		}
- 
- 	case "run":
- 		useTmp = false
--		var out []byte
--		var err error
--		if len(flags)+len(args) == 0 && goGcflags() == "" && !*linkshared {
--			// If we're not using special go command flags,
--			// skip all the go command machinery.
--			// This avoids any time the go command would
--			// spend checking whether, for example, the installed
--			// package runtime is up to date.
--			// Because we run lots of trivial test programs,
--			// the time adds up.
--			pkg := filepath.Join(t.tempDir, "pkg.a")
--			if _, err := runcmd("go", "tool", "compile", "-o", pkg, t.goFileName()); err != nil {
--				t.err = err
--				return
--			}
--			exe := filepath.Join(t.tempDir, "test.exe")
--			cmd := []string{"go", "tool", "link", "-s", "-w"}
--			cmd = append(cmd, "-o", exe, pkg)
--			if _, err := runcmd(cmd...); err != nil {
--				t.err = err
--				return
--			}
--			out, err = runcmd(append([]string{exe}, args...)...)
--		} else {
--			cmd := []string{"go", "run", goGcflags()}
--			if *linkshared {
--				cmd = append(cmd, "-linkshared")
--			}
--			cmd = append(cmd, flags...)
--			cmd = append(cmd, t.goFileName())
--			out, err = runcmd(append(cmd, args...)...)
--		}
-+		out, err := goRun(runcmd, flags, t.goFileName(), args...)
- 		if err != nil {
- 			t.err = err
- 			return
- 		}
- 		if strings.Replace(string(out), "\r\n", "\n", -1) != t.expectedOutput() {
- 			t.err = fmt.Errorf("incorrect output\n%s", out)
- 		}
- 
- 	case "runoutput":
- 		rungatec <- true
- 		defer func() {
- 			<-rungatec
- 		}()
- 		useTmp = false
--		cmd := []string{"go", "run", goGcflags()}
--		if *linkshared {
--			cmd = append(cmd, "-linkshared")
--		}
--		cmd = append(cmd, t.goFileName())
--		out, err := runcmd(append(cmd, args...)...)
-+		out, err := goRun(runcmd, nil, t.goFileName(), args...)
- 		if err != nil {
- 			t.err = err
- 			return
- 		}
- 		tfile := filepath.Join(t.tempDir, "tmp__.go")
- 		if err := ioutil.WriteFile(tfile, out, 0666); err != nil {
- 			t.err = fmt.Errorf("write tempfile:%s", err)
- 			return
- 		}
--		cmd = []string{"go", "run", goGcflags()}
--		if *linkshared {
--			cmd = append(cmd, "-linkshared")
--		}
--		cmd = append(cmd, tfile)
--		out, err = runcmd(cmd...)
-+		out, err = goRun(runcmd, nil, tfile)
- 		if err != nil {
- 			t.err = err
- 			return
- 		}
- 		if string(out) != t.expectedOutput() {
- 			t.err = fmt.Errorf("incorrect output\n%s", out)
- 		}
- 
- 	case "errorcheckoutput":
- 		useTmp = false
--		cmd := []string{"go", "run", goGcflags()}
--		if *linkshared {
--			cmd = append(cmd, "-linkshared")
--		}
--		cmd = append(cmd, t.goFileName())
--		out, err := runcmd(append(cmd, args...)...)
-+		out, err := goRun(runcmd, nil, t.goFileName(), args...)
- 		if err != nil {
- 			t.err = err
- 			return
- 		}
- 		tfile := filepath.Join(t.tempDir, "tmp__.go")
- 		err = ioutil.WriteFile(tfile, out, 0666)
- 		if err != nil {
- 			t.err = fmt.Errorf("write tempfile:%s", err)
- 			return
- 		}
--		cmdline := []string{"go", "tool", "compile", "-e", "-o", "a.o"}
-+		cmdline := []string{findGoCmd(), "tool", "compile", "-e", "-o", "a.o"}
- 		cmdline = append(cmdline, flags...)
- 		cmdline = append(cmdline, tfile)
- 		out, err = runcmd(cmdline...)
- 		if wantError {
- 			if err == nil {
- 				t.err = fmt.Errorf("compilation succeeded unexpectedly\n%s", out)
- 				return
- 			}
-@@ -917,26 +872,37 @@ func (t *test) run() {
- 
- var execCmd []string
- 
- func findExecCmd() []string {
- 	if execCmd != nil {
- 		return execCmd
- 	}
- 	execCmd = []string{} // avoid work the second time
-+	if *target != "" {
-+		execCmd = []string{"go_" + *target + "_exec"}
-+		return execCmd
-+	}
- 	if goos == runtime.GOOS && goarch == runtime.GOARCH {
- 		return execCmd
- 	}
- 	path, err := exec.LookPath(fmt.Sprintf("go_%s_%s_exec", goos, goarch))
- 	if err == nil {
- 		execCmd = []string{path}
- 	}
- 	return execCmd
- }
- 
-+func findGoCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
- func (t *test) String() string {
- 	return filepath.Join(t.dir, t.gofile)
- }
- 
- func (t *test) makeTempDir() {
- 	var err error
- 	t.tempDir, err = ioutil.TempDir("", "")
- 	check(err)
diff --git a/go/patch/go-1.10.2/go3.patch b/go/patch/go-1.10.2/go3.patch
deleted file mode 100644
index 62247a03..00000000
--- a/go/patch/go-1.10.2/go3.patch
+++ /dev/null
@@ -1,980 +0,0 @@
-test: add runtarget action.
-
---- test/fixedbugs/bug248.go
-+++ test/fixedbugs/bug248.go
-@@ -1,38 +1,57 @@
- // +build !nacl,!plan9,!windows
--// run
-+// runtarget
- 
- // Copyright 2009 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
-+func goRun(cmd ...string) {
-+	if *target == "" {
-+		run(cmd[0], cmd[1:]...)
-+	} else {
-+		run("go_"+*target+"_exec", cmd...)
-+	}
-+}
-+
- func main() {
-+	flag.Parse()
- 	// TODO: If we get rid of errchk, re-enable this test on Windows.
- 	errchk, err := filepath.Abs("errchk")
- 	check(err)
- 
- 	err = os.Chdir(filepath.Join("fixedbugs", "bug248.dir"))
- 	check(err)
- 
--	run("go", "tool", "compile", "bug0.go")
--	run("go", "tool", "compile", "bug1.go")
--	run("go", "tool", "compile", "bug2.go")
--	run(errchk, "go", "tool", "compile", "-e", "bug3.go")
--	run("go", "tool", "link", "bug2.o")
--	run(fmt.Sprintf(".%ca.out", filepath.Separator))
-+	run(goCmd(), "tool", "compile", "bug0.go")
-+	run(goCmd(), "tool", "compile", "bug1.go")
-+	run(goCmd(), "tool", "compile", "bug2.go")
-+	run(errchk, goCmd(), "tool", "compile", "-e", "bug3.go")
-+	run(goCmd(), "tool", "link", "bug2.o")
-+	goRun(fmt.Sprintf(".%ca.out", filepath.Separator))
- 
- 	os.Remove("bug0.o")
- 	os.Remove("bug1.o")
- 	os.Remove("bug2.o")
- 	os.Remove("a.out")
- }
- 
- func run(name string, args ...string) {
---- test/fixedbugs/bug302.go
-+++ test/fixedbugs/bug302.go
-@@ -1,28 +1,39 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2010 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
- func main() {
--	run("go", "tool", "compile", filepath.Join("fixedbugs", "bug302.dir", "p.go"))
--	run("go", "tool", "pack", "grc", "pp.a", "p.o")
--	run("go", "tool", "compile", "-I", ".", filepath.Join("fixedbugs", "bug302.dir", "main.go"))
-+	flag.Parse()
-+	run(goCmd(), "tool", "compile", filepath.Join("fixedbugs", "bug302.dir", "p.go"))
-+	run(goCmd(), "tool", "pack", "grc", "pp.a", "p.o")
-+	run(goCmd(), "tool", "compile", "-I", ".", filepath.Join("fixedbugs", "bug302.dir", "main.go"))
- 	os.Remove("p.o")
- 	os.Remove("pp.a")
- 	os.Remove("main.o")
- }
- 
- func run(cmd string, args ...string) {
- 	out, err := exec.Command(cmd, args...).CombinedOutput()
- 	if err != nil {
---- test/fixedbugs/bug345.go
-+++ test/fixedbugs/bug345.go
-@@ -1,34 +1,45 @@
- // +build !nacl,!plan9,!windows
--// run
-+// runtarget
- 
- // Copyright 2011 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
- func main() {
-+	flag.Parse()
- 	// TODO: If we get rid of errchk, re-enable this test on Plan 9 and Windows.
- 	errchk, err := filepath.Abs("errchk")
- 	check(err)
- 
- 	err = os.Chdir(filepath.Join(".", "fixedbugs", "bug345.dir"))
- 	check(err)
- 
--	run("go", "tool", "compile", "io.go")
--	run(errchk, "go", "tool", "compile", "-e", "main.go")
-+	run(goCmd(), "tool", "compile", "io.go")
-+	run(errchk, goCmd(), "tool", "compile", "-e", "main.go")
- 	os.Remove("io.o")
- }
- 
- func run(name string, args ...string) {
- 	cmd := exec.Command(name, args...)
- 	out, err := cmd.CombinedOutput()
- 	if err != nil {
- 		fmt.Println(string(out))
---- test/fixedbugs/bug369.go
-+++ test/fixedbugs/bug369.go
-@@ -1,35 +1,54 @@
- // +build !nacl,!windows
--// run
-+// runtarget
- 
- // Copyright 2011 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Test that compiling with optimization turned on produces faster code.
- 
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
-+func goRun(cmd ...string) {
-+	if *target == "" {
-+		run(cmd[0], cmd[1:]...)
-+	} else {
-+		run("go_"+*target+"_exec", cmd...)
-+	}
-+}
-+
- func main() {
-+	flag.Parse()
- 	err := os.Chdir(filepath.Join(".", "fixedbugs", "bug369.dir"))
- 	check(err)
- 
--	run("go", "tool", "compile", "-N", "-o", "slow.o", "pkg.go")
--	run("go", "tool", "compile", "-o", "fast.o", "pkg.go")
--	run("go", "tool", "compile", "-o", "main.o", "main.go")
--	run("go", "tool", "link", "-o", "a.exe", "main.o")
--	run("." + string(filepath.Separator) + "a.exe")
-+	run(goCmd(), "tool", "compile", "-N", "-o", "slow.o", "pkg.go")
-+	run(goCmd(), "tool", "compile", "-o", "fast.o", "pkg.go")
-+	run(goCmd(), "tool", "compile", "-o", "main.o", "main.go")
-+	run(goCmd(), "tool", "link", "-o", "a.exe", "main.o")
-+	goRun("." + string(filepath.Separator) + "a.exe")
- 
- 	os.Remove("slow.o")
- 	os.Remove("fast.o")
- 	os.Remove("main.o")
- 	os.Remove("a.exe")
- }
- 
- func run(name string, args ...string) {
---- test/fixedbugs/bug429_run.go
-+++ test/fixedbugs/bug429_run.go
-@@ -1,29 +1,49 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Run the bug429.go test.
- 
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
-+func goRun(args ...string) *exec.Cmd {
-+	cmd := []string{"run"}
-+	if *target != "" {
-+		cmd = append(cmd, "-exec", "go_"+*target+"_exec")
-+	}
-+	cmd = append(cmd, args...)
-+	return exec.Command(goCmd(), cmd...)
-+}
-+
- func main() {
--	cmd := exec.Command("go", "run", filepath.Join("fixedbugs", "bug429.go"))
-+	flag.Parse()
-+	cmd := goRun(filepath.Join("fixedbugs", "bug429.go"))
- 	out, err := cmd.CombinedOutput()
- 	if err == nil {
- 		fmt.Println("expected deadlock")
- 		os.Exit(1)
- 	}
- 
- 	want := "fatal error: all goroutines are asleep - deadlock!"
- 	got := string(out)
---- test/fixedbugs/issue10607.go
-+++ test/fixedbugs/issue10607.go
-@@ -1,31 +1,51 @@
- // +build linux,!ppc64 android
--// run
-+// runtarget
- 
- // Copyright 2015 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Test that a -B option is passed through when using both internal
- // and external linking mode.
- 
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
-+func goRun(args ...string) *exec.Cmd {
-+	cmd := []string{"run"}
-+	if *target != "" {
-+		cmd = append(cmd, "-exec", "go_"+*target+"_exec")
-+	}
-+	cmd = append(cmd, args...)
-+	return exec.Command(goCmd(), cmd...)
-+}
-+
- func main() {
--	test("internal")
-+	flag.Parse()
-+	// test("internal")
- 	test("external")
- }
- 
- func test(linkmode string) {
--	out, err := exec.Command("go", "run", "-ldflags", "-B=0x12345678 -linkmode="+linkmode, filepath.Join("fixedbugs", "issue10607a.go")).CombinedOutput()
-+	out, err := goRun("-ldflags", "-B=0x12345678 -linkmode="+linkmode, filepath.Join("fixedbugs", "issue10607a.go")).CombinedOutput()
- 	if err != nil {
- 		fmt.Printf("BUG: linkmode=%s %v\n%s\n", linkmode, err, out)
- 		os.Exit(1)
- 	}
- }
---- test/fixedbugs/issue11771.go
-+++ test/fixedbugs/issue11771.go
-@@ -1,31 +1,42 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2015 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Issue 11771: Magic comments should ignore carriage returns.
- 
- package main
- 
- import (
- 	"bytes"
-+	"flag"
- 	"fmt"
- 	"io/ioutil"
- 	"log"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- 	"runtime"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
- func main() {
-+	flag.Parse()
- 	if runtime.Compiler != "gc" {
- 		return
- 	}
- 
- 	dir, err := ioutil.TempDir("", "go-issue11771")
- 	if err != nil {
- 		log.Fatalf("creating temp dir: %v\n", err)
- 	}
-@@ -47,17 +58,17 @@ func main() {
- func x() {
- }
- `)
- 
- 	if err := ioutil.WriteFile(filepath.Join(dir, "x.go"), buf.Bytes(), 0666); err != nil {
- 		log.Fatal(err)
- 	}
- 
--	cmd := exec.Command("go", "tool", "compile", "x.go")
-+	cmd := exec.Command(goCmd(), "tool", "compile", "x.go")
- 	cmd.Dir = dir
- 	output, err := cmd.CombinedOutput()
- 	if err == nil {
- 		log.Fatal("compile succeeded unexpectedly")
- 	}
- 	if !bytes.Contains(output, []byte("only allowed in runtime")) {
- 		log.Fatalf("wrong error message from compiler; got:\n%s\n", output)
- 	}
---- test/fixedbugs/issue9355.go
-+++ test/fixedbugs/issue9355.go
-@@ -1,34 +1,45 @@
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- 	"regexp"
- 	"runtime"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
- func main() {
-+	flag.Parse()
- 	if runtime.Compiler != "gc" || runtime.GOOS == "nacl" {
- 		return
- 	}
- 
- 	err := os.Chdir(filepath.Join("fixedbugs", "issue9355.dir"))
- 	check(err)
- 
--	out := run("go", "tool", "compile", "-S", "a.go")
-+	out := run(goCmd(), "tool", "compile", "-S", "a.go")
- 	os.Remove("a.o")
- 
- 	// 6g/8g print the offset as dec, but 5g/9g print the offset as hex.
- 	patterns := []string{
- 		`rel 0\+\d t=1 \"\"\.x\+8\r?\n`,       // y = &x.b
- 		`rel 0\+\d t=1 \"\"\.x\+(28|1c)\r?\n`, // z = &x.d.q
- 		`rel 0\+\d t=1 \"\"\.b\+5\r?\n`,       // c = &b[5]
- 		`rel 0\+\d t=1 \"\"\.x\+(88|58)\r?\n`, // w = &x.f[3].r
---- test/fixedbugs/issue9862_run.go
-+++ test/fixedbugs/issue9862_run.go
-@@ -1,26 +1,46 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2015 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Check for compile or link error.
- 
- package main
- 
- import (
-+	"flag"
- 	"os/exec"
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
-+func goRun(args ...string) *exec.Cmd {
-+	cmd := []string{"run"}
-+	if *target != "" {
-+		cmd = append(cmd, "-exec", "go_"+*target+"_exec")
-+	}
-+	cmd = append(cmd, args...)
-+	return exec.Command(goCmd(), cmd...)
-+}
-+
- func main() {
--	out, err := exec.Command("go", "run", "fixedbugs/issue9862.go").CombinedOutput()
-+	flag.Parse()
-+	out, err := goRun("fixedbugs/issue9862.go").CombinedOutput()
- 	outstr := string(out)
- 	if err == nil {
- 		println("go run issue9862.go succeeded, should have failed\n", outstr)
- 		return
- 	}
- 	if !strings.Contains(outstr, "symbol too large") {
- 		println("go run issue9862.go gave unexpected error; want symbol too large:\n", outstr)
- 	}
---- test/linkmain_run.go
-+++ test/linkmain_run.go
-@@ -1,26 +1,36 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Run the sinit test.
- 
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
- func cleanup() {
- 	os.Remove("linkmain.o")
- 	os.Remove("linkmain.a")
- 	os.Remove("linkmain1.o")
- 	os.Remove("linkmain1.a")
- 	os.Remove("linkmain.exe")
- }
- 
-@@ -46,21 +56,23 @@ func runFail(cmdline string) {
- 		fmt.Println(string(out))
- 		fmt.Println("SHOULD HAVE FAILED!")
- 		cleanup()
- 		os.Exit(1)
- 	}
- }
- 
- func main() {
-+	flag.Parse()
-+
- 	// helloworld.go is package main
--	run("go tool compile -o linkmain.o helloworld.go")
--	run("go tool compile -pack -o linkmain.a helloworld.go")
--	run("go tool link -o linkmain.exe linkmain.o")
--	run("go tool link -o linkmain.exe linkmain.a")
-+	run(goCmd() + " tool compile -o linkmain.o helloworld.go")
-+	run(goCmd() + " tool compile -pack -o linkmain.a helloworld.go")
-+	run(goCmd() + " tool link -o linkmain.exe linkmain.o")
-+	run(goCmd() + " tool link -o linkmain.exe linkmain.a")
- 
- 	// linkmain.go is not
--	run("go tool compile -o linkmain1.o linkmain.go")
--	run("go tool compile -pack -o linkmain1.a linkmain.go")
--	runFail("go tool link -o linkmain.exe linkmain1.o")
--	runFail("go tool link -o linkmain.exe linkmain1.a")
-+	run(goCmd() + " tool compile -o linkmain1.o linkmain.go")
-+	run(goCmd() + " tool compile -pack -o linkmain1.a linkmain.go")
-+	runFail(goCmd() + " tool link -o linkmain.exe linkmain1.o")
-+	runFail(goCmd() + " tool link -o linkmain.exe linkmain1.a")
- 	cleanup()
- }
---- test/linkobj.go
-+++ test/linkobj.go
-@@ -1,31 +1,50 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2016 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Test the compiler -linkobj flag.
- 
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"io/ioutil"
- 	"log"
- 	"os"
- 	"os/exec"
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
-+func goRun(cmd ...string) string {
-+	if *target == "" {
-+		return run(cmd...)
-+	} else {
-+		return run(append([]string{"go_"+*target+"_exec"}, cmd...)...)
-+	}
-+}
-+
- var pwd, tmpdir string
- 
- func main() {
-+	flag.Parse()
- 	dir, err := ioutil.TempDir("", "go-test-linkobj-")
- 	if err != nil {
- 		log.Fatal(err)
- 	}
- 	pwd, err = os.Getwd()
- 	if err != nil {
- 		log.Fatal(err)
- 	}
-@@ -71,33 +90,33 @@ func main() {
- 
- 		// The compiler expects the files being read to have the right suffix.
- 		o := "o"
- 		if round == 1 {
- 			o = "a"
- 		}
- 
- 		// inlining is disabled to make sure that the link objects contain needed code.
--		run("go", "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p1."+o, "-linkobj", "p1.lo", "p1.go")
--		run("go", "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p2."+o, "-linkobj", "p2.lo", "p2.go")
--		run("go", "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p3."+o, "-linkobj", "p3.lo", "p3.go")
-+		run(goCmd(), "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p1."+o, "-linkobj", "p1.lo", "p1.go")
-+		run(goCmd(), "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p2."+o, "-linkobj", "p2.lo", "p2.go")
-+		run(goCmd(), "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p3."+o, "-linkobj", "p3.lo", "p3.go")
- 
- 		cp("p1."+o, "p1.oo")
- 		cp("p2."+o, "p2.oo")
- 		cp("p3."+o, "p3.oo")
- 		cp("p1.lo", "p1."+o)
- 		cp("p2.lo", "p2."+o)
- 		cp("p3.lo", "p3."+o)
--		out := runFail("go", "tool", "link", "p2."+o)
-+		out := runFail(goCmd(), "tool", "link", "p2."+o)
- 		if !strings.Contains(out, "not package main") {
- 			fatalf("link p2.o failed but not for package main:\n%s", out)
- 		}
- 
--		run("go", "tool", "link", "-L", ".", "-o", "a.out.exe", "p3."+o)
--		out = run("./a.out.exe")
-+		run(goCmd(), "tool", "link", "-L", ".", "-o", "a.out.exe", "p3."+o)
-+		out = goRun("./a.out.exe")
- 		if !strings.Contains(out, "hello from p1\nhello from p2\nhello from main\n") {
- 			fatalf("running main, incorrect output:\n%s", out)
- 		}
- 
- 		// ensure that mistaken future round can't use these
- 		os.Remove("p1.o")
- 		os.Remove("a.out.exe")
- 	}
---- test/linkx_run.go
-+++ test/linkx_run.go
-@@ -1,35 +1,55 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Run the linkx test.
- 
- package main
- 
- import (
- 	"bytes"
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
-+func goRun(args ...string) *exec.Cmd {
-+	cmd := []string{"run"}
-+	if *target != "" {
-+		cmd = append(cmd, "-exec", "go_"+*target+"_exec")
-+	}
-+	cmd = append(cmd, args...)
-+	return exec.Command(goCmd(), cmd...)
-+}
-+
- func main() {
-+	flag.Parse()
- 	// test(" ") // old deprecated & removed syntax
- 	test("=") // new syntax
- }
- 
- func test(sep string) {
- 	// Successful run
--	cmd := exec.Command("go", "run", "-ldflags=-X main.tbd"+sep+"hello -X main.overwrite"+sep+"trumped -X main.nosuchsymbol"+sep+"neverseen", "linkx.go")
-+	cmd := goRun("-ldflags=-X main.tbd"+sep+"hello -X main.overwrite"+sep+"trumped -X main.nosuchsymbol"+sep+"neverseen", "linkx.go")
- 	var out, errbuf bytes.Buffer
- 	cmd.Stdout = &out
- 	cmd.Stderr = &errbuf
- 	err := cmd.Run()
- 	if err != nil {
- 		fmt.Println(errbuf.String())
- 		fmt.Println(out.String())
- 		fmt.Println(err)
-@@ -39,25 +59,25 @@ func test(sep string) {
- 	want := "hello\ntrumped\n"
- 	got := out.String()
- 	if got != want {
- 		fmt.Printf("got %q want %q\n", got, want)
- 		os.Exit(1)
- 	}
- 
- 	// Issue 8810
--	cmd = exec.Command("go", "run", "-ldflags=-X main.tbd", "linkx.go")
-+	cmd = goRun("-ldflags=-X main.tbd", "linkx.go")
- 	_, err = cmd.CombinedOutput()
- 	if err == nil {
- 		fmt.Println("-X linker flag should not accept keys without values")
- 		os.Exit(1)
- 	}
- 
- 	// Issue 9621
--	cmd = exec.Command("go", "run", "-ldflags=-X main.b=false -X main.x=42", "linkx.go")
-+	cmd = goRun("-ldflags=-X main.b=false -X main.x=42", "linkx.go")
- 	outx, err := cmd.CombinedOutput()
- 	if err == nil {
- 		fmt.Println("-X linker flag should not overwrite non-strings")
- 		os.Exit(1)
- 	}
- 	outstr := string(outx)
- 	if !strings.Contains(outstr, "main.b") {
- 		fmt.Printf("-X linker flag did not diagnose overwrite of main.b:\n%s\n", outstr)
---- test/nosplit.go
-+++ test/nosplit.go
-@@ -1,31 +1,49 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- package main
- 
- import (
- 	"bytes"
-+	"flag"
- 	"fmt"
- 	"io/ioutil"
- 	"log"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- 	"regexp"
--	"runtime"
- 	"strconv"
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
-+func goArch() string {
-+	goarch, err := exec.Command(goCmd(), "env", "GOARCH").Output()
-+	if err != nil {
-+		bug()
-+		fmt.Printf("running go env GOARCH: %v\n", err)
-+	}
-+	return strings.TrimSpace(string(goarch))
-+}
-+
- var tests = `
- # These are test cases for the linker analysis that detects chains of
- # nosplit functions that would cause a stack overflow.
- #
- # Lines beginning with # are comments.
- #
- # Each test case describes a sequence of functions, one per line.
- # Each function definition is the function name, then the frame size,
-@@ -189,22 +207,23 @@ var (
- 	commentRE = regexp.MustCompile(`(?m)^#.*`)
- 	rejectRE  = regexp.MustCompile(`(?s)\A(.+?)((\n|; *)REJECT(.*))?\z`)
- 	lineRE    = regexp.MustCompile(`(\w+) (\d+)( nosplit)?(.*)`)
- 	callRE    = regexp.MustCompile(`\bcall (\w+)\b`)
- 	callindRE = regexp.MustCompile(`\bcallind\b`)
- )
- 
- func main() {
--	goarch := os.Getenv("GOARCH")
-+	flag.Parse()
-+	goarch := goArch()
- 	if goarch == "" {
--		goarch = runtime.GOARCH
-+		return
- 	}
- 
--	version, err := exec.Command("go", "tool", "compile", "-V").Output()
-+	version, err := exec.Command(goCmd(), "tool", "compile", "-V").Output()
- 	if err != nil {
- 		bug()
- 		fmt.Printf("running go tool compile -V: %v\n", err)
- 		return
- 	}
- 	if s := string(version); goarch == "amd64" && strings.Contains(s, "X:") && !strings.Contains(s, "framepointer") {
- 		// Skip this test if framepointer is NOT enabled on AMD64
- 		return
-@@ -340,17 +359,17 @@ TestCases:
- 
- 		if err := ioutil.WriteFile(filepath.Join(dir, "asm.s"), buf.Bytes(), 0666); err != nil {
- 			log.Fatal(err)
- 		}
- 		if err := ioutil.WriteFile(filepath.Join(dir, "main.go"), gobuf.Bytes(), 0666); err != nil {
- 			log.Fatal(err)
- 		}
- 
--		cmd := exec.Command("go", "build")
-+		cmd := exec.Command(goCmd(), "build")
- 		cmd.Dir = dir
- 		output, err := cmd.CombinedOutput()
- 		if err == nil {
- 			nok++
- 			if reject {
- 				bug()
- 				fmt.Printf("accepted incorrectly:\n\t%s\n", indent(strings.TrimSpace(stanza)))
- 			}
---- test/run.go
-+++ test/run.go
-@@ -222,16 +222,26 @@ func goRun(runcmd runCmd, flags []string, goname string, args ...string) (out []
- 		cmd = append(cmd, findExecCmd()...)
- 	}
- 	cmd = append(cmd, flags...)
- 	cmd = append(cmd, goname)
- 	cmd = append(cmd, args...)
- 	return runcmd(cmd...)
- }
- 
-+func goRunTarget(runcmd runCmd, goname string, args ...string) (out []byte, err error) {
-+	cmd := []string{"go_local", "run"}
-+	cmd = append(cmd, goname)
-+	if *target != "" {
-+		cmd = append(cmd, "-target", *target)
-+	}
-+	cmd = append(cmd, args...)
-+	return runcmd(cmd...)
-+}
-+
- // skipError describes why a test was skipped.
- type skipError string
- 
- func (s skipError) Error() string { return string(s) }
- 
- func check(err error) {
- 	if err != nil {
- 		log.Fatal(err)
-@@ -484,17 +494,17 @@ func (t *test) run() {
- 	}
- 
- 	// TODO: Clean up/simplify this switch statement.
- 	switch action {
- 	case "rundircmpout":
- 		action = "rundir"
- 	case "cmpout":
- 		action = "run" // the run case already looks for <dir>/<test>.out files
--	case "compile", "compiledir", "build", "builddir", "run", "buildrun", "runoutput", "rundir":
-+	case "compile", "compiledir", "build", "builddir", "run", "runtarget", "buildrun", "runoutput", "rundir":
- 		// nothing to do
- 	case "errorcheckandrundir":
- 		wantError = false // should be no error if also will run
- 	case "errorcheckwithauto":
- 		action = "errorcheck"
- 		wantAuto = true
- 		wantError = true
- 	case "errorcheck", "errorcheckdir", "errorcheckoutput":
-@@ -807,16 +817,27 @@ func (t *test) run() {
- 		if err != nil {
- 			t.err = err
- 			return
- 		}
- 		if strings.Replace(string(out), "\r\n", "\n", -1) != t.expectedOutput() {
- 			t.err = fmt.Errorf("incorrect output\n%s", out)
- 		}
- 
-+	case "runtarget":
-+		useTmp = false
-+		out, err := goRunTarget(runcmd, t.goFileName(), args...)
-+		if err != nil {
-+			t.err = err
-+			return
-+		}
-+		if strings.Replace(string(out), "\r\n", "\n", -1) != t.expectedOutput() {
-+			t.err = fmt.Errorf("incorrect output\n%s", out)
-+		}
-+
- 	case "runoutput":
- 		rungatec <- true
- 		defer func() {
- 			<-rungatec
- 		}()
- 		useTmp = false
- 		out, err := goRun(runcmd, nil, t.goFileName(), args...)
- 		if err != nil {
---- test/sinit_run.go
-+++ test/sinit_run.go
-@@ -1,28 +1,39 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Run the sinit test.
- 
- package main
- 
- import (
- 	"bytes"
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return "go"
-+}
-+
- func main() {
--	cmd := exec.Command("go", "tool", "compile", "-S", "sinit.go")
-+	flag.Parse()
-+	cmd := exec.Command(goCmd(), "tool", "compile", "-S", "sinit.go")
- 	out, err := cmd.CombinedOutput()
- 	if err != nil {
- 		fmt.Println(string(out))
- 		fmt.Println(err)
- 		os.Exit(1)
- 	}
- 	os.Remove("sinit.o")
- 
diff --git a/go/patch/go-1.10.2/go4.patch b/go/patch/go-1.10.2/go4.patch
deleted file mode 100644
index 290de390..00000000
--- a/go/patch/go-1.10.2/go4.patch
+++ /dev/null
@@ -1,199 +0,0 @@
-runtime, crypto/x509: add -target flag.
-
---- src/crypto/x509/x509_test.go
-+++ src/crypto/x509/x509_test.go
-@@ -13,29 +13,32 @@ import (
- 	"crypto/rsa"
- 	_ "crypto/sha256"
- 	_ "crypto/sha512"
- 	"crypto/x509/pkix"
- 	"encoding/asn1"
- 	"encoding/base64"
- 	"encoding/hex"
- 	"encoding/pem"
-+	"flag"
- 	"fmt"
- 	"internal/testenv"
- 	"math/big"
- 	"net"
- 	"net/url"
- 	"os/exec"
- 	"reflect"
- 	"runtime"
- 	"strings"
- 	"testing"
- 	"time"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
- func TestParsePKCS1PrivateKey(t *testing.T) {
- 	block, _ := pem.Decode([]byte(pemPrivateKey))
- 	priv, err := ParsePKCS1PrivateKey(block.Bytes)
- 	if err != nil {
- 		t.Errorf("Failed to parse private key: %s", err)
- 		return
- 	}
- 	if priv.PublicKey.N.Cmp(rsaPrivateKey.PublicKey.N) != 0 ||
-@@ -1089,17 +1092,23 @@ func TestParsePEMCRL(t *testing.T) {
- 	}
- 
- 	// Can't check the signature here without a package cycle.
- }
- 
- func TestImports(t *testing.T) {
- 	testenv.MustHaveGoRun(t)
- 
--	if err := exec.Command(testenv.GoToolPath(t), "run", "x509_test_import.go").Run(); err != nil {
-+	var cmd *exec.Cmd
-+	if *target == "" {
-+		cmd = exec.Command(testenv.GoToolPath(t), "run", "x509_test_import.go")
-+	} else {
-+		cmd = exec.Command("go_"+*target, "run", "-exec", "go_"+*target+"_exec", "x509_test_import.go")
-+	}
-+	if err := cmd.Run(); err != nil {
- 		t.Errorf("failed to run x509_test_import.go: %s", err)
- 	}
- }
- 
- const derCRLBase64 = "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"
- 
- const pemCRLBase64 = "LS0tLS1CRUdJTiBYNTA5IENSTC0tLS0tDQpNSUlCOWpDQ0FWOENBUUV3RFFZSktvWklodmNOQVFFRkJRQXdiREVhTUJnR0ExVUVDaE1SVWxOQklGTmxZM1Z5DQphWFI1SUVsdVl5NHhIakFjQmdOVkJBTVRGVkpUUVNCUWRXSnNhV01nVW05dmRDQkRRU0IyTVRFdU1Dd0dDU3FHDQpTSWIzRFFFSkFSWWZjbk5oYTJWdmJuSnZiM1J6YVdkdVFISnpZWE5sWTNWeWFYUjVMbU52YlJjTk1URXdNakl6DQpNVGt5T0RNd1doY05NVEV3T0RJeU1Ua3lPRE13V2pDQmpEQktBaEVBckRxb2g5RkhKSFhUN09QZ3V1bjQrQmNODQpNRGt4TVRBeU1UUXlOekE1V2pBbU1Bb0dBMVVkRlFRRENnRUpNQmdHQTFVZEdBUVJHQTh5TURBNU1URXdNakUwDQpNalExTlZvd1BnSVJBTEd6blowOTVQQjVhQU9MUGc1N2ZNTVhEVEF5TVRBeU16RTBOVEF4TkZvd0dqQVlCZ05WDQpIUmdFRVJnUE1qQXdNakV3TWpNeE5EVXdNVFJhb0RBd0xqQWZCZ05WSFNNRUdEQVdnQlQxVERGNlVRTS9MTmVMDQpsNWx2cUhHUXEzZzltekFMQmdOVkhSUUVCQUlDQUlRd0RRWUpLb1pJaHZjTkFRRUZCUUFEZ1lFQUZVNUFzNk16DQpxNVBSc2lmYW9iUVBHaDFhSkx5QytNczVBZ2MwYld5QTNHQWR4dXI1U3BQWmVSV0NCamlQL01FSEJXSkNsQkhQDQpHUmNxNXlJZDNFakRrYUV5eFJhK2k2N0x6dmhJNmMyOUVlNks5cFNZd2ppLzdSVWhtbW5Qclh0VHhsTDBsckxyDQptUVFKNnhoRFJhNUczUUE0Q21VZHNITnZicnpnbUNZcHZWRT0NCi0tLS0tRU5EIFg1MDkgQ1JMLS0tLS0NCg0K"
- 
---- src/runtime/crash_cgo_test.go
-+++ src/runtime/crash_cgo_test.go
-@@ -279,17 +279,17 @@ func testCgoPprof(t *testing.T, buildArg, runArg string) {
- 	}
- 	testenv.MustHaveGoRun(t)
- 
- 	exe, err := buildTestProg(t, "testprogcgo", buildArg)
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 
--	got, err := testenv.CleanCmdEnv(exec.Command(exe, runArg)).CombinedOutput()
-+	got, err := testenv.CleanCmdEnv(goExecCmd(exe, runArg)).CombinedOutput()
- 	if err != nil {
- 		if testenv.Builder() == "linux-amd64-alpine" {
- 			// See Issue 18243 and Issue 19938.
- 			t.Skipf("Skipping failing test on Alpine (golang.org/issue/18243). Ignoring error: %v", err)
- 		}
- 		t.Fatal(err)
- 	}
- 	fn := strings.TrimSpace(string(got))
---- src/runtime/crash_test.go
-+++ src/runtime/crash_test.go
-@@ -17,16 +17,35 @@ import (
- 	"runtime"
- 	"strconv"
- 	"strings"
- 	"sync"
- 	"testing"
- 	"time"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd(t *testing.T) string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return testenv.GoToolPath(t)
-+}
-+
-+func goExecCmd(name string, arg ...string) *exec.Cmd {
-+	var cmd []string
-+	if *target != "" {
-+		cmd = append(cmd, "go_"+*target+"_exec")
-+	}
-+	cmd = append(cmd, name)
-+	cmd = append(cmd, arg...)
-+	return exec.Command(cmd[0], cmd[1:]...)
-+}
-+
- var toRemove []string
- 
- func TestMain(m *testing.M) {
- 	status := m.Run()
- 	for _, file := range toRemove {
- 		os.RemoveAll(file)
- 	}
- 	os.Exit(status)
-@@ -50,17 +69,17 @@ func runTestProg(t *testing.T, binary, name string, env ...string) string {
- 
- 	testenv.MustHaveGoBuild(t)
- 
- 	exe, err := buildTestProg(t, binary)
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 
--	cmd := testenv.CleanCmdEnv(exec.Command(exe, name))
-+	cmd := testenv.CleanCmdEnv(goExecCmd(exe, name))
- 	cmd.Env = append(cmd.Env, env...)
- 	if testing.Short() {
- 		cmd.Env = append(cmd.Env, "RUNTIME_TEST_SHORT=1")
- 	}
- 	var b bytes.Buffer
- 	cmd.Stdout = &b
- 	cmd.Stderr = &b
- 	if err := cmd.Start(); err != nil {
-@@ -125,17 +144,17 @@ func buildTestProg(t *testing.T, binary string, flags ...string) (string, error)
- 		name += "_" + strings.Join(flags, "_")
- 	}
- 	target, ok := testprog.target[name]
- 	if ok {
- 		return target.exe, target.err
- 	}
- 
- 	exe := filepath.Join(testprog.dir, name+".exe")
--	cmd := exec.Command(testenv.GoToolPath(t), append([]string{"build", "-o", exe}, flags...)...)
-+	cmd := exec.Command(goCmd(t), append([]string{"build", "-o", exe}, flags...)...)
- 	cmd.Dir = "testdata/" + binary
- 	out, err := testenv.CleanCmdEnv(cmd).CombinedOutput()
- 	if err != nil {
- 		target.err = fmt.Errorf("building %s %v: %v\n%s", binary, flags, err, out)
- 		testprog.target[name] = target
- 		return "", target.err
- 	}
- 	target.exe = exe
-@@ -456,17 +475,17 @@ func TestPanicLoop(t *testing.T) {
- func TestMemPprof(t *testing.T) {
- 	testenv.MustHaveGoRun(t)
- 
- 	exe, err := buildTestProg(t, "testprog")
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 
--	got, err := testenv.CleanCmdEnv(exec.Command(exe, "MemProf")).CombinedOutput()
-+	got, err := testenv.CleanCmdEnv(goExecCmd(exe, "MemProf")).CombinedOutput()
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 	fn := strings.TrimSpace(string(got))
- 	defer os.Remove(fn)
- 
- 	for try := 0; try < 2; try++ {
- 		cmd := testenv.CleanCmdEnv(exec.Command(testenv.GoToolPath(t), "tool", "pprof", "-alloc_space", "-top"))
---- src/runtime/crash_unix_test.go
-+++ src/runtime/crash_unix_test.go
-@@ -244,17 +244,17 @@ func testPanicSystemstackInternal() {
- }
- 
- func TestSignalExitStatus(t *testing.T) {
- 	testenv.MustHaveGoBuild(t)
- 	exe, err := buildTestProg(t, "testprog")
- 	if err != nil {
- 		t.Fatal(err)
- 	}
--	err = testenv.CleanCmdEnv(exec.Command(exe, "SignalExitStatus")).Run()
-+	err = testenv.CleanCmdEnv(goExecCmd(exe, "SignalExitStatus")).Run()
- 	if err == nil {
- 		t.Error("test program succeeded unexpectedly")
- 	} else if ee, ok := err.(*exec.ExitError); !ok {
- 		t.Errorf("error (%v) has type %T; expected exec.ExitError", err, err)
- 	} else if ws, ok := ee.Sys().(syscall.WaitStatus); !ok {
- 		t.Errorf("error.Sys (%v) has type %T; expected syscall.WaitStatus", ee.Sys(), ee.Sys())
- 	} else if !ws.Signaled() || ws.Signal() != syscall.SIGTERM {
- 		t.Errorf("got %v; expected SIGTERM", ee)
diff --git a/go/patch/go-1.10.2/go5.patch b/go/patch/go-1.10.2/go5.patch
deleted file mode 100644
index 7189c89e..00000000
--- a/go/patch/go-1.10.2/go5.patch
+++ /dev/null
@@ -1,160 +0,0 @@
-runtime: deadlock detection does not work when using external linker.
-
---- src/runtime/crash_test.go
-+++ src/runtime/crash_test.go
-@@ -214,32 +214,37 @@ func testDeadlock(t *testing.T, name string) {
- 	output := runTestProg(t, "testprog", name)
- 	want := "fatal error: all goroutines are asleep - deadlock!\n"
- 	if !strings.HasPrefix(output, want) {
- 		t.Fatalf("output does not start with %q:\n%s", want, output)
- 	}
- }
- 
- func TestSimpleDeadlock(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	testDeadlock(t, "SimpleDeadlock")
- }
- 
- func TestInitDeadlock(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	testDeadlock(t, "InitDeadlock")
- }
- 
- func TestLockedDeadlock(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	testDeadlock(t, "LockedDeadlock")
- }
- 
- func TestLockedDeadlock2(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	testDeadlock(t, "LockedDeadlock2")
- }
- 
- func TestGoexitDeadlock(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	output := runTestProg(t, "testprog", "GoexitDeadlock")
- 	want := "no goroutines (main called runtime.Goexit) - deadlock!"
- 	if !strings.Contains(output, want) {
- 		t.Fatalf("output:\n%s\n\nwant output containing: %s", output, want)
- 	}
- }
- 
- func TestStackOverflow(t *testing.T) {
-@@ -266,16 +271,17 @@ panic: again
- `
- 	if !strings.HasPrefix(output, want) {
- 		t.Fatalf("output does not start with %q:\n%s", want, output)
- 	}
- 
- }
- 
- func TestGoexitCrash(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	output := runTestProg(t, "testprog", "GoexitExit")
- 	want := "no goroutines (main called runtime.Goexit) - deadlock!"
- 	if !strings.Contains(output, want) {
- 		t.Fatalf("output:\n%s\n\nwant output containing: %s", output, want)
- 	}
- }
- 
- func TestGoexitDefer(t *testing.T) {
-@@ -324,16 +330,17 @@ func TestBreakpoint(t *testing.T) {
- 	// "runtime.Breakpoint(...)" instead of "runtime.Breakpoint()".
- 	want := "runtime.Breakpoint("
- 	if !strings.Contains(output, want) {
- 		t.Fatalf("output:\n%s\n\nwant output containing: %s", output, want)
- 	}
- }
- 
- func TestGoexitInPanic(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	// see issue 8774: this code used to trigger an infinite recursion
- 	output := runTestProg(t, "testprog", "GoexitInPanic")
- 	want := "fatal error: no goroutines (main called runtime.Goexit) - deadlock!"
- 	if !strings.HasPrefix(output, want) {
- 		t.Fatalf("output does not start with %q:\n%s", want, output)
- 	}
- }
- 
-@@ -388,16 +395,17 @@ func TestPanicAfterGoexit(t *testing.T) {
- 	output := runTestProg(t, "testprog", "PanicAfterGoexit")
- 	want := "panic: hello"
- 	if !strings.HasPrefix(output, want) {
- 		t.Fatalf("output does not start with %q:\n%s", want, output)
- 	}
- }
- 
- func TestRecoveredPanicAfterGoexit(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	output := runTestProg(t, "testprog", "RecoveredPanicAfterGoexit")
- 	want := "fatal error: no goroutines (main called runtime.Goexit) - deadlock!"
- 	if !strings.HasPrefix(output, want) {
- 		t.Fatalf("output does not start with %q:\n%s", want, output)
- 	}
- }
- 
- func TestRecoverBeforePanicAfterGoexit(t *testing.T) {
---- src/runtime/proc_test.go
-+++ src/runtime/proc_test.go
-@@ -349,19 +349,20 @@ func TestGCFairness2(t *testing.T) {
- 	want := "OK\n"
- 	if output != want {
- 		t.Fatalf("want %s, got %s\n", want, output)
- 	}
- }
- 
- func TestNumGoroutine(t *testing.T) {
- 	output := runTestProg(t, "testprog", "NumGoroutine")
--	want := "1\n"
--	if output != want {
--		t.Fatalf("want %q, got %q", want, output)
-+	want1 := "1\n"
-+	want2 := "2\n"
-+	if output != want1 && output != want2 {
-+		t.Fatalf("want %q, got %q", want1, output)
- 	}
- 
- 	buf := make([]byte, 1<<20)
- 
- 	// Try up to 10 times for a match before giving up.
- 	// This is a fundamentally racy check but it's important
- 	// to notice if NumGoroutine and Stack are _always_ out of sync.
- 	for i := 0; ; i++ {
---- test/fixedbugs/bug429_run.go
-+++ test/fixedbugs/bug429_run.go
-@@ -1,10 +1,10 @@
- // +build !nacl
--// runtarget
-+// skip
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Run the bug429.go test.
- 
- package main
---- test/goprint.go
-+++ test/goprint.go
-@@ -3,19 +3,14 @@
- // Copyright 2011 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
- // license that can be found in the LICENSE file.
- 
- // Test that println can be the target of a go statement.
- 
- package main
- 
--import (
--	"runtime"
--	"time"
--)
-+import "time"
- 
- func main() {
- 	go println(42, true, false, true, 1.5, "world", (chan int)(nil), []int(nil), (map[string]int)(nil), (func())(nil), byte(255))
--	for runtime.NumGoroutine() > 1 {
--		time.Sleep(10*time.Millisecond)
--	}
-+	time.Sleep(100*time.Millisecond)
- }
diff --git a/go/patch/go-1.10.2/go6.patch b/go/patch/go-1.10.2/go6.patch
deleted file mode 100644
index 9f32ed84..00000000
--- a/go/patch/go-1.10.2/go6.patch
+++ /dev/null
@@ -1,230 +0,0 @@
-all: disable some tests that have trouble running remotely.
-
---- src/encoding/gob/encoder_test.go
-+++ src/encoding/gob/encoder_test.go
-@@ -1125,20 +1125,17 @@ func TestBadData(t *testing.T) {
- 		if !strings.Contains(err.Error(), test.error) {
- 			t.Errorf("#%d: decode: expected %q error, got %s", i, test.error, err.Error())
- 		}
- 	}
- }
- 
- // TestHugeWriteFails tests that enormous messages trigger an error.
- func TestHugeWriteFails(t *testing.T) {
--	if testing.Short() {
--		// Requires allocating a monster, so don't do this from all.bash.
--		t.Skip("skipping huge allocation in short mode")
--	}
-+	t.Skip("skipping test due to huge memory requirement")
- 	huge := make([]byte, tooBig)
- 	huge[0] = 7 // Make sure it's not all zeros.
- 	buf := new(bytes.Buffer)
- 	err := NewEncoder(buf).Encode(huge)
- 	if err == nil {
- 		t.Fatalf("expected error for huge slice")
- 	}
- 	if !strings.Contains(err.Error(), "message too big") {
---- src/runtime/crash_cgo_test.go
-+++ src/runtime/crash_cgo_test.go
-@@ -246,20 +246,17 @@ func TestCgoCCodeSIGPROF(t *testing.T) {
- 	got := runTestProg(t, "testprogcgo", "CgoCCodeSIGPROF")
- 	want := "OK\n"
- 	if got != want {
- 		t.Errorf("expected %q got %v", want, got)
- 	}
- }
- 
- func TestCgoCrashTraceback(t *testing.T) {
--	t.Parallel()
--	if runtime.GOOS != "linux" || (runtime.GOARCH != "amd64" && runtime.GOARCH != "ppc64le") {
--		t.Skipf("not yet supported on %s/%s", runtime.GOOS, runtime.GOARCH)
--	}
-+	t.Skipf("skip running remotely")
- 	got := runTestProg(t, "testprogcgo", "CrashTraceback")
- 	for i := 1; i <= 3; i++ {
- 		if !strings.Contains(got, fmt.Sprintf("cgo symbolizer:%d", i)) {
- 			t.Errorf("missing cgo symbolizer:%d", i)
- 		}
- 	}
- }
- 
-@@ -268,20 +265,17 @@ func TestCgoTracebackContext(t *testing.T) {
- 	got := runTestProg(t, "testprogcgo", "TracebackContext")
- 	want := "OK\n"
- 	if got != want {
- 		t.Errorf("expected %q got %v", want, got)
- 	}
- }
- 
- func testCgoPprof(t *testing.T, buildArg, runArg string) {
--	t.Parallel()
--	if runtime.GOOS != "linux" || (runtime.GOARCH != "amd64" && runtime.GOARCH != "ppc64le") {
--		t.Skipf("not yet supported on %s/%s", runtime.GOOS, runtime.GOARCH)
--	}
-+	t.Skipf("skip pprof test")
- 	testenv.MustHaveGoRun(t)
- 
- 	exe, err := buildTestProg(t, "testprogcgo", buildArg)
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 
- 	got, err := testenv.CleanCmdEnv(goExecCmd(exe, runArg)).CombinedOutput()
---- src/runtime/crash_test.go
-+++ src/runtime/crash_test.go
-@@ -476,16 +476,17 @@ func TestPanicDeadlockSyscall(t *testing.T) {
- func TestPanicLoop(t *testing.T) {
- 	output := runTestProg(t, "testprog", "PanicLoop")
- 	if want := "panic while printing panic value"; !strings.Contains(output, want) {
- 		t.Errorf("output does not contain %q:\n%s", want, output)
- 	}
- }
- 
- func TestMemPprof(t *testing.T) {
-+	t.Skipf("skip pprof test")
- 	testenv.MustHaveGoRun(t)
- 
- 	exe, err := buildTestProg(t, "testprog")
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 
- 	got, err := testenv.CleanCmdEnv(goExecCmd(exe, "MemProf")).CombinedOutput()
---- src/runtime/crash_unix_test.go
-+++ src/runtime/crash_unix_test.go
-@@ -169,19 +169,17 @@ func loop(i int, c chan bool) {
- 
- func TestPanicSystemstack(t *testing.T) {
- 	// Test that GOTRACEBACK=crash prints both the system and user
- 	// stack of other threads.
- 
- 	// The GOTRACEBACK=crash handler takes 0.1 seconds even if
- 	// it's not writing a core file and potentially much longer if
- 	// it is. Skip in short mode.
--	if testing.Short() {
--		t.Skip("Skipping in short mode (GOTRACEBACK=crash is slow)")
--	}
-+	t.Skip("Skipping (GOTRACEBACK=crash hangs on arm)")
- 
- 	if runtime.Sigisblocked(int(syscall.SIGQUIT)) {
- 		t.Skip("skipping; SIGQUIT is blocked, see golang.org/issue/19196")
- 	}
- 
- 	t.Parallel()
- 	cmd := exec.Command(os.Args[0], "testPanicSystemstackInternal")
- 	cmd = testenv.CleanCmdEnv(cmd)
-@@ -239,16 +237,17 @@ func init() {
- }
- 
- func testPanicSystemstackInternal() {
- 	runtime.BlockOnSystemStack()
- 	os.Exit(1) // Should be unreachable.
- }
- 
- func TestSignalExitStatus(t *testing.T) {
-+	t.Skipf("skip running remotely")
- 	testenv.MustHaveGoBuild(t)
- 	exe, err := buildTestProg(t, "testprog")
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 	err = testenv.CleanCmdEnv(goExecCmd(exe, "SignalExitStatus")).Run()
- 	if err == nil {
- 		t.Error("test program succeeded unexpectedly")
---- src/runtime/fastlog2_test.go
-+++ src/runtime/fastlog2_test.go
-@@ -11,21 +11,17 @@ import (
- )
- 
- func TestFastLog2(t *testing.T) {
- 	// Compute the euclidean distance between math.Log2 and the FastLog2
- 	// implementation over the range of interest for heap sampling.
- 	const randomBitCount = 26
- 	var e float64
- 
--	inc := 1
--	if testing.Short() {
--		// Check 1K total values, down from 64M.
--		inc = 1 << 16
--	}
-+	inc := 1 << 16
- 	for i := 1; i < 1<<randomBitCount; i += inc {
- 		l, fl := math.Log2(float64(i)), runtime.Fastlog2(float64(i))
- 		d := l - fl
- 		e += d * d
- 	}
- 	e = math.Sqrt(e)
- 
- 	if e > 1.0 {
---- src/runtime/hash_test.go
-+++ src/runtime/hash_test.go
-@@ -156,19 +156,17 @@ func TestSmhasherZeros(t *testing.T) {
- 	for i := 0; i <= N; i++ {
- 		h.addB(b[:i])
- 	}
- 	h.check(t)
- }
- 
- // Strings with up to two nonzero bytes all have distinct hashes.
- func TestSmhasherTwoNonzero(t *testing.T) {
--	if testing.Short() {
--		t.Skip("Skipping in short mode")
--	}
-+	t.Skip("skipping test due to huge memory requirement")
- 	h := newHashSet()
- 	for n := 2; n <= 16; n++ {
- 		twoNonZero(h, n)
- 	}
- 	h.check(t)
- }
- func twoNonZero(h *HashSet, n int) {
- 	b := make([]byte, n)
-@@ -259,19 +257,17 @@ func setbits(h *HashSet, b []byte, i int, k int) {
- 		setbits(h, b, j+1, k-1)
- 		b[j/8] &= byte(^(1 << uint(j&7)))
- 	}
- }
- 
- // Test all possible combinations of n blocks from the set s.
- // "permutation" is a bad name here, but it is what Smhasher uses.
- func TestSmhasherPermutation(t *testing.T) {
--	if testing.Short() {
--		t.Skip("Skipping in short mode")
--	}
-+	t.Skip("skipping test due to huge memory requirement")
- 	permutation(t, []uint32{0, 1, 2, 3, 4, 5, 6, 7}, 8)
- 	permutation(t, []uint32{0, 1 << 29, 2 << 29, 3 << 29, 4 << 29, 5 << 29, 6 << 29, 7 << 29}, 8)
- 	permutation(t, []uint32{0, 1}, 20)
- 	permutation(t, []uint32{0, 1 << 31}, 20)
- 	permutation(t, []uint32{0, 1, 2, 3, 4, 5, 6, 7, 1 << 29, 2 << 29, 3 << 29, 4 << 29, 5 << 29, 6 << 29, 7 << 29}, 6)
- }
- func permutation(t *testing.T, s []uint32, n int) {
- 	b := make([]byte, n*4)
---- src/runtime/pprof/pprof_test.go
-+++ src/runtime/pprof/pprof_test.go
-@@ -278,24 +278,17 @@ func profileOk(t *testing.T, need []string, prof bytes.Buffer, duration time.Dur
- 	return ok
- }
- 
- // Fork can hang if preempted with signals frequently enough (see issue 5517).
- // Ensure that we do not do this.
- func TestCPUProfileWithFork(t *testing.T) {
- 	testenv.MustHaveExec(t)
- 
--	heap := 1 << 30
--	if runtime.GOOS == "android" {
--		// Use smaller size for Android to avoid crash.
--		heap = 100 << 20
--	}
--	if testing.Short() {
--		heap = 100 << 20
--	}
-+	heap := 100 << 20
- 	// This makes fork slower.
- 	garbage := make([]byte, heap)
- 	// Need to touch the slice, otherwise it won't be paged in.
- 	done := make(chan bool)
- 	go func() {
- 		for i := range garbage {
- 			garbage[i] = 42
- 		}
diff --git a/go/patch/go-1.10.3/go0.patch b/go/patch/go-1.10.3/go0.patch
deleted file mode 100644
index f80045c0..00000000
--- a/go/patch/go-1.10.3/go0.patch
+++ /dev/null
@@ -1,27 +0,0 @@
-diff --git src/go/build/deps_test.go src/go/build/deps_test.go
-index 29dbe47d29..53e0e287bc 100644
---- src/go/build/deps_test.go
-+++ src/go/build/deps_test.go
-@@ -191,7 +191,7 @@ var pkgDeps = map[string][]string{
- 	"testing":          {"L2", "flag", "fmt", "internal/race", "os", "runtime/debug", "runtime/pprof", "runtime/trace", "time"},
- 	"testing/iotest":   {"L2", "log"},
- 	"testing/quick":    {"L2", "flag", "fmt", "reflect", "time"},
--	"internal/testenv": {"L2", "OS", "flag", "testing", "syscall"},
-+	"internal/testenv": {"L2", "OS", "os.exec", "flag", "testing", "syscall"},
- 
- 	// L4 is defined as L3+fmt+log+time, because in general once
- 	// you're using L3 packages, use of fmt, log, or time is not a big deal.
-diff --git src/internal/testenv/testenv.go src/internal/testenv/testenv.go
-index 8f69fe0da5..d52b85e122 100644
---- src/internal/testenv/testenv.go
-+++ src/internal/testenv/testenv.go
-@@ -48,6 +48,9 @@ func HasGoBuild() bool {
- 			return false
- 		}
- 	}
-+	if _, err := exec.LookPath("go"); err != nil {
-+	        return false
-+	}
- 	return true
- }
- 
diff --git a/go/patch/go-1.10.3/go1.patch b/go/patch/go-1.10.3/go1.patch
deleted file mode 100644
index e05fcce4..00000000
--- a/go/patch/go-1.10.3/go1.patch
+++ /dev/null
@@ -1,50 +0,0 @@
-diff --git test/chanlinear.go test/chanlinear.go
-index 55fee4ab9b..89533da282 100644
---- test/chanlinear.go
-+++ test/chanlinear.go
-@@ -1,4 +1,4 @@
--// +build darwin linux
-+// +build darwin linux android
- // run
- 
- // Copyright 2014 The Go Authors. All rights reserved.
-diff --git a/test/fixedbugs/bug385_64.go b/test/fixedbugs/bug385_64.go
-index 0f941ca2f4..3bcd62f3ad 100644
---- test/fixedbugs/bug385_64.go
-+++ test/fixedbugs/bug385_64.go
-@@ -1,4 +1,4 @@
--// +build amd64
-+// +build amd64 arm64
- // errorcheck
- 
- // Copyright 2011 The Go Authors. All rights reserved.
-diff --git test/fixedbugs/issue10607.go test/fixedbugs/issue10607.go
-index 8831547da8..9ee6c72bc6 100644
---- test/fixedbugs/issue10607.go
-+++ test/fixedbugs/issue10607.go
-@@ -1,4 +1,4 @@
--// +build linux,!ppc64
-+// +build linux,!ppc64 android
- // run
- 
- // Copyright 2015 The Go Authors. All rights reserved.
-diff --git test/maplinear.go test/maplinear.go
-index 34d0914914..afddab627d 100644
---- test/maplinear.go
-+++ test/maplinear.go
-@@ -1,4 +1,4 @@
--// +build darwin linux
-+// +build darwin linux android
- // run
- 
- // Copyright 2013 The Go Authors. All rights reserved.
-diff --git test/recover4.go test/recover4.go
-index 67ed970ecb..95a89dab00 100644
---- test/recover4.go
-+++ test/recover4.go
-@@ -1,4 +1,4 @@
--// +build linux darwin
-+// +build linux android darwin
- // run
- 
- // Copyright 2015 The Go Authors. All rights reserved.
diff --git a/go/patch/go-1.10.3/go2.patch b/go/patch/go-1.10.3/go2.patch
deleted file mode 100644
index bbd2b744..00000000
--- a/go/patch/go-1.10.3/go2.patch
+++ /dev/null
@@ -1,267 +0,0 @@
-diff --git test/run.go test/run.go
-index 22ec7576f8..ac5d3c3e8d 100644
---- test/run.go
-+++ test/run.go
-@@ -39,9 +39,9 @@ var (
- 	summary        = flag.Bool("summary", false, "show summary of results")
- 	showSkips      = flag.Bool("show_skips", false, "show skipped tests")
- 	runSkips       = flag.Bool("run_skips", false, "run skipped tests (ignore skip and build tags)")
--	linkshared     = flag.Bool("linkshared", false, "")
- 	updateErrors   = flag.Bool("update_errors", false, "update error messages in test file based on compiler output")
- 	runoutputLimit = flag.Int("l", defaultRunOutputLimit(), "number of parallel runoutput tests to run")
-+        target         = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
- 
- 	shard  = flag.Int("shard", 0, "shard index to run. Only applicable if -shards is non-zero.")
- 	shards = flag.Int("shards", 0, "number of shards. If 0, all tests are run. This is used by the continuous build.")
-@@ -194,21 +194,15 @@ func goFiles(dir string) []string {
- type runCmd func(...string) ([]byte, error)
- 
- func compileFile(runcmd runCmd, longname string, flags []string) (out []byte, err error) {
--	cmd := []string{"go", "tool", "compile", "-e"}
-+	cmd := []string{findGoCmd(), "tool", "compile", "-e"}
- 	cmd = append(cmd, flags...)
--	if *linkshared {
--		cmd = append(cmd, "-dynlink", "-installsuffix=dynlink")
--	}
- 	cmd = append(cmd, longname)
- 	return runcmd(cmd...)
- }
- 
- func compileInDir(runcmd runCmd, dir string, flags []string, names ...string) (out []byte, err error) {
--	cmd := []string{"go", "tool", "compile", "-e", "-D", ".", "-I", "."}
-+	cmd := []string{findGoCmd(), "tool", "compile", "-e", "-D", ".", "-I", "."}
- 	cmd = append(cmd, flags...)
--	if *linkshared {
--		cmd = append(cmd, "-dynlink", "-installsuffix=dynlink")
--	}
- 	for _, name := range names {
- 		cmd = append(cmd, filepath.Join(dir, name))
- 	}
-@@ -217,15 +211,24 @@ func compileInDir(runcmd runCmd, dir string, flags []string, names ...string) (o
- 
- func linkFile(runcmd runCmd, goname string) (err error) {
- 	pfile := strings.Replace(goname, ".go", ".o", -1)
--	cmd := []string{"go", "tool", "link", "-w", "-o", "a.exe", "-L", "."}
--	if *linkshared {
--		cmd = append(cmd, "-linkshared", "-installsuffix=dynlink")
--	}
--	cmd = append(cmd, pfile)
--	_, err = runcmd(cmd...)
-+	cmd := []string{findGoCmd, "tool", "link", "-w", "-o", "a.exe", "-L", "."}
-+	_, err = runcmd(findGoCmd(), "tool", "link", "-w", "-o", "a.exe", "-L", ".", pfile)
- 	return
- }
- 
-+
-+func goRun(runcmd runCmd, flags []string, goname string, args ...string) (out []byte, err error) {
-+        cmd := []string{findGoCmd(), "run", goGcflags()}
-+        if len(findExecCmd()) > 0 {
-+                cmd = append(cmd, "-exec")
-+                cmd = append(cmd, findExecCmd()...)
-+        }
-+        cmd = append(cmd, flags...)
-+        cmd = append(cmd, goname)
-+        cmd = append(cmd, args...)
-+        return runcmd(cmd...)
-+}
-+
- // skipError describes why a test was skipped.
- type skipError string
- 
-@@ -595,7 +598,7 @@ func (t *test) run() {
- 
- 	case "errorcheck":
- 		// TODO(gri) remove need for -C (disable printing of columns in error messages)
--		cmdline := []string{"go", "tool", "compile", "-C", "-e", "-o", "a.o"}
-+		cmdline := []string{findGoCmd(), "tool", "compile", "-C", "-e", "-o", "a.o"}
- 		// No need to add -dynlink even if linkshared if we're just checking for errors...
- 		cmdline = append(cmdline, flags...)
- 		cmdline = append(cmdline, long)
-@@ -709,7 +712,7 @@ func (t *test) run() {
- 		}
- 
- 	case "build":
--		_, err := runcmd("go", "build", goGcflags(), "-o", "a.exe", long)
-+		_, err := runcmd(findGoCmd(), "build", goGcflags(), "-o", "a.exe", long)
- 		if err != nil {
- 			t.err = err
- 		}
-@@ -735,7 +738,7 @@ func (t *test) run() {
- 
- 		}
- 		var objs []string
--		cmd := []string{"go", "tool", "compile", "-e", "-D", ".", "-I", ".", "-o", "go.o"}
-+		cmd := []string{findGoCmd(), "tool", "compile", "-e", "-D", ".", "-I", ".", "-o", "go.o"}
- 		if len(asms) > 0 {
- 			cmd = append(cmd, "-asmhdr", "go_asm.h")
- 		}
-@@ -749,7 +752,7 @@ func (t *test) run() {
- 		}
- 		objs = append(objs, "go.o")
- 		if len(asms) > 0 {
--			cmd = []string{"go", "tool", "asm", "-e", "-I", ".", "-o", "asm.o"}
-+			cmd = []string{findGoCmd(), "tool", "asm", "-e", "-I", ".", "-o", "asm.o"}
- 			for _, file := range asms {
- 				cmd = append(cmd, filepath.Join(longdir, file.Name()))
- 			}
-@@ -760,14 +763,14 @@ func (t *test) run() {
- 			}
- 			objs = append(objs, "asm.o")
- 		}
--		cmd = []string{"go", "tool", "pack", "c", "all.a"}
-+		cmd = []string{findGoCmd(), "tool", "pack", "c", "all.a"}
- 		cmd = append(cmd, objs...)
- 		_, err = runcmd(cmd...)
- 		if err != nil {
- 			t.err = err
- 			break
- 		}
--		cmd = []string{"go", "tool", "link", "all.a"}
-+		cmd = []string{findGoCmd(), "tool", "link", "-o", "a.exe", "all.a"}
- 		_, err = runcmd(cmd...)
- 		if err != nil {
- 			t.err = err
-@@ -777,10 +780,7 @@ func (t *test) run() {
- 	case "buildrun": // build binary, then run binary, instead of go run. Useful for timeout tests where failure mode is infinite loop.
- 		// TODO: not supported on NaCl
- 		useTmp = true
--		cmd := []string{"go", "build", goGcflags(), "-o", "a.exe"}
--		if *linkshared {
--			cmd = append(cmd, "-linkshared")
--		}
-+		cmd := []string{findGoCmd(), "build", goGcflags(), "-o", "a.exe"}
- 		longdirgofile := filepath.Join(filepath.Join(cwd, t.dir), t.gofile)
- 		cmd = append(cmd, flags...)
- 		cmd = append(cmd, longdirgofile)
-@@ -789,7 +789,12 @@ func (t *test) run() {
- 			t.err = err
- 			return
- 		}
--		cmd = []string{"./a.exe"}
-+                cmd = []string{}
-+                if len(findExecCmd()) > 0 {
-+                        cmd = append(cmd, findExecCmd()...)
-+                }
-+                cmd = append(cmd, "./a.exe")
-+
- 		out, err = runcmd(append(cmd, args...)...)
- 		if err != nil {
- 			t.err = err
-@@ -802,38 +807,7 @@ func (t *test) run() {
- 
- 	case "run":
- 		useTmp = false
--		var out []byte
--		var err error
--		if len(flags)+len(args) == 0 && goGcflags() == "" && !*linkshared {
--			// If we're not using special go command flags,
--			// skip all the go command machinery.
--			// This avoids any time the go command would
--			// spend checking whether, for example, the installed
--			// package runtime is up to date.
--			// Because we run lots of trivial test programs,
--			// the time adds up.
--			pkg := filepath.Join(t.tempDir, "pkg.a")
--			if _, err := runcmd("go", "tool", "compile", "-o", pkg, t.goFileName()); err != nil {
--				t.err = err
--				return
--			}
--			exe := filepath.Join(t.tempDir, "test.exe")
--			cmd := []string{"go", "tool", "link", "-s", "-w"}
--			cmd = append(cmd, "-o", exe, pkg)
--			if _, err := runcmd(cmd...); err != nil {
--				t.err = err
--				return
--			}
--			out, err = runcmd(append([]string{exe}, args...)...)
--		} else {
--			cmd := []string{"go", "run", goGcflags()}
--			if *linkshared {
--				cmd = append(cmd, "-linkshared")
--			}
--			cmd = append(cmd, flags...)
--			cmd = append(cmd, t.goFileName())
--			out, err = runcmd(append(cmd, args...)...)
--		}
-+		out, err := goRun(runcmd, flags, t.goFileName(), args...)
- 		if err != nil {
- 			t.err = err
- 			return
-@@ -848,12 +822,7 @@ func (t *test) run() {
- 			<-rungatec
- 		}()
- 		useTmp = false
--		cmd := []string{"go", "run", goGcflags()}
--		if *linkshared {
--			cmd = append(cmd, "-linkshared")
--		}
--		cmd = append(cmd, t.goFileName())
--		out, err := runcmd(append(cmd, args...)...)
-+		out, err := goRun(runcmd, nil, t.goFileName(), args...)
- 		if err != nil {
- 			t.err = err
- 			return
-@@ -863,12 +832,7 @@ func (t *test) run() {
- 			t.err = fmt.Errorf("write tempfile:%s", err)
- 			return
- 		}
--		cmd = []string{"go", "run", goGcflags()}
--		if *linkshared {
--			cmd = append(cmd, "-linkshared")
--		}
--		cmd = append(cmd, tfile)
--		out, err = runcmd(cmd...)
-+		out, err = goRun(runcmd, nil, tfile)
- 		if err != nil {
- 			t.err = err
- 			return
-@@ -879,12 +843,7 @@ func (t *test) run() {
- 
- 	case "errorcheckoutput":
- 		useTmp = false
--		cmd := []string{"go", "run", goGcflags()}
--		if *linkshared {
--			cmd = append(cmd, "-linkshared")
--		}
--		cmd = append(cmd, t.goFileName())
--		out, err := runcmd(append(cmd, args...)...)
-+		out, err := goRun(runcmd, nil, t.goFileName(), args...)
- 		if err != nil {
- 			t.err = err
- 			return
-@@ -895,7 +854,7 @@ func (t *test) run() {
- 			t.err = fmt.Errorf("write tempfile:%s", err)
- 			return
- 		}
--		cmdline := []string{"go", "tool", "compile", "-e", "-o", "a.o"}
-+		cmdline := []string{findGoCmd(), "tool", "compile", "-e", "-o", "a.o"}
- 		cmdline = append(cmdline, flags...)
- 		cmdline = append(cmdline, tfile)
- 		out, err = runcmd(cmdline...)
-@@ -922,6 +881,11 @@ func findExecCmd() []string {
- 		return execCmd
- 	}
- 	execCmd = []string{} // avoid work the second time
-+        if *target != "" {
-+                execCmd = []string{"go_" + *target + "_exec"}
-+                return execCmd
-+        }
-+
- 	if goos == runtime.GOOS && goarch == runtime.GOARCH {
- 		return execCmd
- 	}
-@@ -932,6 +896,14 @@ func findExecCmd() []string {
- 	return execCmd
- }
- 
-+func findGoCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
-+
- func (t *test) String() string {
- 	return filepath.Join(t.dir, t.gofile)
- }
diff --git a/go/patch/go-1.10.3/go3.patch b/go/patch/go-1.10.3/go3.patch
deleted file mode 100644
index 223ccb85..00000000
--- a/go/patch/go-1.10.3/go3.patch
+++ /dev/null
@@ -1,732 +0,0 @@
-diff --git test/fixedbugs/bug302.go test/fixedbugs/bug302.go
-index e4de25d5d0..ea566e6e44 100644
---- test/fixedbugs/bug302.go
-+++ test/fixedbugs/bug302.go
-@@ -1,5 +1,5 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2010 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -8,16 +8,27 @@
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
- func main() {
--	run("go", "tool", "compile", filepath.Join("fixedbugs", "bug302.dir", "p.go"))
--	run("go", "tool", "pack", "grc", "pp.a", "p.o")
--	run("go", "tool", "compile", "-I", ".", filepath.Join("fixedbugs", "bug302.dir", "main.go"))
-+	flag.Parse()
-+	run(goCmd(), "tool", "compile", filepath.Join("fixedbugs", "bug302.dir", "p.go"))
-+	run(goCmd(), "tool", "pack", "grc", "pp.a", "p.o")
-+	run(goCmd(), "tool", "compile", "-I", ".", filepath.Join("fixedbugs", "bug302.dir", "main.go"))
- 	os.Remove("p.o")
- 	os.Remove("pp.a")
- 	os.Remove("main.o")
-diff --git test/fixedbugs/bug369.go test/fixedbugs/bug369.go
-index 60162ab1cb..4470d5a076 100644
---- test/fixedbugs/bug369.go
-+++ test/fixedbugs/bug369.go
-@@ -1,5 +1,5 @@
- // +build !nacl,!windows
--// run
-+// runtarget
- 
- // Copyright 2011 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -10,21 +10,40 @@
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+       if *target != "" {
-+               return "go_" + *target
-+       }
-+       return "go"
-+}
-+
-+func goRun(cmd ...string) {
-+       if *target == "" {
-+               run(cmd[0], cmd[1:]...)
-+       } else {
-+               run("go_"+*target+"_exec", cmd...)
-+       }
-+}
-+
- func main() {
-+	flag.Parse()
- 	err := os.Chdir(filepath.Join(".", "fixedbugs", "bug369.dir"))
- 	check(err)
- 
--	run("go", "tool", "compile", "-N", "-o", "slow.o", "pkg.go")
--	run("go", "tool", "compile", "-o", "fast.o", "pkg.go")
--	run("go", "tool", "compile", "-o", "main.o", "main.go")
--	run("go", "tool", "link", "-o", "a.exe", "main.o")
--	run("." + string(filepath.Separator) + "a.exe")
-+	run(goCmd(), "tool", "compile", "-N", "-o", "slow.o", "pkg.go")
-+	run(goCmd(), "tool", "compile", "-o", "fast.o", "pkg.go")
-+	run(goCmd(), "tool", "compile", "-o", "main.o", "main.go")
-+	run(goCmd(), "tool", "link", "-o", "a.exe", "main.o")
-+	goRun("." + string(filepath.Separator) + "a.exe")
- 
- 	os.Remove("slow.o")
- 	os.Remove("fast.o")
-diff --git test/fixedbugs/bug429_run.go test/fixedbugs/bug429_run.go
-index 284033d1f7..e8d18b13e8 100644
---- test/fixedbugs/bug429_run.go
-+++ test/fixedbugs/bug429_run.go
-@@ -1,5 +1,5 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -10,6 +10,7 @@
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
-@@ -17,8 +18,27 @@ import (
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+       if *target != "" {
-+               return "go_" + *target
-+       }
-+       return "go"
-+}
-+
-+func goRun(args ...string) *exec.Cmd {
-+       cmd := []string{"run"}
-+       if *target != "" {
-+               cmd = append(cmd, "-exec", "go_"+*target+"_exec")
-+       }
-+       cmd = append(cmd, args...)
-+       return exec.Command(goCmd(), cmd...)
-+}
-+
- func main() {
--	cmd := exec.Command("go", "run", filepath.Join("fixedbugs", "bug429.go"))
-+	flag.Parse()
-+	cmd := goRun(filepath.Join("fixedbugs", "bug429.go"))
- 	out, err := cmd.CombinedOutput()
- 	if err == nil {
- 		fmt.Println("expected deadlock")
-diff --git test/fixedbugs/issue10607.go test/fixedbugs/issue10607.go
-index 9ee6c72bc6..e819a3085a 100644
---- test/fixedbugs/issue10607.go
-+++ test/fixedbugs/issue10607.go
-@@ -1,5 +1,5 @@
- // +build linux,!ppc64 android
--// run
-+// runtarget
- 
- // Copyright 2015 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -11,19 +11,39 @@
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+       if *target != "" {
-+               return "go_" + *target
-+       }
-+       return "go"
-+}
-+
-+func goRun(args ...string) *exec.Cmd {
-+       cmd := []string{"run"}
-+       if *target != "" {
-+               cmd = append(cmd, "-exec", "go_"+*target+"_exec")
-+       }
-+       cmd = append(cmd, args...)
-+       return exec.Command(goCmd(), cmd...)
-+}
-+
- func main() {
--	test("internal")
-+	flag.Parse()
-+	//test("internal")
- 	test("external")
- }
- 
- func test(linkmode string) {
--	out, err := exec.Command("go", "run", "-ldflags", "-B=0x12345678 -linkmode="+linkmode, filepath.Join("fixedbugs", "issue10607a.go")).CombinedOutput()
-+	out, err := goRun("-ldflags", "-B=0x12345678 -linkmode="+linkmode, filepath.Join("fixedbugs", "issue10607a.go")).CombinedOutput()
- 	if err != nil {
- 		fmt.Printf("BUG: linkmode=%s %v\n%s\n", linkmode, err, out)
- 		os.Exit(1)
-diff --git test/fixedbugs/issue11771.go test/fixedbugs/issue11771.go
-index d91fc5d966..4f55ce6982 100644
---- test/fixedbugs/issue11771.go
-+++ test/fixedbugs/issue11771.go
-@@ -1,5 +1,5 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2015 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -11,6 +11,7 @@ package main
- 
- import (
- 	"bytes"
-+	"flag"
- 	"fmt"
- 	"io/ioutil"
- 	"log"
-@@ -20,7 +21,17 @@ import (
- 	"runtime"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+       if *target != "" {
-+               return "go_" + *target
-+       }
-+       return "go"
-+}
-+
- func main() {
-+	flag.Parse()
- 	if runtime.Compiler != "gc" {
- 		return
- 	}
-@@ -52,7 +63,7 @@ func x() {
- 		log.Fatal(err)
- 	}
- 
--	cmd := exec.Command("go", "tool", "compile", "x.go")
-+	cmd := exec.Command(goCmd(), "tool", "compile", "x.go")
- 	cmd.Dir = dir
- 	output, err := cmd.CombinedOutput()
- 	if err == nil {
-diff --git test/fixedbugs/issue9355.go test/fixedbugs/issue9355.go
-index 10f8c73069..87356c7402 100644
---- test/fixedbugs/issue9355.go
-+++ test/fixedbugs/issue9355.go
-@@ -1,4 +1,4 @@
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -7,6 +7,7 @@
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
-@@ -15,7 +16,17 @@ import (
- 	"runtime"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
- func main() {
-+	flag.Parse()
- 	if runtime.Compiler != "gc" || runtime.GOOS == "nacl" {
- 		return
- 	}
-@@ -23,7 +34,7 @@ func main() {
- 	err := os.Chdir(filepath.Join("fixedbugs", "issue9355.dir"))
- 	check(err)
- 
--	out := run("go", "tool", "compile", "-S", "a.go")
-+	out := run(goCmd(), "tool", "compile", "-S", "a.go")
- 	os.Remove("a.o")
- 
- 	// 6g/8g print the offset as dec, but 5g/9g print the offset as hex.
-diff --git test/fixedbugs/issue9862_run.go test/fixedbugs/issue9862_run.go
-index be22f40580..a72a59fda2 100644
---- test/fixedbugs/issue9862_run.go
-+++ test/fixedbugs/issue9862_run.go
-@@ -1,5 +1,5 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2015 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -10,12 +10,32 @@
- package main
- 
- import (
-+	"flag"
- 	"os/exec"
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
-+func goRun(args ...string) *exec.Cmd {
-+        cmd := []string{"run"}
-+        if *target != "" {
-+                cmd = append(cmd, "-exec", "go_"+*target+"_exec")
-+        }
-+        cmd = append(cmd, args...)
-+        return exec.Command(goCmd(), cmd...)
-+}
-+
- func main() {
--	out, err := exec.Command("go", "run", "fixedbugs/issue9862.go").CombinedOutput()
-+	flag.Parse()
-+	out, err := goRun("fixedbugs/issue9862.go").CombinedOutput()
- 	outstr := string(out)
- 	if err == nil {
- 		println("go run issue9862.go succeeded, should have failed\n", outstr)
-diff --git test/linkmain_run.go test/linkmain_run.go
-index 55de481a81..03666e6b29 100644
---- test/linkmain_run.go
-+++ test/linkmain_run.go
-@@ -1,5 +1,5 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -10,12 +10,22 @@
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
- func cleanup() {
- 	os.Remove("linkmain.o")
- 	os.Remove("linkmain.a")
-@@ -51,16 +61,17 @@ func runFail(cmdline string) {
- }
- 
- func main() {
-+	flag.Parse()
- 	// helloworld.go is package main
--	run("go tool compile -o linkmain.o helloworld.go")
--	run("go tool compile -pack -o linkmain.a helloworld.go")
--	run("go tool link -o linkmain.exe linkmain.o")
--	run("go tool link -o linkmain.exe linkmain.a")
-+	run(goCmd() + " tool compile -o linkmain.o helloworld.go")
-+	run(goCmd() + " tool compile -pack -o linkmain.a helloworld.go")
-+	run(goCmd() + " tool link -o linkmain.exe linkmain.o")
-+	run(goCmd() + " tool link -o linkmain.exe linkmain.a")
- 
- 	// linkmain.go is not
--	run("go tool compile -o linkmain1.o linkmain.go")
--	run("go tool compile -pack -o linkmain1.a linkmain.go")
--	runFail("go tool link -o linkmain.exe linkmain1.o")
--	runFail("go tool link -o linkmain.exe linkmain1.a")
-+	run(goCmd() + " tool compile -o linkmain1.o linkmain.go")
-+	run(goCmd() + " tool compile -pack -o linkmain1.a linkmain.go")
-+	runFail(goCmd() + " tool link -o linkmain.exe linkmain1.o")
-+	runFail(goCmd() + " tool link -o linkmain.exe linkmain1.a")
- 	cleanup()
- }
-diff --git test/linkobj.go test/linkobj.go
-index 8a86aa872f..0d1964e7fb 100644
---- test/linkobj.go
-+++ test/linkobj.go
-@@ -1,5 +1,5 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2016 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -10,6 +10,7 @@
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"io/ioutil"
- 	"log"
-@@ -18,9 +19,27 @@ import (
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
-+func goRun(cmd ...string) string {
-+        if *target == "" {
-+                return run(cmd...)
-+        } else {
-+                return run(append([]string{"go_"+*target+"_exec"}, cmd...)...)
-+        }
-+}
-+
- var pwd, tmpdir string
- 
- func main() {
-+	flag.Parse()
- 	dir, err := ioutil.TempDir("", "go-test-linkobj-")
- 	if err != nil {
- 		log.Fatal(err)
-@@ -37,28 +56,28 @@ func main() {
- 
- 	writeFile("p1.go", `
- 		package p1
--		
-+
- 		func F() {
- 			println("hello from p1")
- 		}
- 	`)
- 	writeFile("p2.go", `
- 		package p2
--		
-+
- 		import "./p1"
- 
- 		func F() {
- 			p1.F()
- 			println("hello from p2")
- 		}
--		
-+
- 		func main() {}
- 	`)
- 	writeFile("p3.go", `
- 		package main
- 
- 		import "./p2"
--		
-+
- 		func main() {
- 			p2.F()
- 			println("hello from main")
-@@ -76,9 +95,9 @@ func main() {
- 		}
- 
- 		// inlining is disabled to make sure that the link objects contain needed code.
--		run("go", "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p1."+o, "-linkobj", "p1.lo", "p1.go")
--		run("go", "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p2."+o, "-linkobj", "p2.lo", "p2.go")
--		run("go", "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p3."+o, "-linkobj", "p3.lo", "p3.go")
-+		run(goCmd(), "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p1."+o, "-linkobj", "p1.lo", "p1.go")
-+		run(goCmd(), "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p2."+o, "-linkobj", "p2.lo", "p2.go")
-+		run(goCmd(), "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p3."+o, "-linkobj", "p3.lo", "p3.go")
- 
- 		cp("p1."+o, "p1.oo")
- 		cp("p2."+o, "p2.oo")
-@@ -86,13 +105,13 @@ func main() {
- 		cp("p1.lo", "p1."+o)
- 		cp("p2.lo", "p2."+o)
- 		cp("p3.lo", "p3."+o)
--		out := runFail("go", "tool", "link", "p2."+o)
-+		out := runFail(goCmd(), "tool", "link", "p2."+o)
- 		if !strings.Contains(out, "not package main") {
- 			fatalf("link p2.o failed but not for package main:\n%s", out)
- 		}
- 
--		run("go", "tool", "link", "-L", ".", "-o", "a.out.exe", "p3."+o)
--		out = run("./a.out.exe")
-+		run(goCmd(), "tool", "link", "-L", ".", "-o", "a.out.exe", "p3."+o)
-+		out = goRun("./a.out.exe")
- 		if !strings.Contains(out, "hello from p1\nhello from p2\nhello from main\n") {
- 			fatalf("running main, incorrect output:\n%s", out)
- 		}
-diff --git test/linkx_run.go test/linkx_run.go
-index cc249c9cfc..530159ab9d 100644
---- test/linkx_run.go
-+++ test/linkx_run.go
-@@ -1,5 +1,5 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -11,20 +11,40 @@ package main
- 
- import (
- 	"bytes"
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
-+func goRun(args ...string) *exec.Cmd {
-+        cmd := []string{"run"}
-+        if *target != "" {
-+                cmd = append(cmd, "-exec", "go_"+*target+"_exec")
-+        }
-+        cmd = append(cmd, args...)
-+        return exec.Command(goCmd(), cmd...)
-+}
-+
- func main() {
-+	flag.Parse()
- 	// test(" ") // old deprecated & removed syntax
- 	test("=") // new syntax
- }
- 
- func test(sep string) {
- 	// Successful run
--	cmd := exec.Command("go", "run", "-ldflags=-X main.tbd"+sep+"hello -X main.overwrite"+sep+"trumped -X main.nosuchsymbol"+sep+"neverseen", "linkx.go")
-+	cmd := goRun("-ldflags=-X main.tbd"+sep+"hello -X main.overwrite"+sep+"trumped -X main.nosuchsymbol"+sep+"neverseen", "linkx.go")
- 	var out, errbuf bytes.Buffer
- 	cmd.Stdout = &out
- 	cmd.Stderr = &errbuf
-@@ -44,7 +64,7 @@ func test(sep string) {
- 	}
- 
- 	// Issue 8810
--	cmd = exec.Command("go", "run", "-ldflags=-X main.tbd", "linkx.go")
-+	cmd = goRun("-ldflags=-X main.tbd", "linkx.go")
- 	_, err = cmd.CombinedOutput()
- 	if err == nil {
- 		fmt.Println("-X linker flag should not accept keys without values")
-@@ -52,7 +72,7 @@ func test(sep string) {
- 	}
- 
- 	// Issue 9621
--	cmd = exec.Command("go", "run", "-ldflags=-X main.b=false -X main.x=42", "linkx.go")
-+	cmd = goRun("-ldflags=-X main.b=false -X main.x=42", "linkx.go")
- 	outx, err := cmd.CombinedOutput()
- 	if err == nil {
- 		fmt.Println("-X linker flag should not overwrite non-strings")
-diff --git test/nosplit.go test/nosplit.go
-index e6cecebde3..fed1c0e510 100644
---- test/nosplit.go
-+++ test/nosplit.go
-@@ -1,5 +1,5 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -9,6 +9,7 @@ package main
- 
- import (
- 	"bytes"
-+	"flag"
- 	"fmt"
- 	"io/ioutil"
- 	"log"
-@@ -21,6 +22,24 @@ import (
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
-+func goArch() string {
-+        goarch, err := exec.Command(goCmd(), "env", "GOARCH").Output()
-+        if err != nil {
-+                bug()
-+                fmt.Printf("running go env GOARCH: %v\n", err)
-+        }
-+        return strings.TrimSpace(string(goarch))
-+}
-+
- var tests = `
- # These are test cases for the linker analysis that detects chains of
- # nosplit functions that would cause a stack overflow.
-@@ -194,12 +213,13 @@ var (
- )
- 
- func main() {
--	goarch := os.Getenv("GOARCH")
-+	flag.Parse()
-+	goarch := goArch()
- 	if goarch == "" {
--		goarch = runtime.GOARCH
-+		return
- 	}
- 
--	version, err := exec.Command("go", "tool", "compile", "-V").Output()
-+	version, err := exec.Command(goCmd(), "tool", "compile", "-V").Output()
- 	if err != nil {
- 		bug()
- 		fmt.Printf("running go tool compile -V: %v\n", err)
-@@ -345,7 +365,7 @@ TestCases:
- 			log.Fatal(err)
- 		}
- 
--		cmd := exec.Command("go", "build")
-+		cmd := exec.Command(goCmd(), "build")
- 		cmd.Dir = dir
- 		output, err := cmd.CombinedOutput()
- 		if err == nil {
-diff --git test/run.go test/run.go
-index ac5d3c3e8d..62041226b0 100644
---- test/run.go
-+++ test/run.go
-@@ -229,6 +229,16 @@ func goRun(runcmd runCmd, flags []string, goname string, args ...string) (out []
-         return runcmd(cmd...)
- }
- 
-+func goRunTarget(runcmd runCmd, goname string, args ...string) (out []byte, err error) {
-+        cmd := []string{"go_local", "run"}
-+        cmd = append(cmd, goname)
-+        if *target != "" {
-+                cmd = append(cmd, "-target", *target)
-+        }
-+        cmd = append(cmd, args...)
-+        return runcmd(cmd...)
-+}
-+
- // skipError describes why a test was skipped.
- type skipError string
- 
-@@ -491,7 +501,7 @@ func (t *test) run() {
- 		action = "rundir"
- 	case "cmpout":
- 		action = "run" // the run case already looks for <dir>/<test>.out files
--	case "compile", "compiledir", "build", "builddir", "run", "buildrun", "runoutput", "rundir":
-+	case "compile", "compiledir", "build", "builddir", "run", "runtarget", "buildrun", "runoutput", "rundir":
- 		// nothing to do
- 	case "errorcheckandrundir":
- 		wantError = false // should be no error if also will run
-@@ -816,6 +826,17 @@ func (t *test) run() {
- 			t.err = fmt.Errorf("incorrect output\n%s", out)
- 		}
- 
-+       case "runtarget":
-+                useTmp = false
-+                out, err := goRunTarget(runcmd, t.goFileName(), args...)
-+                if err != nil {
-+                        t.err = err
-+                        return
-+                }
-+                if strings.Replace(string(out), "\r\n", "\n", -1) != t.expectedOutput() {
-+                        t.err = fmt.Errorf("incorrect output\n%s", out)
-+                }
-+
- 	case "runoutput":
- 		rungatec <- true
- 		defer func() {
-diff --git test/sinit_run.go test/sinit_run.go
-index c9afd3b777..dc885ecffd 100644
---- test/sinit_run.go
-+++ test/sinit_run.go
-@@ -1,5 +1,5 @@
- // +build !nacl
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -11,13 +11,24 @@ package main
- 
- import (
- 	"bytes"
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+       if *target != "" {
-+               return "go_" + *target
-+       }
-+       return "go"
-+}
-+
- func main() {
--	cmd := exec.Command("go", "tool", "compile", "-S", "sinit.go")
-+	flag.Parse()
-+	cmd := exec.Command(goCmd(), "tool", "compile", "-S", "sinit.go")
- 	out, err := cmd.CombinedOutput()
- 	if err != nil {
- 		fmt.Println(string(out))
diff --git a/go/patch/go-1.10.3/go4.patch b/go/patch/go-1.10.3/go4.patch
deleted file mode 100644
index 290de390..00000000
--- a/go/patch/go-1.10.3/go4.patch
+++ /dev/null
@@ -1,199 +0,0 @@
-runtime, crypto/x509: add -target flag.
-
---- src/crypto/x509/x509_test.go
-+++ src/crypto/x509/x509_test.go
-@@ -13,29 +13,32 @@ import (
- 	"crypto/rsa"
- 	_ "crypto/sha256"
- 	_ "crypto/sha512"
- 	"crypto/x509/pkix"
- 	"encoding/asn1"
- 	"encoding/base64"
- 	"encoding/hex"
- 	"encoding/pem"
-+	"flag"
- 	"fmt"
- 	"internal/testenv"
- 	"math/big"
- 	"net"
- 	"net/url"
- 	"os/exec"
- 	"reflect"
- 	"runtime"
- 	"strings"
- 	"testing"
- 	"time"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
- func TestParsePKCS1PrivateKey(t *testing.T) {
- 	block, _ := pem.Decode([]byte(pemPrivateKey))
- 	priv, err := ParsePKCS1PrivateKey(block.Bytes)
- 	if err != nil {
- 		t.Errorf("Failed to parse private key: %s", err)
- 		return
- 	}
- 	if priv.PublicKey.N.Cmp(rsaPrivateKey.PublicKey.N) != 0 ||
-@@ -1089,17 +1092,23 @@ func TestParsePEMCRL(t *testing.T) {
- 	}
- 
- 	// Can't check the signature here without a package cycle.
- }
- 
- func TestImports(t *testing.T) {
- 	testenv.MustHaveGoRun(t)
- 
--	if err := exec.Command(testenv.GoToolPath(t), "run", "x509_test_import.go").Run(); err != nil {
-+	var cmd *exec.Cmd
-+	if *target == "" {
-+		cmd = exec.Command(testenv.GoToolPath(t), "run", "x509_test_import.go")
-+	} else {
-+		cmd = exec.Command("go_"+*target, "run", "-exec", "go_"+*target+"_exec", "x509_test_import.go")
-+	}
-+	if err := cmd.Run(); err != nil {
- 		t.Errorf("failed to run x509_test_import.go: %s", err)
- 	}
- }
- 
- const derCRLBase64 = "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"
- 
- const pemCRLBase64 = "LS0tLS1CRUdJTiBYNTA5IENSTC0tLS0tDQpNSUlCOWpDQ0FWOENBUUV3RFFZSktvWklodmNOQVFFRkJRQXdiREVhTUJnR0ExVUVDaE1SVWxOQklGTmxZM1Z5DQphWFI1SUVsdVl5NHhIakFjQmdOVkJBTVRGVkpUUVNCUWRXSnNhV01nVW05dmRDQkRRU0IyTVRFdU1Dd0dDU3FHDQpTSWIzRFFFSkFSWWZjbk5oYTJWdmJuSnZiM1J6YVdkdVFISnpZWE5sWTNWeWFYUjVMbU52YlJjTk1URXdNakl6DQpNVGt5T0RNd1doY05NVEV3T0RJeU1Ua3lPRE13V2pDQmpEQktBaEVBckRxb2g5RkhKSFhUN09QZ3V1bjQrQmNODQpNRGt4TVRBeU1UUXlOekE1V2pBbU1Bb0dBMVVkRlFRRENnRUpNQmdHQTFVZEdBUVJHQTh5TURBNU1URXdNakUwDQpNalExTlZvd1BnSVJBTEd6blowOTVQQjVhQU9MUGc1N2ZNTVhEVEF5TVRBeU16RTBOVEF4TkZvd0dqQVlCZ05WDQpIUmdFRVJnUE1qQXdNakV3TWpNeE5EVXdNVFJhb0RBd0xqQWZCZ05WSFNNRUdEQVdnQlQxVERGNlVRTS9MTmVMDQpsNWx2cUhHUXEzZzltekFMQmdOVkhSUUVCQUlDQUlRd0RRWUpLb1pJaHZjTkFRRUZCUUFEZ1lFQUZVNUFzNk16DQpxNVBSc2lmYW9iUVBHaDFhSkx5QytNczVBZ2MwYld5QTNHQWR4dXI1U3BQWmVSV0NCamlQL01FSEJXSkNsQkhQDQpHUmNxNXlJZDNFakRrYUV5eFJhK2k2N0x6dmhJNmMyOUVlNks5cFNZd2ppLzdSVWhtbW5Qclh0VHhsTDBsckxyDQptUVFKNnhoRFJhNUczUUE0Q21VZHNITnZicnpnbUNZcHZWRT0NCi0tLS0tRU5EIFg1MDkgQ1JMLS0tLS0NCg0K"
- 
---- src/runtime/crash_cgo_test.go
-+++ src/runtime/crash_cgo_test.go
-@@ -279,17 +279,17 @@ func testCgoPprof(t *testing.T, buildArg, runArg string) {
- 	}
- 	testenv.MustHaveGoRun(t)
- 
- 	exe, err := buildTestProg(t, "testprogcgo", buildArg)
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 
--	got, err := testenv.CleanCmdEnv(exec.Command(exe, runArg)).CombinedOutput()
-+	got, err := testenv.CleanCmdEnv(goExecCmd(exe, runArg)).CombinedOutput()
- 	if err != nil {
- 		if testenv.Builder() == "linux-amd64-alpine" {
- 			// See Issue 18243 and Issue 19938.
- 			t.Skipf("Skipping failing test on Alpine (golang.org/issue/18243). Ignoring error: %v", err)
- 		}
- 		t.Fatal(err)
- 	}
- 	fn := strings.TrimSpace(string(got))
---- src/runtime/crash_test.go
-+++ src/runtime/crash_test.go
-@@ -17,16 +17,35 @@ import (
- 	"runtime"
- 	"strconv"
- 	"strings"
- 	"sync"
- 	"testing"
- 	"time"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd(t *testing.T) string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return testenv.GoToolPath(t)
-+}
-+
-+func goExecCmd(name string, arg ...string) *exec.Cmd {
-+	var cmd []string
-+	if *target != "" {
-+		cmd = append(cmd, "go_"+*target+"_exec")
-+	}
-+	cmd = append(cmd, name)
-+	cmd = append(cmd, arg...)
-+	return exec.Command(cmd[0], cmd[1:]...)
-+}
-+
- var toRemove []string
- 
- func TestMain(m *testing.M) {
- 	status := m.Run()
- 	for _, file := range toRemove {
- 		os.RemoveAll(file)
- 	}
- 	os.Exit(status)
-@@ -50,17 +69,17 @@ func runTestProg(t *testing.T, binary, name string, env ...string) string {
- 
- 	testenv.MustHaveGoBuild(t)
- 
- 	exe, err := buildTestProg(t, binary)
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 
--	cmd := testenv.CleanCmdEnv(exec.Command(exe, name))
-+	cmd := testenv.CleanCmdEnv(goExecCmd(exe, name))
- 	cmd.Env = append(cmd.Env, env...)
- 	if testing.Short() {
- 		cmd.Env = append(cmd.Env, "RUNTIME_TEST_SHORT=1")
- 	}
- 	var b bytes.Buffer
- 	cmd.Stdout = &b
- 	cmd.Stderr = &b
- 	if err := cmd.Start(); err != nil {
-@@ -125,17 +144,17 @@ func buildTestProg(t *testing.T, binary string, flags ...string) (string, error)
- 		name += "_" + strings.Join(flags, "_")
- 	}
- 	target, ok := testprog.target[name]
- 	if ok {
- 		return target.exe, target.err
- 	}
- 
- 	exe := filepath.Join(testprog.dir, name+".exe")
--	cmd := exec.Command(testenv.GoToolPath(t), append([]string{"build", "-o", exe}, flags...)...)
-+	cmd := exec.Command(goCmd(t), append([]string{"build", "-o", exe}, flags...)...)
- 	cmd.Dir = "testdata/" + binary
- 	out, err := testenv.CleanCmdEnv(cmd).CombinedOutput()
- 	if err != nil {
- 		target.err = fmt.Errorf("building %s %v: %v\n%s", binary, flags, err, out)
- 		testprog.target[name] = target
- 		return "", target.err
- 	}
- 	target.exe = exe
-@@ -456,17 +475,17 @@ func TestPanicLoop(t *testing.T) {
- func TestMemPprof(t *testing.T) {
- 	testenv.MustHaveGoRun(t)
- 
- 	exe, err := buildTestProg(t, "testprog")
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 
--	got, err := testenv.CleanCmdEnv(exec.Command(exe, "MemProf")).CombinedOutput()
-+	got, err := testenv.CleanCmdEnv(goExecCmd(exe, "MemProf")).CombinedOutput()
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 	fn := strings.TrimSpace(string(got))
- 	defer os.Remove(fn)
- 
- 	for try := 0; try < 2; try++ {
- 		cmd := testenv.CleanCmdEnv(exec.Command(testenv.GoToolPath(t), "tool", "pprof", "-alloc_space", "-top"))
---- src/runtime/crash_unix_test.go
-+++ src/runtime/crash_unix_test.go
-@@ -244,17 +244,17 @@ func testPanicSystemstackInternal() {
- }
- 
- func TestSignalExitStatus(t *testing.T) {
- 	testenv.MustHaveGoBuild(t)
- 	exe, err := buildTestProg(t, "testprog")
- 	if err != nil {
- 		t.Fatal(err)
- 	}
--	err = testenv.CleanCmdEnv(exec.Command(exe, "SignalExitStatus")).Run()
-+	err = testenv.CleanCmdEnv(goExecCmd(exe, "SignalExitStatus")).Run()
- 	if err == nil {
- 		t.Error("test program succeeded unexpectedly")
- 	} else if ee, ok := err.(*exec.ExitError); !ok {
- 		t.Errorf("error (%v) has type %T; expected exec.ExitError", err, err)
- 	} else if ws, ok := ee.Sys().(syscall.WaitStatus); !ok {
- 		t.Errorf("error.Sys (%v) has type %T; expected syscall.WaitStatus", ee.Sys(), ee.Sys())
- 	} else if !ws.Signaled() || ws.Signal() != syscall.SIGTERM {
- 		t.Errorf("got %v; expected SIGTERM", ee)
diff --git a/go/patch/go-1.10.3/go5.patch b/go/patch/go-1.10.3/go5.patch
deleted file mode 100644
index 658b0346..00000000
--- a/go/patch/go-1.10.3/go5.patch
+++ /dev/null
@@ -1,106 +0,0 @@
-diff --git src/runtime/crash_test.go src/runtime/crash_test.go
-index e34f0fa95f..ea1eb4150a 100644
---- src/runtime/crash_test.go
-+++ src/runtime/crash_test.go
-@@ -219,22 +219,27 @@ func testDeadlock(t *testing.T, name string) {
- }
- 
- func TestSimpleDeadlock(t *testing.T) {
-+        t.Skip("deadlock detection fails with external linker")
- 	testDeadlock(t, "SimpleDeadlock")
- }
- 
- func TestInitDeadlock(t *testing.T) {
-+        t.Skip("deadlock detection fails with external linker")
- 	testDeadlock(t, "InitDeadlock")
- }
- 
- func TestLockedDeadlock(t *testing.T) {
-+        t.Skip("deadlock detection fails with external linker")
- 	testDeadlock(t, "LockedDeadlock")
- }
- 
- func TestLockedDeadlock2(t *testing.T) {
-+        t.Skip("deadlock detection fails with external linker")
- 	testDeadlock(t, "LockedDeadlock2")
- }
- 
- func TestGoexitDeadlock(t *testing.T) {
-+        t.Skip("deadlock detection fails with external linker")
- 	output := runTestProg(t, "testprog", "GoexitDeadlock")
- 	want := "no goroutines (main called runtime.Goexit) - deadlock!"
- 	if !strings.Contains(output, want) {
-@@ -271,6 +276,7 @@ panic: again
- }
- 
- func TestGoexitCrash(t *testing.T) {
-+        t.Skip("deadlock detection fails with external linker")
- 	output := runTestProg(t, "testprog", "GoexitExit")
- 	want := "no goroutines (main called runtime.Goexit) - deadlock!"
- 	if !strings.Contains(output, want) {
-@@ -329,6 +335,7 @@ func TestBreakpoint(t *testing.T) {
- }
- 
- func TestGoexitInPanic(t *testing.T) {
-+        t.Skip("deadlock detection fails with external linker")
- 	// see issue 8774: this code used to trigger an infinite recursion
- 	output := runTestProg(t, "testprog", "GoexitInPanic")
- 	want := "fatal error: no goroutines (main called runtime.Goexit) - deadlock!"
-@@ -393,6 +400,7 @@ func TestPanicAfterGoexit(t *testing.T) {
- }
- 
- func TestRecoveredPanicAfterGoexit(t *testing.T) {
-+        t.Skip("deadlock detection fails with external linker")
- 	output := runTestProg(t, "testprog", "RecoveredPanicAfterGoexit")
- 	want := "fatal error: no goroutines (main called runtime.Goexit) - deadlock!"
- 	if !strings.HasPrefix(output, want) {
-diff --git src/runtime/proc_test.go src/runtime/proc_test.go
-index 2ece829071..942d65eedb 100644
---- src/runtime/proc_test.go
-+++ src/runtime/proc_test.go
-@@ -354,9 +354,10 @@ func TestGCFairness2(t *testing.T) {
- 
- func TestNumGoroutine(t *testing.T) {
- 	output := runTestProg(t, "testprog", "NumGoroutine")
--	want := "1\n"
--	if output != want {
--		t.Fatalf("want %q, got %q", want, output)
-+	want1 := "1\n"
-+	want2 := "2\n"
-+	if output != want1 && output != want2 {
-+		t.Fatalf("want %q, got %q", want1, output)
- 	}
- 
- 	buf := make([]byte, 1<<20)
-diff --git test/fixedbugs/bug429_run.go test/fixedbugs/bug429_run.go
-index e8d18b13e8..6a555286cf 100644
---- test/fixedbugs/bug429_run.go
-+++ test/fixedbugs/bug429_run.go
-@@ -1,5 +1,5 @@
- // +build !nacl
--// runtarget
-+// skip
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-diff --git test/goprint.go test/goprint.go
-index 0648c77e7d..147f0c91db 100644
---- test/goprint.go
-+++ test/goprint.go
-@@ -8,14 +8,9 @@
- 
- package main
- 
--import (
--	"runtime"
--	"time"
--)
-+import "time"
- 
- func main() {
- 	go println(42, true, false, true, 1.5, "world", (chan int)(nil), []int(nil), (map[string]int)(nil), (func())(nil), byte(255))
--	for runtime.NumGoroutine() > 1 {
--		time.Sleep(10*time.Millisecond)
--	}
-+	time.Sleep(100*time.Millisecond)
- }
diff --git a/go/patch/go-1.10.3/go6.patch b/go/patch/go-1.10.3/go6.patch
deleted file mode 100644
index 5fb512a9..00000000
--- a/go/patch/go-1.10.3/go6.patch
+++ /dev/null
@@ -1,142 +0,0 @@
-diff --git src/encoding/gob/encoder_test.go src/encoding/gob/encoder_test.go
-index a1ca252ccd..c66e623499 100644
---- src/encoding/gob/encoder_test.go
-+++ src/encoding/gob/encoder_test.go
-@@ -1130,10 +1130,7 @@ func TestBadData(t *testing.T) {
- 
- // TestHugeWriteFails tests that enormous messages trigger an error.
- func TestHugeWriteFails(t *testing.T) {
--	if testing.Short() {
--		// Requires allocating a monster, so don't do this from all.bash.
--		t.Skip("skipping huge allocation in short mode")
--	}
-+	t.Skip("skipping test due to huge memory requirement")
- 	huge := make([]byte, tooBig)
- 	huge[0] = 7 // Make sure it's not all zeros.
- 	buf := new(bytes.Buffer)
-diff --git src/runtime/crash_cgo_test.go src/runtime/crash_cgo_test.go
-index 998055972a..60fe2a52d2 100644
---- src/runtime/crash_cgo_test.old
-+++ src/runtime/crash_cgo_test.go
-@@ -251,10 +251,7 @@ func TestCgoCCodeSIGPROF(t *testing.T) {
- }
- 
- func TestCgoCrashTraceback(t *testing.T) {
--	t.Parallel()
--	if runtime.GOOS != "linux" || (runtime.GOARCH != "amd64" && runtime.GOARCH != "ppc64le") {
--		t.Skipf("not yet supported on %s/%s", runtime.GOOS, runtime.GOARCH)
--	}
-+	t.Skipf("skip running remotely")
- 	got := runTestProg(t, "testprogcgo", "CrashTraceback")
- 	for i := 1; i <= 3; i++ {
- 		if !strings.Contains(got, fmt.Sprintf("cgo symbolizer:%d", i)) {
-@@ -273,10 +270,7 @@ func TestCgoTracebackContext(t *testing.T) {
- }
- 
- func testCgoPprof(t *testing.T, buildArg, runArg string) {
--	t.Parallel()
--	if runtime.GOOS != "linux" || (runtime.GOARCH != "amd64" && runtime.GOARCH != "ppc64le") {
--		t.Skipf("not yet supported on %s/%s", runtime.GOOS, runtime.GOARCH)
--	}
-+	t.Skipf("skip pprof test")
- 	testenv.MustHaveGoRun(t)
- 
- 	exe, err := buildTestProg(t, "testprogcgo", buildArg)
-diff --git src/runtime/crash_test.go src/runtime/crash_test.go
-index 3607992788..e53ffb6a81 100644
---- src/runtime/crash_test.go
-+++ src/runtime/crash_test.go
-@@ -481,6 +481,7 @@ func TestPanicLoop(t *testing.T) {
- }
- 
- func TestMemPprof(t *testing.T) {
-+	t.Skipf("skip pprof test")
- 	testenv.MustHaveGoRun(t)
- 
- 	exe, err := buildTestProg(t, "testprog")
-diff --git src/runtime/crash_unix_test.go src/runtime/crash_unix_test.go
-index 02891ec1ad..fd2723f16e 100644
---- src/runtime/crash_unix_test.go
-+++ src/runtime/crash_unix_test.go
-@@ -174,9 +174,7 @@ func TestPanicSystemstack(t *testing.T) {
- 	// The GOTRACEBACK=crash handler takes 0.1 seconds even if
- 	// it's not writing a core file and potentially much longer if
- 	// it is. Skip in short mode.
--	if testing.Short() {
--		t.Skip("Skipping in short mode (GOTRACEBACK=crash is slow)")
--	}
-+	t.Skip("Skipping (GOTRACEBACK=crash hangs on arm)")
- 
- 	if runtime.Sigisblocked(int(syscall.SIGQUIT)) {
- 		t.Skip("skipping; SIGQUIT is blocked, see golang.org/issue/19196")
-@@ -244,6 +242,7 @@ func testPanicSystemstackInternal() {
- }
- 
- func TestSignalExitStatus(t *testing.T) {
-+        t.Skipf("skip running remotely")
- 	testenv.MustHaveGoBuild(t)
- 	exe, err := buildTestProg(t, "testprog")
- 	if err != nil {
-diff --git src/runtime/fastlog2_test.go src/runtime/fastlog2_test.go
-index ae0f40b2bb..a93933d7ac 100644
---- src/runtime/fastlog2_test.go
-+++ src/runtime/fastlog2_test.go
-@@ -16,11 +16,7 @@ func TestFastLog2(t *testing.T) {
- 	const randomBitCount = 26
- 	var e float64
- 
--	inc := 1
--	if testing.Short() {
--		// Check 1K total values, down from 64M.
--		inc = 1 << 16
--	}
-+	inc := 1 << 16
- 	for i := 1; i < 1<<randomBitCount; i += inc {
- 		l, fl := math.Log2(float64(i)), runtime.Fastlog2(float64(i))
- 		d := l - fl
-diff --git src/runtime/hash_test.go src/runtime/hash_test.go
-index 1400579cda..4c5de7fbef 100644
---- src/runtime/hash_test.go
-+++ src/runtime/hash_test.go
-@@ -161,9 +161,7 @@ func TestSmhasherZeros(t *testing.T) {
- 
- // Strings with up to two nonzero bytes all have distinct hashes.
- func TestSmhasherTwoNonzero(t *testing.T) {
--	if testing.Short() {
--		t.Skip("Skipping in short mode")
--	}
-+	t.Skip("skipping test due to huge memory requirement")
- 	h := newHashSet()
- 	for n := 2; n <= 16; n++ {
- 		twoNonZero(h, n)
-@@ -264,9 +262,7 @@ func setbits(h *HashSet, b []byte, i int, k int) {
- // Test all possible combinations of n blocks from the set s.
- // "permutation" is a bad name here, but it is what Smhasher uses.
- func TestSmhasherPermutation(t *testing.T) {
--	if testing.Short() {
--		t.Skip("Skipping in short mode")
--	}
-+	t.Skip("skipping test due to huge memory requirement")
- 	permutation(t, []uint32{0, 1, 2, 3, 4, 5, 6, 7}, 8)
- 	permutation(t, []uint32{0, 1 << 29, 2 << 29, 3 << 29, 4 << 29, 5 << 29, 6 << 29, 7 << 29}, 8)
- 	permutation(t, []uint32{0, 1}, 20)
-diff --git src/runtime/pprof/pprof_test.go src/runtime/pprof/pprof_test.go
-index 44d514393e..f46f00894c 100644
---- src/runtime/pprof/pprof_test.go
-+++ src/runtime/pprof/pprof_test.go
-@@ -283,14 +283,7 @@ func profileOk(t *testing.T, need []string, prof bytes.Buffer, duration time.Dur
- func TestCPUProfileWithFork(t *testing.T) {
- 	testenv.MustHaveExec(t)
- 
--	heap := 1 << 30
--	if runtime.GOOS == "android" {
--		// Use smaller size for Android to avoid crash.
--		heap = 100 << 20
--	}
--	if testing.Short() {
--		heap = 100 << 20
--	}
-+	heap = 100 << 20
- 	// This makes fork slower.
- 	garbage := make([]byte, heap)
- 	// Need to touch the slice, otherwise it won't be paged in.
diff --git a/go/patch/go-1.11.2/go0.patch b/go/patch/go-1.11.2/go0.patch
deleted file mode 100644
index f80045c0..00000000
--- a/go/patch/go-1.11.2/go0.patch
+++ /dev/null
@@ -1,27 +0,0 @@
-diff --git src/go/build/deps_test.go src/go/build/deps_test.go
-index 29dbe47d29..53e0e287bc 100644
---- src/go/build/deps_test.go
-+++ src/go/build/deps_test.go
-@@ -191,7 +191,7 @@ var pkgDeps = map[string][]string{
- 	"testing":          {"L2", "flag", "fmt", "internal/race", "os", "runtime/debug", "runtime/pprof", "runtime/trace", "time"},
- 	"testing/iotest":   {"L2", "log"},
- 	"testing/quick":    {"L2", "flag", "fmt", "reflect", "time"},
--	"internal/testenv": {"L2", "OS", "flag", "testing", "syscall"},
-+	"internal/testenv": {"L2", "OS", "os.exec", "flag", "testing", "syscall"},
- 
- 	// L4 is defined as L3+fmt+log+time, because in general once
- 	// you're using L3 packages, use of fmt, log, or time is not a big deal.
-diff --git src/internal/testenv/testenv.go src/internal/testenv/testenv.go
-index 8f69fe0da5..d52b85e122 100644
---- src/internal/testenv/testenv.go
-+++ src/internal/testenv/testenv.go
-@@ -48,6 +48,9 @@ func HasGoBuild() bool {
- 			return false
- 		}
- 	}
-+	if _, err := exec.LookPath("go"); err != nil {
-+	        return false
-+	}
- 	return true
- }
- 
diff --git a/go/patch/go-1.11.2/go1.patch b/go/patch/go-1.11.2/go1.patch
deleted file mode 100644
index e05fcce4..00000000
--- a/go/patch/go-1.11.2/go1.patch
+++ /dev/null
@@ -1,50 +0,0 @@
-diff --git test/chanlinear.go test/chanlinear.go
-index 55fee4ab9b..89533da282 100644
---- test/chanlinear.go
-+++ test/chanlinear.go
-@@ -1,4 +1,4 @@
--// +build darwin linux
-+// +build darwin linux android
- // run
- 
- // Copyright 2014 The Go Authors. All rights reserved.
-diff --git a/test/fixedbugs/bug385_64.go b/test/fixedbugs/bug385_64.go
-index 0f941ca2f4..3bcd62f3ad 100644
---- test/fixedbugs/bug385_64.go
-+++ test/fixedbugs/bug385_64.go
-@@ -1,4 +1,4 @@
--// +build amd64
-+// +build amd64 arm64
- // errorcheck
- 
- // Copyright 2011 The Go Authors. All rights reserved.
-diff --git test/fixedbugs/issue10607.go test/fixedbugs/issue10607.go
-index 8831547da8..9ee6c72bc6 100644
---- test/fixedbugs/issue10607.go
-+++ test/fixedbugs/issue10607.go
-@@ -1,4 +1,4 @@
--// +build linux,!ppc64
-+// +build linux,!ppc64 android
- // run
- 
- // Copyright 2015 The Go Authors. All rights reserved.
-diff --git test/maplinear.go test/maplinear.go
-index 34d0914914..afddab627d 100644
---- test/maplinear.go
-+++ test/maplinear.go
-@@ -1,4 +1,4 @@
--// +build darwin linux
-+// +build darwin linux android
- // run
- 
- // Copyright 2013 The Go Authors. All rights reserved.
-diff --git test/recover4.go test/recover4.go
-index 67ed970ecb..95a89dab00 100644
---- test/recover4.go
-+++ test/recover4.go
-@@ -1,4 +1,4 @@
--// +build linux darwin
-+// +build linux android darwin
- // run
- 
- // Copyright 2015 The Go Authors. All rights reserved.
diff --git a/go/patch/go-1.11.2/go2.patch b/go/patch/go-1.11.2/go2.patch
deleted file mode 100644
index 362a53fe..00000000
--- a/go/patch/go-1.11.2/go2.patch
+++ /dev/null
@@ -1,277 +0,0 @@
-diff --git test/run.go test/run.go
-index ad38d420c9..e2b93d35da 100644
---- test/run.go
-+++ test/run.go
-@@ -36,13 +36,13 @@ var (
- 	summary        = flag.Bool("summary", false, "show summary of results")
- 	showSkips      = flag.Bool("show_skips", false, "show skipped tests")
- 	runSkips       = flag.Bool("run_skips", false, "run skipped tests (ignore skip and build tags)")
--	linkshared     = flag.Bool("linkshared", false, "")
- 	updateErrors   = flag.Bool("update_errors", false, "update error messages in test file based on compiler output")
- 	runoutputLimit = flag.Int("l", defaultRunOutputLimit(), "number of parallel runoutput tests to run")
- 
- 	shard  = flag.Int("shard", 0, "shard index to run. Only applicable if -shards is non-zero.")
- 	shards = flag.Int("shards", 0, "number of shards. If 0, all tests are run. This is used by the continuous build.")
- )
-+	target         = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
- 
- var (
- 	goos, goarch string
-@@ -207,25 +207,19 @@ func goFiles(dir string) []string {
- type runCmd func(...string) ([]byte, error)
- 
- func compileFile(runcmd runCmd, longname string, flags []string) (out []byte, err error) {
--	cmd := []string{goTool(), "tool", "compile", "-e"}
-+	cmd := []string{findGoCmd, "tool", "compile", "-e"}
- 	cmd = append(cmd, flags...)
--	if *linkshared {
--		cmd = append(cmd, "-dynlink", "-installsuffix=dynlink")
--	}
- 	cmd = append(cmd, longname)
- 	return runcmd(cmd...)
- }
- 
- func compileInDir(runcmd runCmd, dir string, flags []string, localImports bool, names ...string) (out []byte, err error) {
--	cmd := []string{goTool(), "tool", "compile", "-e"}
-+	cmd := []string{findGoCmd(), "tool", "compile", "-e"}
- 	if localImports {
- 		// Set relative path for local imports and import search path to current dir.
- 		cmd = append(cmd, "-D", ".", "-I", ".")
- 	}
- 	cmd = append(cmd, flags...)
--	if *linkshared {
--		cmd = append(cmd, "-dynlink", "-installsuffix=dynlink")
--	}
- 	for _, name := range names {
- 		cmd = append(cmd, filepath.Join(dir, name))
- 	}
-@@ -234,15 +228,28 @@ func compileInDir(runcmd runCmd, dir string, flags []string, localImports bool,
- 
- func linkFile(runcmd runCmd, goname string) (err error) {
- 	pfile := strings.Replace(goname, ".go", ".o", -1)
--	cmd := []string{goTool(), "tool", "link", "-w", "-o", "a.exe", "-L", "."}
-+	cmd := []string{findGoCmd(), "tool", "link", "-w", "-o", "a.exe", "-L", "."}
- 	if *linkshared {
- 		cmd = append(cmd, "-linkshared", "-installsuffix=dynlink")
- 	}
- 	cmd = append(cmd, pfile)
--	_, err = runcmd(cmd...)
-+	_, err = runcmd(findGoCmd(), "tool", "link", "-w", "-o", "a.exe", "-L", ".", pfile)
- 	return
- }
- 
-+
-+func goRun(runcmd runCmd, flags []string, goname string, args ...string) (out []byte, err error) {
-+        cmd := []string{findGoCmd(), "run", goGcflags()}
-+        if len(findExecCmd()) > 0 {
-+                cmd = append(cmd, "-exec")
-+                cmd = append(cmd, findExecCmd()...)
-+        }
-+        cmd = append(cmd, flags...)
-+        cmd = append(cmd, goname)
-+        cmd = append(cmd, args...)
-+        return runcmd(cmd...)
-+}
-+
- // skipError describes why a test was skipped.
- type skipError string
- 
-@@ -646,7 +653,7 @@ func (t *test) run() {
- 		// Fail if wantError is true and compilation was successful and vice versa.
- 		// Match errors produced by gc against errors in comments.
- 		// TODO(gri) remove need for -C (disable printing of columns in error messages)
--		cmdline := []string{goTool(), "tool", "compile", "-C", "-e", "-o", "a.o"}
-+		cmdline := []string{findGoCmd(), "tool", "compile", "-C", "-e", "-o", "a.o"}
- 		// No need to add -dynlink even if linkshared if we're just checking for errors...
- 		cmdline = append(cmdline, flags...)
- 		cmdline = append(cmdline, long)
-@@ -773,7 +780,7 @@ func (t *test) run() {
- 
- 	case "build":
- 		// Build Go file.
--		_, err := runcmd(goTool(), "build", goGcflags(), "-o", "a.exe", long)
-+		_, err := runcmd(findGoCmd(), "build", goGcflags(), "-o", "a.exe", long)
- 		if err != nil {
- 			t.err = err
- 		}
-@@ -799,7 +806,7 @@ func (t *test) run() {
-
- 		}
- 		var objs []string
--		cmd := []string{goTool(), "tool", "compile", "-e", "-D", ".", "-I", ".", "-o", "go.o"}
-+		cmd := []string{findGoCmd(), "tool", "compile", "-e", "-D", ".", "-I", ".", "-o", "go.o"}
- 		if len(asms) > 0 {
- 			cmd = append(cmd, "-asmhdr", "go_asm.h")
- 		}
-@@ -813,7 +820,7 @@ func (t *test) run() {
- 		}
- 		objs = append(objs, "go.o")
- 		if len(asms) > 0 {
--			cmd = []string{goTool(), "tool", "asm", "-e", "-I", ".", "-o", "asm.o"}
-+			cmd = []string{findGoCmd(), "tool", "asm", "-e", "-I", ".", "-o", "asm.o"}
- 			for _, file := range asms {
- 				cmd = append(cmd, filepath.Join(longdir, file.Name()))
- 			}
-@@ -857,14 +864,14 @@ func (t *test) run() {
- 			}
- 			objs = append(objs, "asm.o")
- 		}
--		cmd = []string{goTool(), "tool", "pack", "c", "all.a"}
-+		cmd = []string{findGoCmd(), "tool", "pack", "c", "all.a"}
- 		cmd = append(cmd, objs...)
- 		_, err = runcmd(cmd...)
- 		if err != nil {
- 			t.err = err
- 			break
- 		}
--		cmd = []string{goTool(), "tool", "link", "-o", "a.exe", "all.a"}
-+		cmd = []string{findGoCmd(), "tool", "link", "-o", "a.exe", "all.a"}
- 		_, err = runcmd(cmd...)
- 		if err != nil {
- 			t.err = err
-@@ -886,10 +893,7 @@ func (t *test) run() {
- 		// Build an executable from Go file, then run it, verify its output.
- 		// Useful for timeout tests where failure mode is infinite loop.
- 		// TODO: not supported on NaCl
--		cmd := []string{goTool(), "build", goGcflags(), "-o", "a.exe"}
--		if *linkshared {
--			cmd = append(cmd, "-linkshared")
--		}
-+		cmd := []string{findGoCmd(), "build", goGcflags(), "-o", "a.exe"}
- 		longdirgofile := filepath.Join(filepath.Join(cwd, t.dir), t.gofile)
- 		cmd = append(cmd, flags...)
- 		cmd = append(cmd, longdirgofile)
-@@ -898,8 +902,13 @@ func (t *test) run() {
- 			t.err = err
- 			return
- 		}
--		cmd = []string{"./a.exe"}
--		out, err = runcmd(append(cmd, args...)...)
-+		cmd = []string{}
-+                if len(findExecCmd()) > 0 {
-+                        cmd = append(cmd, findExecCmd()...)
-+                }
-+                cmd = append(cmd, "./a.exe")
-+
-+ 		out, err = runcmd(append(cmd, args...)...)
- 		if err != nil {
- 			t.err = err
- 			return
-@@ -914,38 +923,7 @@ func (t *test) run() {
- 		// otherwise build an executable and run it.
- 		// Verify the output.
- 		useTmp = false
--		var out []byte
--		var err error
--		if len(flags)+len(args) == 0 && goGcflags() == "" && !*linkshared {
--			// If we're not using special go command flags,
--			// skip all the go command machinery.
--			// This avoids any time the go command would
--			// spend checking whether, for example, the installed
--			// package runtime is up to date.
--			// Because we run lots of trivial test programs,
--			// the time adds up.
--			pkg := filepath.Join(t.tempDir, "pkg.a")
--			if _, err := runcmd(goTool(), "tool", "compile", "-o", pkg, t.goFileName()); err != nil {
--				t.err = err
--				return
--			}
--			exe := filepath.Join(t.tempDir, "test.exe")
--			cmd := []string{goTool(), "tool", "link", "-s", "-w"}
--			cmd = append(cmd, "-o", exe, pkg)
--			if _, err := runcmd(cmd...); err != nil {
--				t.err = err
--				return
--			}
--			out, err = runcmd(append([]string{exe}, args...)...)
--		} else {
--			cmd := []string{goTool(), "run", goGcflags()}
--			if *linkshared {
--				cmd = append(cmd, "-linkshared")
--			}
--			cmd = append(cmd, flags...)
--			cmd = append(cmd, t.goFileName())
--			out, err = runcmd(append(cmd, args...)...)
--		}
-+		out, err := goRun(runcmd, flags, t.goFileName(), args...)
- 		if err != nil {
- 			t.err = err
- 			return
-@@ -962,12 +940,7 @@ func (t *test) run() {
- 			<-rungatec
- 		}()
- 		useTmp = false
--		cmd := []string{goTool(), "run", goGcflags()}
--		if *linkshared {
--			cmd = append(cmd, "-linkshared")
--		}
--		cmd = append(cmd, t.goFileName())
--		out, err := runcmd(append(cmd, args...)...)
-+		out, err := goRun(runcmd, nil, t.goFileName(), args...)
- 		if err != nil {
- 			t.err = err
- 			return
-@@ -977,12 +950,7 @@ func (t *test) run() {
- 			t.err = fmt.Errorf("write tempfile:%s", err)
- 			return
- 		}
--		cmd = []string{goTool(), "run", goGcflags()}
--		if *linkshared {
--			cmd = append(cmd, "-linkshared")
--		}
--		cmd = append(cmd, tfile)
--		out, err = runcmd(cmd...)
-+		out, err = goRun(runcmd, nil, tfile)
- 		if err != nil {
- 			t.err = err
- 			return
-@@ -995,12 +963,7 @@ func (t *test) run() {
- 		// Run Go file and write its output into temporary Go file.
- 		// Compile and errorCheck generated Go file.
- 		useTmp = false
--		cmd := []string{goTool(), "run", goGcflags()}
--		if *linkshared {
--			cmd = append(cmd, "-linkshared")
--		}
--		cmd = append(cmd, t.goFileName())
--		out, err := runcmd(append(cmd, args...)...)
-+		out, err := goRun(runcmd, nil, t.goFileName(), args...)
- 		if err != nil {
- 			t.err = err
- 			return
-@@ -1011,7 +974,7 @@ func (t *test) run() {
- 			t.err = fmt.Errorf("write tempfile:%s", err)
- 			return
- 		}
--		cmdline := []string{goTool(), "tool", "compile", "-e", "-o", "a.o"}
-+		cmdline := []string{findGoCmd(), "tool", "compile", "-e", "-o", "a.o"}
- 		cmdline = append(cmdline, flags...)
- 		cmdline = append(cmdline, tfile)
- 		out, err = runcmd(cmdline...)
-@@ -1038,6 +1001,11 @@ func findExecCmd() []string {
- 		return execCmd
- 	}
- 	execCmd = []string{} // avoid work the second time
-+        if *target != "" {
-+                execCmd = []string{"go_" + *target + "_exec"}
-+                return execCmd
-+        }
-+
- 	if goos == runtime.GOOS && goarch == runtime.GOARCH {
- 		return execCmd
- 	}
-@@ -1048,6 +1016,14 @@ func findExecCmd() []string {
- 	return execCmd
- }
- 
-+func findGoCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
-+
- func (t *test) String() string {
- 	return filepath.Join(t.dir, t.gofile)
- }
diff --git a/go/patch/go-1.11.2/go3.patch b/go/patch/go-1.11.2/go3.patch
deleted file mode 100644
index c97cd302..00000000
--- a/go/patch/go-1.11.2/go3.patch
+++ /dev/null
@@ -1,730 +0,0 @@
-diff --git test/fixedbugs/bug302.go test/fixedbugs/bug302.go
-index c763b87786..470841f676 100644
---- test/fixedbugs/bug302.go
-+++ test/fixedbugs/bug302.go
-@@ -1,5 +1,5 @@
- // +build !nacl,!js
--// run
-+// runtarget
- 
- // Copyright 2010 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -8,16 +8,28 @@
- package main
- 
- import (
-+        "flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
-+
- func main() {
--	run("go", "tool", "compile", filepath.Join("fixedbugs", "bug302.dir", "p.go"))
--	run("go", "tool", "pack", "grc", "pp.a", "p.o")
--	run("go", "tool", "compile", "-I", ".", filepath.Join("fixedbugs", "bug302.dir", "main.go"))
-+        flag.Parse()
-+	run(goCmd(), "tool", "compile", filepath.Join("fixedbugs", "bug302.dir", "p.go"))
-+	run(goCmd(), "tool", "pack", "grc", "pp.a", "p.o")
-+	run(goCmd(), "tool", "compile", "-I", ".", filepath.Join("fixedbugs", "bug302.dir", "main.go"))
- 	os.Remove("p.o")
- 	os.Remove("pp.a")
- 	os.Remove("main.o")
-diff --git test/fixedbugs/bug369.go test/fixedbugs/bug369.go
-index e2a1147735..769364d503 100644
---- test/fixedbugs/bug369.go
-+++ test/fixedbugs/bug369.go
-@@ -1,5 +1,5 @@
- // +build !nacl,!js,!windows
--// run
-+// runtarget
- 
- // Copyright 2011 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -10,21 +10,40 @@
- package main
- 
- import (
-+        "flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+       if *target != "" {
-+               return "go_" + *target
-+       }
-+       return "go"
-+}
-+
-+func goRun(cmd ...string) {
-+       if *target == "" {
-+               run(cmd[0], cmd[1:]...)
-+       } else {
-+               run("go_"+*target+"_exec", cmd...)
-+       }
-+}
-+
- func main() {
-+        flag.Parse()
- 	err := os.Chdir(filepath.Join(".", "fixedbugs", "bug369.dir"))
- 	check(err)
- 
--	run("go", "tool", "compile", "-N", "-o", "slow.o", "pkg.go")
--	run("go", "tool", "compile", "-o", "fast.o", "pkg.go")
--	run("go", "tool", "compile", "-o", "main.o", "main.go")
--	run("go", "tool", "link", "-o", "a.exe", "main.o")
--	run("." + string(filepath.Separator) + "a.exe")
-+	run(goCmd(), "tool", "compile", "-N", "-o", "slow.o", "pkg.go")
-+	run(goCmd(), "tool", "compile", "-o", "fast.o", "pkg.go")
-+	run(goCmd(), "tool", "compile", "-o", "main.o", "main.go")
-+	run(goCmd(), "tool", "link", "-o", "a.exe", "main.o")
-+	goRun("." + string(filepath.Separator) + "a.exe")
- 
- 	os.Remove("slow.o")
- 	os.Remove("fast.o")
-diff --git test/fixedbugs/bug429_run.go test/fixedbugs/bug429_run.go
-index c6a02aae5e..30298de97b 100644
---- test/fixedbugs/bug429_run.go
-+++ test/fixedbugs/bug429_run.go
-@@ -1,5 +1,5 @@
- // +build !nacl,!js
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -10,6 +10,7 @@
- package main
- 
- import (
-+        "flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
-@@ -17,8 +18,27 @@ import (
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+       if *target != "" {
-+               return "go_" + *target
-+       }
-+       return "go"
-+}
-+
-+func goRun(args ...string) *exec.Cmd {
-+       cmd := []string{"run"}
-+       if *target != "" {
-+               cmd = append(cmd, "-exec", "go_"+*target+"_exec")
-+       }
-+       cmd = append(cmd, args...)
-+       return exec.Command(goCmd(), cmd...)
-+}
-+
- func main() {
--	cmd := exec.Command("go", "run", filepath.Join("fixedbugs", "bug429.go"))
-+        flag.Parse()
-+	cmd := goRun(filepath.Join("fixedbugs", "bug429.go"))
- 	out, err := cmd.CombinedOutput()
- 	if err == nil {
- 		fmt.Println("expected deadlock")
-diff --git test/fixedbugs/issue10607.go test/fixedbugs/issue10607.go
-index 9ee6c72bc6..e819a3085a 100644
---- test/fixedbugs/issue10607.go
-+++ test/fixedbugs/issue10607.go
-@@ -1,5 +1,5 @@
- // +build linux,!ppc64 android
--// run
-+// runtarget
- 
- // Copyright 2015 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -11,19 +11,39 @@
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"path/filepath"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+       if *target != "" {
-+               return "go_" + *target
-+       }
-+       return "go"
-+}
-+
-+func goRun(args ...string) *exec.Cmd {
-+       cmd := []string{"run"}
-+       if *target != "" {
-+               cmd = append(cmd, "-exec", "go_"+*target+"_exec")
-+       }
-+       cmd = append(cmd, args...)
-+       return exec.Command(goCmd(), cmd...)
-+}
-+
- func main() {
--	test("internal")
-+	flag.Parse()
-+	//test("internal")
- 	test("external")
- }
- 
- func test(linkmode string) {
--	out, err := exec.Command("go", "run", "-ldflags", "-B=0x12345678 -linkmode="+linkmode, filepath.Join("fixedbugs", "issue10607a.go")).CombinedOutput()
-+	out, err := goRun("-ldflags", "-B=0x12345678 -linkmode="+linkmode, filepath.Join("fixedbugs", "issue10607a.go")).CombinedOutput()
- 	if err != nil {
- 		fmt.Printf("BUG: linkmode=%s %v\n%s\n", linkmode, err, out)
- 		os.Exit(1)
-diff --git test/fixedbugs/issue11771.go test/fixedbugs/issue11771.go
-index 99d7060d44..777cb7b9c4 100644
---- test/fixedbugs/issue11771.go
-+++ test/fixedbugs/issue11771.go
-@@ -1,5 +1,5 @@
- // +build !nacl,!js
--// run
-+// runtarget
- 
- // Copyright 2015 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -11,6 +11,7 @@ package main
- 
- import (
- 	"bytes"
-+	"flag"
- 	"fmt"
- 	"io/ioutil"
- 	"log"
-@@ -19,8 +20,17 @@ import (
- 	"path/filepath"
- 	"runtime"
- )
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+       if *target != "" {
-+               return "go_" + *target
-+       }
-+       return "go"
-+}
- 
- func main() {
-+	flag.Parse()
- 	if runtime.Compiler != "gc" {
- 		return
- 	}
-@@ -52,7 +62,7 @@ func x() {
- 		log.Fatal(err)
- 	}
- 
--	cmd := exec.Command("go", "tool", "compile", "x.go")
-+	cmd := exec.Command(goCmd(), "tool", "compile", "x.go")
- 	cmd.Dir = dir
- 	output, err := cmd.CombinedOutput()
- 	if err == nil {
-diff --git test/fixedbugs/issue9355.go test/fixedbugs/issue9355.go
-index 9657e64491..bad099f440 100644
---- test/fixedbugs/issue9355.go
-+++ test/fixedbugs/issue9355.go
-@@ -1,4 +1,4 @@
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -7,6 +7,7 @@
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
-@@ -15,7 +16,17 @@ import (
- 	"runtime"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
- func main() {
-+        flag.Parse()
- 	if runtime.Compiler != "gc" || runtime.GOOS == "nacl" || runtime.GOOS == "js" {
- 		return
- 	}
-@@ -23,7 +34,7 @@ func main() {
- 	err := os.Chdir(filepath.Join("fixedbugs", "issue9355.dir"))
- 	check(err)
- 
--	out := run("go", "tool", "compile", "-S", "a.go")
-+	out := run(goCmd(), "tool", "compile", "-S", "a.go")
- 	os.Remove("a.o")
- 
- 	// 6g/8g print the offset as dec, but 5g/9g print the offset as hex.
-diff --git test/fixedbugs/issue9862_run.go test/fixedbugs/issue9862_run.go
-index 299e809545..02b8ea83c2 100644
---- test/fixedbugs/issue9862_run.go
-+++ test/fixedbugs/issue9862_run.go
-@@ -1,5 +1,5 @@
- // +build !nacl,!js
--// run
-+// runtarget
- 
- // Copyright 2015 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -10,12 +10,32 @@
- package main
- 
- import (
-+	"flag"
- 	"os/exec"
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
-+func goRun(args ...string) *exec.Cmd {
-+        cmd := []string{"run"}
-+        if *target != "" {
-+                cmd = append(cmd, "-exec", "go_"+*target+"_exec")
-+        }
-+        cmd = append(cmd, args...)
-+        return exec.Command(goCmd(), cmd...)
-+}
-+
- func main() {
--	out, err := exec.Command("go", "run", "fixedbugs/issue9862.go").CombinedOutput()
-+	flag.Parse()
-+	out, err := goRun("fixedbugs/issue9862.go").CombinedOutput()
- 	outstr := string(out)
- 	if err == nil {
- 		println("go run issue9862.go succeeded, should have failed\n", outstr)
-diff --git test/linkmain_run.go test/linkmain_run.go
-index 68d53e8cad..0aa5e0fe2d 100644
---- test/linkmain_run.go
-+++ test/linkmain_run.go
-@@ -1,5 +1,5 @@
- // +build !nacl,!js
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -10,12 +10,22 @@
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
- func cleanup() {
- 	os.Remove("linkmain.o")
- 	os.Remove("linkmain.a")
-@@ -51,16 +61,18 @@ func runFail(cmdline string) {
- }
- 
- func main() {
-+	flag.Parse()
-+
- 	// helloworld.go is package main
--	run("go tool compile -o linkmain.o helloworld.go")
--	run("go tool compile -pack -o linkmain.a helloworld.go")
--	run("go tool link -o linkmain.exe linkmain.o")
--	run("go tool link -o linkmain.exe linkmain.a")
-+	run(goCmd() + " tool compile -o linkmain.o helloworld.go")
-+	run(goCmd() + " tool compile -pack -o linkmain.a helloworld.go")
-+	run(goCmd() + " tool link -o linkmain.exe linkmain.o")
-+	run(goCmd() + " tool link -o linkmain.exe linkmain.a")
- 
- 	// linkmain.go is not
--	run("go tool compile -o linkmain1.o linkmain.go")
--	run("go tool compile -pack -o linkmain1.a linkmain.go")
--	runFail("go tool link -o linkmain.exe linkmain1.o")
--	runFail("go tool link -o linkmain.exe linkmain1.a")
-+	run(goCmd() + " tool compile -o linkmain1.o linkmain.go")
-+	run(goCmd() + " tool compile -pack -o linkmain1.a linkmain.go")
-+	runFail(goCmd() + " tool link -o linkmain.exe linkmain1.o")
-+	runFail(goCmd() + " tool link -o linkmain.exe linkmain1.a")
- 	cleanup()
- }
-diff --git test/linkobj.go test/linkobj.go
-index 2902d23f4b..c17dfd3da9 100644
---- test/linkobj.go
-+++ test/linkobj.go
-@@ -1,5 +1,5 @@
- // +build !nacl,!js
--// run
-+// runtarget
- 
- // Copyright 2016 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -10,6 +10,7 @@
- package main
- 
- import (
-+	"flag"
- 	"fmt"
- 	"io/ioutil"
- 	"log"
-@@ -18,9 +19,27 @@ import (
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
-+func goRun(cmd ...string) string {
-+        if *target == "" {
-+                return run(cmd...)
-+        } else {
-+                return run(append([]string{"go_"+*target+"_exec"}, cmd...)...)
-+        }
-+}
-+
- var pwd, tmpdir string
- 
- func main() {
-+	flag.Parse()
- 	dir, err := ioutil.TempDir("", "go-test-linkobj-")
- 	if err != nil {
- 		log.Fatal(err)
-@@ -37,28 +56,28 @@ func main() {
- 
- 	writeFile("p1.go", `
- 		package p1
--		
-+
- 		func F() {
- 			println("hello from p1")
- 		}
- 	`)
- 	writeFile("p2.go", `
- 		package p2
--		
-+
- 		import "./p1"
- 
- 		func F() {
- 			p1.F()
- 			println("hello from p2")
- 		}
--		
-+
- 		func main() {}
- 	`)
- 	writeFile("p3.go", `
- 		package main
- 
- 		import "./p2"
--		
-+
- 		func main() {
- 			p2.F()
- 			println("hello from main")
-@@ -76,9 +95,9 @@ func main() {
- 		}
- 
- 		// inlining is disabled to make sure that the link objects contain needed code.
--		run("go", "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p1."+o, "-linkobj", "p1.lo", "p1.go")
--		run("go", "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p2."+o, "-linkobj", "p2.lo", "p2.go")
--		run("go", "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p3."+o, "-linkobj", "p3.lo", "p3.go")
-+		run(goCmd(), "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p1."+o, "-linkobj", "p1.lo", "p1.go")
-+		run(goCmd(), "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p2."+o, "-linkobj", "p2.lo", "p2.go")
-+		run(goCmd(), "tool", "compile", pkg, "-D", ".", "-I", ".", "-l", "-o", "p3."+o, "-linkobj", "p3.lo", "p3.go")
- 
- 		cp("p1."+o, "p1.oo")
- 		cp("p2."+o, "p2.oo")
-@@ -86,13 +105,13 @@ func main() {
- 		cp("p1.lo", "p1."+o)
- 		cp("p2.lo", "p2."+o)
- 		cp("p3.lo", "p3."+o)
--		out := runFail("go", "tool", "link", "p2."+o)
-+		out := runFail(goCmd(), "tool", "link", "p2."+o)
- 		if !strings.Contains(out, "not package main") {
- 			fatalf("link p2.o failed but not for package main:\n%s", out)
- 		}
- 
--		run("go", "tool", "link", "-L", ".", "-o", "a.out.exe", "p3."+o)
--		out = run("./a.out.exe")
-+		run(goCmd(), "tool", "link", "-L", ".", "-o", "a.out.exe", "p3."+o)
-+		out = goRun("./a.out.exe")
- 		if !strings.Contains(out, "hello from p1\nhello from p2\nhello from main\n") {
- 			fatalf("running main, incorrect output:\n%s", out)
- 		}
-diff --git test/linkx_run.go test/linkx_run.go
-index ca9d31612a..631b95ee67 100644
---- test/linkx_run.go
-+++ test/linkx_run.go
-@@ -1,5 +1,5 @@
- // +build !nacl,!js
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -11,20 +11,40 @@ package main
- 
- import (
- 	"bytes"
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
-+func goRun(args ...string) *exec.Cmd {
-+        cmd := []string{"run"}
-+        if *target != "" {
-+                cmd = append(cmd, "-exec", "go_"+*target+"_exec")
-+        }
-+        cmd = append(cmd, args...)
-+        return exec.Command(goCmd(), cmd...)
-+}
-+
- func main() {
-+	flag.Parse()
- 	// test(" ") // old deprecated & removed syntax
- 	test("=") // new syntax
- }
- 
- func test(sep string) {
- 	// Successful run
--	cmd := exec.Command("go", "run", "-ldflags=-X main.tbd"+sep+"hello -X main.overwrite"+sep+"trumped -X main.nosuchsymbol"+sep+"neverseen", "linkx.go")
-+	cmd := goRun("-ldflags=-X main.tbd"+sep+"hello -X main.overwrite"+sep+"trumped -X main.nosuchsymbol"+sep+"neverseen", "linkx.go")
- 	var out, errbuf bytes.Buffer
- 	cmd.Stdout = &out
- 	cmd.Stderr = &errbuf
-@@ -44,7 +64,7 @@ func test(sep string) {
- 	}
- 
- 	// Issue 8810
--	cmd = exec.Command("go", "run", "-ldflags=-X main.tbd", "linkx.go")
-+	cmd = goRun("-ldflags=-X main.tbd", "linkx.go")
- 	_, err = cmd.CombinedOutput()
- 	if err == nil {
- 		fmt.Println("-X linker flag should not accept keys without values")
-@@ -52,7 +72,7 @@ func test(sep string) {
- 	}
- 
- 	// Issue 9621
--	cmd = exec.Command("go", "run", "-ldflags=-X main.b=false -X main.x=42", "linkx.go")
-+	cmd = goRun("-ldflags=-X main.b=false -X main.x=42", "linkx.go")
- 	outx, err := cmd.CombinedOutput()
- 	if err == nil {
- 		fmt.Println("-X linker flag should not overwrite non-strings")
-diff --git test/nosplit.go test/nosplit.go
-index e6cd04e563..baeea80e37 100644
---- test/nosplit.go
-+++ test/nosplit.go
-@@ -1,5 +1,5 @@
- // +build !nacl,!js
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -9,6 +9,7 @@ package main
- 
- import (
- 	"bytes"
-+	"flag"
- 	"fmt"
- 	"io/ioutil"
- 	"log"
-@@ -21,6 +22,24 @@ import (
- 	"strings"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+        if *target != "" {
-+                return "go_" + *target
-+        }
-+        return "go"
-+}
-+
-+func goArch() string {
-+        goarch, err := exec.Command(goCmd(), "env", "GOARCH").Output()
-+        if err != nil {
-+                bug()
-+                fmt.Printf("running go env GOARCH: %v\n", err)
-+        }
-+        return strings.TrimSpace(string(goarch))
-+}
-+
- var tests = `
- # These are test cases for the linker analysis that detects chains of
- # nosplit functions that would cause a stack overflow.
-@@ -194,12 +213,13 @@ var (
- )
- 
- func main() {
--	goarch := os.Getenv("GOARCH")
-+	flag.Parse()
-+	goarch := goArch()
- 	if goarch == "" {
--		goarch = runtime.GOARCH
-+		return
- 	}
- 
--	version, err := exec.Command("go", "tool", "compile", "-V").Output()
-+	version, err := exec.Command(goCmd(), "tool", "compile", "-V").Output()
- 	if err != nil {
- 		bug()
- 		fmt.Printf("running go tool compile -V: %v\n", err)
-@@ -345,7 +365,7 @@ TestCases:
- 			log.Fatal(err)
- 		}
- 
--		cmd := exec.Command("go", "build")
-+		cmd := exec.Command(goCmd(), "build")
- 		cmd.Dir = dir
- 		output, err := cmd.CombinedOutput()
- 		if err == nil {
-diff --git test/run.go test/run.go
-index 2af3ee43ba..28c87c3583 100644
---- test/run.go
-+++ test/run.go
-@@ -246,6 +246,16 @@ func goRun(runcmd runCmd, flags []string, goname string, args ...string) (out []
- }
- 
- 
-+func goRunTarget(runcmd runCmd, goname string, args ...string) (out []byte, err error) {
-+        cmd := []string{"go_local", "run"}
-+        cmd = append(cmd, goname)
-+        if *target != "" {
-+                cmd = append(cmd, "-target", *target)
-+        }
-+        cmd = append(cmd, args...)
-+        return runcmd(cmd...)
-+}
-+
- // skipError describes why a test was skipped.
- type skipError string
- 
-@@ -505,7 +515,7 @@ func (t *test) run() {
- 
- 	// TODO: Clean up/simplify this switch statement.
- 	switch action {
--	case "compile", "compiledir", "build", "builddir", "buildrundir", "run", "buildrun", "runoutput", "rundir", "asmcheck":
-+	case "compile", "compiledir", "build", "builddir", "buildrundir", "run", "runtarget", "buildrun", "runoutput", "rundir", "asmcheck":
- 		// nothing to do
- 	case "errorcheckandrundir":
- 		wantError = false // should be no error if also will run
-@@ -894,6 +904,17 @@ func (t *test) run() {
- 			t.err = fmt.Errorf("incorrect output\n%s", out)
- 		}
- 
-+       case "runtarget":
-+                useTmp = false
-+                out, err := goRunTarget(runcmd, t.goFileName(), args...)
-+                if err != nil {
-+                        t.err = err
-+                        return
-+                }
-+                if strings.Replace(string(out), "\r\n", "\n", -1) != t.expectedOutput() {
-+                        t.err = fmt.Errorf("incorrect output\n%s", out)
-+                }
-+
- 	case "runoutput":
- 		// Run Go file and write its output into temporary Go file.
- 		// Run generated Go file and verify its output.
-diff --git test/sinit_run.go test/sinit_run.go
-index fdd19c492f..0b3cb76083 100644
---- test/sinit_run.go
-+++ test/sinit_run.go
-@@ -1,5 +1,5 @@
- // +build !nacl,!js
--// run
-+// runtarget
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-@@ -11,11 +11,21 @@ package main
- 
- import (
- 	"bytes"
-+	"flag"
- 	"fmt"
- 	"os"
- 	"os/exec"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd() string {
-+       if *target != "" {
-+               return "go_" + *target
-+       }
-+       return "go"
-+}
-+
- func main() {
- 	cmd := exec.Command("go", "tool", "compile", "-S", "sinit.go")
- 	out, err := cmd.CombinedOutput()
diff --git a/go/patch/go-1.11.2/go4.patch b/go/patch/go-1.11.2/go4.patch
deleted file mode 100644
index 290de390..00000000
--- a/go/patch/go-1.11.2/go4.patch
+++ /dev/null
@@ -1,199 +0,0 @@
-runtime, crypto/x509: add -target flag.
-
---- src/crypto/x509/x509_test.go
-+++ src/crypto/x509/x509_test.go
-@@ -13,29 +13,32 @@ import (
- 	"crypto/rsa"
- 	_ "crypto/sha256"
- 	_ "crypto/sha512"
- 	"crypto/x509/pkix"
- 	"encoding/asn1"
- 	"encoding/base64"
- 	"encoding/hex"
- 	"encoding/pem"
-+	"flag"
- 	"fmt"
- 	"internal/testenv"
- 	"math/big"
- 	"net"
- 	"net/url"
- 	"os/exec"
- 	"reflect"
- 	"runtime"
- 	"strings"
- 	"testing"
- 	"time"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
- func TestParsePKCS1PrivateKey(t *testing.T) {
- 	block, _ := pem.Decode([]byte(pemPrivateKey))
- 	priv, err := ParsePKCS1PrivateKey(block.Bytes)
- 	if err != nil {
- 		t.Errorf("Failed to parse private key: %s", err)
- 		return
- 	}
- 	if priv.PublicKey.N.Cmp(rsaPrivateKey.PublicKey.N) != 0 ||
-@@ -1089,17 +1092,23 @@ func TestParsePEMCRL(t *testing.T) {
- 	}
- 
- 	// Can't check the signature here without a package cycle.
- }
- 
- func TestImports(t *testing.T) {
- 	testenv.MustHaveGoRun(t)
- 
--	if err := exec.Command(testenv.GoToolPath(t), "run", "x509_test_import.go").Run(); err != nil {
-+	var cmd *exec.Cmd
-+	if *target == "" {
-+		cmd = exec.Command(testenv.GoToolPath(t), "run", "x509_test_import.go")
-+	} else {
-+		cmd = exec.Command("go_"+*target, "run", "-exec", "go_"+*target+"_exec", "x509_test_import.go")
-+	}
-+	if err := cmd.Run(); err != nil {
- 		t.Errorf("failed to run x509_test_import.go: %s", err)
- 	}
- }
- 
- const derCRLBase64 = "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"
- 
- const pemCRLBase64 = "LS0tLS1CRUdJTiBYNTA5IENSTC0tLS0tDQpNSUlCOWpDQ0FWOENBUUV3RFFZSktvWklodmNOQVFFRkJRQXdiREVhTUJnR0ExVUVDaE1SVWxOQklGTmxZM1Z5DQphWFI1SUVsdVl5NHhIakFjQmdOVkJBTVRGVkpUUVNCUWRXSnNhV01nVW05dmRDQkRRU0IyTVRFdU1Dd0dDU3FHDQpTSWIzRFFFSkFSWWZjbk5oYTJWdmJuSnZiM1J6YVdkdVFISnpZWE5sWTNWeWFYUjVMbU52YlJjTk1URXdNakl6DQpNVGt5T0RNd1doY05NVEV3T0RJeU1Ua3lPRE13V2pDQmpEQktBaEVBckRxb2g5RkhKSFhUN09QZ3V1bjQrQmNODQpNRGt4TVRBeU1UUXlOekE1V2pBbU1Bb0dBMVVkRlFRRENnRUpNQmdHQTFVZEdBUVJHQTh5TURBNU1URXdNakUwDQpNalExTlZvd1BnSVJBTEd6blowOTVQQjVhQU9MUGc1N2ZNTVhEVEF5TVRBeU16RTBOVEF4TkZvd0dqQVlCZ05WDQpIUmdFRVJnUE1qQXdNakV3TWpNeE5EVXdNVFJhb0RBd0xqQWZCZ05WSFNNRUdEQVdnQlQxVERGNlVRTS9MTmVMDQpsNWx2cUhHUXEzZzltekFMQmdOVkhSUUVCQUlDQUlRd0RRWUpLb1pJaHZjTkFRRUZCUUFEZ1lFQUZVNUFzNk16DQpxNVBSc2lmYW9iUVBHaDFhSkx5QytNczVBZ2MwYld5QTNHQWR4dXI1U3BQWmVSV0NCamlQL01FSEJXSkNsQkhQDQpHUmNxNXlJZDNFakRrYUV5eFJhK2k2N0x6dmhJNmMyOUVlNks5cFNZd2ppLzdSVWhtbW5Qclh0VHhsTDBsckxyDQptUVFKNnhoRFJhNUczUUE0Q21VZHNITnZicnpnbUNZcHZWRT0NCi0tLS0tRU5EIFg1MDkgQ1JMLS0tLS0NCg0K"
- 
---- src/runtime/crash_cgo_test.go
-+++ src/runtime/crash_cgo_test.go
-@@ -279,17 +279,17 @@ func testCgoPprof(t *testing.T, buildArg, runArg string) {
- 	}
- 	testenv.MustHaveGoRun(t)
- 
- 	exe, err := buildTestProg(t, "testprogcgo", buildArg)
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 
--	got, err := testenv.CleanCmdEnv(exec.Command(exe, runArg)).CombinedOutput()
-+	got, err := testenv.CleanCmdEnv(goExecCmd(exe, runArg)).CombinedOutput()
- 	if err != nil {
- 		if testenv.Builder() == "linux-amd64-alpine" {
- 			// See Issue 18243 and Issue 19938.
- 			t.Skipf("Skipping failing test on Alpine (golang.org/issue/18243). Ignoring error: %v", err)
- 		}
- 		t.Fatal(err)
- 	}
- 	fn := strings.TrimSpace(string(got))
---- src/runtime/crash_test.go
-+++ src/runtime/crash_test.go
-@@ -17,16 +17,35 @@ import (
- 	"runtime"
- 	"strconv"
- 	"strings"
- 	"sync"
- 	"testing"
- 	"time"
- )
- 
-+var target = flag.String("target", "", "if non empty, use 'go_target' to compile test files and 'go_target_exec' to run the binaries")
-+
-+func goCmd(t *testing.T) string {
-+	if *target != "" {
-+		return "go_" + *target
-+	}
-+	return testenv.GoToolPath(t)
-+}
-+
-+func goExecCmd(name string, arg ...string) *exec.Cmd {
-+	var cmd []string
-+	if *target != "" {
-+		cmd = append(cmd, "go_"+*target+"_exec")
-+	}
-+	cmd = append(cmd, name)
-+	cmd = append(cmd, arg...)
-+	return exec.Command(cmd[0], cmd[1:]...)
-+}
-+
- var toRemove []string
- 
- func TestMain(m *testing.M) {
- 	status := m.Run()
- 	for _, file := range toRemove {
- 		os.RemoveAll(file)
- 	}
- 	os.Exit(status)
-@@ -50,17 +69,17 @@ func runTestProg(t *testing.T, binary, name string, env ...string) string {
- 
- 	testenv.MustHaveGoBuild(t)
- 
- 	exe, err := buildTestProg(t, binary)
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 
--	cmd := testenv.CleanCmdEnv(exec.Command(exe, name))
-+	cmd := testenv.CleanCmdEnv(goExecCmd(exe, name))
- 	cmd.Env = append(cmd.Env, env...)
- 	if testing.Short() {
- 		cmd.Env = append(cmd.Env, "RUNTIME_TEST_SHORT=1")
- 	}
- 	var b bytes.Buffer
- 	cmd.Stdout = &b
- 	cmd.Stderr = &b
- 	if err := cmd.Start(); err != nil {
-@@ -125,17 +144,17 @@ func buildTestProg(t *testing.T, binary string, flags ...string) (string, error)
- 		name += "_" + strings.Join(flags, "_")
- 	}
- 	target, ok := testprog.target[name]
- 	if ok {
- 		return target.exe, target.err
- 	}
- 
- 	exe := filepath.Join(testprog.dir, name+".exe")
--	cmd := exec.Command(testenv.GoToolPath(t), append([]string{"build", "-o", exe}, flags...)...)
-+	cmd := exec.Command(goCmd(t), append([]string{"build", "-o", exe}, flags...)...)
- 	cmd.Dir = "testdata/" + binary
- 	out, err := testenv.CleanCmdEnv(cmd).CombinedOutput()
- 	if err != nil {
- 		target.err = fmt.Errorf("building %s %v: %v\n%s", binary, flags, err, out)
- 		testprog.target[name] = target
- 		return "", target.err
- 	}
- 	target.exe = exe
-@@ -456,17 +475,17 @@ func TestPanicLoop(t *testing.T) {
- func TestMemPprof(t *testing.T) {
- 	testenv.MustHaveGoRun(t)
- 
- 	exe, err := buildTestProg(t, "testprog")
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 
--	got, err := testenv.CleanCmdEnv(exec.Command(exe, "MemProf")).CombinedOutput()
-+	got, err := testenv.CleanCmdEnv(goExecCmd(exe, "MemProf")).CombinedOutput()
- 	if err != nil {
- 		t.Fatal(err)
- 	}
- 	fn := strings.TrimSpace(string(got))
- 	defer os.Remove(fn)
- 
- 	for try := 0; try < 2; try++ {
- 		cmd := testenv.CleanCmdEnv(exec.Command(testenv.GoToolPath(t), "tool", "pprof", "-alloc_space", "-top"))
---- src/runtime/crash_unix_test.go
-+++ src/runtime/crash_unix_test.go
-@@ -244,17 +244,17 @@ func testPanicSystemstackInternal() {
- }
- 
- func TestSignalExitStatus(t *testing.T) {
- 	testenv.MustHaveGoBuild(t)
- 	exe, err := buildTestProg(t, "testprog")
- 	if err != nil {
- 		t.Fatal(err)
- 	}
--	err = testenv.CleanCmdEnv(exec.Command(exe, "SignalExitStatus")).Run()
-+	err = testenv.CleanCmdEnv(goExecCmd(exe, "SignalExitStatus")).Run()
- 	if err == nil {
- 		t.Error("test program succeeded unexpectedly")
- 	} else if ee, ok := err.(*exec.ExitError); !ok {
- 		t.Errorf("error (%v) has type %T; expected exec.ExitError", err, err)
- 	} else if ws, ok := ee.Sys().(syscall.WaitStatus); !ok {
- 		t.Errorf("error.Sys (%v) has type %T; expected syscall.WaitStatus", ee.Sys(), ee.Sys())
- 	} else if !ws.Signaled() || ws.Signal() != syscall.SIGTERM {
- 		t.Errorf("got %v; expected SIGTERM", ee)
diff --git a/go/patch/go-1.11.2/go5.patch b/go/patch/go-1.11.2/go5.patch
deleted file mode 100644
index c0807e9b..00000000
--- a/go/patch/go-1.11.2/go5.patch
+++ /dev/null
@@ -1,106 +0,0 @@
-diff --git src/runtime/crash_test.go src/runtime/crash_test.go
-index 81cf5df42d..3607992788 100644
---- src/runtime/crash_test.go
-+++ src/runtime/crash_test.go
-@@ -219,22 +219,27 @@ func testDeadlock(t *testing.T, name string) {
- }
- 
- func TestSimpleDeadlock(t *testing.T) {
-+        t.Skip("deadlock detection fails with external linker")
- 	testDeadlock(t, "SimpleDeadlock")
- }
- 
- func TestInitDeadlock(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	testDeadlock(t, "InitDeadlock")
- }
- 
- func TestLockedDeadlock(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	testDeadlock(t, "LockedDeadlock")
- }
- 
- func TestLockedDeadlock2(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	testDeadlock(t, "LockedDeadlock2")
- }
- 
- func TestGoexitDeadlock(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	output := runTestProg(t, "testprog", "GoexitDeadlock")
- 	want := "no goroutines (main called runtime.Goexit) - deadlock!"
- 	if !strings.Contains(output, want) {
-@@ -271,6 +276,7 @@ panic: again
- }
- 
- func TestGoexitCrash(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	output := runTestProg(t, "testprog", "GoexitExit")
- 	want := "no goroutines (main called runtime.Goexit) - deadlock!"
- 	if !strings.Contains(output, want) {
-@@ -329,6 +335,7 @@ func TestBreakpoint(t *testing.T) {
- }
- 
- func TestGoexitInPanic(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	// see issue 8774: this code used to trigger an infinite recursion
- 	output := runTestProg(t, "testprog", "GoexitInPanic")
- 	want := "fatal error: no goroutines (main called runtime.Goexit) - deadlock!"
-@@ -393,6 +400,7 @@ func TestPanicAfterGoexit(t *testing.T) {
- }
- 
- func TestRecoveredPanicAfterGoexit(t *testing.T) {
-+	t.Skip("deadlock detection fails with external linker")
- 	output := runTestProg(t, "testprog", "RecoveredPanicAfterGoexit")
- 	want := "fatal error: no goroutines (main called runtime.Goexit) - deadlock!"
- 	if !strings.HasPrefix(output, want) {
-diff --git src/runtime/proc_test.go src/runtime/proc_test.go
-index ad325987ac..d9d6feb498 100644
---- src/runtime/proc_test.go
-+++ src/runtime/proc_test.go
-@@ -373,9 +373,10 @@ func TestGCFairness2(t *testing.T) {
- 
- func TestNumGoroutine(t *testing.T) {
- 	output := runTestProg(t, "testprog", "NumGoroutine")
--	want := "1\n"
--	if output != want {
--		t.Fatalf("want %q, got %q", want, output)
-+	want1 := "1\n"
-+	want2 := "2\n"
-+	if output != want1 && out != want2 {
-+		t.Fatalf("want %q, got %q", want1, output)
- 	}
- 
- 	buf := make([]byte, 1<<20)
-diff --git test/fixedbugs/bug429_run.go test/fixedbugs/bug429_run.go
-index 30298de97b..3301a11ad9 100644
---- test/fixedbugs/bug429_run.go
-+++ test/fixedbugs/bug429_run.go
-@@ -1,5 +1,5 @@
- // +build !nacl,!js
--// runtarget
-+// skip
- 
- // Copyright 2014 The Go Authors. All rights reserved.
- // Use of this source code is governed by a BSD-style
-diff --git test/goprint.go test/goprint.go
-index 57eeac53a8..5951d4694f 100644
---- test/goprint.go
-+++ test/goprint.go
-@@ -8,14 +8,9 @@
- 
- package main
- 
--import (
--	"runtime"
--	"time"
--)
-+import "time"
- 
- func main() {
- 	go println(42, true, false, true, 1.5, "world", (chan int)(nil), []int(nil), (map[string]int)(nil), (func())(nil), byte(255))
--	for runtime.NumGoroutine() > 1 {
--		time.Sleep(10*time.Millisecond)
--	}
-+	time.Sleep(100*time.Millisecond)
- }
diff --git a/go/patch/go-1.11.2/go6.patch b/go/patch/go-1.11.2/go6.patch
deleted file mode 100644
index 04134c77..00000000
--- a/go/patch/go-1.11.2/go6.patch
+++ /dev/null
@@ -1,149 +0,0 @@
-diff --git src/encoding/gob/encoder_test.go src/encoding/gob/encoder_test.go
-index dc9bbcf35d..10c30a91af 100644
---- src/encoding/gob/encoder_test.go
-+++ src/encoding/gob/encoder_test.go
-@@ -1131,13 +1131,7 @@ func TestBadData(t *testing.T) {
- 
- // TestHugeWriteFails tests that enormous messages trigger an error.
- func TestHugeWriteFails(t *testing.T) {
--	if runtime.GOARCH == "wasm" {
--		t.Skip("out of memory on wasm")
--	}
--	if testing.Short() {
--		// Requires allocating a monster, so don't do this from all.bash.
--		t.Skip("skipping huge allocation in short mode")
--	}
-+	t.Skip("skipping test due to huge memory requirement")
- 	huge := make([]byte, tooBig)
- 	huge[0] = 7 // Make sure it's not all zeros.
- 	buf := new(bytes.Buffer)
-diff --git src/runtime/crash_cgo_test.go src/runtime/crash_cgo_test.go
-index 9ff4bbe121..5fa1340cb2 100644
---- src/runtime/crash_cgo_test.go
-+++ src/runtime/crash_cgo_test.go
-@@ -238,14 +238,7 @@ func TestCgoCCodeSIGPROF(t *testing.T) {
- }
- 
- func TestCgoCrashTraceback(t *testing.T) {
--	t.Parallel()
--	switch platform := runtime.GOOS + "/" + runtime.GOARCH; platform {
--	case "darwin/amd64":
--	case "linux/amd64":
--	case "linux/ppc64le":
--	default:
--		t.Skipf("not yet supported on %s", platform)
--	}
-+	t.Skipf("skip running remotely")
- 	got := runTestProg(t, "testprogcgo", "CrashTraceback")
- 	for i := 1; i <= 3; i++ {
- 		if !strings.Contains(got, fmt.Sprintf("cgo symbolizer:%d", i)) {
-@@ -264,10 +257,7 @@ func TestCgoTracebackContext(t *testing.T) {
- }
- 
- func testCgoPprof(t *testing.T, buildArg, runArg string) {
--	t.Parallel()
--	if runtime.GOOS != "linux" || (runtime.GOARCH != "amd64" && runtime.GOARCH != "ppc64le") {
--		t.Skipf("not yet supported on %s/%s", runtime.GOOS, runtime.GOARCH)
--	}
-+        t.Skipf("skip pprof test")
- 	testenv.MustHaveGoRun(t)
- 
- 	exe, err := buildTestProg(t, "testprogcgo", buildArg)
-diff --git src/runtime/crash_test.go src/runtime/crash_test.go
-index 3607992788..e53ffb6a81 100644
---- src/runtime/crash_test.go
-+++ src/runtime/crash_test.go
-@@ -481,6 +481,7 @@ func TestPanicLoop(t *testing.T) {
- }
- 
- func TestMemPprof(t *testing.T) {
-+	t.Skipf("skip pprof test")
- 	testenv.MustHaveGoRun(t)
- 
- 	exe, err := buildTestProg(t, "testprog")
-diff --git src/runtime/crash_unix_test.go src/runtime/crash_unix_test.go
-index 02891ec1ad..fd2723f16e 100644
---- src/runtime/crash_unix_test.go
-+++ src/runtime/crash_unix_test.go
-@@ -174,9 +174,7 @@ func TestPanicSystemstack(t *testing.T) {
- 	// The GOTRACEBACK=crash handler takes 0.1 seconds even if
- 	// it's not writing a core file and potentially much longer if
- 	// it is. Skip in short mode.
--	if testing.Short() {
--		t.Skip("Skipping in short mode (GOTRACEBACK=crash is slow)")
--	}
-+	t.Skip("Skipping (GOTRACEBACK=crash hangs on arm)")
- 
- 	if runtime.Sigisblocked(int(syscall.SIGQUIT)) {
- 		t.Skip("skipping; SIGQUIT is blocked, see golang.org/issue/19196")
-@@ -244,6 +242,7 @@ func testPanicSystemstackInternal() {
- }
- 
- func TestSignalExitStatus(t *testing.T) {
-+        t.Skipf("skip running remotely")
- 	testenv.MustHaveGoBuild(t)
- 	exe, err := buildTestProg(t, "testprog")
- 	if err != nil {
-diff --git src/runtime/fastlog2_test.go src/runtime/fastlog2_test.go
-index ae0f40b2bb..a93933d7ac 100644
---- src/runtime/fastlog2_test.go
-+++ src/runtime/fastlog2_test.go
-@@ -16,11 +16,7 @@ func TestFastLog2(t *testing.T) {
- 	const randomBitCount = 26
- 	var e float64
- 
--	inc := 1
--	if testing.Short() {
--		// Check 1K total values, down from 64M.
--		inc = 1 << 16
--	}
-+	inc := 1 << 16
- 	for i := 1; i < 1<<randomBitCount; i += inc {
- 		l, fl := math.Log2(float64(i)), runtime.Fastlog2(float64(i))
- 		d := l - fl
-diff --git src/runtime/hash_test.go src/runtime/hash_test.go
-index 7b8ebc4f3c..9fc5b995fc 100644
---- src/runtime/hash_test.go
-+++ src/runtime/hash_test.go
-@@ -164,9 +164,7 @@ func TestSmhasherTwoNonzero(t *testing.T) {
- 	if GOARCH == "wasm" {
- 		t.Skip("Too slow on wasm")
- 	}
--	if testing.Short() {
--		t.Skip("Skipping in short mode")
--	}
-+	t.Skip("skipping test due to huge memory requirement")
- 	h := newHashSet()
- 	for n := 2; n <= 16; n++ {
- 		twoNonZero(h, n)
-@@ -273,9 +271,7 @@ func TestSmhasherPermutation(t *testing.T) {
- 	if GOARCH == "wasm" {
- 		t.Skip("Too slow on wasm")
- 	}
--	if testing.Short() {
--		t.Skip("Skipping in short mode")
--	}
-+	t.Skip("skipping test due to huge memory requirement")
- 	permutation(t, []uint32{0, 1, 2, 3, 4, 5, 6, 7}, 8)
- 	permutation(t, []uint32{0, 1 << 29, 2 << 29, 3 << 29, 4 << 29, 5 << 29, 6 << 29, 7 << 29}, 8)
- 	permutation(t, []uint32{0, 1}, 20)
-diff --git src/runtime/pprof/pprof_test.go src/runtime/pprof/pprof_test.go
-index 44d514393e..f46f00894c 100644
---- src/runtime/pprof/pprof_test.go
-+++ src/runtime/pprof/pprof_test.go
-@@ -283,14 +283,7 @@ func profileOk(t *testing.T, need []string, prof bytes.Buffer, duration time.Dur
- func TestCPUProfileWithFork(t *testing.T) {
- 	testenv.MustHaveExec(t)
- 
--	heap := 1 << 30
--	if runtime.GOOS == "android" {
--		// Use smaller size for Android to avoid crash.
--		heap = 100 << 20
--	}
--	if testing.Short() {
--		heap = 100 << 20
--	}
-+	heap = 100 << 20
- 	// This makes fork slower.
- 	garbage := make([]byte, heap)
- 	// Need to touch the slice, otherwise it won't be paged in.
diff --git a/go/push_goroot b/go/push_goroot
deleted file mode 100755
index 0d7706e1..00000000
--- a/go/push_goroot
+++ /dev/null
@@ -1,29 +0,0 @@
-#!/bin/bash
-set -e -o pipefail
-
-# This script copies a locally built GOROOT to a remote device.
-#
-# Usage: push_goroot <target>...
-#
-# This script can work with both ChromeOS/Android devices.
-#
-# It uses "target_tmpdir" to figure out where to copy GOROOT on the device.
-# It uses "target_sh" to remotely execute commands on the device.
-# It uses "target_cp" to transfer files to the device.
-
-goroot="$(target_tmpdir)/goroot"
-for target in "$@"
-do
-	echo -n "pushing goroot to ${target} ... "
-	target_sh ${target} "rm -rf ${goroot}"
-	target_sh ${target} "mkdir -p ${goroot}/pkg"
-
-	cd "$(go_${target} env GOROOT)"
-	pkgdir="pkg/$(go_${target} env GOOS)_$(go_${target} env GOARCH)"
-	target_cp "${pkgdir}" ${target}:${goroot}/pkg
-
-	target_cp "src" ${target}:${goroot}
-	target_cp "lib" ${target}:${goroot}
-	[[ -d test ]] && target_cp "test" ${target}:${goroot}
-	echo "done"
-done
diff --git a/go/test_go b/go/test_go
deleted file mode 100755
index 548712f5..00000000
--- a/go/test_go
+++ /dev/null
@@ -1,81 +0,0 @@
-#!/bin/bash
-
-# This script runs tests for the Go toolchain on target devices.
-# It can be used for both ChromeOS and Android targets.
-#
-# Many of the test drivers that come from upstream do not support
-# cross-compiling and running the tests remotely. The patches in
-# the ./patch/ directory must be applied to the upstream sources
-# to add this support.
-#
-# Usage: test_go [-v] [-vv] [-full] <target>...
-#   -v: enable verbose test output from compiler tests.
-#   -v: enable verbose test output from standard library tests.
-#   -full: run all standard library tests (without the -short flag).
-
-verbose_run_test=""
-verbose_go_test=""
-testflags="-short"
-while [[ "$1" == -* ]]
-do
-	case "$1" in
-		-v) verbose_run_test="-v" ;;
-		-vv) verbose_go_test="-v" ;;
-		-full) testflags="-timeout=2h" ;;
-		*) echo "unrecognized flag: $1" ;;
-	esac
-	shift
-done
-
-go_local build -o runtest test/run.go
-runtest="${PWD}/runtest"
-
-function run_test()
-	{
-	GOOS="$(go_${target} env GOOS)" GOARCH="$(go_${target} env GOARCH)" ${runtest} -n=1 ${verbose_run_test} -show_skips -summary -target="${target}" "$@"
-	}
-
-function go_test()
-	{
-	go_${target} test -p=1 ${verbose_go_test} -exec="go_${target}_exec" ${testflags} "$@"
-	}
-
-function go_test_target()
-	{
-	go_local test -p=1 ${verbose_go_test} ${testflags} "$@" -target="${target}"
-	}
-
-for target in "$@"
-do
-	echo
-	echo "## ${target}"
-	push_goroot ${target}
-
-	echo
-	echo "# test"
-	(cd test && run_test)
-
-	echo
-	echo "# std"
-	go_test std
-
-	echo
-	echo "# GOMAXPROCS=2 -cpu=1,2,4 runtime"
-	GOMAXPROCS=2 go_test -cpu=1,2,4 runtime
-
-	echo
-	echo "# -cpu=10 sync"
-	go_test -cpu=10 sync
-
-	echo
-	echo "# runtime crypto/x509 -target=${target}"
-	go_test_target runtime crypto/x509
-
-	echo
-	echo "# misc/cgo/{stdio,life}"
-	run_test misc/cgo/{stdio,life}
-
-	echo
-	echo "# misc/cgo/{test,testtls,nocgo}"
-	GOTRACEBACK=2 go_test ./misc/cgo/{test,testtls,nocgo}
-done
diff --git a/image_chromeos.py b/image_chromeos.py
deleted file mode 100755
index 38300591..00000000
--- a/image_chromeos.py
+++ /dev/null
@@ -1,562 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Script to image a ChromeOS device.
-
-This script images a remote ChromeOS device with a specific image."
-"""
-
-
-__author__ = "asharif@google.com (Ahmad Sharif)"
-
-import argparse
-import filecmp
-import glob
-import os
-import re
-import shutil
-import sys
-import tempfile
-import time
-
-from cros_utils import command_executer
-from cros_utils import locks
-from cros_utils import logger
-from cros_utils import misc
-from cros_utils.file_utils import FileUtils
-
-
-checksum_file = "/usr/local/osimage_checksum_file"
-lock_file = "/tmp/image_chromeos_lock/image_chromeos_lock"
-
-
-def Usage(parser, message):
-    print("ERROR: %s" % message)
-    parser.print_help()
-    sys.exit(0)
-
-
-def CheckForCrosFlash(chromeos_root, remote, log_level):
-    cmd_executer = command_executer.GetCommandExecuter(log_level=log_level)
-
-    # Check to see if remote machine has cherrypy, ctypes
-    command = "python -c 'import cherrypy, ctypes'"
-    ret = cmd_executer.CrosRunCommand(
-        command, chromeos_root=chromeos_root, machine=remote
-    )
-    logger.GetLogger().LogFatalIf(
-        ret == 255, f"Failed ssh to {remote} (for checking cherrypy)"
-    )
-    logger.GetLogger().LogFatalIf(
-        ret != 0,
-        f"Failed to find cherrypy or ctypes on '{remote}', "
-        "cros flash cannot work.",
-    )
-
-
-def DisableCrosBeeps(chromeos_root, remote, log_level):
-    """Disable annoying chromebooks beeps after reboots."""
-    cmd_executer = command_executer.GetCommandExecuter(log_level=log_level)
-
-    command = "/usr/bin/futility gbb --set --flash --flags=0x1"
-    logger.GetLogger().LogOutput("Trying to disable beeping.")
-
-    ret, o, _ = cmd_executer.CrosRunCommandWOutput(
-        command, chromeos_root=chromeos_root, machine=remote
-    )
-    if ret != 0:
-        logger.GetLogger().LogOutput(o)
-        logger.GetLogger().LogOutput("Failed to disable beeps.")
-
-
-def FindChromeOSImage(image_file, chromeos_root):
-    """Find path for ChromeOS image inside chroot.
-
-    This function could be called with image paths that are either inside
-    or outside the chroot.  In either case the path needs to be translated
-    to an real/absolute path inside the chroot.
-    Example input paths:
-    /usr/local/google/home/uname/chromeos/out/tmp/my-test-images/image
-    ~/chromiumos/src/build/images/board/latest/image
-    /tmp/peppy-release/R67-1235.0.0/image
-
-    Corresponding example output paths:
-    /tmp/my-test-images/image
-    /mnt/host/source/src/build/images/board/latest/image
-    /tmp/peppy-release/R67-1235.0,0/image
-    """
-
-    sys.path.insert(0, chromeos_root)
-
-    from chromite.lib import path_util
-
-    return path_util.ToChrootPath(image_file, source_path=chromeos_root)
-
-
-def DoImage(argv):
-    """Image ChromeOS."""
-
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "-c",
-        "--chromeos_root",
-        dest="chromeos_root",
-        help="Target directory for ChromeOS installation.",
-    )
-    parser.add_argument("-r", "--remote", dest="remote", help="Target device.")
-    parser.add_argument(
-        "-i", "--image", dest="image", help="Image binary file."
-    )
-    parser.add_argument(
-        "-b", "--board", dest="board", help="Target board override."
-    )
-    parser.add_argument(
-        "-f",
-        "--force",
-        dest="force",
-        action="store_true",
-        default=False,
-        help="Force an image even if it is non-test.",
-    )
-    parser.add_argument(
-        "-n",
-        "--no_lock",
-        dest="no_lock",
-        default=False,
-        action="store_true",
-        help="Do not attempt to lock remote before imaging.  "
-        "This option should only be used in cases where the "
-        "exclusive lock has already been acquired (e.g. in "
-        "a script that calls this one).",
-    )
-    parser.add_argument(
-        "-l",
-        "--logging_level",
-        dest="log_level",
-        default="verbose",
-        help="Amount of logging to be used. Valid levels are "
-        "'quiet', 'average', and 'verbose'.",
-    )
-    parser.add_argument("-a", "--image_args", dest="image_args")
-    parser.add_argument(
-        "--keep_stateful",
-        dest="keep_stateful",
-        default=False,
-        action="store_true",
-        help="Do not clobber the stateful partition.",
-    )
-
-    options = parser.parse_args(argv[1:])
-
-    if not options.log_level in command_executer.LOG_LEVEL:
-        Usage(parser, "--logging_level must be 'quiet', 'average' or 'verbose'")
-    else:
-        log_level = options.log_level
-
-    # Common initializations
-    cmd_executer = command_executer.GetCommandExecuter(log_level=log_level)
-    l = logger.GetLogger()
-
-    if options.chromeos_root is None:
-        Usage(parser, "--chromeos_root must be set")
-
-    if options.remote is None:
-        Usage(parser, "--remote must be set")
-
-    options.chromeos_root = os.path.expanduser(options.chromeos_root)
-
-    if options.board is None:
-        board = cmd_executer.CrosLearnBoard(
-            options.chromeos_root, options.remote
-        )
-    else:
-        board = options.board
-
-    if options.image is None:
-        images_dir = misc.GetImageDir(options.chromeos_root, board)
-        image = os.path.join(images_dir, "latest", "chromiumos_test_image.bin")
-        if not os.path.exists(image):
-            image = os.path.join(images_dir, "latest", "chromiumos_image.bin")
-        is_xbuddy_image = False
-    else:
-        image = options.image
-        is_xbuddy_image = image.startswith("xbuddy://")
-        if not is_xbuddy_image:
-            image = os.path.expanduser(image)
-
-    if not is_xbuddy_image:
-        image = os.path.realpath(image)
-
-    if not os.path.exists(image) and not is_xbuddy_image:
-        Usage(parser, "Image file: " + image + " does not exist!")
-
-    try:
-        should_unlock = False
-        if not options.no_lock:
-            try:
-                _ = locks.AcquireLock(
-                    list(options.remote.split()), options.chromeos_root
-                )
-                should_unlock = True
-            except Exception as e:
-                raise RuntimeError("Error acquiring machine: %s" % str(e))
-
-        reimage = False
-        local_image = False
-        if not is_xbuddy_image:
-            local_image = True
-            image_checksum = FileUtils().Md5File(image, log_level=log_level)
-
-            command = "cat " + checksum_file
-            ret, device_checksum, _ = cmd_executer.CrosRunCommandWOutput(
-                command,
-                chromeos_root=options.chromeos_root,
-                machine=options.remote,
-            )
-
-            device_checksum = device_checksum.strip()
-            image_checksum = str(image_checksum)
-
-            l.LogOutput("Image checksum: " + image_checksum)
-            l.LogOutput("Device checksum: " + device_checksum)
-
-            if image_checksum != device_checksum:
-                [found, located_image] = LocateOrCopyImage(
-                    options.chromeos_root, image, board=board
-                )
-
-                reimage = True
-                l.LogOutput("Checksums do not match. Re-imaging...")
-
-                chroot_image = FindChromeOSImage(
-                    located_image, options.chromeos_root
-                )
-
-                is_test_image = IsImageModdedForTest(
-                    options.chromeos_root, chroot_image, log_level
-                )
-
-                if not is_test_image and not options.force:
-                    logger.GetLogger().LogFatal(
-                        "Have to pass --force to image a " "non-test image!"
-                    )
-        else:
-            reimage = True
-            found = True
-            l.LogOutput("Using non-local image; Re-imaging...")
-
-        if reimage:
-            # If the device has /tmp mounted as noexec, image_to_live.sh can fail.
-            command = "mount -o remount,rw,exec /tmp"
-            cmd_executer.CrosRunCommand(
-                command,
-                chromeos_root=options.chromeos_root,
-                machine=options.remote,
-            )
-
-            # Check to see if cros flash will work for the remote machine.
-            CheckForCrosFlash(options.chromeos_root, options.remote, log_level)
-
-            # Disable the annoying chromebook beeps after reboot.
-            DisableCrosBeeps(options.chromeos_root, options.remote, log_level)
-
-            cros_flash_args = [
-                "cros",
-                "flash",
-                "--board=%s" % board,
-            ]
-            if not options.keep_stateful:
-                cros_flash_args.append("--clobber-stateful")
-            # New arguments should be added here.
-
-            # The last two arguments are positional and have to be at the end.
-            cros_flash_args.append(options.remote)
-            if local_image:
-                cros_flash_args.append(chroot_image)
-            else:
-                cros_flash_args.append(image)
-
-            command = " ".join(cros_flash_args)
-
-            # Workaround for crosbug.com/35684.
-            os.chmod(misc.GetChromeOSKeyFile(options.chromeos_root), 0o600)
-
-            if log_level == "average":
-                cmd_executer.SetLogLevel("verbose")
-            retries = 0
-            while True:
-                if log_level == "quiet":
-                    l.LogOutput("CMD : %s" % command)
-                ret = cmd_executer.ChrootRunCommand(
-                    options.chromeos_root, command, command_timeout=1800
-                )
-                if ret == 0 or retries >= 2:
-                    break
-                retries += 1
-                if log_level == "quiet":
-                    l.LogOutput("Imaging failed. Retry # %d." % retries)
-
-            if log_level == "average":
-                cmd_executer.SetLogLevel(log_level)
-
-            logger.GetLogger().LogFatalIf(ret, "Image command failed")
-
-            # Unfortunately cros_image_to_target.py sometimes returns early when the
-            # machine isn't fully up yet.
-            ret = EnsureMachineUp(
-                options.chromeos_root, options.remote, log_level
-            )
-
-            # If this is a non-local image, then the ret returned from
-            # EnsureMachineUp is the one that will be returned by this function;
-            # in that case, make sure the value in 'ret' is appropriate.
-            if not local_image and ret:
-                ret = 0
-            else:
-                ret = 1
-
-            if local_image:
-                if log_level == "average":
-                    l.LogOutput("Verifying image.")
-                command = "echo %s > %s && chmod -w %s" % (
-                    image_checksum,
-                    checksum_file,
-                    checksum_file,
-                )
-                ret = cmd_executer.CrosRunCommand(
-                    command,
-                    chromeos_root=options.chromeos_root,
-                    machine=options.remote,
-                )
-                logger.GetLogger().LogFatalIf(ret, "Writing checksum failed.")
-
-                successfully_imaged = VerifyChromeChecksum(
-                    options.chromeos_root,
-                    chroot_image,
-                    options.remote,
-                    log_level,
-                )
-                logger.GetLogger().LogFatalIf(
-                    not successfully_imaged, "Image verification failed!"
-                )
-                TryRemountPartitionAsRW(
-                    options.chromeos_root, options.remote, log_level
-                )
-
-            if not found:
-                temp_dir = os.path.dirname(located_image)
-                l.LogOutput("Deleting temp image dir: %s" % temp_dir)
-                shutil.rmtree(temp_dir)
-            l.LogOutput("Image updated.")
-        else:
-            l.LogOutput("Checksums match, skip image update and reboot.")
-            command = "reboot && exit"
-            _ = cmd_executer.CrosRunCommand(
-                command,
-                chromeos_root=options.chromeos_root,
-                machine=options.remote,
-            )
-            # Wait 30s after reboot.
-            time.sleep(30)
-
-    finally:
-        if should_unlock:
-            locks.ReleaseLock(
-                list(options.remote.split()), options.chromeos_root
-            )
-
-    return ret
-
-
-def LocateOrCopyImage(chromeos_root, image, board=None):
-    l = logger.GetLogger()
-    if board is None:
-        board_glob = "*"
-    else:
-        board_glob = board
-
-    chromeos_root_realpath = os.path.realpath(chromeos_root)
-    image = os.path.realpath(image)
-
-    if image.startswith("%s/" % chromeos_root_realpath):
-        return [True, image]
-
-    # First search within the existing build dirs for any matching files.
-    images_glob = "%s/src/build/images/%s/*/*.bin" % (
-        chromeos_root_realpath,
-        board_glob,
-    )
-    images_list = glob.glob(images_glob)
-    for potential_image in images_list:
-        if filecmp.cmp(potential_image, image):
-            l.LogOutput(
-                "Found matching image %s in chromeos_root." % potential_image
-            )
-            return [True, potential_image]
-    # We did not find an image. Copy it in the src dir and return the copied
-    # file.
-    if board is None:
-        board = ""
-    base_dir = "%s/src/build/images/%s" % (chromeos_root_realpath, board)
-    if not os.path.isdir(base_dir):
-        os.makedirs(base_dir)
-    temp_dir = tempfile.mkdtemp(prefix="%s/tmp" % base_dir)
-    new_image = "%s/%s" % (temp_dir, os.path.basename(image))
-    l.LogOutput(
-        "No matching image found. Copying %s to %s" % (image, new_image)
-    )
-    shutil.copyfile(image, new_image)
-    return [False, new_image]
-
-
-def GetImageMountCommand(image, rootfs_mp, stateful_mp):
-    image_dir = os.path.dirname(image)
-    image_file = os.path.basename(image)
-    mount_command = (
-        "cd /mnt/host/source/src/scripts &&"
-        "./mount_gpt_image.sh --from=%s --image=%s"
-        " --safe --read_only"
-        " --rootfs_mountpt=%s"
-        " --stateful_mountpt=%s"
-        % (image_dir, image_file, rootfs_mp, stateful_mp)
-    )
-    return mount_command
-
-
-def MountImage(
-    chromeos_root,
-    image,
-    rootfs_mp,
-    stateful_mp,
-    log_level,
-    unmount=False,
-    extra_commands="",
-):
-    cmd_executer = command_executer.GetCommandExecuter(log_level=log_level)
-    command = GetImageMountCommand(image, rootfs_mp, stateful_mp)
-    if unmount:
-        command = "%s --unmount" % command
-    if extra_commands:
-        command = "%s ; %s" % (command, extra_commands)
-    ret, out, _ = cmd_executer.ChrootRunCommandWOutput(chromeos_root, command)
-    logger.GetLogger().LogFatalIf(ret, "Mount/unmount command failed!")
-    return out
-
-
-def IsImageModdedForTest(chromeos_root, image, log_level):
-    if log_level != "verbose":
-        log_level = "quiet"
-    command = "mktemp -d"
-    cmd_executer = command_executer.GetCommandExecuter(log_level=log_level)
-    _, rootfs_mp, _ = cmd_executer.ChrootRunCommandWOutput(
-        chromeos_root, command
-    )
-    _, stateful_mp, _ = cmd_executer.ChrootRunCommandWOutput(
-        chromeos_root, command
-    )
-    rootfs_mp = rootfs_mp.strip()
-    stateful_mp = stateful_mp.strip()
-    lsb_release_file = os.path.join(rootfs_mp, "etc/lsb-release")
-    extra = "grep CHROMEOS_RELEASE_TRACK %s | grep -i test" % lsb_release_file
-    output = MountImage(
-        chromeos_root,
-        image,
-        rootfs_mp,
-        stateful_mp,
-        log_level,
-        extra_commands=extra,
-    )
-    is_test_image = re.search("test", output, re.IGNORECASE)
-    MountImage(
-        chromeos_root, image, rootfs_mp, stateful_mp, log_level, unmount=True
-    )
-    return is_test_image
-
-
-def VerifyChromeChecksum(chromeos_root, image, remote, log_level):
-    command = "mktemp -d"
-    cmd_executer = command_executer.GetCommandExecuter(log_level=log_level)
-    _, rootfs_mp, _ = cmd_executer.ChrootRunCommandWOutput(
-        chromeos_root, command
-    )
-    _, stateful_mp, _ = cmd_executer.ChrootRunCommandWOutput(
-        chromeos_root, command
-    )
-    rootfs_mp = rootfs_mp.strip()
-    stateful_mp = stateful_mp.strip()
-    chrome_file = "%s/opt/google/chrome/chrome" % rootfs_mp
-    extra = "md5sum %s" % chrome_file
-    out = MountImage(
-        chromeos_root,
-        image,
-        rootfs_mp,
-        stateful_mp,
-        log_level,
-        extra_commands=extra,
-    )
-    image_chrome_checksum = out.strip().split()[0]
-    MountImage(
-        chromeos_root, image, rootfs_mp, stateful_mp, log_level, unmount=True
-    )
-
-    command = "md5sum /opt/google/chrome/chrome"
-    [_, o, _] = cmd_executer.CrosRunCommandWOutput(
-        command, chromeos_root=chromeos_root, machine=remote
-    )
-    device_chrome_checksum = o.split()[0]
-    return image_chrome_checksum.strip() == device_chrome_checksum.strip()
-
-
-# Remount partition as writable.
-# TODO: auto-detect if an image is built using --noenable_rootfs_verification.
-def TryRemountPartitionAsRW(chromeos_root, remote, log_level):
-    l = logger.GetLogger()
-    cmd_executer = command_executer.GetCommandExecuter(log_level=log_level)
-    command = "sudo mount -o remount,rw /"
-    ret = cmd_executer.CrosRunCommand(
-        command,
-        chromeos_root=chromeos_root,
-        machine=remote,
-        terminated_timeout=10,
-    )
-    if ret:
-        ## Safely ignore.
-        l.LogWarning(
-            "Failed to remount partition as rw, "
-            "probably the image was not built with "
-            '"--noenable_rootfs_verification", '
-            "you can safely ignore this."
-        )
-    else:
-        l.LogOutput("Re-mounted partition as writable.")
-
-
-def EnsureMachineUp(chromeos_root, remote, log_level):
-    l = logger.GetLogger()
-    cmd_executer = command_executer.GetCommandExecuter(log_level=log_level)
-    timeout = 600
-    magic = "abcdefghijklmnopqrstuvwxyz"
-    command = "echo %s" % magic
-    start_time = time.time()
-    while True:
-        current_time = time.time()
-        if current_time - start_time > timeout:
-            l.LogError(
-                "Timeout of %ss reached. Machine still not up. Aborting."
-                % timeout
-            )
-            return False
-        ret = cmd_executer.CrosRunCommand(
-            command, chromeos_root=chromeos_root, machine=remote
-        )
-        if not ret:
-            return True
-
-
-if __name__ == "__main__":
-    retval = DoImage(sys.argv)
-    sys.exit(retval)
diff --git a/llvm_extra/create_ebuild_file.py b/llvm_extra/create_ebuild_file.py
old mode 100755
new mode 100644
index 2a1a69a3..3816d5fc
--- a/llvm_extra/create_ebuild_file.py
+++ b/llvm_extra/create_ebuild_file.py
@@ -1,5 +1,3 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
 # Copyright 2018 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -151,7 +149,3 @@ def main():
         outfile.write("".join(text))
 
     return 0
-
-
-if __name__ == "__main__":
-    sys.exit(main())
diff --git a/llvm_patches/-lldb-Add-terminfo-dependency-for-ncurses-support.patch b/llvm_patches/-lldb-Add-terminfo-dependency-for-ncurses-support.patch
new file mode 100644
index 00000000..868b6a56
--- /dev/null
+++ b/llvm_patches/-lldb-Add-terminfo-dependency-for-ncurses-support.patch
@@ -0,0 +1,58 @@
+From 41875593066a603ff4cd746a430e481fd7d3a180 Mon Sep 17 00:00:00 2001
+From: Jordan R Abrahams-Whitehead <ajordanr@google.com>
+Date: Tue, 1 Oct 2024 18:41:28 +0000
+Subject: [PATCH] [lldb] Add terminfo dependency for ncurses support
+
+For ChromiumOS, terminfo is separate from the ncurses
+shared libraries. We need to explicitly look for libtinfo,
+and add that as a dependency.
+
+Original patch from:
+https://github.com/spack/spack/commit/9ea261265010eacd250691a8361f661d0576f25c
+
+patch.cherry: false
+patch.platforms: chromiumos
+patch.version_range.from: 537678
+patch.version_range.until: null
+
+---
+ lldb/cmake/modules/FindCursesAndPanel.cmake | 12 ++++++++----
+ 1 file changed, 8 insertions(+), 4 deletions(-)
+
+diff --git a/lldb/cmake/modules/FindCursesAndPanel.cmake b/lldb/cmake/modules/FindCursesAndPanel.cmake
+index aaadf214bf54..cd2a68a01a0d 100644
+--- a/lldb/cmake/modules/FindCursesAndPanel.cmake
++++ b/lldb/cmake/modules/FindCursesAndPanel.cmake
+@@ -2,12 +2,15 @@
+ # FindCursesAndPanel
+ # -----------
+ #
+-# Find the curses and panel library as a whole.
++# Find the curses, terminfo, and panel library as a whole.
++# NOTE: This is patched due to a terminfo dependency error introduced in
++# https://github.com/llvm/llvm-project/pull/92865
+ 
+-if(CURSES_INCLUDE_DIRS AND CURSES_LIBRARIES AND PANEL_LIBRARIES)
++if(CURSES_INCLUDE_DIRS AND CURSES_LIBRARIES AND TINFO_LIBRARIES AND PANEL_LIBRARIES)
+   set(CURSESANDPANEL_FOUND TRUE)
+ else()
+   find_package(Curses QUIET)
++  find_package(TINFO_LIBRARIES NAMES tinfo DOC "The curses tinfo library" QUIET)
+   find_library(PANEL_LIBRARIES NAMES panel DOC "The curses panel library" QUIET)
+   include(FindPackageHandleStandardArgs)
+   find_package_handle_standard_args(CursesAndPanel
+@@ -16,9 +19,10 @@ else()
+                                     REQUIRED_VARS
+                                       CURSES_INCLUDE_DIRS
+                                       CURSES_LIBRARIES
++                                      TINFO_LIBRARIES
+                                       PANEL_LIBRARIES)
+-  if(CURSES_FOUND AND PANEL_LIBRARIES)
+-    mark_as_advanced(CURSES_INCLUDE_DIRS CURSES_LIBRARIES PANEL_LIBRARIES)
++  if(CURSES_FOUND AND TINFO_LIBRARIES AND PANEL_LIBRARIES)
++    mark_as_advanced(CURSES_INCLUDE_DIRS CURSES_LIBRARIES TINFO_LIBRARIES PANEL_LIBRARIES)
+   endif()
+ endif()
+ 
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/671af65d003bb0e26d29ee63469f8ea7e65ecfee.patch b/llvm_patches/671af65d003bb0e26d29ee63469f8ea7e65ecfee.patch
new file mode 100644
index 00000000..b1e0520c
--- /dev/null
+++ b/llvm_patches/671af65d003bb0e26d29ee63469f8ea7e65ecfee.patch
@@ -0,0 +1,32 @@
+From 671af65d003bb0e26d29ee63469f8ea7e65ecfee Mon Sep 17 00:00:00 2001
+From: Jordan R Abrahams-Whitehead <ajordanr@google.com>
+Date: Fri, 22 Sep 2023 17:31:18 +0000
+Subject: [PATCH] Revert "add_tablegen: Quick fix to reflect LLVM_TABLEGEN to
+ llvm-min-tblgen"
+
+This reverts commit 95d4506dda79d49e55fdd0e4da7bf81487167aa1.
+---
+ llvm/cmake/modules/TableGen.cmake | 7 -------
+ 1 file changed, 7 deletions(-)
+
+diff --git a/llvm/cmake/modules/TableGen.cmake b/llvm/cmake/modules/TableGen.cmake
+index 7fd6628ef55d..be16127c724e 100644
+--- a/llvm/cmake/modules/TableGen.cmake
++++ b/llvm/cmake/modules/TableGen.cmake
+@@ -154,13 +154,6 @@ macro(add_tablegen target project)
+     endif()
+   endif()
+ 
+-  # FIXME: Quick fix to reflect LLVM_TABLEGEN to llvm-min-tblgen
+-  if("${target}" STREQUAL "llvm-min-tblgen"
+-      AND NOT "${LLVM_TABLEGEN}" STREQUAL ""
+-      AND NOT "${LLVM_TABLEGEN}" STREQUAL "llvm-tblgen")
+-    set(${project}_TABLEGEN_DEFAULT "${LLVM_TABLEGEN}")
+-  endif()
+-
+   if(ADD_TABLEGEN_EXPORT)
+     set(${project}_TABLEGEN "${${project}_TABLEGEN_DEFAULT}" CACHE
+       STRING "Native TableGen executable. Saves building one when cross-compiling.")
+-- 
+2.42.0.515.g380fc7ccd1-goog
+
diff --git a/llvm_patches/Adds-a-allowlist-of-packages-that-have-known-memory-v5.patch b/llvm_patches/Adds-a-allowlist-of-packages-that-have-known-memory-v5.patch
new file mode 100644
index 00000000..198d1282
--- /dev/null
+++ b/llvm_patches/Adds-a-allowlist-of-packages-that-have-known-memory-v5.patch
@@ -0,0 +1,92 @@
+From a41d7031412c61cdf55b67830dc37e6f50985f83 Mon Sep 17 00:00:00 2001
+From: Jordan R Abrahams-Whitehead <ajordanr@google.com>
+Date: Fri, 7 Jun 2024 21:16:54 +0000
+Subject: [PATCH] Adds a allowlist of packages that have known memory leaks
+
+This adds an allowlist of packages that have known memory leaks.
+So that the leak detector will not report memory leaks from these packages.
+
+---
+ compiler-rt/lib/lsan/lsan_common.cpp          |  3 ++
+ .../lib/lsan/lsan_default_suppression.h       | 34 +++++++++++++++++++
+ .../lib/sanitizer_common/sanitizer_flags.inc  |  2 +-
+ 3 files changed, 38 insertions(+), 1 deletion(-)
+ create mode 100644 compiler-rt/lib/lsan/lsan_default_suppression.h
+
+diff --git a/compiler-rt/lib/lsan/lsan_common.cpp b/compiler-rt/lib/lsan/lsan_common.cpp
+index 8b1af5b629fb..4c8ad52b7d32 100644
+--- a/compiler-rt/lib/lsan/lsan_common.cpp
++++ b/compiler-rt/lib/lsan/lsan_common.cpp
+@@ -12,6 +12,7 @@
+ //===----------------------------------------------------------------------===//
+ 
+ #include "lsan_common.h"
++#include "lsan_default_suppression.h"
+ 
+ #include "sanitizer_common/sanitizer_common.h"
+ #include "sanitizer_common/sanitizer_flag_parser.h"
+@@ -136,6 +137,8 @@ void LeakSuppressionContext::LazyInit() {
+     if (&__lsan_default_suppressions)
+       context.Parse(__lsan_default_suppressions());
+     context.Parse(kStdSuppressions);
++    context.Parse(kLSanDefaultSuppressions);
++
+     if (flags()->use_tls && flags()->use_ld_allocations)
+       suppress_module = GetLinker();
+   }
+diff --git a/compiler-rt/lib/lsan/lsan_default_suppression.h b/compiler-rt/lib/lsan/lsan_default_suppression.h
+new file mode 100644
+index 000000000000..707c1ba0bbb6
+--- /dev/null
++++ b/compiler-rt/lib/lsan/lsan_default_suppression.h
+@@ -0,0 +1,34 @@
++namespace __lsan {
++
++char kLSanDefaultSuppressions[] =
++// ================ Leaks in third-party code ================
++
++// False positives in libfontconfig. http://crbug.com/39050
++"leak:libfontconfig\n"
++
++// Leaks in Nvidia's libGL.
++"leak:libGL.so\n"
++
++"leak:libnssutil3\n"
++"leak:libnspr4\n"
++"leak:libnss3\n"
++"leak:libplds4\n"
++"leak:libnssckbi\n"
++
++// XRandR has several one time leaks.
++"leak:libxrandr\n"
++
++// xrandr leak. http://crbug.com/119677
++"leak:XRRFindDisplay\n"
++
++// leak on session_manager. http://crbug.com/378805
++"leak:/sbin/session_manager\n"
++
++// leak on cryptohome. http://crbug.com/508281
++"leak:/usr/sbin/cryptohome\n"
++
++// leak on buffet. http://crbug.com/473700
++//"leak:/usr/bin/buffet\n"
++// End of suppressions.
++;  // Please keep this semicolon.
++}
+diff --git a/compiler-rt/lib/sanitizer_common/sanitizer_flags.inc b/compiler-rt/lib/sanitizer_common/sanitizer_flags.inc
+index 6148ae56067c..c0b5b599e8ba 100644
+--- a/compiler-rt/lib/sanitizer_common/sanitizer_flags.inc
++++ b/compiler-rt/lib/sanitizer_common/sanitizer_flags.inc
+@@ -170,7 +170,7 @@ COMMON_FLAG(const char *, cov_pcs_out, "",
+ COMMON_FLAG(bool, full_address_space, false,
+             "Sanitize complete address space; "
+             "by default kernel area on 32-bit platforms will not be sanitized")
+-COMMON_FLAG(bool, print_suppressions, true,
++COMMON_FLAG(bool, print_suppressions, false,
+             "Print matched suppressions at exit.")
+ COMMON_FLAG(
+     bool, disable_coredump, (SANITIZER_WORDSIZE == 64) && !SANITIZER_GO,
+-- 
+2.45.2.505.gda0bf45e8d-goog
+
diff --git a/llvm_patches/BOLT-Increase-max-allocation-size-to-allow-BOLTing-clang-and-rustc.patch b/llvm_patches/BOLT-Increase-max-allocation-size-to-allow-BOLTing-clang-and-rustc.patch
new file mode 100644
index 00000000..3dd5c129
--- /dev/null
+++ b/llvm_patches/BOLT-Increase-max-allocation-size-to-allow-BOLTing-clang-and-rustc.patch
@@ -0,0 +1,26 @@
+From 28972d5fb13a36b9258b48244ed61ec91778e069 Mon Sep 17 00:00:00 2001
+From: Yi Kong <yikong@google.com>
+Date: Fri, 3 Jun 2022 02:06:19 +0800
+Subject: [PATCH] [BOLT] Increase max allocation size to allow BOLTing clang and
+ rustc
+
+---
+ bolt/runtime/instr.cpp | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/bolt/runtime/instr.cpp b/bolt/runtime/instr.cpp
+index d111599e3b7..2786ec3cbd5 100644
+--- a/bolt/runtime/instr.cpp
++++ b/bolt/runtime/instr.cpp
+@@ -206,7 +206,7 @@ public:
+ 
+ private:
+   static constexpr uint64_t Magic = 0x1122334455667788ull;
+-  uint64_t MaxSize = 0xa00000;
++  uint64_t MaxSize = 0xd00000;
+   uint8_t *StackBase{nullptr};
+   uint64_t StackSize{0};
+   bool Shared{false};
+-- 
+2.36.1.255.ge46751e96f-goog
+
diff --git a/llvm_patches/Disable-Cast-Assertion.patch b/llvm_patches/Disable-Cast-Assertion.patch
new file mode 100644
index 00000000..df593eb1
--- /dev/null
+++ b/llvm_patches/Disable-Cast-Assertion.patch
@@ -0,0 +1,48 @@
+From b03f99e57e606076d1aa77e88df7a1c1a077a14e Mon Sep 17 00:00:00 2001
+From: Jordan R Abrahams-Whitehead <ajordanr@google.com>
+Date: Fri, 14 Jun 2024 21:29:09 +0000
+Subject: [PATCH] Disable Cast Assertion
+
+This CL creates a local patch to disable an assert in LLVM for Casting.
+This is fix is more of a temporary workaround while
+github.com/llvm/llvm-project/issues/39319 is open.
+
+This fixes a clang crash compiling boost with debug enabled (b/271428130)
+
+---
+ llvm/include/llvm/Support/Casting.h | 4 ----
+ 1 file changed, 4 deletions(-)
+
+diff --git a/llvm/include/llvm/Support/Casting.h b/llvm/include/llvm/Support/Casting.h
+index 2391c1a531a5..3059a587fffc 100644
+--- a/llvm/include/llvm/Support/Casting.h
++++ b/llvm/include/llvm/Support/Casting.h
+@@ -563,25 +563,21 @@ template <typename First, typename Second, typename... Rest, typename From>
+ 
+ template <typename To, typename From>
+ [[nodiscard]] inline decltype(auto) cast(const From &Val) {
+-  assert(isa<To>(Val) && "cast<Ty>() argument of incompatible type!");
+   return CastInfo<To, const From>::doCast(Val);
+ }
+ 
+ template <typename To, typename From>
+ [[nodiscard]] inline decltype(auto) cast(From &Val) {
+-  assert(isa<To>(Val) && "cast<Ty>() argument of incompatible type!");
+   return CastInfo<To, From>::doCast(Val);
+ }
+ 
+ template <typename To, typename From>
+ [[nodiscard]] inline decltype(auto) cast(From *Val) {
+-  assert(isa<To>(Val) && "cast<Ty>() argument of incompatible type!");
+   return CastInfo<To, From *>::doCast(Val);
+ }
+ 
+ template <typename To, typename From>
+ [[nodiscard]] inline decltype(auto) cast(std::unique_ptr<From> &&Val) {
+-  assert(isa<To>(Val) && "cast<Ty>() argument of incompatible type!");
+   return CastInfo<To, std::unique_ptr<From>>::doCast(std::move(Val));
+ }
+ 
+-- 
+2.45.2.627.g7a2c4fd464-goog
+
diff --git a/llvm_patches/Disable-vfork-events-v2.patch b/llvm_patches/Disable-vfork-events-v2.patch
new file mode 100644
index 00000000..0ad290e3
--- /dev/null
+++ b/llvm_patches/Disable-vfork-events-v2.patch
@@ -0,0 +1,31 @@
+From 5cd24994e3ffa20c3c30890d7f40fdae03beb0e7 Mon Sep 17 00:00:00 2001
+From: Emre Kultursay <emrekultursay@google.com>
+Date: Mon, 19 Sep 2022 12:55:03 +0900
+Subject: [PATCH] Disable vfork events v2
+
+They cause the debugger to detach.
+
+Bug: 243919451
+Bug: 243434753
+Test: vulkan tutorial02
+Change-Id: Ib27d0c072505685efc66276899c54238e1e7ebcd
+---
+ .../Plugins/Process/gdb-remote/GDBRemoteCommunicationClient.cpp | 2 --
+ 1 file changed, 2 deletions(-)
+
+diff --git a/lldb/source/Plugins/Process/gdb-remote/GDBRemoteCommunicationClient.cpp b/lldb/source/Plugins/Process/gdb-remote/GDBRemoteCommunicationClient.cpp
+index 83ba27783da4..691f64b02b60 100644
+--- a/lldb/source/Plugins/Process/gdb-remote/GDBRemoteCommunicationClient.cpp
++++ b/lldb/source/Plugins/Process/gdb-remote/GDBRemoteCommunicationClient.cpp
+@@ -353,8 +353,6 @@ void GDBRemoteCommunicationClient::GetRemoteQSupported() {
+   // build the qSupported packet
+   std::vector<std::string> features = {"xmlRegisters=i386,arm,mips,arc",
+                                        "multiprocess+",
+-                                       "fork-events+",
+-                                       "vfork-events+",
+                                        "swbreak+",
+                                        "hwbreak+"};
+   StreamString packet;
+-- 
+2.46.0.792.g87dc391469-goog
+
diff --git a/llvm_patches/Disable-vfork-fork-events.patch b/llvm_patches/Disable-vfork-fork-events.patch
new file mode 100644
index 00000000..ef61eea5
--- /dev/null
+++ b/llvm_patches/Disable-vfork-fork-events.patch
@@ -0,0 +1,30 @@
+From c43d719afbad784f49fc2647211e0f747b4d26aa Mon Sep 17 00:00:00 2001
+From: Emre Kultursay <emrekultursay@google.com>
+Date: Mon, 19 Sep 2022 12:55:03 +0900
+Subject: [PATCH 1/1] Disable vfork events
+
+They cause the debugger to detach.
+
+Bug: 243919451
+Bug: 243434753
+Test: vulkan tutorial02
+---
+ .../Process/gdb-remote/GDBRemoteCommunicationClient.cpp        | 3 +--
+ 1 file changed, 1 insertion(+), 2 deletions(-)
+
+diff --git a/lldb/source/Plugins/Process/gdb-remote/GDBRemoteCommunicationClient.cpp b/lldb/source/Plugins/Process/gdb-remote/GDBRemoteCommunicationClient.cpp
+index 580cdde57d80..73c6a14f9164 100644
+--- a/lldb/source/Plugins/Process/gdb-remote/GDBRemoteCommunicationClient.cpp
++++ b/lldb/source/Plugins/Process/gdb-remote/GDBRemoteCommunicationClient.cpp
+@@ -342,8 +342,7 @@ void GDBRemoteCommunicationClient::GetRemoteQSupported() {
+ 
+   // build the qSupported packet
+   std::vector<std::string> features = {"xmlRegisters=i386,arm,mips,arc",
+-                                       "multiprocess+", "fork-events+",
+-                                       "vfork-events+"};
++                                       "multiprocess+"};
+   StreamString packet;
+   packet.PutCString("qSupported");
+   for (uint32_t i = 0; i < features.size(); ++i) {
+-- 
+2.31.1.windows.1
diff --git a/llvm_patches/Ignore-inlinable-calls-if-unrolling-is-forced.patch b/llvm_patches/Ignore-inlinable-calls-if-unrolling-is-forced.patch
new file mode 100644
index 00000000..b2f4c5b1
--- /dev/null
+++ b/llvm_patches/Ignore-inlinable-calls-if-unrolling-is-forced.patch
@@ -0,0 +1,55 @@
+From bd00e30250bdf1bd1a6a2cbf2b2a195cae82c0a5 Mon Sep 17 00:00:00 2001
+From: George Burgess IV <gbiv@google.com>
+Date: Wed, 3 Apr 2024 16:18:27 -0600
+Subject: [PATCH] Ignore inlinable calls if unrolling is forced
+
+`NumInlineCandidates` counts candidates that are _very likely_ to be
+inlined. This is a useful metric, but causes linker warnings if:
+- the loop to be unrolled has had unrolling forced by the user, and
+- the inliner fails to inline the call (e.g., because it's considered a
+  very cold callsite)
+---
+ llvm/lib/Transforms/Scalar/LoopUnrollPass.cpp | 11 ++++++++---
+ 1 file changed, 8 insertions(+), 3 deletions(-)
+
+diff --git a/llvm/lib/Transforms/Scalar/LoopUnrollPass.cpp b/llvm/lib/Transforms/Scalar/LoopUnrollPass.cpp
+index 446aa497026d..7184fc0188e9 100644
+--- a/llvm/lib/Transforms/Scalar/LoopUnrollPass.cpp
++++ b/llvm/lib/Transforms/Scalar/LoopUnrollPass.cpp
+@@ -1126,9 +1126,11 @@ tryToUnrollLoop(Loop *L, DominatorTree &DT, LoopInfo *LI, ScalarEvolution &SE,
+   // automatic unrolling from interfering with the user requested
+   // transformation.
+   Loop *ParentL = L->getParentLoop();
++  const bool UnrollIsForcedByUser =
++      hasUnrollTransformation(L) == TM_ForcedByUser;
+   if (ParentL != nullptr &&
+       hasUnrollAndJamTransformation(ParentL) == TM_ForcedByUser &&
+-      hasUnrollTransformation(L) != TM_ForcedByUser) {
++      !UnrollIsForcedByUser) {
+     LLVM_DEBUG(dbgs() << "Not unrolling loop since parent loop has"
+                       << " llvm.loop.unroll_and_jam.\n");
+     return LoopUnrollResult::Unmodified;
+@@ -1138,7 +1140,7 @@ tryToUnrollLoop(Loop *L, DominatorTree &DT, LoopInfo *LI, ScalarEvolution &SE,
+   // loop has an explicit unroll-and-jam pragma. This is to prevent automatic
+   // unrolling from interfering with the user requested transformation.
+   if (hasUnrollAndJamTransformation(L) == TM_ForcedByUser &&
+-      hasUnrollTransformation(L) != TM_ForcedByUser) {
++      !UnrollIsForcedByUser) {
+     LLVM_DEBUG(
+         dbgs()
+         << "  Not unrolling loop since it has llvm.loop.unroll_and_jam.\n");
+@@ -1188,7 +1190,10 @@ tryToUnrollLoop(Loop *L, DominatorTree &DT, LoopInfo *LI, ScalarEvolution &SE,
+   if (OptForSize)
+     UP.Threshold = std::max(UP.Threshold, LoopSize + 1);
+ 
+-  if (UCE.NumInlineCandidates != 0) {
++  // Ignore the potential for inlining if unrolling is forced by the user. It's
++  // only _very likely_ that `NumInlineCandidates` functions will be inlined;
++  // things like profile data can make this significantly less likely.
++  if (!UnrollIsForcedByUser && UCE.NumInlineCandidates != 0) {
+     LLVM_DEBUG(dbgs() << "  Not unrolling loop with inlinable calls.\n");
+     return LoopUnrollResult::Unmodified;
+   }
+-- 
+2.45.2.627.g7a2c4fd464-goog
+
diff --git a/llvm_patches/PATCHES.json b/llvm_patches/PATCHES.json
new file mode 100644
index 00000000..11ca1b93
--- /dev/null
+++ b/llvm_patches/PATCHES.json
@@ -0,0 +1,596 @@
+[
+  {
+    "metadata": {
+      "info": [],
+      "title": "Adds a allowlist of packages that have known memory leaks"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "Adds-a-allowlist-of-packages-that-have-known-memory-v5.patch",
+    "version_range": {
+      "from": 445947,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "Sets the ASAN message to go to /va/log/asan${pid} by\ndefault"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "clang-12.0-asan-default-path.patch",
+    "version_range": {
+      "from": 408666,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "Fixes crbug/591436. Force fallback to traditional executable detection"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "llvm-8.0-clang-executable-detection.v2.patch",
+    "version_range": {
+      "from": 370808,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "Temporary workaround. Breakpad cannot handle Dwarf\nVersion 4. Works around issue by forcing Debug Frames\nversion of Dwarf to be version 1, while leaving the rest of\nthe Dwarf data at version 4"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "llvm-3.9-dwarf-version.patch",
+    "version_range": {
+      "from": 0,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "Uses argv[0] to get host system information. Created\na wrapper of LLD and real lld becomes lld.elf so the LLD\ncannot get the host system information and dies"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "lld-10.0-invoke-name-v2.patch",
+    "version_range": {
+      "from": 502154,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "Fixes clang.elf calling clang.elf because direct\ncalls to clang.elf misses all environment setup in clang\nwrapper. Calls to clang wrapper instead of clang.elf binary"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "llvm-17.0-invocation.patch",
+    "version_range": {
+      "from": 489485,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "[libcxx] v4: Crash when dereferencing nullopt for std::optional"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "libcxx-crash-when-dereferencing-nullopt-v4.patch",
+    "version_range": {
+      "from": 520603,
+      "until": 547379
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "v5: Crash when dereferencing nullopt for std::optional"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "v5-Crash-when-dereferencing-nullopt-for-std-optional.patch",
+    "version_range": {
+      "from": 547379,
+      "until": 563880
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "v6: Crash when dereferencing nullopt for std::optional"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "v6-Crash-when-dereferencing-nullopt-for-std-optional.patch",
+    "version_range": {
+      "from": 563880,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "Increase max allocation size to allow BOLTing clang and rustc"
+    },
+    "platforms": [
+      "android",
+      "chromiumos"
+    ],
+    "rel_patch_path": "BOLT-Increase-max-allocation-size-to-allow-BOLTing-clang-and-rustc.patch",
+    "version_range": {
+      "from": 450784,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "Disable vfork/fork events"
+    },
+    "platforms": [
+      "android",
+      "chromiumos"
+    ],
+    "rel_patch_path": "Disable-vfork-fork-events.patch",
+    "version_range": {
+      "from": 468909,
+      "until": 547379
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "Disable vfork events v2"
+    },
+    "platforms": [
+      "android",
+      "chromiumos"
+    ],
+    "rel_patch_path": "Disable-vfork-events-v2.patch",
+    "version_range": {
+      "from": 547379,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "v2: Revert [Driver] Allow target override containing . in executable name"
+    },
+    "platforms": [
+      "android",
+      "chromiumos"
+    ],
+    "rel_patch_path": "Revert-Driver-Allow-target-override-containing-.-in-executable-name-v2.patch",
+    "version_range": {
+      "from": 521684,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "Disable Cast Assertion"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "Disable-Cast-Assertion.patch",
+    "version_range": {
+      "from": 475826,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "inline-cost: skip threshold checks"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "inline-cost-skip-threshold-checks.patch",
+    "version_range": {
+      "from": 510928,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "[LoopUnroll] Ignore inlinable calls if unrolling is forced"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "Ignore-inlinable-calls-if-unrolling-is-forced.patch",
+    "version_range": {
+      "from": 516547,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "[PATCH] [scudo] Add baseline Scudo config for ChromeOS"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "scudo-Add-baseline-Scudo-config-for-ChromeOS-v2.patch",
+    "version_range": {
+      "from": 510928,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "Disable querying git in benchmarking code"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "disable-querying-git-in-benchmarking-code.patch",
+    "version_range": {
+      "from": null,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "Revert \"add_tablegen: Quick fix to reflect LLVM_TABLEGEN to llvm-min-tblgen\""
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "671af65d003bb0e26d29ee63469f8ea7e65ecfee.patch",
+    "version_range": {
+      "from": 498229,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "0a00d32c5f88fce89006dcde6e235bc77d7b495e",
+      "title": "[lldb] Add armv7a and armv8a ArchSpecs (#106433)"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/0a00d32c5f88fce89006dcde6e235bc77d7b495e.patch",
+    "version_range": {
+      "from": 530567,
+      "until": 547992
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "[libc++] Make _LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTABLE a no-op on baremetal"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "libc-Make-_LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTAB.patch",
+    "version_range": {
+      "from": 530567,
+      "until": 563880
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "[libc++] Make _LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTABLE a no-op on baremetal"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "v2-libc-Make-_LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTAB.patch",
+    "version_range": {
+      "from": 563880,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "[lldb] Add terminfo dependency for ncurses support"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "-lldb-Add-terminfo-dependency-for-ncurses-support.patch",
+    "version_range": {
+      "from": 537678,
+      "until": 566148
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "815bf0f190383e14068f191bb0d16d62fbb6f8b3",
+      "title": "Revert \"[Clang] [Test] Use lit Syntax for Environment Variables in Clang subproject\" (#106267)"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/815bf0f190383e14068f191bb0d16d62fbb6f8b3.patch",
+    "version_range": {
+      "from": 547155,
+      "until": 547783
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "e3ce979f1b3ac1e7f2d0261d3abffbd12064eae6",
+      "title": "Revert \"[clang] Increase the default expression nesting limit (#104717)\""
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/e3ce979f1b3ac1e7f2d0261d3abffbd12064eae6.patch",
+    "version_range": {
+      "from": 546878,
+      "until": 547382
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "fd7904a07bc26950fa7735fb6871a064e3ebc836",
+      "title": "Revert \"[lldb] Speculative fix for trap_frame_sym_ctx.test\""
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/fd7904a07bc26950fa7735fb6871a064e3ebc836.patch",
+    "version_range": {
+      "from": 547186,
+      "until": 547400
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "9671ed1afcd1e3aab754a9b905d511ffd52f3624",
+      "title": "Revert \"LSV: forbid load-cycles when vectorizing; fix bug (#104815)\" (#106245)"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/9671ed1afcd1e3aab754a9b905d511ffd52f3624.patch",
+    "version_range": {
+      "from": 547233,
+      "until": 547704
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "438ad9f2bf25575c474313de4ad85a5da6f69e4c",
+      "title": "[clang-format] Revert \"[clang-format][NFC] Delete TT_LambdaArrow (#70\u2026 (#105923)"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/438ad9f2bf25575c474313de4ad85a5da6f69e4c.patch",
+    "version_range": {
+      "from": 516925,
+      "until": 547893
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "Partial Revert \"[Driver] Support using toolchain libc and libc++ for baremetal (#96736)\""
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "Partial-Revert-Driver-Support-using-toolchain-libc-and-libc-for-baremetal-96736-.patch",
+    "version_range": {
+      "from": 547379,
+      "until": null
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "d29a50f358e71a695b23e456d66ed2924617deb9",
+      "title": "Revert \"[lldb] Allow fetching of RA register when above fault handler (#98566)\""
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/d29a50f358e71a695b23e456d66ed2924617deb9.patch",
+    "version_range": {
+      "from": 542750,
+      "until": 556712
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "412e1af19a248fd5650e6828695c78454a9195fb",
+      "title": "Revert \"[AArch64] Lower alias mask to a whilewr\" (#120261)"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/412e1af19a248fd5650e6828695c78454a9195fb.patch",
+    "version_range": {
+      "from": 545094,
+      "until": 560071
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "cros: add llvm-rev file"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cros-add-llvm-rev-file.patch",
+    "version_range": {
+      "from": null,
+      "until": 563880
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "d2aff182d379c9b84cebe0fdf58907f4de768f1e",
+      "title": "Revert \"TLS loads opimization (hoist)\" (#114740)"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/d2aff182d379c9b84cebe0fdf58907f4de768f1e.patch",
+    "version_range": {
+      "from": 455363,
+      "until": 555342
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "3ef64f7ab5b8651eab500cd944984379fce5f639",
+      "title": "Revert \"Enable logf128 constant folding for hosts with 128bit long double (#104929)\""
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/3ef64f7ab5b8651eab500cd944984379fce5f639.patch",
+    "version_range": {
+      "from": 547379,
+      "until": 547478
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "title": "Revert \"[lldb] Add terminfo dependency for ncurses support\""
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "Revert-lldb-Add-terminfo-dependency-for-ncurses-support-.patch",
+    "version_range": {
+      "from": 537678,
+      "until": 566148
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "8fff0c181f26a5e8b2344c061ebf2559118b1160",
+      "title": "[lldb] Add terminfo dependency for ncurses support (#126810)"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/8fff0c181f26a5e8b2344c061ebf2559118b1160.patch",
+    "version_range": {
+      "from": 537678,
+      "until": 565379
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "bb6a273d9ab9ee90dbb957e541f4d810fffb22ee",
+      "title": "[lldb] Fix manual CURSES_LIBRARIES tinfo finding (#128245)"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/bb6a273d9ab9ee90dbb957e541f4d810fffb22ee.patch",
+    "version_range": {
+      "from": 537678,
+      "until": 566148
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "c9d0a464c9f3a0a66f35d0ca28f36a96efc6961b",
+      "title": "Revert \"[mlir][math]Update `convertPowfOp` `ExpandPatterns.cpp`\" (#126063)"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/c9d0a464c9f3a0a66f35d0ca28f36a96efc6961b.patch",
+    "version_range": {
+      "from": 563468,
+      "until": 564416
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "0227396417d4625bc93affdd8957ff8d90c76299",
+      "title": "Revert \"[libc++] Reduce std::conjunction overhead (#124259)\""
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/0227396417d4625bc93affdd8957ff8d90c76299.patch",
+    "version_range": {
+      "from": 563049,
+      "until": 564591
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "c65ed964657c93d51f3e05de9e0609419768a143",
+      "title": "Revert \"[reland][libc][bazel] Enable software prefetching for memcpy\" (#127189)"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/c65ed964657c93d51f3e05de9e0609419768a143.patch",
+    "version_range": {
+      "from": 554178,
+      "until": 565287
+    }
+  },
+  {
+    "metadata": {
+      "info": [],
+      "original_sha": "cd6e959102888279dc7e75a41ebd75a08ac3f7a5",
+      "title": "Revert \"[MC] Explicitly mark MCSymbol for MO_ExternalSymbol\" (#133291)"
+    },
+    "platforms": [
+      "chromiumos"
+    ],
+    "rel_patch_path": "cherry/cd6e959102888279dc7e75a41ebd75a08ac3f7a5.patch",
+    "version_range": {
+      "from": 550362,
+      "until": 570122
+    }
+  }
+]
diff --git a/llvm_patches/Partial-Revert-Driver-Support-using-toolchain-libc-and-libc-for-baremetal-96736-.patch b/llvm_patches/Partial-Revert-Driver-Support-using-toolchain-libc-and-libc-for-baremetal-96736-.patch
new file mode 100644
index 00000000..27848812
--- /dev/null
+++ b/llvm_patches/Partial-Revert-Driver-Support-using-toolchain-libc-and-libc-for-baremetal-96736-.patch
@@ -0,0 +1,80 @@
+From 5fa933172737b3cefca4b78965b9d8f6895965af Mon Sep 17 00:00:00 2001
+From: Jordan R Abrahams-Whitehead <ajordanr@google.com>
+Date: Thu, 14 Nov 2024 21:14:12 +0000
+Subject: [PATCH] Partial Revert "[Driver] Support using toolchain libc and
+ libc++ for baremetal (#96736)"
+
+This reverts commit 135483bf968bc72a9544a9f2640f73f196ca8cbc.
+
+This is a partial revert, reverting only the portions of
+135483bf968bc72a9544a9f2640f73f196ca8cbc which actually break our
+embedded projects.
+
+The original commit breaks FPMCU targets because it does not respect the
+--sysroot or -isysroot clang flags, which should re-root the system
+paths. This commit introduces a new way (and order) to add system
+includes which ends up violating our sysroot, and thus causing
+mismatched headers.
+
+Upstream Commit:
+https://github.com/llvm/llvm-project/commit/135483bf968bc72a9544a9f2640f73f196ca8cbc
+
+patch.cherry: false
+patch.platforms: chromiumos
+patch.version_range.from: 547379
+patch.version_range.until: null
+
+BUG=b:377375234
+TEST=Built chromeos-base/chromeos-fpmcu-bloonchipper-unittests
+
+---
+ clang/lib/Driver/ToolChains/BareMetal.cpp | 34 -----------------------
+ 1 file changed, 34 deletions(-)
+
+diff --git a/clang/lib/Driver/ToolChains/BareMetal.cpp b/clang/lib/Driver/ToolChains/BareMetal.cpp
+index 0f5f32f3f8f4..57a015b2feee 100644
+--- a/clang/lib/Driver/ToolChains/BareMetal.cpp
++++ b/clang/lib/Driver/ToolChains/BareMetal.cpp
+@@ -302,40 +302,6 @@ void BareMetal::AddClangCXXStdlibIncludeArgs(const ArgList &DriverArgs,
+     return;
+ 
+   const Driver &D = getDriver();
+-  std::string Target = getTripleString();
+-
+-  auto AddCXXIncludePath = [&](StringRef Path) {
+-    std::string Version = detectLibcxxVersion(Path);
+-    if (Version.empty())
+-      return;
+-
+-    {
+-      // First the per-target include dir: include/<target>/c++/v1.
+-      SmallString<128> TargetDir(Path);
+-      llvm::sys::path::append(TargetDir, Target, "c++", Version);
+-      addSystemInclude(DriverArgs, CC1Args, TargetDir);
+-    }
+-
+-    {
+-      // Then the generic dir: include/c++/v1.
+-      SmallString<128> Dir(Path);
+-      llvm::sys::path::append(Dir, "c++", Version);
+-      addSystemInclude(DriverArgs, CC1Args, Dir);
+-    }
+-  };
+-
+-  switch (GetCXXStdlibType(DriverArgs)) {
+-    case ToolChain::CST_Libcxx: {
+-      SmallString<128> P(D.Dir);
+-      llvm::sys::path::append(P, "..", "include");
+-      AddCXXIncludePath(P);
+-      break;
+-    }
+-    case ToolChain::CST_Libstdcxx:
+-      // We only support libc++ toolchain installation.
+-      break;
+-  }
+-
+   std::string SysRoot(computeSysRoot());
+   if (SysRoot.empty())
+     return;
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/Revert-Driver-Allow-target-override-containing-.-in-executable-name-v2.patch b/llvm_patches/Revert-Driver-Allow-target-override-containing-.-in-executable-name-v2.patch
new file mode 100644
index 00000000..0ec845cf
--- /dev/null
+++ b/llvm_patches/Revert-Driver-Allow-target-override-containing-.-in-executable-name-v2.patch
@@ -0,0 +1,54 @@
+From bcba9ea5d205f7b9978276da3bef9f0e5ad78aaa Mon Sep 17 00:00:00 2001
+From: Yi Kong <yikong@google.com>
+Date: Tue, 28 Feb 2023 12:50:21 +0900
+Subject: [PATCH] Revert "[Driver] Allow target override containing . in
+ executable name"
+
+This reverts commit 0d7eba1aeed853798bd8012f786c305e83a97b67.
+diff --git a/clang/lib/Driver/ToolChain.cpp b/clang/lib/Driver/ToolChain.cpp
+index 08b1fd01b3c0..6ffe6e6f4c88 100644
+--- a/clang/lib/Driver/ToolChain.cpp
++++ b/clang/lib/Driver/ToolChain.cpp
+@@ -352,7 +352,7 @@ static const DriverSuffix *FindDriverSuffix(StringRef ProgName, size_t &Pos) {
+ /// Normalize the program name from argv[0] by stripping the file extension if
+ /// present and lower-casing the string on Windows.
+ static std::string normalizeProgramName(llvm::StringRef Argv0) {
+-  std::string ProgName = std::string(llvm::sys::path::filename(Argv0));
++  std::string ProgName = std::string(llvm::sys::path::stem(Argv0));
+   if (is_style_windows(llvm::sys::path::Style::native)) {
+     // Transform to lowercase for case insensitive file systems.
+     std::transform(ProgName.begin(), ProgName.end(), ProgName.begin(),
+@@ -371,13 +371,6 @@ static const DriverSuffix *parseDriverSuffix(StringRef ProgName, size_t &Pos) {
+   // added via -target as implicit first argument.
+   const DriverSuffix *DS = FindDriverSuffix(ProgName, Pos);
+ 
+-  if (!DS && ProgName.ends_with(".exe")) {
+-    // Try again after stripping the executable suffix:
+-    // clang++.exe -> clang++
+-    ProgName = ProgName.drop_back(StringRef(".exe").size());
+-    DS = FindDriverSuffix(ProgName, Pos);
+-  }
+-
+   if (!DS) {
+     // Try again after stripping any trailing version number:
+     // clang++3.5 -> clang++
+diff --git a/clang/test/Driver/target-override.c b/clang/test/Driver/target-override.c
+index 2c605ac9a03d..aef89cc9a9dd 100644
+--- a/clang/test/Driver/target-override.c
++++ b/clang/test/Driver/target-override.c
+@@ -3,7 +3,6 @@
+ 
+ // RUN: rm -rf %t && mkdir %t
+ // RUN: ln -s %clang %t/i386-clang
+-// RUN: ln -s %clang %t/x86_64-pc-freebsd13.1-clang
+ 
+ // Check if invocation of "foo-clang" adds option "-target foo".
+ //
+@@ -14,7 +13,3 @@
+ //
+ // RUN: %t/i386-clang -c --target=x86_64 -### %s 2>&1 | FileCheck -check-prefix CHECK-TG2 %s
+ // CHECK-TG2: Target: x86_64
+-
+-/// Check if invocation of "arch-vendor-osX.Y-clang" adds option "-target arch-vendor-osX.Y".
+-// RUN: %t/x86_64-pc-freebsd13.1-clang -c -### %s 2>&1 | FileCheck -check-prefix CHECK-TG3 %s
+-// CHECK-TG3: Target: x86_64-pc-freebsd13.1
diff --git a/llvm_patches/Revert-lldb-Add-terminfo-dependency-for-ncurses-support-.patch b/llvm_patches/Revert-lldb-Add-terminfo-dependency-for-ncurses-support-.patch
new file mode 100644
index 00000000..1b695cb2
--- /dev/null
+++ b/llvm_patches/Revert-lldb-Add-terminfo-dependency-for-ncurses-support-.patch
@@ -0,0 +1,59 @@
+From 234780ef33626acb37cd2e771d0839f36894c908 Mon Sep 17 00:00:00 2001
+From: Jordan R Abrahams-Whitehead <ajordanr@google.com>
+Date: Wed, 26 Feb 2025 19:25:38 +0000
+Subject: [PATCH] Revert "[lldb] Add terminfo dependency for ncurses support"
+
+This reverts commit 41875593066a603ff4cd746a430e481fd7d3a180.
+
+This commit is no longer needed given upstream has merged
+a proper fix with commits:
+
+[1] 8fff0c181f26a5e8b2344c061ebf2559118b1160
+[2] bb6a273d9ab9ee90dbb957e541f4d810fffb22ee
+
+patch.cherry: false
+patch.platforms: chromiumos
+patch.version_range.from: 537678
+patch.version_range.until: 566148
+
+---
+ lldb/cmake/modules/FindCursesAndPanel.cmake | 12 ++++--------
+ 1 file changed, 4 insertions(+), 8 deletions(-)
+
+diff --git a/lldb/cmake/modules/FindCursesAndPanel.cmake b/lldb/cmake/modules/FindCursesAndPanel.cmake
+index cd2a68a01a0d..aaadf214bf54 100644
+--- a/lldb/cmake/modules/FindCursesAndPanel.cmake
++++ b/lldb/cmake/modules/FindCursesAndPanel.cmake
+@@ -2,15 +2,12 @@
+ # FindCursesAndPanel
+ # -----------
+ #
+-# Find the curses, terminfo, and panel library as a whole.
+-# NOTE: This is patched due to a terminfo dependency error introduced in
+-# https://github.com/llvm/llvm-project/pull/92865
++# Find the curses and panel library as a whole.
+ 
+-if(CURSES_INCLUDE_DIRS AND CURSES_LIBRARIES AND TINFO_LIBRARIES AND PANEL_LIBRARIES)
++if(CURSES_INCLUDE_DIRS AND CURSES_LIBRARIES AND PANEL_LIBRARIES)
+   set(CURSESANDPANEL_FOUND TRUE)
+ else()
+   find_package(Curses QUIET)
+-  find_package(TINFO_LIBRARIES NAMES tinfo DOC "The curses tinfo library" QUIET)
+   find_library(PANEL_LIBRARIES NAMES panel DOC "The curses panel library" QUIET)
+   include(FindPackageHandleStandardArgs)
+   find_package_handle_standard_args(CursesAndPanel
+@@ -19,10 +16,9 @@ else()
+                                     REQUIRED_VARS
+                                       CURSES_INCLUDE_DIRS
+                                       CURSES_LIBRARIES
+-                                      TINFO_LIBRARIES
+                                       PANEL_LIBRARIES)
+-  if(CURSES_FOUND AND TINFO_LIBRARIES AND PANEL_LIBRARIES)
+-    mark_as_advanced(CURSES_INCLUDE_DIRS CURSES_LIBRARIES TINFO_LIBRARIES PANEL_LIBRARIES)
++  if(CURSES_FOUND AND PANEL_LIBRARIES)
++    mark_as_advanced(CURSES_INCLUDE_DIRS CURSES_LIBRARIES PANEL_LIBRARIES)
+   endif()
+ endif()
+ 
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/0227396417d4625bc93affdd8957ff8d90c76299.patch b/llvm_patches/cherry/0227396417d4625bc93affdd8957ff8d90c76299.patch
new file mode 100644
index 00000000..298d8a25
--- /dev/null
+++ b/llvm_patches/cherry/0227396417d4625bc93affdd8957ff8d90c76299.patch
@@ -0,0 +1,99 @@
+From 877d668ca557ee5d48d4c7321c8dfbda72712f7a Mon Sep 17 00:00:00 2001
+From: Nikolas Klauser <nikolasklauser@berlin.de>
+Date: Fri, 7 Feb 2025 15:40:16 +0100
+Subject: [PATCH] Revert "[libc++] Reduce std::conjunction overhead (#124259)"
+
+It turns out that the new implementation takes significantly more stack
+memory for some reason.
+
+This reverts commit 2696e4fb9567d23ce065a067e7f4909b310daf50.
+
+patch.cherry: true
+patch.metadata.original_sha: 0227396417d4625bc93affdd8957ff8d90c76299
+patch.platforms: chromiumos
+patch.version_range.from: 563049
+patch.version_range.until: 564591
+
+---
+ libcxx/include/__type_traits/conjunction.h | 42 ++++++++++++----------
+ 1 file changed, 24 insertions(+), 18 deletions(-)
+
+diff --git a/libcxx/include/__type_traits/conjunction.h b/libcxx/include/__type_traits/conjunction.h
+index 6b6717a50a46..ad9656acd47e 100644
+--- a/libcxx/include/__type_traits/conjunction.h
++++ b/libcxx/include/__type_traits/conjunction.h
+@@ -10,6 +10,8 @@
+ #define _LIBCPP___TYPE_TRAITS_CONJUNCTION_H
+ 
+ #include <__config>
++#include <__type_traits/conditional.h>
++#include <__type_traits/enable_if.h>
+ #include <__type_traits/integral_constant.h>
+ #include <__type_traits/is_same.h>
+ 
+@@ -19,29 +21,22 @@
+ 
+ _LIBCPP_BEGIN_NAMESPACE_STD
+ 
+-template <bool>
+-struct _AndImpl;
++template <class...>
++using __expand_to_true _LIBCPP_NODEBUG = true_type;
+ 
+-template <>
+-struct _AndImpl<true> {
+-  template <class _Res, class _First, class... _Rest>
+-  using _Result _LIBCPP_NODEBUG =
+-      typename _AndImpl<bool(_First::value) && sizeof...(_Rest) != 0>::template _Result<_First, _Rest...>;
+-};
++template <class... _Pred>
++__expand_to_true<__enable_if_t<_Pred::value>...> __and_helper(int);
+ 
+-template <>
+-struct _AndImpl<false> {
+-  template <class _Res, class...>
+-  using _Result _LIBCPP_NODEBUG = _Res;
+-};
++template <class...>
++false_type __and_helper(...);
+ 
+ // _And always performs lazy evaluation of its arguments.
+ //
+ // However, `_And<_Pred...>` itself will evaluate its result immediately (without having to
+ // be instantiated) since it is an alias, unlike `conjunction<_Pred...>`, which is a struct.
+ // If you want to defer the evaluation of `_And<_Pred...>` itself, use `_Lazy<_And, _Pred...>`.
+-template <class... _Args>
+-using _And _LIBCPP_NODEBUG = typename _AndImpl<sizeof...(_Args) != 0>::template _Result<true_type, _Args...>;
++template <class... _Pred>
++using _And _LIBCPP_NODEBUG = decltype(std::__and_helper<_Pred...>(0));
+ 
+ template <bool... _Preds>
+ struct __all_dummy;
+@@ -51,11 +46,22 @@ struct __all : _IsSame<__all_dummy<_Pred...>, __all_dummy<((void)_Pred, true)...
+ 
+ #if _LIBCPP_STD_VER >= 17
+ 
+-template <class... _Args>
+-struct _LIBCPP_NO_SPECIALIZATIONS conjunction : _And<_Args...> {};
++template <class...>
++struct _LIBCPP_NO_SPECIALIZATIONS conjunction : true_type {};
++
++_LIBCPP_DIAGNOSTIC_PUSH
++#  if __has_warning("-Winvalid-specialization")
++_LIBCPP_CLANG_DIAGNOSTIC_IGNORED("-Winvalid-specialization")
++#  endif
++template <class _Arg>
++struct conjunction<_Arg> : _Arg {};
++
++template <class _Arg, class... _Args>
++struct conjunction<_Arg, _Args...> : conditional_t<!bool(_Arg::value), _Arg, conjunction<_Args...>> {};
++_LIBCPP_DIAGNOSTIC_POP
+ 
+ template <class... _Args>
+-_LIBCPP_NO_SPECIALIZATIONS inline constexpr bool conjunction_v = _And<_Args...>::value;
++_LIBCPP_NO_SPECIALIZATIONS inline constexpr bool conjunction_v = conjunction<_Args...>::value;
+ 
+ #endif // _LIBCPP_STD_VER >= 17
+ 
+-- 
+2.49.0.504.g3bcea36a83-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/0a00d32c5f88fce89006dcde6e235bc77d7b495e.patch b/llvm_patches/cherry/0a00d32c5f88fce89006dcde6e235bc77d7b495e.patch
new file mode 100644
index 00000000..182ac210
--- /dev/null
+++ b/llvm_patches/cherry/0a00d32c5f88fce89006dcde6e235bc77d7b495e.patch
@@ -0,0 +1,63 @@
+From 0a00d32c5f88fce89006dcde6e235bc77d7b495e Mon Sep 17 00:00:00 2001
+From: Jordan R AW <ajordanr@google.com>
+Date: Thu, 29 Aug 2024 10:16:17 -0700
+Subject: [PATCH] [lldb] Add armv7a and armv8a ArchSpecs (#106433)
+
+armv7a and armv8a are common names for the application subarch for arm.
+
+These names in particular are used in ChromeOS, Android, and a few other
+known applications. In ChromeOS, we encountered a bug where armv7a arch
+was not recognised and segfaulted when starting an executable on an
+arm32 device.
+
+Google Issue Tracker:
+https://issuetracker.google.com/361414339
+---
+ lldb/include/lldb/Utility/ArchSpec.h | 2 ++
+ lldb/source/Utility/ArchSpec.cpp     | 4 ++++
+ 2 files changed, 6 insertions(+)
+
+diff --git a/lldb/include/lldb/Utility/ArchSpec.h b/lldb/include/lldb/Utility/ArchSpec.h
+index 50830b889b91..5990f984b09e 100644
+--- a/lldb/include/lldb/Utility/ArchSpec.h
++++ b/lldb/include/lldb/Utility/ArchSpec.h
+@@ -123,6 +123,7 @@ public:
+     eCore_arm_armv6,
+     eCore_arm_armv6m,
+     eCore_arm_armv7,
++    eCore_arm_armv7a,
+     eCore_arm_armv7l,
+     eCore_arm_armv7f,
+     eCore_arm_armv7s,
+@@ -145,6 +146,7 @@ public:
+     eCore_thumbv7em,
+     eCore_arm_arm64,
+     eCore_arm_armv8,
++    eCore_arm_armv8a,
+     eCore_arm_armv8l,
+     eCore_arm_arm64e,
+     eCore_arm_arm64_32,
+diff --git a/lldb/source/Utility/ArchSpec.cpp b/lldb/source/Utility/ArchSpec.cpp
+index 4fd1a800023c..85bb85044ec1 100644
+--- a/lldb/source/Utility/ArchSpec.cpp
++++ b/lldb/source/Utility/ArchSpec.cpp
+@@ -60,6 +60,8 @@ static const CoreDefinition g_core_definitions[] = {
+      "armv6m"},
+     {eByteOrderLittle, 4, 2, 4, llvm::Triple::arm, ArchSpec::eCore_arm_armv7,
+      "armv7"},
++    {eByteOrderLittle, 4, 2, 4, llvm::Triple::arm, ArchSpec::eCore_arm_armv7a,
++     "armv7a"},
+     {eByteOrderLittle, 4, 2, 4, llvm::Triple::arm, ArchSpec::eCore_arm_armv7l,
+      "armv7l"},
+     {eByteOrderLittle, 4, 2, 4, llvm::Triple::arm, ArchSpec::eCore_arm_armv7f,
+@@ -102,6 +104,8 @@ static const CoreDefinition g_core_definitions[] = {
+      ArchSpec::eCore_arm_arm64, "arm64"},
+     {eByteOrderLittle, 8, 4, 4, llvm::Triple::aarch64,
+      ArchSpec::eCore_arm_armv8, "armv8"},
++    {eByteOrderLittle, 8, 4, 4, llvm::Triple::aarch64,
++     ArchSpec::eCore_arm_armv8a, "armv8a"},
+     {eByteOrderLittle, 4, 2, 4, llvm::Triple::arm, ArchSpec::eCore_arm_armv8l,
+      "armv8l"},
+     {eByteOrderLittle, 8, 4, 4, llvm::Triple::aarch64,
+-- 
+2.46.0.469.g59c65b2a67-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/3ef64f7ab5b8651eab500cd944984379fce5f639.patch b/llvm_patches/cherry/3ef64f7ab5b8651eab500cd944984379fce5f639.patch
new file mode 100644
index 00000000..82df4a1c
--- /dev/null
+++ b/llvm_patches/cherry/3ef64f7ab5b8651eab500cd944984379fce5f639.patch
@@ -0,0 +1,301 @@
+From b2b5fccad7ab2abd773c3b1dbadd28006811bf1d Mon Sep 17 00:00:00 2001
+From: NAKAMURA Takumi <geek4civic@gmail.com>
+Date: Sun, 25 Aug 2024 08:17:48 +0900
+Subject: [PATCH] Revert "Enable logf128 constant folding for hosts with 128bit
+ long double (#104929)"
+
+ConstantFolding behaves differently depending on host's `HAS_IEE754_FLOAT128`.
+LLVM should not change the behavior depending on host configurations.
+
+This reverts commit 14c7e4a1844904f3db9b2dc93b722925a8c66b27.
+(llvmorg-20-init-3262-g14c7e4a18449 and llvmorg-20-init-3498-g001e423ac626)
+
+patch.cherry: true
+patch.metadata.original_sha: 3ef64f7ab5b8651eab500cd944984379fce5f639
+patch.platforms: chromiumos
+patch.version_range.from: 547379
+patch.version_range.until: 547478
+
+---
+ llvm/CMakeLists.txt                   |  2 ++
+ llvm/cmake/config-ix.cmake            | 18 +++++++++-------
+ llvm/include/llvm/ADT/APFloat.h       | 15 +++++++++++---
+ llvm/include/llvm/ADT/APInt.h         |  8 +++++++
+ llvm/include/llvm/Support/float128.h  | 14 +++++++------
+ llvm/lib/Analysis/CMakeLists.txt      |  6 ++++++
+ llvm/lib/Analysis/ConstantFolding.cpp | 30 ++++++---------------------
+ llvm/lib/Support/APFloat.cpp          | 24 +++++++++++++++++++--
+ 8 files changed, 75 insertions(+), 42 deletions(-)
+
+diff --git a/llvm/CMakeLists.txt b/llvm/CMakeLists.txt
+index 0c4b17720461..2a5a09fc3634 100644
+--- a/llvm/CMakeLists.txt
++++ b/llvm/CMakeLists.txt
+@@ -560,6 +560,8 @@ set(LLVM_USE_STATIC_ZSTD FALSE CACHE BOOL "Use static version of zstd. Can be TR
+ 
+ set(LLVM_ENABLE_CURL "OFF" CACHE STRING "Use libcurl for the HTTP client if available. Can be ON, OFF, or FORCE_ON")
+ 
++set(LLVM_HAS_LOGF128 "OFF" CACHE STRING "Use logf128 to constant fold fp128 logarithm calls. Can be ON, OFF, or FORCE_ON")
++
+ set(LLVM_ENABLE_HTTPLIB "OFF" CACHE STRING "Use cpp-httplib HTTP server library if available. Can be ON, OFF, or FORCE_ON")
+ 
+ set(LLVM_Z3_INSTALL_DIR "" CACHE STRING "Install directory of the Z3 solver.")
+diff --git a/llvm/cmake/config-ix.cmake b/llvm/cmake/config-ix.cmake
+index 471dd1615c2e..f76eacb9d513 100644
+--- a/llvm/cmake/config-ix.cmake
++++ b/llvm/cmake/config-ix.cmake
+@@ -246,6 +246,17 @@ else()
+   set(HAVE_LIBEDIT 0)
+ endif()
+ 
++if(LLVM_HAS_LOGF128)
++  include(CheckCXXSymbolExists)
++  check_cxx_symbol_exists(logf128 math.h HAS_LOGF128)
++
++  if(LLVM_HAS_LOGF128 STREQUAL FORCE_ON AND NOT HAS_LOGF128)
++    message(FATAL_ERROR "Failed to configure logf128")
++  endif()
++
++  set(LLVM_HAS_LOGF128 "${HAS_LOGF128}")
++endif()
++
+ # function checks
+ check_symbol_exists(arc4random "stdlib.h" HAVE_DECL_ARC4RANDOM)
+ find_package(Backtrace)
+@@ -259,13 +270,6 @@ if(C_SUPPORTS_WERROR_UNGUARDED_AVAILABILITY_NEW)
+   set(CMAKE_REQUIRED_FLAGS "${CMAKE_REQUIRED_FLAGS} -Werror=unguarded-availability-new")
+ endif()
+ 
+-check_cxx_symbol_exists(logf128 cmath HAS_LOGF128)
+-check_symbol_exists(__powerpc64le__ "" __PPC64LE)
+-if(HAS_LOGF128 AND NOT __PPC64LE)
+-    set(LLVM_HAS_LOGF128 On)
+-    add_compile_definitions(HAS_LOGF128)
+-endif()
+-
+ # Determine whether we can register EH tables.
+ check_symbol_exists(__register_frame "${CMAKE_CURRENT_LIST_DIR}/unwind.h" HAVE_REGISTER_FRAME)
+ check_symbol_exists(__deregister_frame "${CMAKE_CURRENT_LIST_DIR}/unwind.h" HAVE_DEREGISTER_FRAME)
+diff --git a/llvm/include/llvm/ADT/APFloat.h b/llvm/include/llvm/ADT/APFloat.h
+index 925d03d4c066..7039e961bff8 100644
+--- a/llvm/include/llvm/ADT/APFloat.h
++++ b/llvm/include/llvm/ADT/APFloat.h
+@@ -19,6 +19,7 @@
+ #include "llvm/ADT/ArrayRef.h"
+ #include "llvm/ADT/FloatingPointMode.h"
+ #include "llvm/Support/ErrorHandling.h"
++#include "llvm/Support/float128.h"
+ #include <memory>
+ 
+ #define APFLOAT_DISPATCH_ON_SEMANTICS(METHOD_CALL)                             \
+@@ -377,6 +378,9 @@ public:
+   Expected<opStatus> convertFromString(StringRef, roundingMode);
+   APInt bitcastToAPInt() const;
+   double convertToDouble() const;
++#ifdef HAS_IEE754_FLOAT128
++  float128 convertToQuad() const;
++#endif
+   float convertToFloat() const;
+ 
+   /// @}
+@@ -1270,9 +1274,14 @@ public:
+   /// shorter semantics, like IEEEsingle and others.
+   double convertToDouble() const;
+ 
+-  /// Return true if this APFloat has quadruple precision floating point
+-  /// semantics
+-  bool isValidIEEEQuad() const;
++  /// Converts this APFloat to host float value.
++  ///
++  /// \pre The APFloat must be built using semantics, that can be represented by
++  /// the host float type without loss of precision. It can be IEEEquad and
++  /// shorter semantics, like IEEEdouble and others.
++#ifdef HAS_IEE754_FLOAT128
++  float128 convertToQuad() const;
++#endif
+ 
+   /// Converts this APFloat to host float value.
+   ///
+diff --git a/llvm/include/llvm/ADT/APInt.h b/llvm/include/llvm/ADT/APInt.h
+index 13837413ae49..65ba3f15305c 100644
+--- a/llvm/include/llvm/ADT/APInt.h
++++ b/llvm/include/llvm/ADT/APInt.h
+@@ -17,6 +17,7 @@
+ 
+ #include "llvm/Support/Compiler.h"
+ #include "llvm/Support/MathExtras.h"
++#include "llvm/Support/float128.h"
+ #include <cassert>
+ #include <climits>
+ #include <cstring>
+@@ -1676,6 +1677,13 @@ public:
+   /// any bit width. Exactly 64 bits will be translated.
+   double bitsToDouble() const { return llvm::bit_cast<double>(getWord(0)); }
+ 
++#ifdef HAS_IEE754_FLOAT128
++  float128 bitsToQuad() const {
++    __uint128_t ul = ((__uint128_t)U.pVal[1] << 64) + U.pVal[0];
++    return llvm::bit_cast<float128>(ul);
++  }
++#endif
++
+   /// Converts APInt bits to a float
+   ///
+   /// The conversion does not do a translation from integer to float, it just
+diff --git a/llvm/include/llvm/Support/float128.h b/llvm/include/llvm/Support/float128.h
+index 618b320086ba..e15a98dc5a67 100644
+--- a/llvm/include/llvm/Support/float128.h
++++ b/llvm/include/llvm/Support/float128.h
+@@ -9,16 +9,18 @@
+ #ifndef LLVM_FLOAT128
+ #define LLVM_FLOAT128
+ 
+-#include <cmath>
+-
+ namespace llvm {
+ 
+-#ifdef HAS_LOGF128
+-#if !defined(__LONG_DOUBLE_IBM128__) && (__SIZEOF_INT128__ == 16)
+-typedef decltype(logf128(0.)) float128;
++#if defined(__clang__) && defined(__FLOAT128__) &&                             \
++    defined(__SIZEOF_INT128__) && !defined(__LONG_DOUBLE_IBM128__)
++#define HAS_IEE754_FLOAT128
++typedef __float128 float128;
++#elif defined(__FLOAT128__) && defined(__SIZEOF_INT128__) &&                   \
++    !defined(__LONG_DOUBLE_IBM128__) &&                                        \
++    (defined(__GNUC__) || defined(__GNUG__))
+ #define HAS_IEE754_FLOAT128
++typedef _Float128 float128;
+ #endif
+-#endif // HAS_LOGF128
+ 
+ } // namespace llvm
+ #endif // LLVM_FLOAT128
+diff --git a/llvm/lib/Analysis/CMakeLists.txt b/llvm/lib/Analysis/CMakeLists.txt
+index 3127f45cc54c..393803fad893 100644
+--- a/llvm/lib/Analysis/CMakeLists.txt
++++ b/llvm/lib/Analysis/CMakeLists.txt
+@@ -162,3 +162,9 @@ add_llvm_component_library(LLVMAnalysis
+   Support
+   TargetParser
+   )
++
++include(CheckCXXSymbolExists)
++check_cxx_symbol_exists(logf128 math.h HAS_LOGF128)
++if(HAS_LOGF128)
++ target_compile_definitions(LLVMAnalysis PRIVATE HAS_LOGF128)
++endif()
+diff --git a/llvm/lib/Analysis/ConstantFolding.cpp b/llvm/lib/Analysis/ConstantFolding.cpp
+index 81c4d4ec5be4..defcacdfa8b1 100644
+--- a/llvm/lib/Analysis/ConstantFolding.cpp
++++ b/llvm/lib/Analysis/ConstantFolding.cpp
+@@ -54,7 +54,6 @@
+ #include "llvm/Support/ErrorHandling.h"
+ #include "llvm/Support/KnownBits.h"
+ #include "llvm/Support/MathExtras.h"
+-#include "llvm/Support/float128.h"
+ #include <cassert>
+ #include <cerrno>
+ #include <cfenv>
+@@ -1742,7 +1741,7 @@ Constant *GetConstantFoldFPValue(double V, Type *Ty) {
+   llvm_unreachable("Can only constant fold half/float/double");
+ }
+ 
+-#if defined(HAS_IEE754_FLOAT128)
++#if defined(HAS_IEE754_FLOAT128) && defined(HAS_LOGF128)
+ Constant *GetConstantFoldFPValue128(float128 V, Type *Ty) {
+   if (Ty->isFP128Ty())
+     return ConstantFP::get(Ty, V);
+@@ -1782,25 +1781,11 @@ Constant *ConstantFoldFP(double (*NativeFP)(double), const APFloat &V,
+   return GetConstantFoldFPValue(Result, Ty);
+ }
+ 
+-#if defined(HAS_IEE754_FLOAT128)
+-float128 ConvertToQuad(const APFloat &Apf) {
+-  APInt Api = Apf.bitcastToAPInt();
+-  __uint128_t Uint128 =
+-      ((__uint128_t)Api.extractBitsAsZExtValue(64, 64) << 64) +
+-      Api.extractBitsAsZExtValue(64, 0);
+-  return llvm::bit_cast<float128>(Uint128);
+-}
+-#endif
+-
+-#if defined(HAS_IEE754_FLOAT128)
++#if defined(HAS_IEE754_FLOAT128) && defined(HAS_LOGF128)
+ Constant *ConstantFoldFP128(float128 (*NativeFP)(float128), const APFloat &V,
+                             Type *Ty) {
+   llvm_fenv_clearexcept();
+-  if (!V.isValidIEEEQuad())
+-    return nullptr;
+-
+-  float128 Result = NativeFP(ConvertToQuad(V));
+-
++  float128 Result = NativeFP(V.convertToQuad());
+   if (llvm_fenv_testexcept()) {
+     llvm_fenv_clearexcept();
+     return nullptr;
+@@ -2129,16 +2114,13 @@ static Constant *ConstantFoldScalarCall1(StringRef Name,
+     if (IntrinsicID == Intrinsic::canonicalize)
+       return constantFoldCanonicalize(Ty, Call, U);
+ 
+-#if defined(HAS_IEE754_FLOAT128)
++#if defined(HAS_IEE754_FLOAT128) && defined(HAS_LOGF128)
+     if (Ty->isFP128Ty()) {
+       if (IntrinsicID == Intrinsic::log) {
+-        APFloat Value = Op->getValueAPF();
+-        if (!Value.isValidIEEEQuad())
+-          return nullptr;
+-
+-        float128 Result = logf128(ConvertToQuad(Value));
++        float128 Result = logf128(Op->getValueAPF().convertToQuad());
+         return GetConstantFoldFPValue128(Result, Ty);
+       }
++
+       LibFunc Fp128Func = NotLibFunc;
+       if (TLI->getLibFunc(Name, Fp128Func) && TLI->has(Fp128Func) &&
+           Fp128Func == LibFunc_logl)
+diff --git a/llvm/lib/Support/APFloat.cpp b/llvm/lib/Support/APFloat.cpp
+index 2ddf99f56f88..7f68c5ab9b7c 100644
+--- a/llvm/lib/Support/APFloat.cpp
++++ b/llvm/lib/Support/APFloat.cpp
+@@ -3749,6 +3749,15 @@ double IEEEFloat::convertToDouble() const {
+   return api.bitsToDouble();
+ }
+ 
++#ifdef HAS_IEE754_FLOAT128
++float128 IEEEFloat::convertToQuad() const {
++  assert(semantics == (const llvm::fltSemantics *)&semIEEEquad &&
++         "Float semantics are not IEEEquads");
++  APInt api = bitcastToAPInt();
++  return api.bitsToQuad();
++}
++#endif
++
+ /// Integer bit is explicit in this format.  Intel hardware (387 and later)
+ /// does not support these bit patterns:
+ ///  exponent = all 1's, integer bit 0, significand 0 ("pseudoinfinity")
+@@ -5397,9 +5406,20 @@ double APFloat::convertToDouble() const {
+   return Temp.getIEEE().convertToDouble();
+ }
+ 
+-bool APFloat::isValidIEEEQuad() const {
+-  return (&getSemantics() == (const llvm::fltSemantics *)&semIEEEquad);
++#ifdef HAS_IEE754_FLOAT128
++float128 APFloat::convertToQuad() const {
++  if (&getSemantics() == (const llvm::fltSemantics *)&semIEEEquad)
++    return getIEEE().convertToQuad();
++  assert(getSemantics().isRepresentableBy(semIEEEquad) &&
++         "Float semantics is not representable by IEEEquad");
++  APFloat Temp = *this;
++  bool LosesInfo;
++  opStatus St = Temp.convert(semIEEEquad, rmNearestTiesToEven, &LosesInfo);
++  assert(!(St & opInexact) && !LosesInfo && "Unexpected imprecision");
++  (void)St;
++  return Temp.getIEEE().convertToQuad();
+ }
++#endif
+ 
+ float APFloat::convertToFloat() const {
+   if (&getSemantics() == (const llvm::fltSemantics *)&semIEEEsingle)
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/412e1af19a248fd5650e6828695c78454a9195fb.patch b/llvm_patches/cherry/412e1af19a248fd5650e6828695c78454a9195fb.patch
new file mode 100644
index 00000000..0e9c27d9
--- /dev/null
+++ b/llvm_patches/cherry/412e1af19a248fd5650e6828695c78454a9195fb.patch
@@ -0,0 +1,1257 @@
+From 5f980d32fdf3f5175860169451c7104ac32f83a7 Mon Sep 17 00:00:00 2001
+From: Sam Tebbs <samuel.tebbs@arm.com>
+Date: Fri, 20 Dec 2024 16:14:57 +0000
+Subject: [PATCH] Revert "[AArch64] Lower alias mask to a whilewr" (#120261)
+
+Reverts llvm/llvm-project#100769
+
+A bug in the lowering (the subtraction should be reversed) was found
+after merging and it will all be replaced by #117007 anyway.
+
+patch.cherry: true
+patch.metadata.original_sha: 412e1af19a248fd5650e6828695c78454a9195fb
+patch.platforms: chromiumos
+patch.version_range.from: 545094
+patch.version_range.until: 560071
+
+---
+ .../Target/AArch64/AArch64ISelLowering.cpp    |  121 --
+ llvm/test/CodeGen/AArch64/whilewr.ll          | 1086 -----------------
+ 2 files changed, 1207 deletions(-)
+ delete mode 100644 llvm/test/CodeGen/AArch64/whilewr.ll
+
+diff --git a/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp b/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
+index 8c2f85657ff8..be3468507797 100644
+--- a/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
++++ b/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
+@@ -1529,7 +1529,6 @@ AArch64TargetLowering::AArch64TargetLowering(const TargetMachine &TM,
+       setOperationAction(ISD::VECREDUCE_AND, VT, Custom);
+       setOperationAction(ISD::VECREDUCE_OR, VT, Custom);
+       setOperationAction(ISD::VECREDUCE_XOR, VT, Custom);
+-      setOperationAction(ISD::OR, VT, Custom);
+ 
+       setOperationAction(ISD::SELECT_CC, VT, Expand);
+       setOperationAction(ISD::INSERT_VECTOR_ELT, VT, Custom);
+@@ -13940,128 +13939,8 @@ static SDValue tryLowerToSLI(SDNode *N, SelectionDAG &DAG) {
+   return ResultSLI;
+ }
+ 
+-/// Try to lower the construction of a pointer alias mask to a WHILEWR.
+-/// The mask's enabled lanes represent the elements that will not overlap across
+-/// one loop iteration. This tries to match:
+-/// or (splat (setcc_lt (sub ptrA, ptrB), -(element_size - 1))),
+-///    (get_active_lane_mask 0, (div (sub ptrA, ptrB), element_size))
+-SDValue tryWhileWRFromOR(SDValue Op, SelectionDAG &DAG,
+-                         const AArch64Subtarget &Subtarget) {
+-  if (!Subtarget.hasSVE2())
+-    return SDValue();
+-  SDValue LaneMask = Op.getOperand(0);
+-  SDValue Splat = Op.getOperand(1);
+-
+-  if (Splat.getOpcode() != ISD::SPLAT_VECTOR)
+-    std::swap(LaneMask, Splat);
+-
+-  if (LaneMask.getOpcode() != ISD::INTRINSIC_WO_CHAIN ||
+-      LaneMask.getConstantOperandVal(0) != Intrinsic::get_active_lane_mask ||
+-      Splat.getOpcode() != ISD::SPLAT_VECTOR)
+-    return SDValue();
+-
+-  SDValue Cmp = Splat.getOperand(0);
+-  if (Cmp.getOpcode() != ISD::SETCC)
+-    return SDValue();
+-
+-  CondCodeSDNode *Cond = cast<CondCodeSDNode>(Cmp.getOperand(2));
+-
+-  auto ComparatorConst = dyn_cast<ConstantSDNode>(Cmp.getOperand(1));
+-  if (!ComparatorConst || ComparatorConst->getSExtValue() > 0 ||
+-      Cond->get() != ISD::CondCode::SETLT)
+-    return SDValue();
+-  unsigned CompValue = std::abs(ComparatorConst->getSExtValue());
+-  unsigned EltSize = CompValue + 1;
+-  if (!isPowerOf2_64(EltSize) || EltSize > 8)
+-    return SDValue();
+-
+-  SDValue Diff = Cmp.getOperand(0);
+-  if (Diff.getOpcode() != ISD::SUB || Diff.getValueType() != MVT::i64)
+-    return SDValue();
+-
+-  if (!isNullConstant(LaneMask.getOperand(1)) ||
+-      (EltSize != 1 && LaneMask.getOperand(2).getOpcode() != ISD::SRA))
+-    return SDValue();
+-
+-  // The number of elements that alias is calculated by dividing the positive
+-  // difference between the pointers by the element size. An alias mask for i8
+-  // elements omits the division because it would just divide by 1
+-  if (EltSize > 1) {
+-    SDValue DiffDiv = LaneMask.getOperand(2);
+-    auto DiffDivConst = dyn_cast<ConstantSDNode>(DiffDiv.getOperand(1));
+-    if (!DiffDivConst || DiffDivConst->getZExtValue() != Log2_64(EltSize))
+-      return SDValue();
+-    if (EltSize > 2) {
+-      // When masking i32 or i64 elements, the positive value of the
+-      // possibly-negative difference comes from a select of the difference if
+-      // it's positive, otherwise the difference plus the element size if it's
+-      // negative: pos_diff = diff < 0 ? (diff + 7) : diff
+-      SDValue Select = DiffDiv.getOperand(0);
+-      // Make sure the difference is being compared by the select
+-      if (Select.getOpcode() != ISD::SELECT_CC || Select.getOperand(3) != Diff)
+-        return SDValue();
+-      // Make sure it's checking if the difference is less than 0
+-      if (!isNullConstant(Select.getOperand(1)) ||
+-          cast<CondCodeSDNode>(Select.getOperand(4))->get() !=
+-              ISD::CondCode::SETLT)
+-        return SDValue();
+-      // An add creates a positive value from the negative difference
+-      SDValue Add = Select.getOperand(2);
+-      if (Add.getOpcode() != ISD::ADD || Add.getOperand(0) != Diff)
+-        return SDValue();
+-      if (auto *AddConst = dyn_cast<ConstantSDNode>(Add.getOperand(1));
+-          !AddConst || AddConst->getZExtValue() != EltSize - 1)
+-        return SDValue();
+-    } else {
+-      // When masking i16 elements, this positive value comes from adding the
+-      // difference's sign bit to the difference itself. This is equivalent to
+-      // the 32 bit and 64 bit case: pos_diff = diff + sign_bit (diff)
+-      SDValue Add = DiffDiv.getOperand(0);
+-      if (Add.getOpcode() != ISD::ADD || Add.getOperand(0) != Diff)
+-        return SDValue();
+-      // A logical right shift by 63 extracts the sign bit from the difference
+-      SDValue Shift = Add.getOperand(1);
+-      if (Shift.getOpcode() != ISD::SRL || Shift.getOperand(0) != Diff)
+-        return SDValue();
+-      if (auto *ShiftConst = dyn_cast<ConstantSDNode>(Shift.getOperand(1));
+-          !ShiftConst || ShiftConst->getZExtValue() != 63)
+-        return SDValue();
+-    }
+-  } else if (LaneMask.getOperand(2) != Diff)
+-    return SDValue();
+-
+-  SDValue StorePtr = Diff.getOperand(0);
+-  SDValue ReadPtr = Diff.getOperand(1);
+-
+-  unsigned IntrinsicID = 0;
+-  switch (EltSize) {
+-  case 1:
+-    IntrinsicID = Intrinsic::aarch64_sve_whilewr_b;
+-    break;
+-  case 2:
+-    IntrinsicID = Intrinsic::aarch64_sve_whilewr_h;
+-    break;
+-  case 4:
+-    IntrinsicID = Intrinsic::aarch64_sve_whilewr_s;
+-    break;
+-  case 8:
+-    IntrinsicID = Intrinsic::aarch64_sve_whilewr_d;
+-    break;
+-  default:
+-    return SDValue();
+-  }
+-  SDLoc DL(Op);
+-  SDValue ID = DAG.getConstant(IntrinsicID, DL, MVT::i32);
+-  return DAG.getNode(ISD::INTRINSIC_WO_CHAIN, DL, Op.getValueType(), ID,
+-                     StorePtr, ReadPtr);
+-}
+-
+ SDValue AArch64TargetLowering::LowerVectorOR(SDValue Op,
+                                              SelectionDAG &DAG) const {
+-  if (SDValue SV =
+-          tryWhileWRFromOR(Op, DAG, DAG.getSubtarget<AArch64Subtarget>()))
+-    return SV;
+-
+   if (useSVEForFixedLengthVectorVT(Op.getValueType(),
+                                    !Subtarget->isNeonAvailable()))
+     return LowerToScalableOp(Op, DAG);
+diff --git a/llvm/test/CodeGen/AArch64/whilewr.ll b/llvm/test/CodeGen/AArch64/whilewr.ll
+deleted file mode 100644
+index 9f1ea8507923..000000000000
+--- a/llvm/test/CodeGen/AArch64/whilewr.ll
++++ /dev/null
+@@ -1,1086 +0,0 @@
+-; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 4
+-; RUN: llc %s -mtriple=aarch64-linux-gnu -mattr=+sve2 -o - | FileCheck %s
+-; RUN: llc %s -mtriple=aarch64-linux-gnu -mattr=+sve -o - | FileCheck %s --check-prefix=CHECK-NOSVE2
+-
+-define <vscale x 16 x i1> @whilewr_8(ptr noalias %a, ptr %b, ptr %c, i32 %n) {
+-; CHECK-LABEL: whilewr_8:
+-; CHECK:       // %bb.0: // %entry
+-; CHECK-NEXT:    whilewr p0.b, x1, x2
+-; CHECK-NEXT:    ret
+-;
+-; CHECK-NOSVE2-LABEL: whilewr_8:
+-; CHECK-NOSVE2:       // %bb.0: // %entry
+-; CHECK-NOSVE2-NEXT:    sub x8, x1, x2
+-; CHECK-NOSVE2-NEXT:    cmp x8, #0
+-; CHECK-NOSVE2-NEXT:    cset w9, lt
+-; CHECK-NOSVE2-NEXT:    whilelo p0.b, xzr, x8
+-; CHECK-NOSVE2-NEXT:    sbfx x8, x9, #0, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p1.b, xzr, x8
+-; CHECK-NOSVE2-NEXT:    sel p0.b, p0, p0.b, p1.b
+-; CHECK-NOSVE2-NEXT:    ret
+-entry:
+-  %c14 = ptrtoint ptr %c to i64
+-  %b15 = ptrtoint ptr %b to i64
+-  %sub.diff = sub i64 %b15, %c14
+-  %neg.compare = icmp slt i64 %sub.diff, 0
+-  %.splatinsert = insertelement <vscale x 16 x i1> poison, i1 %neg.compare, i64 0
+-  %.splat = shufflevector <vscale x 16 x i1> %.splatinsert, <vscale x 16 x i1> poison, <vscale x 16 x i32> zeroinitializer
+-  %ptr.diff.lane.mask = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %sub.diff)
+-  %active.lane.mask.alias = or <vscale x 16 x i1> %ptr.diff.lane.mask, %.splat
+-  ret <vscale x 16 x i1> %active.lane.mask.alias
+-}
+-
+-define <vscale x 16 x i1> @whilewr_commutative(ptr noalias %a, ptr %b, ptr %c, i32 %n) {
+-; CHECK-LABEL: whilewr_commutative:
+-; CHECK:       // %bb.0: // %entry
+-; CHECK-NEXT:    whilewr p0.b, x1, x2
+-; CHECK-NEXT:    ret
+-;
+-; CHECK-NOSVE2-LABEL: whilewr_commutative:
+-; CHECK-NOSVE2:       // %bb.0: // %entry
+-; CHECK-NOSVE2-NEXT:    sub x8, x1, x2
+-; CHECK-NOSVE2-NEXT:    cmp x8, #0
+-; CHECK-NOSVE2-NEXT:    cset w9, lt
+-; CHECK-NOSVE2-NEXT:    whilelo p0.b, xzr, x8
+-; CHECK-NOSVE2-NEXT:    sbfx x8, x9, #0, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p1.b, xzr, x8
+-; CHECK-NOSVE2-NEXT:    mov p0.b, p1/m, p1.b
+-; CHECK-NOSVE2-NEXT:    ret
+-entry:
+-  %c14 = ptrtoint ptr %c to i64
+-  %b15 = ptrtoint ptr %b to i64
+-  %sub.diff = sub i64 %b15, %c14
+-  %neg.compare = icmp slt i64 %sub.diff, 0
+-  %.splatinsert = insertelement <vscale x 16 x i1> poison, i1 %neg.compare, i64 0
+-  %.splat = shufflevector <vscale x 16 x i1> %.splatinsert, <vscale x 16 x i1> poison, <vscale x 16 x i32> zeroinitializer
+-  %ptr.diff.lane.mask = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %sub.diff)
+-  %active.lane.mask.alias = or <vscale x 16 x i1> %.splat, %ptr.diff.lane.mask
+-  ret <vscale x 16 x i1> %active.lane.mask.alias
+-}
+-
+-define <vscale x 8 x i1> @whilewr_16(ptr noalias %a, ptr %b, ptr %c, i32 %n) {
+-; CHECK-LABEL: whilewr_16:
+-; CHECK:       // %bb.0: // %entry
+-; CHECK-NEXT:    whilewr p0.h, x1, x2
+-; CHECK-NEXT:    ret
+-;
+-; CHECK-NOSVE2-LABEL: whilewr_16:
+-; CHECK-NOSVE2:       // %bb.0: // %entry
+-; CHECK-NOSVE2-NEXT:    sub x8, x1, x2
+-; CHECK-NOSVE2-NEXT:    cmn x8, #1
+-; CHECK-NOSVE2-NEXT:    add x8, x8, x8, lsr #63
+-; CHECK-NOSVE2-NEXT:    cset w9, lt
+-; CHECK-NOSVE2-NEXT:    sbfx x9, x9, #0, #1
+-; CHECK-NOSVE2-NEXT:    asr x8, x8, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p0.h, xzr, x9
+-; CHECK-NOSVE2-NEXT:    whilelo p1.h, xzr, x8
+-; CHECK-NOSVE2-NEXT:    mov p0.b, p1/m, p1.b
+-; CHECK-NOSVE2-NEXT:    ret
+-entry:
+-  %b14 = ptrtoint ptr %b to i64
+-  %c15 = ptrtoint ptr %c to i64
+-  %sub.diff = sub i64 %b14, %c15
+-  %diff = sdiv i64 %sub.diff, 2
+-  %neg.compare = icmp slt i64 %sub.diff, -1
+-  %.splatinsert = insertelement <vscale x 8 x i1> poison, i1 %neg.compare, i64 0
+-  %.splat = shufflevector <vscale x 8 x i1> %.splatinsert, <vscale x 8 x i1> poison, <vscale x 8 x i32> zeroinitializer
+-  %ptr.diff.lane.mask = tail call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i64(i64 0, i64 %diff)
+-  %active.lane.mask.alias = or <vscale x 8 x i1> %ptr.diff.lane.mask, %.splat
+-  ret <vscale x 8 x i1> %active.lane.mask.alias
+-}
+-
+-define <vscale x 4 x i1> @whilewr_32(ptr noalias %a, ptr %b, ptr %c, i32 %n) {
+-; CHECK-LABEL: whilewr_32:
+-; CHECK:       // %bb.0: // %entry
+-; CHECK-NEXT:    whilewr p0.s, x1, x2
+-; CHECK-NEXT:    ret
+-;
+-; CHECK-NOSVE2-LABEL: whilewr_32:
+-; CHECK-NOSVE2:       // %bb.0: // %entry
+-; CHECK-NOSVE2-NEXT:    sub x8, x1, x2
+-; CHECK-NOSVE2-NEXT:    add x9, x8, #3
+-; CHECK-NOSVE2-NEXT:    cmp x8, #0
+-; CHECK-NOSVE2-NEXT:    csel x9, x9, x8, lt
+-; CHECK-NOSVE2-NEXT:    cmn x8, #3
+-; CHECK-NOSVE2-NEXT:    cset w8, lt
+-; CHECK-NOSVE2-NEXT:    asr x9, x9, #2
+-; CHECK-NOSVE2-NEXT:    sbfx x8, x8, #0, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p1.s, xzr, x9
+-; CHECK-NOSVE2-NEXT:    whilelo p0.s, xzr, x8
+-; CHECK-NOSVE2-NEXT:    mov p0.b, p1/m, p1.b
+-; CHECK-NOSVE2-NEXT:    ret
+-entry:
+-  %b12 = ptrtoint ptr %b to i64
+-  %c13 = ptrtoint ptr %c to i64
+-  %sub.diff = sub i64 %b12, %c13
+-  %diff = sdiv i64 %sub.diff, 4
+-  %neg.compare = icmp slt i64 %sub.diff, -3
+-  %.splatinsert = insertelement <vscale x 4 x i1> poison, i1 %neg.compare, i64 0
+-  %.splat = shufflevector <vscale x 4 x i1> %.splatinsert, <vscale x 4 x i1> poison, <vscale x 4 x i32> zeroinitializer
+-  %ptr.diff.lane.mask = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 0, i64 %diff)
+-  %active.lane.mask.alias = or <vscale x 4 x i1> %ptr.diff.lane.mask, %.splat
+-  ret <vscale x 4 x i1> %active.lane.mask.alias
+-}
+-
+-define <vscale x 2 x i1> @whilewr_64(ptr noalias %a, ptr %b, ptr %c, i32 %n) {
+-; CHECK-LABEL: whilewr_64:
+-; CHECK:       // %bb.0: // %entry
+-; CHECK-NEXT:    whilewr p0.d, x1, x2
+-; CHECK-NEXT:    ret
+-;
+-; CHECK-NOSVE2-LABEL: whilewr_64:
+-; CHECK-NOSVE2:       // %bb.0: // %entry
+-; CHECK-NOSVE2-NEXT:    sub x8, x1, x2
+-; CHECK-NOSVE2-NEXT:    add x9, x8, #7
+-; CHECK-NOSVE2-NEXT:    cmp x8, #0
+-; CHECK-NOSVE2-NEXT:    csel x9, x9, x8, lt
+-; CHECK-NOSVE2-NEXT:    cmn x8, #7
+-; CHECK-NOSVE2-NEXT:    cset w8, lt
+-; CHECK-NOSVE2-NEXT:    asr x9, x9, #3
+-; CHECK-NOSVE2-NEXT:    sbfx x8, x8, #0, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p1.d, xzr, x9
+-; CHECK-NOSVE2-NEXT:    whilelo p0.d, xzr, x8
+-; CHECK-NOSVE2-NEXT:    mov p0.b, p1/m, p1.b
+-; CHECK-NOSVE2-NEXT:    ret
+-entry:
+-  %b12 = ptrtoint ptr %b to i64
+-  %c13 = ptrtoint ptr %c to i64
+-  %sub.diff = sub i64 %b12, %c13
+-  %diff = sdiv i64 %sub.diff, 8
+-  %neg.compare = icmp slt i64 %sub.diff, -7
+-  %.splatinsert = insertelement <vscale x 2 x i1> poison, i1 %neg.compare, i64 0
+-  %.splat = shufflevector <vscale x 2 x i1> %.splatinsert, <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer
+-  %ptr.diff.lane.mask = tail call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 0, i64 %diff)
+-  %active.lane.mask.alias = or <vscale x 2 x i1> %ptr.diff.lane.mask, %.splat
+-  ret <vscale x 2 x i1> %active.lane.mask.alias
+-}
+-
+-define <vscale x 1 x i1> @no_whilewr_128(ptr noalias %a, ptr %b, ptr %c, i32 %n) {
+-; CHECK-LABEL: no_whilewr_128:
+-; CHECK:       // %bb.0: // %entry
+-; CHECK-NEXT:    sub x8, x1, x2
+-; CHECK-NEXT:    index z0.d, #0, #1
+-; CHECK-NEXT:    ptrue p0.d
+-; CHECK-NEXT:    add x9, x8, #15
+-; CHECK-NEXT:    cmp x8, #0
+-; CHECK-NEXT:    csel x9, x9, x8, lt
+-; CHECK-NEXT:    cmn x8, #15
+-; CHECK-NEXT:    asr x9, x9, #4
+-; CHECK-NEXT:    cset w8, lt
+-; CHECK-NEXT:    sbfx x8, x8, #0, #1
+-; CHECK-NEXT:    mov z1.d, x9
+-; CHECK-NEXT:    whilelo p1.d, xzr, x8
+-; CHECK-NEXT:    cmphi p0.d, p0/z, z1.d, z0.d
+-; CHECK-NEXT:    punpklo p1.h, p1.b
+-; CHECK-NEXT:    punpklo p0.h, p0.b
+-; CHECK-NEXT:    sel p0.b, p0, p0.b, p1.b
+-; CHECK-NEXT:    ret
+-;
+-; CHECK-NOSVE2-LABEL: no_whilewr_128:
+-; CHECK-NOSVE2:       // %bb.0: // %entry
+-; CHECK-NOSVE2-NEXT:    sub x8, x1, x2
+-; CHECK-NOSVE2-NEXT:    index z0.d, #0, #1
+-; CHECK-NOSVE2-NEXT:    ptrue p0.d
+-; CHECK-NOSVE2-NEXT:    add x9, x8, #15
+-; CHECK-NOSVE2-NEXT:    cmp x8, #0
+-; CHECK-NOSVE2-NEXT:    csel x9, x9, x8, lt
+-; CHECK-NOSVE2-NEXT:    cmn x8, #15
+-; CHECK-NOSVE2-NEXT:    asr x9, x9, #4
+-; CHECK-NOSVE2-NEXT:    cset w8, lt
+-; CHECK-NOSVE2-NEXT:    sbfx x8, x8, #0, #1
+-; CHECK-NOSVE2-NEXT:    mov z1.d, x9
+-; CHECK-NOSVE2-NEXT:    whilelo p1.d, xzr, x8
+-; CHECK-NOSVE2-NEXT:    cmphi p0.d, p0/z, z1.d, z0.d
+-; CHECK-NOSVE2-NEXT:    punpklo p1.h, p1.b
+-; CHECK-NOSVE2-NEXT:    punpklo p0.h, p0.b
+-; CHECK-NOSVE2-NEXT:    sel p0.b, p0, p0.b, p1.b
+-; CHECK-NOSVE2-NEXT:    ret
+-entry:
+-  %b12 = ptrtoint ptr %b to i64
+-  %c13 = ptrtoint ptr %c to i64
+-  %sub.diff = sub i64 %b12, %c13
+-  %diff = sdiv i64 %sub.diff, 16
+-  %neg.compare = icmp slt i64 %sub.diff, -15
+-  %.splatinsert = insertelement <vscale x 1 x i1> poison, i1 %neg.compare, i64 0
+-  %.splat = shufflevector <vscale x 1 x i1> %.splatinsert, <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer
+-  %ptr.diff.lane.mask = tail call <vscale x 1 x i1> @llvm.get.active.lane.mask.nxv1i1.i64(i64 0, i64 %diff)
+-  %active.lane.mask.alias = or <vscale x 1 x i1> %ptr.diff.lane.mask, %.splat
+-  ret <vscale x 1 x i1> %active.lane.mask.alias
+-}
+-
+-define void @whilewr_loop_8(ptr noalias %a, ptr %b, ptr %c, i32 %n) {
+-; CHECK-LABEL: whilewr_loop_8:
+-; CHECK:       // %bb.0: // %entry
+-; CHECK-NEXT:    cmp w3, #1
+-; CHECK-NEXT:    b.lt .LBB6_3
+-; CHECK-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NEXT:    whilewr p0.b, x1, x2
+-; CHECK-NEXT:    mov w9, w3
+-; CHECK-NEXT:    mov x8, xzr
+-; CHECK-NEXT:    whilelo p1.b, xzr, x9
+-; CHECK-NEXT:    cntp x10, p0, p0.b
+-; CHECK-NEXT:    and x10, x10, #0xff
+-; CHECK-NEXT:  .LBB6_2: // %vector.body
+-; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NEXT:    and p1.b, p1/z, p1.b, p0.b
+-; CHECK-NEXT:    ld1b { z0.b }, p1/z, [x0, x8]
+-; CHECK-NEXT:    ld1b { z1.b }, p1/z, [x1, x8]
+-; CHECK-NEXT:    add z0.b, z1.b, z0.b
+-; CHECK-NEXT:    st1b { z0.b }, p1, [x2, x8]
+-; CHECK-NEXT:    add x8, x8, x10
+-; CHECK-NEXT:    whilelo p1.b, x8, x9
+-; CHECK-NEXT:    b.mi .LBB6_2
+-; CHECK-NEXT:  .LBB6_3: // %for.cond.cleanup
+-; CHECK-NEXT:    ret
+-;
+-; CHECK-NOSVE2-LABEL: whilewr_loop_8:
+-; CHECK-NOSVE2:       // %bb.0: // %entry
+-; CHECK-NOSVE2-NEXT:    cmp w3, #1
+-; CHECK-NOSVE2-NEXT:    b.lt .LBB6_3
+-; CHECK-NOSVE2-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NOSVE2-NEXT:    sub x9, x1, x2
+-; CHECK-NOSVE2-NEXT:    mov x8, xzr
+-; CHECK-NOSVE2-NEXT:    cmp x9, #0
+-; CHECK-NOSVE2-NEXT:    cset w10, lt
+-; CHECK-NOSVE2-NEXT:    whilelo p0.b, xzr, x9
+-; CHECK-NOSVE2-NEXT:    sbfx x9, x10, #0, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p1.b, xzr, x9
+-; CHECK-NOSVE2-NEXT:    mov w9, w3
+-; CHECK-NOSVE2-NEXT:    sel p0.b, p0, p0.b, p1.b
+-; CHECK-NOSVE2-NEXT:    whilelo p1.b, xzr, x9
+-; CHECK-NOSVE2-NEXT:    cntp x10, p0, p0.b
+-; CHECK-NOSVE2-NEXT:    and x10, x10, #0xff
+-; CHECK-NOSVE2-NEXT:  .LBB6_2: // %vector.body
+-; CHECK-NOSVE2-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NOSVE2-NEXT:    and p1.b, p1/z, p1.b, p0.b
+-; CHECK-NOSVE2-NEXT:    ld1b { z0.b }, p1/z, [x0, x8]
+-; CHECK-NOSVE2-NEXT:    ld1b { z1.b }, p1/z, [x1, x8]
+-; CHECK-NOSVE2-NEXT:    add z0.b, z1.b, z0.b
+-; CHECK-NOSVE2-NEXT:    st1b { z0.b }, p1, [x2, x8]
+-; CHECK-NOSVE2-NEXT:    add x8, x8, x10
+-; CHECK-NOSVE2-NEXT:    whilelo p1.b, x8, x9
+-; CHECK-NOSVE2-NEXT:    b.mi .LBB6_2
+-; CHECK-NOSVE2-NEXT:  .LBB6_3: // %for.cond.cleanup
+-; CHECK-NOSVE2-NEXT:    ret
+-entry:
+-  %cmp11 = icmp sgt i32 %n, 0
+-  br i1 %cmp11, label %for.body.preheader, label %for.cond.cleanup
+-
+-for.body.preheader:
+-  %c14 = ptrtoint ptr %c to i64
+-  %b15 = ptrtoint ptr %b to i64
+-  %wide.trip.count = zext nneg i32 %n to i64
+-  %sub.diff = sub i64 %b15, %c14
+-  %neg.compare = icmp slt i64 %sub.diff, 0
+-  %.splatinsert = insertelement <vscale x 16 x i1> poison, i1 %neg.compare, i64 0
+-  %.splat = shufflevector <vscale x 16 x i1> %.splatinsert, <vscale x 16 x i1> poison, <vscale x 16 x i32> zeroinitializer
+-  %ptr.diff.lane.mask = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %sub.diff)
+-  %active.lane.mask.alias = or <vscale x 16 x i1> %ptr.diff.lane.mask, %.splat
+-  %active.lane.mask.entry = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %wide.trip.count)
+-  %0 = zext <vscale x 16 x i1> %active.lane.mask.alias to <vscale x 16 x i8>
+-  %1 = tail call i8 @llvm.vector.reduce.add.nxv16i8(<vscale x 16 x i8> %0)
+-  %2 = zext i8 %1 to i64
+-  br label %vector.body
+-
+-vector.body:
+-  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
+-  %active.lane.mask = phi <vscale x 16 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
+-  %3 = and <vscale x 16 x i1> %active.lane.mask, %active.lane.mask.alias
+-  %4 = getelementptr inbounds i8, ptr %a, i64 %index
+-  %wide.masked.load = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr %4, i32 1, <vscale x 16 x i1> %3, <vscale x 16 x i8> poison)
+-  %5 = getelementptr inbounds i8, ptr %b, i64 %index
+-  %wide.masked.load16 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr %5, i32 1, <vscale x 16 x i1> %3, <vscale x 16 x i8> poison)
+-  %6 = add <vscale x 16 x i8> %wide.masked.load16, %wide.masked.load
+-  %7 = getelementptr inbounds i8, ptr %c, i64 %index
+-  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %6, ptr %7, i32 1, <vscale x 16 x i1> %3)
+-  %index.next = add i64 %index, %2
+-  %active.lane.mask.next = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %index.next, i64 %wide.trip.count)
+-  %8 = extractelement <vscale x 16 x i1> %active.lane.mask.next, i64 0
+-  br i1 %8, label %vector.body, label %for.cond.cleanup
+-
+-for.cond.cleanup:
+-  ret void
+-}
+-
+-define void @whilewr_loop_16(ptr noalias %a, ptr %b, ptr %c, i32 %n) {
+-; CHECK-LABEL: whilewr_loop_16:
+-; CHECK:       // %bb.0: // %entry
+-; CHECK-NEXT:    cmp w3, #1
+-; CHECK-NEXT:    b.lt .LBB7_3
+-; CHECK-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NEXT:    mov w8, w3
+-; CHECK-NEXT:    whilewr p1.h, x1, x2
+-; CHECK-NEXT:    mov x9, xzr
+-; CHECK-NEXT:    whilelo p0.h, xzr, x8
+-; CHECK-NEXT:    and p0.b, p1/z, p1.b, p0.b
+-; CHECK-NEXT:  .LBB7_2: // %vector.body
+-; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NEXT:    ld1h { z0.h }, p0/z, [x0, x9, lsl #1]
+-; CHECK-NEXT:    ld1h { z1.h }, p0/z, [x1, x9, lsl #1]
+-; CHECK-NEXT:    add z0.h, z1.h, z0.h
+-; CHECK-NEXT:    st1h { z0.h }, p0, [x2, x9, lsl #1]
+-; CHECK-NEXT:    inch x9
+-; CHECK-NEXT:    whilelo p0.h, x9, x8
+-; CHECK-NEXT:    b.mi .LBB7_2
+-; CHECK-NEXT:  .LBB7_3: // %for.cond.cleanup
+-; CHECK-NEXT:    ret
+-;
+-; CHECK-NOSVE2-LABEL: whilewr_loop_16:
+-; CHECK-NOSVE2:       // %bb.0: // %entry
+-; CHECK-NOSVE2-NEXT:    cmp w3, #1
+-; CHECK-NOSVE2-NEXT:    b.lt .LBB7_3
+-; CHECK-NOSVE2-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NOSVE2-NEXT:    mov w9, w3
+-; CHECK-NOSVE2-NEXT:    sub x10, x1, x2
+-; CHECK-NOSVE2-NEXT:    mov x8, xzr
+-; CHECK-NOSVE2-NEXT:    whilelo p0.h, xzr, x9
+-; CHECK-NOSVE2-NEXT:    cmn x10, #1
+-; CHECK-NOSVE2-NEXT:    add x10, x10, x10, lsr #63
+-; CHECK-NOSVE2-NEXT:    cset w11, lt
+-; CHECK-NOSVE2-NEXT:    sbfx x11, x11, #0, #1
+-; CHECK-NOSVE2-NEXT:    asr x10, x10, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p1.h, xzr, x11
+-; CHECK-NOSVE2-NEXT:    whilelo p2.h, xzr, x10
+-; CHECK-NOSVE2-NEXT:    cnth x10
+-; CHECK-NOSVE2-NEXT:    mov p1.b, p2/m, p2.b
+-; CHECK-NOSVE2-NEXT:    and p0.b, p1/z, p1.b, p0.b
+-; CHECK-NOSVE2-NEXT:  .LBB7_2: // %vector.body
+-; CHECK-NOSVE2-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NOSVE2-NEXT:    ld1h { z0.h }, p0/z, [x0, x8, lsl #1]
+-; CHECK-NOSVE2-NEXT:    ld1h { z1.h }, p0/z, [x1, x8, lsl #1]
+-; CHECK-NOSVE2-NEXT:    add z0.h, z1.h, z0.h
+-; CHECK-NOSVE2-NEXT:    st1h { z0.h }, p0, [x2, x8, lsl #1]
+-; CHECK-NOSVE2-NEXT:    add x8, x8, x10
+-; CHECK-NOSVE2-NEXT:    whilelo p0.h, x8, x9
+-; CHECK-NOSVE2-NEXT:    b.mi .LBB7_2
+-; CHECK-NOSVE2-NEXT:  .LBB7_3: // %for.cond.cleanup
+-; CHECK-NOSVE2-NEXT:    ret
+-entry:
+-  %cmp11 = icmp sgt i32 %n, 0
+-  br i1 %cmp11, label %for.body.preheader, label %for.cond.cleanup
+-
+-for.body.preheader:
+-  %b14 = ptrtoint ptr %b to i64
+-  %c15 = ptrtoint ptr %c to i64
+-  %wide.trip.count = zext nneg i32 %n to i64
+-  %0 = tail call i64 @llvm.vscale.i64()
+-  %1 = shl nuw nsw i64 %0, 3
+-  %active.lane.mask.entry = tail call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i64(i64 0, i64 %wide.trip.count)
+-  %sub.diff = sub i64 %b14, %c15
+-  %diff = sdiv i64 %sub.diff, 2
+-  %neg.compare = icmp slt i64 %sub.diff, -1
+-  %.splatinsert = insertelement <vscale x 8 x i1> poison, i1 %neg.compare, i64 0
+-  %.splat = shufflevector <vscale x 8 x i1> %.splatinsert, <vscale x 8 x i1> poison, <vscale x 8 x i32> zeroinitializer
+-  %ptr.diff.lane.mask = tail call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i64(i64 0, i64 %diff)
+-  %active.lane.mask.alias = or <vscale x 8 x i1> %ptr.diff.lane.mask, %.splat
+-  %2 = and <vscale x 8 x i1> %active.lane.mask.alias, %active.lane.mask.entry
+-  br label %vector.body
+-
+-vector.body:
+-  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
+-  %active.lane.mask = phi <vscale x 8 x i1> [ %2, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
+-  %3 = getelementptr inbounds i16, ptr %a, i64 %index
+-  %wide.masked.load = tail call <vscale x 8 x i16> @llvm.masked.load.nxv8i16.p0(ptr %3, i32 2, <vscale x 8 x i1> %active.lane.mask, <vscale x 8 x i16> poison)
+-  %4 = getelementptr inbounds i16, ptr %b, i64 %index
+-  %wide.masked.load16 = tail call <vscale x 8 x i16> @llvm.masked.load.nxv8i16.p0(ptr %4, i32 2, <vscale x 8 x i1> %active.lane.mask, <vscale x 8 x i16> poison)
+-  %5 = add <vscale x 8 x i16> %wide.masked.load16, %wide.masked.load
+-  %6 = getelementptr inbounds i16, ptr %c, i64 %index
+-  tail call void @llvm.masked.store.nxv8i16.p0(<vscale x 8 x i16> %5, ptr %6, i32 2, <vscale x 8 x i1> %active.lane.mask)
+-  %index.next = add i64 %index, %1
+-  %active.lane.mask.next = tail call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i64(i64 %index.next, i64 %wide.trip.count)
+-  %7 = extractelement <vscale x 8 x i1> %active.lane.mask.next, i64 0
+-  br i1 %7, label %vector.body, label %for.cond.cleanup
+-
+-for.cond.cleanup:
+-  ret void
+-}
+-
+-define void @whilewr_loop_32(ptr noalias %a, ptr %b, ptr %c, i32 %n) {
+-; CHECK-LABEL: whilewr_loop_32:
+-; CHECK:       // %bb.0: // %entry
+-; CHECK-NEXT:    cmp w3, #1
+-; CHECK-NEXT:    b.lt .LBB8_3
+-; CHECK-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NEXT:    mov w8, w3
+-; CHECK-NEXT:    whilewr p1.s, x1, x2
+-; CHECK-NEXT:    mov x9, xzr
+-; CHECK-NEXT:    whilelo p0.s, xzr, x8
+-; CHECK-NEXT:    and p0.b, p1/z, p1.b, p0.b
+-; CHECK-NEXT:  .LBB8_2: // %vector.body
+-; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NEXT:    ld1w { z0.s }, p0/z, [x0, x9, lsl #2]
+-; CHECK-NEXT:    ld1w { z1.s }, p0/z, [x1, x9, lsl #2]
+-; CHECK-NEXT:    add z0.s, z1.s, z0.s
+-; CHECK-NEXT:    st1w { z0.s }, p0, [x2, x9, lsl #2]
+-; CHECK-NEXT:    incw x9
+-; CHECK-NEXT:    whilelo p0.s, x9, x8
+-; CHECK-NEXT:    b.mi .LBB8_2
+-; CHECK-NEXT:  .LBB8_3: // %for.cond.cleanup
+-; CHECK-NEXT:    ret
+-;
+-; CHECK-NOSVE2-LABEL: whilewr_loop_32:
+-; CHECK-NOSVE2:       // %bb.0: // %entry
+-; CHECK-NOSVE2-NEXT:    cmp w3, #1
+-; CHECK-NOSVE2-NEXT:    b.lt .LBB8_3
+-; CHECK-NOSVE2-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NOSVE2-NEXT:    mov w9, w3
+-; CHECK-NOSVE2-NEXT:    sub x10, x1, x2
+-; CHECK-NOSVE2-NEXT:    mov x8, xzr
+-; CHECK-NOSVE2-NEXT:    whilelo p0.s, xzr, x9
+-; CHECK-NOSVE2-NEXT:    add x11, x10, #3
+-; CHECK-NOSVE2-NEXT:    cmp x10, #0
+-; CHECK-NOSVE2-NEXT:    csel x11, x11, x10, lt
+-; CHECK-NOSVE2-NEXT:    cmn x10, #3
+-; CHECK-NOSVE2-NEXT:    cset w10, lt
+-; CHECK-NOSVE2-NEXT:    asr x11, x11, #2
+-; CHECK-NOSVE2-NEXT:    sbfx x10, x10, #0, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p2.s, xzr, x11
+-; CHECK-NOSVE2-NEXT:    whilelo p1.s, xzr, x10
+-; CHECK-NOSVE2-NEXT:    cntw x10
+-; CHECK-NOSVE2-NEXT:    mov p1.b, p2/m, p2.b
+-; CHECK-NOSVE2-NEXT:    and p0.b, p1/z, p1.b, p0.b
+-; CHECK-NOSVE2-NEXT:  .LBB8_2: // %vector.body
+-; CHECK-NOSVE2-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NOSVE2-NEXT:    ld1w { z0.s }, p0/z, [x0, x8, lsl #2]
+-; CHECK-NOSVE2-NEXT:    ld1w { z1.s }, p0/z, [x1, x8, lsl #2]
+-; CHECK-NOSVE2-NEXT:    add z0.s, z1.s, z0.s
+-; CHECK-NOSVE2-NEXT:    st1w { z0.s }, p0, [x2, x8, lsl #2]
+-; CHECK-NOSVE2-NEXT:    add x8, x8, x10
+-; CHECK-NOSVE2-NEXT:    whilelo p0.s, x8, x9
+-; CHECK-NOSVE2-NEXT:    b.mi .LBB8_2
+-; CHECK-NOSVE2-NEXT:  .LBB8_3: // %for.cond.cleanup
+-; CHECK-NOSVE2-NEXT:    ret
+-entry:
+-  %cmp9 = icmp sgt i32 %n, 0
+-  br i1 %cmp9, label %for.body.preheader, label %for.cond.cleanup
+-
+-for.body.preheader:
+-  %b12 = ptrtoint ptr %b to i64
+-  %c13 = ptrtoint ptr %c to i64
+-  %wide.trip.count = zext nneg i32 %n to i64
+-  %0 = tail call i64 @llvm.vscale.i64()
+-  %1 = shl nuw nsw i64 %0, 2
+-  %active.lane.mask.entry = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 0, i64 %wide.trip.count)
+-  %sub.diff = sub i64 %b12, %c13
+-  %diff = sdiv i64 %sub.diff, 4
+-  %neg.compare = icmp slt i64 %sub.diff, -3
+-  %.splatinsert = insertelement <vscale x 4 x i1> poison, i1 %neg.compare, i64 0
+-  %.splat = shufflevector <vscale x 4 x i1> %.splatinsert, <vscale x 4 x i1> poison, <vscale x 4 x i32> zeroinitializer
+-  %ptr.diff.lane.mask = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 0, i64 %diff)
+-  %active.lane.mask.alias = or <vscale x 4 x i1> %ptr.diff.lane.mask, %.splat
+-  %2 = and <vscale x 4 x i1> %active.lane.mask.alias, %active.lane.mask.entry
+-  br label %vector.body
+-
+-vector.body:
+-  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
+-  %active.lane.mask = phi <vscale x 4 x i1> [ %2, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
+-  %3 = getelementptr inbounds i32, ptr %a, i64 %index
+-  %wide.masked.load = tail call <vscale x 4 x i32> @llvm.masked.load.nxv4i32.p0(ptr %3, i32 4, <vscale x 4 x i1> %active.lane.mask, <vscale x 4 x i32> poison)
+-  %4 = getelementptr inbounds i32, ptr %b, i64 %index
+-  %wide.masked.load14 = tail call <vscale x 4 x i32> @llvm.masked.load.nxv4i32.p0(ptr %4, i32 4, <vscale x 4 x i1> %active.lane.mask, <vscale x 4 x i32> poison)
+-  %5 = add <vscale x 4 x i32> %wide.masked.load14, %wide.masked.load
+-  %6 = getelementptr inbounds i32, ptr %c, i64 %index
+-  tail call void @llvm.masked.store.nxv4i32.p0(<vscale x 4 x i32> %5, ptr %6, i32 4, <vscale x 4 x i1> %active.lane.mask)
+-  %index.next = add i64 %index, %1
+-  %active.lane.mask.next = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 %index.next, i64 %wide.trip.count)
+-  %7 = extractelement <vscale x 4 x i1> %active.lane.mask.next, i64 0
+-  br i1 %7, label %vector.body, label %for.cond.cleanup
+-
+-for.cond.cleanup:
+-  ret void
+-}
+-
+-define void @whilewr_loop_64(ptr noalias %a, ptr %b, ptr %c, i32 %n) {
+-; CHECK-LABEL: whilewr_loop_64:
+-; CHECK:       // %bb.0: // %entry
+-; CHECK-NEXT:    cmp w3, #1
+-; CHECK-NEXT:    b.lt .LBB9_3
+-; CHECK-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NEXT:    mov w8, w3
+-; CHECK-NEXT:    whilewr p1.d, x1, x2
+-; CHECK-NEXT:    mov x9, xzr
+-; CHECK-NEXT:    whilelo p0.d, xzr, x8
+-; CHECK-NEXT:    and p0.b, p1/z, p1.b, p0.b
+-; CHECK-NEXT:  .LBB9_2: // %vector.body
+-; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NEXT:    ld1d { z0.d }, p0/z, [x0, x9, lsl #3]
+-; CHECK-NEXT:    ld1d { z1.d }, p0/z, [x1, x9, lsl #3]
+-; CHECK-NEXT:    add z0.d, z1.d, z0.d
+-; CHECK-NEXT:    st1d { z0.d }, p0, [x2, x9, lsl #3]
+-; CHECK-NEXT:    incd x9
+-; CHECK-NEXT:    whilelo p0.d, x9, x8
+-; CHECK-NEXT:    b.mi .LBB9_2
+-; CHECK-NEXT:  .LBB9_3: // %for.cond.cleanup
+-; CHECK-NEXT:    ret
+-;
+-; CHECK-NOSVE2-LABEL: whilewr_loop_64:
+-; CHECK-NOSVE2:       // %bb.0: // %entry
+-; CHECK-NOSVE2-NEXT:    cmp w3, #1
+-; CHECK-NOSVE2-NEXT:    b.lt .LBB9_3
+-; CHECK-NOSVE2-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NOSVE2-NEXT:    mov w9, w3
+-; CHECK-NOSVE2-NEXT:    sub x10, x1, x2
+-; CHECK-NOSVE2-NEXT:    mov x8, xzr
+-; CHECK-NOSVE2-NEXT:    whilelo p0.d, xzr, x9
+-; CHECK-NOSVE2-NEXT:    add x11, x10, #7
+-; CHECK-NOSVE2-NEXT:    cmp x10, #0
+-; CHECK-NOSVE2-NEXT:    csel x11, x11, x10, lt
+-; CHECK-NOSVE2-NEXT:    cmn x10, #7
+-; CHECK-NOSVE2-NEXT:    cset w10, lt
+-; CHECK-NOSVE2-NEXT:    asr x11, x11, #3
+-; CHECK-NOSVE2-NEXT:    sbfx x10, x10, #0, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p2.d, xzr, x11
+-; CHECK-NOSVE2-NEXT:    whilelo p1.d, xzr, x10
+-; CHECK-NOSVE2-NEXT:    cntd x10
+-; CHECK-NOSVE2-NEXT:    mov p1.b, p2/m, p2.b
+-; CHECK-NOSVE2-NEXT:    and p0.b, p1/z, p1.b, p0.b
+-; CHECK-NOSVE2-NEXT:  .LBB9_2: // %vector.body
+-; CHECK-NOSVE2-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NOSVE2-NEXT:    ld1d { z0.d }, p0/z, [x0, x8, lsl #3]
+-; CHECK-NOSVE2-NEXT:    ld1d { z1.d }, p0/z, [x1, x8, lsl #3]
+-; CHECK-NOSVE2-NEXT:    add z0.d, z1.d, z0.d
+-; CHECK-NOSVE2-NEXT:    st1d { z0.d }, p0, [x2, x8, lsl #3]
+-; CHECK-NOSVE2-NEXT:    add x8, x8, x10
+-; CHECK-NOSVE2-NEXT:    whilelo p0.d, x8, x9
+-; CHECK-NOSVE2-NEXT:    b.mi .LBB9_2
+-; CHECK-NOSVE2-NEXT:  .LBB9_3: // %for.cond.cleanup
+-; CHECK-NOSVE2-NEXT:    ret
+-entry:
+-  %cmp9 = icmp sgt i32 %n, 0
+-  br i1 %cmp9, label %for.body.preheader, label %for.cond.cleanup
+-
+-for.body.preheader:
+-  %b12 = ptrtoint ptr %b to i64
+-  %c13 = ptrtoint ptr %c to i64
+-  %wide.trip.count = zext nneg i32 %n to i64
+-  %0 = tail call i64 @llvm.vscale.i64()
+-  %1 = shl nuw nsw i64 %0, 1
+-  %active.lane.mask.entry = tail call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 0, i64 %wide.trip.count)
+-  %sub.diff = sub i64 %b12, %c13
+-  %diff = sdiv i64 %sub.diff, 8
+-  %neg.compare = icmp slt i64 %sub.diff, -7
+-  %.splatinsert = insertelement <vscale x 2 x i1> poison, i1 %neg.compare, i64 0
+-  %.splat = shufflevector <vscale x 2 x i1> %.splatinsert, <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer
+-  %ptr.diff.lane.mask = tail call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 0, i64 %diff)
+-  %active.lane.mask.alias = or <vscale x 2 x i1> %ptr.diff.lane.mask, %.splat
+-  %2 = and <vscale x 2 x i1> %active.lane.mask.alias, %active.lane.mask.entry
+-  br label %vector.body
+-
+-vector.body:
+-  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
+-  %active.lane.mask = phi <vscale x 2 x i1> [ %2, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
+-  %3 = getelementptr inbounds i64, ptr %a, i64 %index
+-  %wide.masked.load = tail call <vscale x 2 x i64> @llvm.masked.load.nxv2i64.p0(ptr %3, i32 8, <vscale x 2 x i1> %active.lane.mask, <vscale x 2 x i64> poison)
+-  %4 = getelementptr inbounds i64, ptr %b, i64 %index
+-  %wide.masked.load14 = tail call <vscale x 2 x i64> @llvm.masked.load.nxv2i64.p0(ptr %4, i32 8, <vscale x 2 x i1> %active.lane.mask, <vscale x 2 x i64> poison)
+-  %5 = add <vscale x 2 x i64> %wide.masked.load14, %wide.masked.load
+-  %6 = getelementptr inbounds i64, ptr %c, i64 %index
+-  tail call void @llvm.masked.store.nxv2i64.p0(<vscale x 2 x i64> %5, ptr %6, i32 8, <vscale x 2 x i1> %active.lane.mask)
+-  %index.next = add i64 %index, %1
+-  %active.lane.mask.next = tail call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 %index.next, i64 %wide.trip.count)
+-  %7 = extractelement <vscale x 2 x i1> %active.lane.mask.next, i64 0
+-  br i1 %7, label %vector.body, label %for.cond.cleanup
+-
+-for.cond.cleanup:
+-  ret void
+-}
+-
+-define void @whilewr_loop_multiple_8(ptr %a, ptr %b, ptr %c, i32 %n) {
+-; CHECK-LABEL: whilewr_loop_multiple_8:
+-; CHECK:       // %bb.0: // %entry
+-; CHECK-NEXT:    cmp w3, #1
+-; CHECK-NEXT:    b.lt .LBB10_3
+-; CHECK-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NEXT:    whilewr p0.b, x0, x2
+-; CHECK-NEXT:    mov w9, w3
+-; CHECK-NEXT:    mov x8, xzr
+-; CHECK-NEXT:    whilewr p1.b, x1, x2
+-; CHECK-NEXT:    and p0.b, p0/z, p0.b, p1.b
+-; CHECK-NEXT:    whilelo p1.b, xzr, x9
+-; CHECK-NEXT:    cntp x10, p0, p0.b
+-; CHECK-NEXT:    and x10, x10, #0xff
+-; CHECK-NEXT:  .LBB10_2: // %vector.body
+-; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NEXT:    and p1.b, p1/z, p1.b, p0.b
+-; CHECK-NEXT:    ld1b { z0.b }, p1/z, [x0, x8]
+-; CHECK-NEXT:    ld1b { z1.b }, p1/z, [x1, x8]
+-; CHECK-NEXT:    add z0.b, z1.b, z0.b
+-; CHECK-NEXT:    st1b { z0.b }, p1, [x2, x8]
+-; CHECK-NEXT:    add x8, x8, x10
+-; CHECK-NEXT:    whilelo p1.b, x8, x9
+-; CHECK-NEXT:    b.mi .LBB10_2
+-; CHECK-NEXT:  .LBB10_3: // %for.cond.cleanup
+-; CHECK-NEXT:    ret
+-;
+-; CHECK-NOSVE2-LABEL: whilewr_loop_multiple_8:
+-; CHECK-NOSVE2:       // %bb.0: // %entry
+-; CHECK-NOSVE2-NEXT:    cmp w3, #1
+-; CHECK-NOSVE2-NEXT:    b.lt .LBB10_3
+-; CHECK-NOSVE2-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NOSVE2-NEXT:    sub x9, x0, x2
+-; CHECK-NOSVE2-NEXT:    mov x8, xzr
+-; CHECK-NOSVE2-NEXT:    cmp x9, #0
+-; CHECK-NOSVE2-NEXT:    cset w10, lt
+-; CHECK-NOSVE2-NEXT:    whilelo p0.b, xzr, x9
+-; CHECK-NOSVE2-NEXT:    sub x9, x1, x2
+-; CHECK-NOSVE2-NEXT:    sbfx x10, x10, #0, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p1.b, xzr, x10
+-; CHECK-NOSVE2-NEXT:    cmp x9, #0
+-; CHECK-NOSVE2-NEXT:    cset w10, lt
+-; CHECK-NOSVE2-NEXT:    whilelo p3.b, xzr, x9
+-; CHECK-NOSVE2-NEXT:    mov w9, w3
+-; CHECK-NOSVE2-NEXT:    sbfx x10, x10, #0, #1
+-; CHECK-NOSVE2-NEXT:    sel p0.b, p0, p0.b, p1.b
+-; CHECK-NOSVE2-NEXT:    whilelo p2.b, xzr, x10
+-; CHECK-NOSVE2-NEXT:    sel p1.b, p3, p3.b, p2.b
+-; CHECK-NOSVE2-NEXT:    and p0.b, p0/z, p0.b, p1.b
+-; CHECK-NOSVE2-NEXT:    whilelo p1.b, xzr, x9
+-; CHECK-NOSVE2-NEXT:    cntp x10, p0, p0.b
+-; CHECK-NOSVE2-NEXT:    and x10, x10, #0xff
+-; CHECK-NOSVE2-NEXT:  .LBB10_2: // %vector.body
+-; CHECK-NOSVE2-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NOSVE2-NEXT:    and p1.b, p1/z, p1.b, p0.b
+-; CHECK-NOSVE2-NEXT:    ld1b { z0.b }, p1/z, [x0, x8]
+-; CHECK-NOSVE2-NEXT:    ld1b { z1.b }, p1/z, [x1, x8]
+-; CHECK-NOSVE2-NEXT:    add z0.b, z1.b, z0.b
+-; CHECK-NOSVE2-NEXT:    st1b { z0.b }, p1, [x2, x8]
+-; CHECK-NOSVE2-NEXT:    add x8, x8, x10
+-; CHECK-NOSVE2-NEXT:    whilelo p1.b, x8, x9
+-; CHECK-NOSVE2-NEXT:    b.mi .LBB10_2
+-; CHECK-NOSVE2-NEXT:  .LBB10_3: // %for.cond.cleanup
+-; CHECK-NOSVE2-NEXT:    ret
+-entry:
+-  %cmp11 = icmp sgt i32 %n, 0
+-  br i1 %cmp11, label %for.body.preheader, label %for.cond.cleanup
+-
+-for.body.preheader:
+-  %c14 = ptrtoint ptr %c to i64
+-  %a15 = ptrtoint ptr %a to i64
+-  %b16 = ptrtoint ptr %b to i64
+-  %wide.trip.count = zext nneg i32 %n to i64
+-  %sub.diff = sub i64 %a15, %c14
+-  %neg.compare = icmp slt i64 %sub.diff, 0
+-  %.splatinsert = insertelement <vscale x 16 x i1> poison, i1 %neg.compare, i64 0
+-  %.splat = shufflevector <vscale x 16 x i1> %.splatinsert, <vscale x 16 x i1> poison, <vscale x 16 x i32> zeroinitializer
+-  %ptr.diff.lane.mask = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %sub.diff)
+-  %active.lane.mask.alias = or <vscale x 16 x i1> %ptr.diff.lane.mask, %.splat
+-  %sub.diff18 = sub i64 %b16, %c14
+-  %neg.compare20 = icmp slt i64 %sub.diff18, 0
+-  %.splatinsert21 = insertelement <vscale x 16 x i1> poison, i1 %neg.compare20, i64 0
+-  %.splat22 = shufflevector <vscale x 16 x i1> %.splatinsert21, <vscale x 16 x i1> poison, <vscale x 16 x i32> zeroinitializer
+-  %ptr.diff.lane.mask23 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %sub.diff18)
+-  %active.lane.mask.alias24 = or <vscale x 16 x i1> %ptr.diff.lane.mask23, %.splat22
+-  %0 = and <vscale x 16 x i1> %active.lane.mask.alias, %active.lane.mask.alias24
+-  %active.lane.mask.entry = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %wide.trip.count)
+-  %1 = zext <vscale x 16 x i1> %0 to <vscale x 16 x i8>
+-  %2 = tail call i8 @llvm.vector.reduce.add.nxv16i8(<vscale x 16 x i8> %1)
+-  %3 = zext i8 %2 to i64
+-  br label %vector.body
+-
+-vector.body:
+-  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
+-  %active.lane.mask = phi <vscale x 16 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
+-  %4 = and <vscale x 16 x i1> %active.lane.mask, %0
+-  %5 = getelementptr inbounds i8, ptr %a, i64 %index
+-  %wide.masked.load = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr %5, i32 1, <vscale x 16 x i1> %4, <vscale x 16 x i8> poison)
+-  %6 = getelementptr inbounds i8, ptr %b, i64 %index
+-  %wide.masked.load25 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr %6, i32 1, <vscale x 16 x i1> %4, <vscale x 16 x i8> poison)
+-  %7 = add <vscale x 16 x i8> %wide.masked.load25, %wide.masked.load
+-  %8 = getelementptr inbounds i8, ptr %c, i64 %index
+-  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %7, ptr %8, i32 1, <vscale x 16 x i1> %4)
+-  %index.next = add i64 %index, %3
+-  %active.lane.mask.next = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %index.next, i64 %wide.trip.count)
+-  %9 = extractelement <vscale x 16 x i1> %active.lane.mask.next, i64 0
+-  br i1 %9, label %vector.body, label %for.cond.cleanup
+-
+-for.cond.cleanup:
+-  ret void
+-}
+-
+-define void @whilewr_loop_multiple_16(ptr %a, ptr %b, ptr %c, i32 %n) {
+-; CHECK-LABEL: whilewr_loop_multiple_16:
+-; CHECK:       // %bb.0: // %entry
+-; CHECK-NEXT:    cmp w3, #1
+-; CHECK-NEXT:    b.lt .LBB11_3
+-; CHECK-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NEXT:    whilewr p0.h, x0, x2
+-; CHECK-NEXT:    mov w9, w3
+-; CHECK-NEXT:    mov x8, xzr
+-; CHECK-NEXT:    whilewr p1.h, x1, x2
+-; CHECK-NEXT:    and p0.b, p0/z, p0.b, p1.b
+-; CHECK-NEXT:    whilelo p1.h, xzr, x9
+-; CHECK-NEXT:    cntp x10, p0, p0.h
+-; CHECK-NEXT:    and x10, x10, #0xff
+-; CHECK-NEXT:  .LBB11_2: // %vector.body
+-; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NEXT:    and p1.b, p1/z, p1.b, p0.b
+-; CHECK-NEXT:    ld1h { z0.h }, p1/z, [x0, x8, lsl #1]
+-; CHECK-NEXT:    ld1h { z1.h }, p1/z, [x1, x8, lsl #1]
+-; CHECK-NEXT:    add z0.h, z1.h, z0.h
+-; CHECK-NEXT:    st1h { z0.h }, p1, [x2, x8, lsl #1]
+-; CHECK-NEXT:    add x8, x8, x10
+-; CHECK-NEXT:    whilelo p1.h, x8, x9
+-; CHECK-NEXT:    b.mi .LBB11_2
+-; CHECK-NEXT:  .LBB11_3: // %for.cond.cleanup
+-; CHECK-NEXT:    ret
+-;
+-; CHECK-NOSVE2-LABEL: whilewr_loop_multiple_16:
+-; CHECK-NOSVE2:       // %bb.0: // %entry
+-; CHECK-NOSVE2-NEXT:    cmp w3, #1
+-; CHECK-NOSVE2-NEXT:    b.lt .LBB11_3
+-; CHECK-NOSVE2-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NOSVE2-NEXT:    sub x9, x0, x2
+-; CHECK-NOSVE2-NEXT:    mov x8, xzr
+-; CHECK-NOSVE2-NEXT:    cmn x9, #1
+-; CHECK-NOSVE2-NEXT:    add x9, x9, x9, lsr #63
+-; CHECK-NOSVE2-NEXT:    cset w10, lt
+-; CHECK-NOSVE2-NEXT:    sbfx x10, x10, #0, #1
+-; CHECK-NOSVE2-NEXT:    asr x9, x9, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p0.h, xzr, x10
+-; CHECK-NOSVE2-NEXT:    sub x10, x1, x2
+-; CHECK-NOSVE2-NEXT:    whilelo p1.h, xzr, x9
+-; CHECK-NOSVE2-NEXT:    add x9, x10, x10, lsr #63
+-; CHECK-NOSVE2-NEXT:    cmn x10, #1
+-; CHECK-NOSVE2-NEXT:    cset w10, lt
+-; CHECK-NOSVE2-NEXT:    asr x9, x9, #1
+-; CHECK-NOSVE2-NEXT:    mov p0.b, p1/m, p1.b
+-; CHECK-NOSVE2-NEXT:    sbfx x10, x10, #0, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p3.h, xzr, x9
+-; CHECK-NOSVE2-NEXT:    mov w9, w3
+-; CHECK-NOSVE2-NEXT:    whilelo p2.h, xzr, x10
+-; CHECK-NOSVE2-NEXT:    sel p1.b, p3, p3.b, p2.b
+-; CHECK-NOSVE2-NEXT:    and p0.b, p0/z, p0.b, p1.b
+-; CHECK-NOSVE2-NEXT:    whilelo p1.h, xzr, x9
+-; CHECK-NOSVE2-NEXT:    cntp x10, p0, p0.h
+-; CHECK-NOSVE2-NEXT:    and x10, x10, #0xff
+-; CHECK-NOSVE2-NEXT:  .LBB11_2: // %vector.body
+-; CHECK-NOSVE2-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NOSVE2-NEXT:    and p1.b, p1/z, p1.b, p0.b
+-; CHECK-NOSVE2-NEXT:    ld1h { z0.h }, p1/z, [x0, x8, lsl #1]
+-; CHECK-NOSVE2-NEXT:    ld1h { z1.h }, p1/z, [x1, x8, lsl #1]
+-; CHECK-NOSVE2-NEXT:    add z0.h, z1.h, z0.h
+-; CHECK-NOSVE2-NEXT:    st1h { z0.h }, p1, [x2, x8, lsl #1]
+-; CHECK-NOSVE2-NEXT:    add x8, x8, x10
+-; CHECK-NOSVE2-NEXT:    whilelo p1.h, x8, x9
+-; CHECK-NOSVE2-NEXT:    b.mi .LBB11_2
+-; CHECK-NOSVE2-NEXT:  .LBB11_3: // %for.cond.cleanup
+-; CHECK-NOSVE2-NEXT:    ret
+-entry:
+-  %cmp11 = icmp sgt i32 %n, 0
+-  br i1 %cmp11, label %for.body.preheader, label %for.cond.cleanup
+-
+-for.body.preheader:
+-  %c14 = ptrtoint ptr %c to i64
+-  %a15 = ptrtoint ptr %a to i64
+-  %b16 = ptrtoint ptr %b to i64
+-  %wide.trip.count = zext nneg i32 %n to i64
+-  %sub.diff = sub i64 %a15, %c14
+-  %diff = sdiv i64 %sub.diff, 2
+-  %neg.compare = icmp slt i64 %sub.diff, -1
+-  %.splatinsert = insertelement <vscale x 8 x i1> poison, i1 %neg.compare, i64 0
+-  %.splat = shufflevector <vscale x 8 x i1> %.splatinsert, <vscale x 8 x i1> poison, <vscale x 8 x i32> zeroinitializer
+-  %ptr.diff.lane.mask = tail call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i64(i64 0, i64 %diff)
+-  %active.lane.mask.alias = or <vscale x 8 x i1> %ptr.diff.lane.mask, %.splat
+-  %sub.diff18 = sub i64 %b16, %c14
+-  %diff19 = sdiv i64 %sub.diff18, 2
+-  %neg.compare20 = icmp slt i64 %sub.diff18, -1
+-  %.splatinsert21 = insertelement <vscale x 8 x i1> poison, i1 %neg.compare20, i64 0
+-  %.splat22 = shufflevector <vscale x 8 x i1> %.splatinsert21, <vscale x 8 x i1> poison, <vscale x 8 x i32> zeroinitializer
+-  %ptr.diff.lane.mask23 = tail call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i64(i64 0, i64 %diff19)
+-  %active.lane.mask.alias24 = or <vscale x 8 x i1> %ptr.diff.lane.mask23, %.splat22
+-  %0 = and <vscale x 8 x i1> %active.lane.mask.alias, %active.lane.mask.alias24
+-  %active.lane.mask.entry = tail call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i64(i64 0, i64 %wide.trip.count)
+-  %1 = zext <vscale x 8 x i1> %0 to <vscale x 8 x i8>
+-  %2 = tail call i8 @llvm.vector.reduce.add.nxv8i8(<vscale x 8 x i8> %1)
+-  %3 = zext i8 %2 to i64
+-  br label %vector.body
+-
+-vector.body:
+-  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
+-  %active.lane.mask = phi <vscale x 8 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
+-  %4 = and <vscale x 8 x i1> %active.lane.mask, %0
+-  %5 = getelementptr inbounds i16, ptr %a, i64 %index
+-  %wide.masked.load = tail call <vscale x 8 x i16> @llvm.masked.load.nxv8i16.p0(ptr %5, i32 2, <vscale x 8 x i1> %4, <vscale x 8 x i16> poison)
+-  %6 = getelementptr inbounds i16, ptr %b, i64 %index
+-  %wide.masked.load25 = tail call <vscale x 8 x i16> @llvm.masked.load.nxv8i16.p0(ptr %6, i32 2, <vscale x 8 x i1> %4, <vscale x 8 x i16> poison)
+-  %7 = add <vscale x 8 x i16> %wide.masked.load25, %wide.masked.load
+-  %8 = getelementptr inbounds i16, ptr %c, i64 %index
+-  tail call void @llvm.masked.store.nxv8i16.p0(<vscale x 8 x i16> %7, ptr %8, i32 2, <vscale x 8 x i1> %4)
+-  %index.next = add i64 %index, %3
+-  %active.lane.mask.next = tail call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i64(i64 %index.next, i64 %wide.trip.count)
+-  %9 = extractelement <vscale x 8 x i1> %active.lane.mask.next, i64 0
+-  br i1 %9, label %vector.body, label %for.cond.cleanup
+-
+-for.cond.cleanup:
+-  ret void
+-}
+-
+-define void @whilewr_loop_multiple_32(ptr %a, ptr %b, ptr %c, i32 %n) {
+-; CHECK-LABEL: whilewr_loop_multiple_32:
+-; CHECK:       // %bb.0: // %entry
+-; CHECK-NEXT:    cmp w3, #1
+-; CHECK-NEXT:    b.lt .LBB12_3
+-; CHECK-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NEXT:    whilewr p0.s, x0, x2
+-; CHECK-NEXT:    mov w9, w3
+-; CHECK-NEXT:    mov x8, xzr
+-; CHECK-NEXT:    whilewr p1.s, x1, x2
+-; CHECK-NEXT:    and p0.b, p0/z, p0.b, p1.b
+-; CHECK-NEXT:    whilelo p1.s, xzr, x9
+-; CHECK-NEXT:    cntp x10, p0, p0.s
+-; CHECK-NEXT:    and x10, x10, #0xff
+-; CHECK-NEXT:  .LBB12_2: // %vector.body
+-; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NEXT:    and p1.b, p1/z, p1.b, p0.b
+-; CHECK-NEXT:    ld1w { z0.s }, p1/z, [x0, x8, lsl #2]
+-; CHECK-NEXT:    ld1w { z1.s }, p1/z, [x1, x8, lsl #2]
+-; CHECK-NEXT:    add z0.s, z1.s, z0.s
+-; CHECK-NEXT:    st1w { z0.s }, p1, [x2, x8, lsl #2]
+-; CHECK-NEXT:    add x8, x8, x10
+-; CHECK-NEXT:    whilelo p1.s, x8, x9
+-; CHECK-NEXT:    b.mi .LBB12_2
+-; CHECK-NEXT:  .LBB12_3: // %for.cond.cleanup
+-; CHECK-NEXT:    ret
+-;
+-; CHECK-NOSVE2-LABEL: whilewr_loop_multiple_32:
+-; CHECK-NOSVE2:       // %bb.0: // %entry
+-; CHECK-NOSVE2-NEXT:    cmp w3, #1
+-; CHECK-NOSVE2-NEXT:    b.lt .LBB12_3
+-; CHECK-NOSVE2-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NOSVE2-NEXT:    sub x9, x0, x2
+-; CHECK-NOSVE2-NEXT:    mov x8, xzr
+-; CHECK-NOSVE2-NEXT:    add x10, x9, #3
+-; CHECK-NOSVE2-NEXT:    cmp x9, #0
+-; CHECK-NOSVE2-NEXT:    csel x10, x10, x9, lt
+-; CHECK-NOSVE2-NEXT:    cmn x9, #3
+-; CHECK-NOSVE2-NEXT:    asr x9, x10, #2
+-; CHECK-NOSVE2-NEXT:    cset w10, lt
+-; CHECK-NOSVE2-NEXT:    sbfx x10, x10, #0, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p0.s, xzr, x9
+-; CHECK-NOSVE2-NEXT:    sub x9, x1, x2
+-; CHECK-NOSVE2-NEXT:    whilelo p1.s, xzr, x10
+-; CHECK-NOSVE2-NEXT:    add x10, x9, #3
+-; CHECK-NOSVE2-NEXT:    cmp x9, #0
+-; CHECK-NOSVE2-NEXT:    csel x10, x10, x9, lt
+-; CHECK-NOSVE2-NEXT:    cmn x9, #3
+-; CHECK-NOSVE2-NEXT:    sel p0.b, p0, p0.b, p1.b
+-; CHECK-NOSVE2-NEXT:    cset w9, lt
+-; CHECK-NOSVE2-NEXT:    asr x10, x10, #2
+-; CHECK-NOSVE2-NEXT:    sbfx x9, x9, #0, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p3.s, xzr, x10
+-; CHECK-NOSVE2-NEXT:    whilelo p2.s, xzr, x9
+-; CHECK-NOSVE2-NEXT:    mov w9, w3
+-; CHECK-NOSVE2-NEXT:    sel p1.b, p3, p3.b, p2.b
+-; CHECK-NOSVE2-NEXT:    and p0.b, p0/z, p0.b, p1.b
+-; CHECK-NOSVE2-NEXT:    whilelo p1.s, xzr, x9
+-; CHECK-NOSVE2-NEXT:    cntp x10, p0, p0.s
+-; CHECK-NOSVE2-NEXT:    and x10, x10, #0xff
+-; CHECK-NOSVE2-NEXT:  .LBB12_2: // %vector.body
+-; CHECK-NOSVE2-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NOSVE2-NEXT:    and p1.b, p1/z, p1.b, p0.b
+-; CHECK-NOSVE2-NEXT:    ld1w { z0.s }, p1/z, [x0, x8, lsl #2]
+-; CHECK-NOSVE2-NEXT:    ld1w { z1.s }, p1/z, [x1, x8, lsl #2]
+-; CHECK-NOSVE2-NEXT:    add z0.s, z1.s, z0.s
+-; CHECK-NOSVE2-NEXT:    st1w { z0.s }, p1, [x2, x8, lsl #2]
+-; CHECK-NOSVE2-NEXT:    add x8, x8, x10
+-; CHECK-NOSVE2-NEXT:    whilelo p1.s, x8, x9
+-; CHECK-NOSVE2-NEXT:    b.mi .LBB12_2
+-; CHECK-NOSVE2-NEXT:  .LBB12_3: // %for.cond.cleanup
+-; CHECK-NOSVE2-NEXT:    ret
+-entry:
+-  %cmp9 = icmp sgt i32 %n, 0
+-  br i1 %cmp9, label %for.body.preheader, label %for.cond.cleanup
+-
+-for.body.preheader:
+-  %c12 = ptrtoint ptr %c to i64
+-  %a13 = ptrtoint ptr %a to i64
+-  %b14 = ptrtoint ptr %b to i64
+-  %wide.trip.count = zext nneg i32 %n to i64
+-  %sub.diff = sub i64 %a13, %c12
+-  %diff = sdiv i64 %sub.diff, 4
+-  %neg.compare = icmp slt i64 %sub.diff, -3
+-  %.splatinsert = insertelement <vscale x 4 x i1> poison, i1 %neg.compare, i64 0
+-  %.splat = shufflevector <vscale x 4 x i1> %.splatinsert, <vscale x 4 x i1> poison, <vscale x 4 x i32> zeroinitializer
+-  %ptr.diff.lane.mask = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 0, i64 %diff)
+-  %active.lane.mask.alias = or <vscale x 4 x i1> %ptr.diff.lane.mask, %.splat
+-  %sub.diff16 = sub i64 %b14, %c12
+-  %diff17 = sdiv i64 %sub.diff16, 4
+-  %neg.compare18 = icmp slt i64 %sub.diff16, -3
+-  %.splatinsert19 = insertelement <vscale x 4 x i1> poison, i1 %neg.compare18, i64 0
+-  %.splat20 = shufflevector <vscale x 4 x i1> %.splatinsert19, <vscale x 4 x i1> poison, <vscale x 4 x i32> zeroinitializer
+-  %ptr.diff.lane.mask21 = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 0, i64 %diff17)
+-  %active.lane.mask.alias22 = or <vscale x 4 x i1> %ptr.diff.lane.mask21, %.splat20
+-  %0 = and <vscale x 4 x i1> %active.lane.mask.alias, %active.lane.mask.alias22
+-  %active.lane.mask.entry = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 0, i64 %wide.trip.count)
+-  %1 = zext <vscale x 4 x i1> %0 to <vscale x 4 x i8>
+-  %2 = tail call i8 @llvm.vector.reduce.add.nxv4i8(<vscale x 4 x i8> %1)
+-  %3 = zext i8 %2 to i64
+-  br label %vector.body
+-
+-vector.body:
+-  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
+-  %active.lane.mask = phi <vscale x 4 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
+-  %4 = and <vscale x 4 x i1> %active.lane.mask, %0
+-  %5 = getelementptr inbounds i32, ptr %a, i64 %index
+-  %wide.masked.load = tail call <vscale x 4 x i32> @llvm.masked.load.nxv4i32.p0(ptr %5, i32 4, <vscale x 4 x i1> %4, <vscale x 4 x i32> poison)
+-  %6 = getelementptr inbounds i32, ptr %b, i64 %index
+-  %wide.masked.load23 = tail call <vscale x 4 x i32> @llvm.masked.load.nxv4i32.p0(ptr %6, i32 4, <vscale x 4 x i1> %4, <vscale x 4 x i32> poison)
+-  %7 = add <vscale x 4 x i32> %wide.masked.load23, %wide.masked.load
+-  %8 = getelementptr inbounds i32, ptr %c, i64 %index
+-  tail call void @llvm.masked.store.nxv4i32.p0(<vscale x 4 x i32> %7, ptr %8, i32 4, <vscale x 4 x i1> %4)
+-  %index.next = add i64 %index, %3
+-  %active.lane.mask.next = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 %index.next, i64 %wide.trip.count)
+-  %9 = extractelement <vscale x 4 x i1> %active.lane.mask.next, i64 0
+-  br i1 %9, label %vector.body, label %for.cond.cleanup
+-
+-for.cond.cleanup:
+-  ret void
+-}
+-
+-define void @whilewr_loop_multiple_64(ptr %a, ptr %b, ptr %c, i32 %n) {
+-; CHECK-LABEL: whilewr_loop_multiple_64:
+-; CHECK:       // %bb.0: // %entry
+-; CHECK-NEXT:    cmp w3, #1
+-; CHECK-NEXT:    b.lt .LBB13_3
+-; CHECK-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NEXT:    whilewr p0.d, x0, x2
+-; CHECK-NEXT:    mov w9, w3
+-; CHECK-NEXT:    mov x8, xzr
+-; CHECK-NEXT:    whilewr p1.d, x1, x2
+-; CHECK-NEXT:    and p0.b, p0/z, p0.b, p1.b
+-; CHECK-NEXT:    whilelo p1.d, xzr, x9
+-; CHECK-NEXT:    cntp x10, p0, p0.d
+-; CHECK-NEXT:    and x10, x10, #0xff
+-; CHECK-NEXT:  .LBB13_2: // %vector.body
+-; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NEXT:    and p1.b, p1/z, p1.b, p0.b
+-; CHECK-NEXT:    ld1d { z0.d }, p1/z, [x0, x8, lsl #3]
+-; CHECK-NEXT:    ld1d { z1.d }, p1/z, [x1, x8, lsl #3]
+-; CHECK-NEXT:    add z0.d, z1.d, z0.d
+-; CHECK-NEXT:    st1d { z0.d }, p1, [x2, x8, lsl #3]
+-; CHECK-NEXT:    add x8, x8, x10
+-; CHECK-NEXT:    whilelo p1.d, x8, x9
+-; CHECK-NEXT:    b.mi .LBB13_2
+-; CHECK-NEXT:  .LBB13_3: // %for.cond.cleanup
+-; CHECK-NEXT:    ret
+-;
+-; CHECK-NOSVE2-LABEL: whilewr_loop_multiple_64:
+-; CHECK-NOSVE2:       // %bb.0: // %entry
+-; CHECK-NOSVE2-NEXT:    cmp w3, #1
+-; CHECK-NOSVE2-NEXT:    b.lt .LBB13_3
+-; CHECK-NOSVE2-NEXT:  // %bb.1: // %for.body.preheader
+-; CHECK-NOSVE2-NEXT:    sub x9, x0, x2
+-; CHECK-NOSVE2-NEXT:    mov x8, xzr
+-; CHECK-NOSVE2-NEXT:    add x10, x9, #7
+-; CHECK-NOSVE2-NEXT:    cmp x9, #0
+-; CHECK-NOSVE2-NEXT:    csel x10, x10, x9, lt
+-; CHECK-NOSVE2-NEXT:    cmn x9, #7
+-; CHECK-NOSVE2-NEXT:    asr x9, x10, #3
+-; CHECK-NOSVE2-NEXT:    cset w10, lt
+-; CHECK-NOSVE2-NEXT:    sbfx x10, x10, #0, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p0.d, xzr, x9
+-; CHECK-NOSVE2-NEXT:    sub x9, x1, x2
+-; CHECK-NOSVE2-NEXT:    whilelo p1.d, xzr, x10
+-; CHECK-NOSVE2-NEXT:    add x10, x9, #7
+-; CHECK-NOSVE2-NEXT:    cmp x9, #0
+-; CHECK-NOSVE2-NEXT:    csel x10, x10, x9, lt
+-; CHECK-NOSVE2-NEXT:    cmn x9, #7
+-; CHECK-NOSVE2-NEXT:    sel p0.b, p0, p0.b, p1.b
+-; CHECK-NOSVE2-NEXT:    cset w9, lt
+-; CHECK-NOSVE2-NEXT:    asr x10, x10, #3
+-; CHECK-NOSVE2-NEXT:    sbfx x9, x9, #0, #1
+-; CHECK-NOSVE2-NEXT:    whilelo p3.d, xzr, x10
+-; CHECK-NOSVE2-NEXT:    whilelo p2.d, xzr, x9
+-; CHECK-NOSVE2-NEXT:    mov w9, w3
+-; CHECK-NOSVE2-NEXT:    sel p1.b, p3, p3.b, p2.b
+-; CHECK-NOSVE2-NEXT:    and p0.b, p0/z, p0.b, p1.b
+-; CHECK-NOSVE2-NEXT:    whilelo p1.d, xzr, x9
+-; CHECK-NOSVE2-NEXT:    cntp x10, p0, p0.d
+-; CHECK-NOSVE2-NEXT:    and x10, x10, #0xff
+-; CHECK-NOSVE2-NEXT:  .LBB13_2: // %vector.body
+-; CHECK-NOSVE2-NEXT:    // =>This Inner Loop Header: Depth=1
+-; CHECK-NOSVE2-NEXT:    and p1.b, p1/z, p1.b, p0.b
+-; CHECK-NOSVE2-NEXT:    ld1d { z0.d }, p1/z, [x0, x8, lsl #3]
+-; CHECK-NOSVE2-NEXT:    ld1d { z1.d }, p1/z, [x1, x8, lsl #3]
+-; CHECK-NOSVE2-NEXT:    add z0.d, z1.d, z0.d
+-; CHECK-NOSVE2-NEXT:    st1d { z0.d }, p1, [x2, x8, lsl #3]
+-; CHECK-NOSVE2-NEXT:    add x8, x8, x10
+-; CHECK-NOSVE2-NEXT:    whilelo p1.d, x8, x9
+-; CHECK-NOSVE2-NEXT:    b.mi .LBB13_2
+-; CHECK-NOSVE2-NEXT:  .LBB13_3: // %for.cond.cleanup
+-; CHECK-NOSVE2-NEXT:    ret
+-entry:
+-  %cmp9 = icmp sgt i32 %n, 0
+-  br i1 %cmp9, label %for.body.preheader, label %for.cond.cleanup
+-
+-for.body.preheader:
+-  %c12 = ptrtoint ptr %c to i64
+-  %a13 = ptrtoint ptr %a to i64
+-  %b14 = ptrtoint ptr %b to i64
+-  %wide.trip.count = zext nneg i32 %n to i64
+-  %sub.diff = sub i64 %a13, %c12
+-  %diff = sdiv i64 %sub.diff, 8
+-  %neg.compare = icmp slt i64 %sub.diff, -7
+-  %.splatinsert = insertelement <vscale x 2 x i1> poison, i1 %neg.compare, i64 0
+-  %.splat = shufflevector <vscale x 2 x i1> %.splatinsert, <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer
+-  %ptr.diff.lane.mask = tail call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 0, i64 %diff)
+-  %active.lane.mask.alias = or <vscale x 2 x i1> %ptr.diff.lane.mask, %.splat
+-  %sub.diff16 = sub i64 %b14, %c12
+-  %diff17 = sdiv i64 %sub.diff16, 8
+-  %neg.compare18 = icmp slt i64 %sub.diff16, -7
+-  %.splatinsert19 = insertelement <vscale x 2 x i1> poison, i1 %neg.compare18, i64 0
+-  %.splat20 = shufflevector <vscale x 2 x i1> %.splatinsert19, <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer
+-  %ptr.diff.lane.mask21 = tail call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 0, i64 %diff17)
+-  %active.lane.mask.alias22 = or <vscale x 2 x i1> %ptr.diff.lane.mask21, %.splat20
+-  %0 = and <vscale x 2 x i1> %active.lane.mask.alias, %active.lane.mask.alias22
+-  %active.lane.mask.entry = tail call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 0, i64 %wide.trip.count)
+-  %1 = zext <vscale x 2 x i1> %0 to <vscale x 2 x i8>
+-  %2 = tail call i8 @llvm.vector.reduce.add.nxv2i8(<vscale x 2 x i8> %1)
+-  %3 = zext i8 %2 to i64
+-  br label %vector.body
+-
+-vector.body:
+-  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
+-  %active.lane.mask = phi <vscale x 2 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
+-  %4 = and <vscale x 2 x i1> %active.lane.mask, %0
+-  %5 = getelementptr inbounds i64, ptr %a, i64 %index
+-  %wide.masked.load = tail call <vscale x 2 x i64> @llvm.masked.load.nxv2i64.p0(ptr %5, i32 8, <vscale x 2 x i1> %4, <vscale x 2 x i64> poison)
+-  %6 = getelementptr inbounds i64, ptr %b, i64 %index
+-  %wide.masked.load23 = tail call <vscale x 2 x i64> @llvm.masked.load.nxv2i64.p0(ptr %6, i32 8, <vscale x 2 x i1> %4, <vscale x 2 x i64> poison)
+-  %7 = add <vscale x 2 x i64> %wide.masked.load23, %wide.masked.load
+-  %8 = getelementptr inbounds i64, ptr %c, i64 %index
+-  tail call void @llvm.masked.store.nxv2i64.p0(<vscale x 2 x i64> %7, ptr %8, i32 8, <vscale x 2 x i1> %4)
+-  %index.next = add i64 %index, %3
+-  %active.lane.mask.next = tail call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 %index.next, i64 %wide.trip.count)
+-  %9 = extractelement <vscale x 2 x i1> %active.lane.mask.next, i64 0
+-  br i1 %9, label %vector.body, label %for.cond.cleanup
+-
+-for.cond.cleanup:
+-  ret void
+-}
+-
+-declare i64 @llvm.vscale.i64()
+-
+-declare <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64, i64)
+-
+-declare <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nocapture, i32 immarg, <vscale x 16 x i1>, <vscale x 16 x i8>)
+-
+-declare void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8>, ptr nocapture, i32 immarg, <vscale x 16 x i1>)
+-
+-declare <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i64(i64, i64)
+-
+-declare <vscale x 8 x i16> @llvm.masked.load.nxv8i16.p0(ptr nocapture, i32 immarg, <vscale x 8 x i1>, <vscale x 8 x i16>)
+-
+-declare void @llvm.masked.store.nxv8i16.p0(<vscale x 8 x i16>, ptr nocapture, i32 immarg, <vscale x 8 x i1>)
+-
+-declare <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64, i64)
+-
+-declare <vscale x 4 x i32> @llvm.masked.load.nxv4i32.p0(ptr nocapture, i32 immarg, <vscale x 4 x i1>, <vscale x 4 x i32>)
+-
+-declare void @llvm.masked.store.nxv4i32.p0(<vscale x 4 x i32>, ptr nocapture, i32 immarg, <vscale x 4 x i1>)
+-
+-declare <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64, i64)
+-
+-declare <vscale x 2 x i64> @llvm.masked.load.nxv2i64.p0(ptr nocapture, i32 immarg, <vscale x 2 x i1>, <vscale x 2 x i64>)
+-
+-declare void @llvm.masked.store.nxv2i64.p0(<vscale x 2 x i64>, ptr nocapture, i32 immarg, <vscale x 2 x i1>)
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/438ad9f2bf25575c474313de4ad85a5da6f69e4c.patch b/llvm_patches/cherry/438ad9f2bf25575c474313de4ad85a5da6f69e4c.patch
new file mode 100644
index 00000000..40d3db35
--- /dev/null
+++ b/llvm_patches/cherry/438ad9f2bf25575c474313de4ad85a5da6f69e4c.patch
@@ -0,0 +1,321 @@
+From bd59cc8103930a251107038992ffa3e738e6a2f6 Mon Sep 17 00:00:00 2001
+From: Owen Pan <owenpiano@gmail.com>
+Date: Wed, 28 Aug 2024 18:23:54 -0700
+Subject: [PATCH] =?UTF-8?q?[clang-format]=20Revert=20"[clang-format][NFC]?=
+ =?UTF-8?q?=20Delete=20TT=5FLambdaArrow=20(#70=E2=80=A6=20(#105923)?=
+MIME-Version: 1.0
+Content-Type: text/plain; charset=UTF-8
+Content-Transfer-Encoding: 8bit
+
+519)"
+
+This reverts commit e00d32afb9d33a1eca48e2b041c9688436706c5b and adds a
+test for lambda arrow SplitPenalty.
+
+Fixes #105480.
+
+patch.cherry: true
+patch.metadata.original_sha: 438ad9f2bf25575c474313de4ad85a5da6f69e4c
+patch.platforms: chromiumos
+patch.version_range.from: 516925
+patch.version_range.until: 547893
+
+---
+ clang/lib/Format/ContinuationIndenter.cpp     | 10 +++---
+ clang/lib/Format/FormatToken.h                |  3 +-
+ clang/lib/Format/TokenAnnotator.cpp           | 33 ++++++++++--------
+ clang/lib/Format/UnwrappedLineParser.cpp      |  2 +-
+ clang/unittests/Format/TokenAnnotatorTest.cpp | 34 ++++++++++++++-----
+ 5 files changed, 50 insertions(+), 32 deletions(-)
+
+diff --git a/clang/lib/Format/ContinuationIndenter.cpp b/clang/lib/Format/ContinuationIndenter.cpp
+index 4fcb776db45b..04d4aec9ef7a 100644
+--- a/clang/lib/Format/ContinuationIndenter.cpp
++++ b/clang/lib/Format/ContinuationIndenter.cpp
+@@ -885,10 +885,8 @@ void ContinuationIndenter::addTokenOnCurrentLine(LineState &State, bool DryRun,
+     CurrentState.ContainsUnwrappedBuilder = true;
+   }
+ 
+-  if (Current.is(TT_TrailingReturnArrow) &&
+-      Style.Language == FormatStyle::LK_Java) {
++  if (Current.is(TT_LambdaArrow) && Style.Language == FormatStyle::LK_Java)
+     CurrentState.NoLineBreak = true;
+-  }
+   if (Current.isMemberAccess() && Previous.is(tok::r_paren) &&
+       (Previous.MatchingParen &&
+        (Previous.TotalLength - Previous.MatchingParen->TotalLength > 10))) {
+@@ -1043,7 +1041,7 @@ unsigned ContinuationIndenter::addTokenOnNewLine(LineState &State,
+   //
+   // is common and should be formatted like a free-standing function. The same
+   // goes for wrapping before the lambda return type arrow.
+-  if (Current.isNot(TT_TrailingReturnArrow) &&
++  if (Current.isNot(TT_LambdaArrow) &&
+       (!Style.isJavaScript() || Current.NestingLevel != 0 ||
+        !PreviousNonComment || PreviousNonComment->isNot(tok::equal) ||
+        !Current.isOneOf(Keywords.kw_async, Keywords.kw_function))) {
+@@ -1303,7 +1301,7 @@ unsigned ContinuationIndenter::getNewLineColumn(const LineState &State) {
+     }
+     return CurrentState.Indent;
+   }
+-  if (Current.is(TT_TrailingReturnArrow) &&
++  if (Current.is(TT_LambdaArrow) &&
+       Previous.isOneOf(tok::kw_noexcept, tok::kw_mutable, tok::kw_constexpr,
+                        tok::kw_consteval, tok::kw_static, TT_AttributeSquare)) {
+     return ContinuationIndent;
+@@ -1637,7 +1635,7 @@ unsigned ContinuationIndenter::moveStateToNextToken(LineState &State,
+   }
+   if (Current.isOneOf(TT_BinaryOperator, TT_ConditionalExpr) && Newline)
+     CurrentState.NestedBlockIndent = State.Column + Current.ColumnWidth + 1;
+-  if (Current.isOneOf(TT_LambdaLSquare, TT_TrailingReturnArrow))
++  if (Current.isOneOf(TT_LambdaLSquare, TT_LambdaArrow))
+     CurrentState.LastSpace = State.Column;
+   if (Current.is(TT_RequiresExpression) &&
+       Style.RequiresExpressionIndentation == FormatStyle::REI_Keyword) {
+diff --git a/clang/lib/Format/FormatToken.h b/clang/lib/Format/FormatToken.h
+index 2d386d99aa84..03c0cbd60961 100644
+--- a/clang/lib/Format/FormatToken.h
++++ b/clang/lib/Format/FormatToken.h
+@@ -102,6 +102,7 @@ namespace format {
+   TYPE(JsTypeColon)                                                            \
+   TYPE(JsTypeOperator)                                                         \
+   TYPE(JsTypeOptionalQuestion)                                                 \
++  TYPE(LambdaArrow)                                                            \
+   TYPE(LambdaDefinitionLParen)                                                 \
+   TYPE(LambdaLBrace)                                                           \
+   TYPE(LambdaLSquare)                                                          \
+@@ -726,7 +727,7 @@ public:
+   bool isMemberAccess() const {
+     return isOneOf(tok::arrow, tok::period, tok::arrowstar) &&
+            !isOneOf(TT_DesignatedInitializerPeriod, TT_TrailingReturnArrow,
+-                    TT_LeadingJavaAnnotation);
++                    TT_LambdaArrow, TT_LeadingJavaAnnotation);
+   }
+ 
+   bool isPointerOrReference() const {
+diff --git a/clang/lib/Format/TokenAnnotator.cpp b/clang/lib/Format/TokenAnnotator.cpp
+index f8bf8d9570d9..72115c3a9216 100644
+--- a/clang/lib/Format/TokenAnnotator.cpp
++++ b/clang/lib/Format/TokenAnnotator.cpp
+@@ -833,7 +833,7 @@ private:
+         }
+         // An arrow after an ObjC method expression is not a lambda arrow.
+         if (CurrentToken->is(TT_ObjCMethodExpr) && CurrentToken->Next &&
+-            CurrentToken->Next->is(TT_TrailingReturnArrow)) {
++            CurrentToken->Next->is(TT_LambdaArrow)) {
+           CurrentToken->Next->overwriteFixedType(TT_Unknown);
+         }
+         Left->MatchingParen = CurrentToken;
+@@ -1770,8 +1770,10 @@ private:
+       }
+       break;
+     case tok::arrow:
+-      if (Tok->Previous && Tok->Previous->is(tok::kw_noexcept))
++      if (Tok->isNot(TT_LambdaArrow) && Tok->Previous &&
++          Tok->Previous->is(tok::kw_noexcept)) {
+         Tok->setType(TT_TrailingReturnArrow);
++      }
+       break;
+     case tok::equal:
+       // In TableGen, there must be a value after "=";
+@@ -2057,11 +2059,11 @@ private:
+             TT_LambdaLSquare, TT_LambdaLBrace, TT_AttributeMacro, TT_IfMacro,
+             TT_ForEachMacro, TT_TypenameMacro, TT_FunctionLBrace,
+             TT_ImplicitStringLiteral, TT_InlineASMBrace, TT_FatArrow,
+-            TT_NamespaceMacro, TT_OverloadedOperator, TT_RegexLiteral,
+-            TT_TemplateString, TT_ObjCStringLiteral, TT_UntouchableMacroFunc,
+-            TT_StatementAttributeLikeMacro, TT_FunctionLikeOrFreestandingMacro,
+-            TT_ClassLBrace, TT_EnumLBrace, TT_RecordLBrace, TT_StructLBrace,
+-            TT_UnionLBrace, TT_RequiresClause,
++            TT_LambdaArrow, TT_NamespaceMacro, TT_OverloadedOperator,
++            TT_RegexLiteral, TT_TemplateString, TT_ObjCStringLiteral,
++            TT_UntouchableMacroFunc, TT_StatementAttributeLikeMacro,
++            TT_FunctionLikeOrFreestandingMacro, TT_ClassLBrace, TT_EnumLBrace,
++            TT_RecordLBrace, TT_StructLBrace, TT_UnionLBrace, TT_RequiresClause,
+             TT_RequiresClauseInARequiresExpression, TT_RequiresExpression,
+             TT_RequiresExpressionLParen, TT_RequiresExpressionLBrace,
+             TT_BracedListLBrace)) {
+@@ -2247,7 +2249,7 @@ private:
+       Contexts.back().IsExpression = true;
+     } else if (Current.is(TT_TrailingReturnArrow)) {
+       Contexts.back().IsExpression = false;
+-    } else if (Current.is(Keywords.kw_assert)) {
++    } else if (Current.isOneOf(TT_LambdaArrow, Keywords.kw_assert)) {
+       Contexts.back().IsExpression = Style.Language == FormatStyle::LK_Java;
+     } else if (Current.Previous &&
+                Current.Previous->is(TT_CtorInitializerColon)) {
+@@ -2382,7 +2384,7 @@ private:
+       AutoFound = true;
+     } else if (Current.is(tok::arrow) &&
+                Style.Language == FormatStyle::LK_Java) {
+-      Current.setType(TT_TrailingReturnArrow);
++      Current.setType(TT_LambdaArrow);
+     } else if (Current.is(tok::arrow) && Style.isVerilog()) {
+       // The implication operator.
+       Current.setType(TT_BinaryOperator);
+@@ -3286,7 +3288,7 @@ private:
+       }
+       if (Current->is(TT_JsComputedPropertyName))
+         return prec::Assignment;
+-      if (Current->is(TT_TrailingReturnArrow))
++      if (Current->is(TT_LambdaArrow))
+         return prec::Comma;
+       if (Current->is(TT_FatArrow))
+         return prec::Assignment;
+@@ -4211,7 +4213,7 @@ unsigned TokenAnnotator::splitPenalty(const AnnotatedLine &Line,
+   }
+   if (Right.is(TT_PointerOrReference))
+     return 190;
+-  if (Right.is(TT_TrailingReturnArrow))
++  if (Right.is(TT_LambdaArrow))
+     return 110;
+   if (Left.is(tok::equal) && Right.is(tok::l_brace))
+     return 160;
+@@ -5289,9 +5291,10 @@ bool TokenAnnotator::spaceRequiredBefore(const AnnotatedLine &Line,
+     return false;
+   }
+ 
+-  if (Right.is(TT_TrailingReturnArrow) || Left.is(TT_TrailingReturnArrow))
++  if (Right.isOneOf(TT_TrailingReturnArrow, TT_LambdaArrow) ||
++      Left.isOneOf(TT_TrailingReturnArrow, TT_LambdaArrow)) {
+     return true;
+-
++  }
+   if (Left.is(tok::comma) && Right.isNot(TT_OverloadedOperatorLParen) &&
+       // In an unexpanded macro call we only find the parentheses and commas
+       // in a line; the commas and closing parenthesis do not require a space.
+@@ -6326,8 +6329,8 @@ bool TokenAnnotator::canBreakBefore(const AnnotatedLine &Line,
+   return Left.isOneOf(tok::comma, tok::coloncolon, tok::semi, tok::l_brace,
+                       tok::kw_class, tok::kw_struct, tok::comment) ||
+          Right.isMemberAccess() ||
+-         Right.isOneOf(TT_TrailingReturnArrow, tok::lessless, tok::colon,
+-                       tok::l_square, tok::at) ||
++         Right.isOneOf(TT_TrailingReturnArrow, TT_LambdaArrow, tok::lessless,
++                       tok::colon, tok::l_square, tok::at) ||
+          (Left.is(tok::r_paren) &&
+           Right.isOneOf(tok::identifier, tok::kw_const)) ||
+          (Left.is(tok::l_paren) && Right.isNot(tok::r_paren)) ||
+diff --git a/clang/lib/Format/UnwrappedLineParser.cpp b/clang/lib/Format/UnwrappedLineParser.cpp
+index 5f1a88d4bcd7..9997a8705bdd 100644
+--- a/clang/lib/Format/UnwrappedLineParser.cpp
++++ b/clang/lib/Format/UnwrappedLineParser.cpp
+@@ -2326,7 +2326,7 @@ bool UnwrappedLineParser::tryToParseLambda() {
+       // This might or might not actually be a lambda arrow (this could be an
+       // ObjC method invocation followed by a dereferencing arrow). We might
+       // reset this back to TT_Unknown in TokenAnnotator.
+-      FormatTok->setFinalizedType(TT_TrailingReturnArrow);
++      FormatTok->setFinalizedType(TT_LambdaArrow);
+       SeenArrow = true;
+       nextToken();
+       break;
+diff --git a/clang/unittests/Format/TokenAnnotatorTest.cpp b/clang/unittests/Format/TokenAnnotatorTest.cpp
+index 99798de43e70..de524fbe982f 100644
+--- a/clang/unittests/Format/TokenAnnotatorTest.cpp
++++ b/clang/unittests/Format/TokenAnnotatorTest.cpp
+@@ -42,6 +42,8 @@ protected:
+   EXPECT_EQ((FormatTok)->getPrecedence(), Prec) << *(FormatTok)
+ #define EXPECT_BRACE_KIND(FormatTok, Kind)                                     \
+   EXPECT_EQ(FormatTok->getBlockKind(), Kind) << *(FormatTok)
++#define EXPECT_SPLIT_PENALTY(FormatTok, Penalty)                               \
++  EXPECT_EQ(FormatTok->SplitPenalty, Penalty) << *(FormatTok)
+ #define EXPECT_TOKEN(FormatTok, Kind, Type)                                    \
+   do {                                                                         \
+     EXPECT_TOKEN_KIND(FormatTok, Kind);                                        \
+@@ -1706,21 +1708,21 @@ TEST_F(TokenAnnotatorTest, UnderstandsLambdas) {
+   ASSERT_EQ(Tokens.size(), 9u) << Tokens;
+   EXPECT_TOKEN(Tokens[0], tok::l_square, TT_LambdaLSquare);
+   EXPECT_TOKEN(Tokens[2], tok::l_paren, TT_LambdaDefinitionLParen);
+-  EXPECT_TOKEN(Tokens[4], tok::arrow, TT_TrailingReturnArrow);
++  EXPECT_TOKEN(Tokens[4], tok::arrow, TT_LambdaArrow);
+   EXPECT_TOKEN(Tokens[6], tok::l_brace, TT_LambdaLBrace);
+ 
+   Tokens = annotate("[]() -> auto & {}");
+   ASSERT_EQ(Tokens.size(), 10u) << Tokens;
+   EXPECT_TOKEN(Tokens[0], tok::l_square, TT_LambdaLSquare);
+   EXPECT_TOKEN(Tokens[2], tok::l_paren, TT_LambdaDefinitionLParen);
+-  EXPECT_TOKEN(Tokens[4], tok::arrow, TT_TrailingReturnArrow);
++  EXPECT_TOKEN(Tokens[4], tok::arrow, TT_LambdaArrow);
+   EXPECT_TOKEN(Tokens[7], tok::l_brace, TT_LambdaLBrace);
+ 
+   Tokens = annotate("[]() -> auto * {}");
+   ASSERT_EQ(Tokens.size(), 10u) << Tokens;
+   EXPECT_TOKEN(Tokens[0], tok::l_square, TT_LambdaLSquare);
+   EXPECT_TOKEN(Tokens[2], tok::l_paren, TT_LambdaDefinitionLParen);
+-  EXPECT_TOKEN(Tokens[4], tok::arrow, TT_TrailingReturnArrow);
++  EXPECT_TOKEN(Tokens[4], tok::arrow, TT_LambdaArrow);
+   EXPECT_TOKEN(Tokens[7], tok::l_brace, TT_LambdaLBrace);
+ 
+   Tokens = annotate("[] {}");
+@@ -1736,20 +1738,20 @@ TEST_F(TokenAnnotatorTest, UnderstandsLambdas) {
+   Tokens = annotate("[] -> auto {}");
+   ASSERT_EQ(Tokens.size(), 7u) << Tokens;
+   EXPECT_TOKEN(Tokens[0], tok::l_square, TT_LambdaLSquare);
+-  EXPECT_TOKEN(Tokens[2], tok::arrow, TT_TrailingReturnArrow);
++  EXPECT_TOKEN(Tokens[2], tok::arrow, TT_LambdaArrow);
+   EXPECT_TOKEN(Tokens[4], tok::l_brace, TT_LambdaLBrace);
+ 
+   Tokens = annotate("[] -> struct S { return {}; }");
+   ASSERT_EQ(Tokens.size(), 12u) << Tokens;
+   EXPECT_TOKEN(Tokens[0], tok::l_square, TT_LambdaLSquare);
+-  EXPECT_TOKEN(Tokens[2], tok::arrow, TT_TrailingReturnArrow);
++  EXPECT_TOKEN(Tokens[2], tok::arrow, TT_LambdaArrow);
+   EXPECT_TOKEN(Tokens[5], tok::l_brace, TT_LambdaLBrace);
+ 
+   Tokens = annotate("foo([&](u32 bar) __attribute__((attr)) -> void {});");
+   ASSERT_EQ(Tokens.size(), 22u) << Tokens;
+   EXPECT_TOKEN(Tokens[2], tok::l_square, TT_LambdaLSquare);
+   EXPECT_TOKEN(Tokens[5], tok::l_paren, TT_LambdaDefinitionLParen);
+-  EXPECT_TOKEN(Tokens[15], tok::arrow, TT_TrailingReturnArrow);
++  EXPECT_TOKEN(Tokens[15], tok::arrow, TT_LambdaArrow);
+   EXPECT_TOKEN(Tokens[17], tok::l_brace, TT_LambdaLBrace);
+ 
+   Tokens = annotate("[] <typename T> () {}");
+@@ -1838,7 +1840,7 @@ TEST_F(TokenAnnotatorTest, UnderstandsLambdas) {
+   EXPECT_TOKEN(Tokens[0], tok::l_square, TT_LambdaLSquare);
+   EXPECT_TOKEN(Tokens[2], tok::less, TT_TemplateOpener);
+   EXPECT_TOKEN(Tokens[6], tok::l_paren, TT_LambdaDefinitionLParen);
+-  EXPECT_TOKEN(Tokens[10], tok::arrow, TT_TrailingReturnArrow);
++  EXPECT_TOKEN(Tokens[10], tok::arrow, TT_LambdaArrow);
+   EXPECT_TOKEN(Tokens[12], tok::kw_requires, TT_RequiresClause);
+   EXPECT_TRUE(Tokens[16]->ClosesRequiresClause);
+   EXPECT_TOKEN(Tokens[17], tok::l_brace, TT_LambdaLBrace);
+@@ -1907,7 +1909,7 @@ TEST_F(TokenAnnotatorTest, UnderstandsLambdas) {
+   EXPECT_TOKEN(Tokens[2], tok::less, TT_TemplateOpener);
+   EXPECT_TOKEN(Tokens[6], tok::kw_requires, TT_RequiresClause);
+   EXPECT_TRUE(Tokens[10]->ClosesRequiresClause);
+-  EXPECT_TOKEN(Tokens[11], tok::arrow, TT_TrailingReturnArrow);
++  EXPECT_TOKEN(Tokens[11], tok::arrow, TT_LambdaArrow);
+   EXPECT_TOKEN(Tokens[13], tok::l_brace, TT_LambdaLBrace);
+ 
+   Tokens = annotate("[] <typename T> requires Foo<T> (T t) requires Bar<T> {}");
+@@ -3342,7 +3344,7 @@ TEST_F(TokenAnnotatorTest, FunctionTryBlock) {
+   EXPECT_TOKEN(Tokens[3], tok::l_paren, TT_FunctionDeclarationLParen);
+   EXPECT_TOKEN(Tokens[11], tok::colon, TT_CtorInitializerColon);
+   EXPECT_TOKEN(Tokens[14], tok::l_square, TT_LambdaLSquare);
+-  EXPECT_TOKEN(Tokens[16], tok::arrow, TT_TrailingReturnArrow);
++  EXPECT_TOKEN(Tokens[16], tok::arrow, TT_LambdaArrow);
+   EXPECT_TOKEN(Tokens[20], tok::l_brace, TT_LambdaLBrace);
+   EXPECT_TOKEN(Tokens[31], tok::comma, TT_CtorInitializerComma);
+   EXPECT_TOKEN(Tokens[36], tok::l_brace, TT_FunctionLBrace);
+@@ -3369,6 +3371,20 @@ TEST_F(TokenAnnotatorTest, GNULanguageStandard) {
+   EXPECT_TOKEN(Tokens[2], tok::spaceship, TT_BinaryOperator);
+ }
+ 
++TEST_F(TokenAnnotatorTest, SplitPenalty) {
++  auto Style = getLLVMStyle();
++  Style.ColumnLimit = 20;
++
++  auto Tokens = annotate("class foo {\n"
++                         "  auto bar()\n"
++                         "      -> bool;\n"
++                         "};",
++                         Style);
++  ASSERT_EQ(Tokens.size(), 13u) << Tokens;
++  EXPECT_TOKEN(Tokens[7], tok::arrow, TT_TrailingReturnArrow);
++  EXPECT_SPLIT_PENALTY(Tokens[7], 23u);
++}
++
+ } // namespace
+ } // namespace format
+ } // namespace clang
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/815bf0f190383e14068f191bb0d16d62fbb6f8b3.patch b/llvm_patches/cherry/815bf0f190383e14068f191bb0d16d62fbb6f8b3.patch
new file mode 100644
index 00000000..13685d56
--- /dev/null
+++ b/llvm_patches/cherry/815bf0f190383e14068f191bb0d16d62fbb6f8b3.patch
@@ -0,0 +1,86 @@
+From eb6385859d4df71f030b1d8679ca1b98eba4c780 Mon Sep 17 00:00:00 2001
+From: Harini0924 <harinidonthula@gmail.com>
+Date: Tue, 27 Aug 2024 18:52:08 -0700
+Subject: [PATCH] Revert "[Clang] [Test] Use lit Syntax for Environment
+ Variables in Clang subproject" (#106267)
+
+Reverts llvm/llvm-project#102647
+
+I am reverting this change because the `readfile` doesn't actually
+perform any useful operation, and yet, for some reason, the test still
+passed. This indicates that the modification was unnecessary and could
+lead to confusion or unexpected behavior in the future.
+
+patch.cherry: true
+patch.metadata.original_sha: 815bf0f190383e14068f191bb0d16d62fbb6f8b3
+patch.platforms: chromiumos
+patch.version_range.from: 547155
+patch.version_range.until: 547783
+
+---
+ clang/test/ClangScanDeps/pr61006.cppm     | 10 +++++-----
+ clang/test/Driver/coverage.c              |  4 ++--
+ clang/test/Driver/program-path-priority.c |  4 ++--
+ 3 files changed, 9 insertions(+), 9 deletions(-)
+
+diff --git a/clang/test/ClangScanDeps/pr61006.cppm b/clang/test/ClangScanDeps/pr61006.cppm
+index 9ce6edaf2010..f75edd38c81b 100644
+--- a/clang/test/ClangScanDeps/pr61006.cppm
++++ b/clang/test/ClangScanDeps/pr61006.cppm
+@@ -6,13 +6,13 @@
+ // RUN: mkdir -p %t
+ // RUN: split-file %s %t
+ //
+-// RUN: %clang -print-resource-dir > %t/resource-dir.txt && \
++// RUN: EXPECTED_RESOURCE_DIR=`%clang -print-resource-dir` && \
+ // RUN: ln -s %clang++ %t/clang++ && \
+-// RUN: sed "s|EXPECTED_RESOURCE_DIR|%{readfile:%t/resource-dir.txt}|g; s|DIR|%/t|g" %t/P1689.json.in > %t/P1689.json && \
+-// RUN: env EXPECTED_RESOURCE_DIR=%{readfile:%t/resource-dir.txt} clang-scan-deps -compilation-database %t/P1689.json -format=p1689 | FileCheck %t/a.cpp -DPREFIX=%/t && \
+-// RUN: env EXPECTED_RESOURCE_DIR=%{readfile:%t/resource-dir.txt} clang-scan-deps -format=p1689 \
++// RUN: sed "s|EXPECTED_RESOURCE_DIR|$EXPECTED_RESOURCE_DIR|g; s|DIR|%/t|g" %t/P1689.json.in > %t/P1689.json && \
++// RUN: clang-scan-deps -compilation-database %t/P1689.json -format=p1689 | FileCheck %t/a.cpp -DPREFIX=%/t && \
++// RUN: clang-scan-deps -format=p1689 \
+ // RUN:   -- %t/clang++ -std=c++20 -c -fprebuilt-module-path=%t %t/a.cpp -o %t/a.o \
+-// RUN:      -resource-dir %{env:EXPECTED_RESOURCE_DIR} | FileCheck %t/a.cpp -DPREFIX=%/t
++// RUN:      -resource-dir $EXPECTED_RESOURCE_DIR | FileCheck %t/a.cpp -DPREFIX=%/t
+ 
+ //--- P1689.json.in
+ [
+diff --git a/clang/test/Driver/coverage.c b/clang/test/Driver/coverage.c
+index ab791ada2d35..e5ed064aab45 100644
+--- a/clang/test/Driver/coverage.c
++++ b/clang/test/Driver/coverage.c
+@@ -18,7 +18,7 @@
+ // GCNO-LOCATION-REL: "-coverage-notes-file={{.*}}{{/|\\\\}}foo/bar.gcno"
+ 
+ /// GCC allows PWD to change the paths.
+-// RUN: %if system-linux %{ env PWD=/proc/self/cwd %clang -### -c --coverage %s -o foo/bar.o 2>&1 | FileCheck --check-prefix=PWD %s %}
++// RUN: %if system-linux %{ PWD=/proc/self/cwd %clang -### -c --coverage %s -o foo/bar.o 2>&1 | FileCheck --check-prefix=PWD %s %}
+ // PWD: "-coverage-notes-file=/proc/self/cwd/foo/bar.gcno" "-coverage-data-file=/proc/self/cwd/foo/bar.gcda"
+ 
+ /// Don't warn -Wunused-command-line-argument.
+@@ -50,6 +50,6 @@
+ // LINK2: -cc1{{.*}} "-coverage-notes-file={{.*}}{{/|\\\\}}f/gb.gcno" "-coverage-data-file={{.*}}{{/|\\\\}}f/gb.gcda"
+ 
+ /// GCC allows PWD to change the paths.
+-// RUN: %if system-linux %{ env PWD=/proc/self/cwd %clang -### --coverage d/a.c d/b.c -o e/x -fprofile-dir=f 2>&1 | FileCheck %s --check-prefix=LINK3 %}
++// RUN: %if system-linux %{ PWD=/proc/self/cwd %clang -### --coverage d/a.c d/b.c -o e/x -fprofile-dir=f 2>&1 | FileCheck %s --check-prefix=LINK3 %}
+ // LINK3: -cc1{{.*}} "-coverage-notes-file=/proc/self/cwd/e/x-a.gcno" "-coverage-data-file=f/proc/self/cwd/e/x-a.gcda"
+ // LINK3: -cc1{{.*}} "-coverage-notes-file=/proc/self/cwd/e/x-b.gcno" "-coverage-data-file=f/proc/self/cwd/e/x-b.gcda"
+diff --git a/clang/test/Driver/program-path-priority.c b/clang/test/Driver/program-path-priority.c
+index 358a06d7c6d1..c940c4ced944 100644
+--- a/clang/test/Driver/program-path-priority.c
++++ b/clang/test/Driver/program-path-priority.c
+@@ -87,8 +87,8 @@
+ 
+ /// <default-triple>-gcc has lowest priority so <triple>-gcc
+ /// on PATH beats default triple in program path
+-// RUN: %t/clang --version | grep "Target:" | cut -d ' ' -f2 > %t/default-triple.txt
+-// RUN: env DEFAULT_TRIPLE=%{readfile:%t/default-triple.txt} touch %t/%{env:DEFAULT_TRIPLE}-gcc && chmod +x %t/%{env:DEFAULT_TRIPLE}-gcc
++// RUN: DEFAULT_TRIPLE=`%t/clang --version | grep "Target:" | cut -d ' ' -f2`
++// RUN: touch %t/$DEFAULT_TRIPLE-gcc && chmod +x %t/$DEFAULT_TRIPLE-gcc
+ // RUN: touch %t/%target_triple-gcc && chmod +x %t/%target_triple-gcc
+ // RUN: env "PATH=%t/env/" %t/clang -### -target notreal-none-elf %s 2>&1 | \
+ // RUN:   FileCheck --check-prefix=DEFAULT_TRIPLE_GCC %s
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/8fff0c181f26a5e8b2344c061ebf2559118b1160.patch b/llvm_patches/cherry/8fff0c181f26a5e8b2344c061ebf2559118b1160.patch
new file mode 100644
index 00000000..eeca6ab1
--- /dev/null
+++ b/llvm_patches/cherry/8fff0c181f26a5e8b2344c061ebf2559118b1160.patch
@@ -0,0 +1,98 @@
+From b9ad28fbe9bfd7c1cdbeb773f70bf6abddc77271 Mon Sep 17 00:00:00 2001
+From: Jordan R AW <ajordanr@google.com>
+Date: Fri, 14 Feb 2025 21:37:39 -0800
+Subject: [PATCH] [lldb] Add terminfo dependency for ncurses support (#126810)
+
+For some operating systems (e.g. chromiumos), terminfo is a separate
+package and library from ncurses. Both are still requirements for curses
+support in lldb, individually.
+
+This is a rework of this original spack commit:
+
+https://github.com/spack/spack/commit/9ea261265010eacd250691a8361f661d0576f25c
+
+Instead though, this PR uses CMake to detect whether the symbol is
+present and defined in the curses library, and only falls back to a separate
+tinfo if not found.
+
+Without this fix, LLDB cannot be built on these systems.
+
+Fixes #101368
+
+patch.cherry: true
+patch.metadata.original_sha: 8fff0c181f26a5e8b2344c061ebf2559118b1160
+patch.platforms: chromiumos
+patch.version_range.from: 537678
+patch.version_range.until: 565379
+
+---
+ lldb/cmake/modules/FindCursesAndPanel.cmake | 42 ++++++++++++++++++---
+ 1 file changed, 37 insertions(+), 5 deletions(-)
+
+diff --git a/lldb/cmake/modules/FindCursesAndPanel.cmake b/lldb/cmake/modules/FindCursesAndPanel.cmake
+index aaadf214bf54..75ebaa35d7ea 100644
+--- a/lldb/cmake/modules/FindCursesAndPanel.cmake
++++ b/lldb/cmake/modules/FindCursesAndPanel.cmake
+@@ -2,23 +2,55 @@
+ # FindCursesAndPanel
+ # -----------
+ #
+-# Find the curses and panel library as a whole.
++# Find the curses, terminfo, and panel library as a whole.
+ 
+-if(CURSES_INCLUDE_DIRS AND CURSES_LIBRARIES AND PANEL_LIBRARIES)
++include(CMakePushCheckState)
++
++function(lldb_check_curses_tinfo CURSES_LIBRARIES CURSES_HAS_TINFO)
++  cmake_reset_check_state()
++  set(CMAKE_REQUIRED_LIBRARIES "${CURSES_LIBRARIES}")
++  # acs_map is one of many symbols that are part of tinfo but could
++  # be bundled in curses.
++  check_symbol_exists(acs_map "curses.h" CURSES_HAS_TINFO)
++endfunction()
++
++if(CURSES_INCLUDE_DIRS AND CURSES_LIBRARIES AND TINFO_LIBRARIES AND PANEL_LIBRARIES)
+   set(CURSESANDPANEL_FOUND TRUE)
+ else()
+   find_package(Curses QUIET)
+   find_library(PANEL_LIBRARIES NAMES panel DOC "The curses panel library" QUIET)
+   include(FindPackageHandleStandardArgs)
++
++  if(CURSES_FOUND AND PANEL_LIBRARIES)
++    # Sometimes the curses libraries define their own terminfo symbols,
++    # other times they're extern and are defined by a separate terminfo library.
++    # Auto-detect which.
++    lldb_check_curses_tinfo("${CURSES_LIBRARIES}" CURSES_HAS_TINFO)
++    if (NOT CURSES_HAS_TINFO)
++      message(STATUS "curses library missing terminfo symbols, looking for tinfo separately")
++      find_library(TINFO_LIBRARIES NAMES tinfo DOC "The curses tinfo library" QUIET)
++      list(APPEND CURSES_LIBRARIES "${TINFO_LIBRARIES}")
++    endif()
++    set(HAS_TERMINFO_SYMBOLS "$<OR:$<BOOL:${TERMINFO_LIBRARIES}>,$<BOOL:${CURSES_HAS_TINFO}>>")
++  endif()
++
+   find_package_handle_standard_args(CursesAndPanel
+                                     FOUND_VAR
+                                       CURSESANDPANEL_FOUND
+                                     REQUIRED_VARS
+                                       CURSES_INCLUDE_DIRS
+                                       CURSES_LIBRARIES
+-                                      PANEL_LIBRARIES)
+-  if(CURSES_FOUND AND PANEL_LIBRARIES)
+-    mark_as_advanced(CURSES_INCLUDE_DIRS CURSES_LIBRARIES PANEL_LIBRARIES)
++                                      PANEL_LIBRARIES
++                                      HAS_TERMINFO_SYMBOLS)
++
++  if(CURSES_FOUND AND PANEL_LIBRARIES AND HAS_TERMINFO_SYMBOLS)
++    mark_as_advanced(CURSES_INCLUDE_DIRS
++                      PANEL_LIBRARIES
++                      HAS_TERMINFO_SYMBOLS
++                      CURSES_HAS_TINFO)
++  endif()
++  if(TINFO_LIBRARIES)
++    mark_as_advanced(TINFO_LIBRARIES)
+   endif()
+ endif()
+ 
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/9671ed1afcd1e3aab754a9b905d511ffd52f3624.patch b/llvm_patches/cherry/9671ed1afcd1e3aab754a9b905d511ffd52f3624.patch
new file mode 100644
index 00000000..56e898c6
--- /dev/null
+++ b/llvm_patches/cherry/9671ed1afcd1e3aab754a9b905d511ffd52f3624.patch
@@ -0,0 +1,155 @@
+From e6eea0e3cf5af08d1a920621cb91fb15e9068df2 Mon Sep 17 00:00:00 2001
+From: Danial Klimkin <dklimkin@google.com>
+Date: Tue, 27 Aug 2024 18:45:22 +0200
+Subject: [PATCH] Revert "LSV: forbid load-cycles when vectorizing; fix bug
+ (#104815)" (#106245)
+
+This reverts commit c46b41aaa6eaa787f808738d14c61a2f8b6d839f.
+
+Multiple tests time out, either due to performance hit (see comment) or
+a cycle.
+
+patch.cherry: true
+patch.metadata.original_sha: 9671ed1afcd1e3aab754a9b905d511ffd52f3624
+patch.platforms: chromiumos
+patch.version_range.from: 547233
+patch.version_range.until: 547704
+
+---
+ .../Vectorize/LoadStoreVectorizer.cpp         | 24 +-----
+ .../LoadStoreVectorizer/AArch64/pr37865.ll    | 79 +------------------
+ 2 files changed, 5 insertions(+), 98 deletions(-)
+
+diff --git a/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp b/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp
+index dd37c95eca61..02ec1d5c259c 100644
+--- a/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp
++++ b/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp
+@@ -998,32 +998,10 @@ bool Vectorizer::isSafeToMove(
+   LLVM_DEBUG(dbgs() << "LSV: isSafeToMove(" << *ChainElem << " -> "
+                     << *ChainBegin << ")\n");
+ 
+-  assert(isa<LoadInst>(ChainElem) == IsLoadChain &&
+-         isa<LoadInst>(ChainBegin) == IsLoadChain);
+-
++  assert(isa<LoadInst>(ChainElem) == IsLoadChain);
+   if (ChainElem == ChainBegin)
+     return true;
+ 
+-  if constexpr (IsLoadChain) {
+-    // If ChainElem depends on ChainBegin, they're not safe to reorder.
+-    SmallVector<Instruction *, 8> Worklist;
+-    Worklist.emplace_back(ChainElem);
+-    while (!Worklist.empty()) {
+-      Instruction *I = Worklist.pop_back_val();
+-      for (Use &O : I->operands()) {
+-        if (isa<PHINode>(O))
+-          continue;
+-        if (auto *J = dyn_cast<Instruction>(O)) {
+-          if (J == ChainBegin) {
+-            LLVM_DEBUG(dbgs() << "LSV: dependent loads; not safe to reorder\n");
+-            return false;
+-          }
+-          Worklist.emplace_back(J);
+-        }
+-      }
+-    }
+-  }
+-
+   // Invariant loads can always be reordered; by definition they are not
+   // clobbered by stores.
+   if (isInvariantLoad(ChainElem))
+diff --git a/llvm/test/Transforms/LoadStoreVectorizer/AArch64/pr37865.ll b/llvm/test/Transforms/LoadStoreVectorizer/AArch64/pr37865.ll
+index 0beca8c15305..833e70814c29 100644
+--- a/llvm/test/Transforms/LoadStoreVectorizer/AArch64/pr37865.ll
++++ b/llvm/test/Transforms/LoadStoreVectorizer/AArch64/pr37865.ll
+@@ -1,22 +1,9 @@
+-; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
+-; RUN: opt -mtriple=aarch64 -passes=load-store-vectorizer -S %s | FileCheck %s
+-
+-; LSV was attempting to vectorize this earlier, but crashed while re-ordering
+-; instructions due to the load-load cycle. Now, the candidate loads are no
+-; longer considered safe for reordering.
++; REQUIRES: asserts
++; RUN: not --crash opt -mtriple=aarch64 -passes=load-store-vectorizer \
++; RUN:   -disable-output %s 2>&1 | FileCheck %s
+ 
+ define i32 @load_cycle(ptr %x) {
+-; CHECK-LABEL: define i32 @load_cycle(
+-; CHECK-SAME: ptr [[X:%.*]]) {
+-; CHECK-NEXT:  [[ENTRY:.*:]]
+-; CHECK-NEXT:    [[GEP_X_1:%.*]] = getelementptr inbounds [2 x i32], ptr [[X]], i32 0, i32 1
+-; CHECK-NEXT:    [[LOAD_X_1:%.*]] = load i32, ptr [[GEP_X_1]], align 4
+-; CHECK-NEXT:    [[REM:%.*]] = urem i32 [[LOAD_X_1]], 1
+-; CHECK-NEXT:    [[GEP_X_2:%.*]] = getelementptr inbounds [2 x i32], ptr [[X]], i32 [[REM]], i32 0
+-; CHECK-NEXT:    [[LOAD_X_2:%.*]] = load i32, ptr [[GEP_X_2]], align 4
+-; CHECK-NEXT:    [[RET:%.*]] = add i32 [[LOAD_X_2]], [[LOAD_X_1]]
+-; CHECK-NEXT:    ret i32 [[RET]]
+-;
++; CHECK: Unexpected cycle while re-ordering instructions
+ entry:
+   %gep.x.1 = getelementptr inbounds [2 x i32], ptr %x, i32 0, i32 1
+   %load.x.1 = load i32, ptr %gep.x.1
+@@ -26,61 +13,3 @@ entry:
+   %ret = add i32 %load.x.2, %load.x.1
+   ret i32 %ret
+ }
+-
+-define i32 @load_cycle2(ptr %x, i32 %y) {
+-; CHECK-LABEL: define i32 @load_cycle2(
+-; CHECK-SAME: ptr [[X:%.*]], i32 [[Y:%.*]]) {
+-; CHECK-NEXT:  [[ENTRY:.*:]]
+-; CHECK-NEXT:    [[GEP_X_1:%.*]] = getelementptr inbounds [2 x i32], ptr [[X]], i32 [[Y]], i32 1
+-; CHECK-NEXT:    [[LOAD_X_1:%.*]] = load i32, ptr [[GEP_X_1]], align 4
+-; CHECK-NEXT:    [[MUL:%.*]] = mul i32 [[LOAD_X_1]], 2
+-; CHECK-NEXT:    [[ADD:%.*]] = add i32 [[Y]], [[MUL]]
+-; CHECK-NEXT:    [[SUB_1:%.*]] = sub i32 [[ADD]], [[LOAD_X_1]]
+-; CHECK-NEXT:    [[SUB_2:%.*]] = sub i32 [[SUB_1]], [[LOAD_X_1]]
+-; CHECK-NEXT:    [[GEP_X_2:%.*]] = getelementptr inbounds [2 x i32], ptr [[X]], i32 [[SUB_2]], i32 0
+-; CHECK-NEXT:    [[LOAD_X_2:%.*]] = load i32, ptr [[GEP_X_2]], align 4
+-; CHECK-NEXT:    [[RET:%.*]] = add i32 [[LOAD_X_2]], [[LOAD_X_1]]
+-; CHECK-NEXT:    ret i32 [[RET]]
+-;
+-entry:
+-  %gep.x.1 = getelementptr inbounds [2 x i32], ptr %x, i32 %y, i32 1
+-  %load.x.1 = load i32, ptr %gep.x.1
+-  %mul = mul i32 %load.x.1, 2
+-  %add = add i32 %y, %mul
+-  %sub.1 = sub i32 %add, %load.x.1
+-  %sub.2 = sub i32 %sub.1, %load.x.1
+-  %gep.x.2 = getelementptr inbounds [2 x i32], ptr %x, i32 %sub.2, i32 0
+-  %load.x.2 = load i32, ptr %gep.x.2
+-  %ret = add i32 %load.x.2, %load.x.1
+-  ret i32 %ret
+-}
+-
+-@global.1 = global i32 0
+-@global.2 = global [1 x [3 x i32]] zeroinitializer
+-
+-define i16 @load_cycle3() {
+-; CHECK-LABEL: define i16 @load_cycle3() {
+-; CHECK-NEXT:  [[ENTRY:.*:]]
+-; CHECK-NEXT:    [[LOAD_1:%.*]] = load i32, ptr @global.1, align 4
+-; CHECK-NEXT:    [[UREM_1:%.*]] = urem i32 [[LOAD_1]], 1
+-; CHECK-NEXT:    [[GEP_1:%.*]] = getelementptr inbounds [1 x [3 x i32]], ptr @global.2, i32 0, i32 [[UREM_1]]
+-; CHECK-NEXT:    [[GEP_2:%.*]] = getelementptr inbounds [3 x i32], ptr [[GEP_1]], i32 0, i32 2
+-; CHECK-NEXT:    [[LOAD_2:%.*]] = load i32, ptr [[GEP_2]], align 4
+-; CHECK-NEXT:    [[UREM_2:%.*]] = urem i32 [[LOAD_2]], 1
+-; CHECK-NEXT:    [[GEP_3:%.*]] = getelementptr inbounds [1 x [3 x i32]], ptr @global.2, i32 0, i32 [[UREM_2]]
+-; CHECK-NEXT:    [[GEP_4:%.*]] = getelementptr inbounds [3 x i32], ptr [[GEP_3]], i32 0, i32 1
+-; CHECK-NEXT:    [[LOAD_3:%.*]] = load i32, ptr [[GEP_4]], align 4
+-; CHECK-NEXT:    ret i16 0
+-;
+-entry:
+-  %load.1 = load i32, ptr @global.1
+-  %urem.1 = urem i32 %load.1, 1
+-  %gep.1 = getelementptr inbounds [1 x [3 x i32]], ptr @global.2, i32 0, i32 %urem.1
+-  %gep.2 = getelementptr inbounds [3 x i32], ptr %gep.1, i32 0, i32 2
+-  %load.2 = load i32, ptr %gep.2
+-  %urem.2 = urem i32 %load.2, 1
+-  %gep.3 = getelementptr inbounds [1 x [3 x i32]], ptr @global.2, i32 0, i32 %urem.2
+-  %gep.4 = getelementptr inbounds [3 x i32], ptr %gep.3, i32 0, i32 1
+-  %load.3 = load i32, ptr %gep.4
+-  ret i16 0
+-}
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/bb6a273d9ab9ee90dbb957e541f4d810fffb22ee.patch b/llvm_patches/cherry/bb6a273d9ab9ee90dbb957e541f4d810fffb22ee.patch
new file mode 100644
index 00000000..6da471cc
--- /dev/null
+++ b/llvm_patches/cherry/bb6a273d9ab9ee90dbb957e541f4d810fffb22ee.patch
@@ -0,0 +1,78 @@
+From 7f7a97ac699a296d80dc54efb267187c2609dacb Mon Sep 17 00:00:00 2001
+From: Jordan R AW <ajordanr@google.com>
+Date: Sat, 22 Feb 2025 09:13:46 -0800
+Subject: [PATCH] [lldb] Fix manual CURSES_LIBRARIES tinfo finding (#128245)
+
+At present, we automatically detect terminfo symbols in CURSES_LIBRARIES
+after it is found through find_package. However, by introducing a check
+for TINFO_LIBRARIES, we break systems which pass all of
+CURSES_INCLUDE_DIRS, CURSES_LIBRARIES, and PANEL_LIBRARIES individually
+without passing TINFO_LIBRARIES. We'd rather not expose TINFO_LIBRARIES
+at all.
+
+This commit preemptively attempts to fix issues encountered on systems
+that manually pass CURSES_LIBRARIES which already contain the necessary
+terminfo symbols (e.g. 'acs_map').
+
+See this breakage in Google Fuchsia:
+https://issues.fuchsia.dev/397455029
+
+References Issue #101368
+
+patch.cherry: true
+patch.metadata.original_sha: bb6a273d9ab9ee90dbb957e541f4d810fffb22ee
+patch.platforms: chromiumos
+patch.version_range.from: 537678
+patch.version_range.until: 566148
+
+---
+ lldb/cmake/modules/FindCursesAndPanel.cmake | 20 ++++++++++++++++----
+ 1 file changed, 16 insertions(+), 4 deletions(-)
+
+diff --git a/lldb/cmake/modules/FindCursesAndPanel.cmake b/lldb/cmake/modules/FindCursesAndPanel.cmake
+index 75ebaa35d7ea..8628059f91ba 100644
+--- a/lldb/cmake/modules/FindCursesAndPanel.cmake
++++ b/lldb/cmake/modules/FindCursesAndPanel.cmake
+@@ -6,15 +6,25 @@
+ 
+ include(CMakePushCheckState)
+ 
+-function(lldb_check_curses_tinfo CURSES_LIBRARIES CURSES_HAS_TINFO)
++function(lldb_check_curses_tinfo CURSES_INCLUDE_DIRS CURSES_LIBRARIES CURSES_HAS_TINFO)
+   cmake_reset_check_state()
++  set(CMAKE_REQUIRED_INCLUDES "${CURSES_INCLUDE_DIRS}")
+   set(CMAKE_REQUIRED_LIBRARIES "${CURSES_LIBRARIES}")
+   # acs_map is one of many symbols that are part of tinfo but could
+   # be bundled in curses.
+   check_symbol_exists(acs_map "curses.h" CURSES_HAS_TINFO)
+ endfunction()
+ 
+-if(CURSES_INCLUDE_DIRS AND CURSES_LIBRARIES AND TINFO_LIBRARIES AND PANEL_LIBRARIES)
++if(CURSES_INCLUDE_DIRS AND CURSES_LIBRARIES AND PANEL_LIBRARIES)
++  if(NOT HAS_TERMINFO_SYMBOLS)
++    lldb_check_curses_tinfo("${CURSES_INCLUDE_DIRS}"
++                            "${CURSES_LIBRARIES}"
++                            CURSES_HAS_TINFO)
++    if(NOT CURSES_HAS_TINFO)
++      message(WARNING "CURSES_LIBRARIES was provided manually but is missing terminfo symbols")
++    endif()
++    mark_as_advanced(CURSES_HAS_TINFO)
++  endif()
+   set(CURSESANDPANEL_FOUND TRUE)
+ else()
+   find_package(Curses QUIET)
+@@ -25,8 +35,10 @@ else()
+     # Sometimes the curses libraries define their own terminfo symbols,
+     # other times they're extern and are defined by a separate terminfo library.
+     # Auto-detect which.
+-    lldb_check_curses_tinfo("${CURSES_LIBRARIES}" CURSES_HAS_TINFO)
+-    if (NOT CURSES_HAS_TINFO)
++    lldb_check_curses_tinfo("${CURSES_INCLUDE_DIRS}"
++                            "${CURSES_LIBRARIES}"
++                            CURSES_HAS_TINFO)
++    if(NOT CURSES_HAS_TINFO)
+       message(STATUS "curses library missing terminfo symbols, looking for tinfo separately")
+       find_library(TINFO_LIBRARIES NAMES tinfo DOC "The curses tinfo library" QUIET)
+       list(APPEND CURSES_LIBRARIES "${TINFO_LIBRARIES}")
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/c65ed964657c93d51f3e05de9e0609419768a143.patch b/llvm_patches/cherry/c65ed964657c93d51f3e05de9e0609419768a143.patch
new file mode 100644
index 00000000..b115ed42
--- /dev/null
+++ b/llvm_patches/cherry/c65ed964657c93d51f3e05de9e0609419768a143.patch
@@ -0,0 +1,36 @@
+From 6899bf76634f02f6a912f4c2d716d21f153a622e Mon Sep 17 00:00:00 2001
+From: Guillaume Chatelet <gchatelet@google.com>
+Date: Fri, 14 Feb 2025 11:19:26 +0100
+Subject: [PATCH] Revert "[reland][libc][bazel] Enable software prefetching for
+ memcpy" (#127189)
+
+Reverts llvm/llvm-project#113886
+
+We suspect this has caused internal performance regressions, reverting
+while root causing it more thoroughly.
+
+patch.cherry: true
+patch.metadata.original_sha: c65ed964657c93d51f3e05de9e0609419768a143
+patch.platforms: chromiumos
+patch.version_range.from: 554178
+patch.version_range.until: 565287
+
+---
+ .../bazel/llvm-project-overlay/libc/libc_configure_options.bzl  | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/utils/bazel/llvm-project-overlay/libc/libc_configure_options.bzl b/utils/bazel/llvm-project-overlay/libc/libc_configure_options.bzl
+index 96d7fa86e9dd..f65da9e98226 100644
+--- a/utils/bazel/llvm-project-overlay/libc/libc_configure_options.bzl
++++ b/utils/bazel/llvm-project-overlay/libc/libc_configure_options.bzl
+@@ -24,7 +24,7 @@ LIBC_CONFIGURE_OPTIONS = [
+     # Documentation in libc/src/string/memory_utils/...
+     # "LIBC_COPT_MEMCPY_USE_EMBEDDED_TINY",
+     # "LIBC_COPT_MEMCPY_X86_USE_REPMOVSB_FROM_SIZE",
+-    "LIBC_COPT_MEMCPY_X86_USE_SOFTWARE_PREFETCHING",
++    # "LIBC_COPT_MEMCPY_X86_USE_SOFTWARE_PREFETCHING",
+     "LIBC_COPT_MEMSET_X86_USE_SOFTWARE_PREFETCHING",
+ 
+     # Documentation in libc/docs/dev/printf_behavior.rst
+-- 
+2.49.0.504.g3bcea36a83-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/c9d0a464c9f3a0a66f35d0ca28f36a96efc6961b.patch b/llvm_patches/cherry/c9d0a464c9f3a0a66f35d0ca28f36a96efc6961b.patch
new file mode 100644
index 00000000..6e2cebb3
--- /dev/null
+++ b/llvm_patches/cherry/c9d0a464c9f3a0a66f35d0ca28f36a96efc6961b.patch
@@ -0,0 +1,197 @@
+From 8dfdc65fd7be92ff84a59e71161f6a9b307baa3a Mon Sep 17 00:00:00 2001
+From: Han-Chung Wang <hanhan0912@gmail.com>
+Date: Thu, 6 Feb 2025 05:19:45 -0800
+Subject: [PATCH] Revert "[mlir][math]Update `convertPowfOp`
+ `ExpandPatterns.cpp`" (#126063)
+
+Reverts llvm/llvm-project#124402
+
+It breaks an integration test in downstream project (i.e., IREE), which
+produces NANs. Talked to the author @ita9naiwa, and we agree to reland
+the PR after we find the issue.
+
+patch.cherry: true
+patch.metadata.original_sha: c9d0a464c9f3a0a66f35d0ca28f36a96efc6961b
+patch.platforms: chromiumos
+patch.version_range.from: 563468
+patch.version_range.until: 564416
+
+---
+ .../Math/Transforms/ExpandPatterns.cpp        | 25 +++++--
+ mlir/test/Dialect/Math/expand-math.mlir       | 71 +++++++++++++------
+ .../mlir-runner/test-expand-math-approx.mlir  |  5 ++
+ 3 files changed, 74 insertions(+), 27 deletions(-)
+
+diff --git a/mlir/lib/Dialect/Math/Transforms/ExpandPatterns.cpp b/mlir/lib/Dialect/Math/Transforms/ExpandPatterns.cpp
+index 30bcdfc45837..3dadf9474cf4 100644
+--- a/mlir/lib/Dialect/Math/Transforms/ExpandPatterns.cpp
++++ b/mlir/lib/Dialect/Math/Transforms/ExpandPatterns.cpp
+@@ -311,8 +311,7 @@ static LogicalResult convertFPowIOp(math::FPowIOp op,
+   return success();
+ }
+ 
+-// Converts Powf(float a, float b) (meaning a^b) to exp^(b * ln(a))
+-// Restricting a >= 0
++// Converts  Powf(float a, float b) (meaning a^b) to exp^(b * ln(a))
+ static LogicalResult convertPowfOp(math::PowFOp op, PatternRewriter &rewriter) {
+   ImplicitLocOpBuilder b(op->getLoc(), rewriter);
+   Value operandA = op.getOperand(0);
+@@ -320,10 +319,21 @@ static LogicalResult convertPowfOp(math::PowFOp op, PatternRewriter &rewriter) {
+   Type opType = operandA.getType();
+   Value zero = createFloatConst(op->getLoc(), opType, 0.00, rewriter);
+   Value one = createFloatConst(op->getLoc(), opType, 1.00, rewriter);
++  Value two = createFloatConst(op->getLoc(), opType, 2.00, rewriter);
++  Value negOne = createFloatConst(op->getLoc(), opType, -1.00, rewriter);
++  Value opASquared = b.create<arith::MulFOp>(opType, operandA, operandA);
++  Value opBHalf = b.create<arith::DivFOp>(opType, operandB, two);
+ 
+-  Value logA = b.create<math::LogOp>(opType, operandA);
+-  Value mult = b.create<arith::MulFOp>(opType, operandB, logA);
++  Value logA = b.create<math::LogOp>(opType, opASquared);
++  Value mult = b.create<arith::MulFOp>(opType, opBHalf, logA);
+   Value expResult = b.create<math::ExpOp>(opType, mult);
++  Value negExpResult = b.create<arith::MulFOp>(opType, expResult, negOne);
++  Value remainder = b.create<arith::RemFOp>(opType, operandB, two);
++  Value negCheck =
++      b.create<arith::CmpFOp>(arith::CmpFPredicate::OLT, operandA, zero);
++  Value oddPower =
++      b.create<arith::CmpFOp>(arith::CmpFPredicate::ONE, remainder, zero);
++  Value oddAndNeg = b.create<arith::AndIOp>(op->getLoc(), oddPower, negCheck);
+ 
+   // First, we select between the exp value and the adjusted value for odd
+   // powers of negatives. Then, we ensure that one is produced if `b` is zero.
+@@ -331,9 +341,10 @@ static LogicalResult convertPowfOp(math::PowFOp op, PatternRewriter &rewriter) {
+   // `exp(0 * ln(0)) = exp(0 *-inf) = exp(-nan) = -nan`.
+   Value zeroCheck =
+       b.create<arith::CmpFOp>(arith::CmpFPredicate::OEQ, operandB, zero);
+-  Value finalResult =
+-      b.create<arith::SelectOp>(op->getLoc(), zeroCheck, one, expResult);
+-  rewriter.replaceOp(op, finalResult);
++  Value res = b.create<arith::SelectOp>(op->getLoc(), oddAndNeg, negExpResult,
++                                        expResult);
++  res = b.create<arith::SelectOp>(op->getLoc(), zeroCheck, one, res);
++  rewriter.replaceOp(op, res);
+   return success();
+ }
+ 
+diff --git a/mlir/test/Dialect/Math/expand-math.mlir b/mlir/test/Dialect/Math/expand-math.mlir
+index 5b443e9e8d4e..6055ed0504c8 100644
+--- a/mlir/test/Dialect/Math/expand-math.mlir
++++ b/mlir/test/Dialect/Math/expand-math.mlir
+@@ -202,15 +202,25 @@ func.func @roundf_func(%a: f32) -> f32 {
+ 
+ // CHECK-LABEL:   func @powf_func
+ // CHECK-SAME:    ([[ARG0:%.+]]: f64, [[ARG1:%.+]]: f64)
+-func.func @powf_func(%a: f64, %b: f64) -> f64 {
++func.func @powf_func(%a: f64, %b: f64) ->f64 {
+   // CHECK-DAG: [[CST0:%.+]] = arith.constant 0.000000e+00
+   // CHECK-DAG: [[CST1:%.+]] = arith.constant 1.0
+-  // CHECK: [[LOGA:%.+]] = math.log [[ARG0]]
+-  // CHECK: [[MULB:%.+]] = arith.mulf [[ARG1]], [[LOGA]]
+-  // CHECK: [[EXP:%.+]] = math.exp [[MULB]]
+-  // CHECK: [[CMPF:%.+]] = arith.cmpf oeq, [[ARG1]], [[CST0]]
+-  // CHECK: [[SEL:%.+]] = arith.select [[CMPF]], [[CST1]], [[EXP]]
+-  // CHECK: return [[SEL]]
++  // CHECK-DAG: [[TWO:%.+]] = arith.constant 2.000000e+00
++  // CHECK-DAG: [[NEGONE:%.+]] = arith.constant -1.000000e+00
++  // CHECK-DAG: [[SQR:%.+]] = arith.mulf [[ARG0]], [[ARG0]]
++  // CHECK-DAG: [[HALF:%.+]] = arith.divf [[ARG1]], [[TWO]]
++  // CHECK-DAG: [[LOG:%.+]] = math.log [[SQR]]
++  // CHECK-DAG: [[MULT:%.+]] = arith.mulf [[HALF]], [[LOG]]
++  // CHECK-DAG: [[EXPR:%.+]] = math.exp [[MULT]]
++  // CHECK-DAG: [[NEGEXPR:%.+]] = arith.mulf [[EXPR]], [[NEGONE]]
++  // CHECK-DAG: [[REMF:%.+]] = arith.remf [[ARG1]], [[TWO]]
++  // CHECK-DAG: [[CMPNEG:%.+]] = arith.cmpf olt, [[ARG0]]
++  // CHECK-DAG: [[CMPZERO:%.+]] = arith.cmpf one, [[REMF]]
++  // CHECK-DAG: [[AND:%.+]] = arith.andi [[CMPZERO]], [[CMPNEG]]
++  // CHECK-DAG: [[CMPZERO:%.+]] = arith.cmpf oeq, [[ARG1]], [[CST0]]
++  // CHECK-DAG: [[SEL:%.+]] = arith.select [[AND]], [[NEGEXPR]], [[EXPR]]
++  // CHECK-DAG: [[SEL1:%.+]] = arith.select [[CMPZERO]], [[CST1]], [[SEL]]
++  // CHECK: return [[SEL1]]
+   %ret = math.powf %a, %b : f64
+   return %ret : f64
+ }
+@@ -592,15 +602,26 @@ func.func @math_fpowi_to_powf_tensor(%0 : tensor<8xf32>, %1: tensor<8xi32>) -> t
+   return %2 : tensor<8xf32>
+ }
+ // CHECK-SAME: (%[[ARG0:.*]]: tensor<8xf32>, %[[ARG1:.*]]: tensor<8xi32>) -> tensor<8xf32> {
+-// CHECK-DAG:    %[[CST1:.+]] = arith.constant dense<1.000000e+00> : tensor<8xf32>
++// CHECK-DAG:    %[[CSTNEG1:.*]] = arith.constant dense<-1.000000e+00> : tensor<8xf32>
++// CHECK-DAG:    %[[CST2:.*]] = arith.constant dense<2.000000e+00> : tensor<8xf32>
+ // CHECK-DAG:    %[[CST0:.*]] = arith.constant dense<0.000000e+00> : tensor<8xf32>
+-// CHECK: %[[TOFP:.*]] = arith.sitofp %[[ARG1]] : tensor<8xi32> to tensor<8xf32>
+-// CHECK: %[[LOGA:.*]] = math.log %[[ARG0]] : tensor<8xf32>
+-// CHECK: %[[MUL:.*]] = arith.mulf %[[TOFP]], %[[LOGA]] : tensor<8xf32>
+-// CHECK: %[[EXP:.*]] = math.exp %[[MUL]] : tensor<8xf32>
+-// CHECK: %[[CMP:.*]] = arith.cmpf oeq, %[[TOFP]], %[[CST0]] : tensor<8xf32>
+-// CHECK: %[[SEL:.*]] = arith.select %[[CMP]], %[[CST1]], %[[EXP]] : tensor<8xi1>, tensor<8xf32>
+-// CHECK: return %[[SEL]]
++// CHECK-DAG:    %[[CST1:.+]] = arith.constant dense<1.000000e+00> : tensor<8xf32>
++// CHECK:        %[[TOFP:.*]] = arith.sitofp %[[ARG1]] : tensor<8xi32> to tensor<8xf32>
++// CHECK:        %[[SQ:.*]] = arith.mulf %[[ARG0]], %[[ARG0]] : tensor<8xf32>
++// CHECK:        %[[DIV:.*]] = arith.divf %[[TOFP]], %[[CST2]] : tensor<8xf32>
++// CHECK:        %[[LG:.*]] = math.log %[[SQ]] : tensor<8xf32>
++// CHECK:        %[[MUL:.*]] = arith.mulf %[[DIV]], %[[LG]] : tensor<8xf32>
++// CHECK:        %[[EXP:.*]] = math.exp %[[MUL]] : tensor<8xf32>
++// CHECK:        %[[MUL1:.*]] = arith.mulf %[[EXP]], %[[CSTNEG1]] : tensor<8xf32>
++// CHECK:        %[[REM:.*]] = arith.remf %[[TOFP]], %[[CST2]] : tensor<8xf32>
++// CHECK:        %[[CMPF:.*]] = arith.cmpf olt, %[[ARG0]], %[[CST0]] : tensor<8xf32>
++// CHECK:        %[[CMPF1:.*]] = arith.cmpf one, %[[REM]], %[[CST0]] : tensor<8xf32>
++// CHECK:        %[[AND:.*]] = arith.andi %[[CMPF1]], %[[CMPF]] : tensor<8xi1>
++// CHECK:        %[[CMPZERO:.*]] = arith.cmpf oeq, %[[TOFP]], %[[CST0]]
++// CHECK:        %[[SEL:.*]] = arith.select %[[AND]], %[[MUL1]], %[[EXP]] : tensor<8xi1>, tensor<8xf32>
++// CHECK:        %[[SEL1:.+]] = arith.select %[[CMPZERO]], %[[CST1]], %[[SEL]]
++// CHECK:      return %[[SEL1]] : tensor<8xf32>
++
+ // -----
+ 
+ // CHECK-LABEL:   func.func @math_fpowi_to_powf_scalar
+@@ -609,15 +630,25 @@ func.func @math_fpowi_to_powf_scalar(%0 : f32, %1: i64) -> f32 {
+   return %2 : f32
+ }
+ // CHECK-SAME: (%[[ARG0:.*]]: f32, %[[ARG1:.*]]: i64) -> f32 {
++// CHECK-DAG:    %[[CSTNEG1:.*]] = arith.constant -1.000000e+00 : f32
++// CHECK-DAG:    %[[CST2:.*]] = arith.constant 2.000000e+00 : f32
+ // CHECK-DAG:    %[[CST0:.*]] = arith.constant 0.000000e+00 : f32
+ // CHECK-DAG:    %[[CST1:.+]] = arith.constant 1.000000e+00 : f32
+ // CHECK:        %[[TOFP:.*]] = arith.sitofp %[[ARG1]] : i64 to f32
+-// CHECK:        %[[LOGA:.*]] = math.log %[[ARG0]] : f32
+-// CHECK:        %[[MUL:.*]] = arith.mulf %[[TOFP]], %[[LOGA]] : f32
++// CHECK:        %[[SQ:.*]] = arith.mulf %[[ARG0]], %[[ARG0]] : f32
++// CHECK:        %[[DIV:.*]] = arith.divf %[[TOFP]], %[[CST2]] : f32
++// CHECK:        %[[LG:.*]] = math.log %[[SQ]] : f32
++// CHECK:        %[[MUL:.*]] = arith.mulf %[[DIV]], %[[LG]] : f32
+ // CHECK:        %[[EXP:.*]] = math.exp %[[MUL]] : f32
+-// CHECK:        %[[CMP:.*]] = arith.cmpf oeq, %[[TOFP]], %[[CST0]] : f32
+-// CHECK:        %[[SEL:.*]] = arith.select %[[CMP]], %[[CST1]], %[[EXP]] : f32
+-// CHECK:       return %[[SEL]] : f32
++// CHECK:        %[[MUL1:.*]] = arith.mulf %[[EXP]], %[[CSTNEG1]] : f32
++// CHECK:        %[[REM:.*]] = arith.remf %[[TOFP]], %[[CST2]] : f32
++// CHECK:        %[[CMPF:.*]] = arith.cmpf olt, %[[ARG0]], %[[CST0]] : f32
++// CHECK:        %[[CMPF1:.*]] = arith.cmpf one, %[[REM]], %[[CST0]] : f32
++// CHECK:        %[[AND:.*]] = arith.andi %[[CMPF1]], %[[CMPF]] : i1
++// CHECK:        %[[CMPZERO:.*]] = arith.cmpf oeq, %[[TOFP]], %[[CST0]]
++// CHECK:        %[[SEL:.*]] = arith.select %[[AND]], %[[MUL1]], %[[EXP]] : f32
++// CHECK:        %[[SEL1:.+]] = arith.select %[[CMPZERO]], %[[CST1]], %[[SEL]]
++// CHECK:       return %[[SEL1]] : f32
+ 
+ // -----
+ 
+diff --git a/mlir/test/mlir-runner/test-expand-math-approx.mlir b/mlir/test/mlir-runner/test-expand-math-approx.mlir
+index d1916c28878b..106b48a2daea 100644
+--- a/mlir/test/mlir-runner/test-expand-math-approx.mlir
++++ b/mlir/test/mlir-runner/test-expand-math-approx.mlir
+@@ -202,6 +202,11 @@ func.func @powf() {
+   %a_p = arith.constant 2.0 : f64
+   call @func_powff64(%a, %a_p) : (f64, f64) -> ()
+ 
++  // CHECK-NEXT: -27
++  %b   = arith.constant -3.0 : f64
++  %b_p = arith.constant 3.0 : f64
++  call @func_powff64(%b, %b_p) : (f64, f64) -> ()
++
+   // CHECK-NEXT: 2.343
+   %c   = arith.constant 2.343 : f64
+   %c_p = arith.constant 1.000 : f64
+-- 
+2.49.0.504.g3bcea36a83-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/cd6e959102888279dc7e75a41ebd75a08ac3f7a5.patch b/llvm_patches/cherry/cd6e959102888279dc7e75a41ebd75a08ac3f7a5.patch
new file mode 100644
index 00000000..47232de5
--- /dev/null
+++ b/llvm_patches/cherry/cd6e959102888279dc7e75a41ebd75a08ac3f7a5.patch
@@ -0,0 +1,44 @@
+From 06595f74b453cdb536d6c3161bac48ac355fb840 Mon Sep 17 00:00:00 2001
+From: Eli Friedman <efriedma@quicinc.com>
+Date: Thu, 27 Mar 2025 17:46:42 -0700
+Subject: [PATCH] Revert "[MC] Explicitly mark MCSymbol for MO_ExternalSymbol"
+ (#133291)
+
+Reverts llvm/llvm-project#108880 .
+
+The patch has no regression test, no description of why the fix is
+necessary, and the code is modifying MC datastructures in a way that's
+forbidden in the AsmPrinter.
+
+Fixes #132055.
+
+patch.cherry: true
+patch.metadata.original_sha: cd6e959102888279dc7e75a41ebd75a08ac3f7a5
+patch.platforms: chromiumos
+patch.version_range.from: 550362
+patch.version_range.until: 570122
+
+---
+ llvm/lib/Target/X86/X86MCInstLower.cpp | 6 +-----
+ 1 file changed, 1 insertion(+), 5 deletions(-)
+
+diff --git a/llvm/lib/Target/X86/X86MCInstLower.cpp b/llvm/lib/Target/X86/X86MCInstLower.cpp
+index 0f8fbf5be1c9..4bb9a269f2d8 100644
+--- a/llvm/lib/Target/X86/X86MCInstLower.cpp
++++ b/llvm/lib/Target/X86/X86MCInstLower.cpp
+@@ -348,12 +348,8 @@ MCOperand X86MCInstLower::LowerMachineOperand(const MachineInstr *MI,
+     return MCOperand::createImm(MO.getImm());
+   case MachineOperand::MO_MachineBasicBlock:
+   case MachineOperand::MO_GlobalAddress:
++  case MachineOperand::MO_ExternalSymbol:
+     return LowerSymbolOperand(MO, GetSymbolFromOperand(MO));
+-  case MachineOperand::MO_ExternalSymbol: {
+-    MCSymbol *Sym = GetSymbolFromOperand(MO);
+-    Sym->setExternal(true);
+-    return LowerSymbolOperand(MO, Sym);
+-  }
+   case MachineOperand::MO_MCSymbol:
+     return LowerSymbolOperand(MO, MO.getMCSymbol());
+   case MachineOperand::MO_JumpTableIndex:
+-- 
+2.49.0.504.g3bcea36a83-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/d29a50f358e71a695b23e456d66ed2924617deb9.patch b/llvm_patches/cherry/d29a50f358e71a695b23e456d66ed2924617deb9.patch
new file mode 100644
index 00000000..f170c26c
--- /dev/null
+++ b/llvm_patches/cherry/d29a50f358e71a695b23e456d66ed2924617deb9.patch
@@ -0,0 +1,43 @@
+From 21c8adbb29cfa0df30c55899a4a56495e2d2e04c Mon Sep 17 00:00:00 2001
+From: Jason Molenda <jmolenda@apple.com>
+Date: Tue, 19 Nov 2024 15:52:42 -0800
+Subject: [PATCH] Revert "[lldb] Allow fetching of RA register when above fault
+ handler (#98566)"
+
+This reverts commit fd424179dcb3417fc0675f77d2bf06c750dd1c33.
+
+This patch has two problems.  First, it is unnecessary, Pavel landed
+a fix a week or so before mine which solves this problem in
+bbd54e08b08f5ccd38c4665178e65c58f7b14459 .  Second, the fix is
+incorrect; for a function above a trap handler, where all registers
+are available, this patch would have lldb fetch the return address
+register from frame 0.  This might be 10 frames up in the stack;
+the frame 0 return address register is incorrect.  The change would
+have been correct a short bit later than this, but Pavel's fix is
+executed earlier in the function and none of this is needed.
+
+patch.cherry: true
+patch.metadata.original_sha: d29a50f358e71a695b23e456d66ed2924617deb9
+patch.platforms: chromiumos
+patch.version_range.from: 542750
+patch.version_range.until: 556712
+
+---
+ lldb/source/Target/RegisterContextUnwind.cpp | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/lldb/source/Target/RegisterContextUnwind.cpp b/lldb/source/Target/RegisterContextUnwind.cpp
+index a61228d092d8..f74f1dc0e1b8 100644
+--- a/lldb/source/Target/RegisterContextUnwind.cpp
++++ b/lldb/source/Target/RegisterContextUnwind.cpp
+@@ -1401,7 +1401,7 @@ RegisterContextUnwind::SavedLocationForRegister(
+       // it's still live in the actual register. Handle this specially.
+ 
+       if (!have_unwindplan_regloc && return_address_reg.IsValid() &&
+-          BehavesLikeZerothFrame()) {
++          IsFrameZero()) {
+         if (return_address_reg.GetAsKind(eRegisterKindLLDB) !=
+             LLDB_INVALID_REGNUM) {
+           lldb_private::UnwindLLDB::RegisterLocation new_regloc;
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/d2aff182d379c9b84cebe0fdf58907f4de768f1e.patch b/llvm_patches/cherry/d2aff182d379c9b84cebe0fdf58907f4de768f1e.patch
new file mode 100644
index 00000000..aa716b20
--- /dev/null
+++ b/llvm_patches/cherry/d2aff182d379c9b84cebe0fdf58907f4de768f1e.patch
@@ -0,0 +1,1050 @@
+From 5d655218ef7b1c79170b11e417ee2e6d7716e25a Mon Sep 17 00:00:00 2001
+From: abhishek-kaushik22 <abhishek.kaushik@intel.com>
+Date: Thu, 7 Nov 2024 17:10:28 +0800
+Subject: [PATCH] Revert "TLS loads opimization (hoist)" (#114740)
+
+This reverts commit c31014322c0b5ae596da129cbb844fb2198b4ef4.
+
+Based on the discussions in #112772, this pass is not needed after the
+introduction of `llvm.threadlocal.address` intrinsic.
+
+Fixes https://github.com/llvm/llvm-project/issues/112771.
+
+patch.cherry: true
+patch.metadata.original_sha: d2aff182d379c9b84cebe0fdf58907f4de768f1e
+patch.platforms: chromiumos
+patch.version_range.from: 455363
+patch.version_range.until: 555342
+
+---
+ llvm/docs/LangRef.rst                         |   5 -
+ llvm/include/llvm/InitializePasses.h          |   1 -
+ llvm/include/llvm/LinkAllPasses.h             |   1 -
+ llvm/include/llvm/Transforms/Scalar.h         |   6 -
+ .../llvm/Transforms/Scalar/TLSVariableHoist.h | 131 --------
+ llvm/lib/CodeGen/TargetPassConfig.cpp         |   3 -
+ llvm/lib/Passes/PassBuilder.cpp               |   1 -
+ llvm/lib/Passes/PassRegistry.def              |   1 -
+ llvm/lib/Transforms/Scalar/CMakeLists.txt     |   1 -
+ llvm/lib/Transforms/Scalar/Scalar.cpp         |   1 -
+ .../Transforms/Scalar/TLSVariableHoist.cpp    | 293 ------------------
+ llvm/test/CodeGen/AArch64/O3-pipeline.ll      |   1 -
+ llvm/test/CodeGen/AMDGPU/llc-pipeline.ll      |   7 -
+ llvm/test/CodeGen/ARM/O3-pipeline.ll          |   1 -
+ llvm/test/CodeGen/LoongArch/opt-pipeline.ll   |   1 -
+ llvm/test/CodeGen/M68k/pipeline.ll            |   1 -
+ llvm/test/CodeGen/PowerPC/O3-pipeline.ll      |   1 -
+ llvm/test/CodeGen/RISCV/O3-pipeline.ll        |   1 -
+ llvm/test/CodeGen/X86/opt-pipeline.ll         |   2 -
+ llvm/test/CodeGen/X86/tls-loads-control.ll    | 248 ---------------
+ llvm/test/CodeGen/X86/tls-loads-control2.ll   |  50 ---
+ llvm/tools/llc/llc.cpp                        |   1 -
+ 22 files changed, 758 deletions(-)
+ delete mode 100644 llvm/include/llvm/Transforms/Scalar/TLSVariableHoist.h
+ delete mode 100644 llvm/lib/Transforms/Scalar/TLSVariableHoist.cpp
+ delete mode 100644 llvm/test/CodeGen/X86/tls-loads-control.ll
+ delete mode 100644 llvm/test/CodeGen/X86/tls-loads-control2.ll
+
+diff --git a/llvm/docs/LangRef.rst b/llvm/docs/LangRef.rst
+index 445980a18e7e..cbf67cd92e9b 100644
+--- a/llvm/docs/LangRef.rst
++++ b/llvm/docs/LangRef.rst
+@@ -2469,11 +2469,6 @@ example:
+     function with a tail call. The prototype of a thunk should not be used for
+     optimization purposes. The caller is expected to cast the thunk prototype to
+     match the thunk target prototype.
+-
+-``"tls-load-hoist"``
+-    This attribute indicates that the function will try to reduce redundant
+-    tls address calculation by hoisting tls variable.
+-
+ ``uwtable[(sync|async)]``
+     This attribute indicates that the ABI being targeted requires that
+     an unwind table entry be produced for this function even if we can
+diff --git a/llvm/include/llvm/InitializePasses.h b/llvm/include/llvm/InitializePasses.h
+index cc5e93c58f56..5003f0df907f 100644
+--- a/llvm/include/llvm/InitializePasses.h
++++ b/llvm/include/llvm/InitializePasses.h
+@@ -301,7 +301,6 @@ void initializeTailDuplicatePass(PassRegistry &);
+ void initializeTargetLibraryInfoWrapperPassPass(PassRegistry &);
+ void initializeTargetPassConfigPass(PassRegistry &);
+ void initializeTargetTransformInfoWrapperPassPass(PassRegistry &);
+-void initializeTLSVariableHoistLegacyPassPass(PassRegistry &);
+ void initializeTwoAddressInstructionLegacyPassPass(PassRegistry &);
+ void initializeTypeBasedAAWrapperPassPass(PassRegistry &);
+ void initializeTypePromotionLegacyPass(PassRegistry &);
+diff --git a/llvm/include/llvm/LinkAllPasses.h b/llvm/include/llvm/LinkAllPasses.h
+index 1da02153d846..9f419c0258ed 100644
+--- a/llvm/include/llvm/LinkAllPasses.h
++++ b/llvm/include/llvm/LinkAllPasses.h
+@@ -112,7 +112,6 @@ struct ForcePassLinking {
+     (void)llvm::createSROAPass();
+     (void)llvm::createSingleLoopExtractorPass();
+     (void)llvm::createTailCallEliminationPass();
+-    (void)llvm::createTLSVariableHoistPass();
+     (void)llvm::createConstantHoistingPass();
+     (void)llvm::createCodeGenPrepareLegacyPass();
+     (void)llvm::createPostInlineEntryExitInstrumenterPass();
+diff --git a/llvm/include/llvm/Transforms/Scalar.h b/llvm/include/llvm/Transforms/Scalar.h
+index 17f4327eb3e1..fc772a7639c4 100644
+--- a/llvm/include/llvm/Transforms/Scalar.h
++++ b/llvm/include/llvm/Transforms/Scalar.h
+@@ -151,12 +151,6 @@ Pass *createMergeICmpsLegacyPass();
+ FunctionPass *createInferAddressSpacesPass(unsigned AddressSpace = ~0u);
+ extern char &InferAddressSpacesID;
+ 
+-//===----------------------------------------------------------------------===//
+-//
+-// TLSVariableHoist - This pass reduce duplicated TLS address call.
+-//
+-FunctionPass *createTLSVariableHoistPass();
+-
+ //===----------------------------------------------------------------------===//
+ //
+ // PartiallyInlineLibCalls - Tries to inline the fast path of library
+diff --git a/llvm/include/llvm/Transforms/Scalar/TLSVariableHoist.h b/llvm/include/llvm/Transforms/Scalar/TLSVariableHoist.h
+deleted file mode 100644
+index 2a1b02b40eeb..000000000000
+--- a/llvm/include/llvm/Transforms/Scalar/TLSVariableHoist.h
++++ /dev/null
+@@ -1,131 +0,0 @@
+-//==- TLSVariableHoist.h ------ Remove Redundant TLS Loads -------*- C++ -*-==//
+-//
+-// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+-// See https://llvm.org/LICENSE.txt for license information.
+-// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+-//
+-//===----------------------------------------------------------------------===//
+-//
+-// This pass identifies/eliminates Redundant TLS Loads if related option is set.
+-// For example:
+-// static __thread int x;
+-// int g();
+-// int f(int c) {
+-//   int *px = &x;
+-//   while (c--)
+-//     *px += g();
+-//   return *px;
+-// }
+-//
+-// will generate Redundant TLS Loads by compiling it with
+-// clang++ -fPIC -ftls-model=global-dynamic -O2 -S
+-//
+-// .LBB0_2:                                # %while.body
+-//                                         # =>This Inner Loop Header: Depth=1
+-//         callq   _Z1gv@PLT
+-//         movl    %eax, %ebp
+-//         leaq    _ZL1x@TLSLD(%rip), %rdi
+-//         callq   __tls_get_addr@PLT
+-//         addl    _ZL1x@DTPOFF(%rax), %ebp
+-//         movl    %ebp, _ZL1x@DTPOFF(%rax)
+-//         addl    $-1, %ebx
+-//         jne     .LBB0_2
+-//         jmp     .LBB0_3
+-// .LBB0_4:                                # %entry.while.end_crit_edge
+-//         leaq    _ZL1x@TLSLD(%rip), %rdi
+-//         callq   __tls_get_addr@PLT
+-//         movl    _ZL1x@DTPOFF(%rax), %ebp
+-//
+-// The Redundant TLS Loads will hurt the performance, especially in loops.
+-// So we try to eliminate/move them if required by customers, let it be:
+-//
+-// # %bb.0:                                # %entry
+-//         ...
+-//         movl    %edi, %ebx
+-//         leaq    _ZL1x@TLSLD(%rip), %rdi
+-//         callq   __tls_get_addr@PLT
+-//         leaq    _ZL1x@DTPOFF(%rax), %r14
+-//         testl   %ebx, %ebx
+-//         je      .LBB0_1
+-// .LBB0_2:                                # %while.body
+-//                                         # =>This Inner Loop Header: Depth=1
+-//         callq   _Z1gv@PLT
+-//         addl    (%r14), %eax
+-//         movl    %eax, (%r14)
+-//         addl    $-1, %ebx
+-//         jne     .LBB0_2
+-//         jmp     .LBB0_3
+-//
+-//===----------------------------------------------------------------------===//
+-
+-#ifndef LLVM_TRANSFORMS_SCALAR_TLSVARIABLEHOIST_H
+-#define LLVM_TRANSFORMS_SCALAR_TLSVARIABLEHOIST_H
+-
+-#include "llvm/ADT/MapVector.h"
+-#include "llvm/ADT/SmallVector.h"
+-#include "llvm/Analysis/LoopInfo.h"
+-#include "llvm/IR/PassManager.h"
+-
+-namespace llvm {
+-
+-class BasicBlock;
+-class DominatorTree;
+-class Function;
+-class GlobalVariable;
+-class Instruction;
+-
+-/// A private "module" namespace for types and utilities used by
+-/// TLSVariableHoist. These are implementation details and should
+-/// not be used by clients.
+-namespace tlshoist {
+-
+-/// Keeps track of the user of a TLS variable and the operand index
+-/// where the variable is used.
+-struct TLSUser {
+-  Instruction *Inst;
+-  unsigned OpndIdx;
+-
+-  TLSUser(Instruction *Inst, unsigned Idx) : Inst(Inst), OpndIdx(Idx) {}
+-};
+-
+-/// Keeps track of a TLS variable candidate and its users.
+-struct TLSCandidate {
+-  SmallVector<TLSUser, 8> Users;
+-
+-  /// Add the user to the use list and update the cost.
+-  void addUser(Instruction *Inst, unsigned Idx) {
+-    Users.push_back(TLSUser(Inst, Idx));
+-  }
+-};
+-
+-} // end namespace tlshoist
+-
+-class TLSVariableHoistPass : public PassInfoMixin<TLSVariableHoistPass> {
+-public:
+-  PreservedAnalyses run(Function &F, FunctionAnalysisManager &AM);
+-
+-  // Glue for old PM.
+-  bool runImpl(Function &F, DominatorTree &DT, LoopInfo &LI);
+-
+-private:
+-  DominatorTree *DT;
+-  LoopInfo *LI;
+-
+-  /// Keeps track of TLS variable candidates found in the function.
+-  using TLSCandMapType = MapVector<GlobalVariable *, tlshoist::TLSCandidate>;
+-  TLSCandMapType TLSCandMap;
+-
+-  void collectTLSCandidates(Function &Fn);
+-  void collectTLSCandidate(Instruction *Inst);
+-  Instruction *getNearestLoopDomInst(BasicBlock *BB, Loop *L);
+-  Instruction *getDomInst(Instruction *I1, Instruction *I2);
+-  BasicBlock::iterator findInsertPos(Function &Fn, GlobalVariable *GV,
+-                                     BasicBlock *&PosBB);
+-  Instruction *genBitCastInst(Function &Fn, GlobalVariable *GV);
+-  bool tryReplaceTLSCandidates(Function &Fn);
+-  bool tryReplaceTLSCandidate(Function &Fn, GlobalVariable *GV);
+-};
+-
+-} // end namespace llvm
+-
+-#endif // LLVM_TRANSFORMS_SCALAR_TLSVARIABLEHOIST_H
+diff --git a/llvm/lib/CodeGen/TargetPassConfig.cpp b/llvm/lib/CodeGen/TargetPassConfig.cpp
+index 1d52ebe6717f..035246a539e9 100644
+--- a/llvm/lib/CodeGen/TargetPassConfig.cpp
++++ b/llvm/lib/CodeGen/TargetPassConfig.cpp
+@@ -881,9 +881,6 @@ void TargetPassConfig::addIRPasses() {
+   if (!DisableExpandReductions)
+     addPass(createExpandReductionsPass());
+ 
+-  if (getOptLevel() != CodeGenOptLevel::None)
+-    addPass(createTLSVariableHoistPass());
+-
+   // Convert conditional moves to conditional jumps when profitable.
+   if (getOptLevel() != CodeGenOptLevel::None && !DisableSelectOptimize)
+     addPass(createSelectOptimizePass());
+diff --git a/llvm/lib/Passes/PassBuilder.cpp b/llvm/lib/Passes/PassBuilder.cpp
+index 17eed97fd950..b4a73462c1e0 100644
+--- a/llvm/lib/Passes/PassBuilder.cpp
++++ b/llvm/lib/Passes/PassBuilder.cpp
+@@ -281,7 +281,6 @@
+ #include "llvm/Transforms/Scalar/SpeculativeExecution.h"
+ #include "llvm/Transforms/Scalar/StraightLineStrengthReduce.h"
+ #include "llvm/Transforms/Scalar/StructurizeCFG.h"
+-#include "llvm/Transforms/Scalar/TLSVariableHoist.h"
+ #include "llvm/Transforms/Scalar/TailRecursionElimination.h"
+ #include "llvm/Transforms/Scalar/WarnMissedTransforms.h"
+ #include "llvm/Transforms/Utils/AddDiscriminators.h"
+diff --git a/llvm/lib/Passes/PassRegistry.def b/llvm/lib/Passes/PassRegistry.def
+index d6067089c6b5..98f69c97b764 100644
+--- a/llvm/lib/Passes/PassRegistry.def
++++ b/llvm/lib/Passes/PassRegistry.def
+@@ -466,7 +466,6 @@ FUNCTION_PASS("slsr", StraightLineStrengthReducePass())
+ FUNCTION_PASS("stack-protector", StackProtectorPass(TM))
+ FUNCTION_PASS("strip-gc-relocates", StripGCRelocates())
+ FUNCTION_PASS("tailcallelim", TailCallElimPass())
+-FUNCTION_PASS("tlshoist", TLSVariableHoistPass())
+ FUNCTION_PASS("transform-warning", WarnMissedTransformationsPass())
+ FUNCTION_PASS("trigger-crash-function", TriggerCrashFunctionPass())
+ FUNCTION_PASS("trigger-verifier-error", TriggerVerifierErrorPass())
+diff --git a/llvm/lib/Transforms/Scalar/CMakeLists.txt b/llvm/lib/Transforms/Scalar/CMakeLists.txt
+index 939a14572395..84a5b02043d0 100644
+--- a/llvm/lib/Transforms/Scalar/CMakeLists.txt
++++ b/llvm/lib/Transforms/Scalar/CMakeLists.txt
+@@ -78,7 +78,6 @@ add_llvm_component_library(LLVMScalarOpts
+   StraightLineStrengthReduce.cpp
+   StructurizeCFG.cpp
+   TailRecursionElimination.cpp
+-  TLSVariableHoist.cpp
+   WarnMissedTransforms.cpp
+ 
+   ADDITIONAL_HEADER_DIRS
+diff --git a/llvm/lib/Transforms/Scalar/Scalar.cpp b/llvm/lib/Transforms/Scalar/Scalar.cpp
+index 7aeee1d31f7e..6bbb870ab3f9 100644
+--- a/llvm/lib/Transforms/Scalar/Scalar.cpp
++++ b/llvm/lib/Transforms/Scalar/Scalar.cpp
+@@ -43,7 +43,6 @@ void llvm::initializeScalarOpts(PassRegistry &Registry) {
+   initializeStructurizeCFGLegacyPassPass(Registry);
+   initializeSinkingLegacyPassPass(Registry);
+   initializeTailCallElimPass(Registry);
+-  initializeTLSVariableHoistLegacyPassPass(Registry);
+   initializeSeparateConstOffsetFromGEPLegacyPassPass(Registry);
+   initializeSpeculativeExecutionLegacyPassPass(Registry);
+   initializeStraightLineStrengthReduceLegacyPassPass(Registry);
+diff --git a/llvm/lib/Transforms/Scalar/TLSVariableHoist.cpp b/llvm/lib/Transforms/Scalar/TLSVariableHoist.cpp
+deleted file mode 100644
+index 58ea5b68d548..000000000000
+--- a/llvm/lib/Transforms/Scalar/TLSVariableHoist.cpp
++++ /dev/null
+@@ -1,293 +0,0 @@
+-//===- TLSVariableHoist.cpp -------- Remove Redundant TLS Loads ---------===//
+-//
+-// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+-// See https://llvm.org/LICENSE.txt for license information.
+-// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+-//
+-//===----------------------------------------------------------------------===//
+-//
+-// This pass identifies/eliminate Redundant TLS Loads if related option is set.
+-// The example: Please refer to the comment at the head of TLSVariableHoist.h.
+-//
+-//===----------------------------------------------------------------------===//
+-
+-#include "llvm/ADT/SmallVector.h"
+-#include "llvm/IR/BasicBlock.h"
+-#include "llvm/IR/Dominators.h"
+-#include "llvm/IR/Function.h"
+-#include "llvm/IR/InstrTypes.h"
+-#include "llvm/IR/Instruction.h"
+-#include "llvm/IR/Instructions.h"
+-#include "llvm/IR/IntrinsicInst.h"
+-#include "llvm/IR/Module.h"
+-#include "llvm/IR/Value.h"
+-#include "llvm/InitializePasses.h"
+-#include "llvm/Pass.h"
+-#include "llvm/Support/Casting.h"
+-#include "llvm/Support/Debug.h"
+-#include "llvm/Support/raw_ostream.h"
+-#include "llvm/Transforms/Scalar.h"
+-#include "llvm/Transforms/Scalar/TLSVariableHoist.h"
+-#include <algorithm>
+-#include <cassert>
+-#include <cstdint>
+-#include <iterator>
+-#include <utility>
+-
+-using namespace llvm;
+-using namespace tlshoist;
+-
+-#define DEBUG_TYPE "tlshoist"
+-
+-static cl::opt<bool> TLSLoadHoist(
+-    "tls-load-hoist", cl::init(false), cl::Hidden,
+-    cl::desc("hoist the TLS loads in PIC model to eliminate redundant "
+-             "TLS address calculation."));
+-
+-namespace {
+-
+-/// The TLS Variable hoist pass.
+-class TLSVariableHoistLegacyPass : public FunctionPass {
+-public:
+-  static char ID; // Pass identification, replacement for typeid
+-
+-  TLSVariableHoistLegacyPass() : FunctionPass(ID) {
+-    initializeTLSVariableHoistLegacyPassPass(*PassRegistry::getPassRegistry());
+-  }
+-
+-  bool runOnFunction(Function &Fn) override;
+-
+-  StringRef getPassName() const override { return "TLS Variable Hoist"; }
+-
+-  void getAnalysisUsage(AnalysisUsage &AU) const override {
+-    AU.setPreservesCFG();
+-    AU.addRequired<DominatorTreeWrapperPass>();
+-    AU.addRequired<LoopInfoWrapperPass>();
+-  }
+-
+-private:
+-  TLSVariableHoistPass Impl;
+-};
+-
+-} // end anonymous namespace
+-
+-char TLSVariableHoistLegacyPass::ID = 0;
+-
+-INITIALIZE_PASS_BEGIN(TLSVariableHoistLegacyPass, "tlshoist",
+-                      "TLS Variable Hoist", false, false)
+-INITIALIZE_PASS_DEPENDENCY(DominatorTreeWrapperPass)
+-INITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)
+-INITIALIZE_PASS_END(TLSVariableHoistLegacyPass, "tlshoist",
+-                    "TLS Variable Hoist", false, false)
+-
+-FunctionPass *llvm::createTLSVariableHoistPass() {
+-  return new TLSVariableHoistLegacyPass();
+-}
+-
+-/// Perform the TLS Variable Hoist optimization for the given function.
+-bool TLSVariableHoistLegacyPass::runOnFunction(Function &Fn) {
+-  if (skipFunction(Fn))
+-    return false;
+-
+-  LLVM_DEBUG(dbgs() << "********** Begin TLS Variable Hoist **********\n");
+-  LLVM_DEBUG(dbgs() << "********** Function: " << Fn.getName() << '\n');
+-
+-  bool MadeChange =
+-      Impl.runImpl(Fn, getAnalysis<DominatorTreeWrapperPass>().getDomTree(),
+-                   getAnalysis<LoopInfoWrapperPass>().getLoopInfo());
+-
+-  if (MadeChange) {
+-    LLVM_DEBUG(dbgs() << "********** Function after TLS Variable Hoist: "
+-                      << Fn.getName() << '\n');
+-    LLVM_DEBUG(dbgs() << Fn);
+-  }
+-  LLVM_DEBUG(dbgs() << "********** End TLS Variable Hoist **********\n");
+-
+-  return MadeChange;
+-}
+-
+-void TLSVariableHoistPass::collectTLSCandidate(Instruction *Inst) {
+-  // Skip all cast instructions. They are visited indirectly later on.
+-  if (Inst->isCast())
+-    return;
+-
+-  // Scan all operands.
+-  for (unsigned Idx = 0, E = Inst->getNumOperands(); Idx != E; ++Idx) {
+-    auto *GV = dyn_cast<GlobalVariable>(Inst->getOperand(Idx));
+-    if (!GV || !GV->isThreadLocal())
+-      continue;
+-
+-    // Add Candidate to TLSCandMap (GV --> Candidate).
+-    TLSCandMap[GV].addUser(Inst, Idx);
+-  }
+-}
+-
+-void TLSVariableHoistPass::collectTLSCandidates(Function &Fn) {
+-  // First, quickly check if there is TLS Variable.
+-  Module *M = Fn.getParent();
+-
+-  bool HasTLS = llvm::any_of(
+-      M->globals(), [](GlobalVariable &GV) { return GV.isThreadLocal(); });
+-
+-  // If non, directly return.
+-  if (!HasTLS)
+-    return;
+-
+-  TLSCandMap.clear();
+-
+-  // Then, collect TLS Variable info.
+-  for (BasicBlock &BB : Fn) {
+-    // Ignore unreachable basic blocks.
+-    if (!DT->isReachableFromEntry(&BB))
+-      continue;
+-
+-    for (Instruction &Inst : BB)
+-      collectTLSCandidate(&Inst);
+-  }
+-}
+-
+-static bool oneUseOutsideLoop(tlshoist::TLSCandidate &Cand, LoopInfo *LI) {
+-  if (Cand.Users.size() != 1)
+-    return false;
+-
+-  BasicBlock *BB = Cand.Users[0].Inst->getParent();
+-  if (LI->getLoopFor(BB))
+-    return false;
+-
+-  return true;
+-}
+-
+-Instruction *TLSVariableHoistPass::getNearestLoopDomInst(BasicBlock *BB,
+-                                                         Loop *L) {
+-  assert(L && "Unexcepted Loop status!");
+-
+-  // Get the outermost loop.
+-  while (Loop *Parent = L->getParentLoop())
+-    L = Parent;
+-
+-  BasicBlock *PreHeader = L->getLoopPreheader();
+-
+-  // There is unique predecessor outside the loop.
+-  if (PreHeader)
+-    return PreHeader->getTerminator();
+-
+-  BasicBlock *Header = L->getHeader();
+-  BasicBlock *Dom = Header;
+-  for (BasicBlock *PredBB : predecessors(Header))
+-    Dom = DT->findNearestCommonDominator(Dom, PredBB);
+-
+-  assert(Dom && "Not find dominator BB!");
+-  Instruction *Term = Dom->getTerminator();
+-
+-  return Term;
+-}
+-
+-Instruction *TLSVariableHoistPass::getDomInst(Instruction *I1,
+-                                              Instruction *I2) {
+-  if (!I1)
+-    return I2;
+-  return DT->findNearestCommonDominator(I1, I2);
+-}
+-
+-BasicBlock::iterator TLSVariableHoistPass::findInsertPos(Function &Fn,
+-                                                         GlobalVariable *GV,
+-                                                         BasicBlock *&PosBB) {
+-  tlshoist::TLSCandidate &Cand = TLSCandMap[GV];
+-
+-  // We should hoist the TLS use out of loop, so choose its nearest instruction
+-  // which dominate the loop and the outside loops (if exist).
+-  Instruction *LastPos = nullptr;
+-  for (auto &User : Cand.Users) {
+-    BasicBlock *BB = User.Inst->getParent();
+-    Instruction *Pos = User.Inst;
+-    if (Loop *L = LI->getLoopFor(BB)) {
+-      Pos = getNearestLoopDomInst(BB, L);
+-      assert(Pos && "Not find insert position out of loop!");
+-    }
+-    Pos = getDomInst(LastPos, Pos);
+-    LastPos = Pos;
+-  }
+-
+-  assert(LastPos && "Unexpected insert position!");
+-  BasicBlock *Parent = LastPos->getParent();
+-  PosBB = Parent;
+-  return LastPos->getIterator();
+-}
+-
+-// Generate a bitcast (no type change) to replace the uses of TLS Candidate.
+-Instruction *TLSVariableHoistPass::genBitCastInst(Function &Fn,
+-                                                  GlobalVariable *GV) {
+-  BasicBlock *PosBB = &Fn.getEntryBlock();
+-  BasicBlock::iterator Iter = findInsertPos(Fn, GV, PosBB);
+-  Type *Ty = GV->getType();
+-  auto *CastInst = new BitCastInst(GV, Ty, "tls_bitcast");
+-  CastInst->insertInto(PosBB, Iter);
+-  return CastInst;
+-}
+-
+-bool TLSVariableHoistPass::tryReplaceTLSCandidate(Function &Fn,
+-                                                  GlobalVariable *GV) {
+-
+-  tlshoist::TLSCandidate &Cand = TLSCandMap[GV];
+-
+-  // If only used 1 time and not in loops, we no need to replace it.
+-  if (oneUseOutsideLoop(Cand, LI))
+-    return false;
+-
+-  // Generate a bitcast (no type change)
+-  auto *CastInst = genBitCastInst(Fn, GV);
+-
+-  // to replace the uses of TLS Candidate
+-  for (auto &User : Cand.Users)
+-    User.Inst->setOperand(User.OpndIdx, CastInst);
+-
+-  return true;
+-}
+-
+-bool TLSVariableHoistPass::tryReplaceTLSCandidates(Function &Fn) {
+-  if (TLSCandMap.empty())
+-    return false;
+-
+-  bool Replaced = false;
+-  for (auto &GV2Cand : TLSCandMap) {
+-    GlobalVariable *GV = GV2Cand.first;
+-    Replaced |= tryReplaceTLSCandidate(Fn, GV);
+-  }
+-
+-  return Replaced;
+-}
+-
+-/// Optimize expensive TLS variables in the given function.
+-bool TLSVariableHoistPass::runImpl(Function &Fn, DominatorTree &DT,
+-                                   LoopInfo &LI) {
+-  if (Fn.hasOptNone())
+-    return false;
+-
+-  if (!TLSLoadHoist && !Fn.getAttributes().hasFnAttr("tls-load-hoist"))
+-    return false;
+-
+-  this->LI = &LI;
+-  this->DT = &DT;
+-  assert(this->LI && this->DT && "Unexcepted requirement!");
+-
+-  // Collect all TLS variable candidates.
+-  collectTLSCandidates(Fn);
+-
+-  bool MadeChange = tryReplaceTLSCandidates(Fn);
+-
+-  return MadeChange;
+-}
+-
+-PreservedAnalyses TLSVariableHoistPass::run(Function &F,
+-                                            FunctionAnalysisManager &AM) {
+-
+-  auto &LI = AM.getResult<LoopAnalysis>(F);
+-  auto &DT = AM.getResult<DominatorTreeAnalysis>(F);
+-
+-  if (!runImpl(F, DT, LI))
+-    return PreservedAnalyses::all();
+-
+-  PreservedAnalyses PA;
+-  PA.preserveSet<CFGAnalyses>();
+-  return PA;
+-}
+diff --git a/llvm/test/CodeGen/AArch64/O3-pipeline.ll b/llvm/test/CodeGen/AArch64/O3-pipeline.ll
+index 3465b717261c..abdedd58294f 100644
+--- a/llvm/test/CodeGen/AArch64/O3-pipeline.ll
++++ b/llvm/test/CodeGen/AArch64/O3-pipeline.ll
+@@ -64,7 +64,6 @@
+ ; CHECK-NEXT:       Scalarize Masked Memory Intrinsics
+ ; CHECK-NEXT:       Expand reduction intrinsics
+ ; CHECK-NEXT:       Natural Loop Information
+-; CHECK-NEXT:       TLS Variable Hoist
+ ; CHECK-NEXT:       Post-Dominator Tree Construction
+ ; CHECK-NEXT:       Branch Probability Analysis
+ ; CHECK-NEXT:       Block Frequency Analysis
+diff --git a/llvm/test/CodeGen/AMDGPU/llc-pipeline.ll b/llvm/test/CodeGen/AMDGPU/llc-pipeline.ll
+index 1b1ea52520c0..a445fb9d3ca2 100644
+--- a/llvm/test/CodeGen/AMDGPU/llc-pipeline.ll
++++ b/llvm/test/CodeGen/AMDGPU/llc-pipeline.ll
+@@ -224,8 +224,6 @@
+ ; GCN-O1-NEXT:      Instrument function entry/exit with calls to e.g. mcount() (post inlining)
+ ; GCN-O1-NEXT:      Scalarize Masked Memory Intrinsics
+ ; GCN-O1-NEXT:      Expand reduction intrinsics
+-; GCN-O1-NEXT:      Natural Loop Information
+-; GCN-O1-NEXT:      TLS Variable Hoist
+ ; GCN-O1-NEXT:    CallGraph Construction
+ ; GCN-O1-NEXT:    Call Graph SCC Pass Manager
+ ; GCN-O1-NEXT:      AMDGPU Annotate Kernel Features
+@@ -512,8 +510,6 @@
+ ; GCN-O1-OPTS-NEXT:      Instrument function entry/exit with calls to e.g. mcount() (post inlining)
+ ; GCN-O1-OPTS-NEXT:      Scalarize Masked Memory Intrinsics
+ ; GCN-O1-OPTS-NEXT:      Expand reduction intrinsics
+-; GCN-O1-OPTS-NEXT:      Natural Loop Information
+-; GCN-O1-OPTS-NEXT:      TLS Variable Hoist
+ ; GCN-O1-OPTS-NEXT:      Early CSE
+ ; GCN-O1-OPTS-NEXT:    CallGraph Construction
+ ; GCN-O1-OPTS-NEXT:    Call Graph SCC Pass Manager
+@@ -819,8 +815,6 @@
+ ; GCN-O2-NEXT:      Instrument function entry/exit with calls to e.g. mcount() (post inlining)
+ ; GCN-O2-NEXT:      Scalarize Masked Memory Intrinsics
+ ; GCN-O2-NEXT:      Expand reduction intrinsics
+-; GCN-O2-NEXT:      Natural Loop Information
+-; GCN-O2-NEXT:      TLS Variable Hoist
+ ; GCN-O2-NEXT:      Early CSE
+ ; GCN-O2-NEXT:    CallGraph Construction
+ ; GCN-O2-NEXT:    Call Graph SCC Pass Manager
+@@ -1135,7 +1129,6 @@
+ ; GCN-O3-NEXT:      Scalarize Masked Memory Intrinsics
+ ; GCN-O3-NEXT:      Expand reduction intrinsics
+ ; GCN-O3-NEXT:      Natural Loop Information
+-; GCN-O3-NEXT:      TLS Variable Hoist
+ ; GCN-O3-NEXT:      Basic Alias Analysis (stateless AA impl)
+ ; GCN-O3-NEXT:      Function Alias Analysis Results
+ ; GCN-O3-NEXT:      Memory Dependence Analysis
+diff --git a/llvm/test/CodeGen/ARM/O3-pipeline.ll b/llvm/test/CodeGen/ARM/O3-pipeline.ll
+index 9b983d96f793..0be61d7da816 100644
+--- a/llvm/test/CodeGen/ARM/O3-pipeline.ll
++++ b/llvm/test/CodeGen/ARM/O3-pipeline.ll
+@@ -42,7 +42,6 @@
+ ; CHECK-NEXT:      Scalarize Masked Memory Intrinsics
+ ; CHECK-NEXT:      Expand reduction intrinsics
+ ; CHECK-NEXT:      Natural Loop Information
+-; CHECK-NEXT:      TLS Variable Hoist
+ ; CHECK-NEXT:      Scalar Evolution Analysis
+ ; CHECK-NEXT:      Basic Alias Analysis (stateless AA impl)
+ ; CHECK-NEXT:      Function Alias Analysis Results
+diff --git a/llvm/test/CodeGen/LoongArch/opt-pipeline.ll b/llvm/test/CodeGen/LoongArch/opt-pipeline.ll
+index 50f154663177..8409f38fc313 100644
+--- a/llvm/test/CodeGen/LoongArch/opt-pipeline.ll
++++ b/llvm/test/CodeGen/LoongArch/opt-pipeline.ll
+@@ -65,7 +65,6 @@
+ ; LAXX-NEXT:       Scalarize Masked Memory Intrinsics
+ ; LAXX-NEXT:       Expand reduction intrinsics
+ ; LAXX-NEXT:       Natural Loop Information
+-; LAXX-NEXT:       TLS Variable Hoist
+ ; LAXX-NEXT:       Type Promotion
+ ; LAXX-NEXT:       CodeGen Prepare
+ ; LAXX-NEXT:       Dominator Tree Construction
+diff --git a/llvm/test/CodeGen/M68k/pipeline.ll b/llvm/test/CodeGen/M68k/pipeline.ll
+index 76d72f5a5042..eecf4e3c036b 100644
+--- a/llvm/test/CodeGen/M68k/pipeline.ll
++++ b/llvm/test/CodeGen/M68k/pipeline.ll
+@@ -36,7 +36,6 @@
+ ; CHECK-NEXT:      Scalarize Masked Memory Intrinsics
+ ; CHECK-NEXT:      Expand reduction intrinsics
+ ; CHECK-NEXT:      Natural Loop Information
+-; CHECK-NEXT:      TLS Variable Hoist
+ ; CHECK-NEXT:      CodeGen Prepare
+ ; CHECK-NEXT:      Dominator Tree Construction
+ ; CHECK-NEXT:      Exception handling preparation
+diff --git a/llvm/test/CodeGen/PowerPC/O3-pipeline.ll b/llvm/test/CodeGen/PowerPC/O3-pipeline.ll
+index 60d42704ca79..62d63ded382a 100644
+--- a/llvm/test/CodeGen/PowerPC/O3-pipeline.ll
++++ b/llvm/test/CodeGen/PowerPC/O3-pipeline.ll
+@@ -66,7 +66,6 @@
+ ; CHECK-NEXT:       Scalarize Masked Memory Intrinsics
+ ; CHECK-NEXT:       Expand reduction intrinsics
+ ; CHECK-NEXT:       Natural Loop Information
+-; CHECK-NEXT:       TLS Variable Hoist
+ ; CHECK-NEXT:       CodeGen Prepare
+ ; CHECK-NEXT:       Dominator Tree Construction
+ ; CHECK-NEXT:       Exception handling preparation
+diff --git a/llvm/test/CodeGen/RISCV/O3-pipeline.ll b/llvm/test/CodeGen/RISCV/O3-pipeline.ll
+index 44c270fdc3c2..64833b5b5f17 100644
+--- a/llvm/test/CodeGen/RISCV/O3-pipeline.ll
++++ b/llvm/test/CodeGen/RISCV/O3-pipeline.ll
+@@ -67,7 +67,6 @@
+ ; CHECK-NEXT:       Scalarize Masked Memory Intrinsics
+ ; CHECK-NEXT:       Expand reduction intrinsics
+ ; CHECK-NEXT:       Natural Loop Information
+-; CHECK-NEXT:       TLS Variable Hoist
+ ; CHECK-NEXT:       Type Promotion
+ ; CHECK-NEXT:       CodeGen Prepare
+ ; CHECK-NEXT:       Dominator Tree Construction
+diff --git a/llvm/test/CodeGen/X86/opt-pipeline.ll b/llvm/test/CodeGen/X86/opt-pipeline.ll
+index 12c16a03b134..80d96091ce00 100644
+--- a/llvm/test/CodeGen/X86/opt-pipeline.ll
++++ b/llvm/test/CodeGen/X86/opt-pipeline.ll
+@@ -62,8 +62,6 @@
+ ; CHECK-NEXT:       Instrument function entry/exit with calls to e.g. mcount() (post inlining)
+ ; CHECK-NEXT:       Scalarize Masked Memory Intrinsics
+ ; CHECK-NEXT:       Expand reduction intrinsics
+-; CHECK-NEXT:       Natural Loop Information
+-; CHECK-NEXT:       TLS Variable Hoist
+ ; CHECK-NEXT:       Interleaved Access Pass
+ ; CHECK-NEXT:       X86 Partial Reduction
+ ; CHECK-NEXT:       Expand indirectbr instructions
+diff --git a/llvm/test/CodeGen/X86/tls-loads-control.ll b/llvm/test/CodeGen/X86/tls-loads-control.ll
+deleted file mode 100644
+index 8d9bf61c53fa..000000000000
+--- a/llvm/test/CodeGen/X86/tls-loads-control.ll
++++ /dev/null
+@@ -1,248 +0,0 @@
+-; RUN: llc -mtriple=x86_64-unknown-unknown -O2 --relocation-model=pic --tls-load-hoist=true --stop-after=tlshoist -o - %s | FileCheck %s
+-; RUN: llc -mtriple=x86_64-unknown-unknown -O2 --relocation-model=pic --stop-after=tlshoist -o - %s | FileCheck %s
+-
+-; This test come from compiling clang/test/CodeGen/intel/tls_loads.cpp with:
+-; (clang tls_loads.cpp -fPIC -ftls-model=global-dynamic -O2 -S -emit-llvm)
+-
+-; // Variable declaration and definition:
+-; thread_local int thl_x;
+-; thread_local int thl_x2;
+-;
+-; struct SS {
+-;   char thl_c;
+-;   int num;
+-; };
+-;
+-; int gfunc();
+-; int gfunc2(int);
+-
+-; // First function (@_Z2f1i):
+-; int f1(int c) {
+-;   while (c)
+-;     c++;
+-;
+-;   int *px = &thl_x;
+-;   c -= gfunc();
+-;
+-;   while(c++) {
+-;     c = gfunc();
+-;     while (c--)
+-;       *px += gfunc2(thl_x2);
+-;   }
+-;   return *px;
+-; }
+-
+-$_ZTW5thl_x = comdat any
+-
+-$_ZTW6thl_x2 = comdat any
+-
+-@thl_x = thread_local global i32 0, align 4
+-@thl_x2 = thread_local global i32 0, align 4
+-@_ZZ2f2iE2st.0 = internal thread_local unnamed_addr global i8 0, align 4
+-@_ZZ2f2iE2st.1 = internal thread_local unnamed_addr global i32 0, align 4
+-
+-; Function Attrs: mustprogress uwtable
+-define noundef i32 @_Z2f1i(i32 noundef %c) local_unnamed_addr #0 {
+-; CHECK-LABEL: _Z2f1i
+-; CHECK:      entry:
+-; CHECK-NEXT:   %call = tail call noundef i32 @_Z5gfuncv()
+-; CHECK-NEXT:   %phi.cmp = icmp eq i32 %call, 0
+-; CHECK-NEXT:   %tls_bitcast1 = bitcast ptr @thl_x to ptr
+-; CHECK-NEXT:   br i1 %phi.cmp, label %while.end11, label %while.body4.preheader
+-
+-; CHECK:      while.body4.preheader:
+-; CHECK-NEXT:   %tls_bitcast = bitcast ptr @thl_x2 to ptr
+-; CHECK-NEXT:   br label %while.body4
+-
+-; CHECK:      while.body4:
+-; CHECK-NEXT:   %call5 = tail call noundef i32 @_Z5gfuncv()
+-; CHECK-NEXT:   %tobool7.not18 = icmp eq i32 %call5, 0
+-; CHECK-NEXT:   br i1 %tobool7.not18, label %while.body4.backedge, label %while.body8.preheader
+-
+-; CHECK:      while.body8.preheader:
+-; CHECK-NEXT:   br label %while.body8
+-
+-; CHECK:      while.body4.backedge.loopexit:
+-; CHECK-NEXT:   br label %while.body4.backedge
+-
+-; CHECK:      while.body4.backedge:
+-; CHECK-NEXT:   br label %while.body4, !llvm.loop !4
+-
+-; CHECK:      while.body8:
+-; CHECK-NEXT:   %c.addr.219 = phi i32 [ %dec, %while.body8 ], [ %call5, %while.body8.preheader ]
+-; CHECK-NEXT:   %dec = add i32 %c.addr.219, -1
+-; CHECK-NEXT:   %0 = load i32, ptr %tls_bitcast, align 4
+-; CHECK-NEXT:   %call9 = tail call noundef i32 @_Z6gfunc2i(i32 noundef %0)
+-; CHECK-NEXT:   %1 = load i32, ptr %tls_bitcast1, align 4
+-; CHECK-NEXT:   %add = add nsw i32 %1, %call9
+-; CHECK-NEXT:   store i32 %add, ptr %tls_bitcast1, align 4
+-; CHECK-NEXT:   %tobool7.not = icmp eq i32 %dec, 0
+-; CHECK-NEXT:   br i1 %tobool7.not, label %while.body4.backedge.loopexit, label %while.body8, !llvm.loop !4
+-
+-; CHECK:      while.end11:
+-; CHECK-NEXT:   %2 = load i32, ptr %tls_bitcast1, align 4
+-; CHECK-NEXT:   ret i32 %2
+-
+-entry:
+-  %call = tail call noundef i32 @_Z5gfuncv()
+-  %phi.cmp = icmp eq i32 %call, 0
+-  br i1 %phi.cmp, label %while.end11, label %while.body4
+-
+-while.body4:                                      ; preds = %entry, %while.body4.backedge
+-  %call5 = tail call noundef i32 @_Z5gfuncv()
+-  %tobool7.not18 = icmp eq i32 %call5, 0
+-  br i1 %tobool7.not18, label %while.body4.backedge, label %while.body8
+-
+-while.body4.backedge:                             ; preds = %while.body8, %while.body4
+-  br label %while.body4, !llvm.loop !4
+-
+-while.body8:                                      ; preds = %while.body4, %while.body8
+-  %c.addr.219 = phi i32 [ %dec, %while.body8 ], [ %call5, %while.body4 ]
+-  %dec = add nsw i32 %c.addr.219, -1
+-  %0 = load i32, ptr @thl_x2, align 4
+-  %call9 = tail call noundef i32 @_Z6gfunc2i(i32 noundef %0)
+-  %1 = load i32, ptr @thl_x, align 4
+-  %add = add nsw i32 %1, %call9
+-  store i32 %add, ptr @thl_x, align 4
+-  %tobool7.not = icmp eq i32 %dec, 0
+-  br i1 %tobool7.not, label %while.body4.backedge, label %while.body8, !llvm.loop !4
+-
+-while.end11:                                      ; preds = %entry
+-  %2 = load i32, ptr @thl_x, align 4
+-  ret i32 %2
+-}
+-
+-; // Sencond function (@_Z2f2i):
+-; int f2(int c) {
+-;   thread_local struct SS st;
+-;   c += gfunc();
+-;   while (c--) {
+-;     thl_x += gfunc();
+-;     st.thl_c += (char)gfunc();
+-;     st.num += gfunc();
+-;   }
+-;   return thl_x;
+-; }
+-declare noundef i32 @_Z5gfuncv() local_unnamed_addr #1
+-
+-declare noundef i32 @_Z6gfunc2i(i32 noundef) local_unnamed_addr #1
+-
+-; Function Attrs: mustprogress uwtable
+-define noundef i32 @_Z2f2i(i32 noundef %c) local_unnamed_addr #0 {
+-; CHECK-LABEL: _Z2f2i
+-; CHECK:      entry:
+-; CHECK-NEXT:   %call = tail call noundef i32 @_Z5gfuncv()
+-; CHECK-NEXT:   %add = add nsw i32 %call, %c
+-; CHECK-NEXT:   %tobool.not12 = icmp eq i32 %add, 0
+-; CHECK-NEXT:   %tls_bitcast = bitcast ptr @thl_x to ptr
+-; CHECK-NEXT:   br i1 %tobool.not12, label %while.end, label %while.body.preheader
+-
+-; CHECK:      while.body.preheader:
+-; CHECK-NEXT:   %tls_bitcast1 = bitcast ptr @_ZZ2f2iE2st.0 to ptr
+-; CHECK-NEXT:   %tls_bitcast2 = bitcast ptr @_ZZ2f2iE2st.1 to ptr
+-; CHECK-NEXT:   br label %while.body
+-
+-; CHECK:      while.body:
+-; CHECK-NEXT:   %c.addr.013 = phi i32 [ %dec, %while.body ], [ %add, %while.body.preheader ]
+-; CHECK-NEXT:   %dec = add i32 %c.addr.013, -1
+-; CHECK-NEXT:   %call1 = tail call noundef i32 @_Z5gfuncv()
+-; CHECK-NEXT:   %0 = load i32, ptr %tls_bitcast, align 4
+-; CHECK-NEXT:   %add2 = add nsw i32 %0, %call1
+-; CHECK-NEXT:   store i32 %add2, ptr %tls_bitcast, align 4
+-; CHECK-NEXT:   %call3 = tail call noundef i32 @_Z5gfuncv()
+-; CHECK-NEXT:   %1 = load i8, ptr %tls_bitcast1, align 4
+-; CHECK-NEXT:   %2 = trunc i32 %call3 to i8
+-; CHECK-NEXT:   %conv7 = add i8 %1, %2
+-; CHECK-NEXT:   store i8 %conv7, ptr %tls_bitcast1, align 4
+-; CHECK-NEXT:   %call8 = tail call noundef i32 @_Z5gfuncv()
+-; CHECK-NEXT:   %3 = load i32, ptr %tls_bitcast2, align 4
+-; CHECK-NEXT:   %add9 = add nsw i32 %3, %call8
+-; CHECK-NEXT:   store i32 %add9, ptr %tls_bitcast2, align 4
+-; CHECK-NEXT:   %tobool.not = icmp eq i32 %dec, 0
+-; CHECK-NEXT:   br i1 %tobool.not, label %while.end.loopexit, label %while.body
+-
+-; CHECK:      while.end.loopexit:
+-; CHECK-NEXT:   br label %while.end
+-
+-; CHECK:      while.end:
+-; CHECK-NEXT:   %4 = load i32, ptr %tls_bitcast, align 4
+-; CHECK-NEXT:   ret i32 %4
+-entry:
+-  %call = tail call noundef i32 @_Z5gfuncv()
+-  %add = add nsw i32 %call, %c
+-  %tobool.not12 = icmp eq i32 %add, 0
+-  br i1 %tobool.not12, label %while.end, label %while.body
+-
+-while.body:                                       ; preds = %entry, %while.body
+-  %c.addr.013 = phi i32 [ %dec, %while.body ], [ %add, %entry ]
+-  %dec = add nsw i32 %c.addr.013, -1
+-  %call1 = tail call noundef i32 @_Z5gfuncv()
+-  %0 = load i32, ptr @thl_x, align 4
+-  %add2 = add nsw i32 %0, %call1
+-  store i32 %add2, ptr @thl_x, align 4
+-  %call3 = tail call noundef i32 @_Z5gfuncv()
+-  %1 = load i8, ptr @_ZZ2f2iE2st.0, align 4
+-  %2 = trunc i32 %call3 to i8
+-  %conv7 = add i8 %1, %2
+-  store i8 %conv7, ptr @_ZZ2f2iE2st.0, align 4
+-  %call8 = tail call noundef i32 @_Z5gfuncv()
+-  %3 = load i32, ptr @_ZZ2f2iE2st.1, align 4
+-  %add9 = add nsw i32 %3, %call8
+-  store i32 %add9, ptr @_ZZ2f2iE2st.1, align 4
+-  %tobool.not = icmp eq i32 %dec, 0
+-  br i1 %tobool.not, label %while.end, label %while.body
+-
+-while.end:                                        ; preds = %while.body, %entry
+-  %4 = load i32, ptr @thl_x, align 4
+-  ret i32 %4
+-}
+-
+-; // Third function (@_Z2f3i):
+-; int f3(int c) {
+-;   int *px = &thl_x;
+-;   gfunc2(*px);
+-;   gfunc2(*px);
+-;   return 1;
+-; }
+-
+-; Function Attrs: mustprogress uwtable
+-define noundef i32 @_Z2f3i(i32 noundef %c) local_unnamed_addr #0 {
+-; CHECK-LABEL: _Z2f3i
+-; CHECK:      entry:
+-; CHECK-NEXT:   %tls_bitcast = bitcast ptr @thl_x to ptr
+-; CHECK-NEXT:   %0 = load i32, ptr %tls_bitcast, align 4
+-; CHECK-NEXT:   %call = tail call noundef i32 @_Z6gfunc2i(i32 noundef %0)
+-; CHECK-NEXT:   %1 = load i32, ptr %tls_bitcast, align 4
+-; CHECK-NEXT:   %call1 = tail call noundef i32 @_Z6gfunc2i(i32 noundef %1)
+-; CHECK-NEXT:   ret i32 1
+-entry:
+-  %0 = load i32, ptr @thl_x, align 4
+-  %call = tail call noundef i32 @_Z6gfunc2i(i32 noundef %0)
+-  %1 = load i32, ptr @thl_x, align 4
+-  %call1 = tail call noundef i32 @_Z6gfunc2i(i32 noundef %1)
+-  ret i32 1
+-}
+-
+-; Function Attrs: uwtable
+-define weak_odr hidden noundef ptr @_ZTW5thl_x() local_unnamed_addr #2 comdat {
+-  ret ptr @thl_x
+-}
+-
+-; Function Attrs: uwtable
+-define weak_odr hidden noundef ptr @_ZTW6thl_x2() local_unnamed_addr #2 comdat {
+-  ret ptr @thl_x2
+-}
+-
+-attributes #0 = { mustprogress uwtable "tls-load-hoist" "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
+-attributes #1 = { "frame-pointer"="none" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
+-attributes #2 = { uwtable "frame-pointer"="none" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
+-
+-!llvm.module.flags = !{!0, !1, !2}
+-!llvm.ident = !{!3}
+-
+-!0 = !{i32 1, !"wchar_size", i32 4}
+-!1 = !{i32 7, !"PIC Level", i32 2}
+-!2 = !{i32 7, !"uwtable", i32 2}
+-!3 = !{!"clang version 15.0.0"}
+-!4 = distinct !{!4, !5}
+-!5 = !{!"llvm.loop.mustprogress"}
+diff --git a/llvm/test/CodeGen/X86/tls-loads-control2.ll b/llvm/test/CodeGen/X86/tls-loads-control2.ll
+deleted file mode 100644
+index fb0f1d2d7398..000000000000
+--- a/llvm/test/CodeGen/X86/tls-loads-control2.ll
++++ /dev/null
+@@ -1,50 +0,0 @@
+-; RUN: opt -S -mtriple=x86_64-unknown-unknown -passes=tlshoist --relocation-model=pic --tls-load-hoist=true -o - %s | FileCheck %s --check-prefix=HOIST0
+-; RUN: opt -S -mtriple=x86_64-unknown-unknown -passes=tlshoist --relocation-model=pic -o - %s | FileCheck %s --check-prefix=HOIST2
+-
+-$_ZTW5thl_x = comdat any
+-
+-@thl_x = thread_local global i32 0, align 4
+-
+-; Function Attrs: mustprogress uwtable
+-define i32 @_Z2f1i(i32 %c) local_unnamed_addr #0 {
+-entry:
+-  %0 = load i32, ptr @thl_x, align 4
+-  %call = tail call i32 @_Z5gfunci(i32 %0)
+-  %1 = load i32, ptr @thl_x, align 4
+-  %call1 = tail call i32 @_Z5gfunci(i32 %1)
+-  ret i32 1
+-}
+-
+-;HOIST0-LABEL: _Z2f1i
+-;HOIST0:     entry:
+-;HOIST0-NEXT:  %tls_bitcast = bitcast ptr @thl_x to ptr
+-;HOIST0-NEXT:  %0 = load i32, ptr %tls_bitcast, align 4
+-;HOIST0-NEXT:  %call = tail call i32 @_Z5gfunci(i32 %0)
+-;HOIST0-NEXT:  %1 = load i32, ptr %tls_bitcast, align 4
+-;HOIST0-NEXT:  %call1 = tail call i32 @_Z5gfunci(i32 %1)
+-;HOIST0-NEXT:  ret i32 1
+-
+-;HOIST2-LABEL: _Z2f1i
+-;HOIST2:     entry:
+-;HOIST2-NEXT:  %0 = load i32, ptr @thl_x, align 4
+-;HOIST2-NEXT:  %call = tail call i32 @_Z5gfunci(i32 %0)
+-;HOIST2-NEXT:  %1 = load i32, ptr @thl_x, align 4
+-;HOIST2-NEXT:  %call1 = tail call i32 @_Z5gfunci(i32 %1)
+-;HOIST2-NEXT:  ret i32 1
+-
+-declare i32 @_Z5gfunci(i32) local_unnamed_addr #1
+-
+-; Function Attrs: uwtable
+-define weak_odr hidden ptr @_ZTW5thl_x() local_unnamed_addr #2 comdat {
+-  ret ptr @thl_x
+-}
+-
+-attributes #0 = { mustprogress uwtable "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
+-attributes #1 = { "frame-pointer"="none" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
+-attributes #2 = { uwtable "frame-pointer"="none" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
+-
+-!llvm.module.flags = !{!0, !1, !2}
+-
+-!0 = !{i32 1, !"wchar_size", i32 4}
+-!1 = !{i32 7, !"PIC Level", i32 2}
+-!2 = !{i32 7, !"uwtable", i32 1}
+diff --git a/llvm/tools/llc/llc.cpp b/llvm/tools/llc/llc.cpp
+index 80c84a977c26..c1201107787e 100644
+--- a/llvm/tools/llc/llc.cpp
++++ b/llvm/tools/llc/llc.cpp
+@@ -346,7 +346,6 @@ int main(int argc, char **argv) {
+   initializeHardwareLoopsLegacyPass(*Registry);
+   initializeTransformUtils(*Registry);
+   initializeReplaceWithVeclibLegacyPass(*Registry);
+-  initializeTLSVariableHoistLegacyPassPass(*Registry);
+ 
+   // Initialize debugging passes.
+   initializeScavengerTestPass(*Registry);
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/e3ce979f1b3ac1e7f2d0261d3abffbd12064eae6.patch b/llvm_patches/cherry/e3ce979f1b3ac1e7f2d0261d3abffbd12064eae6.patch
new file mode 100644
index 00000000..0d09c2a4
--- /dev/null
+++ b/llvm_patches/cherry/e3ce979f1b3ac1e7f2d0261d3abffbd12064eae6.patch
@@ -0,0 +1,69 @@
+From 696a2a0a02b23000c2ea96e6501eda120a8d9d29 Mon Sep 17 00:00:00 2001
+From: Aaron Ballman <aaron@aaronballman.com>
+Date: Fri, 23 Aug 2024 09:48:02 -0400
+Subject: [PATCH] Revert "[clang] Increase the default expression nesting limit
+ (#104717)"
+
+This reverts commit 7597e0930638e0a20ca9bfc193a3d89575ce4469.
+
+It caused several buildbot failures due to stack overflows with the
+parser test.
+
+patch.cherry: true
+patch.metadata.original_sha: e3ce979f1b3ac1e7f2d0261d3abffbd12064eae6
+patch.platforms: chromiumos
+patch.version_range.from: 546878
+patch.version_range.until: 547382
+
+---
+ clang/docs/ReleaseNotes.rst           | 2 --
+ clang/include/clang/Driver/Options.td | 2 +-
+ clang/test/Parser/parser_overflow.c   | 4 ++--
+ 3 files changed, 3 insertions(+), 5 deletions(-)
+
+diff --git a/clang/docs/ReleaseNotes.rst b/clang/docs/ReleaseNotes.rst
+index 93040c2eee2c..70ff5dedab21 100644
+--- a/clang/docs/ReleaseNotes.rst
++++ b/clang/docs/ReleaseNotes.rst
+@@ -174,8 +174,6 @@ Deprecated Compiler Flags
+ Modified Compiler Flags
+ -----------------------
+ 
+-- The compiler flag `-fbracket-depth` default value is increased from 256 to 2048.
+-
+ - The ``-ffp-model`` option has been updated to enable a more limited set of
+   optimizations when the ``fast`` argument is used and to accept a new argument,
+   ``aggressive``. The behavior of ``-ffp-model=aggressive`` is equivalent
+diff --git a/clang/include/clang/Driver/Options.td b/clang/include/clang/Driver/Options.td
+index 111608d30ff8..7e40e99e9ba2 100644
+--- a/clang/include/clang/Driver/Options.td
++++ b/clang/include/clang/Driver/Options.td
+@@ -7976,7 +7976,7 @@ def fapply_global_visibility_to_externs : Flag<["-"], "fapply-global-visibility-
+   MarshallingInfoFlag<LangOpts<"SetVisibilityForExternDecls">>;
+ def fbracket_depth : Separate<["-"], "fbracket-depth">,
+   HelpText<"Maximum nesting level for parentheses, brackets, and braces">,
+-  MarshallingInfoInt<LangOpts<"BracketDepth">, "2048">;
++  MarshallingInfoInt<LangOpts<"BracketDepth">, "256">;
+ defm const_strings : BoolOption<"f", "const-strings",
+   LangOpts<"ConstStrings">, DefaultFalse,
+   PosFlag<SetTrue, [], [ClangOption, CC1Option], "Use">,
+diff --git a/clang/test/Parser/parser_overflow.c b/clang/test/Parser/parser_overflow.c
+index 53c79bc06d99..9514e808550a 100644
+--- a/clang/test/Parser/parser_overflow.c
++++ b/clang/test/Parser/parser_overflow.c
+@@ -1,5 +1,5 @@
+ // RUN: not %clang_cc1 %s -fsyntax-only -DHUGE 2>&1 | FileCheck %s
+-// RUN: %clang_cc1 %s -fsyntax-only
++// RUN: not %clang_cc1 %s -fsyntax-only 2>&1 | FileCheck %s
+ // RUN: not %clang_cc1 %s -fsyntax-only -fbracket-depth 299 2>&1 | FileCheck %s
+ // RUN: %clang_cc1 %s -fsyntax-only -fbracket-depth 300
+ // RUN: not %clang %s -fsyntax-only -fbracket-depth=299 2>&1 | FileCheck %s
+@@ -15,5 +15,5 @@ void foo(void) {
+ #endif
+ }
+ 
+-// CHECK: fatal error: bracket nesting level exceeded maximum of {{2048|299}}
++// CHECK: fatal error: bracket nesting level exceeded maximum of {{256|299}}
+ // CHECK: note: use -fbracket-depth=N to increase maximum nesting level
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/cherry/fd7904a07bc26950fa7735fb6871a064e3ebc836.patch b/llvm_patches/cherry/fd7904a07bc26950fa7735fb6871a064e3ebc836.patch
new file mode 100644
index 00000000..289a822e
--- /dev/null
+++ b/llvm_patches/cherry/fd7904a07bc26950fa7735fb6871a064e3ebc836.patch
@@ -0,0 +1,32 @@
+From 03f9644037afe99a5b987c276658d796e06292b2 Mon Sep 17 00:00:00 2001
+From: Adrian Prantl <aprantl@apple.com>
+Date: Fri, 23 Aug 2024 09:25:24 -0700
+Subject: [PATCH] Revert "[lldb] Speculative fix for trap_frame_sym_ctx.test"
+
+This reverts commit 19d3f3417100dc99caa4394fbd26fc0c4702264e.
+
+patch.cherry: true
+patch.metadata.original_sha: fd7904a07bc26950fa7735fb6871a064e3ebc836
+patch.platforms: chromiumos
+patch.version_range.from: 547186
+patch.version_range.until: 547400
+
+---
+ lldb/test/Shell/Unwind/trap_frame_sym_ctx.test | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/lldb/test/Shell/Unwind/trap_frame_sym_ctx.test b/lldb/test/Shell/Unwind/trap_frame_sym_ctx.test
+index 08a26616240e..1bf1fb1d6e85 100644
+--- a/lldb/test/Shell/Unwind/trap_frame_sym_ctx.test
++++ b/lldb/test/Shell/Unwind/trap_frame_sym_ctx.test
+@@ -15,7 +15,7 @@ breakpoint set -n bar
+ process launch
+ # CHECK: stop reason = breakpoint 1.1
+ 
+-thread backtrace -u
++thread backtrace
+ # CHECK: frame #0: {{.*}}`bar
+ # CHECK: frame #1: {{.*}}`tramp
+ # CHECK: frame #2: {{.*}}`main
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/clang-12.0-asan-default-path.patch b/llvm_patches/clang-12.0-asan-default-path.patch
new file mode 100644
index 00000000..b9b6ad8e
--- /dev/null
+++ b/llvm_patches/clang-12.0-asan-default-path.patch
@@ -0,0 +1,31 @@
+From d83201605cdb869ac3741fa03d199b9689053ac9 Mon Sep 17 00:00:00 2001
+From: Jordan R Abrahams-Whitehead <ajordanr@google.com>
+Date: Wed, 12 Jun 2024 21:41:58 +0000
+Subject: [PATCH] clang-12.0-asan-default-path
+
+By default, the ASAN message goes to STDERR. This causes problem if we cannot
+log the ASAN message, and we can not figure out why the ASAN check fails.
+This change set the ASAN message goes to /var/log/asan${pid} by default.
+
+As a workaround for b/223869781, this is set to /tmp/asan.
+
+---
+ compiler-rt/lib/sanitizer_common/sanitizer_flags.inc | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/compiler-rt/lib/sanitizer_common/sanitizer_flags.inc b/compiler-rt/lib/sanitizer_common/sanitizer_flags.inc
+index 6148ae56067c..a5884521ec7e 100644
+--- a/compiler-rt/lib/sanitizer_common/sanitizer_flags.inc
++++ b/compiler-rt/lib/sanitizer_common/sanitizer_flags.inc
+@@ -52,7 +52,7 @@ COMMON_FLAG(bool, handle_ioctl, false, "Intercept and handle ioctl requests.")
+ COMMON_FLAG(int, malloc_context_size, 1,
+             "Max number of stack frames kept for each allocation/deallocation.")
+ COMMON_FLAG(
+-    const char *, log_path, nullptr,
++    const char *, log_path, "/tmp/asan",
+     "Write logs to \"log_path.pid\". The special values are \"stdout\" and "
+     "\"stderr\". If unspecified, defaults to \"stderr\".")
+ COMMON_FLAG(
+-- 
+2.45.2.505.gda0bf45e8d-goog
+
diff --git a/llvm_patches/cros-add-llvm-rev-file.patch b/llvm_patches/cros-add-llvm-rev-file.patch
new file mode 100644
index 00000000..074a5908
--- /dev/null
+++ b/llvm_patches/cros-add-llvm-rev-file.patch
@@ -0,0 +1,22 @@
+From e8441acf860b055d4f3f9ae5d1ec8a2db4b257fc Mon Sep 17 00:00:00 2001
+From: George Burgess IV <gbiv@google.com>
+Date: Fri, 10 Jan 2025 07:40:19 -0700
+Subject: [PATCH] cros: add llvm-rev file
+
+BUG=b:389058140
+TEST=None
+
+---
+ cros/llvm-rev | 1 +
+ 1 file changed, 1 insertion(+)
+ create mode 100644 cros/llvm-rev
+
+diff --git a/cros/llvm-rev b/cros/llvm-rev
+new file mode 100644
+index 000000000000..237aa980e055
+--- /dev/null
++++ b/cros/llvm-rev
+@@ -0,0 +1 @@
++547379
+-- 
+2.49.0.395.g12beb8f557-goog
\ No newline at end of file
diff --git a/llvm_patches/disable-querying-git-in-benchmarking-code.patch b/llvm_patches/disable-querying-git-in-benchmarking-code.patch
new file mode 100644
index 00000000..e2f23c7f
--- /dev/null
+++ b/llvm_patches/disable-querying-git-in-benchmarking-code.patch
@@ -0,0 +1,34 @@
+From ac5ed85fd4cef222ed3fd4df00854cbb2b2abd75 Mon Sep 17 00:00:00 2001
+From: Jordan R Abrahams-Whitehead <ajordanr@google.com>
+Date: Sat, 15 Jun 2024 00:14:01 +0000
+Subject: [PATCH] disable querying git in benchmarking code
+
+This benchmarking library will try to query git for the current SHA, and this
+code is called even if llvm-libc's benchmarks are disabled.
+
+Querying git modifies `index.lock` files, which makes Portage's sandbox unhappy.
+Manually set a failure value here instead.
+
+---
+ third-party/benchmark/CMakeLists.txt | 5 +++--
+ 1 file changed, 3 insertions(+), 2 deletions(-)
+
+diff --git a/third-party/benchmark/CMakeLists.txt b/third-party/benchmark/CMakeLists.txt
+index 8af49406d052..71e5603ab290 100644
+--- a/third-party/benchmark/CMakeLists.txt
++++ b/third-party/benchmark/CMakeLists.txt
+@@ -105,8 +105,9 @@ list(APPEND CMAKE_MODULE_PATH "${CMAKE_CURRENT_SOURCE_DIR}/cmake")
+ 
+ 
+ # Read the git tags to determine the project version
+-include(GetGitVersion)
+-get_git_version(GIT_VERSION)
++#include(GetGitVersion)
++#get_git_version(GIT_VERSION)
++set(GIT_VERSION "0.0.0")
+ 
+ # If no git version can be determined, use the version
+ # from the project() command
+-- 
+2.45.2.627.g7a2c4fd464-goog
+
diff --git a/llvm_patches/inline-cost-skip-threshold-checks.patch b/llvm_patches/inline-cost-skip-threshold-checks.patch
new file mode 100644
index 00000000..08ed9253
--- /dev/null
+++ b/llvm_patches/inline-cost-skip-threshold-checks.patch
@@ -0,0 +1,31 @@
+From 4a957141db9f96a449365f1d270ccfb5182bdc14 Mon Sep 17 00:00:00 2001
+From: Jordan R Abrahams-Whitehead <ajordanr@google.com>
+Date: Fri, 14 Jun 2024 21:34:26 +0000
+Subject: [PATCH] inline-cost: skip threshold checks
+
+Some users in CrOS set internal LLVM flags that can tune these to <0. The
+comment admits that <0 is a valid value, and inglorion@ noted that upstream does
+not see an inline threshold here <0 being broken somehow. See b/256193799 for
+more.
+
+---
+ llvm/lib/Analysis/InlineCost.cpp | 3 ---
+ 1 file changed, 3 deletions(-)
+
+diff --git a/llvm/lib/Analysis/InlineCost.cpp b/llvm/lib/Analysis/InlineCost.cpp
+index fa0c30637633..336e887266cb 100644
+--- a/llvm/lib/Analysis/InlineCost.cpp
++++ b/llvm/lib/Analysis/InlineCost.cpp
+@@ -1065,9 +1065,6 @@ class InlineCostCallAnalyzer final : public CallAnalyzer {
+     // While Threshold depends on commandline options that can take negative
+     // values, we want to enforce the invariant that the computed threshold and
+     // bonuses are non-negative.
+-    assert(Threshold >= 0);
+-    assert(SingleBBBonus >= 0);
+-    assert(VectorBonus >= 0);
+ 
+     // Speculatively apply all possible bonuses to Threshold. If cost exceeds
+     // this Threshold any time, and cost cannot decrease, we can stop processing
+-- 
+2.45.2.627.g7a2c4fd464-goog
+
diff --git a/llvm_patches/libc-Make-_LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTAB.patch b/llvm_patches/libc-Make-_LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTAB.patch
new file mode 100644
index 00000000..01156aa3
--- /dev/null
+++ b/llvm_patches/libc-Make-_LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTAB.patch
@@ -0,0 +1,37 @@
+From 9fa733e5e5da74d18f36728aa0b7af0108da222c Mon Sep 17 00:00:00 2001
+From: Tom Hughes <tomhughes@chromium.org>
+Date: Fri, 13 Sep 2024 16:47:06 -0700
+Subject: [PATCH] [libc++] Make _LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTABLE a
+ no-op on baremetal
+
+Baremetal targets tend to use custom linker scripts, which may not
+include the __lcxx_override section.
+
+BUG=b:363082822
+---
+ libcxx/src/include/overridable_function.h | 9 ++++++++-
+ 1 file changed, 8 insertions(+), 1 deletion(-)
+
+diff --git a/libcxx/src/include/overridable_function.h b/libcxx/src/include/overridable_function.h
+index 7b0fba10f47d..ea0cb95f1976 100644
+--- a/libcxx/src/include/overridable_function.h
++++ b/libcxx/src/include/overridable_function.h
+@@ -61,7 +61,14 @@
+ // libc++abi, which needs the same mechanism.
+ //
+ 
+-#if defined(_LIBCPP_OBJECT_FORMAT_MACHO)
++// Baremetal targets tend to use custom linker scripts, which may not include
++// the __lcxx_override section.
++#if defined(LIBCXXABI_BAREMETAL)
++
++#  define _LIBCPP_CAN_DETECT_OVERRIDDEN_FUNCTION 0
++#  define _LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTABLE /* nothing */
++
++#elif defined(_LIBCPP_OBJECT_FORMAT_MACHO)
+ 
+ #  define _LIBCPP_CAN_DETECT_OVERRIDDEN_FUNCTION 1
+ #  define _LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTABLE                                                                 \
+-- 
+2.46.0.662.g92d0881bb0-goog
+
diff --git a/llvm_patches/libcxx-crash-when-dereferencing-nullopt-v4.patch b/llvm_patches/libcxx-crash-when-dereferencing-nullopt-v4.patch
new file mode 100644
index 00000000..acd47a98
--- /dev/null
+++ b/llvm_patches/libcxx-crash-when-dereferencing-nullopt-v4.patch
@@ -0,0 +1,97 @@
+From 76af864ce26e5eeb18b62cb9c8b7c6559e694a90 Mon Sep 17 00:00:00 2001
+From: Grace Cham <hscham@chromium.org>
+Date: Tue, 16 Jul 2024 23:17:37 +0000
+Subject: [PATCH] [libcxx] v4: Crash when dereferencing nullopt for
+ std::optional
+
+Original author hscham <hscham@chromium.org>.
+
+TODO(b/192529039): this is a temporary patch to address security
+concerns when migrating base:Optional and absl::optional to
+std::optional. libcxx upstream is working on decoupling hardening and
+debugging asserts and this patch should be replaced by the upstream
+patch upon completion.
+---
+ libcxx/include/optional | 38 ++++++++++++++++++++++++++++++++------
+ 1 file changed, 32 insertions(+), 6 deletions(-)
+
+diff --git a/libcxx/include/optional b/libcxx/include/optional
+index 99bfd0dd900d..31c66df52cb7 100644
+--- a/libcxx/include/optional
++++ b/libcxx/include/optional
+@@ -177,6 +177,8 @@ namespace std {
+ 
+ */
+ 
++#include <stdio.h>
++
+ #include <__assert>
+ #include <__availability>
+ #include <__compare/compare_three_way_result.h>
+@@ -787,33 +789,57 @@ public:
+     }
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr add_pointer_t<value_type const> operator->() const {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr add_pointer_t<value_type const> operator->() const {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator-> called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return std::addressof(this->__get());
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr add_pointer_t<value_type> operator->() {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr add_pointer_t<value_type> operator->() {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator-> called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return std::addressof(this->__get());
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr const value_type& operator*() const& noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr const value_type& operator*() const& noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator* called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return this->__get();
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr value_type& operator*() & noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr value_type& operator*() & noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator* called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return this->__get();
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr value_type&& operator*() && noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr value_type&& operator*() && noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator* called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return std::move(this->__get());
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr const value_type&& operator*() const&& noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr const value_type&& operator*() const&& noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator* called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return std::move(this->__get());
+   }
+ 
+-- 
+2.45.2.993.g49e7a77208-goog
+
diff --git a/llvm_patches/lld-10.0-invoke-name-v2.patch b/llvm_patches/lld-10.0-invoke-name-v2.patch
new file mode 100644
index 00000000..139ab340
--- /dev/null
+++ b/llvm_patches/lld-10.0-invoke-name-v2.patch
@@ -0,0 +1,29 @@
+From 4f2fe30c0e470993087e471838140be241e70af7 Mon Sep 17 00:00:00 2001
+From: George Burgess IV <gbiv@google.com>
+Date: Fri, 14 Jun 2024 21:19:58 +0000
+Subject: [PATCH] lld-10.0-invoke-name-v2
+
+LLD uses the argv[0] to get the host system information.
+In standalone toolchain used by Simplechrome, real lld becomes
+lld.elf, so the LLD cannot get the host system informaction
+and dies. see crbug.com/701659
+
+---
+ lld/Common/DriverDispatcher.cpp | 1 +
+ 1 file changed, 1 insertion(+)
+
+diff --git a/lld/Common/DriverDispatcher.cpp b/lld/Common/DriverDispatcher.cpp
+index 379a4c6ddabe..f1fd8c91fc60 100644
+--- a/lld/Common/DriverDispatcher.cpp
++++ b/lld/Common/DriverDispatcher.cpp
+@@ -32,6 +32,7 @@ static void err(const Twine &s) { llvm::errs() << s << "\n"; }
+ static Flavor getFlavor(StringRef s) {
+   return StringSwitch<Flavor>(s)
+       .CasesLower("ld", "ld.lld", "gnu", Gnu)
++      .CasesLower("lld.elf", "lld.real", "lld.real.elf", "gnu", Gnu)
+       .CasesLower("wasm", "ld-wasm", Wasm)
+       .CaseLower("link", WinLink)
+       .CasesLower("ld64", "ld64.lld", "darwin", Darwin)
+-- 
+2.45.2.627.g7a2c4fd464-goog
+
diff --git a/llvm_patches/llvm-17.0-invocation.patch b/llvm_patches/llvm-17.0-invocation.patch
new file mode 100644
index 00000000..6b13f762
--- /dev/null
+++ b/llvm_patches/llvm-17.0-invocation.patch
@@ -0,0 +1,32 @@
+From bfb6d4554db6ec7329273007073556d49a21f3d6 Mon Sep 17 00:00:00 2001
+From: Jordan R Abrahams-Whitehead <ajordanr@google.com>
+Date: Fri, 14 Jun 2024 21:25:05 +0000
+Subject: [PATCH] llvm-17.0-invocation
+
+In simple-chrome, clang is a shell wrapper, it calls clang.elf, which is the
+real binary, then, clang.elf calls clang.elf itself. It is the last invocation
+"clang.elf -> clang.elf" that breaks, because direct calls to clang.elf misses
+all environment setup in clang wrapper. Fix this by calling to clang wrapper
+instead of clang.elf binary.
+
+---
+ clang/tools/driver/driver.cpp | 3 +++
+ 1 file changed, 3 insertions(+)
+
+diff --git a/clang/tools/driver/driver.cpp b/clang/tools/driver/driver.cpp
+index 531b5b4a61c1..b3eafeaa77c2 100644
+--- a/clang/tools/driver/driver.cpp
++++ b/clang/tools/driver/driver.cpp
+@@ -451,6 +451,9 @@ int clang_main(int Argc, char **Argv, const llvm::ToolContext &ToolContext) {
+   }
+ 
+   std::string Path = GetExecutablePath(ToolContext.Path, CanonicalPrefixes);
++  size_t PathLen = Path.length();
++  if (PathLen > 4 && Path.substr(PathLen - 4) == ".elf")
++    Path = Path.substr(0, PathLen - 4);
+ 
+   // Whether the cc1 tool should be called inside the current process, or if we
+   // should spawn a new clang subprocess (old behavior).
+-- 
+2.45.2.627.g7a2c4fd464-goog
+
diff --git a/llvm_patches/llvm-3.9-dwarf-version.patch b/llvm_patches/llvm-3.9-dwarf-version.patch
new file mode 100644
index 00000000..40a2003e
--- /dev/null
+++ b/llvm_patches/llvm-3.9-dwarf-version.patch
@@ -0,0 +1,32 @@
+From: Caroline Tice <cmtice@google.com>
+
+llvm-3.9-dwarf-version
+
+This is a temporary workaround.
+
+Currently LLVM generates Dwarf version 4 for all of its debug
+information, including its Debug Frames (CIE) information. 
+Breakpad cannot handle Dwarf Version 4 (see
+https://bugs.chromium.org/p/chromium/issues/detail?id=614788).  We
+tried reducing all of the Dwarf information to version 3, but that
+causes problems with Fission (the splitting of debug information into
+separate files), which requires the Dwarf DIEs to be emitted at
+version 4.  This patch works around that issue by forcing the Debug
+Frames version of Dwarf to be version 1, while leaving the rest of the
+Dwarf data at version 4.  When the replacement for Breakpad comes out
+we will no longer need this patch, as that is supposed to be able to
+handle version 4 frames information.
+
+index 759f90e..d536397 100644
+--- a/llvm/lib/MC/MCDwarf.cpp
++++ b/llvm/lib/MC/MCDwarf.cpp
+@@ -1252,7 +1252,8 @@ static unsigned getCIEVersion(bool IsEH, unsigned DwarfVersion) {
+     return 3;
+   case 4:
+   case 5:
+-    return 4;
++    return 1;
++    //    return 4;  Temporarily disable as workaround for Breakpad issue
+   }
+   llvm_unreachable("Unknown version");
+ }
diff --git a/llvm_patches/llvm-8.0-clang-executable-detection.v2.patch b/llvm_patches/llvm-8.0-clang-executable-detection.v2.patch
new file mode 100644
index 00000000..1785ad44
--- /dev/null
+++ b/llvm_patches/llvm-8.0-clang-executable-detection.v2.patch
@@ -0,0 +1,28 @@
+From a3a1aa198d4f52d3a64fe58d6ca03b103b6f6474 Mon Sep 17 00:00:00 2001
+From: Jordan R Abrahams-Whitehead <ajordanr@google.com>
+Date: Fri, 14 Jun 2024 21:02:42 +0000
+Subject: [PATCH] llvm-8.0-clang-executable-detection.v2
+
+This is to fix crbug/591436, which is a blocker on using clang for
+simple-chrome.
+
+---
+ llvm/lib/Support/Unix/Path.inc | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/llvm/lib/Support/Unix/Path.inc b/llvm/lib/Support/Unix/Path.inc
+index 68ca58fda3b8..92ea04987256 100644
+--- a/llvm/lib/Support/Unix/Path.inc
++++ b/llvm/lib/Support/Unix/Path.inc
+@@ -253,7 +253,7 @@ std::string getMainExecutable(const char *argv0, void *MainAddr) {
+ #elif defined(__linux__) || defined(__CYGWIN__) || defined(__gnu_hurd__)
+   char exe_path[PATH_MAX];
+   const char *aPath = "/proc/self/exe";
+-  if (sys::fs::exists(aPath)) {
++  if (false && sys::fs::exists(aPath)) {
+     // /proc is not always mounted under Linux (chroot for example).
+     ssize_t len = readlink(aPath, exe_path, sizeof(exe_path));
+     if (len < 0)
+-- 
+2.45.2.627.g7a2c4fd464-goog
+
diff --git a/llvm_patches/scudo-Add-baseline-Scudo-config-for-ChromeOS-v2.patch b/llvm_patches/scudo-Add-baseline-Scudo-config-for-ChromeOS-v2.patch
new file mode 100644
index 00000000..d805cc3a
--- /dev/null
+++ b/llvm_patches/scudo-Add-baseline-Scudo-config-for-ChromeOS-v2.patch
@@ -0,0 +1,99 @@
+From 9f04550d40b2f259adace54b8fc81c9cdee6f96b Mon Sep 17 00:00:00 2001
+From: Jordan R Abrahams-Whitehead <ajordanr@google.com>
+Date: Sat, 15 Jun 2024 00:11:11 +0000
+Subject: [PATCH] [scudo] Add baseline Scudo config for ChromeOS v2
+
+---
+ .../lib/scudo/standalone/allocator_config.h   | 51 +++++++++++++++++++
+ compiler-rt/lib/scudo/standalone/platform.h   |  6 +++
+ 2 files changed, 57 insertions(+)
+
+diff --git a/compiler-rt/lib/scudo/standalone/allocator_config.h b/compiler-rt/lib/scudo/standalone/allocator_config.h
+index 3c6aa3acb0e4..a2fe943016ef 100644
+--- a/compiler-rt/lib/scudo/standalone/allocator_config.h
++++ b/compiler-rt/lib/scudo/standalone/allocator_config.h
+@@ -204,6 +204,55 @@ struct AndroidConfig {
+   template <typename Config> using SecondaryT = MapAllocator<Config>;
+ };
+ 
++struct ChromeOSConfig {
++  static const bool MaySupportMemoryTagging = false;
++  template <class A>
++  using TSDRegistryT = TSDRegistryExT<A>; // Exclusive
++
++  // TODO: Retune configs with more experimentation.
++  struct Primary {
++    using SizeClassMap = DefaultSizeClassMap;
++#if SCUDO_CAN_USE_PRIMARY64
++    static const uptr RegionSizeLog = 29U;
++    static const uptr GroupSizeLog = 20U;
++    typedef uptr CompactPtrT;
++    static const uptr CompactPtrScale = 0;
++    static const bool EnableRandomOffset = true;
++    static const uptr MapSizeIncrement = 1UL << 18;
++#else
++    static const uptr RegionSizeLog = 19U;
++    static const uptr GroupSizeLog = 19U;
++    typedef uptr CompactPtrT;
++#endif
++    static const s32 MinReleaseToOsIntervalMs = INT32_MIN;
++    static const s32 MaxReleaseToOsIntervalMs = INT32_MAX;
++  };
++
++#if SCUDO_CAN_USE_PRIMARY64
++  template <typename Config>
++  using PrimaryT = SizeClassAllocator64<Config>;
++#else
++  template <typename Config>
++  using PrimaryT = SizeClassAllocator32<Config>;
++#endif
++
++  struct Secondary {
++    struct Cache {
++      static const u32 EntriesArraySize = 32U;
++      static const u32 QuarantineSize = 0U;
++      static const u32 DefaultMaxEntriesCount = 32U;
++      static const uptr DefaultMaxEntrySize = 1UL << 19;
++      static const s32 MinReleaseToOsIntervalMs = INT32_MIN;
++      static const s32 MaxReleaseToOsIntervalMs = INT32_MAX;
++    };
++    template <typename Config>
++    using CacheT = MapAllocatorCache<Config>;
++  };
++
++  template <typename Config>
++  using SecondaryT = MapAllocator<Config>;
++};
++
+ #if SCUDO_CAN_USE_PRIMARY64
+ struct FuchsiaConfig {
+   static const bool MaySupportMemoryTagging = false;
+@@ -265,6 +314,8 @@ struct TrustyConfig {
+ 
+ #if SCUDO_ANDROID
+ typedef AndroidConfig Config;
++#elif SCUDO_CHROMEOS
++typedef ChromeOSConfig Config;
+ #elif SCUDO_FUCHSIA
+ typedef FuchsiaConfig Config;
+ #elif SCUDO_TRUSTY
+diff --git a/compiler-rt/lib/scudo/standalone/platform.h b/compiler-rt/lib/scudo/standalone/platform.h
+index b71a86be7669..b8b0287ef0b1 100644
+--- a/compiler-rt/lib/scudo/standalone/platform.h
++++ b/compiler-rt/lib/scudo/standalone/platform.h
+@@ -25,6 +25,12 @@
+ #define SCUDO_ANDROID 0
+ #endif
+ 
++#if defined(USE_CHROMEOS_CONFIG)
++#define SCUDO_CHROMEOS 1
++#else
++#define SCUDO_CHROMEOS 0
++#endif
++
+ #if defined(__Fuchsia__)
+ #define SCUDO_FUCHSIA 1
+ #else
+-- 
+2.45.2.627.g7a2c4fd464-goog
+
diff --git a/llvm_patches/v2-libc-Make-_LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTAB.patch b/llvm_patches/v2-libc-Make-_LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTAB.patch
new file mode 100644
index 00000000..b63013c8
--- /dev/null
+++ b/llvm_patches/v2-libc-Make-_LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTAB.patch
@@ -0,0 +1,37 @@
+From 26ffe7961230850ccb403dac6e9da53972cf5fa8 Mon Sep 17 00:00:00 2001
+From: Tom Hughes <tomhughes@chromium.org>
+Date: Fri, 13 Sep 2024 16:47:06 -0700
+Subject: [PATCH] v2: Make _LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTABLE a no-op on
+ baremetal
+
+Baremetal targets tend to use custom linker scripts, which may not
+include the __lcxx_override section.
+
+BUG=b:363082822
+---
+ libcxx/src/include/overridable_function.h | 9 ++++++++-
+ 1 file changed, 8 insertions(+), 1 deletion(-)
+
+diff --git a/libcxx/src/include/overridable_function.h b/libcxx/src/include/overridable_function.h
+index 6c70f6242ddd..7f57472696ed 100644
+--- a/libcxx/src/include/overridable_function.h
++++ b/libcxx/src/include/overridable_function.h
+@@ -63,7 +63,14 @@
+ // want to be defining special sections inside user's executables which use our headers.
+ //
+ 
+-#if defined(_LIBCPP_OBJECT_FORMAT_MACHO)
++// Baremetal targets tend to use custom linker scripts, which may not include
++// the __lcxx_override section.
++#if defined(LIBCXXABI_BAREMETAL)
++
++#  define _LIBCPP_CAN_DETECT_OVERRIDDEN_FUNCTION 0
++#  define _LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTABLE /* nothing */
++
++#elif defined(_LIBCPP_OBJECT_FORMAT_MACHO)
+ 
+ #  define _LIBCPP_CAN_DETECT_OVERRIDDEN_FUNCTION 1
+ #  define _LIBCPP_MAKE_OVERRIDABLE_FUNCTION_DETECTABLE                                                                 \
+-- 
+2.49.0.472.ge94155a9ec-goog
+
diff --git a/llvm_patches/v5-Crash-when-dereferencing-nullopt-for-std-optional.patch b/llvm_patches/v5-Crash-when-dereferencing-nullopt-for-std-optional.patch
new file mode 100644
index 00000000..981ddb1a
--- /dev/null
+++ b/llvm_patches/v5-Crash-when-dereferencing-nullopt-for-std-optional.patch
@@ -0,0 +1,98 @@
+From bfad7870167d3f3712c8a8af1cd294baae2f51ca Mon Sep 17 00:00:00 2001
+From: Grace Cham <hscham@chromium.org>
+Date: Tue, 16 Jul 2024 23:17:37 +0000
+Subject: [PATCH] v5: Crash when dereferencing nullopt for std::optional
+
+Original author hscham <hscham@chromium.org>.
+
+TODO(b/192529039): this is a temporary patch to address security
+concerns when migrating base:Optional and absl::optional to
+std::optional. libcxx upstream is working on decoupling hardening and
+debugging asserts and this patch should be replaced by the upstream
+patch upon completion.
+
+Change-Id: I3324856fff67b0109471bdf44f46bccd773407c2
+---
+ libcxx/include/optional | 38 ++++++++++++++++++++++++++++++++------
+ 1 file changed, 32 insertions(+), 6 deletions(-)
+
+diff --git a/libcxx/include/optional b/libcxx/include/optional
+index 41d7515a2b68..8484666fb11c 100644
+--- a/libcxx/include/optional
++++ b/libcxx/include/optional
+@@ -177,6 +177,8 @@ namespace std {
+ 
+ */
+ 
++#include <stdio.h>
++
+ #include <__assert>
+ #include <__compare/compare_three_way_result.h>
+ #include <__compare/three_way_comparable.h>
+@@ -786,33 +788,57 @@ public:
+     }
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr add_pointer_t<value_type const> operator->() const noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr add_pointer_t<value_type const> operator->() const noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator-> called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return std::addressof(this->__get());
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr add_pointer_t<value_type> operator->() noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr add_pointer_t<value_type> operator->() noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator-> called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return std::addressof(this->__get());
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr const value_type& operator*() const& noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr const value_type& operator*() const& noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator* called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return this->__get();
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr value_type& operator*() & noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr value_type& operator*() & noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator* called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return this->__get();
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr value_type&& operator*() && noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr value_type&& operator*() && noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator* called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return std::move(this->__get());
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr const value_type&& operator*() const&& noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr const value_type&& operator*() const&& noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator* called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return std::move(this->__get());
+   }
+ 
+-- 
+2.46.0.792.g87dc391469-goog
+
diff --git a/llvm_patches/v6-Crash-when-dereferencing-nullopt-for-std-optional.patch b/llvm_patches/v6-Crash-when-dereferencing-nullopt-for-std-optional.patch
new file mode 100644
index 00000000..57c8c492
--- /dev/null
+++ b/llvm_patches/v6-Crash-when-dereferencing-nullopt-for-std-optional.patch
@@ -0,0 +1,99 @@
+From 32dc727d76fd4739e2d98ae47c7e17e89dfa5366 Mon Sep 17 00:00:00 2001
+From: Grace Cham <hscham@chromium.org>
+Date: Tue, 16 Jul 2024 23:17:37 +0000
+Subject: [PATCH] v6: Crash when dereferencing nullopt for std::optional
+
+Original author hscham <hscham@chromium.org>.
+
+TODO(b/192529039): this is a temporary patch to address security
+concerns when migrating base:Optional and absl::optional to
+std::optional. libcxx upstream is working on decoupling hardening and
+debugging asserts and this patch should be replaced by the upstream
+patch upon completion.
+
+Change-Id: I3324856fff67b0109471bdf44f46bccd773407c2
+---
+ libcxx/include/optional | 39 +++++++++++++++++++++++++++++++++------
+ 1 file changed, 33 insertions(+), 6 deletions(-)
+
+diff --git a/libcxx/include/optional b/libcxx/include/optional
+index c325140ee66f..862f97b73f0f 100644
+--- a/libcxx/include/optional
++++ b/libcxx/include/optional
+@@ -177,6 +177,9 @@ namespace std {
+ 
+ */
+ 
++
++#include <stdio.h>
++
+ #if __cplusplus < 201103L && defined(_LIBCPP_USE_FROZEN_CXX03_HEADERS)
+ #  include <__cxx03/optional>
+ #else
+@@ -794,33 +797,57 @@ public:
+     }
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr add_pointer_t<value_type const> operator->() const noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr add_pointer_t<value_type const> operator->() const noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator-> called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return std::addressof(this->__get());
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr add_pointer_t<value_type> operator->() noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr add_pointer_t<value_type> operator->() noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator-> called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return std::addressof(this->__get());
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr const value_type& operator*() const& noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr const value_type& operator*() const& noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator* called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return this->__get();
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr value_type& operator*() & noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr value_type& operator*() & noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator* called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return this->__get();
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr value_type&& operator*() && noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr value_type&& operator*() && noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator* called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return std::move(this->__get());
+   }
+ 
+-  _LIBCPP_HIDE_FROM_ABI constexpr const value_type&& operator*() const&& noexcept {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_AVAILABILITY_THROW_BAD_OPTIONAL_ACCESS constexpr const value_type&& operator*() const&& noexcept {
+     _LIBCPP_ASSERT_VALID_ELEMENT_ACCESS(this->has_value(), "optional operator* called on a disengaged value");
++        if (!this->has_value()) {
++          fprintf(stderr, "optional operator-> called on a disengaged value\n");
++          __builtin_trap();
++        }
+     return std::move(this->__get());
+   }
+ 
+-- 
+2.49.0.472.ge94155a9ec-goog
+
diff --git a/llvm_tools/README.md b/llvm_tools/README.md
index 8072155e..0c906349 100644
--- a/llvm_tools/README.md
+++ b/llvm_tools/README.md
@@ -182,190 +182,9 @@ $ ./patch_manager.py \
 
 ## LLVM Bisection
 
-### `llvm_bisection.py`
+### `llvm_simple_bisect.py`
 
-#### Usage
-
-This script is used to bisect a bad revision of LLVM. After the script finishes,
-the user requires to run the script again to continue the bisection. Intially,
-the script creates a JSON file that does not exist which then continues
-bisection (after invoking the script again) based off of the JSON file.
-
-For example, assuming the revision 369420 is the bad revision:
-
-```
-$ ./llvm_bisection.py \
-  --parallel 3 \
-  --start_rev 369410 \
-  --end_rev 369420 \
-  --last_tested /abs/path/to/tryjob/file/ \
-  --extra_change_lists 513590 \
-  --builder eve-release-tryjob \
-  --options latest-toolchain
-```
-
-The above example bisects the bad revision (369420), starting from the good
-revision 369410 and launching 3 tryjobs in between if possible (`--parallel`).
-Here, the `--last_tested` path is a path to a JSON file that does not exist. The
-tryjobs are tested on the eve board. `--extra_change_lists` and `--options`
-indicate what parameters to pass into launching a tryjob.
-
-For help with the command line arguments of the script, run:
-
-```
-$ ./llvm_bisection.py --help
-```
-
-### `auto_llvm_bisection.py`
-
-#### Usage
-
-This script automates the LLVM bisection process by using `cros buildresult` to
-update the status of each tryjob.
-
-An example when this script would be used to do LLVM bisection overnight
-because tryjobs take very long to finish.
-
-For example, assuming the good revision is 369410 and the bad revision is
-369420, then:
-
-```
-$ ./auto_llvm_bisection.py --start_rev 369410 --end_rev 369420 \
-  --last_tested /abs/path/to/last_tested_file.json \
-  --extra_change_lists 513590 1394249 \
-  --options latest-toolchain nochromesdk \
-  --chromeos_path /path/to/chromeos/chroot \
-  --builder eve-release-tryjob
-```
-
-The example above bisects LLVM between revision 369410 and 369420. If the file
-exists, the script resumes bisection. Otherwise, the script creates the file
-provided by `--last_tested`. `--extra_change_lists` and `--options` are used for
-each tryjob when being submitted. Lastly, the tryjobs are launched for the board
-provided by `--builder` (in this example, for the eve board).
-
-For help with the command line arguments of the script, run:
-
-```
-$ ./auto_llvm_bisection.py --help
-```
-
-### `update_tryjob_status.py`
-
-#### Usage
-
-This script updates a tryjob's 'status' value when bisecting LLVM. This script
-can use the file that was created by `llvm_bisection.py`.
-
-An example when this script would be used is when the result of tryjob that was
-launched was 'fail' (due to flaky infra) but it should really have been
-'success'.
-
-For example, to update a tryjob's 'status' to 'good':
-
-```
-$ ./update_tryjob_status.py \
-  --set_status good \
-  --revision 369412 \
-  --status_file /abs/path/to/tryjob/file
-```
-
-The above example uses the file in `--status_file` to update a tryjob in that
-file that tested the revision 369412 and sets its 'status' value to 'good'.
-
-For help with the command line arguments of the script, run:
-
-```
-$ ./update_tryjob_status.py --help
-```
-
-For example, to update a tryjob's 'status' to 'bad':
-
-```
-$ ./update_tryjob_status.py \
-  --set_status bad \
-  --revision 369412 \
-  --status_file /abs/path/to/tryjob/file
-```
-
-For example, to update a tryjob's 'status' to 'pending':
-
-```
-$ ./update_tryjob_status.py \
-  --set_status pending \
-  --revision 369412 \
-  --status_file /abs/path/to/tryjob/file
-```
-
-For example, to update a tryjob's 'status' to 'skip':
-
-```
-$ ./update_tryjob_status.py \
-  --set_status skip \
-  --revision 369412 \
-  --status_file /abs/path/to/tryjob/file
-```
-
-For example, to update a tryjob's 'status' based off a custom script exit code:
-
-```
-$ ./update_tryjob_status.py \
-  --set_status custom_script \
-  --revision 369412 \
-  --status_file /abs/path/to/tryjob/file \
-  --custom_script /abs/path/to/script.py
-```
-
-### `modify_a_tryjob.py`
-
-#### Usage
-
-This script modifies a tryjob directly given an already created tryjob file when
-bisecting LLVM. The file created by `llvm_bisection.py` can be used in this
-script.
-
-An example when this script would be used is when a tryjob needs to be manually
-added.
-
-For example:
-
-```
-$ ./modify_a_tryjob.py \
-  --modify_a_tryjob add \
-  --revision 369416 \
-  --extra_change_lists 513590 \
-  --options latest-toolchain \
-  --builder eve-release-tryjob \
-  --status_file /abs/path/to/tryjob/file
-```
-
-The above example creates a tryjob that tests revision 369416 on the eve board,
-passing in the extra arguments (`--extra_change_lists` and `--options`). The
-tryjob is then inserted into the file passed in via `--status_file`.
-
-For help with the command line arguments of the script, run:
-
-```
-$ ./modify_a_tryjob.py --help
-```
-
-For example, to remove a tryjob that tested revision 369412:
-
-```
-$ ./modify_a_tryjob.py \
-  --modify_a_tryjob remove \
-  --revision 369412 \
-  --status_file /abs/path/to/tryjob/file
-```
-
-For example, to relaunch a tryjob that tested revision 369418:
-
-```
-$ ./modify_a_tryjob.py \
-  --modify_a_tryjob relaunch \
-  --revision 369418 \
-  --status_file /abs/path/to/tryjob/file
-```
+TODO(ryanbeltran): Please write some docs here.
 
 ## Other Helpful Scripts
 
@@ -428,26 +247,23 @@ r387778
 **Tip**: if you put a symlink called `git-llvm-rev` to this script somewhere on
 your `$PATH`, you can also use it as `git llvm-rev`.
 
-### `get_upstream_patch.py`
+### `get_patch.py`
 
 #### Usage
 
-This script updates the proper ChromeOS packages with LLVM patches of your choosing, and
-copies the patches into patch folders of the packages. This tool supports both git hash
-of commits as well as differential reviews.
+This script updates the proper ChromeOS packages with LLVM patches of your
+choosing, and copies the patches into patch folders of the packages. This tool
+supports both git hash of commits as well as differential reviews.
 
 Usage:
 
 ```
-./get_upstream_patch.py --chromeos_path /abs/path/to/chroot --start_sha llvm
---sha 174c3eb69f19ff2d6a3eeae31d04afe77e62c021 --sha 174c3eb69f19ff2d6a3eeae31d04afe77e62c021
---differential D123456
+get_patch.py --start-ref="HEAD" 47413bb27 p:74791
 ```
 
-It tries to autodetect a lot of things (e.g., packages changed by each sha,
-their ebuild paths, the "start"/"end" revisions to set, etc.) By default the
-script creates a local patch. Use --create_cl option to create a CL instead. For
-more information, please see the `--help`
+It tries to autodetect a lot of things. For more information, please see the
+`--help`. This only pulls down the patches into the current tree, commits and
+uploads must be done manually.
 
 ### `revert_checker.py`
 
diff --git a/llvm_tools/atomic_write_file_unittest.py b/llvm_tools/atomic_write_file_test.py
old mode 100755
new mode 100644
similarity index 94%
rename from llvm_tools/atomic_write_file_unittest.py
rename to llvm_tools/atomic_write_file_test.py
index cc0bb5e4..42f635c8
--- a/llvm_tools/atomic_write_file_unittest.py
+++ b/llvm_tools/atomic_write_file_test.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2023 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -9,7 +8,7 @@ from pathlib import Path
 import tempfile
 import unittest
 
-import atomic_write_file
+from llvm_tools import atomic_write_file
 
 
 class TestAtomicWrite(unittest.TestCase):
@@ -45,7 +44,3 @@ class TestAtomicWrite(unittest.TestCase):
             with filepath.open(encoding="utf-8") as f:
                 lines = f.readlines()
             self.assertEqual(lines[0], new_contents)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/auto_llvm_bisection.py b/llvm_tools/auto_llvm_bisection.py
deleted file mode 100755
index 2b4f6f99..00000000
--- a/llvm_tools/auto_llvm_bisection.py
+++ /dev/null
@@ -1,211 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Performs bisection on LLVM based off a .JSON file."""
-
-import enum
-import json
-import os
-import subprocess
-import sys
-import time
-import traceback
-
-import chroot
-import llvm_bisection
-import update_tryjob_status
-
-
-# Used to re-try for 'llvm_bisection.py' to attempt to launch more tryjobs.
-BISECTION_RETRY_TIME_SECS = 10 * 60
-
-# Wait time to then poll each tryjob whose 'status' value is 'pending'.
-POLL_RETRY_TIME_SECS = 30 * 60
-
-# The number of attempts for 'llvm_bisection.py' to launch more tryjobs.
-#
-# It is reset (break out of the `for` loop/ exit the program) if successfully
-# launched more tryjobs or bisection is finished (no more revisions between
-# start and end of the bisection).
-BISECTION_ATTEMPTS = 3
-
-# The limit for updating all tryjobs whose 'status' is 'pending'.
-#
-# If the time that has passed for polling exceeds this value, then the program
-# will exit with the appropriate exit code.
-POLLING_LIMIT_SECS = 18 * 60 * 60
-
-
-class BuilderStatus(enum.Enum):
-    """Actual values given via 'cros buildresult'."""
-
-    PASS = "pass"
-    FAIL = "fail"
-    RUNNING = "running"
-
-
-# Writing a dict with `.value`s spelled out makes `black`'s style conflict with
-# `cros lint`'s diagnostics.
-builder_status_mapping = {
-    a.value: b.value
-    for a, b in (
-        (BuilderStatus.PASS, update_tryjob_status.TryjobStatus.GOOD),
-        (BuilderStatus.FAIL, update_tryjob_status.TryjobStatus.BAD),
-        (BuilderStatus.RUNNING, update_tryjob_status.TryjobStatus.PENDING),
-    )
-}
-
-
-def GetBuildResult(chromeos_path, buildbucket_id):
-    """Returns the conversion of the result of 'cros buildresult'."""
-
-    # Calls 'cros buildresult' to get the status of the tryjob.
-    try:
-        tryjob_json = subprocess.check_output(
-            [
-                "cros",
-                "buildresult",
-                "--buildbucket-id",
-                str(buildbucket_id),
-                "--report",
-                "json",
-            ],
-            cwd=chromeos_path,
-            stderr=subprocess.STDOUT,
-            encoding="utf-8",
-        )
-    except subprocess.CalledProcessError as err:
-        if "No build found. Perhaps not started" not in err.output:
-            raise
-        return None
-
-    tryjob_content = json.loads(tryjob_json)
-
-    build_result = str(tryjob_content["%d" % buildbucket_id]["status"])
-
-    # The string returned by 'cros buildresult' might not be in the mapping.
-    if build_result not in builder_status_mapping:
-        raise ValueError(
-            '"cros buildresult" return value is invalid: %s' % build_result
-        )
-
-    return builder_status_mapping[build_result]
-
-
-def main():
-    """Bisects LLVM using the result of `cros buildresult` of each tryjob.
-
-    Raises:
-        AssertionError: The script was run inside the chroot.
-    """
-
-    chroot.VerifyOutsideChroot()
-
-    args_output = llvm_bisection.GetCommandLineArgs()
-
-    chroot.VerifyChromeOSRoot(args_output.chromeos_path)
-
-    if os.path.isfile(args_output.last_tested):
-        print("Resuming bisection for %s" % args_output.last_tested)
-    else:
-        print("Starting a new bisection for %s" % args_output.last_tested)
-
-    while True:
-        # Update the status of existing tryjobs
-        if os.path.isfile(args_output.last_tested):
-            update_start_time = time.time()
-            with open(args_output.last_tested, encoding="utf-8") as json_file:
-                json_dict = json.load(json_file)
-            while True:
-                print(
-                    '\nAttempting to update all tryjobs whose "status" is '
-                    '"pending":'
-                )
-                print("-" * 40)
-
-                completed = True
-                for tryjob in json_dict["jobs"]:
-                    if (
-                        tryjob["status"]
-                        == update_tryjob_status.TryjobStatus.PENDING.value
-                    ):
-                        status = GetBuildResult(
-                            args_output.chromeos_path, tryjob["buildbucket_id"]
-                        )
-                        if status:
-                            tryjob["status"] = status
-                        else:
-                            completed = False
-
-                print("-" * 40)
-
-                # Proceed to the next step if all the existing tryjobs have
-                # completed.
-                if completed:
-                    break
-
-                delta_time = time.time() - update_start_time
-
-                if delta_time > POLLING_LIMIT_SECS:
-                    # Something is wrong with updating the tryjobs's 'status'
-                    # via `cros buildresult` (e.g. network issue, etc.).
-                    sys.exit("Failed to update pending tryjobs.")
-
-                print("-" * 40)
-                print("Sleeping for %d minutes." % (POLL_RETRY_TIME_SECS // 60))
-                time.sleep(POLL_RETRY_TIME_SECS)
-
-            # There should always be update from the tryjobs launched in the
-            # last iteration.
-            temp_filename = "%s.new" % args_output.last_tested
-            with open(temp_filename, "w", encoding="utf-8") as temp_file:
-                json.dump(
-                    json_dict, temp_file, indent=4, separators=(",", ": ")
-                )
-            os.rename(temp_filename, args_output.last_tested)
-
-        # Launch more tryjobs.
-        bisection_complete = (
-            llvm_bisection.BisectionExitStatus.BISECTION_COMPLETE.value
-        )
-        for cur_try in range(1, BISECTION_ATTEMPTS + 1):
-            try:
-                print("\nAttempting to launch more tryjobs if possible:")
-                print("-" * 40)
-
-                bisection_ret = llvm_bisection.main(args_output)
-
-                print("-" * 40)
-
-                # Stop if the bisection has completed.
-                if bisection_ret == bisection_complete:
-                    sys.exit(0)
-
-                # Successfully launched more tryjobs.
-                break
-            except Exception:
-                traceback.print_exc()
-
-                print("-" * 40)
-
-                # Exceeded the number of times to launch more tryjobs.
-                if cur_try == BISECTION_ATTEMPTS:
-                    sys.exit("Unable to continue bisection.")
-
-                num_retries_left = BISECTION_ATTEMPTS - cur_try
-
-                print(
-                    "Retries left to continue bisection %d." % num_retries_left
-                )
-
-                print(
-                    "Sleeping for %d minutes."
-                    % (BISECTION_RETRY_TIME_SECS // 60)
-                )
-                time.sleep(BISECTION_RETRY_TIME_SECS)
-
-
-if __name__ == "__main__":
-    main()
diff --git a/llvm_tools/auto_llvm_bisection_unittest.py b/llvm_tools/auto_llvm_bisection_unittest.py
deleted file mode 100755
index 57837ddf..00000000
--- a/llvm_tools/auto_llvm_bisection_unittest.py
+++ /dev/null
@@ -1,286 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Tests for auto bisection of LLVM."""
-
-import json
-import os
-import subprocess
-import time
-import traceback
-import unittest
-from unittest import mock
-
-import auto_llvm_bisection
-import chroot
-import llvm_bisection
-import test_helpers
-import update_tryjob_status
-
-
-class AutoLLVMBisectionTest(unittest.TestCase):
-    """Unittests for auto bisection of LLVM."""
-
-    @mock.patch.object(chroot, "VerifyChromeOSRoot")
-    @mock.patch.object(chroot, "VerifyOutsideChroot", return_value=True)
-    @mock.patch.object(
-        llvm_bisection,
-        "GetCommandLineArgs",
-        return_value=test_helpers.ArgsOutputTest(),
-    )
-    @mock.patch.object(time, "sleep")
-    @mock.patch.object(traceback, "print_exc")
-    @mock.patch.object(llvm_bisection, "main")
-    @mock.patch.object(os.path, "isfile")
-    @mock.patch.object(auto_llvm_bisection, "open")
-    @mock.patch.object(json, "load")
-    @mock.patch.object(auto_llvm_bisection, "GetBuildResult")
-    @mock.patch.object(os, "rename")
-    def testAutoLLVMBisectionPassed(
-        self,
-        # pylint: disable=unused-argument
-        mock_rename,
-        mock_get_build_result,
-        mock_json_load,
-        # pylint: disable=unused-argument
-        mock_open,
-        mock_isfile,
-        mock_llvm_bisection,
-        mock_traceback,
-        mock_sleep,
-        mock_get_args,
-        mock_outside_chroot,
-        mock_chromeos_root,
-    ):
-        mock_isfile.side_effect = [False, False, True, True]
-        mock_llvm_bisection.side_effect = [
-            0,
-            ValueError("Failed to launch more tryjobs."),
-            llvm_bisection.BisectionExitStatus.BISECTION_COMPLETE.value,
-        ]
-        mock_json_load.return_value = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [
-                {
-                    "buildbucket_id": 12345,
-                    "rev": 369411,
-                    "status": update_tryjob_status.TryjobStatus.PENDING.value,
-                }
-            ],
-        }
-        mock_get_build_result.return_value = (
-            update_tryjob_status.TryjobStatus.GOOD.value
-        )
-
-        # Verify the excpetion is raised when successfully found the bad
-        # revision. Uses `sys.exit(0)` to indicate success.
-        with self.assertRaises(SystemExit) as err:
-            auto_llvm_bisection.main()
-
-        self.assertEqual(err.exception.code, 0)
-
-        mock_outside_chroot.assert_called_once()
-        mock_get_args.assert_called_once()
-        self.assertEqual(mock_isfile.call_count, 3)
-        self.assertEqual(mock_llvm_bisection.call_count, 3)
-        mock_traceback.assert_called_once()
-        mock_sleep.assert_called_once()
-
-    @mock.patch.object(chroot, "VerifyChromeOSRoot")
-    @mock.patch.object(chroot, "VerifyOutsideChroot", return_value=True)
-    @mock.patch.object(time, "sleep")
-    @mock.patch.object(traceback, "print_exc")
-    @mock.patch.object(llvm_bisection, "main")
-    @mock.patch.object(os.path, "isfile")
-    @mock.patch.object(
-        llvm_bisection,
-        "GetCommandLineArgs",
-        return_value=test_helpers.ArgsOutputTest(),
-    )
-    def testFailedToStartBisection(
-        self,
-        mock_get_args,
-        mock_isfile,
-        mock_llvm_bisection,
-        mock_traceback,
-        mock_sleep,
-        mock_outside_chroot,
-        mock_chromeos_root,
-    ):
-        mock_isfile.return_value = False
-        mock_llvm_bisection.side_effect = ValueError(
-            "Failed to launch more tryjobs."
-        )
-
-        # Verify the exception is raised when the number of attempts to launched
-        # more tryjobs is exceeded, so unable to continue
-        # bisection.
-        with self.assertRaises(SystemExit) as err:
-            auto_llvm_bisection.main()
-
-        self.assertEqual(err.exception.code, "Unable to continue bisection.")
-
-        mock_chromeos_root.assert_called_once()
-        mock_outside_chroot.assert_called_once()
-        mock_get_args.assert_called_once()
-        self.assertEqual(mock_isfile.call_count, 2)
-        self.assertEqual(mock_llvm_bisection.call_count, 3)
-        self.assertEqual(mock_traceback.call_count, 3)
-        self.assertEqual(mock_sleep.call_count, 2)
-
-    @mock.patch.object(chroot, "VerifyChromeOSRoot")
-    @mock.patch.object(chroot, "VerifyOutsideChroot", return_value=True)
-    @mock.patch.object(
-        llvm_bisection,
-        "GetCommandLineArgs",
-        return_value=test_helpers.ArgsOutputTest(),
-    )
-    @mock.patch.object(time, "time")
-    @mock.patch.object(time, "sleep")
-    @mock.patch.object(os.path, "isfile")
-    @mock.patch.object(auto_llvm_bisection, "open")
-    @mock.patch.object(json, "load")
-    @mock.patch.object(auto_llvm_bisection, "GetBuildResult")
-    def testFailedToUpdatePendingTryJobs(
-        self,
-        mock_get_build_result,
-        mock_json_load,
-        # pylint: disable=unused-argument
-        mock_open,
-        mock_isfile,
-        mock_sleep,
-        mock_time,
-        mock_get_args,
-        mock_outside_chroot,
-        mock_chromeos_root,
-    ):
-        # Simulate behavior of `time.time()` for time passed.
-        @test_helpers.CallCountsToMockFunctions
-        def MockTimePassed(call_count):
-            if call_count < 3:
-                return call_count
-
-            assert False, "Called `time.time()` more than expected."
-
-        mock_isfile.return_value = True
-        mock_json_load.return_value = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [
-                {
-                    "buildbucket_id": 12345,
-                    "rev": 369411,
-                    "status": update_tryjob_status.TryjobStatus.PENDING.value,
-                }
-            ],
-        }
-        mock_get_build_result.return_value = None
-        mock_time.side_effect = MockTimePassed
-        # Reduce the polling limit for the test case to terminate faster.
-        auto_llvm_bisection.POLLING_LIMIT_SECS = 1
-
-        # Verify the exception is raised when unable to update tryjobs whose
-        # 'status' value is 'pending'.
-        with self.assertRaises(SystemExit) as err:
-            auto_llvm_bisection.main()
-
-        self.assertEqual(
-            err.exception.code, "Failed to update pending tryjobs."
-        )
-
-        mock_outside_chroot.assert_called_once()
-        mock_get_args.assert_called_once()
-        self.assertEqual(mock_isfile.call_count, 2)
-        mock_sleep.assert_called_once()
-        self.assertEqual(mock_time.call_count, 3)
-
-    @mock.patch.object(subprocess, "check_output")
-    def testGetBuildResult(self, mock_chroot_command):
-        buildbucket_id = 192
-        status = auto_llvm_bisection.BuilderStatus.PASS.value
-        tryjob_contents = {buildbucket_id: {"status": status}}
-        mock_chroot_command.return_value = json.dumps(tryjob_contents)
-        chroot_path = "/some/path/to/chroot"
-
-        self.assertEqual(
-            auto_llvm_bisection.GetBuildResult(chroot_path, buildbucket_id),
-            update_tryjob_status.TryjobStatus.GOOD.value,
-        )
-
-        mock_chroot_command.assert_called_once_with(
-            [
-                "cros",
-                "buildresult",
-                "--buildbucket-id",
-                str(buildbucket_id),
-                "--report",
-                "json",
-            ],
-            cwd="/some/path/to/chroot",
-            stderr=subprocess.STDOUT,
-            encoding="utf-8",
-        )
-
-    @mock.patch.object(subprocess, "check_output")
-    def testGetBuildResultPassedWithUnstartedTryjob(self, mock_chroot_command):
-        buildbucket_id = 192
-        chroot_path = "/some/path/to/chroot"
-        mock_chroot_command.side_effect = subprocess.CalledProcessError(
-            returncode=1, cmd=[], output="No build found. Perhaps not started"
-        )
-        auto_llvm_bisection.GetBuildResult(chroot_path, buildbucket_id)
-        mock_chroot_command.assert_called_once_with(
-            [
-                "cros",
-                "buildresult",
-                "--buildbucket-id",
-                "192",
-                "--report",
-                "json",
-            ],
-            cwd=chroot_path,
-            stderr=subprocess.STDOUT,
-            encoding="utf-8",
-        )
-
-    @mock.patch.object(subprocess, "check_output")
-    def testGetBuildReusultFailedWithInvalidBuildStatus(
-        self, mock_chroot_command
-    ):
-        chroot_path = "/some/path/to/chroot"
-        buildbucket_id = 50
-        invalid_build_status = "querying"
-        tryjob_contents = {buildbucket_id: {"status": invalid_build_status}}
-        mock_chroot_command.return_value = json.dumps(tryjob_contents)
-
-        # Verify an exception is raised when the return value of `cros
-        # buildresult` is not in the `builder_status_mapping`.
-        with self.assertRaises(ValueError) as err:
-            auto_llvm_bisection.GetBuildResult(chroot_path, buildbucket_id)
-
-        self.assertEqual(
-            str(err.exception),
-            '"cros buildresult" return value is invalid: %s'
-            % invalid_build_status,
-        )
-
-        mock_chroot_command.assert_called_once_with(
-            [
-                "cros",
-                "buildresult",
-                "--buildbucket-id",
-                str(buildbucket_id),
-                "--report",
-                "json",
-            ],
-            cwd=chroot_path,
-            stderr=subprocess.STDOUT,
-            encoding="utf-8",
-        )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/bb_add.py b/llvm_tools/bb_add.py
old mode 100755
new mode 100644
index 1ff83490..d90714e4
--- a/llvm_tools/bb_add.py
+++ b/llvm_tools/bb_add.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -8,20 +7,40 @@
 import argparse
 import logging
 import os
+from pathlib import Path
 import shlex
-import sys
 from typing import Iterable, List
 
-import cros_cls
-import llvm_next
+from llvm_tools import chroot
+from llvm_tools import cros_cls
+from llvm_tools import get_llvm_hash
+from llvm_tools import llvm_next
+
+
+DEFAULT_LLVM_NEXT_BUILDERS = ("chromeos/staging/staging-build-chromiumos-sdk",)
 
 
 def generate_bb_add_command(
     use_llvm_next: bool,
-    disable_werror: bool,
     extra_cls: Iterable[cros_cls.ChangeListURL],
     bots: Iterable[str],
+    tags: Iterable[str],
 ) -> List[str]:
+    """Generates a `bb add` command.
+
+    Args:
+        use_llvm_next: if True, all current llvm-next CLs will be added to the
+            run.
+        extra_cls: A list of extra CLs to add to the run.
+        bots: Bots that should be spawned by this command, e.g.,
+            `chromeos/staging/staging-build-chromiumos-sdk`.
+        tags: Tags that should be applied to the bot invocation(s). This can
+            make searching for the invocations easier using tools like `bb ls`.
+
+    Returns:
+        A command that would spawn the requested builders in the requested
+        configuration.
+    """
     cls: List[cros_cls.ChangeListURL] = []
     if use_llvm_next:
         if not llvm_next.LLVM_NEXT_TESTING_CLS:
@@ -30,39 +49,73 @@ def generate_bb_add_command(
             )
         cls += llvm_next.LLVM_NEXT_TESTING_CLS
 
-    if disable_werror:
-        cls.append(llvm_next.DISABLE_WERROR_CL)
-
     if extra_cls:
         cls += extra_cls
 
     cmd = ["bb", "add"]
     for cl in cls:
         cmd += ("-cl", cl.crrev_url_without_http())
+
+    for tag in tags:
+        cmd += ("-t", tag)
     cmd += bots
     return cmd
 
 
-def main(argv: List[str]) -> None:
-    logging.basicConfig(
-        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
-        "%(message)s",
-        level=logging.INFO,
+def is_pointless_llvm_next_invocation(chromeos_tree: Path) -> bool:
+    """Returns False if llvm-next testing is likely to be useful."""
+    if not llvm_next.LLVM_NEXT_TESTING_CLS:
+        logging.info(
+            "Tests seem pointless: no llvm-next testing CLs are registered."
+        )
+        return True
+
+    current_hash = get_llvm_hash.LLVMHash().GetCrOSCurrentLLVMHash(
+        chromeos_tree
     )
+    if current_hash == llvm_next.LLVM_NEXT_HASH:
+        logging.info(
+            "Tests seem pointless: current LLVM hash (%s) is the same as "
+            "llvm-next",
+            current_hash,
+        )
+        return True
+    logging.info(
+        "Testing seems useful; llvm-next hash is %s", llvm_next.LLVM_NEXT_HASH
+    )
+    return False
+
 
+def parse_opts(argv: List[str]) -> argparse.Namespace:
     parser = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
     parser.add_argument(
-        "--llvm-next",
+        "--add-llvm-next-verification-builders",
         action="store_true",
-        help="Add the current llvm-next patch set.",
+        help="""
+        Add the default series of builders used to help verify llvm-next. Does
+        not imply --llvm-next.
+        """,
+    )
+    parser.add_argument(
+        "--chromeos-tree",
+        type=Path,
+        help="""
+        ChromeOS tree to make modifications in. Will be inferred if none is
+        passed.
+        """,
     )
     parser.add_argument(
-        "--disable-werror",
+        "--dry-run",
         action="store_true",
-        help="Add the 'disable -Werror' patch sets",
+        help="Print the `bb` command, rather than running it.",
+    )
+    parser.add_argument(
+        "--llvm-next",
+        action="store_true",
+        help="Add the current llvm-next patch set.",
     )
     parser.add_argument(
         "--cl",
@@ -73,19 +126,72 @@ def main(argv: List[str]) -> None:
         form crrev.com/c/123456.
         """,
     )
-    parser.add_argument("bot", nargs="+", help="Bot(s) to run `bb add` with.")
+    parser.add_argument(
+        "--skip-if-pointless",
+        action="store_true",
+        help="""
+        If this is passed, the `bb add` will be skipped. It's an error to pass
+        this flag without `--llvm-next`.
+        """,
+    )
+    parser.add_argument(
+        "--tag",
+        action="append",
+        help="""
+        Tag to add to the `bb add` invocation. May be specified multiple times.
+        Tags are arbitrary text.
+        """,
+    )
+    parser.add_argument(
+        "bot", nargs="*", default=[], help="Bot(s) to run `bb add` with."
+    )
     opts = parser.parse_args(argv)
 
+    if opts.skip_if_pointless and not opts.llvm_next:
+        parser.error("--skip-if-pointless may only be used with --llvm-next.")
+
+    if opts.add_llvm_next_verification_builders:
+        opts.bot += DEFAULT_LLVM_NEXT_BUILDERS
+
+    if not opts.bot:
+        parser.error("At least one bot must be specified.")
+
+    if not opts.chromeos_tree:
+        opts.chromeos_tree = chroot.FindChromeOSRootAboveToolchainUtils()
+
+    return opts
+
+
+def main(argv: List[str]) -> None:
+    logging.basicConfig(
+        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
+        "%(message)s",
+        level=logging.INFO,
+    )
+
+    opts = parse_opts(argv)
+
+    if opts.skip_if_pointless and is_pointless_llvm_next_invocation(
+        opts.chromeos_tree
+    ):
+        logging.info(
+            "--skip-if-pointless passed for pointless invocation; quit."
+        )
+        return
+
     cmd = generate_bb_add_command(
         use_llvm_next=opts.llvm_next,
-        disable_werror=opts.disable_werror,
         extra_cls=opts.cl,
         bots=opts.bot,
+        tags=opts.tag or (),
     )
+
+    if opts.dry_run:
+        logging.info(
+            "--dry-run specified; would run: `%s` otherwise", shlex.join(cmd)
+        )
+        return
+
     logging.info("Running `bb add` command: %s...", shlex.join(cmd))
     # execvp raises if it fails, so no need to check.
     os.execvp(cmd[0], cmd)
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/llvm_tools/bb_add_test.py b/llvm_tools/bb_add_test.py
old mode 100755
new mode 100644
index 6f7fe6a7..01d03924
--- a/llvm_tools/bb_add_test.py
+++ b/llvm_tools/bb_add_test.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -8,9 +7,9 @@
 from typing import Iterable
 import unittest
 
-import bb_add
-import cros_cls
-import llvm_next
+from llvm_tools import bb_add
+from llvm_tools import cros_cls
+from llvm_tools import llvm_next
 
 
 _ARBITRARY_BOTS = ["chromeos/cq/amd64-generic-cq"]
@@ -35,30 +34,32 @@ class Test(unittest.TestCase):
         ):
             bb_add.generate_bb_add_command(
                 use_llvm_next=True,
-                disable_werror=False,
                 extra_cls=(),
                 bots=_ARBITRARY_BOTS,
+                tags=(),
             )
 
     def test_generate_bb_add_adds_llvm_next_cls(self):
         self.set_llvm_next_cls((cros_cls.ChangeListURL(123, 1),))
         cmd = bb_add.generate_bb_add_command(
             use_llvm_next=True,
-            disable_werror=False,
             extra_cls=(),
             bots=_ARBITRARY_BOTS,
+            tags=(),
         )
         self.assertEqual(
             cmd, ["bb", "add", "-cl", "crrev.com/c/123/1"] + _ARBITRARY_BOTS
         )
 
-    def test_generate_bb_add_adds_disable_werror_cl(self):
-        self.set_llvm_next_cls((cros_cls.ChangeListURL(123, 1),))
+    def test_generate_bb_add_adds_extra_cls(self):
         cmd = bb_add.generate_bb_add_command(
             use_llvm_next=False,
-            disable_werror=True,
-            extra_cls=(),
+            extra_cls=(
+                cros_cls.ChangeListURL(123, 1),
+                cros_cls.ChangeListURL(126),
+            ),
             bots=_ARBITRARY_BOTS,
+            tags=(),
         )
         self.assertEqual(
             cmd,
@@ -66,21 +67,19 @@ class Test(unittest.TestCase):
                 "bb",
                 "add",
                 "-cl",
-                llvm_next.DISABLE_WERROR_CL.crrev_url_without_http(),
+                "crrev.com/c/123/1",
+                "-cl",
+                "crrev.com/c/126",
             ]
             + _ARBITRARY_BOTS,
         )
 
-    def test_generate_bb_add_adds_extra_cls(self):
-        self.set_llvm_next_cls((cros_cls.ChangeListURL(123, 1),))
+    def test_use_of_tags(self):
         cmd = bb_add.generate_bb_add_command(
             use_llvm_next=False,
-            disable_werror=False,
-            extra_cls=(
-                cros_cls.ChangeListURL(123, 1),
-                cros_cls.ChangeListURL(126),
-            ),
+            extra_cls=(cros_cls.ChangeListURL(126),),
             bots=_ARBITRARY_BOTS,
+            tags=("custom-tag",),
         )
         self.assertEqual(
             cmd,
@@ -88,13 +87,9 @@ class Test(unittest.TestCase):
                 "bb",
                 "add",
                 "-cl",
-                "crrev.com/c/123/1",
-                "-cl",
                 "crrev.com/c/126",
+                "-t",
+                "custom-tag",
             ]
             + _ARBITRARY_BOTS,
         )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/check_clang_diags.py b/llvm_tools/check_clang_diags.py
old mode 100755
new mode 100644
index a33a1274..3d0b7414
--- a/llvm_tools/check_clang_diags.py
+++ b/llvm_tools/check_clang_diags.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2022 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -219,7 +218,3 @@ def main(argv: List[str]) -> None:
         with open(new_state_file_path, "w", encoding="utf-8") as f:
             json.dump(new_state_file, f)
         os.rename(new_state_file_path, state_file)
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/llvm_tools/check_clang_diags_test.py b/llvm_tools/check_clang_diags_test.py
old mode 100755
new mode 100644
index e254f204..8ed1f2ea
--- a/llvm_tools/check_clang_diags_test.py
+++ b/llvm_tools/check_clang_diags_test.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2022 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -9,8 +8,8 @@ import textwrap
 import unittest
 from unittest import mock
 
-import check_clang_diags
 from cros_utils import bugs
+from llvm_tools import check_clang_diags
 
 
 # pylint: disable=protected-access
@@ -106,7 +105,3 @@ class Test(unittest.TestCase):
         self.assertEqual(
             len(create_new_bug_mock.call_args_list), len(expected_calls)
         )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/chroot.py b/llvm_tools/chroot.py
old mode 100755
new mode 100644
index a104bd63..ebecef0b
--- a/llvm_tools/chroot.py
+++ b/llvm_tools/chroot.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2020 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -7,8 +6,8 @@
 
 import os
 from pathlib import Path
-import subprocess
-from typing import Iterable, List, Union
+
+from cros_utils import cros_paths
 
 
 def InChroot() -> bool:
@@ -22,7 +21,6 @@ def VerifyInsideChroot() -> None:
     Raises:
         AssertionError: The script was run outside the chroot.
     """
-
     assert InChroot(), "Script should be run inside the chroot."
 
 
@@ -32,21 +30,14 @@ def VerifyOutsideChroot() -> None:
     Raises:
         AssertionError: The script was run inside the chroot.
     """
-
     assert not InChroot(), "Script should be run outside the chroot."
 
 
-def VerifyChromeOSRoot(chromeos_root: Union[Path, str]) -> None:
-    """Checks whether the path actually points to ChromiumOS checkout root.
-
-    Raises:
-        AssertionError: The path is not ChromiumOS checkout root.
-    """
-
-    subdir = "src/third_party/chromiumos-overlay"
-    path = Path(chromeos_root).expanduser() / subdir
-    msg = f"Wrong ChromeOS path. No {subdir} directory in {chromeos_root} ."
-    assert path.is_dir(), msg
+def IsChromeOSRoot(path: Path) -> bool:
+    """Returns True if `path` looks like the root of a CrOS checkout."""
+    # This could technically be more stringent with checking, but realistically
+    # should only be passed paths inside of CrOS trees.
+    return (path / ".repo").exists()
 
 
 def FindChromeOSRootAbove(chromeos_tree_path: Path) -> Path:
@@ -57,87 +48,22 @@ def FindChromeOSRootAbove(chromeos_tree_path: Path) -> Path:
     Raises:
         ValueError if the given path is not in a ChromeOS tree.
     """
-    if (chromeos_tree_path / ".repo").exists():
+    if IsChromeOSRoot(chromeos_tree_path):
         return chromeos_tree_path
 
     for parent in chromeos_tree_path.parents:
-        if (parent / ".repo").exists():
+        if IsChromeOSRoot(parent):
             return parent
     raise ValueError(f"{chromeos_tree_path} is not in a repo checkout")
 
 
-def GetChrootEbuildPaths(
-    chromeos_root: Union[Path, str],
-    packages: Iterable[str],
-    chroot_name: str = "chroot",
-    out_dir: str = "out",
-) -> List[str]:
-    """Gets the chroot path(s) of the package(s).
-
-    Args:
-        chromeos_root: The absolute path to the chromeos tree to use.
-        packages: A list of a package/packages to
-        be used to find their chroot path.
-        chroot_name: name of the chroot to enter.
-        out_dir: name of the out directory for the chroot.
-
-    Returns:
-        A list of chroot paths of the packages' ebuild files.
+def FindChromeOSRootAboveToolchainUtils() -> Path:
+    """Returns the root of the ChromeOS tree that this checkout exists in.
 
     Raises:
-        ValueError: Failed to get the chroot path of a package.
+        ValueError if this checkout is not in a ChromeOS tree.
     """
-
-    chroot_paths = []
-
-    cros_sdk = [
-        "cros_sdk",
-        f"--chroot={chroot_name}",
-        f"--out-dir={out_dir}",
-    ]
-
-    # Find the chroot path for each package's ebuild.
-    for package in packages:
-        chroot_path = subprocess.check_output(
-            cros_sdk + ["--", "equery", "w", package],
-            cwd=chromeos_root,
-            encoding="utf-8",
-        )
-        chroot_paths.append(chroot_path.strip())
-
-    return chroot_paths
-
-
-def ConvertChrootPathsToAbsolutePaths(
-    chromeos_root: str,
-    chroot_paths: List[str],
-) -> List[str]:
-    """Converts the chroot path(s) to absolute symlink path(s).
-
-    Args:
-        chromeos_root: The absolute path to the chroot.
-        chroot_paths: A list of chroot paths to convert to absolute paths.
-
-    Returns:
-        A list of absolute path(s).
-
-    Raises:
-        ValueError: Invalid prefix for the chroot path or
-        invalid chroot paths were provided.
-    """
-
-    abs_paths = []
-    chroot_prefix = "/mnt/host/source/"
-    # Iterate through the chroot paths.
-    # For each chroot file path, remove '/mnt/host/source/' prefix
-    # and combine the chroot path with the result and add it to the list.
-    for chroot_path in chroot_paths:
-        if not chroot_path.startswith(chroot_prefix):
-            raise ValueError(
-                "Invalid prefix for the chroot path: %s" % chroot_path
-            )
-        rel_path = chroot_path[len(chroot_prefix) :]
-        # combine the chromeos root path + '/src/...'
-        abs_path = os.path.join(chromeos_root, rel_path)
-        abs_paths.append(abs_path)
-    return abs_paths
+    result = cros_paths.script_chromiumos_checkout()
+    if not result:
+        raise ValueError("This script's checkout is not part of a CrOS tree.")
+    return result
diff --git a/llvm_tools/chroot_test.py b/llvm_tools/chroot_test.py
new file mode 100644
index 00000000..11d1d025
--- /dev/null
+++ b/llvm_tools/chroot_test.py
@@ -0,0 +1,28 @@
+# Copyright 2020 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Tests for chroot.py."""
+
+from llvm_tools import chroot
+from llvm_tools import test_helpers
+
+
+class Test(test_helpers.TempDirTestCase):
+    """Tests for chroot.py."""
+
+    def test_chromeos_root_finding_works(self):
+        root = self.make_tempdir()
+        (root / ".repo").mkdir()
+        self.assertEqual(chroot.FindChromeOSRootAbove(root), root)
+
+        subdir = root / "foo" / "bar" / "baz"
+        subdir.mkdir(parents=True)
+        self.assertEqual(chroot.FindChromeOSRootAbove(subdir), root)
+
+    def test_chromeos_root_finding_raises_in_trivial_case(self):
+        root = self.make_tempdir()
+        subdir = root / "foo" / "bar" / "baz"
+        subdir.mkdir(parents=True)
+        with self.assertRaisesRegex(ValueError, "not in a repo checkout$"):
+            chroot.FindChromeOSRootAbove(subdir)
diff --git a/llvm_tools/chroot_unittest.py b/llvm_tools/chroot_unittest.py
deleted file mode 100755
index ff416b9f..00000000
--- a/llvm_tools/chroot_unittest.py
+++ /dev/null
@@ -1,71 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unit tests for chroot helper functions."""
-
-import subprocess
-import unittest
-from unittest import mock
-
-import chroot
-
-
-# These are unittests; protected access is OK to a point.
-# pylint: disable=protected-access
-
-
-class HelperFunctionsTest(unittest.TestCase):
-    """Test class for updating LLVM hashes of packages."""
-
-    @mock.patch.object(subprocess, "check_output")
-    def testSucceedsToGetChrootEbuildPathForPackage(self, mock_chroot_command):
-        package_chroot_path = "/chroot/path/to/package.ebuild"
-
-        # Emulate ChrootRunCommandWOutput behavior when a chroot path is found
-        # for a valid package.
-        mock_chroot_command.return_value = package_chroot_path
-
-        chroot_path = "/test/chroot/path"
-        package_list = ["new-test/package"]
-
-        self.assertEqual(
-            chroot.GetChrootEbuildPaths(chroot_path, package_list),
-            [package_chroot_path],
-        )
-
-        mock_chroot_command.assert_called_once()
-
-    def testFailedToConvertChrootPathWithInvalidPrefix(self):
-        chroot_path = "/path/to/chroot"
-        chroot_file_path = "/src/package.ebuild"
-
-        # Verify the exception is raised when a chroot path does not have the
-        # prefix '/mnt/host/source/'.
-        with self.assertRaises(ValueError) as err:
-            chroot.ConvertChrootPathsToAbsolutePaths(
-                chroot_path, [chroot_file_path]
-            )
-
-        self.assertEqual(
-            str(err.exception),
-            "Invalid prefix for the chroot path: " "%s" % chroot_file_path,
-        )
-
-    def testSucceedsToConvertChrootPathToAbsolutePath(self):
-        chroot_path = "/path/to/chroot"
-        chroot_file_paths = ["/mnt/host/source/src/package.ebuild"]
-
-        expected_abs_path = "/path/to/chroot/src/package.ebuild"
-
-        self.assertEqual(
-            chroot.ConvertChrootPathsToAbsolutePaths(
-                chroot_path, chroot_file_paths
-            ),
-            [expected_abs_path],
-        )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/clean_up_old_llvm_patches.py b/llvm_tools/clean_up_old_llvm_patches.py
old mode 100755
new mode 100644
index d1ae54b2..c326ebe8
--- a/llvm_tools/clean_up_old_llvm_patches.py
+++ b/llvm_tools/clean_up_old_llvm_patches.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -14,48 +13,34 @@ import re
 import subprocess
 import sys
 import textwrap
-from typing import List, Optional
+from typing import List
 
+from cros_utils import cros_paths
 from cros_utils import git_utils
-import patch_utils
+from llvm_tools import patch_utils
 
 
-# The chromiumos-overlay packages to GC patches in.
-PACKAGES_TO_COLLECT = patch_utils.CHROMEOS_PATCHES_JSON_PACKAGES
-
-# Folks who should be on the R-line of any CLs that get uploaded.
-CL_REVIEWERS = (git_utils.REVIEWER_DETECTIVE,)
-
 # Folks who should be on the CC-line of any CLs that get uploaded.
-CL_CC = ("gbiv@google.com",)
-
+CL_CC = ("gbiv@chromium.org",)
 
-def maybe_autodetect_cros_overlay(my_dir: Path) -> Optional[Path]:
-    third_party = my_dir.parent.parent
-    cros_overlay = third_party / "chromiumos-overlay"
-    if cros_overlay.exists():
-        return cros_overlay
-    return None
 
-
-def remove_old_patches(cros_overlay: Path, min_revision: int) -> bool:
+def remove_old_patches(toolchain_utils: Path, min_revision: int) -> bool:
     """Removes patches in cros_overlay. Returns whether changes were made."""
-    patches_removed = 0
-    for package in PACKAGES_TO_COLLECT:
-        logging.info("GC'ing patches from %s...", package)
-        patches_json = cros_overlay / package / "files/PATCHES.json"
-        removed_patch_files = patch_utils.remove_old_patches(
-            min_revision, patches_json
-        )
-        if not removed_patch_files:
-            logging.info("No patches removed from %s", patches_json)
-            continue
+    patches_json = (
+        toolchain_utils / cros_paths.DEFAULT_PATCHES_PATH_IN_TOOLCHAIN_UTILS
+    )
+    logging.info("GC'ing patches from %s...", patches_json)
+    removed_patch_files = patch_utils.remove_old_patches(
+        min_revision, patches_json
+    )
+    if not removed_patch_files:
+        logging.info("No patches removed")
+        return False
 
-        patches_removed += len(removed_patch_files)
-        for patch in removed_patch_files:
-            logging.info("Removing %s...", patch)
-            patch.unlink()
-    return patches_removed != 0
+    for patch in removed_patch_files:
+        logging.info("Removing %s...", patch)
+        patch.unlink()
+    return True
 
 
 def commit_changes(cros_overlay: Path, min_rev: int):
@@ -84,7 +69,6 @@ def upload_changes(cros_overlay: Path, autosubmit_cwd: Path) -> None:
         cros_overlay,
         remote="cros",
         branch="main",
-        reviewers=CL_REVIEWERS,
         cc=CL_CC,
     )
 
@@ -93,7 +77,7 @@ def upload_changes(cros_overlay: Path, autosubmit_cwd: Path) -> None:
 
     cl_id = cl_ids[0]
     logging.info("Uploaded CL http://crrev.com/c/%s successfully.", cl_id)
-    git_utils.try_set_autosubmit_labels(autosubmit_cwd, cl_id)
+    git_utils.set_autoreview_topic_and_labels(autosubmit_cwd, cl_id)
 
 
 def find_chromeos_llvm_version(chromiumos_overlay: Path) -> int:
@@ -120,6 +104,8 @@ def find_android_llvm_version(android_toolchain_tree: Path) -> int:
         android_toolchain_tree
         / "toolchain"
         / "llvm_android"
+        / "src"
+        / "llvm_android"
         / "android_version.py"
     )
 
@@ -153,7 +139,7 @@ def find_android_llvm_version(android_toolchain_tree: Path) -> int:
     return int(match.group(1))
 
 
-def get_opts(my_dir: Path, argv: List[str]) -> argparse.Namespace:
+def get_opts(argv: List[str]) -> argparse.Namespace:
     """Returns options for the script."""
 
     parser = argparse.ArgumentParser(
@@ -182,7 +168,16 @@ def get_opts(my_dir: Path, argv: List[str]) -> argparse.Namespace:
         type=Path,
         help="""
         Path to chromiumos-overlay. Will autodetect if none is specified. If
-        autodetection fails and none is specified, this script will fail.
+        autodetection fails, none is specified, and this flag is needed (for
+        minimum-revision autodetection), this script will fail.
+        """,
+    )
+    parser.add_argument(
+        "--toolchain-utils",
+        type=Path,
+        help="""
+        Path to toolchain-utils. Will use the toolchain-utils executing this if
+        none is specified.
         """,
     )
     parser.add_argument(
@@ -220,18 +215,25 @@ def get_opts(my_dir: Path, argv: List[str]) -> argparse.Namespace:
     )
     opts = parser.parse_args(argv)
 
-    if not opts.chromiumos_overlay:
-        maybe_overlay = maybe_autodetect_cros_overlay(my_dir)
-        if not maybe_overlay:
-            parser.error(
-                "Failed to autodetect --chromiumos-overlay; please pass a value"
-            )
-        opts.chromiumos_overlay = maybe_overlay
+    if not opts.toolchain_utils:
+        opts.toolchain_utils = cros_paths.script_toolchain_utils_root()
 
     if not opts.gerrit_tool_cwd:
         opts.gerrit_tool_cwd = opts.chromiumos_overlay
 
     if opts.autodetect_revision:
+        if not opts.chromiumos_overlay:
+            maybe_cros_root = cros_paths.script_chromiumos_checkout()
+            if not maybe_cros_root:
+                parser.error(
+                    "When using --autodetect-revision, this script must be "
+                    "run from within a CrOS checkout unless you specify "
+                    "--chromiumos-overlay."
+                )
+            opts.chromiumos_overlay = (
+                maybe_cros_root / cros_paths.CHROMIUMOS_OVERLAY
+            )
+
         if not opts.android_toolchain:
             parser.error(
                 "--android-toolchain must be passed with --autodetect-revision"
@@ -257,16 +259,14 @@ def main(argv: List[str]) -> None:
         level=logging.INFO,
     )
 
-    my_dir = Path(__file__).resolve().parent
-    opts = get_opts(my_dir, argv)
-
-    cros_overlay = opts.chromiumos_overlay
+    opts = get_opts(argv)
+    toolchain_utils = opts.toolchain_utils
     gerrit_tool_cwd = opts.gerrit_tool_cwd
     upload = opts.upload_with_autoreview
     commit = opts.commit or upload
     min_revision = opts.revision
 
-    made_changes = remove_old_patches(cros_overlay, min_revision)
+    made_changes = remove_old_patches(toolchain_utils, min_revision)
     if not made_changes:
         logging.info("No changes made; exiting.")
         return
@@ -278,15 +278,11 @@ def main(argv: List[str]) -> None:
         return
 
     logging.info("Committing changes...")
-    commit_changes(cros_overlay, min_revision)
+    commit_changes(toolchain_utils, min_revision)
     if not upload:
         logging.info("Change with removed patches has been committed locally.")
         return
 
     logging.info("Uploading changes...")
-    upload_changes(cros_overlay, gerrit_tool_cwd)
+    upload_changes(toolchain_utils, gerrit_tool_cwd)
     logging.info("Change sent for review.")
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/llvm_tools/clean_up_old_llvm_patches_test.py b/llvm_tools/clean_up_old_llvm_patches_test.py
old mode 100755
new mode 100644
index 02100c8f..3e090031
--- a/llvm_tools/clean_up_old_llvm_patches_test.py
+++ b/llvm_tools/clean_up_old_llvm_patches_test.py
@@ -1,16 +1,11 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
 """Tests for clean_up_old_llvm_patches"""
 
-from pathlib import Path
-import shutil
-import tempfile
-import unittest
-
-import clean_up_old_llvm_patches
+from llvm_tools import clean_up_old_llvm_patches
+from llvm_tools import test_helpers
 
 
 ANDROID_VERSION_PY_EXAMPLE = """
@@ -19,18 +14,18 @@ def get_svn_revision():
 """
 
 
-class Test(unittest.TestCase):
+class Test(test_helpers.TempDirTestCase):
     """Tests for clean_up_old_llvm_patches"""
 
-    def make_tempdir(self) -> Path:
-        tmpdir = Path(tempfile.mkdtemp(prefix="patch_utils_unittest"))
-        self.addCleanup(shutil.rmtree, tmpdir)
-        return tmpdir
-
     def test_android_version_autodetection(self):
         android_root = self.make_tempdir()
         android_version_py = (
-            android_root / "toolchain" / "llvm_android" / "android_version.py"
+            android_root
+            / "toolchain"
+            / "llvm_android"
+            / "src"
+            / "llvm_android"
+            / "android_version.py"
         )
         android_version_py.parent.mkdir(parents=True)
         android_version_py.write_text(
@@ -62,7 +57,3 @@ class Test(unittest.TestCase):
             ),
             123456,
         )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/copy_helpers_to_chromiumos_overlay.py b/llvm_tools/copy_helpers_to_chromiumos_overlay.py
old mode 100755
new mode 100644
index 59a664d5..edb0e219
--- a/llvm_tools/copy_helpers_to_chromiumos_overlay.py
+++ b/llvm_tools/copy_helpers_to_chromiumos_overlay.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -13,15 +12,10 @@ patch_manager ones). This script simplifies the copying of those around.
 
 import argparse
 import os
+from pathlib import Path
 import shutil
-import sys
 
-
-def _find_repo_root(script_root):
-    repo_root = os.path.abspath(os.path.join(script_root, "../../../../"))
-    if not os.path.isdir(os.path.join(repo_root, ".repo")):
-        return None
-    return repo_root
+from cros_utils import cros_paths
 
 
 def main():
@@ -33,21 +27,12 @@ def main():
     )
     args = parser.parse_args()
 
-    my_dir = os.path.abspath(os.path.dirname(__file__))
-
+    my_dir = Path(os.path.abspath(os.path.dirname(__file__)))
     repo_root = args.chromeos_path
     if repo_root is None:
-        repo_root = _find_repo_root(my_dir)
-        if repo_root is None:
-            sys.exit(
-                "Couldn't detect the CrOS checkout root; please provide a "
-                "value for --chromeos_path"
-            )
-
-    chromiumos_overlay = os.path.join(
-        repo_root, "src/third_party/chromiumos-overlay"
-    )
+        repo_root = cros_paths.script_chromiumos_checkout_or_exit()
 
+    chromiumos_overlay = repo_root / cros_paths.CHROMIUMOS_OVERLAY
     clone_files = [
         "failure_modes.py",
         "get_llvm_hash.py",
@@ -56,15 +41,9 @@ def main():
         "subprocess_helpers.py",
     ]
 
-    filesdir = os.path.join(
-        chromiumos_overlay, "sys-devel/llvm/files/patch_manager"
-    )
+    filesdir = chromiumos_overlay / "sys-devel/llvm/files/patch_manager"
     for f in clone_files:
-        source = os.path.join(my_dir, f)
-        dest = os.path.join(filesdir, f)
+        source = my_dir / f
+        dest = filesdir / f
         print("%r => %r" % (source, dest))
         shutil.copyfile(source, dest)
-
-
-if __name__ == "__main__":
-    main()
diff --git a/llvm_tools/create_patch_file.py b/llvm_tools/create_patch_file.py
new file mode 100644
index 00000000..afe99eda
--- /dev/null
+++ b/llvm_tools/create_patch_file.py
@@ -0,0 +1,484 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Create a PATCHES.json file from ChromiumOS LLVM branches.
+
+If a PATCHES.json file already exists, this script edits it with
+only new patches.
+
+Commits on these branches can have metedata footer entries such as:
+
+  patch.cherry: true
+  patch.version_range.from: 0
+  patch.version_range.until: null
+  patch.platforms: chromiumos, android
+
+Which will lead to different metadata in the corresponding PATCHS.json.
+"""
+
+import argparse
+import dataclasses
+import functools
+import json
+import logging
+from pathlib import Path
+import re
+from typing import Dict, Iterable, List, Optional, Set
+
+from cros_utils import cros_paths
+from cros_utils import git_utils
+from llvm_tools import atomic_write_file
+from llvm_tools import get_llvm_hash
+from llvm_tools import git_llvm_rev
+from llvm_tools import llvm_next
+from llvm_tools import patch_utils
+
+
+# Don't allow patches to have file names longer than this number of
+# characters. We should have some number here as titles
+# can be broken, but we also need it long enough to ensure
+# unique file names.
+_MAX_PATCH_NAME_LENGTH = 128
+
+# Default branch pattern to look for.
+_DEFAULT_BRANCH_PATTERN = "*/chromeos/llvm-*"
+
+
+_CHANGE_ID_REGEX = re.compile(r"^change-id:\s*\w+\s*$", re.IGNORECASE)
+_COMMIT_MESSAGE_END_GUESS = re.compile(r"^---(:? .*)?$")
+_CHROMEOS_SCOPED_EMAIL = (
+    "chromeos-scoped@luci-project-accounts.iam.gserviceaccount.com"
+)
+
+
+@dataclasses.dataclass
+class LLVMPatchContext:
+    """Information needed to reason about patches on LLVM branches."""
+
+    llvm_dir: Path
+    patch_dir: Path
+    branch_refs: Iterable[str]
+    main_branch_ref: str
+
+    def __post_init__(self):
+        self.llvm_config = git_llvm_rev.LLVMConfig(
+            git_llvm_rev.MAIN_BRANCH, self.llvm_dir
+        )
+
+
+@dataclasses.dataclass(frozen=True)
+class PatchCombo:
+    """Holds PatchEntry info and the actual git patch contents."""
+
+    entry: patch_utils.PatchEntry
+    contents: str
+
+
+@dataclasses.dataclass(frozen=True)
+class BranchContext:
+    """Information needed to create a singular branch from patches."""
+
+    branch_ref: str
+    merge_base: str
+    llvm_rev: git_llvm_rev.Rev
+    patch_entry_combos: List[PatchCombo]
+
+    @property
+    def patch_entries(self):
+        return [p.entry for p in self.patch_entry_combos]
+
+
+def _maybe_string_to_int(s: Optional[str]) -> Optional[int]:
+    if s is None:
+        return None
+    if s.lower() in {"null", "none"}:
+        return None
+    return int(s)
+
+
+def _get_platforms(commit_metadata: Dict[str, str]) -> List[str]:
+    return sorted(
+        p.strip()
+        for p in commit_metadata.get("patch.platforms", "chromiumos").split(",")
+        if p.strip()
+    )
+
+
+def _get_metadata_info(commit_metadata: Dict[str, str]) -> List[str]:
+    return [
+        p.strip()
+        for p in commit_metadata.get("patch.metadata.info", "").split(",")
+        if p.strip()
+    ]
+
+
+def _get_metadata_original_sha(
+    commit_metadata: Dict[str, str],
+) -> Optional[str]:
+    return commit_metadata.get("patch.metadata.original_sha")
+
+
+@functools.lru_cache
+def _translate_sha_to_rev_cached(
+    llvm_config: git_llvm_rev.LLVMConfig, sha: str
+):
+    return git_llvm_rev.translate_sha_to_rev(llvm_config, sha)
+
+
+def filter_change_id(patch_contents: str) -> str:
+    """Remove the Change-Id line from the commit message."""
+    out = []
+    passed_commit_message = False
+    for line in patch_contents.splitlines(keepends=True):
+        if _COMMIT_MESSAGE_END_GUESS.match(line):
+            passed_commit_message = True
+        elif not passed_commit_message and _CHANGE_ID_REGEX.match(line):
+            # Skip.
+            continue
+        out.append(line)
+    return "".join(out)
+
+
+def create_branch_contexts(
+    patch_context: LLVMPatchContext,
+) -> List[BranchContext]:
+    """Package all LLVM branch data into an easily usable BranchContext."""
+
+    # Compile this regex outside of the O(nm) loop.
+    replace_regex = re.compile(r"\W+")
+    entries: List[BranchContext] = []
+    for branch_ref in patch_context.branch_refs:
+        merge_base = git_utils.merge_base(
+            patch_context.llvm_dir, [patch_context.main_branch_ref, branch_ref]
+        )
+        if not merge_base:
+            logging.warning(
+                "No merge base between '%s' and '%s'. Skipping.",
+                patch_context.main_branch_ref,
+                branch_ref,
+            )
+            continue
+        logging.info(
+            "Merge base for '%s' and '%s': '%s'",
+            patch_context.main_branch_ref,
+            branch_ref,
+            merge_base,
+        )
+        commit_shas = list(
+            git_utils.commits_between(
+                patch_context.llvm_dir, merge_base, branch_ref
+            )
+        )
+        this_branch_combos: List[PatchCombo] = []
+        for commit_sha in commit_shas:
+            # Skip any commits made by the LUCI infrastructure. These should
+            # not exist, but if they do, ignore them.
+            if (
+                git_utils.commit_author_email(
+                    patch_context.llvm_dir,
+                    commit_sha,
+                )
+                == _CHROMEOS_SCOPED_EMAIL
+            ):
+                logging.warning(
+                    "commit %s was made by %s, which is invalid. Skipping.",
+                    commit_sha,
+                    _CHROMEOS_SCOPED_EMAIL,
+                )
+                continue
+            patch_raw_data = filter_change_id(
+                git_utils.format_patch(patch_context.llvm_dir, commit_sha)
+            )
+            commit_metadata = git_utils.parse_message_metadata(
+                patch_raw_data.splitlines()
+            )
+            subject = git_utils.get_message_subject(
+                patch_context.llvm_dir, commit_sha
+            )
+            # When we recreate cherrypicked commits, we want to ensure
+            # we preserve their original upstream SHAs when possible.
+            # This information is sometimes available in the metadata.
+            # We usually try to use the original_sha as the filename too,
+            # when available.
+            original_sha = _get_metadata_original_sha(commit_metadata)
+            if commit_metadata.get("patch.cherry", "false").lower() == "true":
+                if original_sha:
+                    rel_patch_path = f"cherry/{original_sha}.patch"
+                else:
+                    rel_patch_path = f"cherry/{commit_sha}.patch"
+            else:
+                cleaned_name = replace_regex.sub("-", subject)[
+                    : _MAX_PATCH_NAME_LENGTH + 1
+                ]
+                rel_patch_path = f"{cleaned_name}.patch"
+            metadata = {
+                "info": _get_metadata_info(commit_metadata),
+                "title": subject,
+            }
+            if original_sha:
+                metadata["original_sha"] = original_sha
+            entry = patch_utils.PatchEntry(
+                workdir=patch_context.patch_dir,
+                metadata=metadata,
+                rel_patch_path=rel_patch_path,
+                platforms=_get_platforms(commit_metadata),
+                version_range={
+                    "from": _maybe_string_to_int(
+                        commit_metadata.get("patch.version_range.from")
+                    ),
+                    "until": _maybe_string_to_int(
+                        commit_metadata.get("patch.version_range.until")
+                    ),
+                },
+            )
+            this_branch_combos.append(PatchCombo(entry, patch_raw_data))
+        entries.append(
+            BranchContext(
+                branch_ref=branch_ref,
+                merge_base=merge_base,
+                llvm_rev=_translate_sha_to_rev_cached(
+                    patch_context.llvm_config, merge_base
+                ),
+                patch_entry_combos=this_branch_combos,
+            )
+        )
+    return entries
+
+
+def find_new_patches(
+    branch_context: BranchContext,
+    existing_patches: List[patch_utils.PatchEntry],
+) -> List[PatchCombo]:
+    """Find unseen patches committed along a given branch."""
+
+    if not branch_context.patch_entry_combos:
+        # We may not have landed anything yet, so just skip this branch
+        # if so.
+        logging.info(
+            "No commits found on LLVM branch for '%s'. Skipping.",
+            branch_context.branch_ref,
+        )
+        return []
+    applicable_existing = [
+        p
+        for p in existing_patches
+        if p.can_patch_version(branch_context.llvm_rev.number)
+    ]
+    logging.debug("Found applicable patches:")
+    for patch in applicable_existing:
+        logging.debug("* %s", patch.title())
+    # We drop the base commit, which should always be the first one. We may
+    # want to have a more thorough check, but for now, we'll just have an
+    # assert.
+    starting_title = branch_context.patch_entry_combos[0].entry.title()
+    assert "base commit" in starting_title.lower(), (
+        "branch_patches did not start with a base commit"
+        f" (title was '{starting_title}')"
+    )
+    # The 1 + is to make sure we skip over the base commit.
+    len_of_existing_and_base = 1 + len(applicable_existing)
+    if len_of_existing_and_base > len(branch_context.patch_entry_combos):
+        logging.warning(
+            "Expected at least %s patches on branch, but found only %s. Did"
+            " you apply the patches from PATCHES.json to the '%s' branch?",
+            len_of_existing_and_base,
+            len(branch_context.patch_entry_combos),
+            branch_context.branch_ref,
+        )
+    new_patch_combos = branch_context.patch_entry_combos[
+        len_of_existing_and_base:
+    ]
+    if not new_patch_combos:
+        logging.info(
+            "No new patches on LLVM branch for '%s'.", branch_context.branch_ref
+        )
+        return []
+    logging.info(
+        "New patches on LLVM branch for '%s':", branch_context.branch_ref
+    )
+    for combo in new_patch_combos:
+        logging.info("* %s", combo.entry.title())
+    return new_patch_combos
+
+
+def _find_branch_refs(
+    llvm_dir: Path, branch_patterns: Optional[List[str]] = None
+) -> Set[str]:
+    """Return git branch refs which match the given patterns.
+
+    If 'branch_patterns' is not specified or is empty, use a default glob
+    pattern.
+    """
+    branch_patterns = (
+        branch_patterns if branch_patterns else [_DEFAULT_BRANCH_PATTERN]
+    )
+    branch_refs: Set[str] = set()
+    for branch_pattern in branch_patterns:
+        branch_refs.update(git_utils.branch_list(llvm_dir, branch_pattern))
+    return branch_refs
+
+
+def _find_new_patch_combos(
+    chromiumos_root: Path,
+    patch_context: LLVMPatchContext,
+    existing_patches: List[patch_utils.PatchEntry],
+    check_all_branches: bool = False,
+) -> List[PatchCombo]:
+    """Find applicable patches for each branch that need to be added."""
+    # Go through each branch, check if that branch is within the
+    # given bounds, then check if there's any new patches on each branch.
+    # If so, add them to the PATCHES.json and write their contents to
+    # the patch directory.
+    patches_for_each_branch = create_branch_contexts(patch_context)
+    new_patch_combos: List[PatchCombo] = []
+    if check_all_branches:
+        llvm_current_rev = 0
+        llvm_next_rev = float("inf")
+    else:
+        llvm_current_rev = git_llvm_rev.translate_sha_to_rev(
+            patch_context.llvm_config,
+            get_llvm_hash.GetCrOSCurrentLLVMHash(chromiumos_root),
+        ).number
+        llvm_next_rev = llvm_next.LLVM_NEXT_REV
+    for container in patches_for_each_branch:
+        if not llvm_current_rev <= container.llvm_rev.number <= llvm_next_rev:
+            logging.info(
+                "Skipping branch '%s': merge base is outside"
+                " current and next bounds [%s...%s]",
+                container.branch_ref,
+                llvm_current_rev,
+                llvm_next_rev,
+            )
+            continue
+        logging.info(
+            "Checking for new commits on branch '%s'",
+            container.branch_ref,
+        )
+        new_patch_combos += find_new_patches(container, existing_patches)
+    return new_patch_combos
+
+
+def parse_args(argv: List[str]) -> argparse.Namespace:
+    """Parse passed in argv list."""
+
+    parser = argparse.ArgumentParser(
+        description=__doc__,
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+    )
+    chromiumos_root_action = parser.add_argument(
+        "--chromiumos-root",
+        type=Path,
+        help="Path to ChromiumOS root. If not specified, it is autodetected.",
+    )
+    llvm_dir_action = parser.add_argument(
+        "--llvm-dir",
+        type=Path,
+        help="""Path to a ChromiumOS llvm-project directory. If not
+        specified, it is autodetected.""",
+    )
+    parser.add_argument(
+        "--patch-dir",
+        required=True,
+        type=Path,
+        help="""Path to the directory containing the PATCHES.json and
+        its associated patch files. If the PATCHES.json file does not exist,
+        create it.""",
+    )
+    parser.add_argument(
+        "--check-all-branches",
+        action="store_true",
+        help="""By default, we only check for new patches on branches
+        which exist between LLVM Current and LLVM Next. Passing this flag
+        changes the behaviour to instead check every branch which matches
+        the branch patterns.
+        """,
+    )
+    parser.add_argument(
+        "-b",
+        "--branch",
+        action="append",
+        dest="branch_patterns",
+        default=[],
+        help=f"""Search for branches which match a given glob.
+        Default is {_DEFAULT_BRANCH_PATTERN}. This can be passed multiple
+        times to match every necessary branch.
+        """,
+    )
+    args = parser.parse_args(argv)
+    if not args.chromiumos_root:
+        if repo_root := cros_paths.script_chromiumos_checkout():
+            args.chromiumos_root = repo_root
+        else:
+            raise argparse.ArgumentError(
+                chromiumos_root_action,
+                "Could not find chromiumos root automatically."
+                " Pass --chromiumos-root manually.",
+            )
+    llvm_dir_error = argparse.ArgumentError(
+        llvm_dir_action,
+        "Could not find llvm dir automatically. Pass --llvm-dir manually.",
+    )
+    if not args.llvm_dir:
+        if not args.chromiumos_root:
+            raise llvm_dir_error
+        llvm_dir = args.chromiumos_root / cros_paths.LLVM_PROJECT
+        if not (llvm_dir / ".git").is_dir():
+            raise llvm_dir_error
+        args.llvm_dir = llvm_dir
+    return args
+
+
+def main(argv: List[str]):
+    """Entry point for the program."""
+    logging.basicConfig(
+        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
+        "%(message)s",
+        level=logging.INFO,
+    )
+
+    args = parse_args(argv)
+    patches_json_file = args.patch_dir / "PATCHES.json"
+    try:
+        with open(patches_json_file, encoding="utf-8") as f:
+            existing_patches = patch_utils.json_to_patch_entries(
+                args.patch_dir, f
+            )
+    except FileNotFoundError:
+        existing_patches = []
+    main_branch_ref = (
+        f"{git_utils.CROS_EXTERNAL_REMOTE}/{git_utils.CROS_MAIN_BRANCH}"
+    )
+    patch_context = LLVMPatchContext(
+        llvm_dir=args.llvm_dir,
+        patch_dir=args.patch_dir,
+        branch_refs=_find_branch_refs(args.llvm_dir, args.branch_patterns),
+        main_branch_ref=main_branch_ref,
+    )
+    new_patch_combos = _find_new_patch_combos(
+        args.chromiumos_root,
+        patch_context,
+        existing_patches,
+        args.check_all_branches,
+    )
+    if not new_patch_combos:
+        logging.info("No new patches to add. Nothing to do.")
+        return
+    for combo in new_patch_combos:
+        patch_path = combo.entry.patch_path()
+        logging.info("Writing patch '%s'", patch_path)
+        patch_path.parent.mkdir(parents=True, exist_ok=True)
+        with atomic_write_file.atomic_write(patch_path, encoding="utf-8") as f:
+            f.write(combo.contents)
+    logging.info("Writing PATCHES.json to '%s'", patches_json_file)
+    with atomic_write_file.atomic_write(
+        patches_json_file, "w", encoding="utf-8"
+    ) as f:
+        json.dump(
+            [p.to_dict() for p in existing_patches]
+            + [c.entry.to_dict() for c in new_patch_combos],
+            f,
+            indent=2,
+            sort_keys=True,
+        )
+        f.write("\n")
diff --git a/llvm_tools/create_patch_file_test.py b/llvm_tools/create_patch_file_test.py
new file mode 100644
index 00000000..6087aedf
--- /dev/null
+++ b/llvm_tools/create_patch_file_test.py
@@ -0,0 +1,181 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Tests for create_patch_file."""
+
+from pathlib import Path
+from typing import Optional
+import unittest
+
+from llvm_tools import create_patch_file
+from llvm_tools import git_llvm_rev
+from llvm_tools import patch_utils
+
+
+COMMIT_FIXTURE_1 = """Commit Fixture 1
+
+BUG=None
+TEST=CQ
+
+Change-Id: I01234567abcedf
+---
+Change-Id: I01234567abcedf
+"""
+
+COMMIT_FIXTURE_1_CLEAN = """Commit Fixture 1
+
+BUG=None
+TEST=CQ
+
+---
+Change-Id: I01234567abcedf
+"""
+
+COMMIT_FIXTURE_2 = """
+From 2939fec6d34e57ccfc1ee66d4a7885d88db82d16 Mon Sep 17 00:00:00 2001
+From: Jordan R Abrahams-Whitehead <ajordanr@google.com>
+Date: Fri, 31 May 2024 20:53:22 +0000
+Subject: [PATCH] llvm-project: ChromeOS Base Commit
+
+This is the LLVM ChromeOS Base Commit.
+
+This commit marks the start of the ChromeOS patch branch. It introduces
+the OWNERS file, and sets up the 'cros' directory for future use.
+
+Functional patches for the ChromeOS LLVM Toolchain land after this
+commit. This commit does not change how LLVM operates. The parent
+commit to this change determines the LLVM synthetic revision.
+
+BUG=b:343568613
+TEST=CQ
+
+Change-Id: I5cc72b7cfd9ac1c47f6acbf29f5e14314d75a0c6
+---
+ OWNERS         | 3 +++
+ cros/README.md | 7 +++++++
+ 2 files changed, 10 insertions(+)
+ create mode 100644 OWNERS
+ create mode 100644 cros/README.md
+
+diff --git a/OWNERS b/OWNERS
+new file mode 100644
+index 000000000000..e692fc288f75
+--- /dev/null
++++ b/OWNERS
+@@ -0,0 +1,3 @@
++set noparent
++
++include chromiumos/third_party/toolchain-utils:/OWNERS.toolchain
+diff --git a/cros/README.md b/cros/README.md
+new file mode 100644
+index 000000000000..21aeeddbe44f
+--- /dev/null
++++ b/cros/README.md
+@@ -0,0 +1,7 @@
++# CrOS Directory
++
++This directory is used to store arbitrary changes for the ChromeOS Toolchain
++team. Files in this directory are never meant to be upstreamed, and only
++exist for local modification.
++
++See src/third_party/toolchain-utils to see how this directory is configured.
+--
+2.45.2.627.g7a2c4fd464-goog
+"""
+
+UNIVERSAL_DIFF_FIXTURE = """Disable Cast Assertion
+
+Change-Id: Iabcedef123456789
+
+diff --git a/llvm/include/llvm/Support/Casting.h b/llvm/include/llvm/Support/Casting.h
+index 8a2fa94f9cca..e5326c277d05 100644
+--- a/llvm/include/llvm/Support/Casting.h
++++ b/llvm/include/llvm/Support/Casting.h
+
+Change-Id: Iabcedef123456789
+"""
+
+
+class TestCreatePatchFile(unittest.TestCase):
+    """Test harness for create_patch_file."""
+
+    @staticmethod
+    def _make_patch_entry(
+        from_: Optional[int], until: Optional[int], title: str = "Some title"
+    ):
+        return patch_utils.PatchEntry(
+            workdir=Path(),
+            metadata={
+                "info": [],
+                "title": title,
+            },
+            platforms=["some platform"],
+            rel_patch_path="a/path/to/a/patch.patch",
+            version_range={"from": from_, "until": until},
+        )
+
+    def test_find_new_patches_normal(self):
+        """Test that we only find newer patches applied to a given branch."""
+
+        llvm_rev = git_llvm_rev.Rev(git_llvm_rev.MAIN_BRANCH, 1234)
+        version_ranges = (
+            (1, 2),
+            (5, 100),
+            (1234, 1235),
+            (1, 1235),
+            (None, None),
+        )
+        existing_patches = [
+            self._make_patch_entry(from_, until)
+            for from_, until in version_ranges
+        ]
+        branch_version_ranges = ((1, 1984), (1000, 9001))
+        branch_combos = [
+            create_patch_file.PatchCombo(
+                self._make_patch_entry(from_, until),
+                "[some contents]",
+            )
+            for from_, until in ((1234, 1235), (1, 1235), (None, None))
+            + branch_version_ranges
+        ]
+        branch_combos.insert(
+            0,
+            create_patch_file.PatchCombo(
+                self._make_patch_entry(None, None, "llvm-project: Base Commit"),
+                "[PATCH] llvm-project: Base Commit",
+            ),
+        )
+        branch_context = create_patch_file.BranchContext(
+            branch_ref="some-branch-name",
+            merge_base="abcdef",
+            llvm_rev=llvm_rev,
+            patch_entry_combos=branch_combos,
+        )
+        new_patch_combos = create_patch_file.find_new_patches(
+            branch_context, existing_patches
+        )
+        self.assertEqual(
+            new_patch_combos, branch_combos[-len(branch_version_ranges) :]
+        )
+
+    def test_filter_change_id_1(self):
+        """Test filter_change_id."""
+        filtered_fixture = create_patch_file.filter_change_id(COMMIT_FIXTURE_1)
+        self.assertIn("Change-Id", COMMIT_FIXTURE_1)
+        self.assertEqual(filtered_fixture, COMMIT_FIXTURE_1_CLEAN)
+
+    def test_filter_change_id_2(self):
+        """Test filter_change_id again."""
+        fixture_line_count = COMMIT_FIXTURE_2.count("\n")
+        filtered_fixture = create_patch_file.filter_change_id(COMMIT_FIXTURE_2)
+        self.assertNotIn("Change-Id", filtered_fixture)
+        self.assertEqual(fixture_line_count, filtered_fixture.count("\n") + 1)
+
+    def test_filter_change_id_universal(self):
+        """Test filter_change_id again, but for universal diffs."""
+        fixture_line_count = UNIVERSAL_DIFF_FIXTURE.count("\n")
+        filtered_fixture = create_patch_file.filter_change_id(
+            UNIVERSAL_DIFF_FIXTURE
+        )
+        self.assertEqual(fixture_line_count, filtered_fixture.count("\n") + 1)
diff --git a/llvm_tools/cros_cls.py b/llvm_tools/cros_cls.py
index b74132e2..5c607f48 100644
--- a/llvm_tools/cros_cls.py
+++ b/llvm_tools/cros_cls.py
@@ -9,7 +9,7 @@ import json
 import logging
 import re
 import subprocess
-from typing import Any, Dict, Iterable, List, Optional
+from typing import Any, Dict, Iterable, List, Optional, Tuple, Union
 
 
 BuildID = int
@@ -67,26 +67,28 @@ class ChangeListURL:
     patch_set: Optional[int] = None
     internal: bool = False
 
+    _URL_PARSE_RE = re.compile(
+        # Match an optional https:// header.
+        r"(?:https?://)?"
+        # Leaving the CL number and patch set as the next parts, match either
+        # crrev...
+        r"(crrev\.com/[ci]/"
+        # ...or chromium-review URLs. Note that chromium-review can either be
+        # served by googlesource or git.corp.google hosts.
+        r"|(?:chromium|chrome-internal)-review\."
+        r"(?:git\.corp\.google|googlesource)\.com/.*/\+/)"
+        # Match the CL number...
+        r"(\d+)"
+        # and (optionally) the patch-set, as well as consuming any of the
+        # path after the patch-set.
+        r"(?:/(\d+)?(?:/.*)?)?"
+        # Validate any sort of GET params for completeness.
+        r"(?:$|[?&].*)"
+    )
+
     @classmethod
     def parse(cls, url: str) -> "ChangeListURL":
-        url_re = re.compile(
-            # Match an optional https:// header.
-            r"(?:https?://)?"
-            # Match either chromium-review or crrev, leaving the CL number and
-            # patch set as the next parts. These can be parsed in unison.
-            r"(chromium-review\.googlesource\.com.*/\+/"
-            r"|crrev\.com/[ci]/"
-            r"|chrome-internal-review\.googlesource\.com.*/\+/)"
-            # Match the CL number...
-            r"(\d+)"
-            # and (optionally) the patch-set, as well as consuming any of the
-            # path after the patch-set.
-            r"(?:/(\d+)?(?:/.*)?)?"
-            # Validate any sort of GET params for completeness.
-            r"(?:$|[?&].*)"
-        )
-
-        m = url_re.fullmatch(url)
+        m = cls._URL_PARSE_RE.fullmatch(url)
         if not m:
             raise ValueError(
                 f"URL {url!r} was not recognized. Supported URL formats are "
@@ -112,13 +114,18 @@ class ChangeListURL:
             raise ValueError("A patchset number must be specified.")
         return result
 
-    def crrev_url_without_http(self):
+    def crrev_url_without_http(self) -> str:
         namespace = "i" if self.internal else "c"
         result = f"crrev.com/{namespace}/{self.cl_id}"
         if self.patch_set is not None:
             result += f"/{self.patch_set}"
         return result
 
+    @property
+    def gerrit_tool_id(self) -> str:
+        """Returns an identifier for this CL for use with the 'gerrit' tool."""
+        return f"*{self.cl_id}" if self.internal else f"{self.cl_id}"
+
     def __str__(self):
         return f"https://{self.crrev_url_without_http()}"
 
@@ -240,6 +247,17 @@ class CQBoardBuilderOutput:
         return results
 
 
+def fetch_cq_orchestrator_or_board_builder(
+    bot_id: BuildID,
+) -> Tuple[str, Union[CQOrchestratorOutput, CQBoardBuilderOutput]]:
+    """Figures out the builder type of bot_id, then fetches it."""
+    result = _run_bb_decoding_output(["get", str(bot_id)])
+    builder_name = result["builder"]["builder"]
+    if builder_name == "cq-orchestrator":
+        return builder_name, CQOrchestratorOutput.fetch(bot_id)
+    return builder_name, CQBoardBuilderOutput.fetch_many((bot_id,))[0]
+
+
 def parse_release_from_builder_artifacts_link(artifacts_link: str) -> str:
     """Parses the release version from a builder artifacts link.
 
diff --git a/llvm_tools/cros_cls_test.py b/llvm_tools/cros_cls_test.py
old mode 100755
new mode 100644
index 45efaf31..da0dcca9
--- a/llvm_tools/cros_cls_test.py
+++ b/llvm_tools/cros_cls_test.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -7,7 +6,7 @@
 
 import unittest
 
-import cros_cls
+from llvm_tools import cros_cls
 
 
 class TestChangeListURL(unittest.TestCase):
@@ -31,6 +30,24 @@ class TestChangeListURL(unittest.TestCase):
             cros_cls.ChangeListURL(cl_id=654321, patch_set=None, internal=True),
         )
 
+    def test_parsing_long_form_git_corp_url(self):
+        self.assertEqual(
+            cros_cls.ChangeListURL.parse(
+                "chromium-review.git.corp.google.com/c/chromiumos/overlays/"
+                "chromiumos-overlay/+/123456",
+            ),
+            cros_cls.ChangeListURL(cl_id=123456, patch_set=None),
+        )
+
+    def test_parsing_long_form_git_corp_internal_url(self):
+        self.assertEqual(
+            cros_cls.ChangeListURL.parse(
+                "chrome-internal-review.git.corp.google.com/c/chromeos/"
+                "manifest-internal/+/654321"
+            ),
+            cros_cls.ChangeListURL(cl_id=654321, patch_set=None, internal=True),
+        )
+
     def test_parsing_short_internal_url(self):
         self.assertEqual(
             cros_cls.ChangeListURL.parse("crrev.com/i/654321"),
@@ -152,7 +169,3 @@ class Test(unittest.TestCase):
             ),
             "R122-15711.0.0",
         )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/cros_llvm_repo.py b/llvm_tools/cros_llvm_repo.py
new file mode 100644
index 00000000..d2766c97
--- /dev/null
+++ b/llvm_tools/cros_llvm_repo.py
@@ -0,0 +1,33 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Attributes/utilities for the LLVM repo that's bundled with ChromeOS"""
+
+from pathlib import Path
+import subprocess
+from typing import Optional
+
+
+# Git remote to query/fetch for upstream.
+UPSTREAM_REMOTE = "cros"
+# Git branch that contains upstream commits.
+UPSTREAM_MAIN = "upstream/main"
+
+
+def try_get_path() -> Optional[Path]:
+    """Returns the path to the CrOS LLVM Repository, if it exists."""
+    llvm_project = (
+        Path(__file__).resolve().parent.parent.parent / "llvm-project"
+    )
+    return llvm_project if llvm_project.is_dir() else None
+
+
+def fetch_upstream(repo_path: Path):
+    """Runs `git fetch` for the upstream branch in the given repo."""
+    subprocess.run(
+        ["git", "fetch", UPSTREAM_REMOTE, UPSTREAM_MAIN],
+        check=True,
+        cwd=repo_path,
+        stdin=subprocess.DEVNULL,
+    )
diff --git a/llvm_tools/custom_script_example.py b/llvm_tools/custom_script_example.py
deleted file mode 100755
index a2d6a66a..00000000
--- a/llvm_tools/custom_script_example.py
+++ /dev/null
@@ -1,79 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""A custom script example that utilizes the .JSON contents of the tryjob."""
-
-import json
-import sys
-
-import update_tryjob_status
-
-
-def main():
-    """Determines the exit code based off of the contents of the .JSON file."""
-
-    # Index 1 in 'sys.argv' is the path to the .JSON file which contains
-    # the contents of the tryjob.
-    #
-    # Format of the tryjob contents:
-    #   {
-    #     "status" : [TRYJOB_STATUS],
-    #     "buildbucket_id" : [BUILDBUCKET_ID],
-    #     "extra_cls" : [A_LIST_OF_EXTRA_CLS_PASSED_TO_TRYJOB],
-    #     "url" : [GERRIT_URL],
-    #     "builder" : [TRYJOB_BUILDER_LIST],
-    #     "rev" : [REVISION],
-    #     "link" : [LINK_TO_TRYJOB],
-    #     "options" : [A_LIST_OF_OPTIONS_PASSED_TO_TRYJOB]
-    #   }
-    abs_path_json_file = sys.argv[1]
-
-    with open(abs_path_json_file, encoding="utf-8") as f:
-        tryjob_contents = json.load(f)
-
-    CUTOFF_PENDING_REVISION = 369416
-
-    SKIP_REVISION_CUTOFF_START = 369420
-    SKIP_REVISION_CUTOFF_END = 369428
-
-    if (
-        tryjob_contents["status"]
-        == update_tryjob_status.TryjobStatus.PENDING.value
-    ):
-        if tryjob_contents["rev"] <= CUTOFF_PENDING_REVISION:
-            # Exit code 0 means to set the tryjob 'status' as 'good'.
-            sys.exit(0)
-
-        # Exit code 124 means to set the tryjob 'status' as 'bad'.
-        sys.exit(124)
-
-    if tryjob_contents["status"] == update_tryjob_status.TryjobStatus.BAD.value:
-        # Need to take a closer look at the contents of the tryjob to then
-        # decide what that tryjob's 'status' value should be.
-        #
-        # Since the exit code is not in the mapping, an exception will occur
-        # which will save the file in the directory of this custom script
-        # example.
-        sys.exit(1)
-
-    if (
-        tryjob_contents["status"]
-        == update_tryjob_status.TryjobStatus.SKIP.value
-    ):
-        # Validate that the 'skip value is really set between the cutoffs.
-        if (
-            SKIP_REVISION_CUTOFF_START
-            < tryjob_contents["rev"]
-            < SKIP_REVISION_CUTOFF_END
-        ):
-            # Exit code 125 means to set the tryjob 'status' as 'skip'.
-            sys.exit(125)
-
-        if tryjob_contents["rev"] >= SKIP_REVISION_CUTOFF_END:
-            sys.exit(124)
-
-
-if __name__ == "__main__":
-    main()
diff --git a/llvm_tools/fetch_cq_size_diff.py b/llvm_tools/fetch_cq_size_diff.py
old mode 100755
new mode 100644
index a20f7396..64867d02
--- a/llvm_tools/fetch_cq_size_diff.py
+++ b/llvm_tools/fetch_cq_size_diff.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -17,7 +16,6 @@ it can still account for a few MB of diff in an average case.
 import abc
 import argparse
 import dataclasses
-import json
 import logging
 import os
 from pathlib import Path
@@ -26,7 +24,9 @@ import sys
 import tempfile
 from typing import List, Optional, Tuple
 
-import cros_cls
+from cros_utils import cros_image_tools
+from cros_utils import cros_paths
+from llvm_tools import cros_cls
 
 
 @dataclasses.dataclass(frozen=True)
@@ -93,37 +93,66 @@ class DebugInfoArtifact(ComparableArtifact):
         return os.path.getsize(file.parent / chrome_debug)
 
 
+def _calculate_image_size(mount_point: Path) -> int:
+    """Returns the size of the FS mounted at mount_point, in bytes."""
+    df_stdout = subprocess.run(
+        ("df", "--block-size=1", mount_point),
+        check=True,
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+        encoding="utf-8",
+    ).stdout
+
+    # There's a row of header, then a row with the info we want with the
+    # columns:
+    # - filesystem_name
+    # - total_blocks
+    # - used_blocks
+    # - available_blocks
+    # - use%
+    # - mount_point
+    #
+    # Since `--block-size=1` above, all 'blocks' will be 1 byte.
+    rows = df_stdout.strip().splitlines()
+    header = rows[0]
+    if not header.startswith("Filesystem"):
+        raise ValueError(f"`df` header doesn't look as expected: {header!r}")
+
+    data = rows[1]
+    used_blocks = data.split()[2]
+    return int(used_blocks)
+
+
 class ImageSizeArtifact(ComparableArtifact):
     """ComparableArtifact instance for image files."""
 
+    def __init__(self, chromeos_root: Path):
+        self._chromeos_root = chromeos_root
+
     @property
     def artifact_name(self) -> str:
-        return "image.zip"
+        return "chromiumos_base_image.tar.xz"
 
     def _measure_artifact_size(self, file: Path) -> int:
-        binpkg_sizes_name = "chromiumos_base_image.bin-package-sizes.json"
+        tmpdir = file.parent
+        base_image_name = "chromiumos_base_image.bin"
         subprocess.run(
             [
-                "unzip",
-                file.name,
-                binpkg_sizes_name,
+                "tar",
+                "-xaf",
+                file,
             ],
             check=True,
-            cwd=file.parent,
+            cwd=tmpdir,
             stdin=subprocess.DEVNULL,
         )
-        with (file.parent / binpkg_sizes_name).open(encoding="utf-8") as f:
-            loaded = json.load(f)
-            try:
-                size = loaded["total_size"]
-            except KeyError:
-                raise ValueError(f"Missing total_size in {loaded.keys()}")
-
-            if not isinstance(size, int):
-                raise ValueError(
-                    f"total_size was unexpectedly {type(size)}: {size}"
-                )
-            return size
+        mount_dir = tmpdir / "mount"
+        mount_dir.mkdir()
+        image_file = tmpdir / base_image_name
+        with cros_image_tools.mount_image(
+            self._chromeos_root, image_file, mount_dir
+        ):
+            return _calculate_image_size(mount_dir)
 
 
 def is_probably_non_production_builder(builder_name: str) -> bool:
@@ -138,38 +167,15 @@ def is_probably_non_production_builder(builder_name: str) -> bool:
             "-buildtest-",
             "-fuzzer-",
             "-kernelnext-",
+            "-kernel-",
+            "-sdknext-",
             "-ubsan-",
             "-vmtest-",
+            "-vm-",
         )
     )
 
 
-def guess_release_artifact_path(artifact_link: str) -> Optional[str]:
-    """Guesses a close-enough release path for a CQ artifact.
-
-    Returns:
-        A path to the release artifact. Returns None if the given image_zip
-        wasn't generated by a CQ builder.
-
-    >>> guess_release_artifact_path("gs://chromeos-image-archive/brya-cq/"
-        "R121-15677.0.0-90523-8764532770258575633/image.zip")
-    "gs://chromeos-image-archive/brya-release/R121-15677.0.0/image.zip"
-    """
-    artifacts_link = os.path.dirname(artifact_link)
-    release_version = cros_cls.parse_release_from_builder_artifacts_link(
-        artifacts_link
-    )
-    # Scrape the board name from a level above the artifacts directory.
-    builder = os.path.basename(os.path.dirname(artifacts_link))
-    if not builder.endswith("-cq"):
-        return None
-    board = builder[:-3]
-    return (
-        f"gs://chromeos-image-archive/{board}-release/{release_version}/"
-        f"{os.path.basename(artifact_link)}"
-    )
-
-
 def try_gsutil_ls(paths: List[str]) -> List[str]:
     """Returns all of the paths `gsutil` matches from `paths`.
 
@@ -199,7 +205,7 @@ def try_gsutil_ls(paths: List[str]) -> List[str]:
 def find_size_diffable_cq_artifacts(
     cq_build_ids: List[cros_cls.BuildID],
     artifact_name: str,
-) -> Optional[Tuple[str, str]]:
+) -> List[Tuple[str, str]]:
     """Searches the cq-orchestrator builds for candidates for size comparison.
 
     Returns:
@@ -210,26 +216,37 @@ def find_size_diffable_cq_artifacts(
     for cq_build_id in cq_build_ids:
         logging.info("Inspecting CQ build %d...", cq_build_id)
         orch_output = cros_cls.CQOrchestratorOutput.fetch(cq_build_id)
+        production_child_builders = [
+            (name, val)
+            for name, val in orch_output.child_builders.items()
+            if not is_probably_non_production_builder(name)
+        ]
         child_builder_values = cros_cls.CQBoardBuilderOutput.fetch_many(
-            [
-                val
-                for name, val in orch_output.child_builders.items()
-                if not is_probably_non_production_builder(name)
-            ]
+            [x for _, x in production_child_builders]
         )
-        artifacts_links = [
-            x.artifacts_link
-            for x in child_builder_values
-            if x.artifacts_link is not None
+        artifact_dir_links = [
+            (builder_name, output.artifacts_link)
+            for (builder_name, _), output in zip(
+                production_child_builders, child_builder_values
+            )
+            # Only choose successful builders, since failing builders may have
+            # incomplete artifacts (e.g., debug.tgz is "the set of all debuginfo
+            # of successfully built packages," and the build might not have
+            # gotten far enough to produce chromeos-chrome's debug.tgz)
+            if output.artifacts_link is not None and output.status == "SUCCESS"
         ]
-        if not artifacts_links:
+
+        if not artifact_dir_links:
             logging.info("No children of CQ run %d had artifacts", cq_build_id)
             continue
 
-        potential_artifacts = try_gsutil_ls(
-            [os.path.join(x, artifact_name) for x in artifacts_links]
+        available_artifacts = try_gsutil_ls(
+            [
+                os.path.join(artifacts_link, artifact_name)
+                for _, artifacts_link in artifact_dir_links
+            ]
         )
-        if not potential_artifacts:
+        if not available_artifacts:
             logging.info(
                 "No children of CQ run %d produced a(n) %s",
                 cq_build_id,
@@ -238,32 +255,30 @@ def find_size_diffable_cq_artifacts(
             continue
 
         logging.debug(
-            "Found candidate %s files: %s", artifact_name, potential_artifacts
+            "Found candidate %s artifacts: %s",
+            artifact_name,
+            available_artifacts,
         )
-        guessed_paths = [
-            (x, guess_release_artifact_path(x)) for x in potential_artifacts
-        ]
-        logging.debug("Guessed corresponding artifact files: %s", guessed_paths)
-        release_artifacts = try_gsutil_ls([x for _, x in guessed_paths if x])
-        if not release_artifacts:
-            logging.info(
-                "No release %s artifacts could be found for CQ builder %d.",
-                artifact_name,
-                cq_build_id,
+
+        # Match the artifacts up with their builders. `try_gsutil_ls` will
+        # return _some subset_ of the requested artifacts in _some order_.
+        builder_dirs = {
+            gs_path: cq_builder for cq_builder, gs_path in artifact_dir_links
+        }
+        results = []
+        for artifact_link in available_artifacts:
+            artifact_dir = os.path.dirname(artifact_link)
+            builder = builder_dirs.get(artifact_dir)
+            assert builder, (
+                f"Couldn't match artifact in {artifact_dir} with a builder "
+                f"in {builder_dirs.keys()}"
             )
-            continue
+            results.append((builder, artifact_link))
 
-        # `try_gsutil_ls` makes no ordering guarantees; always pick the min()
-        # artifact here for consistency across reruns.
-        selected_release_artifact = min(release_artifacts)
-        logging.info("Selected release artifact: %s", selected_release_artifact)
-        cq_artifact = next(
-            cq_path
-            for cq_path, guessed_path in guessed_paths
-            if guessed_path == selected_release_artifact
-        )
-        return selected_release_artifact, cq_artifact
-    return None
+        # Sort for consistent output.
+        results.sort()
+        return results
+    return []
 
 
 def inspect_gs_impl(
@@ -281,27 +296,70 @@ def inspect_gs_impl(
     logging.info("Diff: %.2f%%", diff_pct * 100)
 
 
-def inspect_cl(opts: argparse.Namespace, artifact: ComparableArtifact) -> None:
-    """Implements the `cl` subcommand of this script."""
-    cq_build_ids = cros_cls.fetch_cq_orchestrator_ids(opts.cl)
+def inspect_single_cl(
+    cl: cros_cls.ChangeListURL, artifact: ComparableArtifact
+) -> List[Tuple[str, str]]:
+    """Inspects a single CL for artifacts.
+
+    Returns:
+        A list of tuples of:
+        - CQ builder name
+        - gs:// path to artifact produced by that builder
+
+        If no artifacts are found, the list is empty.
+    """
+    cq_build_ids = cros_cls.fetch_cq_orchestrator_ids(cl)
     if not cq_build_ids:
-        sys.exit(f"No completed cq-orchestrators found for {opts.cl}")
+        logging.error("No completed cq-orchestrators found for %s", cl)
+        return []
+
+    return find_size_diffable_cq_artifacts(cq_build_ids, artifact.artifact_name)
 
-    # Reverse cq_build_ids so we try the newest first.
-    diffable_artifacts = find_size_diffable_cq_artifacts(
-        cq_build_ids, artifact.artifact_name
-    )
-    if not diffable_artifacts:
-        sys.exit("No diffable artifacts were found")
 
-    baseline, new = diffable_artifacts
+def find_common_artifact(
+    a: List[Tuple[str, str]], b: List[Tuple[str, str]]
+) -> Optional[Tuple[str, str, str]]:
+    """Finds an artifact that can be compared between the given artifact lists.
+
+    The artifact lists should be in the form [(cq_builder_name, artifact_path)].
+
+    Returns:
+        None if none can be found; otherwise, a tuple of:
+            - The selected CQ builder name
+            - The artifact path from 'a'
+            - The artifact path from 'b'
+    """
+    b_map = dict(b)
+    shared_keys = ((k, a_value) for k, a_value in sorted(a) if k in b_map)
+    return next(((k, a_value, b_map[k]) for k, a_value in shared_keys), None)
+
+
+def inspect_cl(opts: argparse.Namespace, artifact: ComparableArtifact) -> None:
+    """Implements the `cl` subcommand of this script."""
+    logging.info("Finding artifacts for baseline CL...")
+    baseline_artifacts = inspect_single_cl(opts.baseline_cl, artifact)
+    logging.debug("Artifacts for baseline CL: %s", baseline_artifacts)
+    logging.info("Finding artifacts for new CL...")
+    new_artifacts = inspect_single_cl(opts.new_cl, artifact)
+    logging.debug("Artifacts for new CL: %s", new_artifacts)
+
+    common_artifact = find_common_artifact(baseline_artifacts, new_artifacts)
+    if common_artifact is None:
+        sys.exit(
+            "Could not find a builder with a common artifact between CLs; "
+            "maybe try CQ+1 again and wait for it to finish on each?"
+        )
+
+    cq_builder, baseline, new = common_artifact
+    logging.info("Selected CQ builder %s for artifact diffing", cq_builder)
     logging.info("Comparing %s (baseline) to %s (new)", baseline, new)
     inspect_gs_impl(baseline, new, artifact)
     logging.warning(
-        "Friendly reminder: CL inspection diffs between your CL and a "
-        "corresponding release build. Size differences up to a few megabytes "
-        "are expected and do not necessarily indicate a size difference "
-        "attributable to your CL."
+        "Friendly reminder: CL inspection diffs between two CQ runs. Depending "
+        "on how these were run, there may be some source skew. If a "
+        "significant, unexpected difference is observed, you can always try "
+        "CQ+1 again (preferably after adding the "
+        "`Disallow-Recycled-Builds: all` footer to your commit messages)."
     )
 
 
@@ -311,6 +369,8 @@ def inspect_gs(opts: argparse.Namespace, artifact: ComparableArtifact) -> None:
 
 
 def main(argv: List[str]) -> None:
+    cros_root = cros_paths.script_chromiumos_checkout_or_exit()
+
     parser = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawDescriptionHelpFormatter,
@@ -333,9 +393,20 @@ def main(argv: List[str]) -> None:
     )
     cl_parser.set_defaults(func=inspect_cl)
     cl_parser.add_argument(
-        "cl",
+        "--baseline-cl",
+        type=cros_cls.ChangeListURL.parse_with_patch_set,
+        help="""
+        Baseline CL to inspect CQ runs of. This must contain a patchset number.
+        """,
+    )
+    cl_parser.add_argument(
+        "--new-cl",
         type=cros_cls.ChangeListURL.parse_with_patch_set,
-        help="CL to inspect CQ runs of. This must contain a patchset number.",
+        help="""
+        New CL to inspect CQ runs of. This must contain a patchset number. Any
+        regressions or wins are reported against this CL (so if you're using
+        this for llvm-next, the llvm-next CL should be passed as this flag).
+        """,
     )
 
     gs_parser = subparsers.add_parser(
@@ -354,13 +425,9 @@ def main(argv: List[str]) -> None:
 
     assert getattr(opts, "func", None), "Unknown subcommand?"
     if opts.image:
-        artifact: ComparableArtifact = ImageSizeArtifact()
+        artifact: ComparableArtifact = ImageSizeArtifact(cros_root)
     else:
         assert opts.debuginfo
         artifact = DebugInfoArtifact()
 
     opts.func(opts, artifact)
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/llvm_tools/fetch_cros_sdk_rolls.py b/llvm_tools/fetch_cros_sdk_rolls.py
old mode 100755
new mode 100644
index b426f147..b6c14b6e
--- a/llvm_tools/fetch_cros_sdk_rolls.py
+++ b/llvm_tools/fetch_cros_sdk_rolls.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2020 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -109,7 +108,3 @@ def main():
             logging.debug("Keeping around tempdir %r to aid debugging", tempdir)
         else:
             shutil.rmtree(tempdir)
-
-
-if __name__ == "__main__":
-    sys.exit(main())
diff --git a/llvm_tools/generate_llvm_revert_report.py b/llvm_tools/generate_llvm_revert_report.py
old mode 100755
new mode 100644
index f6a11128..5c2468bd
--- a/llvm_tools/generate_llvm_revert_report.py
+++ b/llvm_tools/generate_llvm_revert_report.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2023 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -21,8 +20,9 @@ import subprocess
 import sys
 from typing import List, Set, TextIO
 
-import get_upstream_patch
-import revert_checker
+from cros_utils import cros_paths
+from llvm_tools import get_llvm_hash
+from llvm_tools import revert_checker
 
 
 @dataclasses.dataclass(frozen=True)
@@ -78,14 +78,18 @@ def write_reverts_as_csv(write_to: TextIO, reverts: List[RevertInfo]):
 
 
 def main(argv: List[str]):
+    # `cros_root` is hardcoded here, since:
+    # - this one only reads tree state, and
+    # - the person/automation invoking it is almost definitely invoking it _in
+    #   the tree that it should run in_.
+    cros_root = cros_paths.script_chromiumos_checkout_or_exit()
+
     logging.basicConfig(
         format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
         "%(message)s",
         level=logging.INFO,
     )
 
-    my_dir = Path(__name__).resolve().parent
-
     parser = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawDescriptionHelpFormatter,
@@ -93,7 +97,7 @@ def main(argv: List[str]):
     parser.add_argument(
         "-C",
         "--git-dir",
-        default=my_dir.parent.parent / "llvm-project",
+        default=str(cros_root / cros_paths.LLVM_PROJECT),
         help="LLVM git directory to use.",
         # Note that this is left as `type=str` because that's what
         # `revert_checker` expects.
@@ -102,12 +106,6 @@ def main(argv: List[str]):
     parser.add_argument(
         "--llvm-next", action="store_true", help="Use the llvm-next hash"
     )
-    parser.add_argument(
-        "--llvm-dir",
-        help="Directory containing LLVM ebuilds",
-        type=Path,
-        default=my_dir.parent.parent / "chromiumos-overlay/sys-devel/llvm",
-    )
     parser.add_argument(
         "--llvm-head",
         default="cros/upstream/main",
@@ -115,15 +113,15 @@ def main(argv: List[str]):
     )
     opts = parser.parse_args(argv)
 
-    symbolic_sha = "llvm-next" if opts.llvm_next else "llvm"
-    llvm_sha = get_upstream_patch.resolve_symbolic_sha(
-        symbolic_sha,
-        opts.llvm_dir,
-    )
+    if opts.llvm_next:
+        llvm_sha = get_llvm_hash.LLVMHash().GetCrOSLLVMNextHash()
+    else:
+        llvm_sha = get_llvm_hash.LLVMHash().GetCrOSCurrentLLVMHash(cros_root)
+
     logging.info("Resolved %r as the LLVM SHA to check.", llvm_sha)
 
     in_tree_cherrypicks = list_upstream_cherrypicks(
-        opts.llvm_dir / "files/PATCHES.json"
+        cros_root / cros_paths.DEFAULT_PATCHES_PATH
     )
     logging.info("Identified %d local cherrypicks.", len(in_tree_cherrypicks))
 
@@ -153,7 +151,3 @@ def main(argv: List[str]):
     print()
     print("CSV summary of reverts:")
     write_reverts_as_csv(sys.stdout, reverts)
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/llvm_tools/generate_warning_exemption_files.py b/llvm_tools/generate_warning_exemption_files.py
new file mode 100644
index 00000000..117b940f
--- /dev/null
+++ b/llvm_tools/generate_warning_exemption_files.py
@@ -0,0 +1,578 @@
+# Copyright 2025 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Generate a Go file to exempt warnings from fatal_clang_warnings artifacts.
+
+This is intended to be used to mass-exempt warnings for Mage rotations. The file
+will contain one func like:
+
+```
+func getWarningsForLLVM_rNNN(packageNameAndCategory string) []string {
+  // return `-Wno-*` flags required to make the given package build
+}
+```
+
+Where NNN is the provided LLVM revision.
+"""
+
+import argparse
+import collections
+import dataclasses
+import datetime
+import json
+import logging
+import multiprocessing
+import multiprocessing.pool
+import os
+from pathlib import Path
+import re
+import shutil
+import subprocess
+import tempfile
+import textwrap
+from typing import DefaultDict, Dict, Generator, List, Optional, Set, Tuple
+
+from cros_utils import gs
+from llvm_tools import cros_cls
+from llvm_tools import llvm_next
+
+
+# It's a bit iffy to have a constant that's not completely a constant, but for
+# simplicity's sake (esp. with tests, ...)
+GO_COPYRIGHT_HEADER = textwrap.dedent(
+    f"""\
+    // Copyright {datetime.datetime.now().year} The ChromiumOS Authors
+    // Use of this source code is governed by a BSD-style license that can
+    // be found in the LICENSE file.
+    """
+)
+
+
+@dataclasses.dataclass(frozen=True, eq=True, order=True)
+class Builder:
+    """Represents a concrete CQ builder invocation."""
+
+    name: str
+    url: str
+
+
+@dataclasses.dataclass(frozen=True, eq=True, order=True)
+class FatalWarning:
+    """Represents a fatal warning recorded for a specific package."""
+
+    # Warning name, without `-W`. e.g., `all`, `extra`.
+    warning_name: str
+    # Package `${CATEGORY}` this was observed in.
+    category: str
+    # Package `${PN}` this was observed in.
+    package_name: str
+
+
+# This parses two kinds of errors:
+# 1. `clang-17: error: foo [-W...]`
+# 2. `/file/path:123:45: error: foo [-W...]"
+_FATAL_WARNING_RE = re.compile(
+    r"""
+    ^(?:[^:]*:\d+:\d+:\s|clang-\d+:\s)?  # clang-N or the file location
+    error:\s                             # Nonfatal warnings need not apply.
+    .*?\s+                               # Diagnostic message.
+    \[(-W[^\][]+)\]\s*$                  # List of warnings (likely incl.
+                                         # -Werror)
+    """,
+    re.VERBOSE,
+)
+
+
+def scrape_fatal_warning_names_from_stdout(stdout: str) -> List[str]:
+    warning_names = set()
+    for line in stdout.splitlines():
+        m = _FATAL_WARNING_RE.fullmatch(line)
+        if not m:
+            continue
+
+        warning_flags = m.group(1)
+        warning_flags_no_werror = [
+            x for x in warning_flags.split(",") if x != "-Werror"
+        ]
+        if len(warning_flags_no_werror) != 1:
+            raise ValueError(
+                f"Weird: parsed warnings {warning_flags_no_werror} out "
+                f"of {line}"
+            )
+
+        warning_flag = warning_flags_no_werror[0]
+        if not warning_flag.startswith("-W"):
+            raise ValueError(
+                f"Weird: parsed warning flag {warning_flag} without -W out "
+                f"of {line}"
+            )
+        warning_flag_without_w = warning_flag[2:]
+        warning_names.add(warning_flag_without_w)
+    return sorted(warning_names)
+
+
+def parse_fatal_warnings_file(warnings_json_file: Path) -> List[FatalWarning]:
+    logging.debug("Parsing warnings report: %s", warnings_json_file)
+    with warnings_json_file.open(encoding="utf-8") as f:
+        warnings_json = json.load(f)
+
+    # The shape of warnings_json is:
+    # {
+    #   "cwd": "/path/to/directory",
+    #   "command": ["ccache", "full", "compile", "command"],
+    #   "stdout": "stdout/stderr of the build",
+    #   "parent_process_data": [
+    #     {
+    #       "invocation": ["parent", "process", "command"],
+    #       "env": ["ENV1=", "ENV2=value", ""]
+    #     }
+    #   ]
+    # }
+    warning_names = scrape_fatal_warning_names_from_stdout(
+        warnings_json["stdout"]
+    )
+    if not warning_names:
+        logging.warning(
+            "Could not scrape any fatal warning reports from %s; ignoring file",
+            warnings_json_file,
+        )
+        return []
+
+    # Hunt in parent process info for CATEGORY/PN env vars. Note that this isn't
+    # guaranteed to be in the first parent
+    for parent in warnings_json.get("parent_process_data", ()):
+        parent_env = parent.get("env", ())
+        category = ""
+        package_name = ""
+        for e in parent_env:
+            if e.startswith("CATEGORY="):
+                category = e.split("=", 1)[1]
+            elif e.startswith("PN="):
+                package_name = e.split("=", 1)[1]
+
+        if category and package_name:
+            break
+    else:
+        logging.error(
+            "No CATEGORY/PN could be inferred for %s; ignoring file",
+            warnings_json_file,
+        )
+        return []
+
+    results = [
+        FatalWarning(
+            warning_name=x,
+            category=category,
+            package_name=package_name,
+        )
+        for x in warning_names
+    ]
+    logging.debug(
+        "Parsed %d unique fatal warning(s) for %s",
+        len(results),
+        warnings_json_file,
+    )
+    return results
+
+
+def find_all_warning_reports_in(root: Path) -> Generator[Path, None, None]:
+    for dirpath_str, _, filenames in os.walk(root):
+        dirpath = Path(dirpath_str)
+        for filename in filenames:
+            if filename.endswith(".json") and filename.startswith(
+                "warnings_report"
+            ):
+                yield dirpath / filename
+
+
+def parse_all_fatal_warnings(warning_reports: Path) -> List[FatalWarning]:
+    logging.info("Parsing warning reports under %s", warning_reports)
+
+    # Collect these in a set, since multiple reports may refer to the same
+    # warning, and dedup'ing that is nice.
+    fatal_warnings = {
+        x
+        for warning_report in find_all_warning_reports_in(warning_reports)
+        for x in parse_fatal_warnings_file(warning_report)
+    }
+    return sorted(fatal_warnings)
+
+
+# N.B., This is a shallow `frozen=True`.
+@dataclasses.dataclass(frozen=True)
+class PackageWarnings:
+    """Grouped warnings from a package & builders they came from."""
+
+    warning_names: List[str] = dataclasses.field(default_factory=list)
+    builders: Set[Builder] = dataclasses.field(default_factory=set)
+
+
+def create_exemption_comment_for_package(builders: List[Builder]) -> str:
+    if not builders:
+        return "// (No builder links were available for these exemptions)."
+
+    if len(builders) == 1:
+        commentary = "Observed and suppressed on 1 builder during testing."
+    else:
+        commentary = (
+            f"Observed and suppressed on {len(builders)} builders "
+            "during testing."
+        )
+    return textwrap.dedent(
+        f"""\
+        // {commentary}
+        // e.g., {builders[0].name}: {builders[0].url}.
+        """
+    ).rstrip()
+
+
+def create_go_file(
+    llvm_revision: int,
+    fatal_warnings: Dict[FatalWarning, List[Builder]],
+) -> str:
+    """Creates a file that parses as Go to ignore the given warnings.
+
+    Note that this file is not guaranteed to be well-formatted; use of `go fmt`
+    or similar is recommended.
+    """
+    func_name = f"getWarningsForLLVM_r{llvm_revision}"
+    header = textwrap.dedent(
+        f"""\
+
+        package main
+
+        func {func_name}(packageNameAndCategory string) []string {{
+        """
+    )
+
+    # List of pieces of the file to be "".join'ed. Keeps us from n^2 string
+    # concat.
+    file_pieces = [GO_COPYRIGHT_HEADER, header]
+    if not fatal_warnings:
+        file_pieces.append("    return nil\n}\n")
+        return "".join(file_pieces)
+
+    file_pieces.append("    switch packageNameAndCategory {\n")
+
+    grouped_warnings: DefaultDict[Tuple[str, str], PackageWarnings] = (
+        collections.defaultdict(PackageWarnings)
+    )
+    for warning, builders in fatal_warnings.items():
+        w = grouped_warnings[(warning.category, warning.package_name)]
+        w.builders.update(builders)
+        w.warning_names.append(warning.warning_name)
+
+    for (category, package_name), warnings in sorted(grouped_warnings.items()):
+        wno_flags = ", ".join(
+            f'"-Wno-{x}"' for x in sorted(warnings.warning_names)
+        )
+        comment = create_exemption_comment_for_package(
+            sorted(warnings.builders)
+        )
+        case_stmt = textwrap.dedent(
+            f"""\
+            case "{category}/{package_name}":
+                return []string{{ {wno_flags} }}
+            """
+        )
+        indented_case = textwrap.indent(comment + "\n" + case_stmt, "    ")
+        file_pieces.append(indented_case)
+
+    file_pieces.append(
+        textwrap.dedent(
+            """\
+                default:
+                    return nil
+                }
+            }
+            """
+        )
+    )
+    return "".join(file_pieces)
+
+
+def cmd_local(
+    opts: argparse.Namespace,
+) -> Dict[FatalWarning, List[Builder]]:
+    """Implements the `local` subcommand."""
+    builders = []
+    if opts.builder_name:
+        assert opts.builder_url
+        builders.append(Builder(name=opts.builder_name, url=opts.builder_url))
+
+    fatal_warnings = parse_all_fatal_warnings(opts.warning_reports)
+    return {x: builders for x in fatal_warnings}
+
+
+def resolve_builder_artifacts(
+    build_ids: List[int],
+) -> List[Tuple[Builder, str]]:
+    """Resolves build_ids into tuples of (builder, artifacts_gs_link).
+
+    If any of the `build_ids` are cq-orchestrators, this will find their
+    children and return the Builder/artifacts tuples for those instead.
+
+    Raises:
+        ValueError if any of the given build_ids had no associated artifacts.
+        (That is, for cq-orchestrators, if _none_ of their children had
+        artifacts that could be found).
+    """
+    results = []
+    for build_id in build_ids:
+        name, output = cros_cls.fetch_cq_orchestrator_or_board_builder(build_id)
+        if isinstance(output, cros_cls.CQBoardBuilderOutput):
+            logging.info("Finding artifacts for %d...", build_id)
+            named_builders = [(name, build_id, output.artifacts_link)]
+        else:
+            logging.info("Finding child builders for %d...", build_id)
+            child_builders = sorted(output.child_builders.items())
+            sub_builders = cros_cls.CQBoardBuilderOutput.fetch_many(
+                bid for _, bid in child_builders
+            )
+            named_builders = [
+                (name, bid, output.artifacts_link)
+                for (name, bid), output in zip(child_builders, sub_builders)
+            ]
+
+        found_any_artifacts = False
+        for name, build_id, artifacts_link in named_builders:
+            build_url = cros_cls.builder_url(build_id)
+            if not artifacts_link:
+                logging.warning("Ignoring %s; it had no artifacts", build_url)
+                continue
+
+            found_any_artifacts = True
+            builder = Builder(name=name, url=build_url)
+            results.append((builder, artifacts_link))
+
+        if not found_any_artifacts:
+            raise ValueError(f"No artifacts found for {build_id} (or children)")
+    return results
+
+
+def fetch_and_unpack_fatal_warnings_tarball(
+    tmpdir: Path, builder_artifacts: str
+) -> Optional[Path]:
+    tmpdir.mkdir(parents=True, exist_ok=True)
+
+    tarball_suffix = "fatal_clang_warnings.tar.xz"
+    results = gs.ls(os.path.join(builder_artifacts, f"*.{tarball_suffix}"))
+    if not results:
+        return None
+    if len(results) > 1:
+        raise ValueError(
+            f"Builder at {builder_artifacts} had {len(results)} warnings "
+            "tarballs; expected one"
+        )
+
+    gs_path = results[0].gs_path
+    tarball_target = tmpdir / tarball_suffix
+    logging.info("Fetching fatal warnings from %s...", gs_path)
+    gs_result = subprocess.run(
+        (gs.GSUTIL, "cp", gs_path, tarball_target),
+        check=False,
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.DEVNULL,
+        stderr=subprocess.PIPE,
+        encoding="utf-8",
+        errors="replace",
+    )
+
+    if gs_result.returncode:
+        logging.error(
+            "Failed fetching %s; gs stderr: %r", gs_path, gs_result.stderr
+        )
+        gs_result.check_returncode()
+
+    unpack_dir = tmpdir / "unpack"
+    unpack_dir.mkdir()
+    subprocess.run(
+        ("tar", "xaf", tarball_target),
+        check=True,
+        cwd=unpack_dir,
+        stdin=subprocess.DEVNULL,
+    )
+    return unpack_dir
+
+
+def cmd_builders(
+    opts: argparse.Namespace,
+) -> Dict[FatalWarning, List[Builder]]:
+    """Implements the `builders` subcommand."""
+    builder_artifacts = resolve_builder_artifacts(opts.builder_id)
+
+    tmpdir = Path(tempfile.mkdtemp(prefix="generate_warning_exemption_files"))
+    cleanup_tmpdir = False
+    warnings_dict = collections.defaultdict(list)
+    try:
+        unpack_actions = []
+        with multiprocessing.pool.ThreadPool(opts.jobs) as pool:
+            tasks = []
+            for i, (builder, artifacts_url) in enumerate(builder_artifacts):
+                subdir = tmpdir / str(i)
+                tasks.append(
+                    (
+                        builder,
+                        artifacts_url,
+                        pool.apply_async(
+                            fetch_and_unpack_fatal_warnings_tarball,
+                            (subdir, artifacts_url),
+                        ),
+                    )
+                )
+
+            for builder, artifacts_url, task in tasks:
+                unpack_dir = task.get()
+                if not unpack_dir:
+                    logging.info(
+                        "Builder %s had no fatal-warnings artifact; skip",
+                        builder.url,
+                    )
+                    continue
+                unpack_actions.append((builder, artifacts_url, unpack_dir))
+
+        for builder, artifacts_url, unpack_dir in unpack_actions:
+            for fatal_warning in parse_all_fatal_warnings(unpack_dir):
+                warnings_dict[fatal_warning].append(builder)
+
+        cleanup_tmpdir = not opts.keep_tempdir
+    finally:
+        if cleanup_tmpdir:
+            logging.info(
+                "Removing tempdir with builder artifacts at %s...", tmpdir
+            )
+            shutil.rmtree(tmpdir)
+        else:
+            logging.info("Leaving tempdir at %s to aid in debugging", tmpdir)
+
+    # Sort the builders for consistency
+    for builder_list in warnings_dict.values():
+        builder_list.sort()
+
+    return warnings_dict
+
+
+def parse_args(argv: List[str]) -> argparse.Namespace:
+    """Parses flags for this program."""
+    parser = argparse.ArgumentParser(
+        description=__doc__,
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+    )
+    parser.add_argument(
+        "--debug", action="store_true", help="Enable debug logging"
+    )
+    parser.add_argument(
+        "--llvm-revision",
+        type=int,
+        default=llvm_next.LLVM_NEXT_REV,
+        help="""
+        LLVM Revision to start exempting the warnings from. If unspecified,
+        defaults to the llvm-next revision specified in llvm_tools/llvm_next.py.
+        """,
+    )
+    parser.add_argument(
+        "--output", type=Path, required=True, help="`.go` file to output."
+    )
+
+    subparsers = parser.add_subparsers(required=True)
+
+    # 'local' subcommand, for generating from local build logs.
+    subp = subparsers.add_parser("local", help="Generate from local logs.")
+    subp.set_defaults(func=cmd_local)
+    subp.add_argument(
+        "--warning-reports",
+        type=Path,
+        required=True,
+        help="""
+        Path to the root directory to scan for warning reports. This can be a
+        board build directory (e.g., /build/amd64-generic), or the root of a
+        place where a werror-logs tarball is unpacked.
+        """,
+    )
+    subp.add_argument(
+        "--builder-name",
+        help="""
+        Name of the builder that produced the report, e.g., amd64-generic-cq.
+        Must be specified if --builder-url is.
+        """,
+    )
+    subp.add_argument(
+        "--builder-url",
+        help="""
+        URL of the builder that produced the report, e.g.,
+        https://ci.chromium.org/b/1234. Must be specified if --builder-name is.
+        """,
+    )
+
+    # 'builders' subcommand, for fetching artifacts from builders.
+    subp = subparsers.add_parser(
+        "builders",
+        help="Fetch -Werror artifacts from builders; generate from those.",
+    )
+    subp.set_defaults(func=cmd_builders)
+    subp.add_argument(
+        "-j",
+        "--jobs",
+        type=int,
+        default=multiprocessing.cpu_count(),
+        help="""
+        How many log tarballs to fetch concurrently. Defaults to %(default)s.
+        """,
+    )
+    subp.add_argument(
+        "--keep-tempdir",
+        action="store_true",
+        help="Don't delete the tempdir where artifacts get fetched on success.",
+    )
+    subp.add_argument(
+        "--builder-id",
+        action="append",
+        type=int,
+        required=True,
+        help="""
+        Build ID of a builder to grab artifacts from. This can _either_ be a
+        cq-orchestrator (for which all children will be scanned), or a single
+        builder (e.g., amd64-generic-cq). This may be specified multiple times.
+        """,
+    )
+
+    opts = parser.parse_args(argv)
+    if opts.func is cmd_local:
+        if bool(opts.builder_name) != bool(opts.builder_url):
+            parser.error(
+                "--builder-name must be specified if --builder-url is "
+                "specified, and vice versa."
+            )
+
+    return opts
+
+
+def go_fmt_file(contents: str) -> str:
+    """Runs `gofmt` on the given Go file contents."""
+    return subprocess.run(
+        ("gofmt", "-s"),
+        check=True,
+        encoding="utf-8",
+        input=contents,
+        stdout=subprocess.PIPE,
+    ).stdout
+
+
+def main(argv: List[str]) -> None:
+    opts = parse_args(argv)
+
+    logging.basicConfig(
+        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
+        "%(message)s",
+        level=logging.DEBUG if opts.debug else logging.INFO,
+    )
+
+    fatal_warnings: Dict[FatalWarning, List[Builder]] = opts.func(opts)
+    file_contents = create_go_file(
+        llvm_revision=opts.llvm_revision,
+        fatal_warnings=fatal_warnings,
+    )
+    formatted_file = go_fmt_file(file_contents)
+    opts.output.write_text(formatted_file, encoding="utf-8")
+    logging.info("Generated file to %s.", opts.output)
diff --git a/llvm_tools/generate_warning_exemption_files_test.py b/llvm_tools/generate_warning_exemption_files_test.py
new file mode 100644
index 00000000..cb3319c9
--- /dev/null
+++ b/llvm_tools/generate_warning_exemption_files_test.py
@@ -0,0 +1,222 @@
+# Copyright 2025 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Tests for generate_warning_exemption_files."""
+
+import json
+import textwrap
+import unittest
+from unittest import mock
+
+# Rename this so the lines in this test aren't all super-long
+from llvm_tools import generate_warning_exemption_files as gen
+from llvm_tools import test_helpers
+
+
+class Test(test_helpers.TempDirTestCase):
+    """Tests for generate_warning_exemption_files."""
+
+    def test_warning_scraping_works(self):
+        stdout = textwrap.dedent(
+            """\
+            an error about flags:
+            clang-2: error: flag -foo is not supported [-Wfoo,-Werror]
+
+            another error about flags:
+            error: unknown warning option [-Werror,-Wfoo2]
+
+            an error about code:
+            /path/to/foo.cc:12:34: error: don't do this [-Werror,-Wbar]
+
+            a -Warning that's an error by default (thus lacks -Werror in
+            brackets), e.g., b/409989901 and b/325463152.
+            /path/to/foo.cc:12:34: error: don't do this either [-Wdefault-error]
+
+            general non-Werror warning:
+            /path/to/foo.cc:13:34: warning: fine, do this [-Wbaz]
+
+            weird non-Werror warning with , in []:
+            /path/to/foo.cc:14:34: warning: this is OK, too [-Wqux,-Wbaz]
+            """
+        )
+        self.assertEqual(
+            gen.scrape_fatal_warning_names_from_stdout(stdout),
+            ["bar", "default-error", "foo", "foo2"],
+        )
+
+    @mock.patch.object(gen, "scrape_fatal_warning_names_from_stdout")
+    def test_warning_file_parsing_works(self, mock_scrape):
+        mock_scrape.return_value = ["foo"]
+
+        tmpdir = self.make_tempdir()
+        tmpfile = tmpdir / "warnings_report1234.json"
+
+        warnings_file = {
+            # stdout is meaningless since we mock the scraping above.
+            "stdout": "",
+            "parent_process_data": [
+                {},
+                {
+                    "env": [],
+                },
+                {
+                    "env": [
+                        "CATEGORY=foo",
+                    ],
+                },
+                {
+                    "env": [
+                        "PN=bar",
+                    ],
+                },
+                {
+                    "env": [
+                        "CATEGORY=category",
+                        "PN=pkg-name",
+                    ],
+                },
+                {
+                    "env": [
+                        "CATEGORY=not-category",
+                        "PN=not-pkg-name",
+                    ],
+                },
+            ],
+        }
+
+        with tmpfile.open("w", encoding="utf-8") as f:
+            json.dump(warnings_file, f)
+
+        self.assertEqual(
+            gen.parse_fatal_warnings_file(tmpfile),
+            [
+                gen.FatalWarning(
+                    warning_name="foo",
+                    category="category",
+                    package_name="pkg-name",
+                )
+            ],
+        )
+
+    @mock.patch.object(gen, "scrape_fatal_warning_names_from_stdout")
+    def test_warning_file_parsing_handles_no_warnings(self, mock_scrape):
+        mock_scrape.return_value = []
+
+        tmpdir = self.make_tempdir()
+        tmpfile = tmpdir / "warnings_report1234.json"
+        warnings_file = {
+            "stdout": "",
+        }
+        with tmpfile.open("w", encoding="utf-8") as f:
+            json.dump(warnings_file, f)
+
+        self.assertEqual(gen.parse_fatal_warnings_file(tmpfile), [])
+
+    def test_warning_report_enumeration_works(self):
+        tmpdir = self.make_tempdir()
+        warning_reports = (
+            tmpdir / "foo" / "warnings_report1234.json",
+            tmpdir / "bar" / "baz" / "qux" / "warnings_report1235.json",
+        )
+        not_warning_reports = (
+            tmpdir / "foo" / "bar.json",
+            tmpdir / "baz" / "warnings_report1234.json.in_progress",
+            tmpdir / "qux",
+        )
+
+        for f in warning_reports + not_warning_reports:
+            f.parent.mkdir(parents=True, exist_ok=True)
+            f.touch()
+
+        self.assertEqual(
+            sorted(gen.find_all_warning_reports_in(tmpdir)),
+            sorted(warning_reports),
+        )
+
+    ### Below are essentially Go-lden file tests.
+
+    def test_go_file_creation_works_with_no_warnings(self):
+        # Set maxDiff to a very large value, since tiny diffs are
+        # borderline-useless given the size of these files.
+        self.maxDiff = 10000
+        actual = gen.create_go_file(
+            llvm_revision=123,
+            fatal_warnings={},
+        )
+        fname = "getWarningsForLLVM_r123"
+        expected = gen.GO_COPYRIGHT_HEADER + textwrap.dedent(
+            f"""\
+
+            package main
+
+            func {fname}(packageNameAndCategory string) []string {{
+                return nil
+            }}
+            """
+        )
+        self.assertEqual(expected, actual)
+
+    def test_go_file_creation_works_with_a_few_warnings(self):
+        # Set maxDiff to a very large value, since tiny diffs are
+        # borderline-useless given the size of these files.
+        self.maxDiff = 10000
+        amd64_generic = gen.Builder(
+            name="amd64-generic", url="https://amd64-generic-url"
+        )
+        brya = gen.Builder(name="brya", url="https://brya-url")
+        actual = gen.create_go_file(
+            llvm_revision=321,
+            fatal_warnings={
+                gen.FatalWarning(
+                    warning_name="foo",
+                    category="cat",
+                    package_name="pkg",
+                ): [brya],
+                gen.FatalWarning(
+                    warning_name="bar",
+                    category="cat",
+                    package_name="pkg",
+                ): [amd64_generic],
+                gen.FatalWarning(
+                    warning_name="baz",
+                    category="dog",
+                    package_name="pkg",
+                ): [brya],
+                gen.FatalWarning(
+                    warning_name="baz",
+                    category="snek",
+                    package_name="pkg",
+                ): [],
+            },
+        )
+        fname = "getWarningsForLLVM_r321"
+        expected = gen.GO_COPYRIGHT_HEADER + textwrap.dedent(
+            f"""\
+
+            package main
+
+            func {fname}(packageNameAndCategory string) []string {{
+                switch packageNameAndCategory {{
+                // Observed and suppressed on 2 builders during testing.
+                // e.g., amd64-generic: https://amd64-generic-url.
+                case "cat/pkg":
+                    return []string{{ "-Wno-bar", "-Wno-foo" }}
+                // Observed and suppressed on 1 builder during testing.
+                // e.g., brya: https://brya-url.
+                case "dog/pkg":
+                    return []string{{ "-Wno-baz" }}
+                // (No builder links were available for these exemptions).
+                case "snek/pkg":
+                    return []string{{ "-Wno-baz" }}
+                default:
+                    return nil
+                }}
+            }}
+            """
+        )
+        self.assertEqual(expected, actual)
+
+
+if __name__ == "__main__":
+    unittest.main()
diff --git a/llvm_tools/get_llvm_hash.py b/llvm_tools/get_llvm_hash.py
old mode 100755
new mode 100644
index 401a68a5..33b92518
--- a/llvm_tools/get_llvm_hash.py
+++ b/llvm_tools/get_llvm_hash.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -7,7 +6,10 @@
 
 import argparse
 import contextlib
+import dataclasses
+import fcntl
 import functools
+import logging
 import os
 from pathlib import Path
 import re
@@ -17,11 +19,14 @@ import sys
 import tempfile
 from typing import Iterator, Optional, Tuple, Union
 
-import chroot
-import git_llvm_rev
-import llvm_next
-import manifest_utils
-import subprocess_helpers
+from cros_utils import cros_paths
+from cros_utils import git_utils
+from llvm_tools import chroot
+from llvm_tools import cros_llvm_repo
+from llvm_tools import git_llvm_rev
+from llvm_tools import llvm_next
+from llvm_tools import manifest_utils
+from llvm_tools import subprocess_helpers
 
 
 _LLVM_GIT_URL = (
@@ -76,20 +81,6 @@ def GetGitHashFrom(src_dir: Union[Path, str], version: int) -> str:
     )
 
 
-def CheckoutBranch(src_dir: Union[Path, str], branch: str) -> None:
-    """Checks out and pulls from a branch in a git repo.
-
-    Args:
-        src_dir: The LLVM source tree.
-        branch: The git branch to checkout in src_dir.
-
-    Raises:
-        ValueError: Failed to checkout or pull branch version
-    """
-    subprocess_helpers.CheckCommand(["git", "-C", src_dir, "checkout", branch])
-    subprocess_helpers.CheckCommand(["git", "-C", src_dir, "pull"])
-
-
 def ParseLLVMMajorVersion(cmakelist: str) -> Optional[str]:
     """Reads CMakeList.txt file contents for LLVMMajor Version.
 
@@ -122,147 +113,204 @@ def GetLLVMMajorVersion(git_hash: Optional[str] = None) -> str:
           there was a failure to checkout git_hash version
         FileExistsError: The src directory doe not contain CMakeList.txt
     """
-    src_dir = GetAndUpdateLLVMProjectInLLVMTools()
-
     # b/325895866#comment36: the LLVM version number was moved from
     # `llvm/CMakeLists.txt` to `cmake/Modules/LLVMVersion.cmake` in upstream
     # commit 81e20472a0c5a4a8edc5ec38dc345d580681af81 (r530225). Until we no
     # longer care about looking before that, we need to support searching both
     # files.
     cmakelists_paths = (
-        Path(src_dir) / "llvm" / "CMakeLists.txt",
-        Path(src_dir) / "cmake" / "Modules" / "LLVMVersion.cmake",
+        "llvm/CMakeLists.txt",
+        "cmake/Modules/LLVMVersion.cmake",
     )
 
-    with contextlib.ExitStack() as on_exit:
-        if git_hash:
-            subprocess_helpers.CheckCommand(
-                ["git", "-C", src_dir, "checkout", git_hash]
-            )
-            on_exit.callback(CheckoutBranch, src_dir, git_llvm_rev.MAIN_BRANCH)
-
-        for path in cmakelists_paths:
-            try:
-                file_contents = path.read_text(encoding="utf-8")
-            except FileNotFoundError:
-                # If this file DNE (yet), ignore it.
-                continue
-
-            if version := ParseLLVMMajorVersion(file_contents):
-                return version
+    repo = GetCachedUpToDateReadOnlyLLVMRepo()
+    ref = git_hash if git_hash else "HEAD"
+    for path in cmakelists_paths:
+        contents = git_utils.maybe_show_file_at_commit(repo.path, ref, path)
+        if contents is None:
+            # Ignore the file if it doesn't exist yet.
+            continue
+        if version := ParseLLVMMajorVersion(contents):
+            return version
 
     raise ValueError(
         f"Major version could not be parsed from any of {cmakelists_paths}"
     )
 
 
-@contextlib.contextmanager
-def CreateTempLLVMRepo(temp_dir: str) -> Iterator[str]:
-    """Adds a LLVM worktree to 'temp_dir'.
+def _LockAndCloneLLVMProject(clone_target: Path, tmpdir_path: Path):
+    """Creates and locks `tmpdir_path`, and clones LLVM into it.
 
-    Creating a worktree because the LLVM source tree in
-    '../toolchain-utils/llvm_tools/llvm-project-copy' should not be modified.
-
-    This is useful for applying patches to a source tree but do not want to
-    modify the actual LLVM source tree in 'llvm-project-copy'.
+    Multithreading and multiprocessing safe.
 
     Args:
-        temp_dir: An absolute path to the temporary directory to put the
-        worktree in (obtained via 'tempfile.mkdtemp()').
+        clone_target: Where to place llvm-project.
+        tmpdir_path: A temporary directory to sync llvm-project in. Must share
+            the same parent directory as clone_target.
+    """
+    # This code is subtle, and relies on Linux guarantees outlined here:
+    # https://www.kernel.org/doc/Documentation/filesystems/directory-locking
+    #
+    # Specifically, this heavily leverages the idea that there's a total
+    # ordering of dirent additions/removals for each directory in a file
+    # system.
+    assert (
+        tmpdir_path.parent == clone_target.parent
+    ), f"{tmpdir_path} and {clone_target} must share a parent."
+
+    # `exist_ok=True` covers races with other processes.
+    tmpdir_path.mkdir(exist_ok=True)
+    try:
+        tmpdir_fd = os.open(tmpdir_path, os.O_RDONLY | os.O_DIRECTORY)
+    except FileNotFoundError:
+        # If this isn't found, another process removed this dir. This code
+        # _only_ removes this dir on a successful sync, so it must be that the
+        # sync was successful.
+        assert (
+            clone_target.exists()
+        ), f"{clone_target} should exist if {tmpdir_path} doesn't."
+        return
 
-    Yields:
-        The absolute path to 'temp_dir'.
+    try:
+        # Note that the lock is implicitly unlocked by the
+        # `os.close(tmpdir_fd)` in the `finally` block.
+        fcntl.flock(tmpdir_fd, fcntl.LOCK_EX)
+
+        # If a racing sync succeeded, exit early. Note that the existence of
+        # `clone_target` implies that our lock of `tmpdir_fd` may be
+        # non-exclusive (see the comment above `os.rename` below for more), as
+        # racing processes might've removed & recreated `tmpdir_path` since
+        # this one opened it.
+        if clone_target.exists():
+            # Catch FileNotFoundError due to non-exclusivity. In any case, it
+            # must be empty, since syncs are never started if `clone_target`
+            # exists.
+            try:
+                tmpdir_path.rmdir()
+            except FileNotFoundError:
+                pass
+            return
 
-    Raises:
-        subprocess.CalledProcessError: Failed to remove the worktree.
-        ValueError: Failed to add a worktree.
-    """
+        # Clean up from any potentially-incomplete racing syncs.
+        for child in tmpdir_path.iterdir():
+            shutil.rmtree(child)
 
-    abs_path_to_llvm_project_dir = GetAndUpdateLLVMProjectInLLVMTools()
-    subprocess_helpers.CheckCommand(
-        [
-            "git",
-            "-C",
-            abs_path_to_llvm_project_dir,
-            "worktree",
-            "add",
-            "--detach",
-            temp_dir,
-            "origin/%s" % git_llvm_rev.MAIN_BRANCH,
-        ]
-    )
+        subprocess.run(
+            ["git", "clone", _LLVM_GIT_URL, "."],
+            check=True,
+            cwd=tmpdir_path,
+        )
 
-    try:
-        yield temp_dir
+        # This `rename` makes our lock on `tmpdir_path` non-global, which is
+        # less dangerous than it may seem, since it simultaneously brings
+        # `clone_target` into existence.
+        #
+        # Leveraging Linux's file locking guarantees, this means that
+        # `os.open`s of `tmpdir_path`s that are created after this is renamed
+        # _necessarily_ have a view of `tmpdir_path.parent` that includes
+        # `clone_target`.
+        os.rename(tmpdir_path, clone_target)
     finally:
-        if os.path.isdir(temp_dir):
-            subprocess_helpers.check_output(
-                [
-                    "git",
-                    "-C",
-                    abs_path_to_llvm_project_dir,
-                    "worktree",
-                    "remove",
-                    "-f",
-                    temp_dir,
-                ]
-            )
+        os.close(tmpdir_fd)
 
 
-def GetAndUpdateLLVMProjectInLLVMTools() -> str:
-    """Gets the absolute path to 'llvm-project-copy' directory in 'llvm_tools'.
-
-    The intent of this function is to avoid cloning the LLVM repo and then
-    discarding the contents of the repo. The function will create a directory
-    in '../toolchain-utils/llvm_tools' called 'llvm-project-copy' if this
-    directory does not exist yet. If it does not exist, then it will use the
-    LLVMHash() class to clone the LLVM repo into 'llvm-project-copy'.
-    Otherwise, it will clean the contents of that directory and then fetch from
-    the chromium LLVM mirror. In either case, this function will return the
-    absolute path to 'llvm-project-copy' directory.
+def _GetToolchainUtilsCopyOfLLVMProject() -> Path:
+    """Inits and returns ${toolchain_utils}/llvm_tools/llvm-project-copy.
 
     Returns:
-        Absolute path to 'llvm-project-copy' directory in 'llvm_tools'
-
-    Raises:
-        ValueError: LLVM repo (in 'llvm-project-copy' dir.) has changes or
-        failed to checkout to main or failed to fetch from chromium mirror of
-        LLVM.
+        The absolute path to the 'llvm-project-copy' directory in 'llvm_tools'
     """
+    # NOTE: At the moment, the initial sync of this is not thread-safe. It'd be
+    # nice to have a flock of some sort of toolchain-utils-local stamp for
+    # that.
+    llvm_project_copy = Path(__file__).resolve().parent / "llvm-project-copy"
+    if llvm_project_copy.is_dir():
+        return llvm_project_copy
+
+    print(
+        f"llvm-project checkout requested; checking out {llvm_project_copy}.\n"
+        "This may take a while, but only has to be done once.",
+        file=sys.stderr,
+    )
+    tmp_llvm_project_copy = llvm_project_copy.parent / ".llvm-project-copy"
+    _LockAndCloneLLVMProject(
+        clone_target=llvm_project_copy, tmpdir_path=tmp_llvm_project_copy
+    )
+    assert llvm_project_copy.is_dir(), llvm_project_copy
+    return llvm_project_copy
 
-    abs_path_to_llvm_tools_dir = os.path.dirname(os.path.abspath(__file__))
 
-    abs_path_to_llvm_project_dir = os.path.join(
-        abs_path_to_llvm_tools_dir, "llvm-project-copy"
-    )
+@dataclasses.dataclass(frozen=True)
+class ReadOnlyLLVMRepo:
+    """Describes an LLVM repository, and provides some useful ops on it.
+
+    Strictly speaking, `read-only` is a bit of a misnomer: the git data of this
+    repo may be updated by users of this class. The expectation is that the
+    working tree won't be modified, though.
+    """
 
-    if not os.path.isdir(abs_path_to_llvm_project_dir):
-        print(
-            f"Checking out LLVM to {abs_path_to_llvm_project_dir}\n"
-            "so that we can map between commit hashes and revision numbers.\n"
-            "This may take a while, but only has to be done once.",
-            file=sys.stderr,
+    # Path to the repository.
+    path: Path
+    # The name of the remote to query.
+    remote: str
+    # The ref that points to the upstream's main branch.
+    upstream_main: str
+
+    def GetRevisionFromHash(self, git_hash: str) -> int:
+        """Converts a SHA to an svn-like revision."""
+        version = git_llvm_rev.translate_sha_to_rev(
+            git_llvm_rev.LLVMConfig(remote=self.remote, dir=self.path), git_hash
         )
-        os.mkdir(abs_path_to_llvm_project_dir)
+        # Note: branches aren't supported. Always match against
+        # `git_llvm_rev.MAIN_BRANCH` instead of `upstream_main`, since
+        # `git_llvm_rev` doesn't acknowledge `upstream_main`.
+        assert version.branch == git_llvm_rev.MAIN_BRANCH, (
+            "Revisions only make sense on main, but given git hash was "
+            f"on {version.branch}"
+        )
+        return version.number
 
-        LLVMHash().CloneLLVMRepo(abs_path_to_llvm_project_dir)
-    else:
-        # `git status` has a '-s'/'--short' option that shortens the output.
-        # With the '-s' option, if no changes were made to the LLVM repo, then
-        # the output (assigned to 'repo_status') would be empty.
-        repo_status = subprocess_helpers.check_output(
-            ["git", "-C", abs_path_to_llvm_project_dir, "status", "-s"]
+    def GetHashFromRevision(self, revision: int) -> str:
+        """Converts a svn-like revision to a SHA on main."""
+        return git_llvm_rev.translate_rev_to_sha(
+            git_llvm_rev.LLVMConfig(remote=self.remote, dir=self.path),
+            git_llvm_rev.Rev(branch=self.upstream_main, number=revision),
         )
 
-        if repo_status.rstrip():
-            raise ValueError(
-                "LLVM repo in %s has changes, please remove."
-                % abs_path_to_llvm_project_dir
-            )
 
-        CheckoutBranch(abs_path_to_llvm_project_dir, git_llvm_rev.MAIN_BRANCH)
+def GetReadOnlyLLVMRepo() -> ReadOnlyLLVMRepo:
+    """Returns a read-only LLVM repository."""
+    if cros_llvm := cros_llvm_repo.try_get_path():
+        return ReadOnlyLLVMRepo(
+            path=cros_llvm,
+            remote=cros_llvm_repo.UPSTREAM_REMOTE,
+            upstream_main=cros_llvm_repo.UPSTREAM_MAIN,
+        )
+    return ReadOnlyLLVMRepo(
+        path=_GetToolchainUtilsCopyOfLLVMProject(),
+        remote="origin",
+        upstream_main=git_llvm_rev.MAIN_BRANCH,
+    )
+
+
+def GetUpToDateReadOnlyLLVMRepo() -> ReadOnlyLLVMRepo:
+    """GetReadOnlyLLVMRepo, with an added `git fetch` step."""
+    repo = GetReadOnlyLLVMRepo()
+    logging.info("Updating LLVM repository at %s...", repo.path)
+    subprocess.run(
+        ["git", "fetch", "--quiet", repo.remote, repo.upstream_main],
+        check=True,
+        cwd=repo.path,
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.DEVNULL,
+    )
+    return repo
 
-    return abs_path_to_llvm_project_dir
+
+@functools.lru_cache(1)
+def GetCachedUpToDateReadOnlyLLVMRepo() -> ReadOnlyLLVMRepo:
+    """GetUpToDateReadOnlyLLVMRepo, but will cache the result."""
+    return GetUpToDateReadOnlyLLVMRepo()
 
 
 def GetGoogle3LLVMVersion(stable: bool) -> int:
@@ -278,10 +326,7 @@ def GetGoogle3LLVMVersion(stable: bool) -> int:
         subprocess.CalledProcessError: An invalid path has been provided to the
         `cat` command.
     """
-
     subdir = "stable" if stable else "llvm_unstable"
-
-    # Cmd to get latest google3 LLVM version.
     cmd = [
         "cat",
         os.path.join(
@@ -290,14 +335,8 @@ def GetGoogle3LLVMVersion(stable: bool) -> int:
             "installs/llvm/git_origin_rev_id",
         ),
     ]
-
-    # Get latest version.
-    git_hash = subprocess_helpers.check_output(cmd)
-
-    # Change type to an integer
-    return GetVersionFrom(
-        GetAndUpdateLLVMProjectInLLVMTools(), git_hash.rstrip()
-    )
+    git_hash = subprocess_helpers.check_output(cmd).rstrip()
+    return GetCachedUpToDateReadOnlyLLVMRepo().GetRevisionFromHash(git_hash)
 
 
 def IsSvnOption(svn_option: str) -> Union[int, str]:
@@ -347,36 +386,60 @@ def GetLLVMHashAndVersionFromSVNOption(
     """
 
     new_llvm_hash = LLVMHash()
-
+    llvm_repo = GetCachedUpToDateReadOnlyLLVMRepo()
     # Determine which LLVM git hash to retrieve.
     if svn_option == "tot":
         git_hash = new_llvm_hash.GetTopOfTrunkGitHash()
-        version = GetVersionFrom(GetAndUpdateLLVMProjectInLLVMTools(), git_hash)
+        version = llvm_repo.GetRevisionFromHash(git_hash)
     elif isinstance(svn_option, int):
         version = svn_option
-        git_hash = GetGitHashFrom(GetAndUpdateLLVMProjectInLLVMTools(), version)
+        git_hash = llvm_repo.GetHashFromRevision(version)
     else:
         assert svn_option in ("google3", "google3-unstable")
         version = GetGoogle3LLVMVersion(stable=svn_option == "google3")
-
-        git_hash = GetGitHashFrom(GetAndUpdateLLVMProjectInLLVMTools(), version)
+        git_hash = llvm_repo.GetHashFromRevision(version)
 
     return git_hash, version
 
 
-def GetCrOSCurrentLLVMHash(chromeos_tree: Path) -> str:
+def GetCrOSCurrentLLVMHash(chromeos_root: Path) -> str:
     """Retrieves the current ChromeOS LLVM hash.
 
+    Specifically, this returns the _upstream_ hash that ChromeOS' LLVM is based
+    on.
+
     Args:
-        chromeos_tree: A ChromeOS source tree. This is allowed to be
-        arbitrary subdirectory of an actual ChromeOS tree, for convenience.
+        chromeos_root: A ChromeOS source tree root.
 
     Raises:
+        AssertionError if `chromeos_root` isn't a CrOS tree root.
         ManifestValueError if the toolchain manifest doesn't match the
         expected structure.
     """
-    chromeos_root = chroot.FindChromeOSRootAbove(chromeos_tree)
-    return manifest_utils.extract_current_llvm_hash(chromeos_root)
+    assert chroot.IsChromeOSRoot(
+        chromeos_root
+    ), f"{chromeos_root} isn't the root of a ChromeOS checkout"
+    llvm_project = chromeos_root / cros_paths.LLVM_PROJECT
+    hash_or_ref = manifest_utils.extract_current_llvm_hash_or_ref(chromeos_root)
+    refs_heads = "refs/heads/"
+    # If this is a hash, we're done.
+    if not hash_or_ref.startswith(refs_heads):
+        return hash_or_ref
+
+    # Otherwise, find the `merge-base` between upstream and what we have.
+    ref = hash_or_ref[len(refs_heads) :]
+    cros_ref = f"{cros_llvm_repo.UPSTREAM_REMOTE}/{ref}"
+    llvm_repo = GetCachedUpToDateReadOnlyLLVMRepo()
+    llvm_upstream_main = f"{llvm_repo.remote}/{llvm_repo.upstream_main}"
+    merge_base = git_utils.merge_base(
+        llvm_repo.path,
+        [cros_ref, llvm_upstream_main],
+    )
+    if not merge_base:
+        raise ValueError(
+            f"Can't find a merge-base between {cros_ref} and {llvm_upstream_main}"
+        )
+    return merge_base
 
 
 class LLVMHash:
@@ -421,17 +484,18 @@ class LLVMHash:
         Returns:
             The hash as a string that corresponds to the LLVM version.
         """
-        hash_value = GetGitHashFrom(
-            GetAndUpdateLLVMProjectInLLVMTools(), version
-        )
-        return hash_value
+        return GetCachedUpToDateReadOnlyLLVMRepo().GetHashFromRevision(version)
 
     def GetCrOSCurrentLLVMHash(self, chromeos_tree: Path) -> str:
         """Retrieves the current ChromeOS LLVM hash."""
         return GetCrOSCurrentLLVMHash(chromeos_tree)
 
     def GetCrOSLLVMNextHash(self) -> str:
-        """Retrieves the current ChromeOS llvm-next hash."""
+        """Retrieves the current ChromeOS llvm-next hash.
+
+        Specifically, this returns the _upstream_ hash that ChromeOS' LLVM-next
+        is based on.
+        """
         return llvm_next.LLVM_NEXT_HASH
 
     def GetGoogle3LLVMHash(self) -> str:
@@ -452,13 +516,59 @@ class LLVMHash:
         return llvm_tot_git_hash.rstrip().split()[0]
 
 
+def DetectLatestLLVMBranch(
+    chromiumos_tree: Path,
+    rev: int,
+) -> Optional[str]:
+    """Returns the latest llvm-next branch for `rev`.
+
+    If no branches exist for `rev`, returns None.
+    """
+    llvm_project = chromiumos_tree / cros_paths.LLVM_PROJECT
+    # Fetch ahead of time, so we always have the most up-to-date set of remote
+    # refs possible.
+    git_utils.fetch(llvm_project, remote=git_utils.CROS_EXTERNAL_REMOTE)
+    branch_prefix = f"cros/chromeos/llvm-r{rev}-"
+    # Note that `branches` has strings with leading prefixes (e.g., `remotes/`).
+    # The code below is written to ignore those.
+    branches = git_utils.branch_list(llvm_project, glob=f"{branch_prefix}*")
+    llvm_branch_re = re.compile(re.escape(branch_prefix) + r"(\d+)$")
+    most_recent_branch = None
+    most_recent_branch_number = None
+    for branch_path in branches:
+        m = llvm_branch_re.search(branch_path)
+        if not m:
+            logging.warning(
+                "Ignoring llvm branch %s, which doesn't match regex %s?",
+                branch_path,
+                llvm_branch_re,
+            )
+            continue
+
+        branch = branch_path[m.start() : m.end()]
+        branch_number = int(m.group(1))
+        if (
+            most_recent_branch_number is not None
+            and branch_number < most_recent_branch_number
+        ):
+            continue
+
+        most_recent_branch = branch
+        most_recent_branch_number = branch_number
+    return most_recent_branch
+
+
 def main() -> None:
     """Prints the git hash of LLVM.
 
     Parses the command line for the optional command line
     arguments.
     """
-    my_dir = Path(__file__).parent.resolve()
+    logging.basicConfig(
+        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
+        "%(message)s",
+        level=logging.INFO,
+    )
 
     # Create parser and add optional command-line arguments.
     parser = argparse.ArgumentParser(description="Finds the LLVM hash.")
@@ -472,7 +582,6 @@ def main() -> None:
     parser.add_argument(
         "--chromeos_tree",
         type=Path,
-        required=True,
         help="""
         Path to a ChromeOS tree. If not passed, one will be inferred. If none
         can be inferred, this script will fail.
@@ -489,7 +598,7 @@ def main() -> None:
         # be more easily detected (which allows more flexibility in the
         # implementation in the future for things outside of what directly
         # needs this value).
-        chromeos_tree = chroot.FindChromeOSRootAbove(my_dir)
+        chromeos_tree = chroot.FindChromeOSRootAboveToolchainUtils()
 
     new_llvm_hash = LLVMHash()
     if isinstance(cur_llvm_version, int):
@@ -506,7 +615,3 @@ def main() -> None:
     else:
         assert cur_llvm_version == "tot"
         print(new_llvm_hash.GetTopOfTrunkGitHash())
-
-
-if __name__ == "__main__":
-    main()
diff --git a/llvm_tools/get_llvm_hash_unittest.py b/llvm_tools/get_llvm_hash_test.py
old mode 100755
new mode 100644
similarity index 65%
rename from llvm_tools/get_llvm_hash_unittest.py
rename to llvm_tools/get_llvm_hash_test.py
index fda002ce..2f4585a2
--- a/llvm_tools/get_llvm_hash_unittest.py
+++ b/llvm_tools/get_llvm_hash_test.py
@@ -1,22 +1,19 @@
-#!/usr/bin/env python3
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
 """Unit tests for retrieving the LLVM hash."""
 
-import contextlib
 from pathlib import Path
-import shutil
 import subprocess
-import tempfile
 import textwrap
-import unittest
+from typing import Optional
 from unittest import mock
 
-import get_llvm_hash
-import llvm_next
-import subprocess_helpers
+from cros_utils import git_utils
+from llvm_tools import get_llvm_hash
+from llvm_tools import llvm_next
+from llvm_tools import test_helpers
 
 
 # We grab protected stuff from get_llvm_hash. That's OK.
@@ -30,14 +27,9 @@ def mock_run_results(returncode: int, stderr: bytes) -> mock.MagicMock:
     return m
 
 
-class TestGetLLVMHash(unittest.TestCase):
+class TestGetLLVMHash(test_helpers.TempDirTestCase):
     """The LLVMHash test class."""
 
-    def make_tempdir(self):
-        d = Path(tempfile.mkdtemp(prefix="get_llvm_hash_unittest_"))
-        self.addCleanup(shutil.rmtree, d)
-        return d
-
     def setUp(self):
         # We mock out quite a bit. Ensure every test is self-contained.
         get_llvm_hash.GetLLVMMajorVersion.cache_clear()
@@ -107,21 +99,6 @@ class TestGetLLVMHash(unittest.TestCase):
         )
         mock_check_output.assert_called_once()
 
-    @mock.patch.object(subprocess, "Popen")
-    def testCheckoutBranch(self, mock_popen):
-        mock_popen.return_value = contextlib.nullcontext(
-            mock.MagicMock(communicate=lambda: (None, None), returncode=0)
-        )
-        get_llvm_hash.CheckoutBranch("fake/src_dir", "fake_branch")
-        self.assertEqual(
-            mock_popen.call_args_list[0][0],
-            (["git", "-C", "fake/src_dir", "checkout", "fake_branch"],),
-        )
-        self.assertEqual(
-            mock_popen.call_args_list[1][0],
-            (["git", "-C", "fake/src_dir", "pull"],),
-        )
-
     def testParseLLVMMajorVersion(self):
         cmakelist_42 = (
             "set(CMAKE_BUILD_WITH_INSTALL_NAME_DIR ON)\n"
@@ -139,77 +116,79 @@ class TestGetLLVMHash(unittest.TestCase):
             get_llvm_hash.ParseLLVMMajorVersion(invalid_cmakelist)
         )
 
-    @mock.patch.object(get_llvm_hash, "GetAndUpdateLLVMProjectInLLVMTools")
-    @mock.patch.object(subprocess_helpers, "CheckCommand")
+    @mock.patch.object(get_llvm_hash, "GetCachedUpToDateReadOnlyLLVMRepo")
+    @mock.patch.object(git_utils, "maybe_show_file_at_commit")
     def testGetLLVMMajorVersionWithOldPath(
         self,
-        _mock_check_command,
-        mock_update_project,
+        mock_show_file_at_commit,
+        mock_get_up_to_date_repo,
     ):
         src_dir = self.make_tempdir()
-        mock_update_project.return_value = str(src_dir)
-
-        cmakelists = Path(src_dir) / "llvm" / "CMakeLists.txt"
-        cmakelists.parent.mkdir(parents=True)
-        cmakelists.write_text(
-            textwrap.dedent(
+        mock_get_up_to_date_repo.return_value = get_llvm_hash.ReadOnlyLLVMRepo(
+            path=src_dir,
+            remote="origin",
+            upstream_main="main",
+        )
+
+        def show_file_at_commit(
+            repo: Path, ref: str, path: str
+        ) -> Optional[str]:
+            self.assertEqual(repo, src_dir)
+            self.assertEqual(ref, "HEAD")
+            self.assertEqual(path, "llvm/CMakeLists.txt")
+            return textwrap.dedent(
                 """
                 if(NOT DEFINED LLVM_VERSION_MAJOR)
                   set(LLVM_VERSION_MAJOR 12345)
                 endif()
                 """
-            ),
-            encoding="utf-8",
-        )
+            )
+
+        mock_show_file_at_commit.side_effect = show_file_at_commit
         self.assertEqual(get_llvm_hash.GetLLVMMajorVersion(), "12345")
 
-    @mock.patch.object(get_llvm_hash, "GetAndUpdateLLVMProjectInLLVMTools")
-    @mock.patch.object(subprocess_helpers, "CheckCommand")
+    @mock.patch.object(get_llvm_hash, "GetCachedUpToDateReadOnlyLLVMRepo")
+    @mock.patch.object(git_utils, "maybe_show_file_at_commit")
     def testGetLLVMMajorVersionWithNewPath(
         self,
-        _mock_check_command,
-        mock_update_project,
+        mock_show_file_at_commit,
+        mock_get_up_to_date_repo,
     ):
         src_dir = self.make_tempdir()
-        mock_update_project.return_value = str(src_dir)
-
-        old_cmakelists = Path(src_dir) / "llvm" / "CMakeLists.txt"
-        old_cmakelists.parent.mkdir(parents=True)
-        old_cmakelists.write_text(
-            textwrap.dedent(
-                """
-                Some text
-                that has
-                nothing to do with
-                LLVM_VERSION_MAJOR
-                """
-            ),
-            encoding="utf-8",
-        )
-
-        new_cmakelists = (
-            Path(src_dir) / "cmake" / "Modules" / "LLVMVersion.cmake"
-        )
-        new_cmakelists.parent.mkdir(parents=True)
-        new_cmakelists.write_text(
-            textwrap.dedent(
+        mock_get_up_to_date_repo.return_value = get_llvm_hash.ReadOnlyLLVMRepo(
+            path=src_dir,
+            remote="origin",
+            upstream_main="main",
+        )
+
+        def show_file_at_commit(
+            repo: Path, ref: str, path: str
+        ) -> Optional[str]:
+            self.assertEqual(repo, src_dir)
+            self.assertEqual(ref, "HEAD")
+            if path == "llvm/CMakeLists.txt":
+                return textwrap.dedent(
+                    """
+                    Some text
+                    that has
+                    nothing to do with
+                    LLVM_VERSION_MAJOR
+                    """
+                )
+            self.assertEqual(path, "cmake/Modules/LLVMVersion.cmake")
+            return textwrap.dedent(
                 """
                 if(NOT DEFINED LLVM_VERSION_MAJOR)
-                  set(LLVM_VERSION_MAJOR 5432)
+                  set(LLVM_VERSION_MAJOR 12345)
                 endif()
                 """
-            ),
-            encoding="utf-8",
-        )
+            )
 
-        self.assertEqual(get_llvm_hash.GetLLVMMajorVersion(), "5432")
+        mock_show_file_at_commit.side_effect = show_file_at_commit
+        self.assertEqual(get_llvm_hash.GetLLVMMajorVersion(), "12345")
 
     def testGetLLVMNextHash(self):
         self.assertEqual(
             get_llvm_hash.LLVMHash().GetCrOSLLVMNextHash(),
             llvm_next.LLVM_NEXT_HASH,
         )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/get_patch.py b/llvm_tools/get_patch.py
old mode 100755
new mode 100644
index 3d89fbc5..708785df
--- a/llvm_tools/get_patch.py
+++ b/llvm_tools/get_patch.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -27,20 +26,20 @@ import textwrap
 from typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union
 from urllib import request
 
-import atomic_write_file
-import git_llvm_rev
-import patch_utils
+from cros_utils import cros_paths
+from cros_utils import git_utils
+from llvm_tools import atomic_write_file
+from llvm_tools import git_llvm_rev
+from llvm_tools import patch_utils
 
 
-CHROMIUMOS_OVERLAY_PATH = Path("src/third_party/chromiumos-overlay")
-LLVM_PKG_PATH = CHROMIUMOS_OVERLAY_PATH / "sys-devel/llvm"
-COMPILER_RT_PKG_PATH = CHROMIUMOS_OVERLAY_PATH / "sys-libs/compiler-rt"
-LIBCXX_PKG_PATH = CHROMIUMOS_OVERLAY_PATH / "sys-libs/libcxx"
-LIBUNWIND_PKG_PATH = CHROMIUMOS_OVERLAY_PATH / "sys-libs/llvm-libunwind"
-SCUDO_PKG_PATH = CHROMIUMOS_OVERLAY_PATH / "sys-libs/scudo"
-LLDB_PKG_PATH = CHROMIUMOS_OVERLAY_PATH / "dev-util/lldb-server"
+LLVM_PKG_PATH = cros_paths.CHROMIUMOS_OVERLAY / "sys-devel/llvm"
+COMPILER_RT_PKG_PATH = cros_paths.CHROMIUMOS_OVERLAY / "sys-libs/compiler-rt"
+LIBCXX_PKG_PATH = cros_paths.CHROMIUMOS_OVERLAY / "sys-libs/libcxx"
+LIBUNWIND_PKG_PATH = cros_paths.CHROMIUMOS_OVERLAY / "sys-libs/llvm-libunwind"
+SCUDO_PKG_PATH = cros_paths.CHROMIUMOS_OVERLAY / "sys-libs/scudo"
+LLDB_PKG_PATH = cros_paths.CHROMIUMOS_OVERLAY / "dev-util/lldb-server"
 
-LLVM_PROJECT_PATH = Path("src/third_party/llvm-project")
 PATCH_METADATA_FILENAME = "PATCHES.json"
 
 
@@ -192,10 +191,11 @@ class PatchContext:
             pe = patch_utils.PatchEntry(
                 workdir=workdir,
                 metadata={
+                    "info": [],
+                    "original_sha": patch_source.git_ref,
                     "title": get_commit_subj(
                         self.llvm_project_dir, patch_source.git_ref
                     ),
-                    "info": [],
                 },
                 platforms=list(self.platforms),
                 rel_patch_path=rel_patch_path,
@@ -211,7 +211,7 @@ class PatchContext:
                     f"Patch at {pe.rel_patch_path}"
                     " already exists in PATCHES.json"
                 )
-            contents = git_format_patch(
+            contents = _git_format_patch(
                 self.llvm_project_dir,
                 patch_source.git_ref,
             )
@@ -233,6 +233,7 @@ class PatchContext:
                 workdir=workdir,
                 metadata={
                     "title": github_ctx.full_title,
+                    "original_sha": None,
                     "info": [],
                 },
                 rel_patch_path=rel_patch_path,
@@ -291,31 +292,6 @@ def get_commit_subj(git_root_dir: Path, ref: str) -> str:
     return subj
 
 
-def git_format_patch(git_root_dir: Path, ref: str) -> str:
-    """Format a patch for a single git ref.
-
-    Args:
-        git_root_dir: Root directory for a given local git repository.
-        ref: Git ref to make a patch for.
-
-    Returns:
-        The patch file contents.
-    """
-    logging.debug("Formatting patch for %s^..%s", ref, ref)
-    proc = subprocess.run(
-        ["git", "format-patch", "--stdout", f"{ref}^..{ref}"],
-        cwd=git_root_dir,
-        encoding="utf-8",
-        stdout=subprocess.PIPE,
-        check=True,
-    )
-    contents = proc.stdout.strip()
-    if not contents:
-        raise ValueError(f"No git diff between {ref}^..{ref}")
-    logging.debug("Patch diff is %d lines long", contents.count("\n"))
-    return contents
-
-
 def get_llvm_github_pull(pull_number: int) -> Dict[str, Any]:
     """Get information about an LLVM pull request.
 
@@ -452,7 +428,7 @@ class GitHubPRContext:
                 changed_packages = get_changed_packages(
                     worktree_dir, (self.base_ref, "HEAD")
                 )
-                patch_contents = git_format_patch(worktree_dir, "HEAD")
+                patch_contents = _git_format_patch(worktree_dir, "HEAD")
             finally:
                 logging.debug(
                     "Cleaning up worktree and deleting branch %s",
@@ -583,8 +559,14 @@ def _write_patch(title: str, contents: str, path: Path) -> None:
     path.write_text(contents, encoding="utf-8")
 
 
+def _git_format_patch(git_dir: Path, ref: str) -> str:
+    """Wrapper for git_utils.format_patch. Used for mocking."""
+    return git_utils.format_patch(git_dir, ref)
+
+
 def validate_patch_args(
     positional_args: List[str],
+    llvm_project: Path,
 ) -> List[Union[LLVMGitRef, LLVMPullRequest]]:
     """Checks that each ref_or_pr_num is in a valid format."""
     patch_sources = []
@@ -601,8 +583,15 @@ def validate_patch_args(
             logging.info("Patching remote GitHub PR '%s'", pull_request_num)
             patch_source = LLVMPullRequest(pull_request_num)
         else:
-            logging.info("Patching local ref '%s'", arg)
-            patch_source = LLVMGitRef(arg)
+            if git_utils.is_full_git_sha(arg):
+                logging.info("Patching local ref '%s'", arg)
+                full_sha = arg
+            else:
+                full_sha = git_utils.resolve_ref(llvm_project, arg)
+                logging.info(
+                    "Patching local ref '%s' (expands to %s)", arg, full_sha
+                )
+            patch_source = LLVMGitRef(full_sha)
         patch_sources.append(patch_source)
     return patch_sources
 
@@ -618,14 +607,16 @@ def parse_args() -> argparse.Namespace:
     parser.add_argument(
         "-c",
         "--chromiumos-root",
-        help="""Path to the chromiumos source tree root.
+        help="""
+        Path to the chromiumos source tree root.
         Tries to autodetect if not passed.
         """,
     )
     parser.add_argument(
         "-l",
         "--llvm",
-        help="""Path to the llvm dir.
+        help="""
+        Path to the llvm dir.
         Tries to autodetect from chromiumos root if not passed.
         """,
     )
@@ -633,14 +624,14 @@ def parse_args() -> argparse.Namespace:
         "-s",
         "--start-ref",
         default="HEAD",
-        help="""The starting ref for which to apply patches.
-        """,
+        help="The starting ref for which to apply patches.",
     )
     parser.add_argument(
         "-p",
         "--platform",
         action="append",
-        help="""Apply this patch to the give platform. Common options include
+        help="""
+        Apply this patch to the give platform. Common options include
         'chromiumos' and 'android'. Can be specified multiple times to
         apply to multiple platforms. If not passed, platform is set to
         'chromiumos'.
@@ -660,7 +651,8 @@ def parse_args() -> argparse.Namespace:
     parser.add_argument(
         "ref_or_pr_num",
         nargs="+",
-        help="""Git ref or GitHub PR number to make patches.
+        help="""
+        Git ref or GitHub PR number to make patches.
         To patch a GitHub PR, use the syntax p:NNNN (e.g. 'p:123456').
         """,
         type=str,
@@ -673,7 +665,6 @@ def parse_args() -> argparse.Namespace:
         level=logging.DEBUG if args.verbose else logging.INFO,
     )
 
-    args.patch_sources = validate_patch_args(args.ref_or_pr_num)
     if args.chromiumos_root:
         if not _has_repo_child(args.chromiumos_root):
             parser.error("chromiumos root directly passed but has no .repo")
@@ -688,13 +679,15 @@ def parse_args() -> argparse.Namespace:
         )
 
     if not args.llvm:
-        if (args.chromiumos_root / LLVM_PROJECT_PATH).is_dir():
-            args.llvm = args.chromiumos_root / LLVM_PROJECT_PATH
+        if (args.chromiumos_root / cros_paths.LLVM_PROJECT).is_dir():
+            args.llvm = args.chromiumos_root / cros_paths.LLVM_PROJECT
         else:
             parser.error(
                 "Could not autodetect llvm-project dir. Use '-l' to pass the "
                 "llvm-project directly"
             )
+
+    args.patch_sources = validate_patch_args(args.ref_or_pr_num, args.llvm)
     return args
 
 
@@ -716,7 +709,3 @@ def main() -> None:
     )
     for patch_source in args.patch_sources:
         ctx.apply_patches(patch_source)
-
-
-if __name__ == "__main__":
-    main()
diff --git a/llvm_tools/get_patch_unittest.py b/llvm_tools/get_patch_test.py
old mode 100755
new mode 100644
similarity index 93%
rename from llvm_tools/get_patch_unittest.py
rename to llvm_tools/get_patch_test.py
index 8ccdb69b..74a4ba16
--- a/llvm_tools/get_patch_unittest.py
+++ b/llvm_tools/get_patch_test.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -12,8 +11,8 @@ from typing import Any, Dict, Generator, List, Set
 import unittest
 from unittest import mock
 
-import get_patch
-import git_llvm_rev
+from llvm_tools import get_patch
+from llvm_tools import git_llvm_rev
 
 
 COMMIT_FIXTURES: List[Dict[str, Any]] = [
@@ -71,16 +70,16 @@ class TestGetPatch(unittest.TestCase):
         """Set up the mocks and directory structure."""
 
         self.module_patcher = mock.patch.multiple(
-            "get_patch",
+            get_patch,
             get_commit_subj=_mock_get_commit_subj,
-            git_format_patch=_mock_git_format_patch,
+            _git_format_patch=_mock_git_format_patch,
             get_changed_packages=_mock_get_changed_packages,
             _write_patch=_mock_write_patch,
         )
         self.module_patcher.start()
         self.addCleanup(self.module_patcher.stop)
         self.llvm_gitsha_patcher = mock.patch.multiple(
-            "get_patch.LLVMGitRef",
+            get_patch.LLVMGitRef,
             to_rev=_mock_to_rev,
             from_rev=_mock_from_rev,
         )
@@ -94,11 +93,18 @@ class TestGetPatch(unittest.TestCase):
         self.workdir = self.chromiumos_root / get_patch.LLVM_PKG_PATH / "files"
         self.workdir.mkdir(parents=True, exist_ok=True)
 
+        self.patches_json_file = (
+            self.workdir / get_patch.PATCH_METADATA_FILENAME
+        )
+
         def _cleanup_workdir():
             # We individually clean up these directories as a guarantee
             # we aren't creating any extraneous files. We don't want to
             # use shm.rmtree here because we don't want clean up any
             # files unaccounted for.
+            if self.patches_json_file.exists():
+                self.patches_json_file.unlink()
+
             workdir_recurse = self.workdir
             while workdir_recurse not in (self.chromiumos_root, Path.root):
                 workdir_recurse.rmdir()
@@ -106,9 +112,6 @@ class TestGetPatch(unittest.TestCase):
 
         self.addCleanup(_cleanup_workdir)
 
-        self.patches_json_file = (
-            self.workdir / get_patch.PATCH_METADATA_FILENAME
-        )
         start_ref = get_patch.LLVMGitRef("abcdef1234567890")
         self.ctx = get_patch.PatchContext(
             self.llvm_project_dir,
@@ -161,7 +164,11 @@ class TestGetPatch(unittest.TestCase):
         fixture = COMMIT_FIXTURES[1]
         fixture_sha = fixture["sha"]
         expected_json_entry = {
-            "metadata": {"title": fixture["subject"], "info": []},
+            "metadata": {
+                "title": fixture["subject"],
+                "original_sha": fixture_sha,
+                "info": [],
+            },
             "platforms": ["some platform"],
             "rel_patch_path": f"cherry/{fixture_sha}.patch",
             "version_range": {
@@ -180,7 +187,11 @@ class TestGetPatch(unittest.TestCase):
         fixture = COMMIT_FIXTURES[1]
         fixture_sha = fixture["sha"]
         expected_json_entry = {
-            "metadata": {"title": fixture["subject"], "info": []},
+            "metadata": {
+                "title": fixture["subject"],
+                "original_sha": fixture_sha,
+                "info": [],
+            },
             "platforms": ["some platform"],
             "rel_patch_path": f"{fixture_sha}.patch",
             "version_range": {
@@ -227,7 +238,3 @@ class TestGetPatch(unittest.TestCase):
         finally:
             self.ctx.dry_run = old_dry_run
             self.patches_json_file.unlink()
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/get_upstream_patch.py b/llvm_tools/get_upstream_patch.py
deleted file mode 100755
index 7335aade..00000000
--- a/llvm_tools/get_upstream_patch.py
+++ /dev/null
@@ -1,593 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Get an upstream patch to LLVM's PATCHES.json."""
-
-import argparse
-import dataclasses
-import datetime
-import json
-import logging
-import os
-from pathlib import Path
-import subprocess
-import sys
-import typing as t
-
-import chroot
-import get_llvm_hash
-import git
-import git_llvm_rev
-import patch_utils
-
-
-__DOC_EPILOGUE = """
-Example Usage:
-  get_upstream_patch --chromeos_path ~/chromiumos --platform chromiumos \
---sha 1234567 --sha 890abdc
-"""
-
-
-class CherrypickError(ValueError):
-    """A ValueError that highlights the cherry-pick has been seen before"""
-
-
-class CherrypickVersionError(ValueError):
-    """A ValueError that highlights the cherry-pick is before the start_sha"""
-
-
-class PatchApplicationError(ValueError):
-    """A ValueError indicating that a test patch application was unsuccessful"""
-
-
-def validate_patch_application(
-    llvm_dir: Path, svn_version: int, patches_json_fp: Path, patch_props
-):
-    start_sha = get_llvm_hash.GetGitHashFrom(llvm_dir, svn_version)
-    subprocess.run(["git", "-C", llvm_dir, "checkout", start_sha], check=True)
-
-    predecessor_apply_results = patch_utils.apply_all_from_json(
-        svn_version, llvm_dir, patches_json_fp, continue_on_failure=True
-    )
-
-    if predecessor_apply_results.failed_patches:
-        logging.error("Failed to apply patches from PATCHES.json:")
-        for p in predecessor_apply_results.failed_patches:
-            logging.error("Patch title: %s", p.title())
-        raise PatchApplicationError("Failed to apply patch from PATCHES.json")
-
-    patch_entry = patch_utils.PatchEntry.from_dict(
-        patches_json_fp.parent, patch_props
-    )
-    test_apply_result = patch_entry.test_apply(Path(llvm_dir))
-
-    if not test_apply_result:
-        logging.error("Could not apply requested patch")
-        logging.error(test_apply_result.failure_info())
-        raise PatchApplicationError(
-            f'Failed to apply patch: {patch_props["metadata"]["title"]}'
-        )
-
-
-def add_patch(
-    patches_json_path: str,
-    patches_dir: str,
-    relative_patches_dir: str,
-    start_version: git_llvm_rev.Rev,
-    llvm_dir: t.Union[Path, str],
-    rev: t.Union[git_llvm_rev.Rev, str],
-    sha: str,
-    package: str,
-    platforms: t.Iterable[str],
-):
-    """Gets the start and end intervals in 'json_file'.
-
-    Args:
-        patches_json_path: The absolute path to PATCHES.json.
-        patches_dir: The aboslute path to the directory patches are in.
-        relative_patches_dir: The relative path to PATCHES.json.
-        start_version: The base LLVM revision this patch applies to.
-        llvm_dir: The path to LLVM checkout.
-        rev: An LLVM revision (git_llvm_rev.Rev) for a cherrypicking, or a
-        differential revision (str) otherwise.
-        sha: The LLVM git sha that corresponds to the patch. For differential
-        revisions, the git sha from  the local commit created by 'arc patch'
-        is used.
-        package: The LLVM project name this patch applies to.
-        platforms: List of platforms this patch applies to.
-
-    Raises:
-        CherrypickError: A ValueError that highlights the cherry-pick has been
-        seen before.
-        CherrypickRangeError: A ValueError that's raised when the given patch
-        is from before the start_sha.
-    """
-
-    is_cherrypick = isinstance(rev, git_llvm_rev.Rev)
-    if is_cherrypick:
-        file_name = f"{sha}.patch"
-    else:
-        file_name = f"{rev}.patch"
-    rel_patch_path = os.path.join(relative_patches_dir, file_name)
-
-    # Check that we haven't grabbed a patch range that's nonsensical.
-    end_vers = rev.number if isinstance(rev, git_llvm_rev.Rev) else None
-    if end_vers is not None and end_vers <= start_version.number:
-        raise CherrypickVersionError(
-            f"`until` version {end_vers} is earlier or equal to"
-            f" `from` version {start_version.number} for patch"
-            f" {rel_patch_path}"
-        )
-
-    with open(patches_json_path, encoding="utf-8") as f:
-        contents = f.read()
-    indent_len = patch_utils.predict_indent(contents.splitlines())
-    patches_json = json.loads(contents)
-
-    for p in patches_json:
-        rel_path = p["rel_patch_path"]
-        if rel_path == rel_patch_path:
-            raise CherrypickError(
-                f"Patch at {rel_path} already exists in PATCHES.json"
-            )
-        if is_cherrypick:
-            if sha in rel_path:
-                logging.warning(
-                    "Similarly-named patch already exists in PATCHES.json: %r",
-                    rel_path,
-                )
-
-    with open(os.path.join(patches_dir, file_name), "wb") as f:
-        cmd = ["git", "show", sha]
-        # Only apply the part of the patch that belongs to this package, expect
-        # LLVM. This is because some packages are built with LLVM ebuild on X86
-        # but not on the other architectures. e.g. compiler-rt. Therefore
-        # always apply the entire patch to LLVM ebuild as a workaround.
-        if package != "llvm":
-            cmd.append(package_to_project(package))
-        subprocess.check_call(cmd, stdout=f, cwd=llvm_dir)
-
-    commit_subject = subprocess.check_output(
-        ["git", "log", "-n1", "--format=%s", sha],
-        cwd=llvm_dir,
-        encoding="utf-8",
-    )
-    patch_props = {
-        "rel_patch_path": rel_patch_path,
-        "metadata": {
-            "title": commit_subject.strip(),
-            "info": [],
-        },
-        "platforms": sorted(platforms),
-        "version_range": {
-            "from": start_version.number,
-            "until": end_vers,
-        },
-    }
-
-    with patch_utils.git_clean_context(Path(llvm_dir)):
-        validate_patch_application(
-            Path(llvm_dir),
-            start_version.number,
-            Path(patches_json_path),
-            patch_props,
-        )
-
-    patches_json.append(patch_props)
-
-    temp_file = patches_json_path + ".tmp"
-    with open(temp_file, "w", encoding="utf-8") as f:
-        json.dump(
-            patches_json,
-            f,
-            indent=indent_len,
-            separators=(",", ": "),
-            sort_keys=True,
-        )
-        f.write("\n")
-    os.rename(temp_file, patches_json_path)
-
-
-# Resolves a git ref (or similar) to a LLVM SHA.
-def resolve_llvm_ref(llvm_dir: t.Union[Path, str], sha: str) -> str:
-    return subprocess.check_output(
-        ["git", "rev-parse", sha],
-        encoding="utf-8",
-        cwd=llvm_dir,
-    ).strip()
-
-
-# Get the package name of an LLVM project
-def project_to_package(project: str) -> str:
-    if project == "libunwind":
-        return "llvm-libunwind"
-    return project
-
-
-# Get the LLVM project name of a package
-def package_to_project(package: str) -> str:
-    if package == "llvm-libunwind":
-        return "libunwind"
-    return package
-
-
-# Get the LLVM projects change in the specifed sha
-def get_package_names(sha: str, llvm_dir: t.Union[Path, str]) -> list:
-    paths = subprocess.check_output(
-        ["git", "show", "--name-only", "--format=", sha],
-        cwd=llvm_dir,
-        encoding="utf-8",
-    ).splitlines()
-    # Some LLVM projects are built by LLVM ebuild on X86, so always apply the
-    # patch to LLVM ebuild
-    packages = {"llvm"}
-    # Detect if there are more packages to apply the patch to
-    for path in paths:
-        package = project_to_package(path.split("/")[0])
-        if package in ("compiler-rt", "libcxx", "libcxxabi", "llvm-libunwind"):
-            packages.add(package)
-    return list(sorted(packages))
-
-
-def create_patch_for_packages(
-    packages: t.List[str],
-    symlinks: t.List[str],
-    start_rev: git_llvm_rev.Rev,
-    rev: t.Union[git_llvm_rev.Rev, str],
-    sha: str,
-    llvm_dir: t.Union[Path, str],
-    platforms: t.Iterable[str],
-):
-    """Create a patch and add its metadata for each package"""
-    for package, symlink in zip(packages, symlinks):
-        symlink_dir = os.path.dirname(symlink)
-        patches_json_path = os.path.join(symlink_dir, "files/PATCHES.json")
-        relative_patches_dir = "cherry" if package == "llvm" else ""
-        patches_dir = os.path.join(symlink_dir, "files", relative_patches_dir)
-        logging.info("Getting %s (%s) into %s", rev, sha, package)
-        add_patch(
-            patches_json_path,
-            patches_dir,
-            relative_patches_dir,
-            start_rev,
-            llvm_dir,
-            rev,
-            sha,
-            package,
-            platforms=platforms,
-        )
-
-
-def make_cl(
-    llvm_symlink_dir: str,
-    branch: str,
-    commit_messages: t.List[str],
-    reviewers: t.Optional[t.List[str]],
-    cc: t.Optional[t.List[str]],
-):
-    subprocess.check_output(["git", "add", "--all"], cwd=llvm_symlink_dir)
-    git.CommitChanges(llvm_symlink_dir, commit_messages)
-    git.UploadChanges(llvm_symlink_dir, branch, reviewers, cc)
-    git.DeleteBranch(llvm_symlink_dir, branch)
-
-
-def resolve_symbolic_sha(start_sha: str, chromeos_path: Path) -> str:
-    if start_sha == "llvm":
-        return get_llvm_hash.LLVMHash().GetCrOSCurrentLLVMHash(chromeos_path)
-
-    if start_sha == "llvm-next":
-        return get_llvm_hash.LLVMHash().GetCrOSLLVMNextHash()
-
-    return start_sha
-
-
-def find_patches_and_make_cl(
-    chromeos_path: str,
-    patches: t.List[str],
-    start_rev: git_llvm_rev.Rev,
-    llvm_config: git_llvm_rev.LLVMConfig,
-    llvm_symlink_dir: str,
-    allow_failures: bool,
-    create_cl: bool,
-    skip_dependencies: bool,
-    reviewers: t.Optional[t.List[str]],
-    cc: t.Optional[t.List[str]],
-    platforms: t.Iterable[str],
-):
-    converted_patches = [
-        _convert_patch(llvm_config, skip_dependencies, p) for p in patches
-    ]
-    potential_duplicates = _get_duplicate_shas(converted_patches)
-    if potential_duplicates:
-        err_msg = "\n".join(
-            f"{a.patch} == {b.patch}" for a, b in potential_duplicates
-        )
-        raise RuntimeError(f"Found Duplicate SHAs:\n{err_msg}")
-
-    # CL Related variables, only used if `create_cl`
-    commit_messages = [
-        "llvm: get patches from upstream\n",
-    ]
-    branch = (
-        f'get-upstream-{datetime.datetime.now().strftime("%Y%m%d%H%M%S%f")}'
-    )
-
-    if create_cl:
-        git.CreateBranch(llvm_symlink_dir, branch)
-
-    successes = []
-    failures = []
-    for parsed_patch in converted_patches:
-        # Find out the llvm projects changed in this commit
-        packages = get_package_names(parsed_patch.sha, llvm_config.dir)
-        # Find out the ebuild of the corresponding ChromeOS packages
-        ebuild_paths = chroot.GetChrootEbuildPaths(
-            chromeos_path,
-            [
-                "sys-devel/llvm" if package == "llvm" else "sys-libs/" + package
-                for package in packages
-            ],
-        )
-        ebuild_paths = chroot.ConvertChrootPathsToAbsolutePaths(
-            chromeos_path, ebuild_paths
-        )
-        # Create a local patch for all the affected llvm projects
-        try:
-            create_patch_for_packages(
-                packages,
-                ebuild_paths,
-                start_rev,
-                parsed_patch.rev,
-                parsed_patch.sha,
-                llvm_config.dir,
-                platforms=platforms,
-            )
-        except PatchApplicationError as e:
-            if allow_failures:
-                logging.warning(e)
-                failures.append(parsed_patch.sha)
-                continue
-            else:
-                raise e
-        successes.append(parsed_patch.sha)
-
-        if create_cl:
-            commit_messages.extend(
-                [
-                    parsed_patch.git_msg(),
-                    subprocess.check_output(
-                        ["git", "log", "-n1", "--oneline", parsed_patch.sha],
-                        cwd=llvm_config.dir,
-                        encoding="utf-8",
-                    ),
-                ]
-            )
-
-        if parsed_patch.is_differential:
-            subprocess.check_output(
-                ["git", "reset", "--hard", "HEAD^"], cwd=llvm_config.dir
-            )
-
-    if allow_failures:
-        success_list = (":\n\t" + "\n\t".join(successes)) if successes else "."
-        logging.info(
-            "Successfully applied %d patches%s", len(successes), success_list
-        )
-        failure_list = (":\n\t" + "\n\t".join(failures)) if failures else "."
-        logging.info(
-            "Failed to apply %d patches%s", len(failures), failure_list
-        )
-
-    if successes and create_cl:
-        make_cl(
-            llvm_symlink_dir,
-            branch,
-            commit_messages,
-            reviewers,
-            cc,
-        )
-
-
-@dataclasses.dataclass(frozen=True)
-class ParsedPatch:
-    """Class to keep track of bundled patch info."""
-
-    patch: str
-    sha: str
-    is_differential: bool
-    rev: t.Union[git_llvm_rev.Rev, str]
-
-    def git_msg(self) -> str:
-        if self.is_differential:
-            return f"\n\nreviews.llvm.org/{self.patch}\n"
-        return f"\n\nreviews.llvm.org/rG{self.sha}\n"
-
-
-def _convert_patch(
-    llvm_config: git_llvm_rev.LLVMConfig, skip_dependencies: bool, patch: str
-) -> ParsedPatch:
-    """Extract git revision info from a patch.
-
-    Args:
-        llvm_config: LLVM configuration object.
-        skip_dependencies: Pass --skip-dependecies for to `arc`
-        patch: A single patch referent string.
-
-    Returns:
-        A [ParsedPatch] object.
-    """
-
-    # git hash should only have lower-case letters
-    is_differential = patch.startswith("D")
-    if is_differential:
-        subprocess.check_output(
-            [
-                "arc",
-                "patch",
-                "--nobranch",
-                "--skip-dependencies" if skip_dependencies else "--revision",
-                patch,
-            ],
-            cwd=llvm_config.dir,
-        )
-        sha = resolve_llvm_ref(llvm_config.dir, "HEAD")
-        rev: t.Union[git_llvm_rev.Rev, str] = patch
-    else:
-        sha = resolve_llvm_ref(llvm_config.dir, patch)
-        rev = git_llvm_rev.translate_sha_to_rev(llvm_config, sha)
-    return ParsedPatch(
-        patch=patch, sha=sha, rev=rev, is_differential=is_differential
-    )
-
-
-def _get_duplicate_shas(
-    patches: t.List[ParsedPatch],
-) -> t.List[t.Tuple[ParsedPatch, ParsedPatch]]:
-    """Return a list of Patches which have duplicate SHA's"""
-    return [
-        (left, right)
-        for i, left in enumerate(patches)
-        for right in patches[i + 1 :]
-        if left.sha == right.sha
-    ]
-
-
-def get_from_upstream(
-    chromeos_path: str,
-    create_cl: bool,
-    start_sha: str,
-    patches: t.List[str],
-    platforms: t.Iterable[str],
-    allow_failures: bool = False,
-    skip_dependencies: bool = False,
-    reviewers: t.Optional[t.List[str]] = None,
-    cc: t.Optional[t.List[str]] = None,
-):
-    llvm_symlink = chroot.ConvertChrootPathsToAbsolutePaths(
-        chromeos_path,
-        chroot.GetChrootEbuildPaths(chromeos_path, ["sys-devel/llvm"]),
-    )[0]
-    llvm_symlink_dir = os.path.dirname(llvm_symlink)
-
-    git_status = subprocess.check_output(
-        ["git", "status", "-s"], cwd=llvm_symlink_dir, encoding="utf-8"
-    )
-
-    if git_status:
-        error_path = os.path.dirname(os.path.dirname(llvm_symlink_dir))
-        raise ValueError(f"Uncommited changes detected in {error_path}")
-
-    start_sha = resolve_symbolic_sha(start_sha, Path(chromeos_path))
-    logging.info("Base llvm hash == %s", start_sha)
-
-    llvm_config = git_llvm_rev.LLVMConfig(
-        remote="origin", dir=get_llvm_hash.GetAndUpdateLLVMProjectInLLVMTools()
-    )
-    start_sha = resolve_llvm_ref(llvm_config.dir, start_sha)
-
-    find_patches_and_make_cl(
-        chromeos_path=chromeos_path,
-        patches=patches,
-        platforms=platforms,
-        start_rev=git_llvm_rev.translate_sha_to_rev(llvm_config, start_sha),
-        llvm_config=llvm_config,
-        llvm_symlink_dir=llvm_symlink_dir,
-        create_cl=create_cl,
-        skip_dependencies=skip_dependencies,
-        reviewers=reviewers,
-        cc=cc,
-        allow_failures=allow_failures,
-    )
-
-    logging.info("Complete.")
-
-
-def main():
-    chroot.VerifyOutsideChroot()
-    logging.basicConfig(
-        format="%(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
-        "%(message)s",
-        level=logging.INFO,
-    )
-
-    parser = argparse.ArgumentParser(
-        description=__doc__,
-        formatter_class=argparse.RawDescriptionHelpFormatter,
-        epilog=__DOC_EPILOGUE,
-    )
-    parser.add_argument(
-        "--chromeos_path",
-        default=os.path.join(os.path.expanduser("~"), "chromiumos"),
-        help="the path to the chroot (default: %(default)s)",
-    )
-    parser.add_argument(
-        "--start_sha",
-        default="llvm-next",
-        help="LLVM SHA that the patch should start applying at. You can "
-        'specify "llvm" or "llvm-next", as well. Defaults to %(default)s.',
-    )
-    parser.add_argument(
-        "--sha",
-        action="append",
-        default=[],
-        help="The LLVM git SHA to cherry-pick.",
-    )
-    parser.add_argument(
-        "--differential",
-        action="append",
-        default=[],
-        help="The LLVM differential revision to apply. Example: D1234."
-        " Cannot be used for changes already merged upstream; use --sha"
-        " instead for those.",
-    )
-    parser.add_argument(
-        "--platform",
-        action="append",
-        required=True,
-        help="Apply this patch to the give platform. Common options include "
-        '"chromiumos" and "android". Can be specified multiple times to '
-        "apply to multiple platforms",
-    )
-    parser.add_argument(
-        "--allow_failures",
-        action="store_true",
-        help="Skip patches that fail to apply and continue.",
-    )
-    parser.add_argument(
-        "--create_cl",
-        action="store_true",
-        help="Automatically create a CL if specified",
-    )
-    parser.add_argument(
-        "--skip_dependencies",
-        action="store_true",
-        help="Skips a LLVM differential revision's dependencies. Only valid "
-        "when --differential appears exactly once.",
-    )
-    args = parser.parse_args()
-    chroot.VerifyChromeOSRoot(args.chromeos_path)
-
-    if not (args.sha or args.differential):
-        parser.error("--sha or --differential required")
-
-    if args.skip_dependencies and len(args.differential) != 1:
-        parser.error(
-            "--skip_dependencies is only valid when there's exactly one "
-            "supplied differential"
-        )
-
-    get_from_upstream(
-        chromeos_path=args.chromeos_path,
-        allow_failures=args.allow_failures,
-        create_cl=args.create_cl,
-        start_sha=args.start_sha,
-        patches=args.sha + args.differential,
-        skip_dependencies=args.skip_dependencies,
-        platforms=args.platform,
-    )
-
-
-if __name__ == "__main__":
-    sys.exit(main())
diff --git a/llvm_tools/git.py b/llvm_tools/git.py
deleted file mode 100755
index ea4ce3d2..00000000
--- a/llvm_tools/git.py
+++ /dev/null
@@ -1,147 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Git helper functions."""
-
-import collections
-import os
-from pathlib import Path
-import re
-import subprocess
-import tempfile
-from typing import Iterable, Optional, Union
-
-
-CommitContents = collections.namedtuple("CommitContents", ["url", "cl_number"])
-
-
-def IsFullGitSHA(s: str) -> bool:
-    """Returns if `s` looks like a git SHA."""
-    return len(s) == 40 and all(x.isdigit() or "a" <= x <= "f" for x in s)
-
-
-def CreateBranch(repo: Union[Path, str], branch: str) -> None:
-    """Creates a branch in the given repo.
-
-    Args:
-        repo: The absolute path to the repo.
-        branch: The name of the branch to create.
-
-    Raises:
-        ValueError: Failed to create a repo in that directory.
-    """
-
-    if not os.path.isdir(repo):
-        raise ValueError("Invalid directory path provided: %s" % repo)
-
-    subprocess.check_output(["git", "-C", repo, "reset", "HEAD", "--hard"])
-
-    subprocess.check_output(["repo", "start", branch], cwd=repo)
-
-
-def DeleteBranch(repo: Union[Path, str], branch: str) -> None:
-    """Deletes a branch in the given repo.
-
-    Args:
-        repo: The absolute path of the repo.
-        branch: The name of the branch to delete.
-
-    Raises:
-        ValueError: Failed to delete the repo in that directory.
-    """
-
-    if not os.path.isdir(repo):
-        raise ValueError("Invalid directory path provided: %s" % repo)
-
-    def run_checked(cmd):
-        subprocess.run(["git", "-C", repo] + cmd, check=True)
-
-    run_checked(["checkout", "-q", "m/main"])
-    run_checked(["reset", "-q", "HEAD", "--hard"])
-    run_checked(["branch", "-q", "-D", branch])
-
-
-def CommitChanges(
-    repo: Union[Path, str], commit_messages: Iterable[str]
-) -> None:
-    """Commit changes without uploading them.
-
-    Args:
-        repo: The absolute path to the repo where changes were made.
-        commit_messages: Messages to concatenate to form the commit message.
-    """
-    if not os.path.isdir(repo):
-        raise ValueError("Invalid path provided: %s" % repo)
-
-    # Create a git commit.
-    with tempfile.NamedTemporaryFile(mode="w+t", encoding="utf-8") as f:
-        f.write("\n".join(commit_messages))
-        f.flush()
-
-        subprocess.check_output(["git", "commit", "-F", f.name], cwd=repo)
-
-
-def UploadChanges(
-    repo: Union[Path, str],
-    branch: str,
-    reviewers: Optional[Iterable[str]] = None,
-    cc: Optional[Iterable[str]] = None,
-    wip: bool = False,
-) -> CommitContents:
-    """Uploads the changes in the specifed branch of the given repo for review.
-
-    Args:
-        repo: The absolute path to the repo where changes were made.
-        branch: The name of the branch to upload.
-        of the changes made.
-        reviewers: A list of reviewers to add to the CL.
-        cc: A list of contributors to CC about the CL.
-        wip: Whether to upload the change as a work-in-progress.
-
-    Returns:
-        A CommitContents value containing the commit URL and change list number.
-
-    Raises:
-        ValueError: Failed to create a commit or failed to upload the
-        changes for review.
-    """
-
-    if not os.path.isdir(repo):
-        raise ValueError("Invalid path provided: %s" % repo)
-
-    # Upload the changes for review.
-    git_args = [
-        "repo",
-        "upload",
-        "--yes",
-        f'--reviewers={",".join(reviewers)}' if reviewers else "--ne",
-        "--no-verify",
-        f"--br={branch}",
-    ]
-
-    if cc:
-        git_args.append(f'--cc={",".join(cc)}')
-    if wip:
-        git_args.append("--wip")
-
-    out = subprocess.check_output(
-        git_args,
-        stderr=subprocess.STDOUT,
-        cwd=repo,
-        encoding="utf-8",
-    )
-
-    print(out)
-    # Matches both internal and external CLs.
-    found_url = re.search(
-        r"https?://[\w-]*-review.googlesource.com/c/.*/([0-9]+)",
-        out.rstrip(),
-    )
-    if not found_url:
-        raise ValueError("Failed to find change list URL.")
-
-    return CommitContents(
-        url=found_url.group(0), cl_number=int(found_url.group(1))
-    )
diff --git a/llvm_tools/git_llvm_rev.py b/llvm_tools/git_llvm_rev.py
old mode 100755
new mode 100644
index 51fb6fec..a5972763
--- a/llvm_tools/git_llvm_rev.py
+++ b/llvm_tools/git_llvm_rev.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -13,7 +12,6 @@ import argparse
 from pathlib import Path
 import re
 import subprocess
-import sys
 from typing import IO, Iterable, List, NamedTuple, Optional, Tuple, Union
 
 
@@ -44,6 +42,8 @@ known_llvm_rev_sha_pairs = (
     (450000, "906ebd5830e6053b50c52bf098e3586b567e8499"),
     (475000, "530d14a99611a71f8f3eb811920fd7b5c4d4e1f8"),
     (500000, "173855f9b0bdfe45d71272596b510650bfc1ca33"),
+    (525000, "ac3ee1b1aec424c60660fd245f5b53aaffa2f5b1"),
+    (550000, "e5bc842a9c56c1d83543f0232a888db6210efd85"),
 )
 
 # Represents an LLVM git checkout:
@@ -345,7 +345,7 @@ def translate_rev_to_sha_from_baseline(
             )
         if child_rev < want_rev:
             raise ValueError(
-                "Revision {want_rev} is past "
+                f"Revision {want_rev} is past "
                 f"{llvm_config.remote}/{branch_name}. Try updating your tree?"
             )
         baseline_git_sha = child_sha
@@ -482,7 +482,3 @@ def main(argv: List[str]) -> None:
     else:
         sha = translate_rev_to_sha(config, Rev.parse(opts.rev))
         print(sha)
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/llvm_tools/git_llvm_rev_test.py b/llvm_tools/git_llvm_rev_test.py
old mode 100755
new mode 100644
index dbea0fca..ac77fe68
--- a/llvm_tools/git_llvm_rev_test.py
+++ b/llvm_tools/git_llvm_rev_test.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -7,14 +6,13 @@
 
 import unittest
 
-import git_llvm_rev
-import llvm_project
+from llvm_tools import get_llvm_hash
+from llvm_tools import git_llvm_rev
 
 
 def get_llvm_config() -> git_llvm_rev.LLVMConfig:
-    return git_llvm_rev.LLVMConfig(
-        dir=llvm_project.get_location(), remote="origin"
-    )
+    repo = get_llvm_hash.GetCachedUpToDateReadOnlyLLVMRepo()
+    return git_llvm_rev.LLVMConfig(dir=repo.path, remote=repo.remote)
 
 
 class Test(unittest.TestCase):
@@ -170,7 +168,3 @@ class Test(unittest.TestCase):
 # FIXME: When release/10.x happens, it may be nice to have a test-case
 # generally covering that, since it's the first branch that we have to travel
 # back to the base commit for.
-
-if __name__ == "__main__":
-    llvm_project.ensure_up_to_date()
-    unittest.main()
diff --git a/llvm_tools/git_unittest.py b/llvm_tools/git_unittest.py
deleted file mode 100755
index fa756ddf..00000000
--- a/llvm_tools/git_unittest.py
+++ /dev/null
@@ -1,178 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unit tests for git helper functions."""
-
-import os
-import subprocess
-import tempfile
-import unittest
-from unittest import mock
-
-import git
-
-
-# These are unittests; protected access is OK to a point.
-# pylint: disable=protected-access
-
-EXAMPLE_GIT_SHA = "d46d9c1a23118e3943f43fe2dfc9f9c9c0b4aefe"
-
-
-class HelperFunctionsTest(unittest.TestCase):
-    """Test class for updating LLVM hashes of packages."""
-
-    def testIsFullGitSHASuccessCases(self):
-        shas = ("a" * 40, EXAMPLE_GIT_SHA)
-        for s in shas:
-            self.assertTrue(git.IsFullGitSHA(s), s)
-
-    def testIsFullGitSHAFailureCases(self):
-        shas = (
-            "",
-            "A" * 40,
-            "g" * 40,
-            EXAMPLE_GIT_SHA[1:],
-            EXAMPLE_GIT_SHA + "a",
-        )
-        for s in shas:
-            self.assertFalse(git.IsFullGitSHA(s), s)
-
-    @mock.patch.object(os.path, "isdir", return_value=False)
-    def testFailedToCreateBranchForInvalidDirectoryPath(self, mock_isdir):
-        path_to_repo = "/invalid/path/to/repo"
-        branch = "branch-name"
-
-        # Verify the exception is raised when provided an invalid directory
-        # path.
-        with self.assertRaises(ValueError) as err:
-            git.CreateBranch(path_to_repo, branch)
-
-        self.assertEqual(
-            str(err.exception),
-            "Invalid directory path provided: %s" % path_to_repo,
-        )
-
-        mock_isdir.assert_called_once()
-
-    @mock.patch.object(os.path, "isdir", return_value=True)
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    def testSuccessfullyCreatedBranch(self, mock_command_output, mock_isdir):
-        path_to_repo = "/path/to/repo"
-        branch = "branch-name"
-
-        git.CreateBranch(path_to_repo, branch)
-
-        mock_isdir.assert_called_once_with(path_to_repo)
-
-        self.assertEqual(mock_command_output.call_count, 2)
-
-    @mock.patch.object(os.path, "isdir", return_value=False)
-    def testFailedToDeleteBranchForInvalidDirectoryPath(self, mock_isdir):
-        path_to_repo = "/invalid/path/to/repo"
-        branch = "branch-name"
-
-        # Verify the exception is raised on an invalid repo path.
-        with self.assertRaises(ValueError) as err:
-            git.DeleteBranch(path_to_repo, branch)
-
-        self.assertEqual(
-            str(err.exception),
-            "Invalid directory path provided: %s" % path_to_repo,
-        )
-
-        mock_isdir.assert_called_once()
-
-    @mock.patch.object(os.path, "isdir", return_value=True)
-    @mock.patch.object(subprocess, "run", return_value=None)
-    def testSuccessfullyDeletedBranch(self, mock_command_output, mock_isdir):
-        path_to_repo = "/valid/path/to/repo"
-        branch = "branch-name"
-
-        git.DeleteBranch(path_to_repo, branch)
-
-        mock_isdir.assert_called_once_with(path_to_repo)
-
-        self.assertEqual(mock_command_output.call_count, 3)
-
-    @mock.patch.object(os.path, "isdir", return_value=False)
-    def testFailedToUploadChangesForInvalidDirectoryPath(self, mock_isdir):
-        path_to_repo = "/some/path/to/repo"
-        branch = "update-LLVM_NEXT_HASH-a123testhash3"
-
-        # Verify exception is raised when on an invalid repo path.
-        with self.assertRaises(ValueError) as err:
-            git.UploadChanges(path_to_repo, branch)
-
-        self.assertEqual(
-            str(err.exception), "Invalid path provided: %s" % path_to_repo
-        )
-
-        mock_isdir.assert_called_once()
-
-    @mock.patch.object(os.path, "isdir", return_value=True)
-    @mock.patch.object(subprocess, "check_output")
-    @mock.patch.object(tempfile, "NamedTemporaryFile")
-    def testSuccessfullyUploadedChangesForReview(
-        self, mock_tempfile, mock_commands, mock_isdir
-    ):
-        path_to_repo = "/some/path/to/repo"
-        branch = "branch-name"
-        commit_messages = ["Test message"]
-        mock_tempfile.return_value.__enter__.return_value.name = "tmp"
-
-        # A test CL generated by `repo upload`.
-        mock_commands.side_effect = [
-            None,
-            (
-                "remote: https://chromium-review.googlesource."
-                "com/c/chromiumos/overlays/chromiumos-overlay/"
-                "+/193147 Fix stdout"
-            ),
-        ]
-        git.CommitChanges(path_to_repo, commit_messages)
-        change_list = git.UploadChanges(path_to_repo, branch)
-
-        self.assertEqual(change_list.cl_number, 193147)
-
-        mock_isdir.assert_called_with(path_to_repo)
-
-        expected_command = [
-            "git",
-            "commit",
-            "-F",
-            mock_tempfile.return_value.__enter__.return_value.name,
-        ]
-        self.assertEqual(
-            mock_commands.call_args_list[0],
-            mock.call(expected_command, cwd=path_to_repo),
-        )
-
-        expected_cmd = [
-            "repo",
-            "upload",
-            "--yes",
-            "--ne",
-            "--no-verify",
-            "--br=%s" % branch,
-        ]
-        self.assertEqual(
-            mock_commands.call_args_list[1],
-            mock.call(
-                expected_cmd,
-                stderr=subprocess.STDOUT,
-                cwd=path_to_repo,
-                encoding="utf-8",
-            ),
-        )
-
-        self.assertEqual(
-            change_list.url,
-            "https://chromium-review.googlesource.com/c/chromiumos/overlays/"
-            "chromiumos-overlay/+/193147",
-        )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/lint_llvm_patches.py b/llvm_tools/lint_llvm_patches.py
new file mode 100644
index 00000000..71e4656c
--- /dev/null
+++ b/llvm_tools/lint_llvm_patches.py
@@ -0,0 +1,107 @@
+# Copyright 2025 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""A script used to lint patches in llvm_patches/.
+
+Meant to be called from presubmit hooks in toolchain-utils.
+"""
+
+import argparse
+import json
+import logging
+import os
+from pathlib import Path
+import sys
+from typing import List
+
+from cros_utils import cros_paths
+from llvm_tools import patch_utils
+
+
+def load_patches_json(llvm_patches: Path) -> List[patch_utils.PatchEntry]:
+    patches_json = (
+        llvm_patches / cros_paths.DEFAULT_PATCHES_PATH_IN_TOOLCHAIN_UTILS.name
+    )
+    try:
+        with patches_json.open(encoding="utf-8") as f:
+            return patch_utils.json_to_patch_entries(
+                workdir=llvm_patches, json_fd=f
+            )
+    except FileNotFoundError:
+        sys.exit(f"error: PATCHES.json not found at {patches_json}")
+    except json.JSONDecodeError:
+        sys.exit(f"error: PATCHES.json at {patches_json} is ill-formed")
+
+
+def extract_all_patch_paths_from_patches_json(
+    patches_json: List[patch_utils.PatchEntry],
+) -> List[Path]:
+    return [x.workdir / x.rel_patch_path for x in patches_json]
+
+
+def find_all_patch_files_in(base_dir: Path) -> List[Path]:
+    results = []
+    for root, _, files in os.walk(base_dir):
+        proot = Path(root)
+        for file in files:
+            if file.endswith(".patch"):
+                results.append(proot / file)
+    return results
+
+
+def main(argv: List[str]) -> None:
+    toolchain_utils = cros_paths.script_toolchain_utils_root()
+
+    logging.basicConfig(
+        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
+        "%(message)s",
+        level=logging.INFO,
+    )
+
+    parser = argparse.ArgumentParser(
+        description=__doc__,
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+    )
+    # No args to parse for the moment, but responding to `--help` is nice.
+    _ = parser.parse_args(argv)
+
+    llvm_patches = (
+        toolchain_utils
+        / cros_paths.DEFAULT_PATCHES_PATH_IN_TOOLCHAIN_UTILS.parent
+    )
+    referenced_paths = set(
+        extract_all_patch_paths_from_patches_json(
+            load_patches_json(llvm_patches)
+        )
+    )
+    available_paths = set(find_all_patch_files_in(llvm_patches))
+
+    fail = False
+    refed_but_not_available = referenced_paths - available_paths
+    if refed_but_not_available:
+        fail = True
+        print(
+            "Patches are referenced from PATCHES.json, but not present:",
+            file=sys.stderr,
+        )
+        for p in sorted(refed_but_not_available):
+            print(f"  - {p}", file=sys.stderr)
+
+    available_but_not_refed = available_paths - referenced_paths
+    if available_but_not_refed:
+        fail = True
+        print(
+            "Patches are present, but not referenced from PATCHES.json:",
+            file=sys.stderr,
+        )
+        for p in sorted(available_but_not_refed):
+            print(f"  - {p}", file=sys.stderr)
+
+    if fail:
+        sys.exit(1)
+
+    print(
+        "All looks good! PATCHES.json parses, and present `.patch` files all "
+        "correspond to it."
+    )
diff --git a/llvm_tools/llvm_bisection.py b/llvm_tools/llvm_bisection.py
deleted file mode 100755
index 336d4334..00000000
--- a/llvm_tools/llvm_bisection.py
+++ /dev/null
@@ -1,450 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Performs bisection on LLVM based off a .JSON file."""
-
-import argparse
-import enum
-import errno
-import json
-import os
-import subprocess
-import sys
-
-import chroot
-import get_llvm_hash
-import git_llvm_rev
-import modify_a_tryjob
-import update_chromeos_llvm_hash
-import update_tryjob_status
-
-
-class BisectionExitStatus(enum.Enum):
-    """Exit code when performing bisection."""
-
-    # Means that there are no more revisions available to bisect.
-    BISECTION_COMPLETE = 126
-
-
-def GetCommandLineArgs():
-    """Parses the command line for the command line arguments."""
-
-    # Default path to the chroot if a path is not specified.
-    cros_root = os.path.expanduser("~")
-    cros_root = os.path.join(cros_root, "chromiumos")
-
-    # Create parser and add optional command-line arguments.
-    parser = argparse.ArgumentParser(
-        description="Bisects LLVM via tracking a JSON file."
-    )
-
-    # Add argument for other change lists that want to run alongside the tryjob
-    # which has a change list of updating a package's git hash.
-    parser.add_argument(
-        "--parallel",
-        type=int,
-        default=3,
-        help="How many tryjobs to create between the last good version and "
-        "the first bad version (default: %(default)s)",
-    )
-
-    # Add argument for the good LLVM revision for bisection.
-    parser.add_argument(
-        "--start_rev",
-        required=True,
-        type=int,
-        help="The good revision for the bisection.",
-    )
-
-    # Add argument for the bad LLVM revision for bisection.
-    parser.add_argument(
-        "--end_rev",
-        required=True,
-        type=int,
-        help="The bad revision for the bisection.",
-    )
-
-    # Add argument for the absolute path to the file that contains information
-    # on the previous tested svn version.
-    parser.add_argument(
-        "--last_tested",
-        required=True,
-        help="the absolute path to the file that contains the tryjobs",
-    )
-
-    # Add argument for the absolute path to the LLVM source tree.
-    parser.add_argument(
-        "--src_path",
-        help="the path to the LLVM source tree to use (used for retrieving the "
-        "git hash of each version between the last good version and first bad "
-        "version)",
-    )
-
-    # Add argument for other change lists that want to run alongside the tryjob
-    # which has a change list of updating a package's git hash.
-    parser.add_argument(
-        "--extra_change_lists",
-        type=int,
-        nargs="+",
-        help="change lists that would like to be run alongside the change list "
-        "of updating the packages",
-    )
-
-    # Add argument for custom options for the tryjob.
-    parser.add_argument(
-        "--options",
-        required=False,
-        nargs="+",
-        help="options to use for the tryjob testing",
-    )
-
-    # Add argument for the builder to use for the tryjob.
-    parser.add_argument(
-        "--builder", required=True, help="builder to use for the tryjob testing"
-    )
-
-    # Add argument for the description of the tryjob.
-    parser.add_argument(
-        "--description",
-        required=False,
-        nargs="+",
-        help="the description of the tryjob",
-    )
-
-    # Add argument for a specific chroot path.
-    parser.add_argument(
-        "--chromeos_path",
-        default=cros_root,
-        help="the path to the chroot (default: %(default)s)",
-    )
-
-    # Add argument for whether to display command contents to `stdout`.
-    parser.add_argument(
-        "--nocleanup",
-        action="store_false",
-        dest="cleanup",
-        help="Abandon CLs created for bisectoin",
-    )
-
-    args_output = parser.parse_args()
-
-    assert (
-        args_output.start_rev < args_output.end_rev
-    ), "Start revision %d is >= end revision %d" % (
-        args_output.start_rev,
-        args_output.end_rev,
-    )
-
-    if args_output.last_tested and not args_output.last_tested.endswith(
-        ".json"
-    ):
-        raise ValueError(
-            'Filed provided %s does not end in ".json"'
-            % args_output.last_tested
-        )
-
-    return args_output
-
-
-def GetRemainingRange(start, end, tryjobs):
-    """Gets the start and end intervals in 'json_file'.
-
-    Args:
-        start: The start version of the bisection provided via the command line.
-        end: The end version of the bisection provided via the command line.
-        tryjobs: A list of tryjobs where each element is in the following
-        format:
-        [
-            {[TRYJOB_INFORMATION]},
-            {[TRYJOB_INFORMATION]},
-            ...,
-            {[TRYJOB_INFORMATION]}
-        ]
-
-    Returns:
-        The new start version and end version for bisection, a set of revisions
-        that are 'pending' and a set of revisions that are to be skipped.
-
-    Raises:
-        ValueError: The value for 'status' is missing or there is a mismatch
-        between 'start' and 'end' compared to the 'start' and 'end' in the JSON
-        file.
-        AssertionError: The new start version is >= than the new end version.
-    """
-
-    if not tryjobs:
-        return start, end, {}, {}
-
-    # Verify that each tryjob has a value for the 'status' key.
-    for cur_tryjob_dict in tryjobs:
-        if not cur_tryjob_dict.get("status", None):
-            raise ValueError(
-                '"status" is missing or has no value, please '
-                "go to %s and update it" % cur_tryjob_dict["link"]
-            )
-
-    all_bad_revisions = [end]
-    all_bad_revisions.extend(
-        cur_tryjob["rev"]
-        for cur_tryjob in tryjobs
-        if cur_tryjob["status"] == update_tryjob_status.TryjobStatus.BAD.value
-    )
-
-    # The minimum value for the 'bad' field in the tryjobs is the new end
-    # version.
-    bad_rev = min(all_bad_revisions)
-
-    all_good_revisions = [start]
-    all_good_revisions.extend(
-        cur_tryjob["rev"]
-        for cur_tryjob in tryjobs
-        if cur_tryjob["status"] == update_tryjob_status.TryjobStatus.GOOD.value
-    )
-
-    # The maximum value for the 'good' field in the tryjobs is the new start
-    # version.
-    good_rev = max(all_good_revisions)
-
-    # The good version should always be strictly less than the bad version;
-    # otherwise, bisection is broken.
-    assert (
-        good_rev < bad_rev
-    ), "Bisection is broken because %d (good) is >= " "%d (bad)" % (
-        good_rev,
-        bad_rev,
-    )
-
-    # Find all revisions that are 'pending' within 'good_rev' and 'bad_rev'.
-    #
-    # NOTE: The intent is to not launch tryjobs between 'good_rev' and 'bad_rev'
-    # that have already been launched (this set is used when constructing the
-    # list of revisions to launch tryjobs for).
-    pending_revisions = {
-        tryjob["rev"]
-        for tryjob in tryjobs
-        if tryjob["status"] == update_tryjob_status.TryjobStatus.PENDING.value
-        and good_rev < tryjob["rev"] < bad_rev
-    }
-
-    # Find all revisions that are to be skipped within 'good_rev' and 'bad_rev'.
-    #
-    # NOTE: The intent is to not launch tryjobs between 'good_rev' and 'bad_rev'
-    # that have already been marked as 'skip' (this set is used when
-    # constructing the list of revisions to launch tryjobs for).
-    skip_revisions = {
-        tryjob["rev"]
-        for tryjob in tryjobs
-        if tryjob["status"] == update_tryjob_status.TryjobStatus.SKIP.value
-        and good_rev < tryjob["rev"] < bad_rev
-    }
-
-    return good_rev, bad_rev, pending_revisions, skip_revisions
-
-
-def GetCommitsBetween(
-    start, end, parallel, src_path, pending_revisions, skip_revisions
-):
-    """Determines the revisions between start and end."""
-
-    with get_llvm_hash.LLVMHash().CreateTempDirectory() as temp_dir:
-        # We have guaranteed contiguous revision numbers after this,
-        # and that guarnatee simplifies things considerably, so we don't
-        # support anything before it.
-        assert (
-            start >= git_llvm_rev.base_llvm_revision
-        ), f"{start} was too long ago"
-
-        with get_llvm_hash.CreateTempLLVMRepo(temp_dir) as new_repo:
-            if not src_path:
-                src_path = new_repo
-            index_step = (end - (start + 1)) // (parallel + 1)
-            if not index_step:
-                index_step = 1
-            revisions = [
-                rev
-                for rev in range(start + 1, end, index_step)
-                if rev not in pending_revisions and rev not in skip_revisions
-            ]
-            git_hashes = [
-                get_llvm_hash.GetGitHashFrom(src_path, rev) for rev in revisions
-            ]
-            return revisions, git_hashes
-
-
-def Bisect(
-    revisions,
-    git_hashes,
-    bisect_state,
-    last_tested,
-    update_packages,
-    chromeos_path,
-    extra_change_lists,
-    options,
-    builder,
-):
-    """Adds tryjobs and updates the status file with the new tryjobs."""
-
-    try:
-        for svn_revision, git_hash in zip(revisions, git_hashes):
-            tryjob_dict = modify_a_tryjob.AddTryjob(
-                update_packages,
-                git_hash,
-                svn_revision,
-                chromeos_path,
-                extra_change_lists,
-                options,
-                builder,
-                svn_revision,
-            )
-
-            bisect_state["jobs"].append(tryjob_dict)
-    finally:
-        # Do not want to lose progress if there is an exception.
-        if last_tested:
-            new_file = "%s.new" % last_tested
-            with open(new_file, "w", encoding="utf-8") as json_file:
-                json.dump(
-                    bisect_state, json_file, indent=4, separators=(",", ": ")
-                )
-
-            os.rename(new_file, last_tested)
-
-
-def LoadStatusFile(last_tested, start, end):
-    """Loads the status file for bisection."""
-
-    try:
-        with open(last_tested, encoding="utf-8") as f:
-            return json.load(f)
-    except IOError as err:
-        if err.errno != errno.ENOENT:
-            raise
-
-    return {"start": start, "end": end, "jobs": []}
-
-
-def main(args_output):
-    """Bisects LLVM commits.
-
-    Raises:
-        AssertionError: The script was run inside the chroot.
-    """
-
-    chroot.VerifyOutsideChroot()
-    chroot.VerifyChromeOSRoot(args_output.chromeos_path)
-    start = args_output.start_rev
-    end = args_output.end_rev
-
-    bisect_state = LoadStatusFile(args_output.last_tested, start, end)
-    if start != bisect_state["start"] or end != bisect_state["end"]:
-        raise ValueError(
-            f"The start {start} or the end {end} version provided is "
-            f'different than "start" {bisect_state["start"]} or "end" '
-            f'{bisect_state["end"]} in the .JSON file'
-        )
-
-    # Pending and skipped revisions are between 'start_rev' and 'end_rev'.
-    start_rev, end_rev, pending_revs, skip_revs = GetRemainingRange(
-        start, end, bisect_state["jobs"]
-    )
-
-    revisions, git_hashes = GetCommitsBetween(
-        start_rev,
-        end_rev,
-        args_output.parallel,
-        args_output.src_path,
-        pending_revs,
-        skip_revs,
-    )
-
-    # No more revisions between 'start_rev' and 'end_rev', so
-    # bisection is complete.
-    #
-    # This is determined by finding all valid revisions between 'start_rev'
-    # and 'end_rev' and that are NOT in the 'pending' and 'skipped' set.
-    if not revisions:
-        if pending_revs:
-            # Some tryjobs are not finished which may change the actual bad
-            # commit/revision when those tryjobs are finished.
-            no_revisions_message = (
-                f"No revisions between start {start_rev} "
-                f"and end {end_rev} to create tryjobs\n"
-            )
-
-            if pending_revs:
-                no_revisions_message += (
-                    "The following tryjobs are pending:\n"
-                    + "\n".join(str(rev) for rev in pending_revs)
-                    + "\n"
-                )
-
-            if skip_revs:
-                no_revisions_message += (
-                    "The following tryjobs were skipped:\n"
-                    + "\n".join(str(rev) for rev in skip_revs)
-                    + "\n"
-                )
-
-            raise ValueError(no_revisions_message)
-
-        print(f"Finished bisecting for {args_output.last_tested}")
-        if args_output.src_path:
-            bad_llvm_hash = get_llvm_hash.GetGitHashFrom(
-                args_output.src_path, end_rev
-            )
-        else:
-            bad_llvm_hash = get_llvm_hash.LLVMHash().GetLLVMHash(end_rev)
-        print(
-            f"The bad revision is {end_rev} and its commit hash is "
-            f"{bad_llvm_hash}"
-        )
-        if skip_revs:
-            skip_revs_message = (
-                "\nThe following revisions were skipped:\n"
-                + "\n".join(str(rev) for rev in skip_revs)
-            )
-            print(skip_revs_message)
-
-        if args_output.cleanup:
-            # Abandon all the CLs created for bisection
-            gerrit = os.path.join(
-                args_output.chromeos_path, "chromite/bin/gerrit"
-            )
-            for build in bisect_state["jobs"]:
-                try:
-                    subprocess.check_output(
-                        [gerrit, "abandon", str(build["cl"])],
-                        stderr=subprocess.STDOUT,
-                        encoding="utf-8",
-                    )
-                except subprocess.CalledProcessError as err:
-                    # the CL may have been abandoned
-                    if "chromite.lib.gob_util.GOBError" not in err.output:
-                        raise
-
-        return BisectionExitStatus.BISECTION_COMPLETE.value
-
-    for rev in revisions:
-        if (
-            update_tryjob_status.FindTryjobIndex(rev, bisect_state["jobs"])
-            is not None
-        ):
-            raise ValueError(f'Revision {rev} exists already in "jobs"')
-
-    Bisect(
-        revisions,
-        git_hashes,
-        bisect_state,
-        args_output.last_tested,
-        update_chromeos_llvm_hash.DEFAULT_PACKAGES,
-        args_output.chromeos_path,
-        args_output.extra_change_lists,
-        args_output.options,
-        args_output.builder,
-    )
-
-
-if __name__ == "__main__":
-    sys.exit(main(GetCommandLineArgs()))
diff --git a/llvm_tools/llvm_bisection_unittest.py b/llvm_tools/llvm_bisection_unittest.py
deleted file mode 100755
index 273a4c9b..00000000
--- a/llvm_tools/llvm_bisection_unittest.py
+++ /dev/null
@@ -1,590 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-# pylint: disable=protected-access
-
-"""Tests for LLVM bisection."""
-
-import json
-import os
-import subprocess
-import unittest
-from unittest import mock
-
-import chroot
-import get_llvm_hash
-import git_llvm_rev
-import llvm_bisection
-import modify_a_tryjob
-import test_helpers
-
-
-class LLVMBisectionTest(unittest.TestCase):
-    """Unittests for LLVM bisection."""
-
-    def testGetRemainingRangePassed(self):
-        start = 100
-        end = 150
-
-        test_tryjobs = [
-            {
-                "rev": 110,
-                "status": "good",
-                "link": "https://some_tryjob_1_url.com",
-            },
-            {
-                "rev": 120,
-                "status": "good",
-                "link": "https://some_tryjob_2_url.com",
-            },
-            {
-                "rev": 130,
-                "status": "pending",
-                "link": "https://some_tryjob_3_url.com",
-            },
-            {
-                "rev": 135,
-                "status": "skip",
-                "link": "https://some_tryjob_4_url.com",
-            },
-            {
-                "rev": 140,
-                "status": "bad",
-                "link": "https://some_tryjob_5_url.com",
-            },
-        ]
-
-        # Tuple consists of the new good revision, the new bad revision, a set
-        # of 'pending' revisions, and a set of 'skip' revisions.
-        expected_revisions_tuple = 120, 140, {130}, {135}
-
-        self.assertEqual(
-            llvm_bisection.GetRemainingRange(start, end, test_tryjobs),
-            expected_revisions_tuple,
-        )
-
-    def testGetRemainingRangeFailedWithMissingStatus(self):
-        start = 100
-        end = 150
-
-        test_tryjobs = [
-            {
-                "rev": 105,
-                "status": "good",
-                "link": "https://some_tryjob_1_url.com",
-            },
-            {
-                "rev": 120,
-                "status": None,
-                "link": "https://some_tryjob_2_url.com",
-            },
-            {
-                "rev": 140,
-                "status": "bad",
-                "link": "https://some_tryjob_3_url.com",
-            },
-        ]
-
-        with self.assertRaises(ValueError) as err:
-            llvm_bisection.GetRemainingRange(start, end, test_tryjobs)
-
-        error_message = (
-            '"status" is missing or has no value, please '
-            "go to %s and update it" % test_tryjobs[1]["link"]
-        )
-        self.assertEqual(str(err.exception), error_message)
-
-    def testGetRemainingRangeFailedWithInvalidRange(self):
-        start = 100
-        end = 150
-
-        test_tryjobs = [
-            {
-                "rev": 110,
-                "status": "bad",
-                "link": "https://some_tryjob_1_url.com",
-            },
-            {
-                "rev": 125,
-                "status": "skip",
-                "link": "https://some_tryjob_2_url.com",
-            },
-            {
-                "rev": 140,
-                "status": "good",
-                "link": "https://some_tryjob_3_url.com",
-            },
-        ]
-
-        with self.assertRaises(AssertionError) as err:
-            llvm_bisection.GetRemainingRange(start, end, test_tryjobs)
-
-        expected_error_message = (
-            "Bisection is broken because %d (good) is >= "
-            "%d (bad)" % (test_tryjobs[2]["rev"], test_tryjobs[0]["rev"])
-        )
-
-        self.assertEqual(str(err.exception), expected_error_message)
-
-    @mock.patch.object(get_llvm_hash, "GetGitHashFrom")
-    def testGetCommitsBetweenPassed(self, mock_get_git_hash):
-        start = git_llvm_rev.base_llvm_revision
-        end = start + 10
-        test_pending_revisions = {start + 7}
-        test_skip_revisions = {
-            start + 1,
-            start + 2,
-            start + 4,
-            start + 8,
-            start + 9,
-        }
-        parallel = 3
-        abs_path_to_src = "/abs/path/to/src"
-
-        revs = ["a123testhash3", "a123testhash5"]
-        mock_get_git_hash.side_effect = revs
-
-        git_hashes = [
-            git_llvm_rev.base_llvm_revision + 3,
-            git_llvm_rev.base_llvm_revision + 5,
-        ]
-
-        self.assertEqual(
-            llvm_bisection.GetCommitsBetween(
-                start,
-                end,
-                parallel,
-                abs_path_to_src,
-                test_pending_revisions,
-                test_skip_revisions,
-            ),
-            (git_hashes, revs),
-        )
-
-    def testLoadStatusFilePassedWithExistingFile(self):
-        start = 100
-        end = 150
-
-        test_bisect_state = {"start": start, "end": end, "jobs": []}
-
-        # Simulate that the status file exists.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(test_bisect_state, f)
-
-            self.assertEqual(
-                llvm_bisection.LoadStatusFile(temp_json_file, start, end),
-                test_bisect_state,
-            )
-
-    def testLoadStatusFilePassedWithoutExistingFile(self):
-        start = 200
-        end = 250
-
-        expected_bisect_state = {"start": start, "end": end, "jobs": []}
-
-        last_tested = "/abs/path/to/file_that_does_not_exist.json"
-
-        self.assertEqual(
-            llvm_bisection.LoadStatusFile(last_tested, start, end),
-            expected_bisect_state,
-        )
-
-    @mock.patch.object(modify_a_tryjob, "AddTryjob")
-    def testBisectPassed(self, mock_add_tryjob):
-        git_hash_list = ["a123testhash1", "a123testhash2", "a123testhash3"]
-        revisions_list = [102, 104, 106]
-
-        # Simulate behavior of `AddTryjob()` when successfully launched a
-        # tryjob for the updated packages.
-        @test_helpers.CallCountsToMockFunctions
-        def MockAddTryjob(
-            call_count,
-            _packages,
-            _git_hash,
-            _revision,
-            _chroot_path,
-            _extra_cls,
-            _options,
-            _builder,
-            _svn_revision,
-        ):
-            if call_count < 2:
-                return {"rev": revisions_list[call_count], "status": "pending"}
-
-            # Simulate an exception happened along the way when updating the
-            # packages' `LLVM_NEXT_HASH`.
-            if call_count == 2:
-                raise ValueError("Unable to launch tryjob")
-
-            assert False, "Called `AddTryjob()` more than expected."
-
-        # Use the test function to simulate `AddTryjob()`.
-        mock_add_tryjob.side_effect = MockAddTryjob
-
-        start = 100
-        end = 110
-
-        bisection_contents = {"start": start, "end": end, "jobs": []}
-
-        args_output = test_helpers.ArgsOutputTest()
-
-        packages = ["sys-devel/llvm"]
-
-        # Create a temporary .JSON file to simulate a status file for bisection.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisection_contents, f)
-
-            # Verify that the status file is updated when an exception happened
-            # when attempting to launch a revision (i.e. progress is not lost).
-            with self.assertRaises(ValueError) as err:
-                llvm_bisection.Bisect(
-                    revisions_list,
-                    git_hash_list,
-                    bisection_contents,
-                    temp_json_file,
-                    packages,
-                    args_output.chromeos_path,
-                    args_output.extra_change_lists,
-                    args_output.options,
-                    args_output.builders,
-                )
-
-            expected_bisection_contents = {
-                "start": start,
-                "end": end,
-                "jobs": [
-                    {"rev": revisions_list[0], "status": "pending"},
-                    {"rev": revisions_list[1], "status": "pending"},
-                ],
-            }
-
-            # Verify that the launched tryjobs were added to the status file
-            # when an exception happened.
-            with open(temp_json_file, encoding="utf-8") as f:
-                json_contents = json.load(f)
-
-                self.assertEqual(json_contents, expected_bisection_contents)
-
-        self.assertEqual(str(err.exception), "Unable to launch tryjob")
-
-        self.assertEqual(mock_add_tryjob.call_count, 3)
-
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    @mock.patch.object(
-        get_llvm_hash.LLVMHash, "GetLLVMHash", return_value="a123testhash4"
-    )
-    @mock.patch.object(llvm_bisection, "GetCommitsBetween")
-    @mock.patch.object(llvm_bisection, "GetRemainingRange")
-    @mock.patch.object(llvm_bisection, "LoadStatusFile")
-    @mock.patch.object(chroot, "VerifyChromeOSRoot")
-    @mock.patch.object(chroot, "VerifyOutsideChroot", return_value=True)
-    def testMainPassed(
-        self,
-        mock_outside_chroot,
-        mock_chromeos_root,
-        mock_load_status_file,
-        mock_get_range,
-        mock_get_revision_and_hash_list,
-        _mock_get_bad_llvm_hash,
-        mock_abandon_cl,
-    ):
-        start = 500
-        end = 502
-        cl = 1
-
-        bisect_state = {
-            "start": start,
-            "end": end,
-            "jobs": [{"rev": 501, "status": "bad", "cl": cl}],
-        }
-
-        skip_revisions = {501}
-        pending_revisions = {}
-
-        mock_load_status_file.return_value = bisect_state
-
-        mock_get_range.return_value = (
-            start,
-            end,
-            pending_revisions,
-            skip_revisions,
-        )
-
-        mock_get_revision_and_hash_list.return_value = [], []
-
-        args_output = test_helpers.ArgsOutputTest()
-        args_output.start_rev = start
-        args_output.end_rev = end
-        args_output.parallel = 3
-        args_output.src_path = None
-        args_output.chromeos_path = "somepath"
-        args_output.cleanup = True
-
-        self.assertEqual(
-            llvm_bisection.main(args_output),
-            llvm_bisection.BisectionExitStatus.BISECTION_COMPLETE.value,
-        )
-
-        mock_chromeos_root.assert_called_once()
-
-        mock_outside_chroot.assert_called_once()
-
-        mock_load_status_file.assert_called_once()
-
-        mock_get_range.assert_called_once()
-
-        mock_get_revision_and_hash_list.assert_called_once()
-
-        mock_abandon_cl.assert_called_once()
-        self.assertEqual(
-            mock_abandon_cl.call_args,
-            mock.call(
-                [
-                    os.path.join(
-                        args_output.chromeos_path, "chromite/bin/gerrit"
-                    ),
-                    "abandon",
-                    str(cl),
-                ],
-                stderr=subprocess.STDOUT,
-                encoding="utf-8",
-            ),
-        )
-
-    @mock.patch.object(llvm_bisection, "LoadStatusFile")
-    @mock.patch.object(chroot, "VerifyChromeOSRoot")
-    @mock.patch.object(chroot, "VerifyOutsideChroot", return_value=True)
-    def testMainFailedWithInvalidRange(
-        self, mock_chromeos_root, mock_outside_chroot, mock_load_status_file
-    ):
-        start = 500
-        end = 502
-
-        bisect_state = {
-            "start": start - 1,
-            "end": end,
-        }
-
-        mock_load_status_file.return_value = bisect_state
-
-        args_output = test_helpers.ArgsOutputTest()
-        args_output.start_rev = start
-        args_output.end_rev = end
-        args_output.parallel = 3
-        args_output.src_path = None
-
-        with self.assertRaises(ValueError) as err:
-            llvm_bisection.main(args_output)
-
-        error_message = (
-            f"The start {start} or the end {end} version provided is "
-            f'different than "start" {bisect_state["start"]} or "end" '
-            f'{bisect_state["end"]} in the .JSON file'
-        )
-
-        self.assertEqual(str(err.exception), error_message)
-
-        mock_chromeos_root.assert_called_once()
-
-        mock_outside_chroot.assert_called_once()
-
-        mock_load_status_file.assert_called_once()
-
-    @mock.patch.object(llvm_bisection, "GetCommitsBetween")
-    @mock.patch.object(llvm_bisection, "GetRemainingRange")
-    @mock.patch.object(llvm_bisection, "LoadStatusFile")
-    @mock.patch.object(chroot, "VerifyChromeOSRoot")
-    @mock.patch.object(chroot, "VerifyOutsideChroot", return_value=True)
-    def testMainFailedWithPendingBuilds(
-        self,
-        mock_chromeos_root,
-        mock_outside_chroot,
-        mock_load_status_file,
-        mock_get_range,
-        mock_get_revision_and_hash_list,
-    ):
-        start = 500
-        end = 502
-        rev = 501
-
-        bisect_state = {
-            "start": start,
-            "end": end,
-            "jobs": [{"rev": rev, "status": "pending"}],
-        }
-
-        skip_revisions = {}
-        pending_revisions = {rev}
-
-        mock_load_status_file.return_value = bisect_state
-
-        mock_get_range.return_value = (
-            start,
-            end,
-            pending_revisions,
-            skip_revisions,
-        )
-
-        mock_get_revision_and_hash_list.return_value = [], []
-
-        args_output = test_helpers.ArgsOutputTest()
-        args_output.start_rev = start
-        args_output.end_rev = end
-        args_output.parallel = 3
-        args_output.src_path = None
-
-        with self.assertRaises(ValueError) as err:
-            llvm_bisection.main(args_output)
-
-        error_message = (
-            f"No revisions between start {start} and end {end} to "
-            "create tryjobs\nThe following tryjobs are pending:\n"
-            f"{rev}\n"
-        )
-
-        self.assertEqual(str(err.exception), error_message)
-
-        mock_chromeos_root.assert_called_once()
-
-        mock_outside_chroot.assert_called_once()
-
-        mock_load_status_file.assert_called_once()
-
-        mock_get_range.assert_called_once()
-
-        mock_get_revision_and_hash_list.assert_called_once()
-
-    @mock.patch.object(llvm_bisection, "GetCommitsBetween")
-    @mock.patch.object(llvm_bisection, "GetRemainingRange")
-    @mock.patch.object(llvm_bisection, "LoadStatusFile")
-    @mock.patch.object(chroot, "VerifyChromeOSRoot")
-    @mock.patch.object(chroot, "VerifyOutsideChroot", return_value=True)
-    def testMainFailedWithDuplicateBuilds(
-        self,
-        mock_outside_chroot,
-        mock_chromeos_root,
-        mock_load_status_file,
-        mock_get_range,
-        mock_get_revision_and_hash_list,
-    ):
-        start = 500
-        end = 502
-        rev = 501
-        git_hash = "a123testhash1"
-
-        bisect_state = {
-            "start": start,
-            "end": end,
-            "jobs": [{"rev": rev, "status": "pending"}],
-        }
-
-        skip_revisions = {}
-        pending_revisions = {rev}
-
-        mock_load_status_file.return_value = bisect_state
-
-        mock_get_range.return_value = (
-            start,
-            end,
-            pending_revisions,
-            skip_revisions,
-        )
-
-        mock_get_revision_and_hash_list.return_value = [rev], [git_hash]
-
-        args_output = test_helpers.ArgsOutputTest()
-        args_output.start_rev = start
-        args_output.end_rev = end
-        args_output.parallel = 3
-        args_output.src_path = None
-
-        with self.assertRaises(ValueError) as err:
-            llvm_bisection.main(args_output)
-
-        error_message = 'Revision %d exists already in "jobs"' % rev
-        self.assertEqual(str(err.exception), error_message)
-
-        mock_chromeos_root.assert_called_once()
-
-        mock_outside_chroot.assert_called_once()
-
-        mock_load_status_file.assert_called_once()
-
-        mock_get_range.assert_called_once()
-
-        mock_get_revision_and_hash_list.assert_called_once()
-
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    @mock.patch.object(
-        get_llvm_hash.LLVMHash, "GetLLVMHash", return_value="a123testhash4"
-    )
-    @mock.patch.object(llvm_bisection, "GetCommitsBetween")
-    @mock.patch.object(llvm_bisection, "GetRemainingRange")
-    @mock.patch.object(llvm_bisection, "LoadStatusFile")
-    @mock.patch.object(chroot, "VerifyChromeOSRoot")
-    @mock.patch.object(chroot, "VerifyOutsideChroot", return_value=True)
-    def testMainFailedToAbandonCL(
-        self,
-        mock_outside_chroot,
-        mock_chromeos_root,
-        mock_load_status_file,
-        mock_get_range,
-        mock_get_revision_and_hash_list,
-        _mock_get_bad_llvm_hash,
-        mock_abandon_cl,
-    ):
-        start = 500
-        end = 502
-
-        bisect_state = {
-            "start": start,
-            "end": end,
-            "jobs": [{"rev": 501, "status": "bad", "cl": 0}],
-        }
-
-        skip_revisions = {501}
-        pending_revisions = {}
-
-        mock_load_status_file.return_value = bisect_state
-
-        mock_get_range.return_value = (
-            start,
-            end,
-            pending_revisions,
-            skip_revisions,
-        )
-
-        mock_get_revision_and_hash_list.return_value = ([], [])
-
-        error_message = "Error message."
-        mock_abandon_cl.side_effect = subprocess.CalledProcessError(
-            returncode=1, cmd=[], output=error_message
-        )
-
-        args_output = test_helpers.ArgsOutputTest()
-        args_output.start_rev = start
-        args_output.end_rev = end
-        args_output.parallel = 3
-        args_output.src_path = None
-        args_output.cleanup = True
-
-        with self.assertRaises(subprocess.CalledProcessError) as err:
-            llvm_bisection.main(args_output)
-
-        self.assertEqual(err.exception.output, error_message)
-
-        mock_chromeos_root.assert_called_once()
-
-        mock_outside_chroot.assert_called_once()
-
-        mock_load_status_file.assert_called_once()
-
-        mock_get_range.assert_called_once()
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/llvm_local_bisection.sh b/llvm_tools/llvm_local_bisection.sh
deleted file mode 100755
index 0dde49d7..00000000
--- a/llvm_tools/llvm_local_bisection.sh
+++ /dev/null
@@ -1,109 +0,0 @@
-#!/bin/bash -u
-# -*- coding: utf-8 -*-
-# Copyright 2022 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-# llvm_bisection_template.sh
-#
-# This script is meant to be run inside a `git bisect` process, like so:
-#
-#   $ cd <your llvm-project dir>
-#   $ git bisect start
-#   $ git bisect bad <your bad ref>
-#   $ git bisect good <your good ref>
-#   $ git bisect run ~/chromimuos/src/scripts/llvm_bisection_template.sh
-#
-# This template exists as a "batteries included" LLVM bisection script,
-# which will modify the LLVM_NEXT hash to help the mage track down issues
-# locally.
-#
-# Modify the fixme sections below to customize to your bisection use-case.
-
-# FIXME: Replace this for the location of your llvm clone within the chroot.
-# We need this for the git history.
-LLVM_CLONE_PATH="${HOME}/chromiumos/src/third_party/llvm-project"
-
-main () {
-  # Note this builds with USE="llvm-next debug -thinlto -llvm_pgo_use continue-on-patch-failure"
-  build_llvm || exit
-
-  # FIXME: Write your actual bisection command here which uses
-  # LLVM_NEXT here.
-  #
-  # Example bisection command:
-  #
-  #   build_pkg efitools || exit 1
-  #
-  # You can use build_pkg if you want to emerge a package and print
-  # out diagnostics along the way
-  #
-  #   Fail Example: build_pkg "${MY_PACKAGE}" || exit 1
-  #   Skip Example: build_pkg "${MY_PACKAGE}" || exit 125
-  #
-}
-
-# ---------------------------------------------------------------------
-
-# Current LLVM_NEXT_HASH we're using. Does not need to be set.
-CURRENT='UNKNOWN'
-
-logdo () {
-  local cmd="${1}"
-  shift
-  printf '%1 $ %2' "$(date '+%T')" "${cmd}"
-  for i in "$@"; do
-    printf "'%1'" "${i}"
-  done
-  printf "\n"
-  "${cmd}" "$@"
-}
-
-log () {
-  echo "$(date '+%T') | $*"
-}
-
-build_llvm () {
-  cd "${LLVM_CLONE_PATH}" || exit 2  # Exit with error
-  local llvm_ebuild_path
-  llvm_ebuild_path="$(readlink -f "$(equery which llvm)")"
-  CURRENT="$(git rev-parse --short HEAD)"
-  log "Current hash=${CURRENT}"
-  NEW_LINE="LLVM_NEXT_HASH=\"${CURRENT}\""
-  sed -i "s/^LLVM_NEXT_HASH=\".*\"/${NEW_LINE}/" "${llvm_ebuild_path}"
-
-  local logfile="/tmp/build-llvm.${CURRENT}.out"
-  log "Writing logs to ${logfile}"
-  log "sudo USE='llvm-next debug -thinlto -llvm_use_pgo continue-on-patch-failure'" \
-      " emerge sys-devel/llvm"
-  logdo sudo USE='llvm-next debug -thinlto -llvm_use_pgo continue-on-patch-failure' emerge \
-    sys-devel/llvm \
-    &> "${logfile}"
-  local emerge_exit_code="$?"
-  if [[ "${emerge_exit_code}" -ne 0 ]]; then
-    log "FAILED to build llvm with hash=${CURRENT}"
-    log 'Skipping this hash'
-    return 125  # 125 is the "skip" exit code.
-  fi
-  log "Succesfully built LLVM with hash=${CURRENT}"
-  return 0  # Explicitly returning 0 for "good" even if a command errors out
-}
-
-build_pkg () {
-  local pkg="${1}"
-
-  local logfile="/tmp/build-${pkg//\//_}.${CURRENT}.out"
-  log "Writing logs to ${logfile}"
-  log "sudo emerge ${pkg}"
-  logdo sudo emerge "${pkg}" \
-    &> "${logfile}"
-  local emerge_exit_code="$?"
-  if [[ "${emerge_exit_code}" -ne 0 ]]; then
-    log "FAILED to build ${pkg} with hash=${CURRENT}"
-    return 1  # 1 here isn't for bisection, but for chaining with `||`
-  fi
-  log "Successfully built ${pkg} with hash=${CURRENT}"
-  return 0  # Explicitly returning 0 for "good" even if a command errors out
-}
-
-main
diff --git a/llvm_tools/llvm_next.py b/llvm_tools/llvm_next.py
index 1c075e50..0fb28711 100644
--- a/llvm_tools/llvm_next.py
+++ b/llvm_tools/llvm_next.py
@@ -6,25 +6,30 @@
 
 from typing import Iterable
 
-import cros_cls
+from llvm_tools import cros_cls
 
 
-LLVM_NEXT_HASH = "28a8f1b901389c1e478407440f7ccf2d41c71b64"
-LLVM_NEXT_REV = 516547
+LLVM_NEXT_HASH = "386af4a5c64ab75eaee2448dc38f2e34a40bfed0"
+LLVM_NEXT_REV = 563880
 
 # NOTE: Always specify patch-sets for CLs. We don't want uploads by untrusted
 # users to turn into bot invocations w/ untrusted input.
+#
+# Please note that these are (somewhat) automatically curated. See
+# llvm_next_py_autoupdate.py.
+# pylint: disable=line-too-long
+LLVM_NEXT_TESTING_CL_URLS: Iterable[str] = (
+    "https://crrev.com/c/6417962/2",
+    "https://crrev.com/i/8154895/16",
+)
 
 # A list of CLs that constitute the current llvm-next roll.
 # This is taken as the set of CLs that will be landed simultaneously in order
 # to make llvm-next go live.
 #
 # Generally speaking, for simple rolls, this should just contain a link to the
-# Manifest update CL.
-LLVM_NEXT_TESTING_CLS: Iterable[cros_cls.ChangeListURL] = ()
-
-# The CL used to disable -Werror, and report the results.
-DISABLE_WERROR_CL = cros_cls.ChangeListURL(
-    cl_id=2599698,
-    patch_set=5,
+# Manifest update CL, as well as (early on, at least) a link to a CL generated
+# by upload_llvm_testing_helper_cl.py.
+LLVM_NEXT_TESTING_CLS: Iterable[cros_cls.ChangeListURL] = tuple(
+    cros_cls.ChangeListURL.parse(url) for url in LLVM_NEXT_TESTING_CL_URLS
 )
diff --git a/llvm_tools/llvm_next_py_autoupdate.py b/llvm_tools/llvm_next_py_autoupdate.py
new file mode 100644
index 00000000..d97de53a
--- /dev/null
+++ b/llvm_tools/llvm_next_py_autoupdate.py
@@ -0,0 +1,293 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Automatically keeps llvm_next.py in the current toolchain-utils fresh.
+
+The llvm_next.py file this targets is in the same directory as this script in
+toolchain-utils. Actual edits are made in a worktree, and locally `git
+commit`ed by this script.
+
+It does this by:
+    - Removing obsolete testing URLs
+    - Auto-updating patch-sets as appropriate
+"""
+
+import argparse
+import dataclasses
+import json
+import logging
+from pathlib import Path
+import subprocess
+from typing import Iterable, List, Optional, Set, Tuple
+
+from cros_utils import cros_paths
+from cros_utils import git_utils
+from llvm_tools import cros_cls
+from llvm_tools import llvm_next
+
+
+@dataclasses.dataclass(frozen=True, eq=True)
+class GerritCLInfo:
+    """Carries relevant info about a CL."""
+
+    is_abandoned_or_merged: bool
+    is_uploader_a_googler: bool
+    most_recent_patch_set: int
+
+
+def parse_direct_owners_from_file(file_contents: str) -> List[str]:
+    """Parses unrestricted OWNERS emails from the given file contents."""
+    results = []
+    for line in file_contents.splitlines():
+        no_comment = line.split("#", 1)[0].strip()
+        # Skip any lines with embedded spaces. There are many directives in
+        # OWNERS files (e.g., includes, per-file owners, etc). All we care
+        # about here is accounts _directly mentioned_ with unrestricted access.
+        if any(c.isspace() for c in no_comment):
+            continue
+        if "@" in no_comment:
+            results.append(no_comment)
+    return results
+
+
+class LazyToolchainOwners:
+    """Caches OWNERS file entries for the toolchain file.
+
+    Used to cheaply (and lossily) check to see if an @chromium email address is
+    a Googler.
+    """
+
+    def __init__(self, owners_file: Path):
+        if not owners_file.exists():
+            raise ValueError(
+                f"Handed path to nonexistent OWNERS file: {owners_file}"
+            )
+        self._owners_file = owners_file
+        self._owners: Optional[Set[str]] = None
+
+    def contains(self, email: str) -> bool:
+        """Loads OWNERS, and returns if `email` is directly mentioned in it.
+
+        Repeated calls will reuse cached results of loading OWNERS.
+        """
+        if self._owners is None:
+            self._owners = set(
+                parse_direct_owners_from_file(
+                    self._owners_file.read_text(encoding="utf-8")
+                )
+            )
+        return email in self._owners
+
+
+def fetch_cl_info(
+    toolchain_owners: LazyToolchainOwners, cl: cros_cls.ChangeListURL
+) -> GerritCLInfo:
+    gerrit_stdout = subprocess.run(
+        ("gerrit", "--json", "inspect", cl.gerrit_tool_id),
+        check=True,
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+    ).stdout
+    gerrit_info = json.loads(gerrit_stdout)[0]
+
+    # cl_status is a ChangeInfo's status:
+    # https://gerrit-review.googlesource.com/Documentation/rest-api-changes.html#change-info
+    cl_status = gerrit_info.get("status")
+    if cl_status not in ("NEW", "MERGED", "ABANDONED"):
+        raise ValueError(f"Unexpected CL status on {cl}: {cl_status!r}")
+
+    current_ps_info = gerrit_info.get("currentPatchSet", {})
+    current_ps_str = current_ps_info.get("number")
+    try:
+        current_ps = int(current_ps_str)
+    except ValueError:
+        current_ps = None
+
+    # Raise this outside of the exception handler, so the backtrace is easier
+    # to understand.
+    if current_ps is None:
+        raise ValueError(
+            f"Unexpected current patch-set number status on {cl}: "
+            f"{current_ps_str!r}"
+        )
+
+    uploader_email = current_ps_info.get("uploader", {}).get("email")
+    if not uploader_email:
+        logging.warning(
+            "Could not determine uploader for %s; assuming non-Googler",
+            cl,
+        )
+        is_uploader_a_googler = False
+    elif uploader_email.endswith("@google.com"):
+        is_uploader_a_googler = True
+    elif uploader_email.endswith("@chromium.org") and toolchain_owners.contains(
+        uploader_email
+    ):
+        logging.info(
+            "Assuming %s is a Googler, as the email is a toolchain OWNER",
+            uploader_email,
+        )
+        is_uploader_a_googler = True
+    else:
+        logging.info(
+            "%s is neither @google nor an OWNER; assuming not-Googler",
+            uploader_email,
+        )
+        is_uploader_a_googler = False
+
+    return GerritCLInfo(
+        is_abandoned_or_merged=cl_status != "NEW",
+        is_uploader_a_googler=is_uploader_a_googler,
+        most_recent_patch_set=current_ps,
+    )
+
+
+def update_testing_url_list(
+    toolchain_owners: LazyToolchainOwners,
+    current_list: Iterable[str],
+) -> Optional[Tuple[str, List[str]]]:
+    new_list = []
+    change_descriptions = []
+
+    for url in current_list:
+        cl_url = cros_cls.ChangeListURL.parse(url)
+        cl_info = fetch_cl_info(toolchain_owners, cl_url)
+        if cl_info.is_abandoned_or_merged:
+            logging.info("%s was closed; removing from list", cl_url)
+            change_descriptions.append(f"{cl_url} was closed")
+            continue
+
+        if cl_info.most_recent_patch_set == cl_url.patch_set:
+            logging.info(
+                "%s is alive and at most recent patch-set; nothing to do",
+                cl_url,
+            )
+            # Append the URL verbatim to minimize diffs.
+            new_list.append(url)
+            continue
+
+        if not cl_info.is_uploader_a_googler:
+            logging.warning(
+                "CL %s has newer patch-set, but isn't googler-uploaded. Skip.",
+                cl_url,
+            )
+            # Append the URL verbatim to minimize diffs.
+            new_list.append(url)
+            continue
+
+        logging.info("CL %s patch-set was updated; updating.", cl_url)
+        change_descriptions.append(f"{cl_url} had a patch-set update")
+        new_list.append(
+            str(
+                dataclasses.replace(
+                    cl_url,
+                    patch_set=cl_info.most_recent_patch_set,
+                )
+            )
+        )
+
+    # If there are no change descriptions, no meaningful changes were made.
+    if not change_descriptions:
+        return None
+
+    return "\n".join(f"- {x}" for x in change_descriptions), new_list
+
+
+def write_url_list(llvm_next_py_file_path: Path, new_url_list: List[str]):
+    llvm_next_py = llvm_next_py_file_path.read_text(encoding="utf-8")
+    var_start_string = "\nLLVM_NEXT_TESTING_CL_URLS: Iterable[str] = ("
+    testing_cl_urls_start = llvm_next_py.index(var_start_string)
+
+    # In a `cros format`'ed file, are two cases to handle here when finding the
+    # last parenthesis:
+    # 1. it's on the same line
+    # 2. it's on a line of its own
+    # Ignore anything else for simplicity.
+    after_start_paren = testing_cl_urls_start + len(var_start_string)
+    line_end = llvm_next_py.index("\n", after_start_paren)
+    same_line_end_paren = llvm_next_py.find(")", after_start_paren, line_end)
+    if same_line_end_paren != -1:
+        end_paren = same_line_end_paren
+    else:
+        end_paren = llvm_next_py.index("\n)", after_start_paren)
+
+    # N.B., "," is appended to each element rather than part of `"\n".join`,
+    # since single-elem tuples need it.
+    new_list_contents = "\n".join(repr(x) + "," for x in new_url_list)
+    new_llvm_next_py = "\n".join(
+        (
+            llvm_next_py[:after_start_paren],
+            new_list_contents,
+            llvm_next_py[end_paren:],
+        )
+    )
+    llvm_next_py_file_path.write_text(new_llvm_next_py, encoding="utf-8")
+    subprocess.run(
+        ("cros", "format", llvm_next_py_file_path),
+        check=True,
+    )
+
+
+def main(argv: List[str]) -> None:
+    logging.basicConfig(
+        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
+        "%(message)s",
+        level=logging.INFO,
+    )
+
+    parser = argparse.ArgumentParser(
+        description=__doc__,
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+    )
+    parser.add_argument(
+        "--upload",
+        action="store_true",
+        help="""
+        Upload changes after making them, auto-add reviewer(s), and hit CQ+1.
+        """,
+    )
+    opts = parser.parse_args(argv)
+
+    update_result = update_testing_url_list(
+        toolchain_owners=LazyToolchainOwners(
+            owners_file=cros_paths.script_toolchain_utils_root()
+            / "OWNERS.toolchain"
+        ),
+        current_list=llvm_next.LLVM_NEXT_TESTING_CL_URLS,
+    )
+    if not update_result:
+        logging.info("All URLs are up-to-date.")
+        return
+
+    change_descriptions, new_url_list = update_result
+    logging.info("URL list changed; creating commit...")
+    toolchain_utils_root = cros_paths.script_toolchain_utils_root()
+    with git_utils.create_worktree(toolchain_utils_root) as worktree:
+        write_url_list(worktree / "llvm_tools" / "llvm_next.py", new_url_list)
+        sha = git_utils.commit_all_changes(
+            worktree,
+            # Use "\n".join rather than textwrap.dedent, since
+            # `change_descriptions` won't be indented properly
+            message="\n".join(
+                (
+                    "llvm_tools: autoupdate CL list",
+                    "",
+                    change_descriptions,
+                    "",
+                    "BUG=None",
+                    "TEST=CQ+1",
+                )
+            ),
+        )
+        logging.info("SHA of commit: %s", sha)
+        if opts.upload:
+            cl_list = git_utils.upload_to_gerrit(
+                worktree,
+                remote=git_utils.CROS_EXTERNAL_REMOTE,
+                branch=git_utils.CROS_MAIN_BRANCH,
+            )
+            for cl in cl_list:
+                git_utils.set_autoreview_topic_and_labels(
+                    toolchain_utils_root, cl
+                )
diff --git a/llvm_tools/llvm_next_py_autoupdate_test.py b/llvm_tools/llvm_next_py_autoupdate_test.py
new file mode 100644
index 00000000..6f2d80b5
--- /dev/null
+++ b/llvm_tools/llvm_next_py_autoupdate_test.py
@@ -0,0 +1,431 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Tests for llvm_next_py_autoupdate."""
+
+import contextlib
+import dataclasses
+import json
+import subprocess
+import textwrap
+from typing import Dict, Iterable
+from unittest import mock
+
+from llvm_tools import cros_cls
+from llvm_tools import llvm_next_py_autoupdate
+from llvm_tools import test_helpers
+
+
+ARBITRARY_CL_URL = cros_cls.ChangeListURL.parse("crrev.com/c/98765432/1")
+
+
+class Test(test_helpers.TempDirTestCase):
+    """Tests for llvm_next_py_autoupdate."""
+
+    def toolchain_owners_with_listing(
+        self, owners: Iterable[str]
+    ) -> llvm_next_py_autoupdate.LazyToolchainOwners:
+        owners_file_path = self.make_tempdir() / "OWNERS.mock"
+        owners_file_path.write_text("\n".join(owners), encoding="utf-8")
+        return llvm_next_py_autoupdate.LazyToolchainOwners(owners_file_path)
+
+    def empty_toolchain_owners(
+        self,
+    ) -> llvm_next_py_autoupdate.LazyToolchainOwners:
+        return self.toolchain_owners_with_listing(())
+
+    @mock.patch.object(subprocess, "run")
+    def test_fetch_cl_info_works_with_new_cl(self, mock_subprocess_run):
+        mock_run_return_value = mock.MagicMock()
+        mock_run_return_value.stdout = json.dumps(
+            [
+                {
+                    "status": "NEW",
+                    "currentPatchSet": {
+                        "number": "123",
+                    },
+                }
+            ]
+        )
+        mock_subprocess_run.return_value = mock_run_return_value
+        self.assertEqual(
+            llvm_next_py_autoupdate.fetch_cl_info(
+                self.empty_toolchain_owners(), ARBITRARY_CL_URL
+            ),
+            llvm_next_py_autoupdate.GerritCLInfo(
+                is_abandoned_or_merged=False,
+                is_uploader_a_googler=False,
+                most_recent_patch_set=123,
+            ),
+        )
+
+    @mock.patch.object(subprocess, "run")
+    def test_fetch_cl_info_works_with_closed_cl(self, mock_subprocess_run):
+        mock_run_return_value = mock.MagicMock()
+        mock_subprocess_run.return_value = mock_run_return_value
+
+        for closed_status in ("ABANDONED", "MERGED"):
+            mock_run_return_value.stdout = json.dumps(
+                [
+                    {
+                        "status": closed_status,
+                        "currentPatchSet": {
+                            "number": "123",
+                        },
+                    }
+                ]
+            )
+            self.assertEqual(
+                llvm_next_py_autoupdate.fetch_cl_info(
+                    self.empty_toolchain_owners(), ARBITRARY_CL_URL
+                ),
+                llvm_next_py_autoupdate.GerritCLInfo(
+                    is_abandoned_or_merged=True,
+                    is_uploader_a_googler=False,
+                    most_recent_patch_set=123,
+                ),
+            )
+
+    @mock.patch.object(subprocess, "run")
+    def test_fetch_cl_info_determines_googler_is_googler(
+        self, mock_subprocess_run
+    ):
+        mock_run_return_value = mock.MagicMock()
+        mock_run_return_value.stdout = json.dumps(
+            [
+                {
+                    "status": "NEW",
+                    "currentPatchSet": {
+                        "number": "123",
+                        "uploader": {
+                            "email": "foo@google.com",
+                        },
+                    },
+                }
+            ]
+        )
+        mock_subprocess_run.return_value = mock_run_return_value
+        self.assertEqual(
+            llvm_next_py_autoupdate.fetch_cl_info(
+                self.empty_toolchain_owners(), ARBITRARY_CL_URL
+            ),
+            llvm_next_py_autoupdate.GerritCLInfo(
+                is_abandoned_or_merged=False,
+                is_uploader_a_googler=True,
+                most_recent_patch_set=123,
+            ),
+        )
+
+    @mock.patch.object(subprocess, "run")
+    def test_fetch_cl_info_determines_chromium_isnt_googler(
+        self, mock_subprocess_run
+    ):
+        mock_run_return_value = mock.MagicMock()
+        mock_run_return_value.stdout = json.dumps(
+            [
+                {
+                    "status": "NEW",
+                    "currentPatchSet": {
+                        "number": "123",
+                        "uploader": {
+                            "email": "foo@chromium.org",
+                        },
+                    },
+                }
+            ]
+        )
+        mock_subprocess_run.return_value = mock_run_return_value
+        self.assertEqual(
+            llvm_next_py_autoupdate.fetch_cl_info(
+                self.empty_toolchain_owners(), ARBITRARY_CL_URL
+            ),
+            llvm_next_py_autoupdate.GerritCLInfo(
+                is_abandoned_or_merged=False,
+                is_uploader_a_googler=False,
+                most_recent_patch_set=123,
+            ),
+        )
+
+    @mock.patch.object(subprocess, "run")
+    def test_fetch_cl_info_determines_chromium_owner_is_googler(
+        self, mock_subprocess_run
+    ):
+        mock_run_return_value = mock.MagicMock()
+        mock_run_return_value.stdout = json.dumps(
+            [
+                {
+                    "status": "NEW",
+                    "currentPatchSet": {
+                        "number": "123",
+                        "uploader": {
+                            "email": "foo@chromium.org",
+                        },
+                    },
+                }
+            ]
+        )
+        mock_subprocess_run.return_value = mock_run_return_value
+        self.assertEqual(
+            llvm_next_py_autoupdate.fetch_cl_info(
+                self.toolchain_owners_with_listing(
+                    ["bar@google.com", "foo@chromium.org"]
+                ),
+                ARBITRARY_CL_URL,
+            ),
+            llvm_next_py_autoupdate.GerritCLInfo(
+                is_abandoned_or_merged=False,
+                is_uploader_a_googler=True,
+                most_recent_patch_set=123,
+            ),
+        )
+
+    @contextlib.contextmanager
+    def mock_fetch_cl_info(
+        self,
+        mock_cl_info: Dict[
+            cros_cls.ChangeListURL, llvm_next_py_autoupdate.GerritCLInfo
+        ],
+    ):
+        """Mocks `fetch_cl_info` to return `mock_cl_info` entries."""
+
+        def fetch_cl_info_side_effect(
+            _owners: llvm_next_py_autoupdate.LazyToolchainOwners,
+            cl: cros_cls.ChangeListURL,
+        ) -> llvm_next_py_autoupdate.GerritCLInfo:
+            if x := mock_cl_info.get(cl):
+                return x
+            raise ValueError(f"CL without mock info: {cl}")
+
+        with mock.patch.object(
+            llvm_next_py_autoupdate, "fetch_cl_info"
+        ) as mock_fetch_cl_info:
+            mock_fetch_cl_info.side_effect = fetch_cl_info_side_effect
+            yield mock_fetch_cl_info
+
+    def test_update_empty_urls(self):
+        with self.mock_fetch_cl_info(mock_cl_info={}):
+            self.assertIsNone(
+                llvm_next_py_autoupdate.update_testing_url_list(
+                    self.empty_toolchain_owners(), ()
+                )
+            )
+
+    def test_merged_cl_is_removed_by_update(self):
+        mock_cl_info = {
+            ARBITRARY_CL_URL: llvm_next_py_autoupdate.GerritCLInfo(
+                is_abandoned_or_merged=True,
+                is_uploader_a_googler=True,
+                most_recent_patch_set=1,
+            )
+        }
+        with self.mock_fetch_cl_info(mock_cl_info) as mocked_fetch:
+            (
+                messages,
+                new_list,
+            ) = llvm_next_py_autoupdate.update_testing_url_list(
+                self.empty_toolchain_owners(), [str(ARBITRARY_CL_URL)]
+            )
+            mocked_fetch.assert_called_once()
+
+        self.assertEqual(new_list, [])
+        self.assertNotEqual(messages, "")
+
+    def test_update_is_nop_if_no_CLs_changed(self):
+        mock_cl_info = {
+            ARBITRARY_CL_URL: llvm_next_py_autoupdate.GerritCLInfo(
+                is_abandoned_or_merged=False,
+                is_uploader_a_googler=True,
+                most_recent_patch_set=ARBITRARY_CL_URL.patch_set,
+            ),
+        }
+        with self.mock_fetch_cl_info(mock_cl_info) as mocked_fetch:
+            self.assertIsNone(
+                llvm_next_py_autoupdate.update_testing_url_list(
+                    self.empty_toolchain_owners(), [str(ARBITRARY_CL_URL)]
+                )
+            )
+            mocked_fetch.assert_called_once()
+
+    def test_update_happens_if_patch_set_changed(self):
+        new_patch_set = ARBITRARY_CL_URL.patch_set + 1
+        mock_cl_info = {
+            ARBITRARY_CL_URL: llvm_next_py_autoupdate.GerritCLInfo(
+                is_abandoned_or_merged=False,
+                is_uploader_a_googler=True,
+                most_recent_patch_set=new_patch_set,
+            ),
+        }
+        with self.mock_fetch_cl_info(mock_cl_info) as mocked_fetch:
+            (
+                messages,
+                new_list,
+            ) = llvm_next_py_autoupdate.update_testing_url_list(
+                self.empty_toolchain_owners(), [str(ARBITRARY_CL_URL)]
+            )
+            mocked_fetch.assert_called_once()
+
+        self.assertEqual(
+            new_list,
+            [
+                str(
+                    dataclasses.replace(
+                        ARBITRARY_CL_URL,
+                        patch_set=new_patch_set,
+                    )
+                )
+            ],
+        )
+        self.assertNotEqual(messages, "")
+
+    def test_update_skipped_if_patch_set_changed_by_non_googler(self):
+        new_patch_set = ARBITRARY_CL_URL.patch_set + 1
+        mock_cl_info = {
+            ARBITRARY_CL_URL: llvm_next_py_autoupdate.GerritCLInfo(
+                is_abandoned_or_merged=False,
+                is_uploader_a_googler=False,
+                most_recent_patch_set=new_patch_set,
+            ),
+        }
+        with self.mock_fetch_cl_info(mock_cl_info) as mocked_fetch:
+            self.assertIsNone(
+                llvm_next_py_autoupdate.update_testing_url_list(
+                    self.empty_toolchain_owners(), [str(ARBITRARY_CL_URL)]
+                )
+            )
+            mocked_fetch.assert_called_once()
+
+    def assert_only_call_is_cros_format(
+        self, mock_subprocess_run: mock.MagicMock
+    ):
+        mock_subprocess_run.assert_called_once()
+        self.assertEqual(
+            mock_subprocess_run.call_args[0][0][:2],
+            ("cros", "format"),
+        )
+
+    @mock.patch.object(subprocess, "run")
+    def test_updating_empty_cl_list(self, mock_subprocess_run):
+        llvm_next_py = self.make_tempdir() / "llvm_next.py"
+        llvm_next_py.write_text(
+            textwrap.dedent(
+                """\
+                # Some comment
+                LLVM_NEXT_TESTING_CL_URLS: Iterable[str] = ()
+
+                # Some other comment
+                """
+            ),
+            encoding="utf-8",
+        )
+
+        llvm_next_py_autoupdate.write_url_list(
+            llvm_next_py, [str(ARBITRARY_CL_URL)]
+        )
+        self.assertEqual(
+            llvm_next_py.read_text(encoding="utf-8"),
+            textwrap.dedent(
+                f"""\
+                # Some comment
+                LLVM_NEXT_TESTING_CL_URLS: Iterable[str] = (
+                {repr(str(ARBITRARY_CL_URL))},
+                )
+
+                # Some other comment
+                """
+            ),
+        )
+        self.assert_only_call_is_cros_format(mock_subprocess_run)
+
+    @mock.patch.object(subprocess, "run")
+    def test_updating_cl_list_to_be_empty(self, mock_subprocess_run):
+        llvm_next_py = self.make_tempdir() / "llvm_next.py"
+        llvm_next_py.write_text(
+            textwrap.dedent(
+                """\
+                # Some comment
+                LLVM_NEXT_TESTING_CL_URLS: Iterable[str] = (
+                "some CL URL",
+                )
+
+                # Some other comment
+                """
+            ),
+            encoding="utf-8",
+        )
+
+        llvm_next_py_autoupdate.write_url_list(llvm_next_py, [])
+        # N.B., `cros format` will eliminate the unnecesary '\n's.
+        self.assertEqual(
+            llvm_next_py.read_text(encoding="utf-8"),
+            textwrap.dedent(
+                """\
+                # Some comment
+                LLVM_NEXT_TESTING_CL_URLS: Iterable[str] = (
+
+
+                )
+
+                # Some other comment
+                """
+            ),
+        )
+        self.assert_only_call_is_cros_format(mock_subprocess_run)
+
+    @mock.patch.object(subprocess, "run")
+    def test_same_line_cl_paren_works(self, mock_subprocess_run):
+        llvm_next_py = self.make_tempdir() / "llvm_next.py"
+        llvm_next_py.write_text(
+            textwrap.dedent(
+                """\
+                # Some comment
+                LLVM_NEXT_TESTING_CL_URLS: Iterable[str] = ("some CL URL")
+
+                # Some other comment
+                """
+            ),
+            encoding="utf-8",
+        )
+
+        llvm_next_py_autoupdate.write_url_list(llvm_next_py, [])
+        # N.B., `cros format` will eliminate the unnecesary '\n'.
+        self.assertEqual(
+            llvm_next_py.read_text(encoding="utf-8"),
+            textwrap.dedent(
+                """\
+                # Some comment
+                LLVM_NEXT_TESTING_CL_URLS: Iterable[str] = (
+
+                )
+
+                # Some other comment
+                """
+            ),
+        )
+        self.assert_only_call_is_cros_format(mock_subprocess_run)
+
+    def test_owners_file_parsing_functions(self):
+        contents = textwrap.dedent(
+            """\
+            foo@chromium.org
+            bar@google.com
+            """
+        )
+        owners = llvm_next_py_autoupdate.parse_direct_owners_from_file(contents)
+        self.assertEqual(owners, ["foo@chromium.org", "bar@google.com"])
+
+    def test_owners_file_parsing_ignores_exciting_patterns(self):
+        contents = textwrap.dedent(
+            """\
+            # Some commentary
+            foo@chromium.org  # More commentary
+            #Even-More@Commentary
+            per-file some-file = bar@chromium.org
+            include ../OWNERS
+            # OWNERS emails can either be '*' or a valid email. Ignore the
+            # former.
+            *
+            """
+        )
+        owners = llvm_next_py_autoupdate.parse_direct_owners_from_file(contents)
+        self.assertEqual(owners, ["foo@chromium.org"])
diff --git a/llvm_tools/llvm_next_test.py b/llvm_tools/llvm_next_test.py
old mode 100755
new mode 100644
index a4e6a395..974e6910
--- a/llvm_tools/llvm_next_test.py
+++ b/llvm_tools/llvm_next_test.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -7,18 +6,12 @@
 
 import unittest
 
-import llvm_next
+from llvm_tools import llvm_next
 
 
 class Test(unittest.TestCase):
     """Tests for llvm_next."""
 
     def test_all_cls_have_patchesets(self):
-        cls = [llvm_next.DISABLE_WERROR_CL]
-        cls += llvm_next.LLVM_NEXT_TESTING_CLS
-        for cl in cls:
+        for cl in llvm_next.LLVM_NEXT_TESTING_CLS:
             self.assertIsNotNone(cl.patch_set, f"CL {cl} needs a patch-set")
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/llvm_project.py b/llvm_tools/llvm_project.py
deleted file mode 100644
index 961074db..00000000
--- a/llvm_tools/llvm_project.py
+++ /dev/null
@@ -1,72 +0,0 @@
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Module for manipulating llvm-project-copy. Generally intended for tests."""
-
-import datetime
-import os
-import subprocess
-import sys
-
-import get_llvm_hash
-import git_llvm_rev
-
-
-def get_location() -> str:
-    """Gets the absolute path for llvm-project-copy."""
-    my_dir = os.path.dirname(os.path.abspath(__file__))
-    return os.path.join(my_dir, "llvm-project-copy")
-
-
-def ensure_up_to_date() -> None:
-    """Ensures that llvm-project-copy is checked out and semi-up-to-date."""
-
-    checkout = get_location()
-    if not os.path.isdir(checkout):
-        print(
-            "No llvm-project exists locally; syncing it. This takes a while.",
-            file=sys.stderr,
-        )
-        actual_checkout = get_llvm_hash.GetAndUpdateLLVMProjectInLLVMTools()
-        assert checkout == actual_checkout, "%s != %s" % (
-            actual_checkout,
-            checkout,
-        )
-
-    commit_timestamp = subprocess.check_output(
-        [
-            "git",
-            "log",
-            "-n1",
-            "--format=%ct",
-            "origin/" + git_llvm_rev.MAIN_BRANCH,
-        ],
-        cwd=checkout,
-        encoding="utf-8",
-    )
-
-    commit_time = datetime.datetime.fromtimestamp(int(commit_timestamp.strip()))
-    now = datetime.datetime.now()
-
-    time_since_last_commit = now - commit_time
-
-    # Arbitrary, but if it's been more than 2d since we've seen a commit, it's
-    # probably best to bring us up-to-date.
-    if time_since_last_commit <= datetime.timedelta(days=2):
-        return
-
-    print(
-        "%d days have elapsed since the last commit to %s; auto-syncing"
-        % (time_since_last_commit.days, checkout),
-        file=sys.stderr,
-    )
-
-    result = subprocess.run(
-        ["git", "fetch", "origin"], check=False, cwd=checkout
-    )
-    if result.returncode:
-        print(
-            "Sync failed somehow; hoping that things are fresh enough, then...",
-            file=sys.stderr,
-        )
diff --git a/llvm_tools/llvm_project_base_commit.py b/llvm_tools/llvm_project_base_commit.py
new file mode 100644
index 00000000..f936ba24
--- /dev/null
+++ b/llvm_tools/llvm_project_base_commit.py
@@ -0,0 +1,165 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Utilities to create the first commit at the base of a new llvm branch.
+
+Used primarily by new branch workflows and patch management tooling.
+"""
+
+from pathlib import Path
+import re
+import shutil
+
+from cros_utils import git_utils
+from llvm_tools import patch_utils
+
+
+# This isn't a dict to prevent adding a tomli-w dependency.
+PRESUBMIT_CFG_CONTENTS = """\
+[Hook Overrides]
+cros_license_check: True
+long_line_check: True
+
+[Hook Overrides Options]
+cros_license_check: --exclude_regex=.*
+long_line_check: --exclude_regex=.*
+"""
+
+CROS_DIR_README = """\
+# CrOS Directory
+
+This directory is used to store arbitrary changes for the ChromeOS Toolchain
+team. Files in this directory are never meant to be upstreamed, and only
+exist for local modification.
+
+See src/third_party/toolchain-utils to see how this directory is configured.
+"""
+
+BASE_COMMIT_MESSAGE = """\
+llvm-project: ChromeOS Base Commit
+
+This is the LLVM ChromeOS Base Commit.
+
+This commit marks the start of the ChromeOS patch branch. It introduces
+the OWNERS file, and sets up the 'cros' directory for future use.
+
+Functional patches for the ChromeOS LLVM Toolchain land after this
+commit. This commit does not change how LLVM operates. The parent
+commit to this change determines the LLVM synthetic revision.
+
+BUG=None
+TEST=CQ
+"""
+
+
+def make_base_commit(
+    toolchain_utils_dir: Path,
+    llvm_src_dir: Path,
+    llvm_svn_revision: int,
+    chromiumos_overlay: Path,
+) -> None:
+    """Create a commit which represents the base of a ChromeOS branch.
+
+    Internally this is just `write_base_changes` followed by `commit_all`.
+    """
+    write_base_changes(
+        toolchain_utils_dir, llvm_src_dir, llvm_svn_revision, chromiumos_overlay
+    )
+    git_utils.commit_all_changes(llvm_src_dir, BASE_COMMIT_MESSAGE)
+
+
+def write_base_changes(
+    toolchain_utils_dir: Path,
+    llvm_src_dir: Path,
+    llvm_svn_revision: int,
+    chromiumos_overlay: Path,
+) -> None:
+    """Make changes which represents the base of a ChromeOS branch."""
+    toolchain_utils_copy_files = (
+        "OWNERS",
+        "OWNERS.toolchain",
+    )
+    for copy_file in toolchain_utils_copy_files:
+        shutil.copy(toolchain_utils_dir / copy_file, llvm_src_dir / copy_file)
+    (llvm_src_dir / "PRESUBMIT.cfg").write_text(PRESUBMIT_CFG_CONTENTS)
+    write_all_gentoo_cmake_hacks(llvm_src_dir, chromiumos_overlay)
+    set_up_cros_dir(llvm_src_dir, llvm_svn_revision)
+
+
+def set_up_cros_dir(llvm_src_dir: Path, llvm_svn_revision: int) -> None:
+    """Create and init the llvm-project/cros directory."""
+    cros_dir = llvm_src_dir / "cros"
+    cros_dir.mkdir()
+    readme = cros_dir / "README.md"
+    readme.write_text(CROS_DIR_README)
+
+    llvm_base_rev = cros_dir / "llvm-rev"
+    llvm_base_rev.write_text(str(llvm_svn_revision))
+
+
+def write_gentoo_cmake_hack(llvm_src_dir: Path, ebuild_dir: Path) -> None:
+    """Modifies cmake files in LLVM so cmake.eclass doesn't modify them.
+
+    Args:
+        llvm_src_dir: Path to llvm-project git root we want to modify.
+        ebuild_dir: Path to sys-devel/llvm Portage package directory.
+    """
+    # Upstream's `cmake.eclass` will try to override "dangerous" configurations
+    # that override Gentoo settings. There's no way to skip this override, but
+    # it _does_ have logic to detect if it has already run & skips all
+    # modifications in that case. Since LLVM has no such "dangerous" settings,
+    # and the `9999` ebuild never "goes live," it's safe to skip these.
+
+    # The file to modify is the 'main' cmake file, which is determined based on
+    # `CMAKE_USE_DIR`. Parsing that out isn't _too_ painful, so try it.
+    ebuild_path = _find_ebuild_in_dir(ebuild_dir)
+    ebuild_contents = ebuild_path.read_text(encoding="utf-8")
+    cmake_use_dir_re = re.compile(
+        # Use string concatenation rather than re.VERBOSE, since this regex
+        # goes in an error message on failure, and that's _really_ hard to
+        # read.
+        r"^\s*"
+        # While these all use `export`, it's not strictly required by
+        # cmake.eclass.
+        r"(?:export\s+)?" r'CMAKE_USE_DIR="\$\{S\}/([^"]+)"',
+        re.MULTILINE,
+    )
+    cmake_use_dirs = cmake_use_dir_re.findall(ebuild_contents)
+    if len(cmake_use_dirs) != 1:
+        raise ValueError(
+            f"Expected to find 1 match of {cmake_use_dir_re} in "
+            f"{ebuild_path}; found {len(cmake_use_dirs)}"
+        )
+
+    cmake_file = llvm_src_dir / cmake_use_dirs[0] / "CMakeLists.txt"
+    special_marker = "<<< Gentoo configuration >>>"
+    if special_marker in cmake_file.read_text(encoding="utf-8"):
+        return
+
+    with cmake_file.open("a", encoding="utf-8") as f:
+        f.write(
+            f"\n# HACK from llvm_project_base_commit.py:\n# {special_marker}"
+        )
+
+
+def write_all_gentoo_cmake_hacks(
+    llvm_src_dir: Path, chromiumos_overlay: Path
+) -> None:
+    """Writes gentoo cmake hacks for all known LLVM 9999 subprojects."""
+    for subproject in patch_utils.CHROMEOS_LLVM_SUBPACKAGES:
+        # N.B., this function will do nothing if the cmake file it aims to
+        # modify has the marker already. Hence, calling it unconditionally on
+        # all projects is fine.
+        write_gentoo_cmake_hack(llvm_src_dir, chromiumos_overlay / subproject)
+
+
+def _find_ebuild_in_dir(ebuild_dir: Path) -> Path:
+    """Returns the path to a 9999 ebuild in `ebuild_dir`; raises if none."""
+    candidates = list(ebuild_dir.glob("*-9999.ebuild"))
+    if len(candidates) != 1:
+        raise ValueError(
+            f"Expected exactly one 9999 ebuild in {ebuild_dir}; found "
+            f"{candidates}"
+        )
+    return candidates[0]
diff --git a/llvm_tools/llvm_simple_bisect.py b/llvm_tools/llvm_simple_bisect.py
old mode 100755
new mode 100644
index 433fec77..9be77252
--- a/llvm_tools/llvm_simple_bisect.py
+++ b/llvm_tools/llvm_simple_bisect.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -25,7 +24,8 @@ import subprocess
 import sys
 from typing import Optional, Text
 
-import chroot
+from cros_utils import cros_paths
+from llvm_tools import chroot
 
 
 # Git Bisection exit codes
@@ -108,7 +108,7 @@ class CommandResult:
 class LLVMRepo:
     """LLVM Repository git and workon information."""
 
-    REPO_PATH = Path("/mnt/host/source/src/third_party/llvm-project")
+    REPO_PATH = cros_paths.CHROOT_SOURCE_ROOT / cros_paths.LLVM_PROJECT
 
     def __init__(self):
         self.workon: Optional[bool] = None
@@ -345,7 +345,3 @@ def main():
     except Exception:
         logging.exception("Uncaught Exception in main")
         abort(opts.never_abort)
-
-
-if __name__ == "__main__":
-    main()
diff --git a/llvm_tools/manifest_utils.py b/llvm_tools/manifest_utils.py
index e53afa6d..0e03d74a 100644
--- a/llvm_tools/manifest_utils.py
+++ b/llvm_tools/manifest_utils.py
@@ -14,10 +14,11 @@ import subprocess
 from typing import List, Optional, Union
 from xml.etree import ElementTree
 
-import atomic_write_file
+from cros_utils import cros_paths
+from llvm_tools import atomic_write_file
 
 
-LLVM_PROJECT_PATH = "src/third_party/llvm-project"
+LLVM_PROJECT_PATH = str(cros_paths.LLVM_PROJECT)
 
 
 class FormattingError(Exception):
@@ -52,7 +53,7 @@ def _find_llvm_project_in_manifest_tree(
     return None
 
 
-def extract_current_llvm_hash(src_tree: Path) -> str:
+def extract_current_llvm_hash_or_ref(src_tree: Path) -> str:
     """Returns the current LLVM SHA for the CrOS tree rooted at `src_tree`.
 
     Raises:
@@ -61,10 +62,12 @@ def extract_current_llvm_hash(src_tree: Path) -> str:
     xmlroot = ElementTree.parse(
         get_chromeos_manifest_path(src_tree), parser=make_xmlparser()
     ).getroot()
-    return extract_current_llvm_hash_from_xml(xmlroot)
+    return extract_current_llvm_hash_or_ref_from_xml(xmlroot)
 
 
-def extract_current_llvm_hash_from_xml(xmlroot: ElementTree.Element) -> str:
+def extract_current_llvm_hash_or_ref_from_xml(
+    xmlroot: ElementTree.Element,
+) -> str:
     """Returns the current LLVM SHA for the parsed XML file.
 
     Raises:
@@ -86,6 +89,35 @@ def extract_current_llvm_hash_from_xml(xmlroot: ElementTree.Element) -> str:
     return revision
 
 
+def update_chromeos_manifest_in_manifest_dir(
+    revision: str, manifest_dir: Path, chromeos_tree: Optional[Path] = None
+) -> Path:
+    """update_chromeos_manifest, taking the directory to manifest-interal.
+
+    Args:
+        revision: Revision of LLVM
+        manifest_dir: The directory of the manifest change
+        chromeos_tree: The ChromeOS tree to run miscellaneous tools (e.g.,
+            `cros` in). Inferred from `manifest_dir` if none is passed.
+    """
+    if not chromeos_tree:
+        # Since this is just 'anywhere in a ChromeOS tree', no need to find the
+        # _actual_ root.
+        chromeos_tree = manifest_dir
+    # Get an absolute path to this here, since `format_manifest` later may
+    # change the cwd, and `manifest_dir` may be relative to cwd.
+    manifest_path = get_chromeos_manifest_path_from_manifest_dir(
+        manifest_dir
+    ).absolute()
+    parser = make_xmlparser()
+    xmltree = ElementTree.parse(manifest_path, parser)
+    update_chromeos_manifest_tree(revision, xmltree.getroot())
+    with atomic_write_file.atomic_write(manifest_path, mode="wb") as f:
+        xmltree.write(f, encoding="utf-8")
+    format_manifest(manifest_path, cwd=chromeos_tree)
+    return manifest_path
+
+
 def update_chromeos_manifest(revision: str, src_tree: Path) -> Path:
     """Replaces the manifest project revision with 'revision'.
 
@@ -107,19 +139,28 @@ def update_chromeos_manifest(revision: str, src_tree: Path) -> Path:
         UpdateManifestError: The manifest could not be changed.
         FormattingError: The manifest could not be reformatted.
     """
-    manifest_path = get_chromeos_manifest_path(src_tree)
-    parser = make_xmlparser()
-    xmltree = ElementTree.parse(manifest_path, parser)
-    update_chromeos_manifest_tree(revision, xmltree.getroot())
-    with atomic_write_file.atomic_write(manifest_path, mode="wb") as f:
-        xmltree.write(f, encoding="utf-8")
-    format_manifest(manifest_path)
-    return manifest_path
+    return update_chromeos_manifest_in_manifest_dir(
+        revision,
+        get_manifest_internal_path(src_tree),
+        chromeos_tree=src_tree,
+    )
+
+
+def get_manifest_internal_path(src_tree: Path) -> Path:
+    """Return the path to manifest-internal inside of the CrOS tree."""
+    return src_tree / "manifest-internal"
 
 
 def get_chromeos_manifest_path(src_tree: Path) -> Path:
     """Return the path to the toolchain manifest."""
-    return src_tree / "manifest-internal" / "_toolchain.xml"
+    return get_chromeos_manifest_path_from_manifest_dir(
+        get_manifest_internal_path(src_tree)
+    )
+
+
+def get_chromeos_manifest_path_from_manifest_dir(manifest_dir: Path) -> Path:
+    """Return the path to the toolchain manifest in a manifest dir."""
+    return manifest_dir / "_toolchain.xml"
 
 
 def update_chromeos_manifest_tree(revision: str, xmlroot: ElementTree.Element):
@@ -131,11 +172,11 @@ def update_chromeos_manifest_tree(revision: str, xmlroot: ElementTree.Element):
     llvm_project_elem.attrib["revision"] = revision
 
 
-def format_manifest(repo_manifest: Path):
+def format_manifest(repo_manifest: Path, cwd: Optional[Path] = None):
     """Use cros format to format the given manifest."""
     if not shutil.which("cros"):
         raise FormattingError(
             "unable to format manifest, 'cros'" " executable not in PATH"
         )
     cmd: List[Union[str, Path]] = ["cros", "format", repo_manifest]
-    subprocess.run(cmd, check=True)
+    subprocess.run(cmd, check=True, cwd=cwd)
diff --git a/llvm_tools/manifest_utils_unittest.py b/llvm_tools/manifest_utils_test.py
old mode 100755
new mode 100644
similarity index 94%
rename from llvm_tools/manifest_utils_unittest.py
rename to llvm_tools/manifest_utils_test.py
index 28179413..fc281123
--- a/llvm_tools/manifest_utils_unittest.py
+++ b/llvm_tools/manifest_utils_test.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2023 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -12,7 +11,7 @@ on toolchain projects (llvm-project, etc.) which are public.
 import unittest
 from xml.etree import ElementTree
 
-import manifest_utils
+from llvm_tools import manifest_utils
 
 
 MANIFEST_FIXTURE = """<?xml version="1.0" encoding="UTF-8"?>
@@ -83,10 +82,6 @@ class TestManifestUtils(unittest.TestCase):
             parser=manifest_utils.make_xmlparser(),
         )
         self.assertEqual(
-            manifest_utils.extract_current_llvm_hash_from_xml(root),
+            manifest_utils.extract_current_llvm_hash_or_ref_from_xml(root),
             "abcd",
         )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/modify_a_tryjob.py b/llvm_tools/modify_a_tryjob.py
deleted file mode 100755
index 40024a96..00000000
--- a/llvm_tools/modify_a_tryjob.py
+++ /dev/null
@@ -1,372 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Modifies a tryjob based off of arguments."""
-
-import argparse
-import enum
-import json
-import os
-from pathlib import Path
-import sys
-from typing import Dict, Iterable, List, Union
-
-import chroot
-import failure_modes
-import get_llvm_hash
-import git
-import update_chromeos_llvm_hash
-import update_packages_and_run_tests
-import update_tryjob_status
-
-
-class ModifyTryjob(enum.Enum):
-    """Options to modify a tryjob."""
-
-    REMOVE = "remove"
-    RELAUNCH = "relaunch"
-    ADD = "add"
-
-
-def GetCommandLineArgs() -> argparse.Namespace:
-    """Parses the command line for the command line arguments."""
-
-    # Default path to the chroot if a path is not specified.
-    cros_root = os.path.expanduser("~")
-    cros_root = os.path.join(cros_root, "chromiumos")
-
-    # Create parser and add optional command-line arguments.
-    parser = argparse.ArgumentParser(
-        description="Removes, relaunches, or adds a tryjob."
-    )
-
-    # Add argument for the JSON file to use for the update of a tryjob.
-    parser.add_argument(
-        "--status_file",
-        required=True,
-        help="The absolute path to the JSON file that contains the tryjobs "
-        "used for bisecting LLVM.",
-    )
-
-    # Add argument that determines what action to take on the revision
-    # specified.
-    parser.add_argument(
-        "--modify_tryjob",
-        required=True,
-        choices=[modify_tryjob.value for modify_tryjob in ModifyTryjob],
-        help="What action to perform on the tryjob.",
-    )
-
-    # Add argument that determines which revision to search for in the list of
-    # tryjobs.
-    parser.add_argument(
-        "--revision",
-        required=True,
-        type=int,
-        help="The revision to either remove or relaunch.",
-    )
-
-    # Add argument for other change lists that want to run alongside the
-    # tryjob.
-    parser.add_argument(
-        "--extra_change_lists",
-        type=int,
-        nargs="+",
-        help="change lists that would like to be run alongside the change list "
-        "of updating the packages",
-    )
-
-    # Add argument for custom options for the tryjob.
-    parser.add_argument(
-        "--options",
-        required=False,
-        nargs="+",
-        help="options to use for the tryjob testing",
-    )
-
-    # Add argument for the builder to use for the tryjob.
-    parser.add_argument(
-        "--builder", help="builder to use for the tryjob testing"
-    )
-
-    # Add argument for a specific chroot path.
-    parser.add_argument(
-        "--chromeos_path",
-        default=cros_root,
-        help="the path to the chroot (default: %(default)s)",
-    )
-
-    args_output = parser.parse_args()
-
-    if not os.path.isfile(
-        args_output.status_file
-    ) or not args_output.status_file.endswith(".json"):
-        raise ValueError(
-            'File does not exist or does not ending in ".json" '
-            ": %s" % args_output.status_file
-        )
-
-    if (
-        args_output.modify_tryjob == ModifyTryjob.ADD.value
-        and not args_output.builder
-    ):
-        raise ValueError("A builder is required for adding a tryjob.")
-    elif (
-        args_output.modify_tryjob != ModifyTryjob.ADD.value
-        and args_output.builder
-    ):
-        raise ValueError(
-            "Specifying a builder is only available when adding a " "tryjob."
-        )
-
-    return args_output
-
-
-def GetCLAfterUpdatingPackages(
-    packages: Iterable[str],
-    git_hash: str,
-    svn_version: int,
-    chromeos_path: Union[Path, str],
-    svn_option: Union[int, str],
-) -> git.CommitContents:
-    """Updates the packages' LLVM_NEXT."""
-
-    change_list = update_chromeos_llvm_hash.UpdatePackages(
-        packages=packages,
-        manifest_packages=[],
-        llvm_variant=update_chromeos_llvm_hash.LLVMVariant.next,
-        git_hash=git_hash,
-        svn_version=svn_version,
-        chroot_opts=update_chromeos_llvm_hash.ChrootOpts(Path(chromeos_path)),
-        mode=failure_modes.FailureModes.DISABLE_PATCHES,
-        git_hash_source=svn_option,
-        extra_commit_msg_lines=None,
-    )
-
-    # We are calling UpdatePackages with upload_changes=True, in
-    # which case it should always return a git.CommitContents value.
-    assert change_list is not None
-    print("\nSuccessfully updated packages to %d" % svn_version)
-    print("Gerrit URL: %s" % change_list.url)
-    print("Change list number: %d" % change_list.cl_number)
-
-    return change_list
-
-
-def CreateNewTryjobEntryForBisection(
-    cl: int,
-    extra_cls: List[int],
-    options: List[str],
-    builder: str,
-    chromeos_path: Union[Path, str],
-    cl_url: str,
-    revision,
-) -> Dict:
-    """Submits a tryjob and adds additional information."""
-
-    # Get the tryjob results after submitting the tryjob.
-    # Format of 'tryjob_results':
-    # [
-    #   {
-    #     'link' : [TRYJOB_LINK],
-    #     'buildbucket_id' : [BUILDBUCKET_ID],
-    #     'extra_cls' : [EXTRA_CLS_LIST],
-    #     'options' : [EXTRA_OPTIONS_LIST],
-    #     'builder' : [BUILDER_AS_A_LIST]
-    #   }
-    # ]
-    tryjob_results = update_packages_and_run_tests.RunTryJobs(
-        cl, extra_cls, options, [builder], chromeos_path
-    )
-    print("\nTryjob:")
-    print(tryjob_results[0])
-
-    # Add necessary information about the tryjob.
-    tryjob_results[0]["url"] = cl_url
-    tryjob_results[0]["rev"] = revision
-    tryjob_results[0][
-        "status"
-    ] = update_tryjob_status.TryjobStatus.PENDING.value
-    tryjob_results[0]["cl"] = cl
-
-    return tryjob_results[0]
-
-
-def AddTryjob(
-    packages: Iterable[str],
-    git_hash: str,
-    revision: int,
-    chromeos_path: Union[Path, str],
-    extra_cls: List[int],
-    options: List[str],
-    builder: str,
-    svn_option: Union[int, str],
-):
-    """Submits a tryjob."""
-
-    change_list = GetCLAfterUpdatingPackages(
-        packages,
-        git_hash,
-        revision,
-        chromeos_path,
-        svn_option,
-    )
-
-    tryjob_dict = CreateNewTryjobEntryForBisection(
-        change_list.cl_number,
-        extra_cls,
-        options,
-        builder,
-        chromeos_path,
-        change_list.url,
-        revision,
-    )
-
-    return tryjob_dict
-
-
-def PerformTryjobModification(
-    revision: int,
-    modify_tryjob: ModifyTryjob,
-    status_file: Union[Path, str],
-    extra_cls: List[int],
-    options: List[str],
-    builder: str,
-    chromeos_path: Union[Path, str],
-) -> None:
-    """Removes, relaunches, or adds a tryjob.
-
-    Args:
-        revision: The revision associated with the tryjob.
-        modify_tryjob: What action to take on the tryjob.
-          Ex: ModifyTryjob.REMOVE, ModifyTryjob.RELAUNCH, ModifyTryjob.ADD
-        status_file: The .JSON file that contains the tryjobs.
-        extra_cls: Extra change lists to be run alongside tryjob
-        options: Extra options to pass into 'cros tryjob'.
-        builder: The builder to use for 'cros tryjob'.
-        chromeos_path: The absolute path to the chromeos checkout.
-    """
-
-    # Format of 'bisect_contents':
-    # {
-    #   'start': [START_REVISION_OF_BISECTION]
-    #   'end': [END_REVISION_OF_BISECTION]
-    #   'jobs' : [
-    #       {[TRYJOB_INFORMATION]},
-    #       {[TRYJOB_INFORMATION]},
-    #       ...,
-    #       {[TRYJOB_INFORMATION]}
-    #   ]
-    # }
-    with open(status_file, encoding="utf-8") as tryjobs:
-        bisect_contents = json.load(tryjobs)
-
-    if not bisect_contents["jobs"] and modify_tryjob != ModifyTryjob.ADD:
-        sys.exit("No tryjobs in %s" % status_file)
-
-    tryjob_index = update_tryjob_status.FindTryjobIndex(
-        revision, bisect_contents["jobs"]
-    )
-
-    # 'FindTryjobIndex()' returns None if the tryjob was not found.
-    if tryjob_index is None and modify_tryjob != ModifyTryjob.ADD:
-        raise ValueError(
-            "Unable to find tryjob for %d in %s" % (revision, status_file)
-        )
-
-    # Determine the action to take based off of 'modify_tryjob'.
-    if modify_tryjob == ModifyTryjob.REMOVE:
-        del bisect_contents["jobs"][tryjob_index]
-
-        print("Successfully deleted the tryjob of revision %d" % revision)
-    elif modify_tryjob == ModifyTryjob.RELAUNCH:
-        # Need to update the tryjob link and buildbucket ID.
-        tryjob_results = update_packages_and_run_tests.RunTryJobs(
-            bisect_contents["jobs"][tryjob_index]["cl"],
-            bisect_contents["jobs"][tryjob_index]["extra_cls"],
-            bisect_contents["jobs"][tryjob_index]["options"],
-            bisect_contents["jobs"][tryjob_index]["builder"],
-            chromeos_path,
-        )
-
-        bisect_contents["jobs"][tryjob_index][
-            "status"
-        ] = update_tryjob_status.TryjobStatus.PENDING.value
-        bisect_contents["jobs"][tryjob_index]["link"] = tryjob_results[0][
-            "link"
-        ]
-        bisect_contents["jobs"][tryjob_index][
-            "buildbucket_id"
-        ] = tryjob_results[0]["buildbucket_id"]
-
-        print(
-            "Successfully relaunched the tryjob for revision %d and updated "
-            "the tryjob link to %s" % (revision, tryjob_results[0]["link"])
-        )
-    elif modify_tryjob == ModifyTryjob.ADD:
-        # Tryjob exists already.
-        if tryjob_index is not None:
-            raise ValueError(
-                "Tryjob already exists (index is %d) in %s."
-                % (tryjob_index, status_file)
-            )
-
-        # Make sure the revision is within the bounds of the start and end of
-        # the bisection.
-        elif bisect_contents["start"] < revision < bisect_contents["end"]:
-            (
-                git_hash,
-                revision,
-            ) = get_llvm_hash.GetLLVMHashAndVersionFromSVNOption(revision)
-
-            tryjob_dict = AddTryjob(
-                update_chromeos_llvm_hash.DEFAULT_PACKAGES,
-                git_hash,
-                revision,
-                chromeos_path,
-                extra_cls,
-                options,
-                builder,
-                revision,
-            )
-
-            bisect_contents["jobs"].append(tryjob_dict)
-
-            print("Successfully added tryjob of revision %d" % revision)
-        else:
-            raise ValueError("Failed to add tryjob to %s" % status_file)
-    else:
-        raise ValueError(
-            'Invalid "modify_tryjob" option provided: %s' % modify_tryjob
-        )
-
-    with open(status_file, "w", encoding="utf-8") as update_tryjobs:
-        json.dump(
-            bisect_contents, update_tryjobs, indent=4, separators=(",", ": ")
-        )
-
-
-def main() -> None:
-    """Removes, relaunches, or adds a tryjob."""
-
-    chroot.VerifyOutsideChroot()
-
-    args_output = GetCommandLineArgs()
-
-    chroot.VerifyChromeOSRoot(args_output.chromeos_path)
-
-    PerformTryjobModification(
-        args_output.revision,
-        ModifyTryjob(args_output.modify_tryjob),
-        args_output.status_file,
-        args_output.extra_change_lists,
-        args_output.options,
-        args_output.builder,
-        args_output.chromeos_path,
-    )
-
-
-if __name__ == "__main__":
-    main()
diff --git a/llvm_tools/modify_a_tryjob_unittest.py b/llvm_tools/modify_a_tryjob_unittest.py
deleted file mode 100755
index 0f693dc3..00000000
--- a/llvm_tools/modify_a_tryjob_unittest.py
+++ /dev/null
@@ -1,444 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Tests for modifying a tryjob."""
-
-import json
-import unittest
-from unittest import mock
-
-import get_llvm_hash
-import modify_a_tryjob
-import test_helpers
-import update_packages_and_run_tests
-import update_tryjob_status
-
-
-class ModifyATryjobTest(unittest.TestCase):
-    """Unittests for modifying a tryjob."""
-
-    def testNoTryjobsInStatusFile(self):
-        bisect_test_contents = {"start": 369410, "end": 369420, "jobs": []}
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            revision_to_modify = 369411
-
-            args_output = test_helpers.ArgsOutputTest()
-            args_output.builders = None
-            args_output.options = None
-
-            # Verify the exception is raised there are no tryjobs in the status
-            # file and the mode is not to 'add' a tryjob.
-            with self.assertRaises(SystemExit) as err:
-                modify_a_tryjob.PerformTryjobModification(
-                    revision_to_modify,
-                    modify_a_tryjob.ModifyTryjob.REMOVE,
-                    temp_json_file,
-                    args_output.extra_change_lists,
-                    args_output.options,
-                    args_output.builders,
-                    args_output.chromeos_path,
-                )
-
-            self.assertEqual(
-                str(err.exception), "No tryjobs in %s" % temp_json_file
-            )
-
-    # Simulate the behavior of `FindTryjobIndex()` when the index of the tryjob
-    # was not found.
-    @mock.patch.object(
-        update_tryjob_status, "FindTryjobIndex", return_value=None
-    )
-    def testNoTryjobIndexFound(self, mock_find_tryjob_index):
-        bisect_test_contents = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [
-                {"rev": 369411, "status": "pending", "buildbucket_id": 1200}
-            ],
-        }
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            revision_to_modify = 369412
-
-            args_output = test_helpers.ArgsOutputTest()
-            args_output.builders = None
-            args_output.options = None
-
-            # Verify the exception is raised when the index of the tryjob was
-            # not found in the status file and the mode is not to 'add' a
-            # tryjob.
-            with self.assertRaises(ValueError) as err:
-                modify_a_tryjob.PerformTryjobModification(
-                    revision_to_modify,
-                    modify_a_tryjob.ModifyTryjob.REMOVE,
-                    temp_json_file,
-                    args_output.extra_change_lists,
-                    args_output.options,
-                    args_output.builders,
-                    args_output.chromeos_path,
-                )
-
-            self.assertEqual(
-                str(err.exception),
-                "Unable to find tryjob for %d in %s"
-                % (revision_to_modify, temp_json_file),
-            )
-
-        mock_find_tryjob_index.assert_called_once()
-
-    # Simulate the behavior of `FindTryjobIndex()` when the index of the tryjob
-    # was found.
-    @mock.patch.object(update_tryjob_status, "FindTryjobIndex", return_value=0)
-    def testSuccessfullyRemovedTryjobInStatusFile(self, mock_find_tryjob_index):
-        bisect_test_contents = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [
-                {"rev": 369414, "status": "pending", "buildbucket_id": 1200}
-            ],
-        }
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            revision_to_modify = 369414
-
-            args_output = test_helpers.ArgsOutputTest()
-            args_output.builders = None
-            args_output.options = None
-
-            modify_a_tryjob.PerformTryjobModification(
-                revision_to_modify,
-                modify_a_tryjob.ModifyTryjob.REMOVE,
-                temp_json_file,
-                args_output.extra_change_lists,
-                args_output.options,
-                args_output.builders,
-                args_output.chromeos_path,
-            )
-
-            # Verify that the tryjob was removed from the status file.
-            with open(temp_json_file, encoding="utf-8") as status_file:
-                bisect_contents = json.load(status_file)
-
-                expected_file_contents = {
-                    "start": 369410,
-                    "end": 369420,
-                    "jobs": [],
-                }
-
-                self.assertDictEqual(bisect_contents, expected_file_contents)
-
-        mock_find_tryjob_index.assert_called_once()
-
-    # Simulate the behavior of `RunTryJobs()` when successfully submitted a
-    # tryjob.
-    @mock.patch.object(update_packages_and_run_tests, "RunTryJobs")
-    # Simulate the behavior of `FindTryjobIndex()` when the index of the tryjob
-    # was found.
-    @mock.patch.object(update_tryjob_status, "FindTryjobIndex", return_value=0)
-    def testSuccessfullyRelaunchedTryjob(
-        self, mock_find_tryjob_index, mock_run_tryjob
-    ):
-        bisect_test_contents = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [
-                {
-                    "rev": 369411,
-                    "status": "bad",
-                    "link": "https://some_tryjob_link.com",
-                    "buildbucket_id": 1200,
-                    "cl": 123,
-                    "extra_cls": None,
-                    "options": None,
-                    "builder": ["some-builder-tryjob"],
-                }
-            ],
-        }
-
-        tryjob_result = [
-            {"link": "https://some_new_tryjob_link.com", "buildbucket_id": 20}
-        ]
-
-        mock_run_tryjob.return_value = tryjob_result
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            revision_to_modify = 369411
-
-            args_output = test_helpers.ArgsOutputTest()
-            args_output.builders = None
-            args_output.options = None
-
-            modify_a_tryjob.PerformTryjobModification(
-                revision_to_modify,
-                modify_a_tryjob.ModifyTryjob.RELAUNCH,
-                temp_json_file,
-                args_output.extra_change_lists,
-                args_output.options,
-                args_output.builders,
-                args_output.chromeos_path,
-            )
-
-            # Verify that the tryjob's information was updated after submtting
-            # the tryjob.
-            with open(temp_json_file, encoding="utf-8") as status_file:
-                bisect_contents = json.load(status_file)
-
-                expected_file_contents = {
-                    "start": 369410,
-                    "end": 369420,
-                    "jobs": [
-                        {
-                            "rev": 369411,
-                            "status": "pending",
-                            "link": "https://some_new_tryjob_link.com",
-                            "buildbucket_id": 20,
-                            "cl": 123,
-                            "extra_cls": None,
-                            "options": None,
-                            "builder": ["some-builder-tryjob"],
-                        }
-                    ],
-                }
-
-                self.assertDictEqual(bisect_contents, expected_file_contents)
-
-        mock_find_tryjob_index.assert_called_once()
-
-        mock_run_tryjob.assert_called_once()
-
-    # Simulate the behavior of `FindTryjobIndex()` when the index of the tryjob
-    # was found.
-    @mock.patch.object(update_tryjob_status, "FindTryjobIndex", return_value=0)
-    def testAddingTryjobThatAlreadyExists(self, mock_find_tryjob_index):
-        bisect_test_contents = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [
-                {"rev": 369411, "status": "bad", "builder": ["some-builder"]}
-            ],
-        }
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            revision_to_add = 369411
-
-            # Index of the tryjob in 'jobs' list.
-            tryjob_index = 0
-
-            args_output = test_helpers.ArgsOutputTest()
-            args_output.options = None
-
-            # Verify the exception is raised when the tryjob that is going to
-            # added already exists in the status file (found its index).
-            with self.assertRaises(ValueError) as err:
-                modify_a_tryjob.PerformTryjobModification(
-                    revision_to_add,
-                    modify_a_tryjob.ModifyTryjob.ADD,
-                    temp_json_file,
-                    args_output.extra_change_lists,
-                    args_output.options,
-                    args_output.builders,
-                    args_output.chromeos_path,
-                )
-
-            self.assertEqual(
-                str(err.exception),
-                "Tryjob already exists (index is %d) in %s."
-                % (tryjob_index, temp_json_file),
-            )
-
-        mock_find_tryjob_index.assert_called_once()
-
-    # Simulate the behavior of `FindTryjobIndex()` when the tryjob was not
-    # found.
-    @mock.patch.object(
-        update_tryjob_status, "FindTryjobIndex", return_value=None
-    )
-    def testSuccessfullyDidNotAddTryjobOutsideOfBisectionBounds(
-        self, mock_find_tryjob_index
-    ):
-        bisect_test_contents = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [{"rev": 369411, "status": "bad"}],
-        }
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            # Add a revision that is outside of 'start' and 'end'.
-            revision_to_add = 369450
-
-            args_output = test_helpers.ArgsOutputTest()
-            args_output.options = None
-
-            # Verify the exception is raised when adding a tryjob that does not
-            # exist and is not within 'start' and 'end'.
-            with self.assertRaises(ValueError) as err:
-                modify_a_tryjob.PerformTryjobModification(
-                    revision_to_add,
-                    modify_a_tryjob.ModifyTryjob.ADD,
-                    temp_json_file,
-                    args_output.extra_change_lists,
-                    args_output.options,
-                    args_output.builders,
-                    args_output.chromeos_path,
-                )
-
-            self.assertEqual(
-                str(err.exception),
-                "Failed to add tryjob to %s" % temp_json_file,
-            )
-
-        mock_find_tryjob_index.assert_called_once()
-
-    # Simulate the behavior of `AddTryjob()` when successfully submitted the
-    # tryjob and constructed the tryjob information (a dictionary).
-    @mock.patch.object(modify_a_tryjob, "AddTryjob")
-    # Simulate the behavior of `GetLLVMHashAndVersionFromSVNOption()` when
-    # successfully retrieved the git hash of the revision to launch a tryjob
-    # for.
-    @mock.patch.object(
-        get_llvm_hash,
-        "GetLLVMHashAndVersionFromSVNOption",
-        return_value=("a123testhash1", 369418),
-    )
-    # Simulate the behavior of `FindTryjobIndex()` when the tryjob was not
-    # found.
-    @mock.patch.object(
-        update_tryjob_status, "FindTryjobIndex", return_value=None
-    )
-    def testSuccessfullyAddedTryjob(
-        self, mock_find_tryjob_index, mock_get_llvm_hash, mock_add_tryjob
-    ):
-        bisect_test_contents = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [{"rev": 369411, "status": "bad"}],
-        }
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            # Add a revision that is outside of 'start' and 'end'.
-            revision_to_add = 369418
-
-            args_output = test_helpers.ArgsOutputTest()
-            args_output.options = None
-
-            new_tryjob_info = {
-                "rev": revision_to_add,
-                "status": "pending",
-                "options": args_output.options,
-                "extra_cls": args_output.extra_change_lists,
-                "builder": args_output.builders,
-            }
-
-            mock_add_tryjob.return_value = new_tryjob_info
-
-            modify_a_tryjob.PerformTryjobModification(
-                revision_to_add,
-                modify_a_tryjob.ModifyTryjob.ADD,
-                temp_json_file,
-                args_output.extra_change_lists,
-                args_output.options,
-                args_output.builders,
-                args_output.chromeos_path,
-            )
-
-            # Verify that the tryjob was added to the status file.
-            with open(temp_json_file, encoding="utf-8") as status_file:
-                bisect_contents = json.load(status_file)
-
-                expected_file_contents = {
-                    "start": 369410,
-                    "end": 369420,
-                    "jobs": [{"rev": 369411, "status": "bad"}, new_tryjob_info],
-                }
-
-                self.assertDictEqual(bisect_contents, expected_file_contents)
-
-        mock_find_tryjob_index.assert_called_once()
-
-        mock_get_llvm_hash.assert_called_once_with(revision_to_add)
-
-        mock_add_tryjob.assert_called_once()
-
-    # Simulate the behavior of `FindTryjobIndex()` when the tryjob was found.
-    @mock.patch.object(update_tryjob_status, "FindTryjobIndex", return_value=0)
-    def testModifyATryjobOptionDoesNotExist(self, mock_find_tryjob_index):
-        bisect_test_contents = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [{"rev": 369414, "status": "bad"}],
-        }
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            # Add a revision that is outside of 'start' and 'end'.
-            revision_to_modify = 369414
-
-            args_output = test_helpers.ArgsOutputTest()
-            args_output.builders = None
-            args_output.options = None
-
-            # Verify the exception is raised when the modify a tryjob option
-            # does not exist.
-            with self.assertRaises(ValueError) as err:
-                modify_a_tryjob.PerformTryjobModification(
-                    revision_to_modify,
-                    "remove_link",
-                    temp_json_file,
-                    args_output.extra_change_lists,
-                    args_output.options,
-                    args_output.builders,
-                    args_output.chromeos_path,
-                )
-
-            self.assertEqual(
-                str(err.exception),
-                'Invalid "modify_tryjob" option provided: remove_link',
-            )
-
-            mock_find_tryjob_index.assert_called_once()
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/nightly_revert_checker.py b/llvm_tools/nightly_revert_checker.py
old mode 100755
new mode 100644
index 6090e614..55847824
--- a/llvm_tools/nightly_revert_checker.py
+++ b/llvm_tools/nightly_revert_checker.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2020 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -16,17 +15,18 @@ import logging
 import os
 from pathlib import Path
 import pprint
+import re
 import subprocess
-import sys
 import time
-from typing import Any, Callable, Dict, List, NamedTuple, Set, Tuple
+from typing import Any, Callable, Dict, Iterable, List, NamedTuple, Tuple
 
 from cros_utils import email_sender
+from cros_utils import git_utils
 from cros_utils import tiny_render
-import get_llvm_hash
-import get_upstream_patch
-import git_llvm_rev
-import revert_checker
+from llvm_tools import get_llvm_hash
+from llvm_tools import git_llvm_rev
+from llvm_tools import patch_utils
+from llvm_tools import revert_checker
 
 
 ONE_DAY_SECS = 24 * 60 * 60
@@ -93,52 +93,30 @@ class State:
 def _find_interesting_android_shas(
     android_llvm_toolchain_dir: str,
 ) -> List[Tuple[str, str]]:
-    llvm_project = os.path.join(
-        android_llvm_toolchain_dir, "toolchain/llvm-project"
+    llvm_project = Path(android_llvm_toolchain_dir) / "toolchain/llvm-project"
+    aosp_main_sha = git_utils.resolve_ref(llvm_project, "aosp/main")
+    merge_base = subprocess.check_output(
+        ["git", "merge-base", aosp_main_sha, "aosp/upstream-main"],
+        cwd=llvm_project,
+        encoding="utf-8",
+    ).strip()
+    logging.info(
+        "Merge-base for aosp/main (HEAD == %s) and aosp/upstream-main is %s",
+        aosp_main_sha,
+        merge_base,
     )
 
-    def get_llvm_merge_base(branch: str) -> str:
-        head_sha = subprocess.check_output(
-            ["git", "rev-parse", branch],
-            cwd=llvm_project,
-            encoding="utf-8",
-        ).strip()
-        merge_base = subprocess.check_output(
-            ["git", "merge-base", branch, "aosp/upstream-main"],
-            cwd=llvm_project,
-            encoding="utf-8",
-        ).strip()
-        logging.info(
-            "Merge-base for %s (HEAD == %s) and upstream-main is %s",
-            branch,
-            head_sha,
-            merge_base,
-        )
-        return merge_base
-
-    main_legacy = get_llvm_merge_base("aosp/master-legacy")  # nocheck
-    testing_upstream = get_llvm_merge_base("aosp/testing-upstream")
-    result: List[Tuple[str, str]] = [("main-legacy", main_legacy)]
-
-    # If these are the same SHA, there's no point in tracking both.
-    if main_legacy != testing_upstream:
-        result.append(("testing-upstream", testing_upstream))
-    else:
-        logging.info(
-            "main-legacy and testing-upstream are identical; ignoring "
-            "the latter."
-        )
-    return result
+    # Android no longer has a testing branch, so just follow main.
+    return [("aosp/main", merge_base)]
 
 
 def _find_interesting_chromeos_shas(
-    chromeos_base: str,
+    chromeos_path: Path,
 ) -> List[Tuple[str, str]]:
-    chromeos_path = Path(chromeos_base)
     llvm_hash = get_llvm_hash.LLVMHash()
 
     current_llvm = llvm_hash.GetCrOSCurrentLLVMHash(chromeos_path)
-    results = [("llvm", current_llvm)]
+    results: List[Tuple[str, str]] = [("llvm", current_llvm)]
     next_llvm = llvm_hash.GetCrOSLLVMNextHash()
     if current_llvm != next_llvm:
         results.append(("llvm-next", next_llvm))
@@ -265,7 +243,8 @@ class NewRevertInfo:
 
 
 def locate_new_reverts_across_shas(
-    llvm_dir: str,
+    llvm_config: git_llvm_rev.LLVMConfig,
+    upstream_main_branch: str,
     interesting_shas: List[Tuple[str, str]],
     state: State,
 ) -> Tuple[State, List[NewRevertInfo]]:
@@ -275,7 +254,9 @@ def locate_new_reverts_across_shas(
     for friendly_name, sha in interesting_shas:
         logging.info("Finding reverts across %s (%s)", friendly_name, sha)
         all_reverts = revert_checker.find_reverts(
-            llvm_dir, sha, root="origin/" + git_llvm_rev.MAIN_BRANCH
+            str(llvm_config.dir),
+            sha,
+            root=f"{llvm_config.remote}/{upstream_main_branch}",
         )
         logging.info(
             "Detected the following revert(s) across %s:\n%s",
@@ -321,9 +302,22 @@ def locate_new_reverts_across_shas(
     return new_state, revert_infos
 
 
+def detect_latest_cros_llvm_branch(
+    chromeos_path: Path, llvm_config: git_llvm_rev.LLVMConfig, sha: str
+) -> str:
+    rev = git_llvm_rev.translate_sha_to_rev(llvm_config, sha).number
+    result = get_llvm_hash.DetectLatestLLVMBranch(chromeos_path, rev)
+    if not result:
+        raise ValueError(
+            f"No branches in {llvm_config.dir} found for LLVM revision {rev}?"
+        )
+    return result
+
+
 def do_cherrypick(
-    chroot_path: str,
-    llvm_dir: str,
+    chromeos_path: Path,
+    llvm_config: git_llvm_rev.LLVMConfig,
+    upstream_main_branch: str,
     repository: str,
     interesting_shas: List[Tuple[str, str]],
     state: State,
@@ -331,36 +325,50 @@ def do_cherrypick(
     cc: List[str],
 ) -> State:
     def prettify_sha(sha: str) -> tiny_render.Piece:
-        rev = get_llvm_hash.GetVersionFrom(llvm_dir, sha)
+        rev = get_llvm_hash.GetVersionFrom(llvm_config.dir, sha)
         return prettify_sha_for_email(sha, rev)
 
     new_state = State()
-    seen: Set[str] = set()
-
-    new_state, new_reverts = locate_new_reverts_across_shas(
-        llvm_dir, interesting_shas, state
+    new_state, new_revert_infos = locate_new_reverts_across_shas(
+        llvm_config, upstream_main_branch, interesting_shas, state
     )
+    llvm_config_dir = Path(llvm_config.dir)
 
-    for revert_info in new_reverts:
-        if revert_info.friendly_name in seen:
-            continue
-        seen.add(revert_info.friendly_name)
-        for sha, reverted_sha in revert_info.new_reverts:
-            try:
-                # We upload reverts for all platforms by default, since there's
-                # no real reason for them to be CrOS-specific.
-                get_upstream_patch.get_from_upstream(
-                    chroot_path=chroot_path,
-                    create_cl=True,
-                    start_sha=reverted_sha,
-                    patches=[sha],
+    for revert_info in new_revert_infos:
+        logging.info(
+            "Applying new reverts across %s...", revert_info.friendly_name
+        )
+        branch = detect_latest_cros_llvm_branch(
+            chromeos_path, llvm_config, revert_info.sha
+        )
+        # The branch will come in the form 'cros/chromeos/llvm-r${N}-${M}`.
+        # `cros` is the remote
+        assert branch.startswith(
+            "cros/"
+        ), f"Expected {branch} to start with 'cros/'"
+        branch_without_remote = branch[len("cros/") :]
+        branch_head = git_utils.resolve_ref(llvm_config_dir, branch)
+        with git_utils.create_worktree(
+            llvm_config_dir, commitish=branch_head
+        ) as worktree:
+            # Note that it's possible that we see the same SHA in multiple
+            # iterations of this loop. Since we're committing to separate
+            # branches, we need to upload separate patches.
+            for sha, reverted_sha in revert_info.new_reverts:
+                # Always `checkout` the branch's original HEAD, so we don't
+                # create a patch stack on Gerrit. Often the reviewer will want
+                # to keep/drop certain patches; stacking them adds complexity
+                # for questionable benefit.
+                git_utils.discard_changes_and_checkout(worktree, branch_head)
+                _upload_revert_cherry_pick(
+                    sha=sha,
+                    branch_without_remote=branch_without_remote,
+                    reverted_sha=reverted_sha,
+                    llvm_config=llvm_config,
+                    llvm_worktree=worktree,
                     reviewers=reviewers,
                     cc=cc,
-                    platforms=(),
                 )
-            except get_upstream_patch.CherrypickError as e:
-                logging.info("%s, skipping...", str(e))
-
     maybe_email_about_stale_heads(
         new_state,
         repository,
@@ -374,6 +382,140 @@ def do_cherrypick(
     return new_state
 
 
+def _append_footers_to_commit_message(
+    message: str, footers: Iterable[str]
+) -> str:
+    lines = message.rstrip().splitlines()
+    footer_key_re = re.compile(r"^\S+:")
+
+    footer_block = []
+    nonfooter_block = lines
+
+    # Parse out existing footers. Footers may/may not exist in a previous
+    # commit. If they do, they all exist in the last paragraph of a commit
+    # message, and they all match `footer_key_re`.
+    for i, line in reversed(list(enumerate(lines))):
+        if not line:
+            nonfooter_block = lines[:i]
+            footer_block = lines[i + 1 :]
+            break
+
+        # If this line isn't a valid footer line, the paragraph we're in isn't
+        # a series of footers.
+        if not footer_key_re.search(line):
+            break
+
+    footer_block += footers
+    # Add `[""]` to ensure that there's a line separating the nonfooters from
+    # the paragraph of footers.
+    return "\n".join(nonfooter_block + [""] + footer_block)
+
+
+def _upload_revert_cherry_pick(
+    sha: str,
+    branch_without_remote: str,
+    reverted_sha: str,
+    llvm_config: git_llvm_rev.LLVMConfig,
+    llvm_worktree: Path,
+    reviewers: List[str],
+    cc: List[str],
+):
+    """Mockable helper to create and upload patches."""
+    cherry_pick_returncode = subprocess.run(
+        ["git", "cherry-pick", sha],
+        check=False,
+        cwd=llvm_worktree,
+        stdin=subprocess.DEVNULL,
+    ).returncode
+    # If a cherry-pick fails, it could be for one of two reasons:
+    #   1. It's empty.
+    #   2. It's a merge conflict.
+    # In the former case, the cherry-pick is a nop that should be ignored,
+    # since we've already picked it (or equivalent) somehow. In the latter, we
+    # still want to bring it to the mage's attention, so upload it with the
+    # merge-conflict markers baked in. If the mage cares, they can fix it up
+    # and land it.
+    if cherry_pick_returncode:
+        if not git_utils.has_discardable_changes(llvm_worktree):
+            logging.warning(
+                "Cherry-pick of SHA %s would be empty; skipping upload", sha
+            )
+            return
+
+        logging.error(
+            "Cherry-pick failed. Still uploading, but with highlights."
+        )
+        subprocess.run(
+            ["git", "add", "."],
+            check=True,
+            cwd=llvm_worktree,
+            stdin=subprocess.DEVNULL,
+        )
+        subprocess.run(
+            ["git", "cherry-pick", "--continue"],
+            check=True,
+            cwd=llvm_worktree,
+            stdin=subprocess.DEVNULL,
+        )
+        is_cl_a_merge_conflict = True
+    else:
+        is_cl_a_merge_conflict = False
+
+    footer_lines = patch_utils.generate_chromiumos_llvm_footer(
+        is_cherry=True,
+        apply_from=git_llvm_rev.translate_sha_to_rev(
+            llvm_config, reverted_sha
+        ).number,
+        apply_until=git_llvm_rev.translate_sha_to_rev(llvm_config, sha).number,
+        original_sha=sha,
+        platforms=("chromiumos",),
+        info=None,
+    )
+    commit_message = subprocess.run(
+        ["git", "log", "-n1", "--format=%B", sha],
+        check=True,
+        cwd=llvm_worktree,
+        encoding="utf-8",
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+    ).stdout
+
+    new_commit_message = _append_footers_to_commit_message(
+        commit_message, footer_lines
+    )
+    if is_cl_a_merge_conflict:
+        new_commit_message = f"MERGE CONFLICT: {new_commit_message}"
+
+    subprocess.run(
+        ["git", "commit", "--amend", "-m", new_commit_message],
+        check=True,
+        cwd=llvm_worktree,
+        stdin=subprocess.DEVNULL,
+    )
+    cl_head = git_utils.resolve_ref(git_dir=llvm_worktree, ref="HEAD")
+    logging.info("Successfully cherry-picked %s as %s", sha, cl_head)
+    cls = git_utils.upload_to_gerrit(
+        llvm_worktree,
+        ref=cl_head,
+        remote=git_utils.CROS_EXTERNAL_REMOTE,
+        branch=branch_without_remote,
+        reviewers=reviewers,
+        cc=cc,
+    )
+    if is_cl_a_merge_conflict:
+        # Set V-1 for more visibility.
+        for cl in cls:
+            try:
+                git_utils.set_gerrit_label(
+                    cwd=Path(llvm_config.dir),
+                    cl_id=cl,
+                    label_name=git_utils.GERRIT_LABEL_VERIFIED,
+                    label_value="-1",
+                )
+            except subprocess.CalledProcessError:
+                logging.warning("Failed to set V-1 on CL %d; ignoring", cl)
+
+
 def prettify_sha_for_email(
     sha: str,
     rev: int,
@@ -459,25 +601,26 @@ def maybe_email_about_stale_heads(
 
 def do_email(
     is_dry_run: bool,
-    llvm_dir: str,
+    llvm_config: git_llvm_rev.LLVMConfig,
+    upstream_main_branch: str,
     repository: str,
     interesting_shas: List[Tuple[str, str]],
     state: State,
     recipients: _EmailRecipients,
 ) -> State:
     def prettify_sha(sha: str) -> tiny_render.Piece:
-        rev = get_llvm_hash.GetVersionFrom(llvm_dir, sha)
+        rev = get_llvm_hash.GetVersionFrom(llvm_config.dir, sha)
         return prettify_sha_for_email(sha, rev)
 
     def get_sha_description(sha: str) -> tiny_render.Piece:
         return subprocess.check_output(
             ["git", "log", "-n1", "--format=%s", sha],
-            cwd=llvm_dir,
+            cwd=llvm_config.dir,
             encoding="utf-8",
         ).strip()
 
     new_state, new_reverts = locate_new_reverts_across_shas(
-        llvm_dir, interesting_shas, state
+        llvm_config, upstream_main_branch, interesting_shas, state
     )
 
     for revert_info in new_reverts:
@@ -550,6 +693,7 @@ def parse_args(argv: List[str]) -> argparse.Namespace:
     chromeos_subparser.add_argument(
         "--chromeos_dir",
         required=True,
+        type=Path,
         help="Up-to-date CrOS directory to use.",
     )
 
@@ -563,35 +707,6 @@ def parse_args(argv: List[str]) -> argparse.Namespace:
     return parser.parse_args(argv)
 
 
-def find_chroot(
-    opts: argparse.Namespace, cc: List[str]
-) -> Tuple[str, List[Tuple[str, str]], _EmailRecipients]:
-    if opts.repository == "chromeos":
-        chroot_path = opts.chromeos_dir
-        return (
-            chroot_path,
-            _find_interesting_chromeos_shas(chroot_path),
-            _EmailRecipients(well_known=["mage"], direct=cc),
-        )
-    elif opts.repository == "android":
-        if opts.action == "cherry-pick":
-            raise RuntimeError(
-                "android doesn't currently support automatic cherry-picking."
-            )
-
-        chroot_path = opts.android_llvm_toolchain_dir
-        return (
-            chroot_path,
-            _find_interesting_android_shas(chroot_path),
-            _EmailRecipients(
-                well_known=[],
-                direct=["android-llvm-dev@google.com"] + cc,
-            ),
-        )
-    else:
-        raise ValueError(f"Unknown repository {opts.repository}")
-
-
 def main(argv: List[str]) -> int:
     opts = parse_args(argv)
 
@@ -608,7 +723,34 @@ def main(argv: List[str]) -> int:
     reviewers = opts.reviewers if opts.reviewers else []
     cc = opts.cc if opts.cc else []
 
-    chroot_path, interesting_shas, recipients = find_chroot(opts, cc)
+    if opts.repository == "chromeos":
+        chromeos_path = opts.chromeos_dir
+        interesting_shas = _find_interesting_chromeos_shas(chromeos_path)
+        recipients = _EmailRecipients(well_known=["mage"], direct=cc)
+        llvm_config = git_llvm_rev.LLVMConfig(
+            remote=git_utils.CROS_EXTERNAL_REMOTE,
+            dir=llvm_dir,
+        )
+        upstream_main_branch = "upstream/main"
+    elif opts.repository == "android":
+        interesting_shas = _find_interesting_android_shas(
+            opts.android_llvm_toolchain_dir
+        )
+        recipients = _EmailRecipients(
+            well_known=[],
+            direct=["android-llvm-dev@google.com"] + cc,
+        )
+        llvm_config = git_llvm_rev.LLVMConfig(
+            remote="origin",
+            dir=llvm_dir,
+        )
+        upstream_main_branch = git_llvm_rev.MAIN_BRANCH
+        # Set this to placate linting bits. Shouldn't be used by
+        # `opts.repository == "android"` code.
+        chromeos_path = Path()
+    else:
+        raise ValueError(f"Unknown repository {opts.repository}")
+
     logging.info("Interesting SHAs were %r", interesting_shas)
 
     state = _read_state(state_file)
@@ -617,9 +759,15 @@ def main(argv: List[str]) -> int:
     # We want to be as free of obvious side-effects as possible in case
     # something above breaks. Hence, action as late as possible.
     if action == "cherry-pick":
+        if repository != "chromeos":
+            raise RuntimeError(
+                "only chromeos supports automatic cherry-picking."
+            )
+
         new_state = do_cherrypick(
-            chroot_path=chroot_path,
-            llvm_dir=llvm_dir,
+            chromeos_path=chromeos_path,
+            llvm_config=llvm_config,
+            upstream_main_branch=upstream_main_branch,
             repository=repository,
             interesting_shas=interesting_shas,
             state=state,
@@ -629,7 +777,8 @@ def main(argv: List[str]) -> int:
     else:
         new_state = do_email(
             is_dry_run=action == "dry-run",
-            llvm_dir=llvm_dir,
+            llvm_config=llvm_config,
+            upstream_main_branch=upstream_main_branch,
             interesting_shas=interesting_shas,
             repository=repository,
             state=state,
@@ -638,7 +787,3 @@ def main(argv: List[str]) -> int:
 
     _write_state(state_file, new_state)
     return 0
-
-
-if __name__ == "__main__":
-    sys.exit(main(sys.argv[1:]))
diff --git a/llvm_tools/nightly_revert_checker_test.py b/llvm_tools/nightly_revert_checker_test.py
old mode 100755
new mode 100644
index dbcd01dd..98b1bb50
--- a/llvm_tools/nightly_revert_checker_test.py
+++ b/llvm_tools/nightly_revert_checker_test.py
@@ -1,17 +1,17 @@
-#!/usr/bin/env python3
 # Copyright 2020 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
 """Tests for nightly_revert_checker."""
 
+import textwrap
+import time
 import unittest
 from unittest import mock
 
 from cros_utils import tiny_render
-import get_upstream_patch
-import nightly_revert_checker
-import revert_checker
+from llvm_tools import nightly_revert_checker
+from llvm_tools import revert_checker
 
 
 # pylint: disable=protected-access
@@ -137,49 +137,6 @@ class Test(unittest.TestCase):
 
         self.assertEqual(email, expected_email)
 
-    @mock.patch("revert_checker.find_reverts")
-    @mock.patch("get_upstream_patch.get_from_upstream")
-    def test_do_cherrypick_is_called(self, do_cherrypick, find_reverts):
-        find_reverts.return_value = [
-            revert_checker.Revert("12345abcdef", "fedcba54321")
-        ]
-        nightly_revert_checker.do_cherrypick(
-            chroot_path="/path/to/chroot",
-            llvm_dir="/path/to/llvm",
-            repository="repository_name",
-            interesting_shas=[("12345abcdef", "fedcba54321")],
-            state=nightly_revert_checker.State(),
-            reviewers=["meow@chromium.org"],
-            cc=["purr@chromium.org"],
-        )
-
-        do_cherrypick.assert_called_once()
-        find_reverts.assert_called_once()
-
-    @mock.patch("revert_checker.find_reverts")
-    @mock.patch("get_upstream_patch.get_from_upstream")
-    def test_do_cherrypick_handles_cherrypick_error(
-        self, do_cherrypick, find_reverts
-    ):
-        find_reverts.return_value = [
-            revert_checker.Revert("12345abcdef", "fedcba54321")
-        ]
-        do_cherrypick.side_effect = get_upstream_patch.CherrypickError(
-            "Patch at 12345abcdef already exists in PATCHES.json"
-        )
-        nightly_revert_checker.do_cherrypick(
-            chroot_path="/path/to/chroot",
-            llvm_dir="/path/to/llvm",
-            repository="repository_name",
-            interesting_shas=[("12345abcdef", "fedcba54321")],
-            state=nightly_revert_checker.State(),
-            reviewers=["meow@chromium.org"],
-            cc=["purr@chromium.org"],
-        )
-
-        do_cherrypick.assert_called_once()
-        find_reverts.assert_called_once()
-
     def test_sha_prettification_for_email(self):
         sha = "a" * 40
         rev = 123456
@@ -194,7 +151,7 @@ class Test(unittest.TestCase):
             ),
         )
 
-    @mock.patch("time.time")
+    @mock.patch.object(time, "time")
     def test_emailing_about_stale_heads_skips_in_simple_cases(self, time_time):
         now = 1_000_000_000
         time_time.return_value = now
@@ -252,8 +209,8 @@ class Test(unittest.TestCase):
             state, nightly_revert_checker.State.from_json(state.to_json())
         )
 
-    @mock.patch("time.time")
-    @mock.patch("nightly_revert_checker._send_revert_email")
+    @mock.patch.object(time, "time")
+    @mock.patch.object(nightly_revert_checker, "_send_revert_email")
     def test_emailing_about_stale_with_one_report(
         self, send_revert_email, time_time
     ):
@@ -302,6 +259,89 @@ class Test(unittest.TestCase):
             "bug at go/crostc-bug if an update is needed. Thanks!",
         )
 
+    def test_appending_footers_when_none_exist(self):
+        base_message = textwrap.dedent(
+            """\
+            hello: world!
+
+            This is a simple commit message.
+            """
+        ).rstrip()
+        want_message = textwrap.dedent(
+            """\
+            hello: world!
 
-if __name__ == "__main__":
-    unittest.main()
+            This is a simple commit message.
+
+            foo: bar
+            bar: baz
+            """
+        ).rstrip()
+        self.assertEqual(
+            nightly_revert_checker._append_footers_to_commit_message(
+                base_message,
+                ("foo: bar", "bar: baz"),
+            ),
+            want_message,
+        )
+
+    def test_appending_footers_when_some_exist(self):
+        base_message = textwrap.dedent(
+            """\
+            hello: world!
+
+            This is a simple commit message.
+            this: is not a footer though
+            because: it is in the same paragraph as the commit message
+
+            but: this is a footer!
+            """
+        ).rstrip()
+        want_message = textwrap.dedent(
+            """\
+            hello: world!
+
+            This is a simple commit message.
+            this: is not a footer though
+            because: it is in the same paragraph as the commit message
+
+            but: this is a footer!
+            foo: bar
+            """
+        ).rstrip()
+        self.assertEqual(
+            nightly_revert_checker._append_footers_to_commit_message(
+                base_message,
+                ("foo: bar",),
+            ),
+            want_message,
+        )
+
+    def test_appending_footers_when_last_paragraph_is_tricky(self):
+        base_message = textwrap.dedent(
+            """\
+            hello: world!
+
+            This is a simple commit message.
+            this: is not a footer though
+            because: it is in the same paragraph as the commit message
+            """
+        ).rstrip()
+        want_message = textwrap.dedent(
+            """\
+            hello: world!
+
+            This is a simple commit message.
+            this: is not a footer though
+            because: it is in the same paragraph as the commit message
+
+            foo: bar
+            """
+        ).rstrip()
+        self.assertEqual(
+            nightly_revert_checker._append_footers_to_commit_message(
+                base_message,
+                ("foo: bar",),
+            ),
+            want_message,
+        )
diff --git a/llvm_tools/patch_manager.py b/llvm_tools/patch_manager.py
old mode 100755
new mode 100644
index 801f8469..6540a650
--- a/llvm_tools/patch_manager.py
+++ b/llvm_tools/patch_manager.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -12,10 +11,10 @@ from pathlib import Path
 import sys
 from typing import Callable, Iterable, List, Optional, Tuple
 
-import failure_modes
-import get_llvm_hash
-import patch_utils
-import subprocess_helpers
+from llvm_tools import failure_modes
+from llvm_tools import get_llvm_hash
+from llvm_tools import patch_utils
+from llvm_tools import subprocess_helpers
 
 
 class GitBisectionCode(enum.IntEnum):
@@ -80,14 +79,23 @@ def GetCommandLineArgs(sys_argv: Optional[List[str]]):
         "application of. Not used in other modes.",
     )
 
+    group = parser.add_mutually_exclusive_group()
     # Add argument for the option to us git am to commit patch or
     # just using patch.
-    parser.add_argument(
+    group.add_argument(
         "--git_am",
         action="store_true",
         help="If set, use 'git am' to patch instead of GNU 'patch'. ",
     )
 
+    # Add argument for the option to us git am chromiumos to commit patch or
+    # just using patch.
+    group.add_argument(
+        "--chromiumos_apply",
+        action="store_true",
+        help="If set, use 'git am' with chromiumos footer edits.",
+    )
+
     # Parse the command line.
     return parser.parse_args(sys_argv)
 
@@ -268,6 +276,13 @@ def main(sys_argv: List[str]):
             "--patch_metadata_file arg " f"{patches_json_fp} is not a file"
         )
 
+    if args_output.git_am:
+        patch_cmd = patch_utils.git_am
+    elif args_output.chromiumos_apply:
+        patch_cmd = patch_utils.git_am_chromiumos
+    else:
+        patch_cmd = patch_utils.gnu_patch
+
     def _apply_all(args):
         if args.svn_version is None:
             raise ValueError("--svn_version must be set when applying patches")
@@ -275,16 +290,13 @@ def main(sys_argv: List[str]):
             svn_version=args.svn_version,
             llvm_src_dir=llvm_src_dir,
             patches_json_fp=patches_json_fp,
-            patch_cmd=patch_utils.git_am
-            if args.git_am
-            else patch_utils.gnu_patch,
+            patch_cmd=patch_cmd,
             continue_on_failure=args.failure_mode
             == failure_modes.FailureModes.CONTINUE,
         )
         PrintPatchResults(result)
 
     def _disable(args):
-        patch_cmd = patch_utils.git_am if args.git_am else patch_utils.gnu_patch
         patch_utils.update_version_ranges(
             args.svn_version, llvm_src_dir, patches_json_fp, patch_cmd
         )
@@ -314,7 +326,3 @@ def main(sys_argv: List[str]):
 
     if args_output.failure_mode in dispatch_table:
         dispatch_table[args_output.failure_mode](args_output)
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/llvm_tools/patch_manager_unittest.py b/llvm_tools/patch_manager_test.py
old mode 100755
new mode 100644
similarity index 98%
rename from llvm_tools/patch_manager_unittest.py
rename to llvm_tools/patch_manager_test.py
index 30acd24a..fe037bbd
--- a/llvm_tools/patch_manager_unittest.py
+++ b/llvm_tools/patch_manager_test.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -12,9 +11,9 @@ from typing import Callable
 import unittest
 from unittest import mock
 
-import atomic_write_file
-import patch_manager
-import patch_utils
+from llvm_tools import atomic_write_file
+from llvm_tools import patch_manager
+from llvm_tools import patch_utils
 
 
 class PatchManagerTest(unittest.TestCase):
@@ -211,7 +210,3 @@ class PatchManagerTest(unittest.TestCase):
                 _apply_patch_entry_mock3,
                 patch_manager.GitBisectionCode.SKIP,
             )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/patch_sync/src/android_utils.rs b/llvm_tools/patch_sync/src/android_utils.rs
index 70bca189..78606052 100644
--- a/llvm_tools/patch_sync/src/android_utils.rs
+++ b/llvm_tools/patch_sync/src/android_utils.rs
@@ -18,7 +18,8 @@ pub fn get_android_llvm_version(android_checkout: &Path) -> Result<String> {
     let mut command = new_android_cmd(android_checkout, "python3")?;
     command.args([
         "-c",
-        "import android_version; print(android_version.get_svn_revision_number(), end='')",
+        "from src.llvm_android import android_version; \
+         print(android_version.get_svn_revision_number(), end='')",
     ]);
     let output = command.output()?;
     if !output.status.success() {
diff --git a/llvm_tools/patch_sync/src/main.rs b/llvm_tools/patch_sync/src/main.rs
index e3fa4820..fc0bd4ca 100644
--- a/llvm_tools/patch_sync/src/main.rs
+++ b/llvm_tools/patch_sync/src/main.rs
@@ -352,8 +352,8 @@ enum Opt {
         #[structopt(long = "cros-rev")]
         cros_reviewers: Option<String>,
 
-        /// Git ref (e.g. hash) for the ChromiumOS overlay to use as the base.
-        #[structopt(long = "overlay-base-ref")]
+        /// Git ref (e.g. hash) for the ChromiumOS toolchain-utils to use as the base.
+        #[structopt(long = "tc-base-ref")]
         old_cros_ref: String,
 
         /// Path to the Android Open Source Project source repo checkout.
diff --git a/llvm_tools/patch_sync/src/version_control.rs b/llvm_tools/patch_sync/src/version_control.rs
index 55197f98..01889289 100644
--- a/llvm_tools/patch_sync/src/version_control.rs
+++ b/llvm_tools/patch_sync/src/version_control.rs
@@ -9,7 +9,8 @@ use std::fs;
 use std::path::{Path, PathBuf};
 use std::process::{Command, Output};
 
-const CHROMIUMOS_OVERLAY_REL_PATH: &str = "src/third_party/chromiumos-overlay";
+const CROS_TOOLCHAIN_UTILS_REL_PATH: &str = "src/third_party/toolchain-utils";
+const CROS_PATCH_WORKDIR: &str = "llvm_patches";
 const ANDROID_LLVM_REL_PATH: &str = "toolchain/llvm_android";
 
 // Need to checkout the upstream, rather than the local clone.
@@ -36,14 +37,24 @@ impl RepoSetupContext {
             {
                 let crpp = self.cros_patches_path();
                 let cros_git = crpp.parent().unwrap();
+                ensure!(
+                    cros_git.is_dir(),
+                    "cros_git '{}' is not a dir",
+                    cros_git.display()
+                );
                 git_cd_cmd(cros_git, ["checkout", CROS_MAIN_BRANCH])?;
             }
             {
                 let anpp = self.android_patches_path();
                 let android_git = anpp.parent().unwrap();
+                ensure!(
+                    android_git.is_dir(),
+                    "android_git '{}' is not a dir",
+                    android_git.display()
+                );
                 git_cd_cmd(android_git, ["checkout", ANDROID_MAIN_BRANCH])?;
             }
-            repo_cd_cmd(&self.cros_checkout, ["sync", CHROMIUMOS_OVERLAY_REL_PATH])?;
+            repo_cd_cmd(&self.cros_checkout, ["sync", CROS_TOOLCHAIN_UTILS_REL_PATH])?;
             repo_cd_cmd(&self.android_checkout, ["sync", ANDROID_LLVM_REL_PATH])?;
         }
         Ok(())
@@ -52,8 +63,8 @@ impl RepoSetupContext {
     pub fn cros_repo_upload<S: AsRef<str>>(&self, reviewers: &[S]) -> Result<()> {
         let llvm_dir = self
             .cros_checkout
-            .join(CHROMIUMOS_OVERLAY_REL_PATH)
-            .join("sys-devel/llvm");
+            .join(CROS_TOOLCHAIN_UTILS_REL_PATH)
+            .join(CROS_PATCH_WORKDIR);
         ensure!(
             llvm_dir.is_dir(),
             "CrOS LLVM dir {} is not a directory",
@@ -78,7 +89,7 @@ impl RepoSetupContext {
         }
         Self::repo_upload(
             &self.cros_checkout,
-            CHROMIUMOS_OVERLAY_REL_PATH,
+            CROS_TOOLCHAIN_UTILS_REL_PATH,
             &Self::build_commit_msg(
                 "llvm: Synchronize patches from android",
                 "android",
@@ -116,7 +127,7 @@ impl RepoSetupContext {
     }
 
     fn cros_cleanup(&self) -> Result<()> {
-        let git_path = self.cros_checkout.join(CHROMIUMOS_OVERLAY_REL_PATH);
+        let git_path = self.cros_checkout.join(CROS_TOOLCHAIN_UTILS_REL_PATH);
         Self::cleanup_branch(&git_path, CROS_MAIN_BRANCH, WORK_BRANCH_NAME)
             .with_context(|| format!("cleaning up branch {}", WORK_BRANCH_NAME))?;
         Ok(())
@@ -149,16 +160,17 @@ impl RepoSetupContext {
     /// Get the ChromiumOS path to the PATCHES.json file
     pub fn cros_patches_path(&self) -> PathBuf {
         self.cros_checkout
-            .join(CHROMIUMOS_OVERLAY_REL_PATH)
-            .join("sys-devel/llvm/files/PATCHES.json")
+            .join(CROS_TOOLCHAIN_UTILS_REL_PATH)
+            .join(CROS_PATCH_WORKDIR)
+            .join("PATCHES.json")
     }
 
     /// Return the contents of the old PATCHES.json from ChromiumOS
     pub fn old_cros_patch_contents(&self, hash: &str) -> Result<String> {
         Self::old_file_contents(
             hash,
-            &self.cros_checkout.join(CHROMIUMOS_OVERLAY_REL_PATH),
-            Path::new("sys-devel/llvm/files/PATCHES.json"),
+            &self.cros_checkout.join(CROS_TOOLCHAIN_UTILS_REL_PATH),
+            &Path::new(CROS_PATCH_WORKDIR).join("PATCHES.json"),
         )
     }
 
diff --git a/llvm_tools/patch_utils.py b/llvm_tools/patch_utils.py
index c180a87a..9e2a44c6 100644
--- a/llvm_tools/patch_utils.py
+++ b/llvm_tools/patch_utils.py
@@ -24,7 +24,7 @@ from typing import (
     Union,
 )
 
-import atomic_write_file
+from llvm_tools import atomic_write_file
 
 
 APPLIED_RE = re.compile(r"^Applying: (.+) \(#(\d+)\)$")
@@ -34,7 +34,9 @@ HUNK_HEADER_RE = re.compile(r"^@@\s+-(\d+),(\d+)\s+\+(\d+),(\d+)\s+@@")
 HUNK_END_RE = re.compile(r"^--\s*$")
 PATCH_SUBFILE_HEADER_RE = re.compile(r"^\+\+\+ [ab]/(.*)$")
 
-CHROMEOS_PATCHES_JSON_PACKAGES = (
+# A list of all packages in chromiumos-overlay that deploy subsets of the LLVM
+# source tree we ship.
+CHROMEOS_LLVM_SUBPACKAGES = (
     "dev-util/lldb-server",
     "sys-devel/llvm",
     "sys-libs/compiler-rt",
@@ -158,31 +160,6 @@ class PatchResult:
         return s
 
 
-def git_apply(patch_path: Path) -> List[Union[str, Path]]:
-    """Patch a patch file using 'git apply'."""
-    return ["git", "apply", patch_path]
-
-
-def git_am(patch_path: Path) -> List[Union[str, Path]]:
-    """Patch a patch file using 'git am'."""
-    return ["git", "am", "--3way", patch_path]
-
-
-def gnu_patch(root_dir: Path, patch_path: Path) -> List[Union[str, Path]]:
-    """Patch a patch file using GNU 'patch'."""
-    return [
-        "patch",
-        "-d",
-        root_dir.absolute(),
-        "-f",
-        "-E",
-        "-p1",
-        "--no-backup-if-mismatch",
-        "-i",
-        patch_path,
-    ]
-
-
 @dataclasses.dataclass
 class PatchEntry:
     """Object mapping of an entry of PATCHES.json."""
@@ -270,7 +247,7 @@ class PatchEntry:
         self,
         root_dir: Path,
         patch_cmd: Optional[Callable] = None,
-        extra_args: Optional[List[str]] = None,
+        extra_args: Optional[List[Union[str, Path]]] = None,
     ) -> PatchResult:
         """Apply a patch to a given directory."""
         # Cmd to apply a patch in the src unpack path.
@@ -279,34 +256,12 @@ class PatchEntry:
             raise RuntimeError(
                 f"Cannot apply: patch {abs_patch_path} is not a file"
             )
-
-        if not patch_cmd or patch_cmd is gnu_patch:
-            cmd = gnu_patch(root_dir, abs_patch_path) + (extra_args or [])
-        else:
-            cmd = patch_cmd(abs_patch_path) + (extra_args or [])
-
-        try:
-            subprocess.run(
-                cmd, encoding="utf-8", check=True, stdout=subprocess.PIPE
-            )
-        except subprocess.CalledProcessError as e:
-            parsed_hunks = self.parsed_hunks()
-            failed_hunks_id_dict = parse_failed_patch_output(e.stdout)
-            failed_hunks = {}
-            if patch_cmd is gnu_patch:
-                for path, failed_hunk_ids in failed_hunks_id_dict.items():
-                    hunks_for_file = parsed_hunks[path]
-                    failed_hunks[path] = [
-                        hunk
-                        for hunk in hunks_for_file
-                        if hunk.hunk_id in failed_hunk_ids
-                    ]
-            elif failed_hunks_id_dict:
-                # using git am
-                failed_hunks = parsed_hunks
-
-            return PatchResult(succeeded=False, failed_hunks=failed_hunks)
-        return PatchResult(succeeded=True)
+        # TODO(b/343568613)
+        # By default, we still expect to be using gnu_patch.
+        # This is a bad default, and requires some clean up.
+        if not patch_cmd:
+            patch_cmd = gnu_patch
+        return patch_cmd(self, root_dir, abs_patch_path, extra_args)
 
     def test_apply(
         self, root_dir: Path, patch_cmd: Optional[Callable] = None
@@ -315,12 +270,20 @@ class PatchEntry:
 
         When using gnu_patch, this will pass --dry-run.
         When using git_am or git_apply, this will instead
-        use git_apply with --summary.
+        use git_apply with --check.
         """
-        if patch_cmd is git_am or patch_cmd is git_apply:
+        if any(
+            patch_cmd is cmd
+            for cmd in (
+                git_apply,
+                git_am,
+                git_am_chromiumos,
+                git_am_chromiumos_quiet,
+            )
+        ):
             # There is no dry run option for git am,
             # so we use git apply for test.
-            return self.apply(root_dir, git_apply, ["--summary"])
+            return self.apply(root_dir, git_apply, ["--check"])
         if patch_cmd is gnu_patch or patch_cmd is None:
             return self.apply(root_dir, patch_cmd, ["--dry-run"])
         raise ValueError(f"No such patch command: {patch_cmd.__name__}.")
@@ -331,6 +294,231 @@ class PatchEntry:
         return self.metadata.get("title", "")
 
 
+def git_apply(
+    pe: PatchEntry,
+    root_dir: Path,
+    patch_path: Path,
+    extra_args: List[Union[str, Path]],
+) -> PatchResult:
+    """Patch a patch file using 'git apply'."""
+    cmd: List[Union[str, Path]] = ["git", "apply", patch_path]
+    if extra_args:
+        cmd += extra_args
+    return _run_git_applylike(pe, root_dir, cmd)
+
+
+def git_am(
+    pe: PatchEntry,
+    root_dir: Path,
+    patch_path: Path,
+    extra_args: Optional[List[Union[str, Path]]],
+) -> PatchResult:
+    """Patch a patch file using 'git am'."""
+    cmd: List[Union[str, Path]] = ["git", "am", "--3way", patch_path]
+    if extra_args:
+        cmd += extra_args
+    return _run_git_applylike(pe, root_dir, cmd, quiet=True)
+
+
+def git_am_chromiumos(
+    pe: PatchEntry,
+    root_dir: Path,
+    patch_path: Path,
+    extra_args: Optional[List[Union[str, Path]]],
+) -> PatchResult:
+    """Patch a patch file using 'git am', but include footer metadata."""
+    return _git_am_chromiumos_internal(pe, root_dir, patch_path, extra_args)
+
+
+def git_am_chromiumos_quiet(
+    pe: PatchEntry,
+    root_dir: Path,
+    patch_path: Path,
+    extra_args: Optional[List[Union[str, Path]]],
+) -> PatchResult:
+    """Same as git_am_chromiumos, but no stdout."""
+    return _git_am_chromiumos_internal(
+        pe, root_dir, patch_path, extra_args, quiet=True
+    )
+
+
+def _git_am_chromiumos_internal(
+    pe: PatchEntry,
+    root_dir: Path,
+    patch_path: Path,
+    extra_args: Optional[List[Union[str, Path]]],
+    quiet: bool = False,
+) -> PatchResult:
+    cmd: List[Union[str, Path]] = [
+        "git",
+        "am",
+        "--3way",
+        "--keep-non-patch",
+        patch_path,
+    ]
+    if extra_args:
+        cmd += extra_args
+    try:
+        subprocess.run(
+            cmd,
+            encoding="utf-8",
+            check=True,
+            stdin=subprocess.DEVNULL,
+            stdout=subprocess.DEVNULL if quiet else None,
+            cwd=root_dir,
+        )
+    except subprocess.CalledProcessError:
+        failed_hunks = pe.parsed_hunks()
+        return PatchResult(succeeded=False, failed_hunks=failed_hunks)
+    # Now we need to rewrite the commit message with the new footer.
+    original_commit_msg_lines = (
+        subprocess.run(
+            ["git", "show", "-s", "--format=%B", "HEAD"],
+            encoding="utf-8",
+            check=True,
+            stdout=subprocess.PIPE,
+            stdin=subprocess.DEVNULL,
+            cwd=root_dir,
+        )
+        .stdout.strip()
+        .splitlines()
+    )
+    new_commit_msg_lines = []
+    metadata_regex = re.compile(r"(change-id|patch\.[\w.-]+):.+", re.IGNORECASE)
+    new_commit_msg_lines = [
+        line
+        for line in original_commit_msg_lines
+        if not metadata_regex.match(line)
+    ] + _chromiumos_llvm_footer(pe)
+    subprocess.run(
+        ["git", "commit", "--amend", "-m", "\n".join(new_commit_msg_lines)],
+        check=True,
+        encoding="utf-8",
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.DEVNULL if quiet else None,
+        cwd=root_dir,
+    )
+    return PatchResult(succeeded=True)
+
+
+def gnu_patch(
+    pe: PatchEntry,
+    root_dir: Path,
+    patch_path: Path,
+    extra_args: Optional[List[Union[str, Path]]],
+) -> PatchResult:
+    """Patch a patch file using GNU 'patch'."""
+    cmd: List[Union[str, Path]] = [
+        "patch",
+        "-d",
+        root_dir.absolute(),
+        "-f",
+        "-E",
+        "-p1",
+        "--no-backup-if-mismatch",
+        "-i",
+        patch_path,
+    ]
+    if extra_args:
+        cmd += extra_args
+    try:
+        subprocess.run(
+            cmd,
+            encoding="utf-8",
+            check=True,
+            stdout=subprocess.PIPE,
+            stdin=subprocess.DEVNULL,
+        )
+    except subprocess.CalledProcessError as e:
+        parsed_hunks = pe.parsed_hunks()
+        failed_hunks_id_dict = parse_failed_patch_output(e.stdout)
+        failed_hunks = {}
+        for path, failed_hunk_ids in failed_hunks_id_dict.items():
+            hunks_for_file = parsed_hunks[path]
+            failed_hunks[path] = [
+                hunk
+                for hunk in hunks_for_file
+                if hunk.hunk_id in failed_hunk_ids
+            ]
+        return PatchResult(succeeded=False, failed_hunks=failed_hunks)
+    return PatchResult(succeeded=True)
+
+
+def _run_git_applylike(
+    pe: PatchEntry,
+    root_dir: Path,
+    cmd: List[Union[Path, str]],
+    quiet: bool = False,
+):
+    try:
+        subprocess.run(
+            cmd,
+            encoding="utf-8",
+            check=True,
+            stdin=subprocess.DEVNULL,
+            stdout=subprocess.DEVNULL if quiet else None,
+            cwd=root_dir,
+        )
+    except subprocess.CalledProcessError:
+        failed_hunks = pe.parsed_hunks()
+        return PatchResult(succeeded=False, failed_hunks=failed_hunks)
+    return PatchResult(succeeded=True)
+
+
+def generate_chromiumos_llvm_footer(
+    is_cherry: bool,
+    apply_from: Optional[int] = None,
+    apply_until: Optional[int] = None,
+    original_sha: Optional[str] = None,
+    platforms: Iterable[str] = (),
+    info: Optional[str] = None,
+) -> List[str]:
+    """Generates a commit footer given patch metadata.
+
+    Returns:
+        A list of commit footer lines.
+    """
+    # We want to keep the order of these alphabetical,
+    # so the creation of the footer looks a little weird.
+    extra_metadata = []
+    if info:
+        extra_metadata.append("patch.metadata.info: " + ", ".join(info))
+    if original_sha:
+        extra_metadata.append(f"patch.metadata.original_sha: {original_sha}")
+    if platforms:
+        extra_metadata.append("patch.platforms: " + ", ".join(platforms))
+    from_rev = "0"
+    until_rev = "null"
+    if apply_from is not None:
+        from_rev = str(apply_from)
+    if apply_until is not None:
+        until_rev = str(apply_until)
+    return (
+        [
+            "",
+            f"patch.cherry: {str(is_cherry).lower()}",
+        ]
+        + extra_metadata
+        + [
+            f"patch.version_range.from: {from_rev}",
+            f"patch.version_range.until: {until_rev}",
+        ]
+    )
+
+
+def _chromiumos_llvm_footer(pe: PatchEntry) -> List[str]:
+    version_range = pe.version_range or {}
+    metadata = pe.metadata or {}
+    return generate_chromiumos_llvm_footer(
+        is_cherry=pe.rel_patch_path.startswith("cherry/"),
+        apply_from=version_range.get("from"),
+        apply_until=version_range.get("until"),
+        original_sha=metadata.get("original_sha"),
+        platforms=pe.platforms or (),
+        info=metadata.get("info"),
+    )
+
+
 def patch_applies_after(
     version_range: Optional[Dict[str, Optional[int]]], svn_version: int
 ) -> bool:
diff --git a/llvm_tools/patch_utils_unittest.py b/llvm_tools/patch_utils_test.py
old mode 100755
new mode 100644
similarity index 96%
rename from llvm_tools/patch_utils_unittest.py
rename to llvm_tools/patch_utils_test.py
index 311a4dbd..c9dcb51b
--- a/llvm_tools/patch_utils_unittest.py
+++ b/llvm_tools/patch_utils_test.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2022 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -9,23 +8,16 @@ import copy
 import io
 import json
 from pathlib import Path
-import shutil
 import subprocess
-import tempfile
-import unittest
 from unittest import mock
 
-import patch_utils as pu
+from llvm_tools import patch_utils as pu
+from llvm_tools import test_helpers
 
 
-class TestPatchUtils(unittest.TestCase):
+class TestPatchUtils(test_helpers.TempDirTestCase):
     """Test the patch_utils."""
 
-    def make_tempdir(self) -> Path:
-        tmpdir = Path(tempfile.mkdtemp(prefix="patch_utils_unittest"))
-        self.addCleanup(shutil.rmtree, tmpdir)
-        return tmpdir
-
     def test_predict_indent(self):
         test_str1 = """
 a
@@ -233,8 +225,8 @@ Hunk #1 SUCCEEDED at 96 with fuzz 1.
             test_file.write_text("abc")
         self.assertTrue(pu.is_git_dirty(dirpath))
 
-    @mock.patch("patch_utils.git_clean_context", mock.MagicMock)
-    def test_update_version_ranges(self):
+    @mock.patch.object(pu, "git_clean_context")
+    def test_update_version_ranges(self, _mock_git_clean_context):
         """Test the UpdateVersionRanges function."""
         dirpath = self.make_tempdir()
         patches = [
@@ -412,6 +404,3 @@ index c5fd68299eb..4c6e15eeeb9 100644
    // Now we need to do some global optimization transforms.
    // FIXME: It would seem like these should come first in the optimization
 """
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/ready_llvm_branch.py b/llvm_tools/ready_llvm_branch.py
new file mode 100644
index 00000000..aa816cff
--- /dev/null
+++ b/llvm_tools/ready_llvm_branch.py
@@ -0,0 +1,287 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Create and ready an LLVM Branch for ChromiumOS.
+
+Use this script to create and set up an LLVM branch that can
+be tracked by the ChromiumOS manifest.
+
+Examples:
+
+    # Make chromeos/llvm-r530567-1 locally, but doesn't upload or set the
+    # upstream. Convenient for testing.
+    $ ready_llvm_branch.py -r 530567
+
+    # Make chromeos/llvm-r530567-1 and uploads it for review.
+    $ ready_llvm_branch.py -r 530567 --upload
+
+    # Make chromeos/llvm-r530567-2 and uploads it.
+    $ ready_llvm_branch.py -r 530567 --branch-number 2 --upload
+"""
+
+import argparse
+import logging
+from pathlib import Path
+import shlex
+import subprocess
+from typing import Callable, List
+
+from cros_utils import cros_paths
+from cros_utils import git_utils
+from llvm_tools import git_llvm_rev
+from llvm_tools import llvm_project_base_commit
+from llvm_tools import patch_utils
+
+
+def _switch_branch(
+    llvm_src_dir: Path,
+    svn_revision: int,
+    branch_number: int = 1,
+    force_checkout: bool = False,
+) -> str:
+    start_sha = git_llvm_rev.translate_rev_to_sha(
+        git_llvm_rev.LLVMConfig(git_utils.CROS_EXTERNAL_REMOTE, llvm_src_dir),
+        git_llvm_rev.Rev(git_utils.CROS_MAIN_BRANCH, svn_revision),
+    )
+    switch_arg = "-C" if force_checkout else "-c"
+    if force_checkout:
+        git_utils.discard_changes_and_checkout(llvm_src_dir, start_sha)
+    branch_name = f"chromeos/llvm-r{svn_revision}-{branch_number}"
+    cmd = (
+        "git",
+        "switch",
+        switch_arg,
+        branch_name,
+        start_sha,
+    )
+    subprocess.run(cmd, cwd=llvm_src_dir, check=True, stdin=subprocess.DEVNULL)
+    return branch_name
+
+
+def _apply_patches_locally(
+    patches_json: Path,
+    llvm_src_dir: Path,
+    svn_revision: int,
+    continue_on_failure: bool,
+    apply_only: bool,
+) -> None:
+    patch_cmd: Callable = patch_utils.git_am_chromiumos
+    if apply_only:
+        patch_cmd = patch_utils.git_apply
+    patch_utils.apply_all_from_json(
+        svn_version=svn_revision,
+        llvm_src_dir=llvm_src_dir,
+        patches_json_fp=patches_json,
+        patch_cmd=patch_cmd,
+        continue_on_failure=continue_on_failure,
+    )
+
+
+def _maybe_upload_for_review(
+    llvm_src_dir: Path, branch_name: str, dry_run: bool
+) -> None:
+    kwargs = {
+        "remote": git_utils.CROS_EXTERNAL_REMOTE,
+        "branch": branch_name,
+        "topic": f"{branch_name}-patches",
+    }
+    if dry_run:
+        cmd = git_utils.generate_upload_to_gerrit_cmd(**kwargs)
+        upload_command_git = shlex.join(cmd)
+        logging.warning(
+            "Did not upload branch. You can do so manually with:"
+            "\n\n  pushd %s && %s && popd",
+            shlex.quote(str(llvm_src_dir)),
+            upload_command_git,
+        )
+        return
+    logging.info("Uploading branch...")
+    git_utils.upload_to_gerrit(llvm_src_dir, **kwargs)
+
+
+def parse_args(argv: List[str]) -> argparse.Namespace:
+    """Parse passed in argv list."""
+
+    parser = argparse.ArgumentParser(
+        description=__doc__,
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+    )
+    git_revision_group = parser.add_mutually_exclusive_group(required=True)
+    git_revision_group.add_argument(
+        "-r",
+        "--svn-revision",
+        type=int,
+        help="SVN Revision for which to apply patches. e.g. '516547'.",
+    )
+    git_revision_group.add_argument(
+        "--head",
+        action="store_true",
+        help="Apply patches to current HEAD, using the HEAD's revision.",
+    )
+
+    chromiumos_root_action = parser.add_argument(
+        "--chromiumos-root",
+        type=Path,
+        help="""
+        Path to ChromiumOS root to detect the PATCHES.json.
+        If neither this flag nor --patch-file are specified, it is
+        autodetected.
+        """,
+    )
+    parser.add_argument(
+        "--patch-file",
+        type=Path,
+        help="""
+        Path to PATCHES.json. If not specified, it is autodetected
+        from --chromiumos-root.
+        """,
+    )
+    parser.add_argument(
+        "--force-checkout",
+        action="store_true",
+        help="""
+        Runs `git clean` and `git reset --hard` before running, discarding
+        any local changes. If you passed `--svn-revision` and the local branch
+        that would be created exists already, this will also delete that branch.
+        """,
+    )
+    llvm_dir_action = parser.add_argument(
+        "--llvm-dir",
+        type=Path,
+        help="""
+        Path to a ChromiumOS llvm-project directory. If not
+        specified, it is autodetected from --chromiumos-root.
+        """,
+    )
+    parser.add_argument(
+        "--branch-number",
+        default=1,
+        type=int,
+        help="""
+        An index to avoid branch name conflicts. If a branch is already
+        made in the upstream cros remote with the same name, and you want to make
+        another cros branch for the same revision number, then pass this flag
+        with a number other than '1'. Defaults to %(default)s.
+        """,
+    )
+    upload_group = parser.add_mutually_exclusive_group()
+    upload_group.add_argument(
+        "--upload",
+        action="store_true",
+        help="Upload the branch to the correct destination branch.",
+    )
+    upload_group.add_argument(
+        "-a",
+        "--apply",
+        action="store_true",
+        help="Apply only, don't commit. Cannot be passed with --upload.",
+    )
+    parser.add_argument(
+        "-c",
+        "--continue-on-patch-failure",
+        action="store_true",
+        help="Skip patches that fail to apply. Useful with --apply.",
+    )
+    args = parser.parse_args(argv)
+    if not args.chromiumos_root and not args.patch_file:
+        if repo_root := cros_paths.script_chromiumos_checkout():
+            args.chromiumos_root = repo_root
+        else:
+            raise argparse.ArgumentError(
+                chromiumos_root_action,
+                "Could not find chromiumos root automatically."
+                " Pass --chromiumos-root manually.",
+            )
+    if not args.llvm_dir:
+        llvm_dir = args.chromiumos_root / cros_paths.LLVM_PROJECT
+        if not (llvm_dir / ".git").is_dir():
+            raise argparse.ArgumentError(
+                llvm_dir_action,
+                "Could not find llvm dir automatically. Pass --llvm-dir"
+                " manually.",
+            )
+        args.llvm_dir = llvm_dir
+
+    if not args.patch_file:
+        args.patch_file = args.chromiumos_root / cros_paths.DEFAULT_PATCHES_PATH
+
+    return args
+
+
+def main(sys_argv: List[str]) -> None:
+    """Entry point."""
+    logging.basicConfig(
+        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
+        "%(message)s",
+        level=logging.INFO,
+    )
+    args = parse_args(sys_argv)
+    branch_name = ""
+
+    if args.svn_revision:
+        svn_revision = args.svn_revision
+        branch_name = _switch_branch(
+            args.llvm_dir,
+            svn_revision,
+            args.branch_number,
+            args.force_checkout,
+        )
+        logging.info("Created and switched to branch %s.", branch_name)
+    elif args.head:
+        if args.force_checkout:
+            git_utils.discard_changes_and_checkout(args.llvm_dir, "HEAD")
+
+        to_translate = git_utils.merge_base(
+            args.llvm_dir,
+            ["cros/upstream/main", "HEAD"],
+        )
+        if not to_translate:
+            to_translate = "HEAD"
+        svn_revision = git_llvm_rev.translate_sha_to_rev(
+            git_llvm_rev.LLVMConfig(
+                git_utils.CROS_EXTERNAL_REMOTE, args.llvm_dir
+            ),
+            to_translate,
+        ).number
+        logging.info("Applying to HEAD, as r%s", svn_revision)
+    else:
+        raise ValueError("Unreachable: either --head or -r is required")
+    toolchain_utils_dir = args.chromiumos_root / cros_paths.TOOLCHAIN_UTILS
+    chromiumos_overlay = args.chromiumos_root / cros_paths.CHROMIUMOS_OVERLAY
+    if args.apply:
+        llvm_project_base_commit.write_base_changes(
+            toolchain_utils_dir,
+            args.llvm_dir,
+            svn_revision,
+            chromiumos_overlay,
+        )
+        logging.info(
+            "--apply passed, wrote cros base changes, but didn't commit."
+        )
+    else:
+        llvm_project_base_commit.make_base_commit(
+            toolchain_utils_dir,
+            args.llvm_dir,
+            svn_revision,
+            chromiumos_overlay,
+        )
+        logging.info("Committed base commit.")
+    _apply_patches_locally(
+        args.chromiumos_root / cros_paths.DEFAULT_PATCHES_PATH,
+        args.llvm_dir,
+        svn_revision,
+        args.continue_on_patch_failure,
+        args.apply,
+    )
+    if not args.head:
+        _maybe_upload_for_review(
+            llvm_src_dir=args.llvm_dir,
+            branch_name=branch_name,
+            dry_run=not args.upload,
+        )
+    else:
+        logging.warning(
+            "--head passed: Cannot upload this branch for review, as"
+            " changes may not be on a branch."
+        )
diff --git a/llvm_tools/revert_checker.py b/llvm_tools/revert_checker.py
old mode 100755
new mode 100644
index 17914ba8..c3d5dca0
--- a/llvm_tools/revert_checker.py
+++ b/llvm_tools/revert_checker.py
@@ -51,38 +51,78 @@ import logging
 import re
 import subprocess
 import sys
-from typing import Generator, Iterable, List, NamedTuple
-
+from typing import Dict, Generator, Iterable, List, NamedTuple, Optional, Tuple
 
 assert sys.version_info >= (3, 6), "Only Python 3.6+ is supported."
 
 # People are creative with their reverts, and heuristics are a bit difficult.
-# Like 90% of of reverts have "This reverts commit ${full_sha}".
-# Some lack that entirely, while others have many of them specified in ad-hoc
-# ways, while others use short SHAs and whatever.
+# At a glance, most reverts have "This reverts commit ${full_sha}". Many others
+# have `Reverts llvm/llvm-project#${PR_NUMBER}`.
 #
-# The 90% case is trivial to handle (and 100% free + automatic). The extra 10%
-# starts involving human intervention, which is probably not worth it for now.
+# By their powers combined, we should be able to automatically catch something
+# like 80% of reverts with reasonable confidence. At some point, human
+# intervention will always be required (e.g., I saw
+# ```
+# This reverts commit ${commit_sha_1} and
+# also ${commit_sha_2_shorthand}
+# ```
+# during my sample)
+
+_CommitMessageReverts = NamedTuple(
+    "_CommitMessageReverts",
+    [
+        ("potential_shas", List[str]),
+        ("potential_pr_numbers", List[int]),
+    ],
+)
 
 
-def _try_parse_reverts_from_commit_message(commit_message: str) -> List[str]:
+def _try_parse_reverts_from_commit_message(
+    commit_message: str,
+) -> _CommitMessageReverts:
+    """Tries to parse revert SHAs and LLVM PR numbers form the commit message.
+
+    Returns:
+        A namedtuple containing:
+        - A list of potentially reverted SHAs
+        - A list of potentially reverted LLVM PR numbers
+    """
     if not commit_message:
-        return []
+        return _CommitMessageReverts([], [])
 
-    results = re.findall(
-        r"This reverts commit ([a-f0-9]{40})\b", commit_message
+    sha_reverts = re.findall(
+        r"This reverts commit ([a-f0-9]{40})\b",
+        commit_message,
     )
 
     first_line = commit_message.splitlines()[0]
     initial_revert = re.match(r'Revert ([a-f0-9]{6,}) "', first_line)
     if initial_revert:
-        results.append(initial_revert.group(1))
-    return results
+        sha_reverts.append(initial_revert.group(1))
+
+    pr_numbers = [
+        int(x)
+        for x in re.findall(
+            r"Reverts llvm/llvm-project#(\d+)",
+            commit_message,
+        )
+    ]
+
+    return _CommitMessageReverts(
+        potential_shas=sha_reverts,
+        potential_pr_numbers=pr_numbers,
+    )
 
 
-def _stream_stdout(command: List[str]) -> Generator[str, None, None]:
+def _stream_stdout(
+    command: List[str], cwd: Optional[str] = None
+) -> Generator[str, None, None]:
     with subprocess.Popen(
-        command, stdout=subprocess.PIPE, encoding="utf-8", errors="replace"
+        command,
+        cwd=cwd,
+        stdout=subprocess.PIPE,
+        encoding="utf-8",
+        errors="replace",
     ) as p:
         assert p.stdout is not None  # for mypy's happiness.
         yield from p.stdout
@@ -108,9 +148,7 @@ _LogEntry = NamedTuple(
 )
 
 
-def _log_stream(
-    git_dir: str, root_sha: str, end_at_sha: str
-) -> Iterable[_LogEntry]:
+def _log_stream(git_dir: str, root_sha: str, end_at_sha: str) -> Iterable[_LogEntry]:
     sep = 50 * "<>"
     log_command = [
         "git",
@@ -179,17 +217,55 @@ Revert = NamedTuple(
 
 
 def _find_common_parent_commit(git_dir: str, ref_a: str, ref_b: str) -> str:
-    """Finds the closest common parent commit between `ref_a` and `ref_b`."""
+    """Finds the closest common parent commit between `ref_a` and `ref_b`.
+
+    Returns:
+        A SHA. Note that `ref_a` will be returned if `ref_a` is a parent of
+        `ref_b`, and vice-versa.
+    """
     return subprocess.check_output(
         ["git", "-C", git_dir, "merge-base", ref_a, ref_b],
         encoding="utf-8",
     ).strip()
 
 
-def find_reverts(git_dir: str, across_ref: str, root: str) -> List[Revert]:
+def _load_pr_commit_mappings(
+    git_dir: str, root: str, min_ref: str
+) -> Dict[int, List[str]]:
+    git_log = ["git", "log", "--format=%H %s", f"{min_ref}..{root}"]
+    results = collections.defaultdict(list)
+    pr_regex = re.compile(r"\s\(#(\d+)\)$")
+    for line in _stream_stdout(git_log, cwd=git_dir):
+        m = pr_regex.search(line)
+        if not m:
+            continue
+
+        pr_number = int(m.group(1))
+        sha = line.split(None, 1)[0]
+        # N.B., these are kept in log (read: reverse chronological) order,
+        # which is what's expected by `find_reverts`.
+        results[pr_number].append(sha)
+    return results
+
+
+# N.B., max_pr_lookback's default of 20K commits is arbitrary, but should be
+# enough for the 99% case of reverts: rarely should someone land a cleanish
+# revert of a >6 month old change...
+def find_reverts(
+    git_dir: str, across_ref: str, root: str, max_pr_lookback: int = 20000
+) -> List[Revert]:
     """Finds reverts across `across_ref` in `git_dir`, starting from `root`.
 
     These reverts are returned in order of oldest reverts first.
+
+    Args:
+        git_dir: git directory to find reverts in.
+        across_ref: the ref to find reverts across.
+        root: the 'main' ref to look for reverts on.
+        max_pr_lookback: this function uses heuristics to map PR numbers to
+            SHAs. These heuristics require that commit history from `root` to
+            `some_parent_of_root` is loaded in memory. `max_pr_lookback` is how
+            many commits behind `across_ref` should be loaded in memory.
     """
     across_sha = _rev_parse(git_dir, across_ref)
     root_sha = _rev_parse(git_dir, root)
@@ -212,14 +288,45 @@ def find_reverts(git_dir: str, across_ref: str, root: str) -> List[Revert]:
     )
 
     all_reverts = []
+    # Lazily load PR <-> commit mappings, since it can be expensive.
+    pr_commit_mappings = None
     for sha, commit_message in _log_stream(git_dir, root_sha, across_sha):
-        reverts = _try_parse_reverts_from_commit_message(commit_message)
+        reverts, pr_reverts = _try_parse_reverts_from_commit_message(
+            commit_message,
+        )
+        if pr_reverts:
+            if pr_commit_mappings is None:
+                logging.info(
+                    "Loading PR <-> commit mappings. This may take a moment..."
+                )
+                pr_commit_mappings = _load_pr_commit_mappings(
+                    git_dir, root_sha, f"{across_sha}~{max_pr_lookback}"
+                )
+                logging.info(
+                    "Loaded %d PR <-> commit mappings", len(pr_commit_mappings)
+                )
+
+            for reverted_pr_number in pr_reverts:
+                reverted_shas = pr_commit_mappings.get(reverted_pr_number)
+                if not reverted_shas:
+                    logging.warning(
+                        "No SHAs for reverted PR %d (commit %s)",
+                        reverted_pr_number,
+                        sha,
+                    )
+                    continue
+                logging.debug(
+                    "Inferred SHAs %s for reverted PR %d (commit %s)",
+                    reverted_shas,
+                    reverted_pr_number,
+                    sha,
+                )
+                reverts.extend(reverted_shas)
+
         if not reverts:
             continue
 
-        resolved_reverts = sorted(
-            set(_resolve_sha(git_dir, x) for x in reverts)
-        )
+        resolved_reverts = sorted(set(_resolve_sha(git_dir, x) for x in reverts))
         for reverted_sha in resolved_reverts:
             if reverted_sha in intermediate_commits:
                 logging.debug(
@@ -245,16 +352,31 @@ def find_reverts(git_dir: str, across_ref: str, root: str) -> List[Revert]:
                 )
                 continue
 
-            if object_type == "commit":
-                all_reverts.append(Revert(sha, reverted_sha))
+            if object_type != "commit":
+                logging.error(
+                    "%s claims to revert the %s %s, which isn't a commit",
+                    sha,
+                    object_type,
+                    reverted_sha,
+                )
+                continue
+
+            # Rarely, reverts will cite SHAs on other branches (e.g., revert
+            # commit says it reverts a commit with SHA ${X}, but ${X} is not a
+            # parent of the revert). This can happen if e.g., the revert has
+            # been mirrored to another branch. Treat them the same as
+            # reverts of non-commits.
+            if _find_common_parent_commit(git_dir, sha, reverted_sha) != reverted_sha:
+                logging.error(
+                    "%s claims to revert %s, which is a commit that is not "
+                    "a parent of the revert",
+                    sha,
+                    reverted_sha,
+                )
                 continue
 
-            logging.error(
-                "%s claims to revert %s -- which isn't a commit -- %s",
-                sha,
-                object_type,
-                reverted_sha,
-            )
+            all_reverts.append(Revert(sha, reverted_sha))
+
 
     # Since `all_reverts` contains reverts in log order (e.g., newer comes before
     # older), we need to reverse this to keep with our guarantee of older =
@@ -265,18 +387,11 @@ def find_reverts(git_dir: str, across_ref: str, root: str) -> List[Revert]:
 
 def _main() -> None:
     parser = argparse.ArgumentParser(
-        description=__doc__,
-        formatter_class=argparse.RawDescriptionHelpFormatter,
-    )
-    parser.add_argument(
-        "base_ref", help="Git ref or sha to check for reverts around."
-    )
-    parser.add_argument(
-        "-C", "--git_dir", default=".", help="Git directory to use."
-    )
-    parser.add_argument(
-        "root", nargs="+", help="Root(s) to search for commits from."
+        description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter
     )
+    parser.add_argument("base_ref", help="Git ref or sha to check for reverts around.")
+    parser.add_argument("-C", "--git_dir", default=".", help="Git directory to use.")
+    parser.add_argument("root", nargs="+", help="Root(s) to search for commits from.")
     parser.add_argument("--debug", action="store_true")
     parser.add_argument(
         "-u",
@@ -303,17 +418,12 @@ def _main() -> None:
                 seen_reverts.add(revert)
                 all_reverts.append(revert)
 
+    sha_prefix = (
+        "https://github.com/llvm/llvm-project/commit/" if opts.review_url else ""
+    )
     for revert in all_reverts:
-        sha_fmt = (
-            f"https://reviews.llvm.org/rG{revert.sha}"
-            if opts.review_url
-            else revert.sha
-        )
-        reverted_sha_fmt = (
-            f"https://reviews.llvm.org/rG{revert.reverted_sha}"
-            if opts.review_url
-            else revert.reverted_sha
-        )
+        sha_fmt = f"{sha_prefix}{revert.sha}"
+        reverted_sha_fmt = f"{sha_prefix}{revert.reverted_sha}"
         print(f"{sha_fmt} claims to revert {reverted_sha_fmt}")
 
 
diff --git a/llvm_tools/setup_for_workon.py b/llvm_tools/setup_for_workon.py
deleted file mode 100755
index 8c67f816..00000000
--- a/llvm_tools/setup_for_workon.py
+++ /dev/null
@@ -1,300 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2024 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Sets up src/third_party/llvm-project for cros workon from an LLVM ebuild."""
-
-import argparse
-import dataclasses
-import logging
-from pathlib import Path
-import re
-import shlex
-import subprocess
-import sys
-from typing import List, Union
-
-import git_llvm_rev
-
-
-@dataclasses.dataclass(frozen=True)
-class LLVMSourceDir:
-    """An LLVM source dir, with convenient additional accessors."""
-
-    path: Path
-
-    def cros_workon_subdir(self):
-        """Returns the subdir used for communicating with the ebuild."""
-        return self.path / ".ebuild"
-
-
-def apply_patches(
-    llvm_dir: LLVMSourceDir,
-    patch_manager: Path,
-    patch_metadata_file: Path,
-    current_rev: git_llvm_rev.Rev,
-) -> None:
-    """Applies patches using `patch_manager` to `llvm_dir`."""
-    subprocess.run(
-        [
-            patch_manager,
-            f"--svn_version={current_rev.number}",
-            f"--src_path={llvm_dir.path}",
-            f"--patch_metadata_file={patch_metadata_file}",
-        ],
-        check=True,
-        stdin=subprocess.DEVNULL,
-    )
-
-
-def find_ebuild_in_dir(ebuild_dir: Path) -> Path:
-    """Returns the path to a 9999 ebuild in `ebuild_dir`; raises if none."""
-    candidates = list(ebuild_dir.glob("*-9999.ebuild"))
-    if len(candidates) != 1:
-        raise ValueError(
-            f"Expected exactly one 9999 ebuild in {ebuild_dir}; found "
-            f"{candidates}"
-        )
-    return candidates[0]
-
-
-def write_gentoo_cmake_hack(llvm_dir: LLVMSourceDir, ebuild_dir: Path) -> None:
-    """Modifies cmake files in LLVM so cmake.eclass doesn't modify them."""
-    # Upstream's `cmake.eclass` will try to override "dangerous" configurations
-    # that override Gentoo settings. There's no way to skip this override, but
-    # it _does_ have logic to detect if it has already run & skips all
-    # modifications in that case. Since LLVM has no such "dangerous" settings,
-    # and the `9999` ebuild never "goes live," it's safe to skip these.
-
-    # The file to modify is the 'main' cmake file, which is determined based on
-    # `CMAKE_USE_DIR`. Parsing that out isn't _too_ painful, so try it.
-    ebuild_path = find_ebuild_in_dir(ebuild_dir)
-    ebuild_contents = ebuild_path.read_text(encoding="utf-8")
-    cmake_use_dir_re = re.compile(
-        # Use string concatenation rather than re.VERBOSE, since this regex
-        # goes in an error message on failure, and that's _really_ hard to
-        # read.
-        r"^\s*"
-        # While these all use `export`, it's not strictly required by
-        # cmake.eclass.
-        r"(?:export\s+)?" r'CMAKE_USE_DIR="\$\{S\}/([^"]+)"',
-        re.MULTILINE,
-    )
-    cmake_use_dirs = cmake_use_dir_re.findall(ebuild_contents)
-    if len(cmake_use_dirs) != 1:
-        raise ValueError(
-            f"Expected to find 1 match of {cmake_use_dir_re} in "
-            f"{ebuild_path}; found {len(cmake_use_dirs)}"
-        )
-
-    cmake_file = llvm_dir.path / cmake_use_dirs[0] / "CMakeLists.txt"
-    special_marker = "<<< Gentoo configuration >>>"
-    if special_marker in cmake_file.read_text(encoding="utf-8"):
-        return
-
-    with cmake_file.open("a", encoding="utf-8") as f:
-        f.write(f"\n# HACK from setup_from_workon.py:\n# {special_marker}")
-
-
-def write_patch_application_stamp(
-    llvm_dir: LLVMSourceDir, package_name: str
-) -> None:
-    """Writes a stamp file to note that patches have been applied."""
-    stamp_path = (
-        llvm_dir.cros_workon_subdir()
-        / "stamps"
-        / "patches_applied"
-        / package_name
-    )
-    stamp_path.parent.mkdir(parents=True, exist_ok=True)
-    stamp_path.touch()
-
-
-def main(argv: List[str]) -> None:
-    logging.basicConfig(
-        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
-        "%(message)s",
-        level=logging.INFO,
-    )
-
-    my_dir = Path(__file__).resolve().parent
-    parser = argparse.ArgumentParser(
-        description=__doc__,
-        formatter_class=argparse.RawDescriptionHelpFormatter,
-    )
-    parser.add_argument(
-        "--llvm-dir",
-        type=lambda x: LLVMSourceDir(path=Path(x)),
-        default=LLVMSourceDir(path=my_dir.parent.parent / "llvm-project"),
-        help="Path containing a directory with llvm sources.",
-    )
-    parser.add_argument(
-        "--ebuild-dir",
-        type=Path,
-        help="""
-        Directory of the ebuild we're trying to set up. If this isn't
-        specified, `--package` should be specified, and this will be
-        autodetected. Example: ${cros_overlay}/sys-devel/llvm.
-        """,
-    )
-    parser.add_argument(
-        "--clean-llvm",
-        action="store_true",
-        help="""
-        If passed, a series of commands will be run to reset the LLVM directory
-        to HEAD prior to applying patches. **This flag deletes all staged
-        unstaged changes, and deletes all untracked files**.
-        """,
-    )
-    parser.add_argument(
-        "--package",
-        help="""
-        Name of the package to set up for, in the form '${CATEGORY}/${PN}'.
-        This must be provided unless `--ebuild-dir` is provided. Example:
-        sys-devel/llvm.
-        """,
-    )
-    parser.add_argument(
-        "--no-commit",
-        dest="commit",
-        action="store_false",
-        help="Don't create a commit with all changes applied.",
-    )
-    parser.add_argument(
-        "--workon-board",
-        dest="workon_board",
-        help="""
-        Ensure cros workon for the given board after applying changes.
-        Set to 'host' for working on the host system, and not a board.
-        """,
-    )
-
-    checkout_group = parser.add_mutually_exclusive_group(required=True)
-    checkout_group.add_argument(
-        "--checkout",
-        help="""
-        If specified, the llvm directory will be checked out to the given SHA.
-        """,
-    )
-    # The value of this isn't used anywhere; it's just used as an explicit
-    # nudge for checking LLVM out.
-    checkout_group.add_argument(
-        "--no-checkout",
-        action="store_true",
-        help="""
-        Don't check llvm-project out to anything special before running. Useful
-        if you'd like to, for example, workon multiple LLVM projects
-        simultaneously and have already called `setup_for_workon` on another
-        one.
-        """,
-    )
-    opts = parser.parse_args(argv)
-
-    ebuild_dir = opts.ebuild_dir
-    package_name = opts.package
-    if not ebuild_dir and not package_name:
-        parser.error(
-            "At least one of --ebuild-dir or --package must be specified."
-        )
-
-    if not ebuild_dir:
-        # All of these are in chromiumos-overlay, so just use that as a basis.
-        ebuild_dir = my_dir.parent.parent / "chromiumos-overlay" / package_name
-        logging.info("Ebuild directory is %s.", ebuild_dir)
-    elif not package_name:
-        package_name = f"{ebuild_dir.parent.name}/{ebuild_dir.name}"
-        logging.info("Package is %s.", package_name)
-
-    git_housekeeping_commands: List[List[Union[Path, str]]] = []
-    if opts.clean_llvm:
-        git_housekeeping_commands += (
-            ["git", "clean", "-fd", "."],
-            ["git", "reset", "--hard", "HEAD"],
-        )
-
-    if opts.checkout is not None:
-        git_housekeeping_commands.append(
-            ["git", "checkout", "--quiet", opts.checkout],
-        )
-
-    for cmd in git_housekeeping_commands:
-        subprocess.run(
-            cmd,
-            cwd=opts.llvm_dir.path,
-            check=True,
-            stdin=subprocess.DEVNULL,
-        )
-
-    rev = git_llvm_rev.translate_sha_to_rev(
-        git_llvm_rev.LLVMConfig(
-            remote="cros",
-            dir=opts.llvm_dir.path,
-        ),
-        subprocess.run(
-            ["git", "rev-parse", "HEAD"],
-            check=True,
-            cwd=opts.llvm_dir.path,
-            stdin=subprocess.DEVNULL,
-            encoding="utf-8",
-            stdout=subprocess.PIPE,
-        ).stdout.strip(),
-    )
-
-    logging.info("Applying patches...")
-    files_dir = ebuild_dir / "files"
-    apply_patches(
-        opts.llvm_dir,
-        patch_manager=files_dir / "patch_manager" / "patch_manager.py",
-        patch_metadata_file=files_dir / "PATCHES.json",
-        current_rev=rev,
-    )
-    write_patch_application_stamp(opts.llvm_dir, package_name)
-    write_gentoo_cmake_hack(opts.llvm_dir, ebuild_dir)
-
-    if opts.commit:
-        subprocess.run(
-            ["git", "add", "."],
-            check=True,
-            cwd=opts.llvm_dir.path,
-            stdin=subprocess.DEVNULL,
-        )
-        subprocess.run(
-            [
-                "git",
-                "commit",
-                "--message",
-                "Patches applied and markers added.",
-            ],
-            check=True,
-            cwd=opts.llvm_dir.path,
-            stdin=subprocess.DEVNULL,
-        )
-
-    if not opts.workon_board:
-        logging.warning(
-            "Didn't ensure 'workon' for any board or host."
-            " Make sure you've called 'cros workon [...] start %s'"
-            " before building!",
-            package_name,
-        )
-        return
-
-    if opts.workon_board == "host":
-        cmd = ["cros", "workon", "--host", "start", package_name]
-    else:
-        cmd = [
-            "cros",
-            "workon",
-            f"-b={opts.workon_board}",
-            "start",
-            package_name,
-        ]
-    subprocess.run(cmd, check=True, stdin=subprocess.DEVNULL)
-    logging.info(
-        "Successfully workon-ed: %s", shlex.join([str(c) for c in cmd])
-    )
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/llvm_tools/stabilize_all_llvm_packages.py b/llvm_tools/stabilize_all_llvm_packages.py
old mode 100755
new mode 100644
index 40ca29f5..23fcf472
--- a/llvm_tools/stabilize_all_llvm_packages.py
+++ b/llvm_tools/stabilize_all_llvm_packages.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -19,18 +18,15 @@ Run this from inside of the chroot.
 import argparse
 import contextlib
 import logging
-from pathlib import Path
 import subprocess
 import sys
 from typing import List
 
-import chroot
-import get_upstream_patch
-import manifest_utils
-import patch_utils
-
-
-CROS_SOURCE_ROOT = Path("/mnt/host/source")
+from cros_utils import cros_paths
+from llvm_tools import chroot
+from llvm_tools import get_llvm_hash
+from llvm_tools import manifest_utils
+from llvm_tools import patch_utils
 
 
 @contextlib.contextmanager
@@ -39,7 +35,7 @@ def llvm_checked_out_to(checkout_sha: str):
 
     Restores LLVM to the prior SHA when exited.
     """
-    llvm_dir = CROS_SOURCE_ROOT / manifest_utils.LLVM_PROJECT_PATH
+    llvm_dir = cros_paths.CHROOT_SOURCE_ROOT / manifest_utils.LLVM_PROJECT_PATH
     original_sha = subprocess.run(
         ["git", "rev-parse", "HEAD"],
         check=True,
@@ -88,11 +84,10 @@ def llvm_checked_out_to(checkout_sha: str):
 
 
 def resolve_llvm_sha(llvm_next: bool) -> str:
-    sys_devel_llvm = (
-        CROS_SOURCE_ROOT / "src/third_party/chromiumos-overlay/sys-devel/llvm"
-    )
-    sha = "llvm-next" if llvm_next else "llvm"
-    return get_upstream_patch.resolve_symbolic_sha(sha, str(sys_devel_llvm))
+    hash_obj = get_llvm_hash.LLVMHash()
+    if llvm_next:
+        return hash_obj.GetCrOSLLVMNextHash()
+    return hash_obj.GetCrOSCurrentLLVMHash(cros_paths.CHROOT_SOURCE_ROOT)
 
 
 def main(argv: List[str]) -> None:
@@ -120,10 +115,12 @@ def main(argv: List[str]) -> None:
     desired_sha = resolve_llvm_sha(opts.llvm_next)
 
     with llvm_checked_out_to(desired_sha):
-        packages_to_stabilize = patch_utils.CHROMEOS_PATCHES_JSON_PACKAGES
+        packages_to_stabilize = patch_utils.CHROMEOS_LLVM_SUBPACKAGES
         logging.info("Stabilizing %s...", ", ".join(packages_to_stabilize))
 
-        cros_overlay = CROS_SOURCE_ROOT / "src/third_party/chromiumos-overlay"
+        cros_overlay = (
+            cros_paths.CHROOT_SOURCE_ROOT / cros_paths.CHROMIUMOS_OVERLAY
+        )
         return_code = subprocess.run(
             [
                 "cros_mark_as_stable",
@@ -135,7 +132,3 @@ def main(argv: List[str]) -> None:
             stdin=subprocess.DEVNULL,
         ).returncode
         sys.exit(return_code)
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/llvm_tools/test_helpers.py b/llvm_tools/test_helpers.py
index 44d56f82..2caac16f 100644
--- a/llvm_tools/test_helpers.py
+++ b/llvm_tools/test_helpers.py
@@ -5,9 +5,13 @@
 """Helper functions for unit testing."""
 
 import contextlib
+import inspect
 import json
 import os
+from pathlib import Path
+import shutil
 import tempfile
+import unittest
 
 
 class ArgsOutputTest:
@@ -84,3 +88,14 @@ def CreateTemporaryFile(suffix=""):
     finally:
         if os.path.isfile(temp_file_path):
             os.remove(temp_file_path)
+
+
+class TempDirTestCase(unittest.TestCase):
+    """Subclass for test-cases. Provides a `make_tempdir()` function."""
+
+    def make_tempdir(self) -> Path:
+        defining_file = Path(inspect.getfile(self.__class__))
+        test_file_name = Path(defining_file).with_suffix("").name
+        tempdir = Path(tempfile.mkdtemp(prefix=test_file_name + "_"))
+        self.addCleanup(shutil.rmtree, tempdir)
+        return tempdir
diff --git a/llvm_tools/update_chromeos_llvm_hash.py b/llvm_tools/update_chromeos_llvm_hash.py
deleted file mode 100755
index f6161773..00000000
--- a/llvm_tools/update_chromeos_llvm_hash.py
+++ /dev/null
@@ -1,921 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Updates the LLVM hash and uprevs the build of the specified packages.
-
-For each package, a temporary repo is created and the changes are uploaded
-for review.
-"""
-
-import argparse
-import dataclasses
-import enum
-import os
-from pathlib import Path
-import re
-import subprocess
-import textwrap
-from typing import Dict, Iterable, Iterator, List, Optional, Union
-
-import atomic_write_file
-import chroot
-import failure_modes
-import get_llvm_hash
-import git
-import manifest_utils
-import patch_utils
-import subprocess_helpers
-
-
-# Default list of packages to update.
-DEFAULT_PACKAGES = patch_utils.CHROMEOS_PATCHES_JSON_PACKAGES
-
-DEFAULT_MANIFEST_PACKAGES = ["sys-devel/llvm"]
-
-
-# Specify which LLVM hash to update
-class LLVMVariant(enum.Enum):
-    """Represent the LLVM hash in an ebuild file to update."""
-
-    current = "LLVM_HASH"
-    next = "LLVM_NEXT_HASH"
-
-
-@dataclasses.dataclass(frozen=True, eq=True)
-class ChrootOpts:
-    """A class that holds chroot options."""
-
-    chromeos_root: Path
-    chroot_name: str = "chroot"
-    out_name: str = "out"
-
-
-class PortagePackage:
-    """Represents a portage package with location info."""
-
-    def __init__(self, chroot_opts: ChrootOpts, package: str):
-        """Create a new PortagePackage.
-
-        Args:
-            chroot_opts: options that specify the ChromeOS chroot to use.
-            package: "category/package" string.
-        """
-        self.package = package
-        potential_ebuild_path = PortagePackage.find_package_ebuild(
-            chroot_opts, package
-        )
-        if potential_ebuild_path.is_symlink():
-            self.uprev_target: Optional[Path] = potential_ebuild_path.absolute()
-            self.ebuild_path = potential_ebuild_path.resolve()
-        else:
-            # Should have a 9999 ebuild, no uprevs needed.
-            self.uprev_target = None
-            self.ebuild_path = potential_ebuild_path.absolute()
-
-    @staticmethod
-    def find_package_ebuild(chroot_opts: ChrootOpts, package: str) -> Path:
-        """Look up the package's ebuild location."""
-        chromeos_root_str = str(chroot_opts.chromeos_root)
-        ebuild_paths = chroot.GetChrootEbuildPaths(
-            chromeos_root_str,
-            [package],
-            chroot_opts.chroot_name,
-            chroot_opts.out_name,
-        )
-        converted = chroot.ConvertChrootPathsToAbsolutePaths(
-            chromeos_root_str, ebuild_paths
-        )[0]
-        return Path(converted)
-
-    def package_dir(self) -> Path:
-        """Return the package directory."""
-        return self.ebuild_path.parent
-
-    def update(
-        self, llvm_variant: LLVMVariant, git_hash: str, svn_version: int
-    ):
-        """Update the package with the new LLVM git sha and revision.
-
-        Args:
-            llvm_variant: Which LLVM hash to update.
-                Either LLVM_HASH or LLVM_NEXT_HASH.
-            git_hash: Upstream LLVM git hash to update to.
-            svn_version: Matching LLVM revision string for the git_hash.
-        """
-        live_ebuild = self.live_ebuild()
-        if live_ebuild:
-            # Working with a -9999 ebuild package here, no
-            # upreving.
-            UpdateEbuildLLVMHash(
-                live_ebuild, llvm_variant, git_hash, svn_version
-            )
-            return
-        if not self.uprev_target:
-            # We can exit early if we're not working with a live ebuild,
-            # and we don't have something to uprev.
-            raise RuntimeError(
-                "Cannot update: no live ebuild or symlink found"
-                f" for {self.package}"
-            )
-
-        UpdateEbuildLLVMHash(
-            self.ebuild_path, llvm_variant, git_hash, svn_version
-        )
-        if llvm_variant == LLVMVariant.current:
-            UprevEbuildToVersion(str(self.uprev_target), svn_version, git_hash)
-        else:
-            UprevEbuildSymlink(str(self.uprev_target))
-
-    def live_ebuild(self) -> Optional[Path]:
-        """Path to the live ebuild if it exists.
-
-        Returns:
-            The patch to the live ebuild if it exists. None otherwise.
-        """
-        matches = self.package_dir().glob("*-9999.ebuild")
-        return next(matches, None)
-
-
-def defaultCrosRoot() -> Path:
-    """Get default location of chromeos_path.
-
-    The logic assumes that the cros_root is ~/chromiumos, unless llvm_tools is
-    inside of a CrOS checkout, in which case that checkout should be used.
-
-    Returns:
-        The best guess location for the cros checkout.
-    """
-    llvm_tools_path = os.path.realpath(os.path.dirname(__file__))
-    if llvm_tools_path.endswith("src/third_party/toolchain-utils/llvm_tools"):
-        return Path(llvm_tools_path).parent.parent.parent.parent
-    return Path.home() / "chromiumos"
-
-
-def GetCommandLineArgs():
-    """Parses the command line for the optional command line arguments.
-
-    Returns:
-        The log level to use when retrieving the LLVM hash or google3 LLVM
-        version, the chroot path to use for executing chroot commands, a list
-        of a package or packages to update their LLVM next hash, and the LLVM
-        version to use when retrieving the LLVM hash.
-    """
-
-    # Create parser and add optional command-line arguments.
-    parser = argparse.ArgumentParser(
-        description="Updates the build's hash for llvm-next."
-    )
-
-    # Add argument for a specific chroot path.
-    parser.add_argument(
-        "--chromeos_path",
-        type=Path,
-        default=defaultCrosRoot(),
-        help="the path to the chroot (default: %(default)s)",
-    )
-
-    # Add argument for specific builds to uprev and update their llvm-next
-    # hash.
-    parser.add_argument(
-        "--update_packages",
-        default=",".join(DEFAULT_PACKAGES),
-        help="Comma-separated ebuilds to update llvm-next hash for "
-        "(default: %(default)s)",
-    )
-
-    parser.add_argument(
-        "--manifest_packages",
-        default="",
-        help="Comma-separated ebuilds to update manifests for "
-        "(default: %(default)s)",
-    )
-
-    # Add argument for the LLVM hash to update
-    parser.add_argument(
-        "--is_llvm_next",
-        action="store_true",
-        help="which llvm hash to update. If specified, update LLVM_NEXT_HASH. "
-        "Otherwise, update LLVM_HASH",
-    )
-
-    # Add argument for the LLVM version to use.
-    parser.add_argument(
-        "--llvm_version",
-        type=get_llvm_hash.IsSvnOption,
-        required=True,
-        help="which git hash to use. Either a svn revision, or one "
-        f"of {sorted(get_llvm_hash.KNOWN_HASH_SOURCES)}",
-    )
-
-    # Add argument for the mode of the patch management when handling patches.
-    parser.add_argument(
-        "--failure_mode",
-        default=failure_modes.FailureModes.FAIL.value,
-        choices=[
-            failure_modes.FailureModes.FAIL.value,
-            failure_modes.FailureModes.CONTINUE.value,
-            failure_modes.FailureModes.DISABLE_PATCHES.value,
-        ],
-        help="the mode of the patch manager when handling failed patches "
-        "(default: %(default)s)",
-    )
-
-    # Add argument for the patch metadata file.
-    parser.add_argument(
-        "--patch_metadata_file",
-        default="PATCHES.json",
-        help="the .json file that has all the patches and their "
-        "metadata if applicable (default: PATCHES.json inside $FILESDIR)",
-    )
-    parser.add_argument(
-        "--no_repo_manifest",
-        dest="repo_manifest",
-        action="store_false",
-        help="Skip updating the llvm-project revision attribute"
-        " in the internal manifest.",
-    )
-    parser.add_argument(
-        "--no_delete_branch",
-        action="store_true",
-        help="Do not delete the created overlay branch.",
-    )
-    parser.add_argument(
-        "--no_upload_changes",
-        action="store_true",
-        help="Do not upload changes to gerrit.",
-    )
-    parser.add_argument(
-        "--no_patching",
-        action="store_true",
-        help="Do not check or update PATCHES.json.",
-    )
-    # Parse the command line.
-    return parser.parse_args()
-
-
-def UpdateEbuildLLVMHash(
-    ebuild_path: Path,
-    llvm_variant: LLVMVariant,
-    git_hash: str,
-    svn_version: int,
-) -> None:
-    """Updates the LLVM hash in the ebuild.
-
-    The build changes are staged for commit in the temporary repo.
-
-    Args:
-        ebuild_path: The absolute path to the ebuild.
-        llvm_variant: Which LLVM hash to update.
-        git_hash: The new git hash.
-        svn_version: The SVN-style revision number of git_hash.
-
-    Raises:
-        ValueError: Invalid ebuild path provided or failed to stage the commit
-        of the changes or failed to update the LLVM hash.
-    """
-
-    # For each ebuild, read the file in
-    # advance and then create a temporary file
-    # that gets updated with the new LLVM hash
-    # and revision number and then the ebuild file
-    # gets updated to the temporary file.
-    if not os.path.isfile(ebuild_path):
-        raise ValueError(f"Invalid ebuild path provided: {ebuild_path}")
-
-    with open(ebuild_path, encoding="utf-8") as ebuild_file:
-        new_lines = list(
-            ReplaceLLVMHash(ebuild_file, llvm_variant, git_hash, svn_version)
-        )
-    with atomic_write_file.atomic_write(
-        ebuild_path, "w", encoding="utf-8"
-    ) as ebuild_file:
-        ebuild_file.writelines(new_lines)
-    # Stage the changes.
-    subprocess.check_output(
-        ["git", "-C", ebuild_path.parent, "add", ebuild_path]
-    )
-
-
-def ReplaceLLVMHash(
-    ebuild_lines: Iterable[str],
-    llvm_variant: LLVMVariant,
-    git_hash: str,
-    svn_version: int,
-) -> Iterator[str]:
-    """Updates the LLVM git hash.
-
-    Args:
-        ebuild_lines: The contents of the ebuild file.
-        llvm_variant: The LLVM hash to update.
-        git_hash: The new git hash.
-        svn_version: The SVN-style revision number of git_hash.
-
-    Yields:
-        lines of the modified ebuild file
-    """
-    is_updated = False
-    llvm_regex = re.compile(
-        "^" + re.escape(llvm_variant.value) + '="[a-z0-9]+"'
-    )
-    for cur_line in ebuild_lines:
-        if not is_updated and llvm_regex.search(cur_line):
-            # Update the git hash and revision number.
-            cur_line = f'{llvm_variant.value}="{git_hash}" # r{svn_version}\n'
-
-            is_updated = True
-
-        yield cur_line
-
-    if not is_updated:
-        raise ValueError(f"Failed to update {llvm_variant.value}")
-
-
-def UprevEbuildSymlink(symlink: str) -> None:
-    """Uprevs the symlink's revision number.
-
-    Increases the revision number by 1 and stages the change in
-    the temporary repo.
-
-    Args:
-        symlink: The absolute path of an ebuild symlink.
-
-    Raises:
-        ValueError: Failed to uprev the symlink or failed to stage the changes.
-    """
-
-    if not os.path.islink(symlink):
-        raise ValueError(f"Invalid symlink provided: {symlink}")
-
-    new_symlink, is_changed = re.subn(
-        r"r([0-9]+).ebuild",
-        lambda match: "r%s.ebuild" % str(int(match.group(1)) + 1),
-        symlink,
-        count=1,
-    )
-
-    if not is_changed:
-        raise ValueError("Failed to uprev the symlink.")
-
-    # rename the symlink
-    subprocess.check_output(
-        ["git", "-C", os.path.dirname(symlink), "mv", symlink, new_symlink]
-    )
-
-
-def UprevEbuildToVersion(symlink: str, svn_version: int, git_hash: str) -> None:
-    """Uprevs the ebuild's revision number.
-
-    Increases the revision number by 1 and stages the change in
-    the temporary repo.
-
-    Args:
-        symlink: The absolute path of an ebuild symlink.
-        svn_version: The SVN-style revision number of git_hash.
-        git_hash: The new git hash.
-
-    Raises:
-        ValueError: Failed to uprev the ebuild or failed to stage the changes.
-        AssertionError: No llvm version provided for an LLVM uprev
-    """
-
-    if not os.path.islink(symlink):
-        raise ValueError(f"Invalid symlink provided: {symlink}")
-
-    ebuild = os.path.realpath(symlink)
-    llvm_major_version = get_llvm_hash.GetLLVMMajorVersion(git_hash)
-    # llvm
-    package = os.path.basename(os.path.dirname(symlink))
-    if not package:
-        raise ValueError("Tried to uprev an unknown package")
-    if package == "llvm":
-        new_ebuild, is_changed = re.subn(
-            r"(\d+)\.(\d+)_pre([0-9]+)(_p[0-9]+)?",
-            "%s.\\2_pre%s"
-            % (
-                llvm_major_version,
-                str(svn_version),
-            ),
-            ebuild,
-            count=1,
-        )
-    # any other package
-    else:
-        new_ebuild, is_changed = re.subn(
-            r"(\d+)\.(\d+)_pre([0-9]+)",
-            "%s.\\2_pre%s" % (llvm_major_version, str(svn_version)),
-            ebuild,
-            count=1,
-        )
-
-    if not is_changed:  # failed to increment the revision number
-        raise ValueError("Failed to uprev the ebuild.")
-
-    symlink_dir = os.path.dirname(symlink)
-
-    # Rename the ebuild
-    subprocess.check_output(
-        ["git", "-C", symlink_dir, "mv", ebuild, new_ebuild]
-    )
-
-    # Create a symlink of the renamed ebuild
-    new_symlink = new_ebuild[: -len(".ebuild")] + "-r1.ebuild"
-    subprocess.check_output(["ln", "-s", "-r", new_ebuild, new_symlink])
-    subprocess.check_output(["git", "-C", symlink_dir, "add", new_symlink])
-    # Remove the old symlink
-    subprocess.check_output(["git", "-C", symlink_dir, "rm", symlink])
-
-
-def RemovePatchesFromFilesDir(patches: Iterable[str]) -> None:
-    """Removes the patches from $FILESDIR of a package.
-
-    Args:
-        patches: A list of absolute paths of patches to remove
-
-    Raises:
-        ValueError: Failed to remove a patch in $FILESDIR.
-    """
-
-    for patch in patches:
-        subprocess.check_output(
-            ["git", "-C", os.path.dirname(patch), "rm", "-f", patch]
-        )
-
-
-def StagePatchMetadataFileForCommit(patch_metadata_file_path: str) -> None:
-    """Stages the updated patch metadata file for commit.
-
-    Args:
-        patch_metadata_file_path: The absolute path to the patch metadata file.
-
-    Raises:
-        ValueError: Failed to stage the patch metadata file for commit or
-        invalid patch metadata file.
-    """
-
-    if not os.path.isfile(patch_metadata_file_path):
-        raise ValueError(
-            f"Invalid patch metadata file provided: {patch_metadata_file_path}"
-        )
-
-    # Cmd to stage the patch metadata file for commit.
-    subprocess.check_output(
-        [
-            "git",
-            "-C",
-            os.path.dirname(patch_metadata_file_path),
-            "add",
-            patch_metadata_file_path,
-        ]
-    )
-
-
-def StagePackagesPatchResultsForCommit(
-    package_info_dict: Dict[str, patch_utils.PatchInfo],
-    commit_messages: List[str],
-) -> List[str]:
-    """Stages the patch results of the packages to the commit message.
-
-    Args:
-        package_info_dict: A dictionary where the key is the package name and
-        the value is a dictionary that contains information about the patches
-        of the package (key).
-        commit_messages: The commit message that has the updated ebuilds and
-        upreving information.
-
-    Returns:
-        commit_messages with new additions
-    """
-
-    # For each package, check if any patches for that package have
-    # changed, if so, add which patches have changed to the commit
-    # message.
-    for package_name, patch_info in package_info_dict.items():
-        if (
-            patch_info.disabled_patches
-            or patch_info.removed_patches
-            or patch_info.modified_metadata
-        ):
-            cur_package_header = f"\nFor the package {package_name}:"
-            commit_messages.append(cur_package_header)
-
-        # Add to the commit message that the patch metadata file was modified.
-        if patch_info.modified_metadata:
-            patch_metadata_path = patch_info.modified_metadata
-            metadata_file_name = os.path.basename(patch_metadata_path)
-            commit_messages.append(
-                f"The patch metadata file {metadata_file_name} was modified"
-            )
-
-            StagePatchMetadataFileForCommit(patch_metadata_path)
-
-        # Add each disabled patch to the commit message.
-        if patch_info.disabled_patches:
-            commit_messages.append("The following patches were disabled:")
-
-            for patch_path in patch_info.disabled_patches:
-                commit_messages.append(os.path.basename(patch_path))
-
-        # Add each removed patch to the commit message.
-        if patch_info.removed_patches:
-            commit_messages.append("The following patches were removed:")
-
-            for patch_path in patch_info.removed_patches:
-                commit_messages.append(os.path.basename(patch_path))
-
-            RemovePatchesFromFilesDir(patch_info.removed_patches)
-
-    return commit_messages
-
-
-def UpdatePortageManifests(
-    packages: Iterable[str], chromeos_path: Path
-) -> None:
-    """Updates portage manifest files for packages.
-
-    Args:
-        packages: A list of packages to update manifests for.
-        chromeos_path: The absolute path to the chromeos checkout.
-
-    Raises:
-        CalledProcessError: ebuild failed to update manifest.
-    """
-    manifest_ebuilds = chroot.GetChrootEbuildPaths(chromeos_path, packages)
-    for ebuild_path in manifest_ebuilds:
-        ebuild_dir = os.path.dirname(ebuild_path)
-        subprocess_helpers.ChrootRunCommand(
-            chromeos_path, ["ebuild", ebuild_path, "manifest"]
-        )
-        subprocess_helpers.ChrootRunCommand(
-            chromeos_path, ["git", "-C", ebuild_dir, "add", "Manifest"]
-        )
-
-
-def UpdatePackages(
-    packages: Iterable[str],
-    manifest_packages: Iterable[str],
-    llvm_variant: LLVMVariant,
-    git_hash: str,
-    svn_version: int,
-    chroot_opts: ChrootOpts,
-    mode: Optional[failure_modes.FailureModes],
-    git_hash_source: Union[int, str],
-    extra_commit_msg_lines: Optional[Iterable[str]],
-    delete_branch: bool = True,
-    upload_changes: bool = True,
-    wip: bool = False,
-) -> Optional[git.CommitContents]:
-    """Updates an LLVM hash and uprevs the ebuild of the packages.
-
-    A temporary repo is created for the changes. The changes are
-    then uploaded for review.
-
-    Args:
-        packages: A list of all the packages that are going to be updated.
-        manifest_packages: A list of packages to update manifests for.
-        llvm_variant: The LLVM hash to update.
-        git_hash: The new git hash.
-        svn_version: The SVN-style revision number of git_hash.
-        chroot_opts: options that specify the ChromeOS chroot to use.
-        mode: The mode of the patch manager when handling an applicable patch.
-          If None is passed, the patch manager won't be invoked.
-        that failed to apply.
-            Ex. 'FailureModes.FAIL'
-        git_hash_source: The source of which git hash to use based off of.
-            Ex. 'google3', 'tot', or <version> such as 365123
-        extra_commit_msg_lines: extra lines to append to the commit message.
-            Newlines are added automatically.
-        delete_branch: Delete the git branch as a final step.
-        upload_changes: Upload the commit to gerrit as a CL.
-        wip: if True, any changes uploaded will be uploaded as
-            work-in-progress.
-
-    Returns:
-        If upload_changes is set, a git.CommitContents object. Otherwise None.
-    """
-    portage_packages = (PortagePackage(chroot_opts, pkg) for pkg in packages)
-    chromiumos_overlay_path = (
-        chroot_opts.chromeos_root / "src" / "third_party" / "chromiumos-overlay"
-    )
-    branch_name = "update-" + llvm_variant.value + "-" + git_hash
-
-    commit_message_header = "llvm"
-    if llvm_variant == LLVMVariant.next:
-        commit_message_header = "llvm-next"
-    if git_hash_source in get_llvm_hash.KNOWN_HASH_SOURCES:
-        commit_message_header += (
-            f"/{git_hash_source}: upgrade to {git_hash} (r{svn_version})"
-        )
-    else:
-        commit_message_header += f": upgrade to {git_hash} (r{svn_version})"
-
-    commit_lines = [
-        commit_message_header + "\n",
-        "The following packages have been updated:",
-    ]
-
-    # Holds the list of packages that are updating.
-    updated_packages: List[str] = []
-    change_list = None
-    git.CreateBranch(chromiumos_overlay_path, branch_name)
-    try:
-        for pkg in portage_packages:
-            pkg.update(llvm_variant, git_hash, svn_version)
-            updated_packages.append(pkg.package)
-            commit_lines.append(pkg.package)
-        if manifest_packages:
-            UpdatePortageManifests(manifest_packages, chroot_opts.chromeos_root)
-            commit_lines.append("Updated manifest for:")
-            commit_lines.extend(manifest_packages)
-        EnsurePackageMaskContains(chroot_opts.chromeos_root, git_hash)
-        # Handle the patches for each package.
-        if mode is not None:
-            package_info_dict = UpdatePackagesPatchMetadataFile(
-                chroot_opts, svn_version, updated_packages, mode
-            )
-            # Update the commit message if changes were made to a package's
-            # patches.
-            commit_lines = StagePackagesPatchResultsForCommit(
-                package_info_dict, commit_lines
-            )
-        if extra_commit_msg_lines:
-            commit_lines.extend(extra_commit_msg_lines)
-        git.CommitChanges(chromiumos_overlay_path, commit_lines)
-        if upload_changes:
-            change_list = git.UploadChanges(
-                chromiumos_overlay_path,
-                branch_name,
-                wip=wip,
-            )
-    finally:
-        if delete_branch:
-            git.DeleteBranch(chromiumos_overlay_path, branch_name)
-        else:
-            print(f"Not deleting branch {branch_name}")
-    return change_list
-
-
-def EnsurePackageMaskContains(
-    chromeos_path: Union[Path, str], git_hash: str
-) -> None:
-    """Adds the major version of llvm to package.mask if not already present.
-
-    Args:
-        chromeos_path: The absolute path to the chromeos checkout.
-        git_hash: The new git hash.
-
-    Raises:
-        FileExistsError: package.mask not found in ../../chromiumos-overlay
-    """
-
-    llvm_major_version = get_llvm_hash.GetLLVMMajorVersion(git_hash)
-
-    overlay_dir = os.path.join(
-        chromeos_path, "src/third_party/chromiumos-overlay"
-    )
-    mask_path = os.path.join(
-        overlay_dir, "profiles/targets/chromeos/package.mask"
-    )
-    with open(mask_path, "r+", encoding="utf-8") as mask_file:
-        mask_contents = mask_file.read()
-        expected_line = f"=sys-devel/llvm-{llvm_major_version}.0_pre*\n"
-        if expected_line not in mask_contents:
-            mask_file.write(expected_line)
-
-    subprocess.check_output(["git", "-C", overlay_dir, "add", mask_path])
-
-
-def UpdatePackagesPatchMetadataFile(
-    chroot_opts: ChrootOpts,
-    svn_version: int,
-    packages: Iterable[str],
-    mode: failure_modes.FailureModes,
-) -> Dict[str, patch_utils.PatchInfo]:
-    """Updates the packages metadata file.
-
-    Args:
-        chroot_opts: options that specify the ChromeOS chroot to use.
-        svn_version: The version to use for patch management.
-        packages: All the packages to update their patch metadata file.
-        mode: The mode for the patch manager to use when an applicable patch
-        fails to apply.
-            Ex: 'FailureModes.FAIL'
-
-    Returns:
-        A dictionary where the key is the package name and the value is a
-        dictionary that has information on the patches.
-    """
-
-    # A dictionary where the key is the package name and the value is a
-    # dictionary that has information on the patches.
-    package_info: Dict[str, patch_utils.PatchInfo] = {}
-
-    llvm_hash = get_llvm_hash.LLVMHash()
-
-    with llvm_hash.CreateTempDirectory() as temp_dir:
-        with get_llvm_hash.CreateTempLLVMRepo(temp_dir) as dirname:
-            # Ensure that 'svn_version' exists in the chromiumum mirror of
-            # LLVM by finding its corresponding git hash.
-            git_hash = get_llvm_hash.GetGitHashFrom(dirname, svn_version)
-            move_head_cmd = ["git", "-C", dirname, "checkout", git_hash, "-q"]
-            subprocess.run(move_head_cmd, stdout=subprocess.DEVNULL, check=True)
-
-            for cur_package in packages:
-                # Get the absolute path to $FILESDIR of the package.
-                chroot_ebuild_str = subprocess_helpers.ChrootRunCommand(
-                    chroot_opts.chromeos_root,
-                    ["equery", "w", cur_package],
-                    chroot_name=chroot_opts.chroot_name,
-                    out_name=chroot_opts.out_name,
-                ).strip()
-                if not chroot_ebuild_str:
-                    raise RuntimeError(
-                        f"could not find ebuild for {cur_package}"
-                    )
-                chroot_ebuild_path = Path(
-                    chroot.ConvertChrootPathsToAbsolutePaths(
-                        str(chroot_opts.chromeos_root), [chroot_ebuild_str]
-                    )[0]
-                )
-                patches_json_fp = (
-                    chroot_ebuild_path.parent / "files" / "PATCHES.json"
-                )
-                if not patches_json_fp.is_file():
-                    raise RuntimeError(
-                        f"patches file {patches_json_fp} is not a file"
-                    )
-
-                src_path = Path(dirname)
-                with patch_utils.git_clean_context(src_path):
-                    if mode in (
-                        failure_modes.FailureModes.FAIL,
-                        failure_modes.FailureModes.CONTINUE,
-                    ):
-                        patches_info = patch_utils.apply_all_from_json(
-                            svn_version=svn_version,
-                            llvm_src_dir=src_path,
-                            patches_json_fp=patches_json_fp,
-                            continue_on_failure=mode
-                            == failure_modes.FailureModes.CONTINUE,
-                        )
-                    elif mode == failure_modes.FailureModes.DISABLE_PATCHES:
-                        patches_info = patch_utils.update_version_ranges(
-                            svn_version, src_path, patches_json_fp
-                        )
-                    else:
-                        raise RuntimeError(f"unsupported failure mode: {mode}")
-
-                package_info[cur_package] = patches_info
-
-    return package_info
-
-
-def ChangeRepoManifest(
-    git_hash: str,
-    src_tree: Path,
-    extra_commit_msg_lines: Optional[Iterable[str]] = None,
-    delete_branch=True,
-    upload_changes=True,
-):
-    """Change the repo internal manifest for llvm-project.
-
-    Args:
-        git_hash: The LLVM git hash to change to.
-        src_tree: ChromiumOS source tree checkout.
-        extra_commit_msg_lines: Lines to append to the commit message.
-        delete_branch: Delete the branch as a final step.
-        upload_changes: Upload the changes to gerrit.
-
-    Returns:
-        The uploaded changelist CommitContents.
-    """
-    manifest_dir = manifest_utils.get_chromeos_manifest_path(src_tree).parent
-    branch_name = "update-llvm-project-" + git_hash
-    commit_lines = (
-        textwrap.dedent(
-            f"""
-            manifest: Update llvm-project to {git_hash}
-
-            Upgrade the local LLVM revision to match the new llvm ebuild
-            hash. This must be merged along with any chromiumos-overlay
-            changes to LLVM. Automatic uprevs rely on the manifest hash
-            to match what is specified by LLVM_HASH.
-
-            This CL is generated by the update_chromeos_llvm_hash.py script.
-
-            BUG=None
-            TEST=CQ
-            """
-        )
-        .lstrip()
-        .splitlines()
-    )
-
-    change_list = None
-    git.CreateBranch(manifest_dir, branch_name)
-    try:
-        manifest_path = manifest_utils.update_chromeos_manifest(
-            git_hash,
-            src_tree,
-        )
-        subprocess.run(
-            ["git", "-C", manifest_dir, "add", manifest_path.name], check=True
-        )
-        if extra_commit_msg_lines:
-            commit_lines.extend(extra_commit_msg_lines)
-        git.CommitChanges(manifest_dir, commit_lines)
-        if upload_changes:
-            change_list = git.UploadChanges(manifest_dir, branch_name)
-    finally:
-        if delete_branch:
-            git.DeleteBranch(manifest_dir, branch_name)
-        else:
-            print(f"Not deleting branch {branch_name}")
-    return change_list
-
-
-def main():
-    """Updates the LLVM next hash for each package.
-
-    Raises:
-        AssertionError: The script was run inside the chroot.
-    """
-
-    chroot.VerifyOutsideChroot()
-
-    args_output = GetCommandLineArgs()
-
-    chroot.VerifyChromeOSRoot(args_output.chromeos_path)
-
-    llvm_variant = LLVMVariant.current
-    if args_output.is_llvm_next:
-        llvm_variant = LLVMVariant.next
-
-    git_hash_source = args_output.llvm_version
-
-    git_hash, svn_version = get_llvm_hash.GetLLVMHashAndVersionFromSVNOption(
-        git_hash_source
-    )
-    # Filter out empty strings. For example "".split{",") returns [""].
-    packages = set(p for p in args_output.update_packages.split(",") if p)
-    manifest_packages = set(
-        p for p in args_output.manifest_packages.split(",") if p
-    )
-    if not manifest_packages and not args_output.is_llvm_next:
-        # Set default manifest packages only for the current llvm.
-        manifest_packages = set(DEFAULT_MANIFEST_PACKAGES)
-
-    if args_output.no_patching:
-        patch_update_mode = None
-    else:
-        patch_update_mode = failure_modes.FailureModes(args_output.failure_mode)
-
-    change_list = UpdatePackages(
-        packages=packages,
-        manifest_packages=manifest_packages,
-        llvm_variant=llvm_variant,
-        git_hash=git_hash,
-        svn_version=svn_version,
-        chroot_opts=ChrootOpts(args_output.chromeos_path),
-        mode=patch_update_mode,
-        git_hash_source=git_hash_source,
-        extra_commit_msg_lines=None,
-        delete_branch=not args_output.no_delete_branch,
-        upload_changes=not args_output.no_upload_changes,
-    )
-    if change_list:
-        print(f"Successfully updated packages to {git_hash} ({svn_version})")
-        print(f"Gerrit URL: {change_list.url}")
-        print(f"Change list number: {change_list.cl_number}")
-    else:
-        print("--no-upload passed, did not create a change list")
-
-    if args_output.repo_manifest and not args_output.is_llvm_next:
-        print(
-            f"Updating internal manifest to {git_hash} ({svn_version})...",
-            end="",
-        )
-        cq_depend_line = (
-            [f"Cq-Depend: chromium:{change_list.cl_number}"]
-            if change_list
-            else None
-        )
-        change_list = ChangeRepoManifest(
-            git_hash,
-            args_output.chromeos_path,
-            extra_commit_msg_lines=cq_depend_line,
-            delete_branch=not args_output.no_delete_branch,
-            upload_changes=not args_output.no_upload_changes,
-        )
-        print(" Done!")
-        if change_list:
-            print("New repo manifest CL:")
-            print(f"  URL: {change_list.url}")
-            print(f"  CL Number: {change_list.cl_number}")
-        else:
-            print("--no-upload passed, did not create a change list")
-
-
-if __name__ == "__main__":
-    main()
diff --git a/llvm_tools/update_chromeos_llvm_hash_unittest.py b/llvm_tools/update_chromeos_llvm_hash_unittest.py
deleted file mode 100755
index bab1ebb0..00000000
--- a/llvm_tools/update_chromeos_llvm_hash_unittest.py
+++ /dev/null
@@ -1,923 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unit tests for updating LLVM hashes."""
-
-import os
-from pathlib import Path
-import subprocess
-import sys
-import tempfile
-from typing import Optional, Union
-import unittest
-from unittest import mock
-
-import chroot
-import failure_modes
-import get_llvm_hash
-import patch_utils
-import test_helpers
-import update_chromeos_llvm_hash
-
-
-# These are unittests; protected access is OK to a point.
-# pylint: disable=protected-access
-
-
-class UpdateLLVMHashTest(unittest.TestCase):
-    """Test class for updating LLVM hashes of packages."""
-
-    @staticmethod
-    def _make_patch_entry(
-        relpath: Union[str, Path], workdir: Optional[Path] = None
-    ) -> patch_utils.PatchEntry:
-        if workdir is None:
-            workdir = Path("llvm_tools/update_chromeos_llvm_hash_unittest.py")
-        return patch_utils.PatchEntry(
-            workdir=workdir,
-            rel_patch_path=str(relpath),
-            metadata={},
-            platforms=["chromiumos"],
-            version_range={"from": None, "until": None},
-            verify_workdir=False,
-        )
-
-    @mock.patch.object(os.path, "realpath")
-    def testDefaultCrosRootFromCrOSCheckout(self, mock_llvm_tools):
-        llvm_tools_path = (
-            "/path/to/cros/src/third_party/toolchain-utils/llvm_tools"
-        )
-        mock_llvm_tools.return_value = llvm_tools_path
-        self.assertEqual(
-            update_chromeos_llvm_hash.defaultCrosRoot(), Path("/path/to/cros")
-        )
-
-    @mock.patch.object(os.path, "realpath")
-    def testDefaultCrosRootFromOutsideCrOSCheckout(self, mock_llvm_tools):
-        mock_llvm_tools.return_value = "~/toolchain-utils/llvm_tools"
-        self.assertEqual(
-            update_chromeos_llvm_hash.defaultCrosRoot(),
-            Path.home() / "chromiumos",
-        )
-
-    # Simulate behavior of 'os.path.isfile()' when the ebuild path to a package
-    # does not exist.
-    @mock.patch.object(os.path, "isfile", return_value=False)
-    def testFailedToUpdateLLVMHashForInvalidEbuildPath(self, mock_isfile):
-        ebuild_path = Path("/some/path/to/package.ebuild")
-        llvm_variant = update_chromeos_llvm_hash.LLVMVariant.current
-        git_hash = "a123testhash1"
-        svn_version = 1000
-
-        # Verify the exception is raised when the ebuild path does not exist.
-        with self.assertRaises(ValueError) as err:
-            update_chromeos_llvm_hash.UpdateEbuildLLVMHash(
-                ebuild_path, llvm_variant, git_hash, svn_version
-            )
-
-        self.assertEqual(
-            str(err.exception),
-            "Invalid ebuild path provided: %s" % ebuild_path,
-        )
-
-        mock_isfile.assert_called_once()
-
-    # Simulate 'os.path.isfile' behavior on a valid ebuild path.
-    @mock.patch.object(os.path, "isfile", return_value=True)
-    def testFailedToUpdateLLVMHash(self, mock_isfile):
-        # Create a temporary file to simulate an ebuild file of a package.
-        with test_helpers.CreateTemporaryJsonFile() as ebuild_file:
-            with open(ebuild_file, "w", encoding="utf-8") as f:
-                f.write(
-                    "\n".join(
-                        [
-                            "First line in the ebuild",
-                            "Second line in the ebuild",
-                            "Last line in the ebuild",
-                        ]
-                    )
-                )
-
-            llvm_variant = update_chromeos_llvm_hash.LLVMVariant.current
-            git_hash = "a123testhash1"
-            svn_version = 1000
-
-            # Verify the exception is raised when the ebuild file does not have
-            # 'LLVM_HASH'.
-            with self.assertRaises(ValueError) as err:
-                update_chromeos_llvm_hash.UpdateEbuildLLVMHash(
-                    Path(ebuild_file), llvm_variant, git_hash, svn_version
-                )
-
-            self.assertEqual(str(err.exception), "Failed to update LLVM_HASH")
-
-            llvm_variant = update_chromeos_llvm_hash.LLVMVariant.next
-
-        self.assertEqual(mock_isfile.call_count, 2)
-
-    # Simulate 'os.path.isfile' behavior on a valid ebuild path.
-    @mock.patch.object(os.path, "isfile", return_value=True)
-    def testFailedToUpdateLLVMNextHash(self, mock_isfile):
-        # Create a temporary file to simulate an ebuild file of a package.
-        with test_helpers.CreateTemporaryJsonFile() as ebuild_file:
-            with open(ebuild_file, "w", encoding="utf-8") as f:
-                f.write(
-                    "\n".join(
-                        [
-                            "First line in the ebuild",
-                            "Second line in the ebuild",
-                            "Last line in the ebuild",
-                        ]
-                    )
-                )
-
-            llvm_variant = update_chromeos_llvm_hash.LLVMVariant.next
-            git_hash = "a123testhash1"
-            svn_version = 1000
-
-            # Verify the exception is raised when the ebuild file does not have
-            # 'LLVM_NEXT_HASH'.
-            with self.assertRaises(ValueError) as err:
-                update_chromeos_llvm_hash.UpdateEbuildLLVMHash(
-                    Path(ebuild_file), llvm_variant, git_hash, svn_version
-                )
-
-            self.assertEqual(
-                str(err.exception), "Failed to update LLVM_NEXT_HASH"
-            )
-
-        self.assertEqual(mock_isfile.call_count, 2)
-
-    @mock.patch.object(os.path, "isfile", return_value=True)
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    def testSuccessfullyStageTheEbuildForCommitForLLVMHashUpdate(
-        self, mock_stage_commit_command, mock_isfile
-    ):
-        # Create a temporary file to simulate an ebuild file of a package.
-        with test_helpers.CreateTemporaryJsonFile() as ebuild_file:
-            # Updates LLVM_HASH to 'git_hash' and revision to
-            # 'svn_version'.
-            llvm_variant = update_chromeos_llvm_hash.LLVMVariant.current
-            git_hash = "a123testhash1"
-            svn_version = 1000
-
-            with open(ebuild_file, "w", encoding="utf-8") as f:
-                f.write(
-                    "\n".join(
-                        [
-                            "First line in the ebuild",
-                            "Second line in the ebuild",
-                            'LLVM_HASH="a12b34c56d78e90" # r500',
-                            "Last line in the ebuild",
-                        ]
-                    )
-                )
-
-            update_chromeos_llvm_hash.UpdateEbuildLLVMHash(
-                Path(ebuild_file), llvm_variant, git_hash, svn_version
-            )
-
-            expected_file_contents = [
-                "First line in the ebuild\n",
-                "Second line in the ebuild\n",
-                'LLVM_HASH="a123testhash1" # r1000\n',
-                "Last line in the ebuild",
-            ]
-
-            # Verify the new file contents of the ebuild file match the expected
-            # file contents.
-            with open(ebuild_file, encoding="utf-8") as new_file:
-                self.assertListEqual(
-                    new_file.readlines(), expected_file_contents
-                )
-
-        self.assertEqual(mock_isfile.call_count, 2)
-
-        mock_stage_commit_command.assert_called_once()
-
-    @mock.patch.object(os.path, "isfile", return_value=True)
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    def testSuccessfullyStageTheEbuildForCommitForLLVMNextHashUpdate(
-        self, mock_stage_commit_command, mock_isfile
-    ):
-        # Create a temporary file to simulate an ebuild file of a package.
-        with test_helpers.CreateTemporaryJsonFile() as ebuild_file:
-            # Updates LLVM_NEXT_HASH to 'git_hash' and revision to
-            # 'svn_version'.
-            llvm_variant = update_chromeos_llvm_hash.LLVMVariant.next
-            git_hash = "a123testhash1"
-            svn_version = 1000
-
-            with open(ebuild_file, "w", encoding="utf-8") as f:
-                f.write(
-                    "\n".join(
-                        [
-                            "First line in the ebuild",
-                            "Second line in the ebuild",
-                            'LLVM_NEXT_HASH="a12b34c56d78e90" # r500',
-                            "Last line in the ebuild",
-                        ]
-                    )
-                )
-
-            update_chromeos_llvm_hash.UpdateEbuildLLVMHash(
-                Path(ebuild_file), llvm_variant, git_hash, svn_version
-            )
-
-            expected_file_contents = [
-                "First line in the ebuild\n",
-                "Second line in the ebuild\n",
-                'LLVM_NEXT_HASH="a123testhash1" # r1000\n',
-                "Last line in the ebuild",
-            ]
-
-            # Verify the new file contents of the ebuild file match the expected
-            # file contents.
-            with open(ebuild_file, encoding="utf-8") as new_file:
-                self.assertListEqual(
-                    new_file.readlines(), expected_file_contents
-                )
-
-        self.assertEqual(mock_isfile.call_count, 2)
-
-        mock_stage_commit_command.assert_called_once()
-
-    @mock.patch.object(get_llvm_hash, "GetLLVMMajorVersion")
-    @mock.patch.object(os.path, "islink", return_value=False)
-    def testFailedToUprevEbuildToVersionForInvalidSymlink(
-        self, mock_islink, mock_llvm_version
-    ):
-        symlink_path = "/path/to/chromeos/package/package.ebuild"
-        svn_version = 1000
-        git_hash = "badf00d"
-        mock_llvm_version.return_value = "1234"
-
-        # Verify the exception is raised when a invalid symbolic link is
-        # passed in.
-        with self.assertRaises(ValueError) as err:
-            update_chromeos_llvm_hash.UprevEbuildToVersion(
-                symlink_path, svn_version, git_hash
-            )
-
-        self.assertEqual(
-            str(err.exception), "Invalid symlink provided: %s" % symlink_path
-        )
-
-        mock_islink.assert_called_once()
-        mock_llvm_version.assert_not_called()
-
-    @mock.patch.object(os.path, "islink", return_value=False)
-    def testFailedToUprevEbuildSymlinkForInvalidSymlink(self, mock_islink):
-        symlink_path = "/path/to/chromeos/package/package.ebuild"
-
-        # Verify the exception is raised when a invalid symbolic link is
-        # passed in.
-        with self.assertRaises(ValueError) as err:
-            update_chromeos_llvm_hash.UprevEbuildSymlink(symlink_path)
-
-        self.assertEqual(
-            str(err.exception), "Invalid symlink provided: %s" % symlink_path
-        )
-
-        mock_islink.assert_called_once()
-
-    @mock.patch.object(get_llvm_hash, "GetLLVMMajorVersion")
-    # Simulate 'os.path.islink' when a symbolic link is passed in.
-    @mock.patch.object(os.path, "islink", return_value=True)
-    # Simulate 'os.path.realpath' when a symbolic link is passed in.
-    @mock.patch.object(os.path, "realpath", return_value=True)
-    def testFailedToUprevEbuildToVersion(
-        self, mock_realpath, mock_islink, mock_llvm_version
-    ):
-        symlink_path = "/path/to/chromeos/llvm/llvm_pre123_p.ebuild"
-        mock_realpath.return_value = "/abs/path/to/llvm/llvm_pre123_p.ebuild"
-        git_hash = "badf00d"
-        mock_llvm_version.return_value = "1234"
-        svn_version = 1000
-
-        # Verify the exception is raised when the symlink does not match the
-        # expected pattern
-        with self.assertRaises(ValueError) as err:
-            update_chromeos_llvm_hash.UprevEbuildToVersion(
-                symlink_path, svn_version, git_hash
-            )
-
-        self.assertEqual(str(err.exception), "Failed to uprev the ebuild.")
-
-        mock_llvm_version.assert_called_once_with(git_hash)
-        mock_islink.assert_called_once_with(symlink_path)
-
-    # Simulate 'os.path.islink' when a symbolic link is passed in.
-    @mock.patch.object(os.path, "islink", return_value=True)
-    def testFailedToUprevEbuildSymlink(self, mock_islink):
-        symlink_path = "/path/to/chromeos/llvm/llvm_pre123_p.ebuild"
-
-        # Verify the exception is raised when the symlink does not match the
-        # expected pattern
-        with self.assertRaises(ValueError) as err:
-            update_chromeos_llvm_hash.UprevEbuildSymlink(symlink_path)
-
-        self.assertEqual(str(err.exception), "Failed to uprev the symlink.")
-
-        mock_islink.assert_called_once_with(symlink_path)
-
-    @mock.patch.object(get_llvm_hash, "GetLLVMMajorVersion")
-    @mock.patch.object(os.path, "islink", return_value=True)
-    @mock.patch.object(os.path, "realpath")
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    def testSuccessfullyUprevEbuildToVersionLLVM(
-        self,
-        mock_command_output,
-        mock_realpath,
-        mock_islink,
-        mock_llvm_version,
-    ):
-        symlink = "/path/to/llvm/llvm-12.0_pre3_p2-r10.ebuild"
-        ebuild = "/abs/path/to/llvm/llvm-12.0_pre3_p2.ebuild"
-        mock_realpath.return_value = ebuild
-        git_hash = "badf00d"
-        mock_llvm_version.return_value = "1234"
-        svn_version = 1000
-
-        update_chromeos_llvm_hash.UprevEbuildToVersion(
-            symlink, svn_version, git_hash
-        )
-
-        mock_llvm_version.assert_called_once_with(git_hash)
-
-        mock_islink.assert_called()
-
-        mock_realpath.assert_called_once_with(symlink)
-
-        mock_command_output.assert_called()
-
-        # Verify commands
-        symlink_dir = os.path.dirname(symlink)
-        new_ebuild = "/abs/path/to/llvm/llvm-1234.0_pre1000.ebuild"
-        new_symlink = new_ebuild[: -len(".ebuild")] + "-r1.ebuild"
-
-        expected_cmd = ["git", "-C", symlink_dir, "mv", ebuild, new_ebuild]
-        self.assertEqual(
-            mock_command_output.call_args_list[0], mock.call(expected_cmd)
-        )
-
-        expected_cmd = ["ln", "-s", "-r", new_ebuild, new_symlink]
-        self.assertEqual(
-            mock_command_output.call_args_list[1], mock.call(expected_cmd)
-        )
-
-        expected_cmd = ["git", "-C", symlink_dir, "add", new_symlink]
-        self.assertEqual(
-            mock_command_output.call_args_list[2], mock.call(expected_cmd)
-        )
-
-        expected_cmd = ["git", "-C", symlink_dir, "rm", symlink]
-        self.assertEqual(
-            mock_command_output.call_args_list[3], mock.call(expected_cmd)
-        )
-
-    @mock.patch.object(
-        chroot,
-        "GetChrootEbuildPaths",
-        return_value=["/chroot/path/test.ebuild"],
-    )
-    @mock.patch.object(subprocess, "check_output", return_value="")
-    def testManifestUpdate(self, mock_subprocess, mock_ebuild_paths):
-        manifest_packages = ["sys-devel/llvm"]
-        chromeos_path = "/path/to/chromeos"
-        update_chromeos_llvm_hash.UpdatePortageManifests(
-            manifest_packages, Path(chromeos_path)
-        )
-
-        args = mock_subprocess.call_args_list[0]
-        manifest_cmd = (
-            [
-                "cros_sdk",
-                "--chroot=chroot",
-                "--out-dir=out",
-                "--",
-                "ebuild",
-                "/chroot/path/test.ebuild",
-                "manifest",
-            ],
-        )
-        self.assertEqual(args[0], manifest_cmd)
-
-        args = mock_subprocess.call_args_list[1]
-        git_add_cmd = (
-            [
-                "cros_sdk",
-                "--chroot=chroot",
-                "--out-dir=out",
-                "--",
-                "git",
-                "-C",
-                "/chroot/path",
-                "add",
-                "Manifest",
-            ],
-        )
-        self.assertEqual(args[0], git_add_cmd)
-        mock_ebuild_paths.assert_called_once()
-
-    @mock.patch.object(get_llvm_hash, "GetLLVMMajorVersion")
-    @mock.patch.object(os.path, "islink", return_value=True)
-    @mock.patch.object(os.path, "realpath")
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    def testSuccessfullyUprevEbuildToVersionNonLLVM(
-        self, mock_command_output, mock_realpath, mock_islink, mock_llvm_version
-    ):
-        symlink = (
-            "/abs/path/to/compiler-rt/compiler-rt-12.0_pre314159265-r4.ebuild"
-        )
-        ebuild = "/abs/path/to/compiler-rt/compiler-rt-12.0_pre314159265.ebuild"
-        mock_realpath.return_value = ebuild
-        mock_llvm_version.return_value = "1234"
-        svn_version = 1000
-        git_hash = "5678"
-
-        update_chromeos_llvm_hash.UprevEbuildToVersion(
-            symlink, svn_version, git_hash
-        )
-
-        mock_islink.assert_called()
-
-        mock_realpath.assert_called_once_with(symlink)
-
-        mock_llvm_version.assert_called_once_with(git_hash)
-
-        mock_command_output.assert_called()
-
-        # Verify commands
-        symlink_dir = os.path.dirname(symlink)
-        new_ebuild = (
-            "/abs/path/to/compiler-rt/compiler-rt-1234.0_pre1000.ebuild"
-        )
-        new_symlink = new_ebuild[: -len(".ebuild")] + "-r1.ebuild"
-
-        expected_cmd = ["git", "-C", symlink_dir, "mv", ebuild, new_ebuild]
-        self.assertEqual(
-            mock_command_output.call_args_list[0], mock.call(expected_cmd)
-        )
-
-        expected_cmd = ["ln", "-s", "-r", new_ebuild, new_symlink]
-        self.assertEqual(
-            mock_command_output.call_args_list[1], mock.call(expected_cmd)
-        )
-
-        expected_cmd = ["git", "-C", symlink_dir, "add", new_symlink]
-        self.assertEqual(
-            mock_command_output.call_args_list[2], mock.call(expected_cmd)
-        )
-
-        expected_cmd = ["git", "-C", symlink_dir, "rm", symlink]
-        self.assertEqual(
-            mock_command_output.call_args_list[3], mock.call(expected_cmd)
-        )
-
-    @mock.patch.object(os.path, "islink", return_value=True)
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    def testSuccessfullyUprevEbuildSymlink(
-        self, mock_command_output, mock_islink
-    ):
-        symlink_to_uprev = "/symlink/to/package-r1.ebuild"
-
-        update_chromeos_llvm_hash.UprevEbuildSymlink(symlink_to_uprev)
-
-        mock_islink.assert_called_once_with(symlink_to_uprev)
-
-        mock_command_output.assert_called_once()
-
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    def testSuccessfullyRemovedPatchesFromFilesDir(self, mock_run_cmd):
-        patches_to_remove_list = [
-            "/abs/path/to/filesdir/cherry/fix_output.patch",
-            "/abs/path/to/filesdir/display_results.patch",
-        ]
-
-        update_chromeos_llvm_hash.RemovePatchesFromFilesDir(
-            patches_to_remove_list
-        )
-
-        self.assertEqual(mock_run_cmd.call_count, 2)
-
-    @mock.patch.object(os.path, "isfile", return_value=False)
-    def testInvalidPatchMetadataFileStagedForCommit(self, mock_isfile):
-        patch_metadata_path = "/abs/path/to/filesdir/PATCHES"
-
-        # Verify the exception is raised when the absolute path to the patch
-        # metadata file does not exist or is not a file.
-        with self.assertRaises(ValueError) as err:
-            update_chromeos_llvm_hash.StagePatchMetadataFileForCommit(
-                patch_metadata_path
-            )
-
-        self.assertEqual(
-            str(err.exception),
-            "Invalid patch metadata file provided: " "%s" % patch_metadata_path,
-        )
-
-        mock_isfile.assert_called_once()
-
-    @mock.patch.object(os.path, "isfile", return_value=True)
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    def testSuccessfullyStagedPatchMetadataFileForCommit(self, mock_run_cmd, _):
-        patch_metadata_path = "/abs/path/to/filesdir/PATCHES.json"
-
-        update_chromeos_llvm_hash.StagePatchMetadataFileForCommit(
-            patch_metadata_path
-        )
-
-        mock_run_cmd.assert_called_once()
-
-    def testNoPatchResultsForCommit(self):
-        package_1_patch_info = patch_utils.PatchInfo(
-            applied_patches=[self._make_patch_entry("display_results.patch")],
-            failed_patches=[self._make_patch_entry("fixes_output.patch")],
-            non_applicable_patches=[],
-            disabled_patches=[],
-            removed_patches=[],
-            modified_metadata=None,
-        )
-
-        package_2_patch_info = patch_utils.PatchInfo(
-            applied_patches=[
-                self._make_patch_entry("redirects_stdout.patch"),
-                self._make_patch_entry("fix_display.patch"),
-            ],
-            failed_patches=[],
-            non_applicable_patches=[],
-            disabled_patches=[],
-            removed_patches=[],
-            modified_metadata=None,
-        )
-
-        test_package_info_dict = {
-            "test-packages/package1": package_1_patch_info,
-            "test-packages/package2": package_2_patch_info,
-        }
-
-        test_commit_message = ["Updated packages"]
-
-        self.assertListEqual(
-            update_chromeos_llvm_hash.StagePackagesPatchResultsForCommit(
-                test_package_info_dict, test_commit_message
-            ),
-            test_commit_message,
-        )
-
-    @mock.patch.object(
-        update_chromeos_llvm_hash, "StagePatchMetadataFileForCommit"
-    )
-    @mock.patch.object(update_chromeos_llvm_hash, "RemovePatchesFromFilesDir")
-    def testAddedPatchResultsForCommit(
-        self, mock_remove_patches, mock_stage_patches_for_commit
-    ):
-        package_1_patch_info = patch_utils.PatchInfo(
-            applied_patches=[],
-            failed_patches=[],
-            non_applicable_patches=[],
-            disabled_patches=["fixes_output.patch"],
-            removed_patches=[],
-            modified_metadata="/abs/path/to/filesdir/PATCHES.json",
-        )
-
-        package_2_patch_info = patch_utils.PatchInfo(
-            applied_patches=[self._make_patch_entry("fix_display.patch")],
-            failed_patches=[],
-            non_applicable_patches=[],
-            disabled_patches=[],
-            removed_patches=["/abs/path/to/filesdir/redirect_stdout.patch"],
-            modified_metadata="/abs/path/to/filesdir/PATCHES.json",
-        )
-
-        test_package_info_dict = {
-            "test-packages/package1": package_1_patch_info,
-            "test-packages/package2": package_2_patch_info,
-        }
-
-        test_commit_message = ["Updated packages"]
-
-        expected_commit_messages = [
-            "Updated packages",
-            "\nFor the package test-packages/package1:",
-            "The patch metadata file PATCHES.json was modified",
-            "The following patches were disabled:",
-            "fixes_output.patch",
-            "\nFor the package test-packages/package2:",
-            "The patch metadata file PATCHES.json was modified",
-            "The following patches were removed:",
-            "redirect_stdout.patch",
-        ]
-
-        self.assertListEqual(
-            update_chromeos_llvm_hash.StagePackagesPatchResultsForCommit(
-                test_package_info_dict, test_commit_message
-            ),
-            expected_commit_messages,
-        )
-
-        path_to_removed_patch = "/abs/path/to/filesdir/redirect_stdout.patch"
-
-        mock_remove_patches.assert_called_once_with([path_to_removed_patch])
-
-        self.assertEqual(mock_stage_patches_for_commit.call_count, 2)
-
-    def setup_mock_src_tree(self, src_tree: Path):
-        package_dir = (
-            src_tree / "src/third_party/chromiumos-overlay/sys-devel/llvm"
-        )
-        package_dir.mkdir(parents=True)
-        ebuild_path = package_dir / "llvm-00.00_pre0_p0.ebuild"
-        with ebuild_path.open("w", encoding="utf-8") as f:
-            f.writelines(
-                [
-                    'LLVM_HASH="abcdef123456" # r123456',
-                    'LLVM_NEXT_HASH="987654321fedcba" # r99453',
-                ]
-            )
-        symlink_path = package_dir / "llvm-00.00_pre0_p0-r1234.ebuild"
-        symlink_path.symlink_to(ebuild_path)
-        return package_dir, ebuild_path, symlink_path
-
-    def testPortagePackageConstruction(self):
-        with tempfile.TemporaryDirectory(
-            "update_chromeos_llvm_hash.tmp"
-        ) as workdir_str:
-            src_tree = Path(workdir_str)
-            package_dir, ebuild_path, symlink_path = self.setup_mock_src_tree(
-                src_tree
-            )
-
-            # Test that we're upreving if there's a symlink.
-            def mock_find_package_ebuild(_, package_name):
-                self.assertEqual(
-                    package_name,
-                    f"{package_dir.parent.name}/{package_dir.name}",
-                )
-                return symlink_path
-
-            with mock.patch(
-                "update_chromeos_llvm_hash.PortagePackage.find_package_ebuild",
-                mock_find_package_ebuild,
-            ):
-                pkg = update_chromeos_llvm_hash.PortagePackage(
-                    src_tree, "sys-devel/llvm"
-                )
-                self.assertEqual(pkg.uprev_target, symlink_path.absolute())
-                self.assertEqual(pkg.ebuild_path, ebuild_path.absolute())
-                self.assertEqual(pkg.live_ebuild(), None)
-
-                # Make sure if the live ebuild is there, we find it.
-                live_ebuild_path = package_dir / "llvm-9999.ebuild"
-                live_ebuild_path.touch()
-
-                pkg = update_chromeos_llvm_hash.PortagePackage(
-                    src_tree, "sys-devel/llvm"
-                )
-                self.assertEqual(pkg.live_ebuild(), live_ebuild_path)
-
-    @mock.patch("subprocess.run")
-    @mock.patch("subprocess.check_output")
-    @mock.patch.object(get_llvm_hash, "GetLLVMMajorVersion")
-    def testUpdatePackages(
-        self, mock_llvm_major_version, _mock_check_output, _mock_run
-    ):
-        mock_llvm_major_version.return_value = "17"
-        with tempfile.TemporaryDirectory(
-            "update_chromeos_llvm_hash.tmp"
-        ) as workdir_str:
-            src_tree = Path(workdir_str)
-            _package_dir, _ebuild_path, symlink_path = self.setup_mock_src_tree(
-                src_tree
-            )
-
-            def mock_find_package_ebuild(*_):
-                return symlink_path
-
-            with mock.patch(
-                "update_chromeos_llvm_hash.PortagePackage.find_package_ebuild",
-                mock_find_package_ebuild,
-            ):
-                pkg = update_chromeos_llvm_hash.PortagePackage(
-                    src_tree, "sys-devel/llvm"
-                )
-                pkg.update(
-                    update_chromeos_llvm_hash.LLVMVariant.current,
-                    "beef3333",
-                    3333,
-                )
-
-    @mock.patch.object(chroot, "VerifyChromeOSRoot")
-    @mock.patch.object(chroot, "VerifyOutsideChroot")
-    @mock.patch.object(get_llvm_hash, "GetLLVMHashAndVersionFromSVNOption")
-    @mock.patch.object(update_chromeos_llvm_hash, "UpdatePackages")
-    def testMainDefaults(
-        self,
-        mock_update_packages,
-        mock_gethash,
-        mock_outside_chroot,
-        mock_chromeos_root,
-    ):
-        git_hash = "1234abcd"
-        svn_version = 5678
-        mock_gethash.return_value = (git_hash, svn_version)
-        argv = [
-            "./update_chromeos_llvm_hash_unittest.py",
-            "--no_repo_manifest",
-            "--llvm_version",
-            "google3",
-        ]
-
-        with mock.patch.object(sys, "argv", argv) as mock.argv:
-            update_chromeos_llvm_hash.main()
-
-        expected_packages = set(update_chromeos_llvm_hash.DEFAULT_PACKAGES)
-        expected_manifest_packages = set(
-            update_chromeos_llvm_hash.DEFAULT_MANIFEST_PACKAGES,
-        )
-        expected_llvm_variant = update_chromeos_llvm_hash.LLVMVariant.current
-        expected_chroot = update_chromeos_llvm_hash.defaultCrosRoot()
-        mock_update_packages.assert_called_once_with(
-            packages=expected_packages,
-            manifest_packages=expected_manifest_packages,
-            llvm_variant=expected_llvm_variant,
-            git_hash=git_hash,
-            svn_version=svn_version,
-            chroot_opts=update_chromeos_llvm_hash.ChrootOpts(expected_chroot),
-            mode=failure_modes.FailureModes.FAIL,
-            git_hash_source="google3",
-            extra_commit_msg_lines=None,
-            delete_branch=True,
-            upload_changes=True,
-        )
-        mock_outside_chroot.assert_called()
-        mock_chromeos_root.assert_called()
-
-    @mock.patch.object(chroot, "VerifyChromeOSRoot")
-    @mock.patch.object(chroot, "VerifyOutsideChroot")
-    @mock.patch.object(get_llvm_hash, "GetLLVMHashAndVersionFromSVNOption")
-    @mock.patch.object(update_chromeos_llvm_hash, "UpdatePackages")
-    def testMainLlvmNext(
-        self,
-        mock_update_packages,
-        mock_gethash,
-        mock_outside_chroot,
-        mock_chromeos_root,
-    ):
-        git_hash = "1234abcd"
-        svn_version = 5678
-        mock_gethash.return_value = (git_hash, svn_version)
-        argv = [
-            "./update_chromeos_llvm_hash_unittest.py",
-            "--llvm_version",
-            "google3",
-            "--is_llvm_next",
-        ]
-
-        with mock.patch.object(sys, "argv", argv) as mock.argv:
-            update_chromeos_llvm_hash.main()
-
-        expected_packages = set(update_chromeos_llvm_hash.DEFAULT_PACKAGES)
-        expected_llvm_variant = update_chromeos_llvm_hash.LLVMVariant.next
-        expected_chroot = update_chromeos_llvm_hash.defaultCrosRoot()
-        # llvm-next upgrade does not update manifest by default.
-        mock_update_packages.assert_called_once_with(
-            packages=expected_packages,
-            manifest_packages=set(),
-            llvm_variant=expected_llvm_variant,
-            git_hash=git_hash,
-            svn_version=svn_version,
-            chroot_opts=update_chromeos_llvm_hash.ChrootOpts(expected_chroot),
-            mode=failure_modes.FailureModes.FAIL,
-            git_hash_source="google3",
-            extra_commit_msg_lines=None,
-            delete_branch=True,
-            upload_changes=True,
-        )
-        mock_outside_chroot.assert_called()
-        mock_chromeos_root.assert_called()
-
-    @mock.patch.object(chroot, "VerifyChromeOSRoot")
-    @mock.patch.object(chroot, "VerifyOutsideChroot")
-    @mock.patch.object(get_llvm_hash, "GetLLVMHashAndVersionFromSVNOption")
-    @mock.patch.object(update_chromeos_llvm_hash, "UpdatePackages")
-    def testMainAllArgs(
-        self,
-        mock_update_packages,
-        mock_gethash,
-        mock_outside_chroot,
-        mock_chromeos_root,
-    ):
-        packages_to_update = "test-packages/package1,test-libs/lib1"
-        manifest_packages = "test-libs/lib1,test-libs/lib2"
-        failure_mode = failure_modes.FailureModes.DISABLE_PATCHES
-        chromeos_path = Path("/some/path/to/chromeos")
-        llvm_ver = 435698
-        git_hash = "1234abcd"
-        svn_version = 5678
-        mock_gethash.return_value = (git_hash, svn_version)
-
-        argv = [
-            "./update_chromeos_llvm_hash_unittest.py",
-            "--llvm_version",
-            str(llvm_ver),
-            "--is_llvm_next",
-            "--chromeos_path",
-            str(chromeos_path),
-            "--update_packages",
-            packages_to_update,
-            "--manifest_packages",
-            manifest_packages,
-            "--failure_mode",
-            failure_mode.value,
-            "--patch_metadata_file",
-            "META.json",
-            "--no_repo_manifest",
-        ]
-
-        with mock.patch.object(sys, "argv", argv) as mock.argv:
-            update_chromeos_llvm_hash.main()
-
-        expected_packages = {"test-packages/package1", "test-libs/lib1"}
-        expected_manifest_packages = {"test-libs/lib1", "test-libs/lib2"}
-        expected_llvm_variant = update_chromeos_llvm_hash.LLVMVariant.next
-        mock_update_packages.assert_called_once_with(
-            packages=expected_packages,
-            manifest_packages=expected_manifest_packages,
-            llvm_variant=expected_llvm_variant,
-            git_hash=git_hash,
-            svn_version=svn_version,
-            chroot_opts=update_chromeos_llvm_hash.ChrootOpts(chromeos_path),
-            mode=failure_mode,
-            git_hash_source=llvm_ver,
-            extra_commit_msg_lines=None,
-            delete_branch=True,
-            upload_changes=True,
-        )
-        mock_outside_chroot.assert_called()
-        mock_chromeos_root.assert_called()
-
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    @mock.patch.object(get_llvm_hash, "GetLLVMMajorVersion")
-    def testEnsurePackageMaskContainsExisting(
-        self, mock_llvm_version, mock_git_add
-    ):
-        chromeos_path = "absolute/path/to/chromeos"
-        git_hash = "badf00d"
-        mock_llvm_version.return_value = "1234"
-        with mock.patch(
-            "update_chromeos_llvm_hash.open",
-            mock.mock_open(read_data="\n=sys-devel/llvm-1234.0_pre*\n"),
-            create=True,
-        ) as mock_file:
-            update_chromeos_llvm_hash.EnsurePackageMaskContains(
-                chromeos_path, git_hash
-            )
-            handle = mock_file()
-            handle.write.assert_not_called()
-        mock_llvm_version.assert_called_once_with(git_hash)
-
-        overlay_dir = (
-            "absolute/path/to/chromeos/src/third_party/chromiumos-overlay"
-        )
-        mask_path = overlay_dir + "/profiles/targets/chromeos/package.mask"
-        mock_git_add.assert_called_once_with(
-            ["git", "-C", overlay_dir, "add", mask_path]
-        )
-
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    @mock.patch.object(get_llvm_hash, "GetLLVMMajorVersion")
-    def testEnsurePackageMaskContainsNotExisting(
-        self, mock_llvm_version, mock_git_add
-    ):
-        chromeos_path = "absolute/path/to/chromeos"
-        git_hash = "badf00d"
-        mock_llvm_version.return_value = "1234"
-        with mock.patch(
-            "update_chromeos_llvm_hash.open",
-            mock.mock_open(read_data="nothing relevant"),
-            create=True,
-        ) as mock_file:
-            update_chromeos_llvm_hash.EnsurePackageMaskContains(
-                chromeos_path, git_hash
-            )
-            handle = mock_file()
-            handle.write.assert_called_once_with(
-                "=sys-devel/llvm-1234.0_pre*\n"
-            )
-        mock_llvm_version.assert_called_once_with(git_hash)
-
-        overlay_dir = (
-            "absolute/path/to/chromeos/src/third_party/chromiumos-overlay"
-        )
-        mask_path = overlay_dir + "/profiles/targets/chromeos/package.mask"
-        mock_git_add.assert_called_once_with(
-            ["git", "-C", overlay_dir, "add", mask_path]
-        )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/update_packages_and_run_tests.py b/llvm_tools/update_packages_and_run_tests.py
old mode 100755
new mode 100644
index 34837630..7590c46e
--- a/llvm_tools/update_packages_and_run_tests.py
+++ b/llvm_tools/update_packages_and_run_tests.py
@@ -1,603 +1,401 @@
-#!/usr/bin/env python3
-# Copyright 2019 The ChromiumOS Authors
+# Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-"""Runs a tryjob/tryjobs after updating the packages."""
+"""Uploads CLs necessary to run LLVM testing at an arbitrary SHA.
+
+Also has the ability to:
+- kick off a CQ run
+- keep track of the last SHA that testing was requested on, and skip
+  re-uploading if the SHA has not changed.
+"""
 
 import argparse
-import datetime
+import dataclasses
 import json
-import os
+import logging
 from pathlib import Path
 import subprocess
-from typing import Any, Dict, Iterable, List, Optional, Union
-
-import chroot
-import failure_modes
-import get_llvm_hash
-import update_chromeos_llvm_hash
-
-
-VALID_CQ_TRYBOTS = ("llvm", "llvm-next")
-
-
-def GetCommandLineArgs() -> argparse.Namespace:
-    """Parses the command line for the command line arguments.
-
-    Returns:
-        The log level to use when retrieving the LLVM hash or google3 LLVM
-        version, the chroot path to use for executing chroot commands,
-        a list of a package or packages to update their LLVM next hash,
-        and the LLVM version to use when retrieving the LLVM hash.
-    """
-
-    # Default path to the chroot if a path is not specified.
-    cros_root = os.path.expanduser("~")
-    cros_root = os.path.join(cros_root, "chromiumos")
-
-    # Create parser and add optional command-line arguments.
-    parser = argparse.ArgumentParser(
-        description="Update an LLVM hash of packages and run tests."
-    )
-
-    # Add argument for other change lists that want to run alongside the tryjob
-    # which has a change list of updating a package's git hash.
-    parser.add_argument(
-        "--extra_change_lists",
-        type=int,
-        nargs="+",
-        default=[],
-        help="change lists that would like to be run alongside the change list "
-        "of updating the packages",
-    )
-
-    # Add argument for a specific chroot path.
-    parser.add_argument(
-        "--chromeos_path",
-        default=cros_root,
-        help="the path to the ChromeOS tree (default: %(default)s)",
-    )
-
-    # Add argument for a specific chroot path.
-    parser.add_argument(
-        "--chroot_name",
-        default="chroot",
-        help="""
-        the name of the chroot to use in the CrOS checkout. Defaults to
-        'chroot'.
-        """,
-    )
-
-    parser.add_argument(
-        "--chroot_out",
-        help="""
-        the name of the chroot to use in the CrOS checkout. Defaults to
-        'out' if the chroot's name is 'chroot'; otherwise, defaults to
-        '${chroot_name}_out'.
-        """,
-    )
-
-    # Add argument to choose between llvm and llvm-next.
-    parser.add_argument(
-        "--is_llvm_next",
-        action="store_true",
-        help="which llvm hash to update. Update LLVM_NEXT_HASH if specified. "
-        "Otherwise, update LLVM_HASH",
-    )
-
-    # Add argument for the absolute path to the file that contains information
-    # on the previous tested svn version.
-    parser.add_argument(
-        "--last_tested",
-        help="the absolute path to the file that contains the last tested "
-        "arguments.",
-    )
-
-    # Add argument for the LLVM version to use.
-    parser.add_argument(
-        "--llvm_version",
-        type=get_llvm_hash.IsSvnOption,
-        required=True,
-        help="which git hash of LLVM to find "
-        "{google3, ToT, <svn_version>} "
-        "(default: finds the git hash of the google3 LLVM "
-        "version)",
-    )
-
-    # Add argument to add reviewers for the created CL.
-    parser.add_argument(
-        "--reviewers",
-        nargs="+",
-        default=[],
-        help="The reviewers for the package update changelist",
-    )
-
-    subparsers = parser.add_subparsers(dest="subparser_name")
-    subparser_names = []
-    # Testing with the tryjobs.
-    tryjob_subparser = subparsers.add_parser("tryjobs")
-    subparser_names.append("tryjobs")
-    tryjob_subparser.add_argument(
-        "--builders",
-        required=True,
-        nargs="+",
-        default=[],
-        help="builders to use for the tryjob testing",
-    )
-
-    # Add argument for custom options for the tryjob.
-    tryjob_subparser.add_argument(
-        "--options",
-        required=False,
-        nargs="+",
-        default=[],
-        help="options to use for the tryjob testing",
-    )
-
-    # Testing with the recipe builders
-    recipe_subparser = subparsers.add_parser("recipe")
-    subparser_names.append("recipe")
-    recipe_subparser.add_argument(
-        "--options",
-        required=False,
-        nargs="+",
-        default=[],
-        help="options passed to the recipe builders",
-    )
-
-    recipe_subparser.add_argument(
-        "--builders",
-        required=True,
-        nargs="+",
-        default=[],
-        help="recipe builders to launch",
+import textwrap
+from typing import List, Optional
+
+from cros_utils import cros_paths
+from cros_utils import git_utils
+from llvm_tools import atomic_write_file
+from llvm_tools import chroot
+from llvm_tools import get_llvm_hash
+from llvm_tools import llvm_next
+from llvm_tools import manifest_utils
+from llvm_tools import upload_llvm_testing_helper_cl
+
+
+def resolve_llvm_sha(sha_or_special: str) -> str:
+    """Resolves the `--sha` flag to an LLVM SHA."""
+    if sha_or_special == "llvm-next":
+        return llvm_next.LLVM_NEXT_HASH
+    if sha_or_special == "google3":
+        return get_llvm_hash.LLVMHash().GetGoogle3LLVMHash()
+    if sha_or_special == "google3-unstable":
+        return get_llvm_hash.LLVMHash().GetGoogle3LLVMHash()
+    # If this looks like a full git SHA, there's no need to sync the upstream
+    # repo.
+    if git_utils.is_full_git_sha(sha_or_special):
+        return sha_or_special
+    return git_utils.resolve_ref(
+        get_llvm_hash.GetCachedUpToDateReadOnlyLLVMRepo().path, sha_or_special
     )
 
-    # Testing with CQ.
-    cq_subparser = subparsers.add_parser("cq")
-    subparser_names.append("cq")
-
-    # Add argument for specify a cq trybot to test along with other cq builders
-    # e.g. llvm, llvm-next or llvm-tot
-    cq_subparser.add_argument(
-        "--cq_trybot",
-        choices=VALID_CQ_TRYBOTS,
-        help="include the trybot to test together with other cq builders "
-        "available: %(choices)s",
-    )
-
-    args_output = parser.parse_args()
-
-    if args_output.subparser_name not in subparser_names:
-        parser.error("one of %s must be specified" % subparser_names)
-
-    if not args_output.chroot_out:
-        chroot_name = args_output.chroot_name
-        if chroot_name == "chroot":
-            out = "out"
-        else:
-            out = f"{chroot_name}_out"
-        args_output.chroot_out = out
-
-    return args_output
-
 
-def UnchangedSinceLastRun(
-    last_tested_file: Optional[Union[Path, str]],
-    arg_dict: Dict,
-) -> bool:
-    """Gets the arguments used for last run
-
-    Args:
-        last_tested_file: The absolute path to the file that contains the
-          arguments for the last run.
-        arg_dict: The arguments used for this run.
-
-    Returns:
-        Return true if the arguments used for last run exist and are the
-        same as the arguments used for this run. Otherwise return false.
-    """
-
-    if not last_tested_file:
-        return False
-
-    # Get the last tested svn version if the file exists.
-    last_arg_dict = None
+def read_last_tried_sha(retry_state: Path) -> Optional[str]:
+    """Reads the last tried SHA from the state file."""
     try:
-        with open(last_tested_file, encoding="utf-8") as f:
-            last_arg_dict = json.load(f)
-
-    except (IOError, ValueError):
-        return False
+        with retry_state.open(encoding="utf-8") as f:
+            return json.load(f)["last_tried_sha"]
+    except FileNotFoundError:
+        return None
 
-    return arg_dict == last_arg_dict
 
+def write_last_tried_sha(retry_state: Path, sha: str):
+    """Writes the last tried SHA to the state file."""
+    with atomic_write_file.atomic_write(retry_state) as f:
+        json.dump({"last_tried_sha": sha}, f)
 
-def AddReviewers(
-    cl: int,
-    reviewers: Iterable[str],
-    chromeos_path: Union[Path, str],
-) -> None:
-    """Add reviewers for the created CL.
 
-    Args:
-        cl: The CL number to add reviewers to.
-        reviewers: Email addresses of reviewers to add.
-        chromeos_path: The absolute path to the chromeos tree.
-    """
+@dataclasses.dataclass(frozen=True)
+class UploadedCLs:
+    """Listing of CL numbers uploaded by a function."""
 
-    gerrit_abs_path = os.path.join(chromeos_path, "chromite/bin/gerrit")
-    for reviewer in reviewers:
-        cmd = [gerrit_abs_path, "reviewers", str(cl), reviewer]
+    internal: List[int]
+    external: List[int]
 
-        subprocess.check_output(cmd)
 
+def upload_one_cl_to_main(
+    git_dir: Path, sha: str, remote: str, topic: Optional[str] = None
+) -> int:
+    """Uploads exactly one SHA from `git_dir`. Returns the CL number.
 
-def AddLinksToCL(
-    tests: Iterable[Dict[str, Any]],
-    cl: int,
-    chromeos_path: Union[Path, str],
-) -> None:
-    """Adds the test link(s) to the CL as a comment.
-
-    Args:
-        tests: Links to the tests.
-        cl: The number of the CL to add the test links to.
-        chromeos_path: Absolute path to the chromeos tree.
+    Raises:
+        AssertionError if more than one CL was uploaded.
     """
+    cl_ids = git_utils.upload_to_gerrit(
+        git_dir,
+        remote=remote,
+        branch=git_utils.CROS_MAIN_BRANCH,
+        ref=sha,
+        topic=topic,
+    )
+    assert len(cl_ids) == 1, f"Expected to upload one CL; uploaded {cl_ids}"
+    return cl_ids[0]
 
-    # NOTE: Invoking `cros_sdk` does not make each tryjob link appear on its
-    # own line, so invoking the `gerrit` command directly instead of using
-    # `cros_sdk` to do it for us.
-    #
-    # FIXME: Need to figure out why `cros_sdk` does not add each tryjob link as
-    # a newline.
-    gerrit_abs_path = os.path.join(chromeos_path, "chromite/bin/gerrit")
-
-    links = ["Started the following tests:"]
-    links.extend(test["link"] for test in tests)
-
-    add_message_cmd = [gerrit_abs_path, "message", str(cl), "\n".join(links)]
-
-    subprocess.check_output(add_message_cmd)
-
-
-# Testing with tryjobs
-def GetCurrentTimeInUTC() -> datetime.datetime:
-    """Returns the current time via `datetime.datetime.utcnow()`."""
-    return datetime.datetime.utcnow()
-
-
-def GetTryJobCommand(
-    change_list: int,
-    extra_change_lists: Iterable[int],
-    options: Iterable[str],
-    builder: str,
-) -> List[str]:
-    """Constructs the 'tryjob' command.
 
-    Args:
-        change_list: The CL obtained from updating the packages.
-        extra_change_lists: Extra change lists that would like to be run
-          alongside the change list of updating the packages.
-        options: Options to be passed into the tryjob command.
-        builder: The builder to be passed into the tryjob command.
+def create_and_upload_test_helpers_cl(
+    chromeos_tree: Path,
+    dry_run: bool,
+    tot: bool,
+) -> int:
+    """Creates & uploads the LLVM 'test helper' CL.
 
     Returns:
-        The 'tryjob' command with the change list of updating the packages and
-        any extra information that was passed into the command line.
+        The CL number of the test-helper CL, an int referencing an external CL.
+        If dry_run is passed, returns 0.
     """
-
-    tryjob_cmd = ["cros", "tryjob", "--yes", "--json", "-g", "%d" % change_list]
-
-    if extra_change_lists:
-        for extra_cl in extra_change_lists:
-            tryjob_cmd.extend(["-g", "%d" % extra_cl])
-
-    if options:
-        tryjob_cmd.extend("--%s" % option for option in options)
-
-    tryjob_cmd.append(builder)
-
-    return tryjob_cmd
+    chromiumos_overlay = chromeos_tree / cros_paths.CHROMIUMOS_OVERLAY
+    sha = upload_llvm_testing_helper_cl.create_helper_cl_commit_in_worktree_of(
+        chromiumos_overlay, tot
+    )
+    if dry_run:
+        logging.info(
+            "--dry-run passed; skipping upload of test-helpers CL %s", sha
+        )
+        return 0
+    return upload_one_cl_to_main(
+        chromiumos_overlay, sha, remote=git_utils.CROS_EXTERNAL_REMOTE
+    )
 
 
-def RunTryJobs(
-    cl_number: int,
-    extra_change_lists: List[int],
-    options: List[str],
-    builders: Iterable[str],
-    chromeos_path: Union[Path, str],
-) -> List[Dict]:
-    """Runs a tryjob/tryjobs.
+def build_manifest_commit_message(
+    llvm_sha: str,
+    llvm_rev: int,
+    cq_depend_external: Optional[int],
+) -> str:
+    msg = textwrap.dedent(
+        f"""\
+        toolchain.xml: update llvm to {llvm_sha} (r{llvm_rev})
 
-    Args:
-        cl_number: The CL created by updating the packages.
-        extra_change_lists: Any extra change lists that would run alongside the
-          CL that was created by updating the packages ('cl_number').
-        options: Any options to be passed into the 'tryjob' command.
-        builders: All the builders to run the 'tryjob' with.
-        chromeos_path: The absolute path to the chromeos tree.
+        BUG=None
+        TEST=CQ
+        """
+    )
+    if cq_depend_external:
+        msg += f"\n\nCq-Depend: chromium:{cq_depend_external}"
+    return msg
+
+
+def create_and_upload_manifest_cl(
+    *,
+    chromeos_tree: Path,
+    llvm_sha: str,
+    llvm_rev: int,
+    cq_depend_external: Optional[int],
+    dry_run: bool,
+    topic: Optional[str],
+    tot: bool,
+) -> int:
+    """Creates & uploads the LLVM update manifest CL.
 
     Returns:
-        A list that contains stdout contents of each tryjob, where stdout is
-        information (a hashmap) about the tryjob. The hashmap also contains
-        stderr if there was an error when running a tryjob.
-
-    Raises:
-        ValueError: Failed to submit a tryjob.
+        The CL number of the manifest CL, an int referencing an internal CL. If
+        dry_run is passed, returns `0`.
     """
+    manifest_internal = chromeos_tree / "manifest-internal"
+    remote = git_utils.CROS_INTERNAL_REMOTE
+    with git_utils.create_worktree(manifest_internal) as worktree:
+        if tot:
+            git_utils.fetch_and_checkout(
+                worktree,
+                remote=remote,
+                branch=git_utils.CROS_MAIN_BRANCH,
+            )
 
-    # Contains the results of each builder.
-    tests = []
-
-    # Run tryjobs with the change list number obtained from updating the
-    # packages and append additional changes lists and options obtained from the
-    # command line.
-    for builder in builders:
-        cmd = GetTryJobCommand(cl_number, extra_change_lists, options, builder)
-
-        out = subprocess.check_output(cmd, cwd=chromeos_path, encoding="utf-8")
-
-        test_output = json.loads(out)
-
-        buildbucket_id = int(test_output[0]["id"])
-
-        tests.append(
-            {
-                "launch_time": str(GetCurrentTimeInUTC()),
-                "link": "http://ci.chromium.org/b/%s" % buildbucket_id,
-                "buildbucket_id": buildbucket_id,
-                "extra_cls": extra_change_lists,
-                "options": options,
-                "builder": [builder],
-            }
+        manifest_utils.update_chromeos_manifest_in_manifest_dir(
+            llvm_sha,
+            worktree,
+            chromeos_tree=chromeos_tree,
         )
+        commit_msg = build_manifest_commit_message(
+            llvm_sha, llvm_rev, cq_depend_external
+        )
+        sha = git_utils.commit_all_changes(worktree, commit_msg)
 
-    AddLinksToCL(tests, cl_number, chromeos_path)
-
-    return tests
+    if dry_run:
+        logging.info("--dry-run passed; skipping upload of manifest CL %s", sha)
+        return 0
 
+    return upload_one_cl_to_main(
+        manifest_internal,
+        sha,
+        remote=remote,
+        topic=topic,
+    )
 
-def StartRecipeBuilders(
-    cl_number: int,
-    extra_change_lists: List[int],
-    options: List[str],
-    builders: List[str],
-    chromeos_path: Union[Path, str],
-) -> List[Dict]:
-    """Launch recipe builders.
 
-    Args:
-        cl_number: The CL created by updating the packages.
-        extra_change_lists: Any extra change lists that would run alongside the
-          CL that was created by updating the packages ('cl_number').
-        options: Any options to be passed into the 'tryjob' command.
-        builders: All the builders to run the 'tryjob' with.
-        chromeos_path: The absolute path to the chromeos tree.
+def add_cl_comment(
+    chromeos_tree: Path,
+    cl_id: int,
+    internal: bool,
+    comment: str,
+):
+    """Creates & uploads the LLVM update manifest CL.
 
     Returns:
-        A list that contains stdout contents of each builder, where stdout is
-        information (a hashmap) about the tryjob. The hashmap also contains
-        stderr if there was an error when running a tryjob.
-
-    Raises:
-        ValueError: Failed to start a builder.
+        The CL number of the manifest CL, an int referencing an internal CL.
     """
+    cmd = ["gerrit"]
+    if internal:
+        cmd.append("--internal")
+    cmd += ("message", str(cl_id), comment)
+    subprocess.run(
+        cmd,
+        check=True,
+        cwd=chromeos_tree,
+        stdin=subprocess.DEVNULL,
+    )
 
-    # Contains the results of each builder.
-    tests = []
-
-    # Launch a builders with the change list number obtained from updating the
-    # packages and append additional changes lists and options obtained from the
-    # command line.
-    for builder in builders:
-        cmd = ["bb", "add", "-json"]
-
-        if cl_number:
-            cmd.extend(["-cl", "crrev.com/c/%d" % cl_number])
-
-        if extra_change_lists:
-            for cl in extra_change_lists:
-                cmd.extend(["-cl", "crrev.com/c/%d" % cl])
-
-        if options:
-            cmd.extend(options)
-
-        cmd.append(builder)
-
-        out = subprocess.check_output(cmd, cwd=chromeos_path, encoding="utf-8")
-
-        test_output = json.loads(out)
 
-        tests.append(
-            {
-                "launch_time": test_output["createTime"],
-                "link": "http://ci.chromium.org/b/%s" % test_output["id"],
-                "buildbucket_id": test_output["id"],
-                "extra_cls": extra_change_lists,
-                "options": options,
-                "builder": [builder],
-            }
+def create_and_upload_cls(
+    *,
+    chromeos_tree: Path,
+    llvm_sha: str,
+    llvm_rev: int,
+    include_test_helpers: bool,
+    dry_run: bool,
+    manifest_gerrit_topic: Optional[str],
+    tot: bool,
+) -> UploadedCLs:
+    external_cls = []
+    if include_test_helpers:
+        logging.info("Uploading test-helper CL...")
+        test_helper_cl = create_and_upload_test_helpers_cl(
+            chromeos_tree, dry_run, tot
         )
+        external_cls.append(test_helper_cl)
+    else:
+        test_helper_cl = None
+    logging.info("Creating LLVM update CL...")
+    manifest_cl = create_and_upload_manifest_cl(
+        chromeos_tree=chromeos_tree,
+        llvm_sha=llvm_sha,
+        llvm_rev=llvm_rev,
+        cq_depend_external=test_helper_cl,
+        dry_run=dry_run,
+        topic=manifest_gerrit_topic,
+        tot=tot,
+    )
+    # Notably, this is meant to catch `test_helper_cl == 0` (dry_run) or
+    # `test_helper_cl == None` (if none was uploaded)
+    if test_helper_cl:
+        add_cl_comment(
+            chromeos_tree,
+            test_helper_cl,
+            internal=False,
+            comment=f"Corresponding Manifest update: crrev.com/i/{manifest_cl}",
+        )
+    return UploadedCLs(
+        internal=[manifest_cl],
+        external=external_cls,
+    )
 
-    AddLinksToCL(tests, cl_number, chromeos_path)
-
-    return tests
-
-
-# Testing with CQ
-def GetCQDependString(dependent_cls: List[int]) -> Optional[str]:
-    """Get CQ dependency string e.g. `Cq-Depend: chromium:MM, chromium:NN`."""
-
-    if not dependent_cls:
-        return None
-
-    # Cq-Depend must start a new paragraph prefixed with "Cq-Depend".
-    return "Cq-Depend: " + ", ".join(f"chromium:{x}" for x in dependent_cls)
-
-
-def GetCQIncludeTrybotsString(trybot: Optional[str]) -> Optional[str]:
-    """Get Cq-Include-Trybots string, for more llvm testings"""
-
-    if not trybot:
-        return None
-
-    if trybot not in VALID_CQ_TRYBOTS:
-        raise ValueError("%s is not a valid llvm trybot" % trybot)
-
-    # Cq-Include-Trybots must start a new paragraph prefixed
-    # with "Cq-Include-Trybots".
-    return "Cq-Include-Trybots:chromeos/cq:cq-%s-orchestrator" % trybot
-
-
-def StartCQDryRun(
-    cl: int,
-    dependent_cls: List[int],
-    chromeos_path: Union[Path, str],
-) -> None:
-    """Start CQ dry run for the changelist and dependencies."""
-
-    gerrit_abs_path = os.path.join(chromeos_path, "chromite/bin/gerrit")
-
-    cl_list = [cl]
-    cl_list.extend(dependent_cls)
-
-    for changes in cl_list:
-        cq_dry_run_cmd = [gerrit_abs_path, "label-cq", str(changes), "1"]
-
-        subprocess.check_output(cq_dry_run_cmd)
-
-
-def main():
-    """Updates the packages' LLVM hash and run tests.
-
-    Raises:
-        AssertionError: The script was run inside the chroot.
-    """
-
-    chroot.VerifyOutsideChroot()
-
-    args_output = GetCommandLineArgs()
-
-    chroot.VerifyChromeOSRoot(args_output.chromeos_path)
-
-    svn_option = args_output.llvm_version
 
-    git_hash, svn_version = get_llvm_hash.GetLLVMHashAndVersionFromSVNOption(
-        svn_option
+def make_gerrit_cq_dry_run_command(cls: List[int], internal: bool) -> List[str]:
+    assert cls, "Can't make a dry-run command with no CLs to dry-run."
+    cmd = ["gerrit"]
+    if internal:
+        cmd.append("--internal")
+    cmd.append("label-cq")
+    cmd += (str(x) for x in cls)
+    cmd.append("1")
+    return cmd
+
+
+def cq_dry_run_cls(chromeos_tree: Path, cls: UploadedCLs):
+    """Sets CQ+1 on the given uploaded CL listing."""
+    # At the time of writing, this is expected given the context of the script.
+    # Can easily refactor to make `cls.internal` optional, though.
+    gerrit_cmds = []
+    assert cls.internal, "LLVM update without internal CLs?"
+    gerrit_cmds.append(
+        make_gerrit_cq_dry_run_command(cls.internal, internal=True)
     )
-
-    # There is no need to run tryjobs when all the key parameters remain
-    # unchanged from last time.
-
-    # If --last_tested is specified, check if the current run has the same
-    # arguments last time --last_tested is used.
-    if args_output.last_tested:
-        chroot_file_paths = chroot.GetChrootEbuildPaths(
-            args_output.chromeos_path,
-            update_chromeos_llvm_hash.DEFAULT_PACKAGES,
-            args_output.chroot_name,
-            args_output.chroot_out,
+    if cls.external:
+        gerrit_cmds.append(
+            make_gerrit_cq_dry_run_command(cls.external, internal=False)
         )
-        arg_dict = {
-            "svn_version": svn_version,
-            "ebuilds": chroot_file_paths,
-            "extra_cls": args_output.extra_change_lists,
-        }
-        if args_output.subparser_name in ("tryjobs", "recipe"):
-            arg_dict["builders"] = args_output.builders
-            arg_dict["tryjob_options"] = args_output.options
-        if UnchangedSinceLastRun(args_output.last_tested, arg_dict):
-            print(
-                "svn version (%d) matches the last tested svn version in %s"
-                % (svn_version, args_output.last_tested)
-            )
-            return
+    for cmd in gerrit_cmds:
+        subprocess.run(
+            cmd,
+            check=True,
+            cwd=chromeos_tree,
+            stdin=subprocess.DEVNULL,
+        )
+
 
-    llvm_variant = update_chromeos_llvm_hash.LLVMVariant.current
-    if args_output.is_llvm_next:
-        llvm_variant = update_chromeos_llvm_hash.LLVMVariant.next
-
-    extra_commit_msg_lines = []
-    if args_output.subparser_name == "cq":
-        footers = []
-        cq_depend_msg = GetCQDependString(args_output.extra_change_lists)
-        if cq_depend_msg:
-            footers.append(cq_depend_msg)
-        cq_trybot_msg = GetCQIncludeTrybotsString(args_output.cq_trybot)
-        if cq_trybot_msg:
-            footers.append(cq_trybot_msg)
-
-        # We want a single blank line before any of these, so Git properly
-        # interprets them as a footer.
-        if footers:
-            extra_commit_msg_lines.append("")
-            extra_commit_msg_lines += footers
-
-    change_list = update_chromeos_llvm_hash.UpdatePackages(
-        packages=update_chromeos_llvm_hash.DEFAULT_PACKAGES,
-        manifest_packages=[],
-        llvm_variant=llvm_variant,
-        git_hash=git_hash,
-        svn_version=svn_version,
-        chroot_opts=update_chromeos_llvm_hash.ChrootOpts(
-            chromeos_root=Path(args_output.chromeos_path),
-            chroot_name=args_output.chroot_name,
-            out_name=args_output.chroot_out,
-        ),
-        mode=failure_modes.FailureModes.DISABLE_PATCHES,
-        git_hash_source=svn_option,
-        extra_commit_msg_lines=extra_commit_msg_lines,
-        # b/331607705: set WIP on these changes, so the code-review-nudger bot
-        # doesn't ping them.
-        wip=True,
+def parse_opts(argv: List[str]) -> argparse.Namespace:
+    """Parse command-line options."""
+    parser = argparse.ArgumentParser(
+        description=__doc__,
+        formatter_class=argparse.RawDescriptionHelpFormatter,
     )
+    parser.add_argument(
+        "--chromeos-tree",
+        type=Path,
+        help="""
+        ChromeOS tree to make modifications in. Will be inferred if none
+        is passed.
+        """,
+    )
+    parser.add_argument(
+        "--cq",
+        action="store_true",
+        help="After uploading, set CQ+1 on the CL(s) that were uploaded.",
+    )
+    parser.add_argument(
+        "--dry-run",
+        action="store_true",
+        help="If passed, only commit changes locally; don't upload them.",
+    )
+    parser.add_argument(
+        "--include-llvm-test-helper-cls",
+        action="store_true",
+        help="""
+        Also upload CL(s) meant to ease LLVM testing. Namely, this will include
+        logic to disable `-Werror` on packages, and logic to disable patches
+        that no longer apply to LLVM.
+        """,
+    )
+    parser.add_argument(
+        "--manifest-gerrit-topic",
+        help="""
+        If provided, the internal-manifest CL will be uploaded with the given
+        Gerrit topic. This is helpful to associate many CLs over time.
+        """,
+    )
+    parser.add_argument(
+        "--tot",
+        action="store_true",
+        help="""
+        If passed, modified repos will be `git fetch`ed and this script will
+        work on their main branches, rather than working on the version you
+        have locally.
+        """,
+    )
+    parser.add_argument(
+        "--retry-state",
+        type=Path,
+        help="""
+        If passed, this will keep script state in the given file. At the
+        moment, this file is only used to ensure that subsequent runs of this
+        script don't trigger identical uploads.
+        """,
+    )
+    parser.add_argument(
+        "--sha",
+        required=True,
+        help="""
+        SHA to use. This can either be an LLVM SHA, or a special value:
+        `llvm-next`, `google3` or `google3-unstable`.
+        """,
+    )
+    return parser.parse_args(argv)
+
 
-    AddReviewers(
-        change_list.cl_number, args_output.reviewers, args_output.chromeos_path
+def main(argv: List[str]) -> None:
+    logging.basicConfig(
+        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
+        "%(message)s",
+        level=logging.INFO,
     )
 
-    print("Successfully updated packages to %d" % svn_version)
-    print("Gerrit URL: %s" % change_list.url)
-    print("Change list number: %d" % change_list.cl_number)
-
-    if args_output.subparser_name == "tryjobs":
-        tests = RunTryJobs(
-            change_list.cl_number,
-            args_output.extra_change_lists,
-            args_output.options,
-            args_output.builders,
-            args_output.chromeos_path,
-        )
-        print("Tests:")
-        for test in tests:
-            print(test)
-    elif args_output.subparser_name == "recipe":
-        tests = StartRecipeBuilders(
-            change_list.cl_number,
-            args_output.extra_change_lists,
-            args_output.options,
-            args_output.builders,
-            args_output.chromeos_path,
+    opts = parse_opts(argv)
+    dry_run = opts.dry_run
+    chromeos_tree = opts.chromeos_tree
+    if not chromeos_tree:
+        chromeos_tree = chroot.FindChromeOSRootAboveToolchainUtils()
+
+    new_sha = resolve_llvm_sha(opts.sha)
+    logging.info("Using LLVM SHA %s...", new_sha)
+    if opts.retry_state:
+        last_tried_sha = read_last_tried_sha(opts.retry_state)
+        if last_tried_sha == new_sha:
+            logging.info("New SHA is the same as the last tried SHA; quit.")
+            return
+        logging.info(
+            "New SHA is different than the last tried SHA (%s).", last_tried_sha
         )
-        print("Tests:")
-        for test in tests:
-            print(test)
 
-    else:
-        StartCQDryRun(
-            change_list.cl_number,
-            args_output.extra_change_lists,
-            args_output.chromeos_path,
+    logging.info("Getting LLVM revision for SHA %s...", new_sha)
+    new_rev = (
+        get_llvm_hash.GetCachedUpToDateReadOnlyLLVMRepo().GetRevisionFromHash(
+            new_sha
         )
+    )
+    logging.info("LLVM SHA %s == r%d", new_sha, new_rev)
+    uploaded_cls = create_and_upload_cls(
+        chromeos_tree=chromeos_tree,
+        llvm_sha=new_sha,
+        llvm_rev=new_rev,
+        include_test_helpers=opts.include_llvm_test_helper_cls,
+        dry_run=dry_run,
+        manifest_gerrit_topic=opts.manifest_gerrit_topic,
+        tot=opts.tot,
+    )
 
-    # If --last_tested is specified, record the arguments used
-    if args_output.last_tested:
-        with open(args_output.last_tested, "w", encoding="utf-8") as f:
-            json.dump(arg_dict, f, indent=2)
+    if dry_run:
+        logging.info("--dry-run passed; exiting")
+        return
 
+    if opts.cq:
+        logging.info("Setting CQ+1 on the CLs...")
+        cq_dry_run_cls(chromeos_tree, uploaded_cls)
 
-if __name__ == "__main__":
-    main()
+    if opts.retry_state:
+        write_last_tried_sha(opts.retry_state, new_sha)
diff --git a/llvm_tools/update_packages_and_run_tests_test.py b/llvm_tools/update_packages_and_run_tests_test.py
new file mode 100644
index 00000000..d75d7e91
--- /dev/null
+++ b/llvm_tools/update_packages_and_run_tests_test.py
@@ -0,0 +1,78 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Tests for update_packages_and_run_tests.py"""
+
+from pathlib import Path
+import shutil
+import subprocess
+import tempfile
+import unittest
+from unittest import mock
+
+from llvm_tools import update_packages_and_run_tests
+
+
+class Test(unittest.TestCase):
+    """Tests for update_packages_and_run_tests.py"""
+
+    def make_tempdir(self) -> Path:
+        tempdir = tempfile.mkdtemp("run_llvm_tests_at_sha_test_")
+        self.addCleanup(shutil.rmtree, tempdir)
+        return Path(tempdir)
+
+    def test_sha_state_file_handles_file_not_existing(self):
+        tempdir = self.make_tempdir()
+        self.assertIsNone(
+            update_packages_and_run_tests.read_last_tried_sha(
+                tempdir / "does-not-exist"
+            )
+        )
+
+    def test_sha_state_file_round_trips(self):
+        tempdir = self.make_tempdir()
+        state_file = tempdir / "state.json"
+        sha = "a" * 40
+        update_packages_and_run_tests.write_last_tried_sha(state_file, sha)
+        self.assertEqual(
+            update_packages_and_run_tests.read_last_tried_sha(state_file), sha
+        )
+
+    @mock.patch.object(subprocess, "run")
+    def test_gerrit_cq_dry_run_runs_correct_gerrit_commands(self, mock_run):
+        chromeos_tree = self.make_tempdir()
+        update_packages_and_run_tests.cq_dry_run_cls(
+            chromeos_tree,
+            update_packages_and_run_tests.UploadedCLs(
+                internal=[123],
+                external=[456, 789],
+            ),
+        )
+        self.assertEqual(mock_run.call_count, 2)
+        mock_run.assert_any_call(
+            ["gerrit", "label-cq", "456", "789", "1"],
+            check=True,
+            cwd=chromeos_tree,
+            stdin=subprocess.DEVNULL,
+        )
+        mock_run.assert_any_call(
+            ["gerrit", "--internal", "label-cq", "123", "1"],
+            check=True,
+            cwd=chromeos_tree,
+            stdin=subprocess.DEVNULL,
+        )
+
+    @mock.patch.object(subprocess, "run")
+    def test_gerrit_cq_dry_run_only_runs_one_command_if_necessary(
+        self, mock_run
+    ):
+        chromeos_tree = self.make_tempdir()
+        update_packages_and_run_tests.cq_dry_run_cls(
+            chromeos_tree,
+            update_packages_and_run_tests.UploadedCLs(
+                internal=[123],
+                external=[],
+            ),
+        )
+        mock_run.assert_called_once()
diff --git a/llvm_tools/update_packages_and_run_tests_unittest.py b/llvm_tools/update_packages_and_run_tests_unittest.py
deleted file mode 100755
index 9abdc199..00000000
--- a/llvm_tools/update_packages_and_run_tests_unittest.py
+++ /dev/null
@@ -1,549 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Unittests for running tests after updating packages."""
-
-import json
-import subprocess
-import unittest
-from unittest import mock
-
-import chroot
-import get_llvm_hash
-import git
-import test_helpers
-import update_chromeos_llvm_hash
-import update_packages_and_run_tests
-
-
-# Testing with tryjobs.
-class UpdatePackagesAndRunTryjobsTest(unittest.TestCase):
-    """Unittests when running tryjobs after updating packages."""
-
-    def testNoLastTestedFile(self):
-        self.assertEqual(
-            update_packages_and_run_tests.UnchangedSinceLastRun(None, {}), False
-        )
-
-    def testEmptyLastTestedFile(self):
-        with test_helpers.CreateTemporaryFile() as temp_file:
-            self.assertEqual(
-                update_packages_and_run_tests.UnchangedSinceLastRun(
-                    temp_file, {}
-                ),
-                False,
-            )
-
-    def testLastTestedFileDoesNotExist(self):
-        # Simulate 'open()' on a lasted tested file that does not exist.
-        mock.mock_open(read_data="")
-
-        self.assertEqual(
-            update_packages_and_run_tests.UnchangedSinceLastRun(
-                "/some/file/that/does/not/exist.txt", {}
-            ),
-            False,
-        )
-
-    def testMatchedLastTestedFile(self):
-        with test_helpers.CreateTemporaryFile() as last_tested_file:
-            arg_dict = {
-                "svn_version": 1234,
-                "ebuilds": [
-                    "/path/to/package1-r2.ebuild",
-                    "/path/to/package2/package2-r3.ebuild",
-                ],
-                "builders": [
-                    "kevin-llvm-next-toolchain-tryjob",
-                    "eve-llvm-next-toolchain-tryjob",
-                ],
-                "extra_cls": [10, 1],
-                "tryjob_options": ["latest-toolchain", "hwtest"],
-            }
-
-            with open(last_tested_file, "w", encoding="utf-8") as f:
-                f.write(json.dumps(arg_dict, indent=2))
-
-            self.assertEqual(
-                update_packages_and_run_tests.UnchangedSinceLastRun(
-                    last_tested_file, arg_dict
-                ),
-                True,
-            )
-
-    def testGetTryJobCommandWithNoExtraInformation(self):
-        change_list = 1234
-
-        builder = "nocturne"
-
-        expected_cmd = [
-            "cros",
-            "tryjob",
-            "--yes",
-            "--json",
-            "-g",
-            "%d" % change_list,
-            builder,
-        ]
-
-        self.assertEqual(
-            update_packages_and_run_tests.GetTryJobCommand(
-                change_list, None, None, builder
-            ),
-            expected_cmd,
-        )
-
-    def testGetTryJobCommandWithExtraInformation(self):
-        change_list = 4321
-        extra_cls = [1000, 10]
-        options = ["option1", "option2"]
-        builder = "kevin"
-
-        expected_cmd = [
-            "cros",
-            "tryjob",
-            "--yes",
-            "--json",
-            "-g",
-            "%d" % change_list,
-            "-g",
-            "%d" % extra_cls[0],
-            "-g",
-            "%d" % extra_cls[1],
-            "--%s" % options[0],
-            "--%s" % options[1],
-            builder,
-        ]
-
-        self.assertEqual(
-            update_packages_and_run_tests.GetTryJobCommand(
-                change_list, extra_cls, options, builder
-            ),
-            expected_cmd,
-        )
-
-    @mock.patch.object(
-        update_packages_and_run_tests,
-        "GetCurrentTimeInUTC",
-        return_value="2019-09-09",
-    )
-    @mock.patch.object(update_packages_and_run_tests, "AddLinksToCL")
-    @mock.patch.object(subprocess, "check_output")
-    def testSuccessfullySubmittedTryJob(
-        self, mock_cmd, mock_add_links_to_cl, mock_launch_time
-    ):
-        expected_cmd = [
-            "cros",
-            "tryjob",
-            "--yes",
-            "--json",
-            "-g",
-            "%d" % 900,
-            "-g",
-            "%d" % 1200,
-            "--some_option",
-            "builder1",
-        ]
-
-        bb_id = "1234"
-        url = "http://ci.chromium.org/b/%s" % bb_id
-
-        mock_cmd.return_value = json.dumps([{"id": bb_id, "url": url}])
-
-        chromeos_path = "/some/path/to/chromeos"
-        cl = 900
-        extra_cls = [1200]
-        options = ["some_option"]
-        builders = ["builder1"]
-
-        tests = update_packages_and_run_tests.RunTryJobs(
-            cl, extra_cls, options, builders, chromeos_path
-        )
-
-        expected_tests = [
-            {
-                "launch_time": mock_launch_time.return_value,
-                "link": url,
-                "buildbucket_id": int(bb_id),
-                "extra_cls": extra_cls,
-                "options": options,
-                "builder": builders,
-            }
-        ]
-
-        self.assertEqual(tests, expected_tests)
-
-        mock_cmd.assert_called_once_with(
-            expected_cmd, cwd=chromeos_path, encoding="utf-8"
-        )
-
-        mock_add_links_to_cl.assert_called_once()
-
-    @mock.patch.object(update_packages_and_run_tests, "AddLinksToCL")
-    @mock.patch.object(subprocess, "check_output")
-    def testSuccessfullySubmittedRecipeBuilders(
-        self, mock_cmd, mock_add_links_to_cl
-    ):
-        expected_cmd = [
-            "bb",
-            "add",
-            "-json",
-            "-cl",
-            "crrev.com/c/%s" % 900,
-            "-cl",
-            "crrev.com/c/%s" % 1200,
-            "some_option",
-            "builder1",
-        ]
-
-        bb_id = "1234"
-        create_time = "2020-04-18T00:03:53.978767Z"
-
-        mock_cmd.return_value = json.dumps(
-            {"id": bb_id, "createTime": create_time}
-        )
-
-        chromeos_path = "/some/path/to/chromeos"
-        cl = 900
-        extra_cls = [1200]
-        options = ["some_option"]
-        builders = ["builder1"]
-
-        tests = update_packages_and_run_tests.StartRecipeBuilders(
-            cl, extra_cls, options, builders, chromeos_path
-        )
-
-        expected_tests = [
-            {
-                "launch_time": create_time,
-                "link": "http://ci.chromium.org/b/%s" % bb_id,
-                "buildbucket_id": bb_id,
-                "extra_cls": extra_cls,
-                "options": options,
-                "builder": builders,
-            }
-        ]
-
-        self.assertEqual(tests, expected_tests)
-
-        mock_cmd.assert_called_once_with(
-            expected_cmd, cwd=chromeos_path, encoding="utf-8"
-        )
-
-        mock_add_links_to_cl.assert_called_once()
-
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    def testSuccessfullyAddedTestLinkToCL(self, mock_exec_cmd):
-        chromeos_path = "/abs/path/to/chromeos"
-
-        test_cl_number = 1000
-
-        tests = [{"link": "https://some_tryjob_link.com"}]
-
-        update_packages_and_run_tests.AddLinksToCL(
-            tests, test_cl_number, chromeos_path
-        )
-
-        expected_gerrit_message = [
-            "%s/chromite/bin/gerrit" % chromeos_path,
-            "message",
-            str(test_cl_number),
-            "Started the following tests:\n%s" % tests[0]["link"],
-        ]
-
-        mock_exec_cmd.assert_called_once_with(expected_gerrit_message)
-
-    @mock.patch.object(update_packages_and_run_tests, "RunTryJobs")
-    @mock.patch.object(update_chromeos_llvm_hash, "UpdatePackages")
-    @mock.patch.object(update_packages_and_run_tests, "GetCommandLineArgs")
-    @mock.patch.object(get_llvm_hash, "GetLLVMHashAndVersionFromSVNOption")
-    @mock.patch.object(chroot, "VerifyChromeOSRoot")
-    @mock.patch.object(chroot, "VerifyOutsideChroot", return_value=True)
-    @mock.patch.object(chroot, "GetChrootEbuildPaths")
-    def testUpdatedLastTestedFileWithNewTestedRevision(
-        self,
-        mock_get_chroot_build_paths,
-        mock_outside_chroot,
-        mock_chromeos_root,
-        mock_get_hash_and_version,
-        mock_get_commandline_args,
-        mock_update_packages,
-        mock_run_tryjobs,
-    ):
-        # Create a temporary file to simulate the last tested file that
-        # contains a revision.
-        with test_helpers.CreateTemporaryFile() as last_tested_file:
-            builders = [
-                "kevin-llvm-next-toolchain-tryjob",
-                "eve-llvm-next-toolchain-tryjob",
-            ]
-            extra_cls = [10, 1]
-            tryjob_options = ["latest-toolchain", "hwtest"]
-            ebuilds = [
-                "/path/to/package1/package1-r2.ebuild",
-                "/path/to/package2/package2-r3.ebuild",
-            ]
-
-            arg_dict = {
-                "svn_version": 100,
-                "ebuilds": ebuilds,
-                "builders": builders,
-                "extra_cls": extra_cls,
-                "tryjob_options": tryjob_options,
-            }
-            # Parepared last tested file
-            with open(last_tested_file, "w", encoding="utf-8") as f:
-                json.dump(arg_dict, f, indent=2)
-
-            # Call with a changed LLVM svn version
-            args_output = test_helpers.ArgsOutputTest()
-            args_output.chroot_name = "custom-chroot"
-            args_output.chroot_out = "custom-chroot_out"
-            args_output.is_llvm_next = True
-            args_output.extra_change_lists = extra_cls
-            args_output.last_tested = last_tested_file
-            args_output.reviewers = []
-
-            args_output.subparser_name = "tryjobs"
-            args_output.builders = builders
-            args_output.options = tryjob_options
-
-            mock_get_commandline_args.return_value = args_output
-
-            mock_get_chroot_build_paths.return_value = ebuilds
-
-            mock_get_hash_and_version.return_value = ("a123testhash2", 200)
-
-            mock_update_packages.return_value = git.CommitContents(
-                url="https://some_cl_url.com", cl_number=12345
-            )
-
-            mock_run_tryjobs.return_value = [
-                {"link": "https://some_tryjob_url.com", "buildbucket_id": 1234}
-            ]
-
-            update_packages_and_run_tests.main()
-
-            # Verify that the lasted tested file has been updated to the new
-            # LLVM revision.
-            with open(last_tested_file, encoding="utf-8") as f:
-                arg_dict = json.load(f)
-
-                self.assertEqual(arg_dict["svn_version"], 200)
-
-        mock_outside_chroot.assert_called_once()
-
-        mock_chromeos_root.assert_called_once()
-
-        mock_get_commandline_args.assert_called_once()
-
-        mock_get_hash_and_version.assert_called_once()
-
-        mock_run_tryjobs.assert_called_once()
-
-        mock_update_packages.assert_called_once()
-        commit_msg_lines = mock_update_packages.call_args[1][
-            "extra_commit_msg_lines"
-        ]
-        self.assertTrue(
-            isinstance(commit_msg_lines, list), repr(commit_msg_lines)
-        )
-
-
-class UpdatePackagesAndRunTestCQTest(unittest.TestCase):
-    """Unittests for CQ dry run after updating packages."""
-
-    def testGetCQDependString(self):
-        test_no_changelists = []
-        test_single_changelist = [1234]
-        test_multiple_changelists = [1234, 5678]
-
-        self.assertIsNone(
-            update_packages_and_run_tests.GetCQDependString(test_no_changelists)
-        )
-
-        self.assertEqual(
-            update_packages_and_run_tests.GetCQDependString(
-                test_single_changelist
-            ),
-            "Cq-Depend: chromium:1234",
-        )
-
-        self.assertEqual(
-            update_packages_and_run_tests.GetCQDependString(
-                test_multiple_changelists
-            ),
-            "Cq-Depend: chromium:1234, chromium:5678",
-        )
-
-    def testGetCQIncludeTrybotsString(self):
-        test_no_trybot = None
-        test_valid_trybot = "llvm-next"
-        test_invalid_trybot = "invalid-name"
-
-        self.assertIsNone(
-            update_packages_and_run_tests.GetCQIncludeTrybotsString(
-                test_no_trybot
-            )
-        )
-
-        self.assertEqual(
-            update_packages_and_run_tests.GetCQIncludeTrybotsString(
-                test_valid_trybot
-            ),
-            "Cq-Include-Trybots:chromeos/cq:cq-llvm-next-orchestrator",
-        )
-
-        with self.assertRaises(ValueError) as context:
-            update_packages_and_run_tests.GetCQIncludeTrybotsString(
-                test_invalid_trybot
-            )
-
-        self.assertIn("is not a valid llvm trybot", str(context.exception))
-
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    def testStartCQDryRunNoDeps(self, mock_exec_cmd):
-        chromeos_path = "/abs/path/to/chromeos"
-        test_cl_number = 1000
-
-        # test with no deps cls.
-        extra_cls = []
-        update_packages_and_run_tests.StartCQDryRun(
-            test_cl_number, extra_cls, chromeos_path
-        )
-
-        expected_gerrit_message = [
-            "%s/chromite/bin/gerrit" % chromeos_path,
-            "label-cq",
-            str(test_cl_number),
-            "1",
-        ]
-
-        mock_exec_cmd.assert_called_once_with(expected_gerrit_message)
-
-    # Mock ExecCommandAndCaptureOutput for the gerrit command execution.
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    # test with a single deps cl.
-    def testStartCQDryRunSingleDep(self, mock_exec_cmd):
-        chromeos_path = "/abs/path/to/chromeos"
-        test_cl_number = 1000
-
-        extra_cls = [2000]
-        update_packages_and_run_tests.StartCQDryRun(
-            test_cl_number, extra_cls, chromeos_path
-        )
-
-        expected_gerrit_cmd_1 = [
-            "%s/chromite/bin/gerrit" % chromeos_path,
-            "label-cq",
-            str(test_cl_number),
-            "1",
-        ]
-        expected_gerrit_cmd_2 = [
-            "%s/chromite/bin/gerrit" % chromeos_path,
-            "label-cq",
-            str(2000),
-            "1",
-        ]
-
-        self.assertEqual(mock_exec_cmd.call_count, 2)
-        self.assertEqual(
-            mock_exec_cmd.call_args_list[0], mock.call(expected_gerrit_cmd_1)
-        )
-        self.assertEqual(
-            mock_exec_cmd.call_args_list[1], mock.call(expected_gerrit_cmd_2)
-        )
-
-    # Mock ExecCommandAndCaptureOutput for the gerrit command execution.
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    def testStartCQDryRunMultipleDep(self, mock_exec_cmd):
-        chromeos_path = "/abs/path/to/chromeos"
-        test_cl_number = 1000
-
-        # test with multiple deps cls.
-        extra_cls = [3000, 4000]
-        update_packages_and_run_tests.StartCQDryRun(
-            test_cl_number, extra_cls, chromeos_path
-        )
-
-        expected_gerrit_cmd_1 = [
-            "%s/chromite/bin/gerrit" % chromeos_path,
-            "label-cq",
-            str(test_cl_number),
-            "1",
-        ]
-        expected_gerrit_cmd_2 = [
-            "%s/chromite/bin/gerrit" % chromeos_path,
-            "label-cq",
-            str(3000),
-            "1",
-        ]
-        expected_gerrit_cmd_3 = [
-            "%s/chromite/bin/gerrit" % chromeos_path,
-            "label-cq",
-            str(4000),
-            "1",
-        ]
-
-        self.assertEqual(mock_exec_cmd.call_count, 3)
-        self.assertEqual(
-            mock_exec_cmd.call_args_list[0], mock.call(expected_gerrit_cmd_1)
-        )
-        self.assertEqual(
-            mock_exec_cmd.call_args_list[1], mock.call(expected_gerrit_cmd_2)
-        )
-        self.assertEqual(
-            mock_exec_cmd.call_args_list[2], mock.call(expected_gerrit_cmd_3)
-        )
-
-    # Mock ExecCommandAndCaptureOutput for the gerrit command execution.
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    # test with no reviewers.
-    def testAddReviewersNone(self, mock_exec_cmd):
-        chromeos_path = "/abs/path/to/chromeos"
-        reviewers = []
-        test_cl_number = 1000
-
-        update_packages_and_run_tests.AddReviewers(
-            test_cl_number, reviewers, chromeos_path
-        )
-        self.assertTrue(mock_exec_cmd.not_called)
-
-    # Mock ExecCommandAndCaptureOutput for the gerrit command execution.
-    @mock.patch.object(subprocess, "check_output", return_value=None)
-    # test with multiple reviewers.
-    def testAddReviewersMultiple(self, mock_exec_cmd):
-        chromeos_path = "/abs/path/to/chromeos"
-        reviewers = ["none1@chromium.org", "none2@chromium.org"]
-        test_cl_number = 1000
-
-        update_packages_and_run_tests.AddReviewers(
-            test_cl_number, reviewers, chromeos_path
-        )
-
-        expected_gerrit_cmd_1 = [
-            "%s/chromite/bin/gerrit" % chromeos_path,
-            "reviewers",
-            str(test_cl_number),
-            "none1@chromium.org",
-        ]
-        expected_gerrit_cmd_2 = [
-            "%s/chromite/bin/gerrit" % chromeos_path,
-            "reviewers",
-            str(test_cl_number),
-            "none2@chromium.org",
-        ]
-
-        self.assertEqual(mock_exec_cmd.call_count, 2)
-        self.assertEqual(
-            mock_exec_cmd.call_args_list[0], mock.call(expected_gerrit_cmd_1)
-        )
-        self.assertEqual(
-            mock_exec_cmd.call_args_list[1], mock.call(expected_gerrit_cmd_2)
-        )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/update_tryjob_status.py b/llvm_tools/update_tryjob_status.py
deleted file mode 100755
index e68bd6a8..00000000
--- a/llvm_tools/update_tryjob_status.py
+++ /dev/null
@@ -1,307 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Updates the status of a tryjob."""
-
-import argparse
-import enum
-import json
-import os
-import subprocess
-import sys
-
-import chroot
-import test_helpers
-
-
-class TryjobStatus(enum.Enum):
-    """Values for the 'status' field of a tryjob."""
-
-    GOOD = "good"
-    BAD = "bad"
-    PENDING = "pending"
-    SKIP = "skip"
-
-    # Executes the script passed into the command line (this script's exit code
-    # determines the 'status' value of the tryjob).
-    CUSTOM_SCRIPT = "custom_script"
-
-
-class CustomScriptStatus(enum.Enum):
-    """Exit code values of a custom script."""
-
-    # NOTE: Not using 1 for 'bad' because the custom script can raise an
-    # exception which would cause the exit code of the script to be 1, so the
-    # tryjob's 'status' would be updated when there is an exception.
-    #
-    # Exit codes are as follows:
-    #   0: 'good'
-    #   124: 'bad'
-    #   125: 'skip'
-    GOOD = 0
-    BAD = 124
-    SKIP = 125
-
-
-custom_script_exit_value_mapping = {
-    CustomScriptStatus.GOOD.value: TryjobStatus.GOOD.value,
-    CustomScriptStatus.BAD.value: TryjobStatus.BAD.value,
-    CustomScriptStatus.SKIP.value: TryjobStatus.SKIP.value,
-}
-
-
-def GetCommandLineArgs():
-    """Parses the command line for the command line arguments."""
-
-    # Default absoute path to the chroot if not specified.
-    cros_root = os.path.expanduser("~")
-    cros_root = os.path.join(cros_root, "chromiumos")
-
-    # Create parser and add optional command-line arguments.
-    parser = argparse.ArgumentParser(
-        description="Updates the status of a tryjob."
-    )
-
-    # Add argument for the JSON file to use for the update of a tryjob.
-    parser.add_argument(
-        "--status_file",
-        required=True,
-        help="The absolute path to the JSON file that contains the tryjobs "
-        "used for bisecting LLVM.",
-    )
-
-    # Add argument that sets the 'status' field to that value.
-    parser.add_argument(
-        "--set_status",
-        required=True,
-        choices=[tryjob_status.value for tryjob_status in TryjobStatus],
-        help='Sets the "status" field of the tryjob.',
-    )
-
-    # Add argument that determines which revision to search for in the list of
-    # tryjobs.
-    parser.add_argument(
-        "--revision",
-        required=True,
-        type=int,
-        help="The revision to set its status.",
-    )
-
-    # Add argument for the custom script to execute for the 'custom_script'
-    # option in '--set_status'.
-    parser.add_argument(
-        "--custom_script",
-        help="The absolute path to the custom script to execute (its exit code "
-        'should be %d for "good", %d for "bad", or %d for "skip")'
-        % (
-            CustomScriptStatus.GOOD.value,
-            CustomScriptStatus.BAD.value,
-            CustomScriptStatus.SKIP.value,
-        ),
-    )
-
-    args_output = parser.parse_args()
-
-    if not (
-        os.path.isfile(
-            args_output.status_file
-            and not args_output.status_file.endswith(".json")
-        )
-    ):
-        raise ValueError(
-            'File does not exist or does not ending in ".json" '
-            ": %s" % args_output.status_file
-        )
-
-    if (
-        args_output.set_status == TryjobStatus.CUSTOM_SCRIPT.value
-        and not args_output.custom_script
-    ):
-        raise ValueError(
-            "Please provide the absolute path to the script to " "execute."
-        )
-
-    return args_output
-
-
-def FindTryjobIndex(revision, tryjobs_list):
-    """Searches the list of tryjob dictionaries to find 'revision'.
-
-    Uses the key 'rev' for each dictionary and compares the value against
-    'revision.'
-
-    Args:
-        revision: The revision to search for in the tryjobs.
-        tryjobs_list: A list of tryjob dictionaries of the format:
-        {
-            'rev' : [REVISION],
-            'url' : [URL_OF_CL],
-            'cl' : [CL_NUMBER],
-            'link' : [TRYJOB_LINK],
-            'status' : [TRYJOB_STATUS],
-            'buildbucket_id': [BUILDBUCKET_ID]
-        }
-
-    Returns:
-        The index within the list or None to indicate it was not found.
-    """
-
-    for cur_index, cur_tryjob_dict in enumerate(tryjobs_list):
-        if cur_tryjob_dict["rev"] == revision:
-            return cur_index
-
-    return None
-
-
-def GetCustomScriptResult(custom_script, status_file, tryjob_contents):
-    """Returns the conversion of the exit code of the custom script.
-
-    Args:
-        custom_script: Absolute path to the script to be executed.
-        status_file: Absolute path to the file that contains information about
-        the bisection of LLVM.
-        tryjob_contents: A dictionary of the contents of the tryjob (e.g.
-        'status', 'url', 'link', 'buildbucket_id', etc.).
-
-    Returns:
-        The exit code conversion to either return 'good', 'bad', or 'skip'.
-
-    Raises:
-        ValueError: The custom script failed to provide the correct exit code.
-    """
-
-    # Create a temporary file to write the contents of the tryjob at index
-    # 'tryjob_index' (the temporary file path will be passed into the custom
-    # script as a command line argument).
-    with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-        with open(temp_json_file, "w", encoding="utf-8") as tryjob_file:
-            json.dump(
-                tryjob_contents, tryjob_file, indent=4, separators=(",", ": ")
-            )
-
-        exec_script_cmd = [custom_script, temp_json_file]
-
-        # Execute the custom script to get the exit code.
-        with subprocess.Popen(
-            exec_script_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE
-        ) as exec_script_cmd_obj:
-            _, stderr = exec_script_cmd_obj.communicate()
-
-        # Invalid exit code by the custom script.
-        if (
-            exec_script_cmd_obj.returncode
-            not in custom_script_exit_value_mapping
-        ):
-            # Save the .JSON file to the directory of 'status_file'.
-            name_of_json_file = os.path.join(
-                os.path.dirname(status_file), os.path.basename(temp_json_file)
-            )
-
-            os.rename(temp_json_file, name_of_json_file)
-
-            raise ValueError(
-                "Custom script %s exit code %d did not match "
-                'any of the expected exit codes: %d for "good", %d '
-                'for "bad", or %d for "skip".\nPlease check %s for information '
-                "about the tryjob: %s"
-                % (
-                    custom_script,
-                    exec_script_cmd_obj.returncode,
-                    CustomScriptStatus.GOOD.value,
-                    CustomScriptStatus.BAD.value,
-                    CustomScriptStatus.SKIP.value,
-                    name_of_json_file,
-                    stderr,
-                )
-            )
-
-    return custom_script_exit_value_mapping[exec_script_cmd_obj.returncode]
-
-
-def UpdateTryjobStatus(revision, set_status, status_file, custom_script):
-    """Updates a tryjob's 'status' field based off of 'set_status'.
-
-    Args:
-        revision: The revision associated with the tryjob.
-        set_status: What to update the 'status' field to.
-            Ex: TryjobStatus.Good, TryjobStatus.BAD, TryjobStatus.PENDING, or
-            TryjobStatus.
-        status_file: The .JSON file that contains the tryjobs.
-        custom_script: The absolute path to a script that will be executed
-        which will determine the 'status' value of the tryjob.
-    """
-
-    # Format of 'bisect_contents':
-    # {
-    #   'start': [START_REVISION_OF_BISECTION]
-    #   'end': [END_REVISION_OF_BISECTION]
-    #   'jobs' : [
-    #       {[TRYJOB_INFORMATION]},
-    #       {[TRYJOB_INFORMATION]},
-    #       ...,
-    #       {[TRYJOB_INFORMATION]}
-    #   ]
-    # }
-    with open(status_file, encoding="utf-8") as tryjobs:
-        bisect_contents = json.load(tryjobs)
-
-    if not bisect_contents["jobs"]:
-        sys.exit("No tryjobs in %s" % status_file)
-
-    tryjob_index = FindTryjobIndex(revision, bisect_contents["jobs"])
-
-    # 'FindTryjobIndex()' returns None if the revision was not found.
-    if tryjob_index is None:
-        raise ValueError(
-            "Unable to find tryjob for %d in %s" % (revision, status_file)
-        )
-
-    # Set 'status' depending on 'set_status' for the tryjob.
-    if set_status == TryjobStatus.GOOD:
-        bisect_contents["jobs"][tryjob_index][
-            "status"
-        ] = TryjobStatus.GOOD.value
-    elif set_status == TryjobStatus.BAD:
-        bisect_contents["jobs"][tryjob_index]["status"] = TryjobStatus.BAD.value
-    elif set_status == TryjobStatus.PENDING:
-        bisect_contents["jobs"][tryjob_index][
-            "status"
-        ] = TryjobStatus.PENDING.value
-    elif set_status == TryjobStatus.SKIP:
-        bisect_contents["jobs"][tryjob_index][
-            "status"
-        ] = TryjobStatus.SKIP.value
-    elif set_status == TryjobStatus.CUSTOM_SCRIPT:
-        bisect_contents["jobs"][tryjob_index]["status"] = GetCustomScriptResult(
-            custom_script, status_file, bisect_contents["jobs"][tryjob_index]
-        )
-    else:
-        raise ValueError(
-            'Invalid "set_status" option provided: %s' % set_status
-        )
-
-    with open(status_file, "w", encoding="utf-8") as update_tryjobs:
-        json.dump(
-            bisect_contents, update_tryjobs, indent=4, separators=(",", ": ")
-        )
-
-
-def main():
-    """Updates the status of a tryjob."""
-
-    chroot.VerifyOutsideChroot()
-
-    args_output = GetCommandLineArgs()
-
-    UpdateTryjobStatus(
-        args_output.revision,
-        TryjobStatus(args_output.set_status),
-        args_output.status_file,
-        args_output.custom_script,
-    )
-
-
-if __name__ == "__main__":
-    main()
diff --git a/llvm_tools/update_tryjob_status_unittest.py b/llvm_tools/update_tryjob_status_unittest.py
deleted file mode 100755
index 632bab58..00000000
--- a/llvm_tools/update_tryjob_status_unittest.py
+++ /dev/null
@@ -1,540 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Tests when updating a tryjob's status."""
-
-import contextlib
-import json
-import os
-import subprocess
-import unittest
-from unittest import mock
-
-import test_helpers
-import update_tryjob_status
-
-
-class UpdateTryjobStatusTest(unittest.TestCase):
-    """Unittests for updating a tryjob's 'status'."""
-
-    def testFoundTryjobIndex(self):
-        test_tryjobs = [
-            {
-                "rev": 123,
-                "url": "https://some_url_to_CL.com",
-                "cl": "https://some_link_to_tryjob.com",
-                "status": "good",
-                "buildbucket_id": 91835,
-            },
-            {
-                "rev": 1000,
-                "url": "https://some_url_to_CL.com",
-                "cl": "https://some_link_to_tryjob.com",
-                "status": "pending",
-                "buildbucket_id": 10931,
-            },
-        ]
-
-        expected_index = 0
-
-        revision_to_find = 123
-
-        self.assertEqual(
-            update_tryjob_status.FindTryjobIndex(
-                revision_to_find, test_tryjobs
-            ),
-            expected_index,
-        )
-
-    def testNotFindTryjobIndex(self):
-        test_tryjobs = [
-            {
-                "rev": 500,
-                "url": "https://some_url_to_CL.com",
-                "cl": "https://some_link_to_tryjob.com",
-                "status": "bad",
-                "buildbucket_id": 390,
-            },
-            {
-                "rev": 10,
-                "url": "https://some_url_to_CL.com",
-                "cl": "https://some_link_to_tryjob.com",
-                "status": "skip",
-                "buildbucket_id": 10,
-            },
-        ]
-
-        revision_to_find = 250
-
-        self.assertIsNone(
-            update_tryjob_status.FindTryjobIndex(revision_to_find, test_tryjobs)
-        )
-
-    @mock.patch.object(subprocess, "Popen")
-    # Simulate the behavior of `os.rename()` when successfully renamed a file.
-    @mock.patch.object(os, "rename", return_value=None)
-    # Simulate the behavior of `os.path.basename()` when successfully retrieved
-    # the basename of the temp .JSON file.
-    @mock.patch.object(os.path, "basename", return_value="tmpFile.json")
-    def testInvalidExitCodeByCustomScript(
-        self, mock_basename, mock_rename_file, mock_exec_custom_script
-    ):
-        error_message_by_custom_script = "Failed to parse .JSON file"
-
-        # Simulate the behavior of 'subprocess.Popen()' when executing the
-        # custom script.
-        #
-        # `Popen.communicate()` returns a tuple of `stdout` and `stderr`.
-        popen_result = mock.MagicMock()
-        popen_result.communicate.return_value = (
-            None,
-            error_message_by_custom_script,
-        )
-        custom_script_exit_code = 1
-        popen_result.returncode = custom_script_exit_code
-        mock_exec_custom_script.return_value = contextlib.nullcontext(
-            popen_result
-        )
-
-        tryjob_contents = {
-            "status": "good",
-            "rev": 1234,
-            "url": "https://some_url_to_CL.com",
-            "link": "https://some_url_to_tryjob.com",
-        }
-
-        custom_script_path = "/abs/path/to/script.py"
-        status_file_path = "/abs/path/to/status_file.json"
-
-        name_json_file = os.path.join(
-            os.path.dirname(status_file_path), "tmpFile.json"
-        )
-
-        expected_error_message = (
-            "Custom script %s exit code %d did not match "
-            'any of the expected exit codes: %s for "good", '
-            '%d for "bad", or %d for "skip".\nPlease check '
-            "%s for information about the tryjob: %s"
-            % (
-                custom_script_path,
-                custom_script_exit_code,
-                update_tryjob_status.CustomScriptStatus.GOOD.value,
-                update_tryjob_status.CustomScriptStatus.BAD.value,
-                update_tryjob_status.CustomScriptStatus.SKIP.value,
-                name_json_file,
-                error_message_by_custom_script,
-            )
-        )
-
-        # Verify the exception is raised when the exit code by the custom script
-        # does not match any of the exit codes in the mapping of
-        # `custom_script_exit_value_mapping`.
-        with self.assertRaises(ValueError) as err:
-            update_tryjob_status.GetCustomScriptResult(
-                custom_script_path, status_file_path, tryjob_contents
-            )
-
-        self.assertEqual(str(err.exception), expected_error_message)
-
-        mock_exec_custom_script.assert_called_once()
-
-        mock_rename_file.assert_called_once()
-
-        mock_basename.assert_called_once()
-
-    @mock.patch.object(subprocess, "Popen")
-    # Simulate the behavior of `os.rename()` when successfully renamed a file.
-    @mock.patch.object(os, "rename", return_value=None)
-    # Simulate the behavior of `os.path.basename()` when successfully retrieved
-    # the basename of the temp .JSON file.
-    @mock.patch.object(os.path, "basename", return_value="tmpFile.json")
-    def testValidExitCodeByCustomScript(
-        self, mock_basename, mock_rename_file, mock_exec_custom_script
-    ):
-        # Simulate the behavior of 'subprocess.Popen()' when executing the
-        # custom script.
-        #
-        # `Popen.communicate()` returns a tuple of `stdout` and `stderr`.
-        popen_result = mock.MagicMock()
-        popen_result.communicate.return_value = (
-            None,
-            None,
-        )
-        popen_result.returncode = (
-            update_tryjob_status.CustomScriptStatus.GOOD.value
-        )
-        mock_exec_custom_script.return_value = contextlib.nullcontext(
-            popen_result
-        )
-
-        tryjob_contents = {
-            "status": "good",
-            "rev": 1234,
-            "url": "https://some_url_to_CL.com",
-            "link": "https://some_url_to_tryjob.com",
-        }
-
-        custom_script_path = "/abs/path/to/script.py"
-        status_file_path = "/abs/path/to/status_file.json"
-
-        self.assertEqual(
-            update_tryjob_status.GetCustomScriptResult(
-                custom_script_path, status_file_path, tryjob_contents
-            ),
-            update_tryjob_status.TryjobStatus.GOOD.value,
-        )
-
-        mock_exec_custom_script.assert_called_once()
-
-        mock_rename_file.assert_not_called()
-
-        mock_basename.assert_not_called()
-
-    def testNoTryjobsInStatusFileWhenUpdatingTryjobStatus(self):
-        bisect_test_contents = {"start": 369410, "end": 369420, "jobs": []}
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            revision_to_update = 369412
-
-            custom_script = None
-
-            # Verify the exception is raised when the `status_file` does not
-            # have any `jobs` (empty).
-            with self.assertRaises(SystemExit) as err:
-                update_tryjob_status.UpdateTryjobStatus(
-                    revision_to_update,
-                    update_tryjob_status.TryjobStatus.GOOD,
-                    temp_json_file,
-                    custom_script,
-                )
-
-            self.assertEqual(
-                str(err.exception), "No tryjobs in %s" % temp_json_file
-            )
-
-    # Simulate the behavior of `FindTryjobIndex()` when the tryjob does not
-    # exist in the status file.
-    @mock.patch.object(
-        update_tryjob_status, "FindTryjobIndex", return_value=None
-    )
-    def testNotFindTryjobIndexWhenUpdatingTryjobStatus(
-        self, mock_find_tryjob_index
-    ):
-        bisect_test_contents = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [{"rev": 369411, "status": "pending"}],
-        }
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            revision_to_update = 369416
-
-            custom_script = None
-
-            # Verify the exception is raised when the `status_file` does not
-            # have any `jobs` (empty).
-            with self.assertRaises(ValueError) as err:
-                update_tryjob_status.UpdateTryjobStatus(
-                    revision_to_update,
-                    update_tryjob_status.TryjobStatus.SKIP,
-                    temp_json_file,
-                    custom_script,
-                )
-
-            self.assertEqual(
-                str(err.exception),
-                "Unable to find tryjob for %d in %s"
-                % (revision_to_update, temp_json_file),
-            )
-
-        mock_find_tryjob_index.assert_called_once()
-
-    # Simulate the behavior of `FindTryjobIndex()` when the tryjob exists in the
-    # status file.
-    @mock.patch.object(update_tryjob_status, "FindTryjobIndex", return_value=0)
-    def testSuccessfullyUpdatedTryjobStatusToGood(self, mock_find_tryjob_index):
-        bisect_test_contents = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [{"rev": 369411, "status": "pending"}],
-        }
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            revision_to_update = 369411
-
-            # Index of the tryjob that is going to have its 'status' value
-            # updated.
-            tryjob_index = 0
-
-            custom_script = None
-
-            update_tryjob_status.UpdateTryjobStatus(
-                revision_to_update,
-                update_tryjob_status.TryjobStatus.GOOD,
-                temp_json_file,
-                custom_script,
-            )
-
-            # Verify that the tryjob's 'status' has been updated in the status
-            # file.
-            with open(temp_json_file, encoding="utf-8") as status_file:
-                bisect_contents = json.load(status_file)
-
-                self.assertEqual(
-                    bisect_contents["jobs"][tryjob_index]["status"],
-                    update_tryjob_status.TryjobStatus.GOOD.value,
-                )
-
-        mock_find_tryjob_index.assert_called_once()
-
-    # Simulate the behavior of `FindTryjobIndex()` when the tryjob exists in the
-    # status file.
-    @mock.patch.object(update_tryjob_status, "FindTryjobIndex", return_value=0)
-    def testSuccessfullyUpdatedTryjobStatusToBad(self, mock_find_tryjob_index):
-        bisect_test_contents = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [{"rev": 369411, "status": "pending"}],
-        }
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            revision_to_update = 369411
-
-            # Index of the tryjob that is going to have its 'status' value
-            # updated.
-            tryjob_index = 0
-
-            custom_script = None
-
-            update_tryjob_status.UpdateTryjobStatus(
-                revision_to_update,
-                update_tryjob_status.TryjobStatus.BAD,
-                temp_json_file,
-                custom_script,
-            )
-
-            # Verify that the tryjob's 'status' has been updated in the status
-            # file.
-            with open(temp_json_file, encoding="utf-8") as status_file:
-                bisect_contents = json.load(status_file)
-
-                self.assertEqual(
-                    bisect_contents["jobs"][tryjob_index]["status"],
-                    update_tryjob_status.TryjobStatus.BAD.value,
-                )
-
-        mock_find_tryjob_index.assert_called_once()
-
-    # Simulate the behavior of `FindTryjobIndex()` when the tryjob exists in the
-    # status file.
-    @mock.patch.object(update_tryjob_status, "FindTryjobIndex", return_value=0)
-    def testSuccessfullyUpdatedTryjobStatusToPending(
-        self, mock_find_tryjob_index
-    ):
-        bisect_test_contents = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [{"rev": 369411, "status": "skip"}],
-        }
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            revision_to_update = 369411
-
-            # Index of the tryjob that is going to have its 'status' value
-            # updated.
-            tryjob_index = 0
-
-            custom_script = None
-
-            update_tryjob_status.UpdateTryjobStatus(
-                revision_to_update,
-                update_tryjob_status.TryjobStatus.SKIP,
-                temp_json_file,
-                custom_script,
-            )
-
-            # Verify that the tryjob's 'status' has been updated in the status
-            # file.
-            with open(temp_json_file, encoding="utf-8") as status_file:
-                bisect_contents = json.load(status_file)
-
-                self.assertEqual(
-                    bisect_contents["jobs"][tryjob_index]["status"],
-                    update_tryjob_status.TryjobStatus.SKIP.value,
-                )
-
-        mock_find_tryjob_index.assert_called_once()
-
-    # Simulate the behavior of `FindTryjobIndex()` when the tryjob exists in the
-    # status file.
-    @mock.patch.object(update_tryjob_status, "FindTryjobIndex", return_value=0)
-    def testSuccessfullyUpdatedTryjobStatusToSkip(self, mock_find_tryjob_index):
-        bisect_test_contents = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [
-                {
-                    "rev": 369411,
-                    "status": "pending",
-                }
-            ],
-        }
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            revision_to_update = 369411
-
-            # Index of the tryjob that is going to have its 'status' value
-            # updated.
-            tryjob_index = 0
-
-            custom_script = None
-
-            update_tryjob_status.UpdateTryjobStatus(
-                revision_to_update,
-                update_tryjob_status.TryjobStatus.PENDING,
-                temp_json_file,
-                custom_script,
-            )
-
-            # Verify that the tryjob's 'status' has been updated in the status
-            # file.
-            with open(temp_json_file, encoding="utf-8") as status_file:
-                bisect_contents = json.load(status_file)
-
-                self.assertEqual(
-                    bisect_contents["jobs"][tryjob_index]["status"],
-                    update_tryjob_status.TryjobStatus.PENDING.value,
-                )
-
-        mock_find_tryjob_index.assert_called_once()
-
-    @mock.patch.object(update_tryjob_status, "FindTryjobIndex", return_value=0)
-    @mock.patch.object(
-        update_tryjob_status,
-        "GetCustomScriptResult",
-        return_value=update_tryjob_status.TryjobStatus.SKIP.value,
-    )
-    def testUpdatedTryjobStatusToAutoPassedWithCustomScript(
-        self, mock_get_custom_script_result, mock_find_tryjob_index
-    ):
-        bisect_test_contents = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [
-                {"rev": 369411, "status": "pending", "buildbucket_id": 1200}
-            ],
-        }
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            revision_to_update = 369411
-
-            # Index of the tryjob that is going to have its 'status' value
-            # updated.
-            tryjob_index = 0
-
-            custom_script_path = "/abs/path/to/custom_script.py"
-
-            update_tryjob_status.UpdateTryjobStatus(
-                revision_to_update,
-                update_tryjob_status.TryjobStatus.CUSTOM_SCRIPT,
-                temp_json_file,
-                custom_script_path,
-            )
-
-            # Verify that the tryjob's 'status' has been updated in the status
-            # file.
-            with open(temp_json_file, encoding="utf-8") as status_file:
-                bisect_contents = json.load(status_file)
-
-                self.assertEqual(
-                    bisect_contents["jobs"][tryjob_index]["status"],
-                    update_tryjob_status.TryjobStatus.SKIP.value,
-                )
-
-        mock_get_custom_script_result.assert_called_once()
-
-        mock_find_tryjob_index.assert_called_once()
-
-    # Simulate the behavior of `FindTryjobIndex()` when the tryjob exists in the
-    # status file.
-    @mock.patch.object(update_tryjob_status, "FindTryjobIndex", return_value=0)
-    def testSetStatusDoesNotExistWhenUpdatingTryjobStatus(
-        self, mock_find_tryjob_index
-    ):
-        bisect_test_contents = {
-            "start": 369410,
-            "end": 369420,
-            "jobs": [
-                {"rev": 369411, "status": "pending", "buildbucket_id": 1200}
-            ],
-        }
-
-        # Create a temporary .JSON file to simulate a .JSON file that has
-        # bisection contents.
-        with test_helpers.CreateTemporaryJsonFile() as temp_json_file:
-            with open(temp_json_file, "w", encoding="utf-8") as f:
-                test_helpers.WritePrettyJsonFile(bisect_test_contents, f)
-
-            revision_to_update = 369411
-
-            nonexistent_update_status = "revert_status"
-
-            custom_script = None
-
-            # Verify the exception is raised when the `set_status` command line
-            # argument does not exist in the mapping.
-            with self.assertRaises(ValueError) as err:
-                update_tryjob_status.UpdateTryjobStatus(
-                    revision_to_update,
-                    nonexistent_update_status,
-                    temp_json_file,
-                    custom_script,
-                )
-
-            self.assertEqual(
-                str(err.exception),
-                'Invalid "set_status" option provided: revert_status',
-            )
-
-        mock_find_tryjob_index.assert_called_once()
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/upload_llvm_testing_helper_cl.py b/llvm_tools/upload_llvm_testing_helper_cl.py
new file mode 100644
index 00000000..9b8d4648
--- /dev/null
+++ b/llvm_tools/upload_llvm_testing_helper_cl.py
@@ -0,0 +1,171 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Uploads an LLVM 'testing helper' CL.
+
+These CLs make the validation of LLVM easier, and do things like:
+- allowing patches to be disabled if they no longer apply
+- disabling warnings
+"""
+
+import argparse
+import logging
+from pathlib import Path
+from typing import List
+
+from cros_utils import cros_paths
+from cros_utils import git_utils
+from llvm_tools import chroot
+
+
+# Text to add to the bottom of ebuild hooks.
+DISABLE_WARNINGS_BLOCK = r"""
+
+# Disable -Werror where possible, so more serious issues (e.g., compiler
+# crashes) can be more easily surfaced.
+cros_pre_src_configure_disable_werror() {
+  # Add the special env var to toolchain/fatal_clang_warnings. There's logic
+  # in Chromite to search for & upload these directories on all builds,
+  # including failing ones.
+  local d="${CROS_ARTIFACTS_TMP_DIR}/toolchain/fatal_clang_warnings"
+  export CFLAGS+=" -D_CROSTC_FORCE_DISABLE_WERROR=${d} "
+  export CXXFLAGS+=" -D_CROSTC_FORCE_DISABLE_WERROR=${d} "
+  # Set these for ec ebuilds, since those ignore CFLAGS/CXXFLAGS
+  [[ -n "${_ECLASS_CROS_EC:-}" ]] && export EXTRA_CFLAGS+=" -D_CROSTC_FORCE_DISABLE_WERROR=${d} "
+
+  # Also export an env var, since some build systems will ignore our CFLAGS
+  # but not filter the environment.
+  export FORCE_DISABLE_WERROR=1
+}
+"""
+
+# Text to add to the bottom of `profiles/base/use.force`.
+USE_FORCE_BLOCK = r"""
+
+# Allow PGO profile mismatches to continue silently. Without this, LLVM builds
+# will fail unless a PGO profile has been generated (which shouldn't be
+# blocking until we're ready to land llvm-next).
+allow-pgo-mismatch
+"""
+
+COMMIT_MESSAGE = """\
+DO NOT COMMIT: llvm-testing helper CL
+
+This CL was automatically generated to facilitate LLVM testing.
+The script that generated this is located at
+src/third_party/toolchain-utils/llvm_tools/upload_llvm_testing_helper_cl.py.
+
+BUG=None
+TEST=None
+"""
+
+
+def add_force_rebuild_marker(chromiumos_overlay: Path):
+    """Adds a marker to force this change to appear as a toolchain change."""
+    # `touch`ing anything in `sys-devel/llvm/files` causes an LLVM revbump, and
+    # causes all packages to be rebuilt.
+    force_rebuild_file = (
+        chromiumos_overlay / "sys-devel" / "llvm" / "files" / "force_rebuild"
+    )
+    force_rebuild_file.touch()
+
+
+def add_use_force_block(chromiumos_overlay: Path):
+    use_force = chromiumos_overlay / "profiles" / "base" / "use.force"
+    # If this doesn't exist, that _can_ be worked with, but it's a smoke signal
+    # (since e.g., maybe the file no longer takes effect). Have someone
+    # investigate.
+    if not use_force.exists():
+        raise ValueError(f"No file found at {use_force}; refusing to patch")
+    with use_force.open("a", encoding="utf-8") as f:
+        f.write(USE_FORCE_BLOCK)
+
+
+def add_disable_warnings_block(chromiumos_overlay: Path):
+    ebuild_hooks = chromiumos_overlay / "profiles" / "base" / "profile.bashrc"
+    # If this doesn't exist, that _can_ be worked with, but it's a smoke signal
+    # (since e.g., maybe the file no longer takes effect). Have someone
+    # investigate.
+    if not ebuild_hooks.exists():
+        raise ValueError(f"No file found at {ebuild_hooks}; refusing to patch")
+    with ebuild_hooks.open("a", encoding="utf-8") as f:
+        f.write(DISABLE_WARNINGS_BLOCK)
+
+
+def create_helper_cl_commit_in_worktree_of(
+    chromiumos_overlay: Path, tot: bool
+) -> str:
+    """Creates a commit containing the helper CL diff. Returns the SHA.commit"""
+    with git_utils.create_worktree(chromiumos_overlay) as worktree:
+        if tot:
+            git_utils.fetch_and_checkout(
+                worktree,
+                remote=git_utils.CROS_EXTERNAL_REMOTE,
+                branch=git_utils.CROS_MAIN_BRANCH,
+            )
+
+        logging.info("Adding helper changes to CL in %s...", worktree)
+        add_force_rebuild_marker(worktree)
+        add_use_force_block(worktree)
+        add_disable_warnings_block(worktree)
+        return git_utils.commit_all_changes(worktree, COMMIT_MESSAGE)
+
+
+def main(argv: List[str]) -> None:
+    logging.basicConfig(
+        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
+        "%(message)s",
+        level=logging.INFO,
+    )
+
+    parser = argparse.ArgumentParser(
+        description=__doc__,
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+    )
+    parser.add_argument(
+        "--chromeos-tree",
+        type=Path,
+        help="""
+        The ChromeOS tree to update in. The `llvm-project` directory of this
+        may also be consulted. Will try to autodetect if none is specified.
+        """,
+    )
+    parser.add_argument(
+        "--dry-run",
+        action="store_true",
+        help="Commit changes, but don't actually upload them.",
+    )
+    parser.add_argument(
+        "--tot",
+        action="store_true",
+        help="""
+        If passed, modified repos will be `git fetch`ed and this script will
+        work on their main branches, rather than working on the version you
+        have locally.
+        """,
+    )
+    opts = parser.parse_args(argv)
+
+    chromeos_tree = opts.chromeos_tree
+    if not chromeos_tree:
+        chromeos_tree = chroot.FindChromeOSRootAboveToolchainUtils()
+
+    chromiumos_overlay = chromeos_tree / cros_paths.CHROMIUMOS_OVERLAY
+    helper_sha = create_helper_cl_commit_in_worktree_of(
+        chromiumos_overlay, tot=opts.tot
+    )
+    if opts.dry_run:
+        logging.info(
+            "--dry-run specified; not uploading new commit (%s).",
+            helper_sha,
+        )
+        return
+
+    # This logs the CL information, so no need to print anything after this.
+    git_utils.upload_to_gerrit(
+        git_repo=chromiumos_overlay,
+        remote=git_utils.CROS_EXTERNAL_REMOTE,
+        branch=git_utils.CROS_MAIN_BRANCH,
+        ref=helper_sha,
+    )
diff --git a/llvm_tools/upload_llvm_testing_helper_cl_test.py b/llvm_tools/upload_llvm_testing_helper_cl_test.py
new file mode 100644
index 00000000..b25409f4
--- /dev/null
+++ b/llvm_tools/upload_llvm_testing_helper_cl_test.py
@@ -0,0 +1,56 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Tests for upload_llvm_testing_helper_cl"""
+
+from llvm_tools import test_helpers
+from llvm_tools import upload_llvm_testing_helper_cl
+
+
+class Test(test_helpers.TempDirTestCase):
+    """Tests for upload_llvm_testing_helper_cl"""
+
+    def test_force_rebuild_marker_addition(self):
+        chromiumos_overlay = self.make_tempdir()
+        llvm_filesdir = chromiumos_overlay / "sys-devel" / "llvm" / "files"
+        llvm_filesdir.mkdir(parents=True)
+        upload_llvm_testing_helper_cl.add_force_rebuild_marker(
+            chromiumos_overlay
+        )
+        self.assertTrue(
+            (llvm_filesdir / "force_rebuild").exists(),
+            f"Missing force_rebuild marker in {llvm_filesdir}",
+        )
+
+    def test_use_force_block_addition(self):
+        chromiumos_overlay = self.make_tempdir()
+        use_force_file = chromiumos_overlay / "profiles" / "base" / "use.force"
+        use_force_file.parent.mkdir(parents=True)
+        use_force_file.write_text("# Whee", encoding="utf-8")
+
+        upload_llvm_testing_helper_cl.add_use_force_block(chromiumos_overlay)
+        new_contents = use_force_file.read_text(encoding="utf-8")
+
+        self.assertIn("# Whee\n", new_contents)
+        self.assertIn(
+            upload_llvm_testing_helper_cl.USE_FORCE_BLOCK, new_contents
+        )
+
+    def test_warning_disable_block_addition(self):
+        chromiumos_overlay = self.make_tempdir()
+        profile_bashrc = (
+            chromiumos_overlay / "profiles" / "base" / "profile.bashrc"
+        )
+        profile_bashrc.parent.mkdir(parents=True)
+        profile_bashrc.write_text("# Whee", encoding="utf-8")
+
+        upload_llvm_testing_helper_cl.add_disable_warnings_block(
+            chromiumos_overlay
+        )
+        new_contents = profile_bashrc.read_text(encoding="utf-8")
+
+        self.assertIn("# Whee\n", new_contents)
+        self.assertIn(
+            upload_llvm_testing_helper_cl.DISABLE_WARNINGS_BLOCK, new_contents
+        )
diff --git a/llvm_tools/verify_patch_consistency.py b/llvm_tools/verify_patch_consistency.py
new file mode 100644
index 00000000..c941d573
--- /dev/null
+++ b/llvm_tools/verify_patch_consistency.py
@@ -0,0 +1,347 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Verify a given LLVM Branch Creation CL has the right patch stack.
+
+The workflow generally for using this script is:
+
+  1. The CrOSTC Mage requests a new LLVM branch of the format
+     chromeos/llvm-rNNNN-N
+  2. The CrOSTC Mage updates the branch locally using
+     'ready_llvm_branch -r NNNNN --branch-number N --upload'
+  3. The reviewer runs this script on the TOP of the CL stack.
+  4. If this script exits with 0, the reviewer can run the provided
+     gerrit commands to approve and submit the CL stack.
+
+This script exits with 0 if the branch has been verified, 1 if it failed
+to verify.
+
+Examples:
+
+    $ verify_patch_consistency.py --cl 5637483
+
+    $ verify_patch_consistency.py --chromiumos-root ~/chromiumos --cl 5637483
+"""
+
+import argparse
+import json
+from pathlib import Path
+import re
+import subprocess
+import sys
+import textwrap
+from typing import Any, Dict, List, Tuple
+
+from cros_utils import cros_paths
+from cros_utils import git_utils
+from llvm_tools import git_llvm_rev
+from llvm_tools import llvm_project_base_commit
+from llvm_tools import patch_utils
+
+
+def verify_no_cq_depends(
+    worktree_dir: Path,
+    head: str,
+    baseline_hash: str,
+) -> bool:
+    """Checks for Cq-Depends in the given branch.
+
+    Returns:
+        False if Cq-Depends were found and an error was printed.
+    """
+    git_log = git_utils.log(
+        worktree_dir, head=head, stop_at=baseline_hash, log_format="%B"
+    )
+    if not git_log or git_log.isspace():
+        raise ValueError("`git log` was empty - broken head/baseline_hash?")
+
+    cq_depends = re.findall(r"^Cq-Depend:.*$", git_log, re.MULTILINE)
+    if not cq_depends:
+        return True
+
+    # Write out a separator so this section is visually distinct.
+    print(
+        textwrap.dedent(
+            """\
+            ==================================================================
+
+            Cq-Depends detected!
+
+            These are _not_ checked by this script, but the commands that this
+            script tells you to execute _will_ mass-stamp them. Please verify
+            manually that either:
+            - these are all merged (so you can ignore them), or
+            - you are OK with automatically stamping and CQ+2'ing all of them
+
+            If this is the case, rerun this script with the
+            '--ignore-cq-depend' flag to skip this error. Otherwise, please
+            work with the Mage to remove the `Cq-Depend`s.
+            """
+        ).strip(),
+        file=sys.stderr,
+    )
+
+    print("Listing of Cq-Depend found in git log:", file=sys.stderr)
+    for d in cq_depends:
+        print(f"- {d}", file=sys.stderr)
+    return False
+
+
+def verify_in_worktree(
+    toolchain_utils_dir: Path,
+    llvm_src_dir: Path,
+    patches_json: Path,
+    chromiumos_overlay: Path,
+    svn_revision: int,
+    cl_ref: str,
+    ignore_cq_depend: bool = False,
+) -> bool:
+    """Check equality between the local patches and an upstream CL reference.
+
+    Args:
+        toolchain_utils_dir: Path to toolchain_utils.
+        llvm_src_dir: Path to an llvm-project dir.
+        patches_json: A PATCHES.json file to apply.
+        chromiumos_overlay: Path to chromiumos-overlay
+        svn_revision: The synthetic SVN-style revision number to
+            determine which patches apply.
+        cl_ref: Upstream Change List reference name.
+        ignore_cq_depend: if True, this won't warn about Cq-Depend footers in
+            the log for `cl_ref`.
+
+    Returns:
+        True if the local patches match, False otherwise
+    """
+    # We have to fetch to make sure we know that the matching_hash
+    # exists.
+    git_utils.fetch(
+        llvm_src_dir,
+        remote=git_utils.CROS_EXTERNAL_REMOTE,
+        branch=git_utils.CROS_MAIN_BRANCH,
+    )
+    # The cros external remote ("cros") uses its main branch
+    # for the actual upstream revision, not the local chromeos main branch.
+    # This is the same as the upstream/main.
+    matching_hash = git_llvm_rev.translate_rev_to_sha(
+        git_llvm_rev.LLVMConfig(git_utils.CROS_EXTERNAL_REMOTE, llvm_src_dir),
+        git_llvm_rev.Rev(git_llvm_rev.MAIN_BRANCH, svn_revision),
+    )
+    with git_utils.create_worktree(
+        llvm_src_dir, commitish=matching_hash
+    ) as worktree_dir:
+        llvm_project_base_commit.make_base_commit(
+            toolchain_utils_dir,
+            worktree_dir,
+            svn_revision,
+            chromiumos_overlay,
+        )
+        try:
+            patch_utils.apply_all_from_json(
+                svn_version=svn_revision,
+                llvm_src_dir=worktree_dir,
+                patches_json_fp=patches_json,
+                patch_cmd=patch_utils.git_am_chromiumos_quiet,
+            )
+        except RuntimeError:
+            apply_msg = (
+                "FAILED TO VERIFY. Local patches did not apply.",
+                "Make sure your PATCHES.json file is up to date.",
+            )
+            print("\n".join(apply_msg), file=sys.stderr)
+            raise
+        # We have to fetch again inside the worktree for the CL itself.
+        git_utils.fetch(
+            worktree_dir,
+            remote=git_utils.CROS_EXTERNAL_REMOTE,
+            branch=cl_ref,
+        )
+
+        diff = ref_diff(worktree_dir, "HEAD", "FETCH_HEAD")
+        if diff:
+            local_head = git_utils.resolve_ref(worktree_dir, "HEAD")
+            fetch_head = git_utils.resolve_ref(worktree_dir, "FETCH_HEAD")
+            diff_msg = (
+                f"FAILED TO VERIFY. Local patches and CL {cl_ref} differ!",
+                f"Comparing local HEAD {local_head} with"
+                f" FETCH_HEAD {fetch_head}",
+                "",
+                diff,
+            )
+            print("\n".join(diff_msg), file=sys.stderr)
+            return False
+
+        if not (
+            ignore_cq_depend
+            or verify_no_cq_depends(
+                worktree_dir,
+                head="FETCH_HEAD",
+                baseline_hash=matching_hash,
+            )
+        ):
+            return False
+
+    return True
+
+
+def ref_diff(cwd: Path, ref1: str, ref2: str) -> str:
+    """Compute diff between two git refs."""
+    cmd = [
+        "git",
+        "diff",
+        ref1,
+        ref2,
+        "--",
+    ]
+    return subprocess.run(
+        cmd,
+        cwd=cwd,
+        check=True,
+        stdout=subprocess.PIPE,
+        stdin=subprocess.DEVNULL,
+        encoding="utf-8",
+    ).stdout
+
+
+def _gerrit_inspect(cl: int, chromiumos_root: Path) -> List[Dict[str, Any]]:
+    """Gerrit command wrapper for easy mocking."""
+    cmd = ("gerrit", "--json", "inspect", str(cl))
+    return json.loads(
+        subprocess.run(
+            cmd,
+            cwd=chromiumos_root,
+            check=True,
+            stdout=subprocess.PIPE,
+            stdin=subprocess.DEVNULL,
+            encoding="utf-8",
+        ).stdout
+    )
+
+
+def parse_branch(cl: int, chromiumos_root: Path) -> Tuple[int, str]:
+    """Extract the LLVM synthetic revision and git ref from a CL branch."""
+    json_obj = _gerrit_inspect(cl, chromiumos_root)
+    branch_name = json_obj[0]["branch"]
+    ref = json_obj[0]["currentPatchSet"]["ref"]
+    branch_regex = re.compile(r"llvm-r(\d+)")
+    if match := branch_regex.search(branch_name):
+        return int(match.group(1)), ref
+    raise RuntimeError(
+        f"Could not parse SVN revision from CL {cl}'s branch: '{branch_name}'"
+    )
+
+
+def _verified_message(svn_revision: int, cl: str, cl_ref: str) -> str:
+    """Format the 'verified' message body and return it."""
+    gerrit_cmd_template = "gerrit %s $(gerrit --raw --no-pager deps '%s') %s"
+    gerrit_approve_cmd = gerrit_cmd_template % ("label-cr", cl, 2)
+    gerrit_verify_cmd = gerrit_cmd_template % ("label-v", cl, 1)
+    gerrit_cq_cmd = gerrit_cmd_template % ("label-cq", cl, 2)
+    return "-" * 80 + textwrap.dedent(
+        f"""
+        VERIFIED! Local patches for r{svn_revision} are identical to the
+        tree state at remote {cl_ref}. You can approve
+        these changes together with the 'gerrit' command:
+
+          {gerrit_approve_cmd} && {gerrit_verify_cmd}
+
+        Once approved, you can submit these changes with CQ+2:
+
+          {gerrit_cq_cmd}
+        """
+    )
+
+
+def parse_args(argv: List[str]) -> argparse.Namespace:
+    """Parse passed in argv list."""
+
+    parser = argparse.ArgumentParser(
+        description=__doc__,
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+    )
+    chromiumos_root_action = parser.add_argument(
+        "--chromiumos-root",
+        type=Path,
+        help="""
+        Path to ChromiumOS root to detect the PATCHES.json.
+        If neither this flag nor --patch-file are specified, it is
+        autodetected.
+        """,
+    )
+    parser.add_argument(
+        "--patch-file",
+        type=Path,
+        help="""
+        Path to PATCHES.json. If not specified, it is autodetected
+        from --chromiumos-root.
+        """,
+    )
+    parser.add_argument(
+        "--ignore-cq-depend",
+        action="store_true",
+        help="If passed, this script won't error out on Cq-Depend footers.",
+    )
+    llvm_dir_action = parser.add_argument(
+        "--llvm-dir",
+        type=Path,
+        help="""
+        Path to a ChromiumOS llvm-project directory. If not
+        specified, it is autodetected from --chromiumos-root.
+        """,
+    )
+    parser.add_argument(
+        "--cl",
+        required=True,
+        type=int,
+        help="""
+        Top of patch stack CL for a given revision branch.
+        Expected to be in the format of just the CL number.
+        """,
+    )
+    args = parser.parse_args(argv)
+
+    # Set default chromiumos_root
+    if not args.chromiumos_root:
+        if repo_root := cros_paths.script_chromiumos_checkout():
+            args.chromiumos_root = repo_root
+        else:
+            raise argparse.ArgumentError(
+                chromiumos_root_action,
+                "Could not find chromiumos root automatically."
+                " Pass --chromiumos-root manually.",
+            )
+
+    # Set default llvm_dir
+    if not args.llvm_dir:
+        llvm_dir = args.chromiumos_root / cros_paths.LLVM_PROJECT
+        if not (llvm_dir / ".git").is_dir():
+            raise argparse.ArgumentError(
+                llvm_dir_action,
+                "Could not find llvm dir automatically. Pass --llvm-dir"
+                " manually.",
+            )
+        args.llvm_dir = llvm_dir
+
+    # Set default patch_file
+    if not args.patch_file:
+        args.patch_file = args.chromiumos_root / cros_paths.DEFAULT_PATCHES_PATH
+
+    return args
+
+
+def main(argv: List[str]) -> int:
+    """Entry point."""
+    args = parse_args(argv)
+    svn_revision, cl_ref = parse_branch(args.cl, args.chromiumos_root)
+    if not verify_in_worktree(
+        toolchain_utils_dir=args.chromiumos_root / cros_paths.TOOLCHAIN_UTILS,
+        llvm_src_dir=args.llvm_dir,
+        patches_json=args.patch_file,
+        chromiumos_overlay=args.chromiumos_root / cros_paths.CHROMIUMOS_OVERLAY,
+        svn_revision=svn_revision,
+        cl_ref=cl_ref,
+        ignore_cq_depend=args.ignore_cq_depend,
+    ):
+        return 1
+    print(_verified_message(svn_revision, args.cl, cl_ref))
+    return 0
diff --git a/llvm_tools/verify_patch_consistency_test.py b/llvm_tools/verify_patch_consistency_test.py
new file mode 100644
index 00000000..876df030
--- /dev/null
+++ b/llvm_tools/verify_patch_consistency_test.py
@@ -0,0 +1,396 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Tests for verify_patch_consistency."""
+
+import dataclasses
+import json
+import os
+from pathlib import Path
+import subprocess
+import textwrap
+from typing import Callable
+from unittest import mock
+
+from cros_utils import git_utils
+from llvm_tools import git_llvm_rev
+from llvm_tools import llvm_project_base_commit
+from llvm_tools import patch_utils
+from llvm_tools import test_helpers
+from llvm_tools import verify_patch_consistency
+
+
+GERRIT_JSON_FIXTURE = """\
+[
+  {
+    "project": "github.com/llvm/llvm-project",
+    "branch": "chromeos/llvm-r516547-1",
+    "createdOn": 1718657891,
+    "lastUpdated": 1720459316,
+    "id": "If250deb1c592b3cf054cca8c0cd530f3d6fd4f89",
+    "owner": {
+      "name": "User McUserface",
+      "email": "someuserhere@google.com",
+      "username": "User McUserface"
+    },
+    "number": "5637483",
+    "url": "https://crrev.com/c/5637483",
+    "status": "ABANDONED",
+    "subject": "Revert \\"add_tablegen: Quick fix to reflect LLVM_TABLEGEN\\"",
+    "private": false,
+    "topic": null,
+    "currentPatchSet": {
+      "approvals": [
+        {
+          "type": "CRVW",
+          "description": "Code-Review",
+          "value": "0",
+          "grantedOn": 1718657891,
+          "by": {
+            "name": "User McUserface",
+            "email": "someuserhere@google.com",
+            "username": "User McUserface"
+          }
+        },
+        {
+          "type": "COMR",
+          "description": "Commit-Queue",
+          "value": "0",
+          "grantedOn": 1718752263,
+          "by": {
+            "name": "User McUserface",
+            "email": "someuserhere@google.com",
+            "username": "User McUserface"
+          }
+        },
+        {
+          "type": "VRIF",
+          "description": "Verified",
+          "value": "0",
+          "grantedOn": 1718657891,
+          "by": {
+            "name": "User McUserface",
+            "email": "someuserhere@google.com",
+            "username": "User McUserface"
+          }
+        }
+      ],
+      "ref": "refs/changes/83/5637483/1",
+      "revision": "9f316824661b96d0ba586ff48b6128f7e9783f19",
+      "number": "1",
+      "date": 1718657800,
+      "draft": false
+    },
+    "commitMessage": "A commit message",
+    "dependsOn": [
+      {
+        "revision": "210497ee293346804b43ece37fb9d6658c39ab34"
+      }
+    ]
+  }
+]
+"""
+
+
+@dataclasses.dataclass(frozen=True)
+class _RunnerArgs:
+    main_sha: str
+    patch_branch: str
+    toolchain_utils_dir: Path
+    llvm_src_dir: Path
+    patches_json: Path
+    chromiumos_overlay: Path
+
+
+class TestVerifyPatchConsistency(test_helpers.TempDirTestCase):
+    """Test verify_patch_consistency."""
+
+    def __init__(self, *nargs, **kwargs):
+        super().__init__(*nargs, **kwargs)
+        self.git_utils_patcher = None
+        self.git_llvm_rev_patcher = None
+        self.verify_patch_consistency_patcher = None
+
+    @mock.patch.object(verify_patch_consistency, "_gerrit_inspect")
+    def test_parse_branch_simple(self, mock_gerrit_inspect):
+        """Test we're extracting the llvm revision and ref from gerrit json."""
+
+        mock_gerrit_inspect.return_value = [
+            {
+                "branch": "chromeos/llvm-r1234567-42",
+                "currentPatchSet": {"ref": "some_remote_ref"},
+            }
+        ]
+        llvm_rev, ref = verify_patch_consistency.parse_branch(10101, Path())
+        self.assertEqual(llvm_rev, 1234567)
+        self.assertEqual(ref, "some_remote_ref")
+
+    @mock.patch.object(verify_patch_consistency, "_gerrit_inspect")
+    def test_parse_branch_complex(self, mock_gerrit_inspect):
+        """Test parse_branch with a real JSON response."""
+
+        mock_gerrit_inspect.return_value = json.loads(GERRIT_JSON_FIXTURE)
+        llvm_rev, ref = verify_patch_consistency.parse_branch(5637483, Path())
+        self.assertEqual(llvm_rev, 516547)
+        self.assertEqual(ref, "refs/changes/83/5637483/1")
+
+    def _set_up_mocking(self, translate_sha: str, fetch_head_ref: str):
+        # Patch git_utils ----------------------------------------------
+        self.git_utils_patcher = mock.patch.multiple(
+            git_utils,
+            # Do not allow network calls.
+            fetch=lambda *_, **__: None,
+            # FETCH_HEAD may not exist, so just mock resolve_ref. It's only
+            # used in printing here.
+            resolve_ref=lambda *_, **__: "MOCK",
+        )
+        self.git_utils_patcher.start()
+
+        # Patch git_llvm_rev ------------------------------------------
+        self.git_llvm_rev_patcher = mock.patch.multiple(
+            git_llvm_rev,
+            # Don't actually try to search through the LLVM project
+            # git shas. We don't have the known reference SHAs to
+            # do that.
+            translate_rev_to_sha=lambda _, __: translate_sha,
+        )
+        self.git_llvm_rev_patcher.start()
+
+        # Mock verify_patch_consistency -------------------------------
+
+        # NOTE:
+        # We need to capture the original ref_diff reference here,
+        # because otherwise by the time that _mock_ref_diff evaluation
+        # actually happens, the original verify_patch_consistency will
+        # be fully monkeypatched out, and we'll get infinite recursion.
+        ref_diff_capture = verify_patch_consistency.ref_diff
+
+        def _mock_ref_diff(cwd: Path, ref1: str, _: str) -> str:
+            return ref_diff_capture(cwd, ref1, fetch_head_ref)
+
+        self.verify_patch_consistency_patcher = mock.patch.multiple(
+            verify_patch_consistency,
+            ref_diff=_mock_ref_diff,
+        )
+        self.verify_patch_consistency_patcher.start()
+
+    def _stop_mocking(self):
+        if self.verify_patch_consistency_patcher:
+            self.verify_patch_consistency_patcher.stop()
+        if self.git_llvm_rev_patcher:
+            self.git_llvm_rev_patcher.stop()
+        if self.git_utils_patcher:
+            self.git_utils_patcher.stop()
+
+    def _run_llvm_harness(
+        self, tempdir: Path, llvm_svn_revision: int, runner: Callable
+    ):
+        """Set up for full verification tests."""
+
+        # Set up directories and paths.
+        # Current layout is:
+        #
+        #   toolchain-utils/
+        #     OWNERS
+        #     OWNERS.toolchain
+        #   sys-devel/llvm/
+        #     llvm-9999.ebuild
+        #   llvm-project/
+        #   PATCHES.json
+        #   patch.patch
+        #
+        fake_toolchain_utils = tempdir / "toolchain-utils"
+        fake_toolchain_utils.mkdir()
+        (fake_toolchain_utils / "OWNERS").touch()
+        (fake_toolchain_utils / "OWNERS.toolchain").touch()
+        fake_patches_json_path = tempdir / "PATCHES.json"
+        fake_chromiumos_overlay = tempdir / "chromiumos-overlay"
+        for p in patch_utils.CHROMEOS_LLVM_SUBPACKAGES:
+            package_name = os.path.basename(p)
+            live_ebuild = (
+                fake_chromiumos_overlay / p / f"{package_name}-9999.ebuild"
+            )
+            live_ebuild.parent.mkdir(parents=True)
+            # Always use llvm as the CMAKE_USE_DIR; it's simplest to mock, and
+            # we don't gain meaningful additional coverage by varying it.
+            live_ebuild.write_text(
+                'export CMAKE_USE_DIR="${S}/llvm"\n', encoding="utf-8"
+            )
+        patch_name = "patch.patch"
+        with fake_patches_json_path.open("w", encoding="utf-8") as f:
+            json.dump([{"rel_patch_path": patch_name}], f)
+        fake_llvm_src_dir = tempdir / "llvm-project"
+        fake_llvm_src_dir.mkdir()
+        cmake_file = fake_llvm_src_dir / "llvm" / "CMakeLists.txt"
+        cmake_file.parent.mkdir()
+        cmake_file.touch()
+
+        # Set up git state.
+        # There's a lot going on here, but it mostly sets up commits
+        # like so:
+        #
+        #   a main
+        #    \
+        #     -> b -> c patch_branch
+        #
+        # where
+        #   a = Initial commit
+        #   b = ChromeOS Base commit
+        #   c = Hello World Commit
+        subprocess.run(
+            ["git", "init", "-b", "main", "-q"],
+            cwd=fake_llvm_src_dir,
+            check=True,
+        )
+        a_file = fake_llvm_src_dir / "a.txt"
+        a_file.touch()
+        git_utils.commit_all_changes(fake_llvm_src_dir, "Initial commit")
+        main_sha = subprocess.run(
+            ["git", "log", "-n1", "--format=%H"],
+            check=True,
+            cwd=fake_llvm_src_dir,
+            stdout=subprocess.PIPE,
+            encoding="utf-8",
+        ).stdout.strip()
+        patch_branch = "patch_branch"
+        subprocess.run(
+            ["git", "switch", "-C", patch_branch],
+            cwd=fake_llvm_src_dir,
+            check=True,
+        )
+        llvm_project_base_commit.make_base_commit(
+            fake_toolchain_utils,
+            fake_llvm_src_dir,
+            llvm_svn_revision,
+            chromiumos_overlay=fake_chromiumos_overlay,
+        )
+        a_file.write_text("hello world", encoding="utf-8")
+        git_utils.commit_all_changes(fake_llvm_src_dir, "Hello world commit")
+        diff = git_utils.format_patch(fake_llvm_src_dir, "HEAD")
+        subprocess.run(
+            ["git", "switch", "-C", "main"], cwd=fake_llvm_src_dir, check=True
+        )
+        (tempdir / patch_name).write_text(diff, encoding="utf-8")
+        runner(
+            _RunnerArgs(
+                main_sha=main_sha,
+                patch_branch=patch_branch,
+                toolchain_utils_dir=fake_toolchain_utils,
+                llvm_src_dir=fake_llvm_src_dir,
+                patches_json=fake_patches_json_path,
+                chromiumos_overlay=fake_chromiumos_overlay,
+            )
+        )
+
+    def test_failed_verification(self):
+        """Test we can catch a bad patch stack."""
+        tempdir = self.make_tempdir()
+        svn_revision = 1234
+
+        def _runner(args: _RunnerArgs):
+            # Actually run the (failing) verification.
+            # This fails because the "upstream" of fetch_head_ref
+            # is the same as the translated_sha (which is in
+            # the "main" that matches the svn_revision).
+            self._set_up_mocking(
+                translate_sha=args.main_sha, fetch_head_ref=args.main_sha
+            )
+            try:
+                self.assertFalse(
+                    verify_patch_consistency.verify_in_worktree(
+                        toolchain_utils_dir=args.toolchain_utils_dir,
+                        llvm_src_dir=args.llvm_src_dir,
+                        patches_json=args.patches_json,
+                        chromiumos_overlay=args.chromiumos_overlay,
+                        svn_revision=svn_revision,
+                        cl_ref=args.main_sha,
+                        ignore_cq_depend=True,
+                    )
+                )
+            finally:
+                self._stop_mocking()
+
+        self._run_llvm_harness(tempdir, svn_revision, _runner)
+
+    def test_successful_verification(self):
+        """Test we can successfully verify a patch stack."""
+        tempdir = self.make_tempdir()
+        svn_revision = 1234
+
+        def _runner(args: _RunnerArgs):
+            # Actually run the verification. Notably,
+            # the difference here between the fail case is the
+            # fetch_head_ref is the patch_branch which acts
+            # as our "upstream".
+            self._set_up_mocking(
+                translate_sha=args.main_sha,
+                fetch_head_ref=args.patch_branch,
+            )
+            try:
+                self.assertTrue(
+                    verify_patch_consistency.verify_in_worktree(
+                        toolchain_utils_dir=args.toolchain_utils_dir,
+                        llvm_src_dir=args.llvm_src_dir,
+                        patches_json=args.patches_json,
+                        chromiumos_overlay=args.chromiumos_overlay,
+                        svn_revision=svn_revision,
+                        cl_ref=args.patch_branch,
+                        ignore_cq_depend=True,
+                    )
+                )
+            finally:
+                self._stop_mocking()
+
+        self._run_llvm_harness(tempdir, svn_revision, _runner)
+
+    @mock.patch.object(git_utils, "log")
+    def test_cq_depend_detection_passes_if_no_cq_depend(self, git_log_mock):
+        # Put this in a tempdir in case it escapes the mocks somehow; we know
+        # the tempdir is empty.
+        tempdir = self.make_tempdir()
+        git_log_mock.return_value = textwrap.dedent(
+            """\
+            foo bar baz
+            > Cq-Depend: that doesn't matter
+            Cq-Depend that also doesn't matter (no `:`)
+            qux
+            """
+        )
+        self.assertTrue(
+            verify_patch_consistency.verify_no_cq_depends(
+                worktree_dir=tempdir, head="HEAD", baseline_hash="HEAD~"
+            )
+        )
+
+    @mock.patch.object(git_utils, "log")
+    def test_cq_depend_detection_raises_if_no_log_contents(self, git_log_mock):
+        tempdir = self.make_tempdir()
+
+        git_log_mock.return_value = ""
+        with self.assertRaisesRegex(ValueError, "`git log` was empty.*"):
+            verify_patch_consistency.verify_no_cq_depends(
+                worktree_dir=tempdir, head="HEAD", baseline_hash="HEAD~"
+            )
+
+        git_log_mock.return_value = "\n"
+        with self.assertRaisesRegex(ValueError, "`git log` was empty.*"):
+            verify_patch_consistency.verify_no_cq_depends(
+                worktree_dir=tempdir, head="HEAD", baseline_hash="HEAD~"
+            )
+
+    @mock.patch.object(git_utils, "log")
+    def test_cq_depend_detection_fails_if_cq_depend(self, git_log_mock):
+        tempdir = self.make_tempdir()
+        git_log_mock.return_value = textwrap.dedent(
+            """\
+            foo bar baz
+            Cq-Depend: chromium:1234
+            qux
+            """
+        )
+        self.assertFalse(
+            verify_patch_consistency.verify_no_cq_depends(
+                worktree_dir=tempdir, head="HEAD", baseline_hash="HEAD~"
+            )
+        )
diff --git a/llvm_tools/werror_logs.py b/llvm_tools/werror_logs.py
old mode 100755
new mode 100644
index f8a526f1..cda0dc14
--- a/llvm_tools/werror_logs.py
+++ b/llvm_tools/werror_logs.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -41,7 +40,7 @@ import tempfile
 import threading
 from typing import Any, Counter, DefaultDict, Dict, IO, Iterable, List, Optional
 
-import cros_cls
+from llvm_tools import cros_cls
 
 
 _DEFAULT_FETCH_DIRECTORY = Path("/tmp/werror_logs")
@@ -166,7 +165,7 @@ class AggregatedWarnings:
     )
 
     _CWD_PACKAGE_RE = re.compile(
-        r"^(?:/build/[^/]+)?/var/(?:cache|tmp)/portage/([^/]+/[^/]+)/"
+        r"^(?:/build/[^/]+)?/(?:var/)?(?:cache|tmp)/portage/([^/]+/[^/]+)/"
     )
 
     @classmethod
@@ -571,7 +570,3 @@ def main(argv: List[str]) -> None:
 
     assert getattr(opts, "func", None), "Unknown subcommand?"
     opts.func(opts)
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/llvm_tools/werror_logs_test.py b/llvm_tools/werror_logs_test.py
old mode 100755
new mode 100644
index c6489389..64781259
--- a/llvm_tools/werror_logs_test.py
+++ b/llvm_tools/werror_logs_test.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2024 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -8,16 +7,13 @@
 import io
 import logging
 import os
-from pathlib import Path
-import shutil
 import subprocess
-import tempfile
 import textwrap
 from typing import Dict
-import unittest
 from unittest import mock
 
-import werror_logs
+from llvm_tools import test_helpers
+from llvm_tools import werror_logs
 
 
 class SilenceLogs:
@@ -38,7 +34,7 @@ def create_warning_info(packages: Dict[str, int]) -> werror_logs.WarningInfo:
     return x
 
 
-class Test(unittest.TestCase):
+class Test(test_helpers.TempDirTestCase):
     """Tests for werror_logs."""
 
     def silence_logs(self):
@@ -47,11 +43,6 @@ class Test(unittest.TestCase):
         log.addFilter(f)
         self.addCleanup(log.removeFilter, f)
 
-    def make_tempdir(self) -> Path:
-        tempdir = tempfile.mkdtemp("werror_logs_test_")
-        self.addCleanup(shutil.rmtree, tempdir)
-        return Path(tempdir)
-
     def test_clang_warning_parsing_parses_flag_errors(self):
         self.assertEqual(
             werror_logs.ClangWarning.try_parse_line(
@@ -260,6 +251,24 @@ class Test(unittest.TestCase):
             warning_info, create_warning_info({"sys-devel/llvm": len(cwds)})
         )
 
+    def test_guessing_real_packages_correctly(self):
+        # pylint: disable=line-too-long
+        aggregated = werror_logs.AggregatedWarnings()
+        cwds = (
+            "/build/amd64-generic/tmp/portage/dev-util/perf-5.15.68-r4/work/",
+            "/build/amd64-generic/tmp/portage/app-benchmarks/stress-ng-0.13.09/work/stress-ng-0.13.09",
+        )
+        for d in cwds:
+            aggregated.add_report_json(
+                {
+                    "cwd": d,
+                    "stdout": "clang-17: error: foo [-Werror,-Wfoo]",
+                }
+            )
+        self.assertEqual(len(aggregated.warnings), 1)
+        warning, _ = next(iter(aggregated.warnings.items()))
+        self.assertEqual(warning.name, "-Wfoo")
+
     def test_aggregation_raises_if_package_name_cant_be_guessed(self):
         aggregated = werror_logs.AggregatedWarnings()
         with self.assertRaises(werror_logs.UnknownPackageNameError):
@@ -419,7 +428,3 @@ class Test(unittest.TestCase):
                 unpack_dir, download_dir, gs_urls
             )
         self.assertEqual(run_mock.call_count, 3)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/llvm_tools/workon_llvm.sh b/llvm_tools/workon_llvm.sh
new file mode 100755
index 00000000..7b839fe2
--- /dev/null
+++ b/llvm_tools/workon_llvm.sh
@@ -0,0 +1,74 @@
+#!/bin/bash -eu
+
+# Copyright 2025 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+# Enable cros workon for all the LLVM packages, and their cross-compilation
+# variations.
+
+if [[ -z "${1:-}" || "$1" == "--help" || "$1" == "-h" ]]; then
+  >&2 echo "Run cros workon start for all LLVM packages"
+  >&2 echo
+  >&2 echo "USAGE: $0 [-h|--help] BOARD"
+  >&2 echo
+  >&2 echo "  -h,--help:  Print this help."
+  >&2 echo
+  >&2 echo "  BOARD: the board to workon board-specific packages for. If"
+  >&2 echo "    you'd only like to build host packages, pass '-' for this."
+  exit 1
+fi
+
+CROSS_COMPILE_TARGETS=(
+  cross-aarch64-cros-linux-gnu
+  cross-arm-none-eabi
+  cross-armv7a-cros-linux-gnueabihf
+  cross-armv7m-cros-eabi
+  cross-riscv32-cros-elf
+  cross-x86_64-cros-linux-gnu
+)
+
+CROSS_COMPILE_PKGS=(
+  libcxx
+  llvm-libunwind
+  compiler-rt
+)
+
+BOARD_PKGS=(
+  sys-libs/libcxx
+  sys-libs/llvm-libunwind
+  sys-libs/compiler-rt
+  sys-libs/scudo
+  dev-util/lldb-server
+)
+
+workon_host() {
+  local cross_compile_combination
+  cross_compile_combination=()
+  for target in "${CROSS_COMPILE_TARGETS[@]}"; do
+    for pkg in "${CROSS_COMPILE_PKGS[@]}"; do
+      cross_compile_combination+=("${target}/${pkg}")
+    done
+  done
+
+  cros workon --host start \
+    sys-devel/llvm \
+    sys-libs/libcxx \
+    sys-libs/llvm-libunwind \
+    sys-libs/scudo \
+    dev-util/lldb-server \
+    "${cross_compile_combination[@]}"
+}
+
+workon_board() {
+  local board="$1"
+  cros workon -b "${board}" start "${BOARD_PKGS[@]}"
+}
+
+workon_host
+echo "Set up host packages!"
+board="$1"
+if [[ "${board}" != "-" ]]; then
+  workon_board "$1"
+  echo "Set up packages for $1!"
+fi
diff --git a/lock_machine.py b/lock_machine.py
deleted file mode 100755
index 5c2bedb3..00000000
--- a/lock_machine.py
+++ /dev/null
@@ -1,571 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Copyright 2019 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""This module controls locking and unlocking of test machines."""
-
-
-import argparse
-import enum
-import getpass
-import os
-import sys
-
-from cros_utils import command_executer
-from cros_utils import logger
-from cros_utils import machines
-import file_lock_machine
-
-
-class LockException(Exception):
-    """Base class for exceptions in this module."""
-
-
-class MachineNotPingable(LockException):
-    """Raised when machine does not respond to ping."""
-
-
-class LockingError(LockException):
-    """Raised when server fails to lock/unlock machine as requested."""
-
-
-class DontOwnLock(LockException):
-    """Raised when user attmepts to unlock machine locked by someone else."""
-
-    # This should not be raised if the user specified '--force'
-
-
-class MachineType(enum.Enum):
-    """Enum class to hold machine type."""
-
-    LOCAL = "local"
-    CROSFLEET = "crosfleet"
-
-
-class LockManager(object):
-    """Class for locking/unlocking machines vie three different modes.
-
-    This class contains methods for checking the locked status of machines,
-    and for changing the locked status.  It handles HW lab machines and local
-    machines, using appropriate locking mechanisms for each.
-    """
-
-    CROSFLEET_PATH = "crosfleet"
-
-    # TODO(zhizhouy): lease time may needs to be dynamically adjusted. For now we
-    # set it long enough to cover the period to finish nightly rotation tests.
-    LEASE_MINS = 1439
-
-    CROSFLEET_CREDENTIAL = (
-        "/usr/local/google/home/mobiletc-prebuild"
-        "/sheriff_utils/credentials/skylab"
-        "/chromeos-swarming-credential.json"
-    )
-    SWARMING = "~/cipd_binaries/swarming"
-    SUCCESS = 0
-
-    def __init__(
-        self, remotes, force_option, chromeos_root, locks_dir="", log=None
-    ):
-        """Initializes an LockManager object.
-
-        Args:
-          remotes: A list of machine names or ip addresses to be managed.  Names
-            and ip addresses should be represented as strings.  If the list is
-            empty, the lock manager will get all known machines.
-          force_option: A Boolean indicating whether or not to force an unlock of
-            a machine that was locked by someone else.
-          chromeos_root: The ChromeOS chroot to use for the autotest scripts.
-          locks_dir: A directory used for file locking local devices.
-          log: If not None, this is the logger object to be used for writing out
-            informational output messages.  It is expected to be an instance of
-            Logger class from cros_utils/logger.py.
-        """
-        self.chromeos_root = chromeos_root
-        self.user = getpass.getuser()
-        self.logger = log or logger.GetLogger()
-        self.ce = command_executer.GetCommandExecuter(self.logger)
-
-        sys.path.append(chromeos_root)
-
-        self.locks_dir = locks_dir
-
-        self.machines = list(set(remotes)) or []
-        self.toolchain_lab_machines = self.GetAllToolchainLabMachines()
-
-        if not self.machines:
-            self.machines = self.toolchain_lab_machines
-        self.force = force_option
-
-        self.local_machines = []
-        self.crosfleet_machines = []
-
-    def CheckMachine(self, machine, error_msg):
-        """Verifies that machine is responding to ping.
-
-        Args:
-          machine: String containing the name or ip address of machine to check.
-          error_msg: Message to print if ping fails.
-
-        Raises:
-          MachineNotPingable:  If machine is not responding to 'ping'
-        """
-        if not machines.MachineIsPingable(machine, logging_level="none"):
-            cros_machine = machine + ".cros"
-            if not machines.MachineIsPingable(
-                cros_machine, logging_level="none"
-            ):
-                raise MachineNotPingable(error_msg)
-
-    def GetAllToolchainLabMachines(self):
-        """Gets a list of all the toolchain machines in the ChromeOS HW lab.
-
-        Returns:
-          A list of names of the toolchain machines in the ChromeOS HW lab.
-        """
-        machines_file = os.path.join(
-            os.path.dirname(__file__), "crosperf", "default_remotes"
-        )
-        machine_list = []
-        with open(machines_file, "r") as input_file:
-            lines = input_file.readlines()
-            for line in lines:
-                _, remotes = line.split(":")
-                remotes = remotes.strip()
-                for r in remotes.split():
-                    machine_list.append(r.strip())
-        return machine_list
-
-    def GetMachineType(self, m):
-        """Get where the machine is located.
-
-        Args:
-          m: String containing the name or ip address of machine.
-
-        Returns:
-          Value of the type in MachineType Enum.
-        """
-        if m in self.local_machines:
-            return MachineType.LOCAL
-        if m in self.crosfleet_machines:
-            return MachineType.CROSFLEET
-
-    def PrintStatusHeader(self):
-        """Prints the status header lines for machines."""
-        print("\nMachine (Board)\t\t\t\t\tStatus")
-        print("---------------\t\t\t\t\t------")
-
-    def PrintStatus(self, m, state, machine_type):
-        """Prints status for a single machine.
-
-        Args:
-          m: String containing the name or ip address of machine.
-          state: A dictionary of the current state of the machine.
-          machine_type: MachineType to determine where the machine is located.
-        """
-        if state["locked"]:
-            print(
-                "%s (%s)\t\t%slocked by %s since %s"
-                % (
-                    m,
-                    state["board"],
-                    "\t\t" if machine_type == MachineType.LOCAL else "",
-                    state["locked_by"],
-                    state["lock_time"],
-                )
-            )
-        else:
-            print(
-                "%s (%s)\t\t%sunlocked"
-                % (
-                    m,
-                    state["board"],
-                    "\t\t" if machine_type == MachineType.LOCAL else "",
-                )
-            )
-
-    def AddMachineToLocal(self, machine):
-        """Adds a machine to local machine list.
-
-        Args:
-          machine: The machine to be added.
-        """
-        if machine not in self.local_machines:
-            self.local_machines.append(machine)
-
-    def AddMachineToCrosfleet(self, machine):
-        """Adds a machine to crosfleet machine list.
-
-        Args:
-          machine: The machine to be added.
-        """
-        if machine not in self.crosfleet_machines:
-            self.crosfleet_machines.append(machine)
-
-    def ListMachineStates(self, machine_states):
-        """Gets and prints the current status for a list of machines.
-
-        Prints out the current status for all of the machines in the current
-        LockManager's list of machines (set when the object is initialized).
-
-        Args:
-          machine_states: A dictionary of the current state of every machine in
-            the current LockManager's list of machines.  Normally obtained by
-            calling LockManager::GetMachineStates.
-        """
-        self.PrintStatusHeader()
-        for m in machine_states:
-            machine_type = self.GetMachineType(m)
-            state = machine_states[m]
-            self.PrintStatus(m, state, machine_type)
-
-    def UpdateLockInCrosfleet(self, should_lock_machine, machine):
-        """Ask crosfleet to lease/release a machine.
-
-        Args:
-          should_lock_machine: Boolean indicating whether to lock the machine (True)
-            or unlock the machine (False).
-          machine: The machine to update.
-
-        Returns:
-          True if requested action succeeded, else False.
-        """
-        try:
-            if should_lock_machine:
-                ret = self.LeaseCrosfleetMachine(machine)
-            else:
-                ret = self.ReleaseCrosfleetMachine(machine)
-        except Exception:
-            return False
-        return ret
-
-    def UpdateFileLock(self, should_lock_machine, machine):
-        """Use file lock for local machines,
-
-        Args:
-          should_lock_machine: Boolean indicating whether to lock the machine (True)
-            or unlock the machine (False).
-          machine: The machine to update.
-
-        Returns:
-          True if requested action succeeded, else False.
-        """
-        try:
-            if should_lock_machine:
-                ret = file_lock_machine.Machine(machine, self.locks_dir).Lock(
-                    True, sys.argv[0]
-                )
-            else:
-                ret = file_lock_machine.Machine(machine, self.locks_dir).Unlock(
-                    True
-                )
-        except Exception:
-            return False
-        return ret
-
-    def UpdateMachines(self, lock_machines):
-        """Sets the locked state of the machines to the requested value.
-
-        The machines updated are the ones in self.machines (specified when the
-        class object was intialized).
-
-        Args:
-          lock_machines: Boolean indicating whether to lock the machines (True) or
-            unlock the machines (False).
-
-        Returns:
-          A list of the machines whose state was successfully updated.
-        """
-        updated_machines = []
-        action = "Locking" if lock_machines else "Unlocking"
-        for m in self.machines:
-            # TODO(zhizhouy): Handling exceptions with more details when locking
-            # doesn't succeed.
-            machine_type = self.GetMachineType(m)
-            if machine_type == MachineType.CROSFLEET:
-                ret = self.UpdateLockInCrosfleet(lock_machines, m)
-            elif machine_type == MachineType.LOCAL:
-                ret = self.UpdateFileLock(lock_machines, m)
-
-            if ret:
-                self.logger.LogOutput(
-                    "%s %s machine succeeded: %s."
-                    % (action, machine_type.value, m)
-                )
-                updated_machines.append(m)
-            else:
-                self.logger.LogOutput(
-                    "%s %s machine failed: %s."
-                    % (action, machine_type.value, m)
-                )
-
-        self.machines = updated_machines
-        return updated_machines
-
-    def _InternalRemoveMachine(self, machine):
-        """Remove machine from internal list of machines.
-
-        Args:
-          machine: Name of machine to be removed from internal list.
-        """
-        # Check to see if machine is lab machine and if so, make sure it has
-        # ".cros" on the end.
-        cros_machine = machine
-        if machine.find("rack") > 0 and machine.find("row") > 0:
-            if machine.find(".cros") == -1:
-                cros_machine = cros_machine + ".cros"
-
-        self.machines = [
-            m for m in self.machines if m not in (cros_machine, machine)
-        ]
-
-    def CheckMachineLocks(self, machine_states, cmd):
-        """Check that every machine in requested list is in the proper state.
-
-        If the cmd is 'unlock' verify that every machine is locked by requestor.
-        If the cmd is 'lock' verify that every machine is currently unlocked.
-
-        Args:
-          machine_states: A dictionary of the current state of every machine in
-            the current LockManager's list of machines.  Normally obtained by
-            calling LockManager::GetMachineStates.
-          cmd: The user-requested action for the machines: 'lock' or 'unlock'.
-
-        Raises:
-          DontOwnLock: The lock on a requested machine is owned by someone else.
-        """
-        for k, state in machine_states.items():
-            if cmd == "unlock":
-                if not state["locked"]:
-                    self.logger.LogWarning(
-                        "Attempt to unlock already unlocked machine "
-                        "(%s)." % k
-                    )
-                    self._InternalRemoveMachine(k)
-
-                # TODO(zhizhouy): Crosfleet doesn't support host info such as locked_by.
-                # Need to update this when crosfleet supports it.
-                if (
-                    state["locked"]
-                    and state["locked_by"]
-                    and state["locked_by"] != self.user
-                ):
-                    raise DontOwnLock(
-                        "Attempt to unlock machine (%s) locked by someone "
-                        "else (%s)." % (k, state["locked_by"])
-                    )
-            elif cmd == "lock":
-                if state["locked"]:
-                    self.logger.LogWarning(
-                        "Attempt to lock already locked machine (%s)" % k
-                    )
-                    self._InternalRemoveMachine(k)
-
-    def GetMachineStates(self, cmd=""):
-        """Gets the current state of all the requested machines.
-
-        Gets the current state of all the requested machines. Stores the data in a
-        dictionary keyed by machine name.
-
-        Args:
-          cmd: The command for which we are getting the machine states. This is
-            important because if one of the requested machines is missing we raise
-            an exception, unless the requested command is 'add'.
-
-        Returns:
-          A dictionary of machine states for all the machines in the LockManager
-          object.
-        """
-        machine_list = {}
-        for m in self.machines:
-            # For local or crosfleet machines, we simply set {'locked': status} for
-            # them
-            # TODO(zhizhouy): This is a quick fix since crosfleet cannot return host
-            # info as afe does. We need to get more info such as locked_by when
-            # crosfleet supports that.
-            values = {
-                "locked": 0 if cmd == "lock" else 1,
-                "board": "??",
-                "locked_by": "",
-                "lock_time": "",
-            }
-            machine_list[m] = values
-
-        self.ListMachineStates(machine_list)
-
-        return machine_list
-
-    def CheckMachineInCrosfleet(self, machine):
-        """Run command to check if machine is in Crosfleet or not.
-
-        Returns:
-          True if machine in crosfleet, else False
-        """
-        credential = ""
-        if os.path.exists(self.CROSFLEET_CREDENTIAL):
-            credential = "--service-account-json %s" % self.CROSFLEET_CREDENTIAL
-        server = "--server https://chromeos-swarming.appspot.com"
-        dimensions = "--dimension dut_name=%s" % machine.rstrip(".cros")
-
-        cmd = f"{self.SWARMING} bots {server} {credential} {dimensions}"
-        exit_code, stdout, stderr = self.ce.RunCommandWOutput(cmd)
-        if exit_code:
-            raise ValueError(
-                "Querying bots failed (2); stdout: %r; stderr: %r"
-                % (stdout, stderr)
-            )
-
-        # The command will return a json output as stdout. If machine not in
-        # crosfleet, stdout will look like this:
-        #  {
-        #    "death_timeout": "600",
-        #    "now": "TIMESTAMP"
-        #  }
-        # Otherwise there will be a tuple starting with 'items', we simply detect
-        # this keyword for result.
-        return stdout != "[]"
-
-    def LeaseCrosfleetMachine(self, machine):
-        """Run command to lease dut from crosfleet.
-
-        Returns:
-          True if succeeded, False if failed.
-        """
-        credential = ""
-        if os.path.exists(self.CROSFLEET_CREDENTIAL):
-            credential = "-service-account-json %s" % self.CROSFLEET_CREDENTIAL
-        cmd = ("%s dut lease -minutes %s %s %s %s") % (
-            self.CROSFLEET_PATH,
-            self.LEASE_MINS,
-            credential,
-            "-host",
-            machine.rstrip(".cros"),
-        )
-        # Wait 8 minutes for server to start the lease task, if not started,
-        # we will treat it as unavailable.
-        check_interval_time = 480
-        retval = self.ce.RunCommand(cmd, command_timeout=check_interval_time)
-        return retval == self.SUCCESS
-
-    def ReleaseCrosfleetMachine(self, machine):
-        """Run command to release dut from crosfleet.
-
-        Returns:
-          True if succeeded, False if failed.
-        """
-        credential = ""
-        if os.path.exists(self.CROSFLEET_CREDENTIAL):
-            credential = "-service-account-json %s" % self.CROSFLEET_CREDENTIAL
-
-        cmd = ("%s dut abandon %s %s") % (
-            self.CROSFLEET_PATH,
-            credential,
-            machine.rstrip(".cros"),
-        )
-        retval = self.ce.RunCommand(cmd)
-        return retval == self.SUCCESS
-
-
-def Main(argv):
-    """Parse the options, initialize lock manager and dispatch proper method.
-
-    Args:
-      argv: The options with which this script was invoked.
-
-    Returns:
-      0 unless an exception is raised.
-    """
-    parser = argparse.ArgumentParser()
-
-    parser.add_argument(
-        "--list",
-        dest="cmd",
-        action="store_const",
-        const="status",
-        help="List current status of all known machines.",
-    )
-    parser.add_argument(
-        "--lock",
-        dest="cmd",
-        action="store_const",
-        const="lock",
-        help="Lock given machine(s).",
-    )
-    parser.add_argument(
-        "--unlock",
-        dest="cmd",
-        action="store_const",
-        const="unlock",
-        help="Unlock given machine(s).",
-    )
-    parser.add_argument(
-        "--status",
-        dest="cmd",
-        action="store_const",
-        const="status",
-        help="List current status of given machine(s).",
-    )
-    parser.add_argument(
-        "--remote", dest="remote", help="machines on which to operate"
-    )
-    parser.add_argument(
-        "--chromeos_root",
-        dest="chromeos_root",
-        required=True,
-        help="ChromeOS root to use for autotest scripts.",
-    )
-    parser.add_argument(
-        "--force",
-        dest="force",
-        action="store_true",
-        default=False,
-        help="Force lock/unlock of machines, even if not"
-        " current lock owner.",
-    )
-
-    options = parser.parse_args(argv)
-
-    if not options.remote and options.cmd != "status":
-        parser.error("No machines specified for operation.")
-
-    if not os.path.isdir(options.chromeos_root):
-        parser.error("Cannot find chromeos_root: %s." % options.chromeos_root)
-
-    if not options.cmd:
-        parser.error(
-            "No operation selected (--list, --status, --lock, --unlock,"
-            " --add_machine, --remove_machine)."
-        )
-
-    machine_list = []
-    if options.remote:
-        machine_list = options.remote.split()
-
-    lock_manager = LockManager(
-        machine_list, options.force, options.chromeos_root
-    )
-
-    machine_states = lock_manager.GetMachineStates(cmd=options.cmd)
-    cmd = options.cmd
-
-    if cmd == "status":
-        lock_manager.ListMachineStates(machine_states)
-
-    elif cmd == "lock":
-        if not lock_manager.force:
-            lock_manager.CheckMachineLocks(machine_states, cmd)
-            lock_manager.UpdateMachines(True)
-
-    elif cmd == "unlock":
-        if not lock_manager.force:
-            lock_manager.CheckMachineLocks(machine_states, cmd)
-            lock_manager.UpdateMachines(False)
-
-    return 0
-
-
-if __name__ == "__main__":
-    sys.exit(Main(sys.argv[1:]))
diff --git a/make_root_writable.py b/make_root_writable.py
deleted file mode 100755
index 6a15defc..00000000
--- a/make_root_writable.py
+++ /dev/null
@@ -1,261 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Copyright 2021 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Script to make / directory on chromebook writable.
-
-This script updates a remote chromebook to make the / directory writable."
-"""
-
-
-__author__ = "cmtice@google.com (Caroline Tice)"
-
-import argparse
-import os
-import sys
-import time
-
-from cros_utils import command_executer
-from cros_utils import locks
-from cros_utils import logger
-from cros_utils import machines
-from cros_utils import misc
-
-
-lock_file = "/tmp/image_chromeos_lock/image_chromeos_lock"
-
-
-def Usage(parser, message):
-    print("ERROR: %s" % message)
-    parser.print_help()
-    sys.exit(0)
-
-
-def RebootChromebook(chromeos_root, remote, cmd_executer):
-    cmd = "sudo reboot"
-    cmd_executer.CrosRunCommand(
-        cmd, chromeos_root=chromeos_root, machine=remote
-    )
-    time.sleep(10)
-    success = False
-    for _ in range(1, 10):
-        if machines.MachineIsPingable(remote):
-            success = True
-            break
-        time.sleep(1)
-    return success
-
-
-def ParseOutput(output):
-    # See comment in FindPartitionNum.
-    lines = output.split("\n")
-    num_str = "-1"
-    for line in lines:
-        l = line.strip()
-        words = l.split()
-        if (
-            len(words) > 2
-            and words[0] == "sudo"
-            and words[1] == "/usr/share/vboot/bin/make_dev_ssd.sh"
-            and words[-2] == "--partitions"
-        ):
-            num_str = words[-1]
-            break
-    num = int(num_str)
-
-    return num
-
-
-def FindPartitionNum(chromeos_root, remote, logs, cmd_executer):
-    partition_cmd = (
-        "/usr/share/vboot/bin/make_dev_ssd.sh " "--remove_rootfs_verification"
-    )
-    _, output, _ = cmd_executer.CrosRunCommandWOutput(
-        partition_cmd,
-        chromeos_root=chromeos_root,
-        machine=remote,
-        terminated_timeout=10,
-    )
-
-    # The command above, with no --partitions flag, should return output
-    # in the following form:
-
-    # make_dev_ssd.sh: INFO: checking system firmware...
-    #
-    #  ERROR: YOU ARE TRYING TO MODIFY THE LIVE SYSTEM IMAGE /dev/mmcblk0.
-    #
-    #  The system may become unusable after that change, especially when you have
-    #  some auto updates in progress. To make it safer, we suggest you to only
-    #  change the partition you have booted with. To do that, re-execute this
-    #  command as:
-    #
-    #  sudo /usr/share/vboot/bin/make_dev_ssd.sh  --partitions 4
-    #
-    #  If you are sure to modify other partition, please invoke the command again
-    #  and explicitly assign only one target partition for each time
-    # (--partitions N )
-    #
-    # make_dev_ssd.sh: ERROR: IMAGE /dev/mmcblk0 IS NOT MODIFIED.
-
-    # We pass this output to the ParseOutput function where it finds the 'sudo'
-    # line with the partition number and returns the partition number.
-
-    num = ParseOutput(output)
-
-    if num == -1:
-        logs.LogOutput('Failed to find partition number in "%s"' % output)
-    return num
-
-
-def TryRemoveRootfsFromPartition(
-    chromeos_root, remote, cmd_executer, partition_num
-):
-    partition_cmd = (
-        "/usr/share/vboot/bin/make_dev_ssd.sh "
-        "--remove_rootfs_verification --partition %d" % partition_num
-    )
-    ret = cmd_executer.CrosRunCommand(
-        partition_cmd,
-        chromeos_root=chromeos_root,
-        machine=remote,
-        terminated_timeout=10,
-    )
-    return ret
-
-
-def TryRemountPartitionAsRW(chromeos_root, remote, cmd_executer):
-    command = "sudo mount -o remount,rw /"
-    ret = cmd_executer.CrosRunCommand(
-        command,
-        chromeos_root=chromeos_root,
-        machine=remote,
-        terminated_timeout=10,
-    )
-    return ret
-
-
-def Main(argv):
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "-c",
-        "--chromeos_root",
-        dest="chromeos_root",
-        help="Target directory for ChromeOS installation.",
-    )
-    parser.add_argument("-r", "--remote", dest="remote", help="Target device.")
-    parser.add_argument(
-        "-n",
-        "--no_lock",
-        dest="no_lock",
-        default=False,
-        action="store_true",
-        help="Do not attempt to lock remote before imaging.  "
-        "This option should only be used in cases where the "
-        "exclusive lock has already been acquired (e.g. in "
-        "a script that calls this one).",
-    )
-
-    options = parser.parse_args(argv[1:])
-
-    # Common initializations
-    log_level = "average"
-    cmd_executer = command_executer.GetCommandExecuter(log_level=log_level)
-    l = logger.GetLogger()
-
-    if options.chromeos_root is None:
-        Usage(parser, "--chromeos_root must be set")
-
-    if options.remote is None:
-        Usage(parser, "--remote must be set")
-
-    options.chromeos_root = os.path.expanduser(options.chromeos_root)
-
-    try:
-        should_unlock = False
-        if not options.no_lock:
-            try:
-                _ = locks.AcquireLock(
-                    list(options.remote.split()), options.chromeos_root
-                )
-                should_unlock = True
-            except Exception as e:
-                raise RuntimeError("Error acquiring machine: %s" % str(e))
-
-        # Workaround for crosbug.com/35684.
-        os.chmod(misc.GetChromeOSKeyFile(options.chromeos_root), 0o600)
-
-        if log_level == "average":
-            cmd_executer.SetLogLevel("verbose")
-
-        if not machines.MachineIsPingable(options.remote):
-            raise RuntimeError(
-                "Machine %s does not appear to be up." % options.remote
-            )
-
-        ret = TryRemountPartitionAsRW(
-            options.chromeos_root, options.remote, cmd_executer
-        )
-
-        if ret != 0:
-            l.LogOutput(
-                "Initial mount command failed. Looking for root partition"
-                " number."
-            )
-            part_num = FindPartitionNum(
-                options.chromeos_root, options.remote, l, cmd_executer
-            )
-            if part_num != -1:
-                l.LogOutput(
-                    "Attempting to remove rootfs verification on partition %d"
-                    % part_num
-                )
-                ret = TryRemoveRootfsFromPartition(
-                    options.chromeos_root,
-                    options.remote,
-                    cmd_executer,
-                    part_num,
-                )
-                if ret == 0:
-                    l.LogOutput(
-                        "Succeeded in removing roofs verification from"
-                        " partition %d. Rebooting..." % part_num
-                    )
-                    if not RebootChromebook(
-                        options.chromeos_root, options.remote, cmd_executer
-                    ):
-                        raise RuntimeError("Chromebook failed to reboot.")
-                    l.LogOutput(
-                        "Reboot succeeded. Attempting to remount partition."
-                    )
-                    ret = TryRemountPartitionAsRW(
-                        options.chromeos_root, options.remote, cmd_executer
-                    )
-                    if ret == 0:
-                        l.LogOutput("Re-mounted / as writable.")
-                    else:
-                        l.LogOutput("Re-mount failed. / is not writable.")
-                else:
-                    l.LogOutput(
-                        "Failed to remove rootfs verification from partition"
-                        " %d." % part_num
-                    )
-        else:
-            l.LogOutput("Re-mounted / as writable.")
-
-        l.LogOutput("Exiting.")
-
-    finally:
-        if should_unlock:
-            locks.ReleaseLock(
-                list(options.remote.split()), options.chromeos_root
-            )
-
-    return ret
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv)
-    sys.exit(retval)
diff --git a/pgo_tools/auto_update_llvm_pgo_profile.py b/pgo_tools/auto_update_llvm_pgo_profile.py
new file mode 100644
index 00000000..84df2818
--- /dev/null
+++ b/pgo_tools/auto_update_llvm_pgo_profile.py
@@ -0,0 +1,431 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Automatically manages LLVM's PGO profiles.
+
+Specifically, this script:
+    - generates & uploads new PGO profiles for llvm-next, if necessary
+    - ensures that the revisions for said llvm-next profiles are in the
+      associated manifest file
+    - cleans old llvm profiles from the ebuild
+
+Run this outside of the chroot.
+"""
+
+import argparse
+import collections
+import logging
+from pathlib import Path
+import re
+import shlex
+import subprocess
+import textwrap
+from typing import Dict, List, Optional
+
+from cros_utils import cros_paths
+from cros_utils import git_utils
+from llvm_tools import chroot
+from llvm_tools import get_llvm_hash
+from llvm_tools import llvm_next
+from pgo_tools import pgo_utils
+
+
+PROFDATA_REV_PREFIX = "gs://chromeos-localmirror/distfiles/llvm-profdata-r"
+PROFDATA_REV_SUFFIX = ".xz"
+
+# Path to LLVM's 9999 ebuild from chromiumos-overlay
+LLVM_EBUILD_SUBPATH = Path("sys-devel", "llvm", "llvm-9999.ebuild")
+
+
+class GsProfileCache:
+    """Caches which LLVM revisions we have profile information for (in gs://).
+
+    To use this:
+        1. Create an instance of this class using `GsProfileCache.fetch()`.
+        2. Check if we have profile information for a revision:
+           `cache.has_profile_for_rev(123)`.
+        3. Inform the cache that we have information for a revision:
+           `cache.insert_profile(123, suffix="foo")`.
+    """
+
+    def __init__(self, profiles: Dict[int, List[str]]):
+        """Constructs an object.
+
+        Args:
+            profiles: A dict of profile rev to the list of suffixes for these
+                profiles. An empty suffix implies there is no suffix (incl `-`)
+                in the profile name. These should be sorted from oldest ->
+                newest. No list should be empty.
+        """
+        self.profile_revs = profiles
+
+    def has_profile_for_rev(self, rev: int) -> bool:
+        return rev in self.profile_revs
+
+    def has_profile(self, rev: int, suffix: str) -> bool:
+        return self.has_profile_for_rev(rev) and any(
+            x == suffix for x in self.profile_revs[rev]
+        )
+
+    def newest_profile_name_for(self, rev: int) -> str:
+        """Gets the newest profile name for the given rev.
+
+        This name is intended for use in the LLVM ebuild.
+        """
+        if not self.has_profile_for_rev(rev):
+            raise KeyError(f"No profiles for r{rev}")
+        suffix = self.profile_revs[rev][-1]
+        if suffix:
+            suffix = f"-{suffix}"
+        return f"{rev}{suffix}"
+
+    def insert_profile(self, rev: int, suffix: str) -> None:
+        """Inserts a profile with `suffix` at `rev`.
+
+        Assumes that the profile is newer than all other profiles at `rev`.
+        """
+        self.profile_revs.setdefault(rev, []).append(suffix)
+
+    def num_profiles(self) -> int:
+        return sum(len(x) for x in self.profile_revs.values())
+
+    @classmethod
+    def fetch(cls) -> "GsProfileCache":
+        stdout = subprocess.run(
+            [
+                "gsutil",
+                "ls",
+                f"{PROFDATA_REV_PREFIX}*{PROFDATA_REV_SUFFIX}",
+            ],
+            check=True,
+            stdin=subprocess.DEVNULL,
+            stdout=subprocess.PIPE,
+            encoding="utf-8",
+        ).stdout.strip()
+
+        prof_re = re.compile(
+            re.escape(PROFDATA_REV_PREFIX)
+            + r"(\d+)(?:-(.+))?"
+            + re.escape(PROFDATA_REV_SUFFIX)
+        )
+        profiles = collections.defaultdict(list)
+        for line in stdout.splitlines():
+            m = prof_re.fullmatch(line)
+            if not m:
+                if not line.strip():
+                    continue
+                raise ValueError(f"Unparseable line from gs://: {line!r}")
+            profile_rev, suffix = m.groups()
+            profiles[int(profile_rev)].append(suffix if suffix else "")
+
+        # Sort these alphabetically, so getting the 'most recent' profile (by
+        # name) is trivial.
+        for v in profiles.values():
+            v.sort()
+
+        return cls(profiles)
+
+
+def parse_args(argv: List[str]) -> argparse.Namespace:
+    parser = argparse.ArgumentParser(
+        description=__doc__,
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+    )
+    parser.add_argument(
+        "--chromiumos-tree",
+        type=Path,
+        help="""
+        Path to the root of the ChromeOS tree to edit. Autodetected if not
+        specified.
+        """,
+    )
+    parser.add_argument(
+        "--clean-llvm",
+        action="store_true",
+        help="""
+        If a profile needs to be generated and there are uncommitted changes in
+        the LLVM source directory, clean the changes.
+        """,
+    )
+    parser.add_argument(
+        "--dry-run",
+        action="store_true",
+        help="Don't actually upload CLs, or generate new benchmark profiles.",
+    )
+    parser.add_argument(
+        "--force-llvm-next-pgo-generation",
+        action="store_true",
+        help="""
+        Skip checks to see if LLVM profiles already exist. If a profile already
+        exists in gs://, these will not be uploaded. This primarily exists so
+        we can continuously test the PGO profile pipeline.
+        """,
+    )
+    opts = parser.parse_args(argv)
+
+    if not opts.chromiumos_tree:
+        opts.chromiumos_tree = chroot.FindChromeOSRootAboveToolchainUtils()
+    return opts
+
+
+def maybe_upload_new_llvm_next_profile(
+    *,
+    chromiumos_tree: Path,
+    profile_cache: GsProfileCache,
+    dry_run: bool,
+    toolchain_utils: Path,
+    clean_llvm: bool,
+    force_generation: bool,
+) -> None:
+    llvm_next_rev = llvm_next.LLVM_NEXT_REV
+    # NOTE: `profile_suffix` is intentionally hardcoded to ''. Profiles with
+    # non-empty suffixes are expected to be manually generated, and uploaded by
+    # `create_chroot_and_generate_pgo_profile.py`. This script will correctly
+    # detect and roll to these, if necessary.
+    profile_suffix = ""
+    upload_profile = True
+    if profile_cache.has_profile(llvm_next_rev, profile_suffix):
+        if not force_generation:
+            logging.info(
+                "llvm-next profile %d already exists in gs://; no need to "
+                "make a new one",
+                llvm_next_rev,
+            )
+            return
+        logging.info(
+            "llvm-next profile already exists in gs://; forcing generation "
+            "without upload."
+        )
+        upload_profile = False
+
+    create_script = (
+        toolchain_utils
+        / "py"
+        / "bin"
+        / "pgo_tools"
+        / "create_chroot_and_generate_pgo_profile.py"
+    )
+
+    llvm_next_branch = get_llvm_hash.DetectLatestLLVMBranch(
+        chromiumos_tree, rev=llvm_next_rev
+    )
+    if not llvm_next_branch:
+        raise ValueError(f"No LLVM branches found in CrOS for r{llvm_next_rev}")
+
+    logging.info(
+        "Generating a PGO profile for LLVM r%d from branch %s",
+        llvm_next_rev,
+        llvm_next_branch,
+    )
+    cmd: pgo_utils.Command = [
+        create_script,
+        f"--chromiumos-tree={chromiumos_tree}",
+        f"--branch={llvm_next_branch}",
+        f"--profile-suffix={profile_suffix}",
+    ]
+    logging.info(
+        "Generating %s a PGO profile for LLVM r%d",
+        "and uploading" if upload_profile else "without uploading",
+        llvm_next_rev,
+    )
+    if upload_profile:
+        cmd.append("--upload")
+    if clean_llvm:
+        cmd.append("--clean-llvm")
+
+    if dry_run:
+        logging.info(
+            "Skipping PGO profile generation for llvm r%d due to --dry-run. "
+            "Would run: %s",
+            llvm_next_rev,
+            shlex.join(str(x) for x in cmd),
+        )
+        profile_cache.insert_profile(llvm_next_rev, profile_suffix)
+        return
+
+    llvm_project = chromiumos_tree / cros_paths.LLVM_PROJECT
+    if not clean_llvm and git_utils.has_discardable_changes(llvm_project):
+        raise ValueError(
+            f"Uncommitted changes exist in {llvm_project}. Please get rid of "
+            "them before running this script (e.g., with "
+            "`git clean -fd && git reset --hard HEAD`)"
+        )
+
+    initial_head = git_utils.resolve_ref(git_dir=llvm_project, ref="HEAD")
+    try:
+        pgo_utils.run(cmd)
+    finally:
+        logging.info("Restoring llvm-project to its original state...")
+        git_utils.discard_changes_and_checkout(
+            git_dir=llvm_project, ref=initial_head
+        )
+
+    if upload_profile:
+        profile_cache.insert_profile(llvm_next_rev, profile_suffix)
+
+
+def overwrite_llvm_pgo_listing(
+    chromiumos_overlay: Path, profile_names: List[str]
+) -> bool:
+    ebuild = chromiumos_overlay / LLVM_EBUILD_SUBPATH
+    contents = ebuild.read_text(encoding="utf-8")
+    new_pgo_listing = "\t" + "\n\t".join(profile_names)
+
+    array_start = "\nLLVM_PGO_PROFILE_REVS=(\n"
+    array_start_index = contents.index(array_start)
+    array_end_index = contents.index("\n)", array_start_index)
+
+    new_contents = (
+        contents[: array_start_index + len(array_start)]
+        + new_pgo_listing
+        + contents[array_end_index:]
+    )
+    if new_contents == contents:
+        return False
+    ebuild.write_text(new_contents, encoding="utf-8")
+    return True
+
+
+def update_llvm_ebuild_manifest(
+    chromeos_tree: Path, chromiumos_overlay: Path
+) -> None:
+    overlay_relpath = chromiumos_overlay.relative_to(chromeos_tree)
+    overlay_chroot_path = Path("/mnt") / "host" / "source" / overlay_relpath
+    llvm_9999 = overlay_chroot_path / LLVM_EBUILD_SUBPATH
+    ebuild_manifest_cmd = shlex.join(["ebuild", str(llvm_9999), "manifest"])
+    logging.info("Running `%s` in the chroot...", ebuild_manifest_cmd)
+    subprocess.run(
+        ["cros_sdk", "--", "bash", "-c", ebuild_manifest_cmd],
+        check=True,
+        cwd=chromeos_tree,
+    )
+
+
+def create_llvm_pgo_ebuild_update(
+    chromeos_root: Path,
+    chromiumos_overlay: Path,
+    profile_cache: GsProfileCache,
+    dry_run: bool,
+) -> Optional[str]:
+    current_llvm_sha = get_llvm_hash.LLVMHash().GetCrOSCurrentLLVMHash(
+        chromeos_root
+    )
+    current_llvm_rev = (
+        get_llvm_hash.GetCachedUpToDateReadOnlyLLVMRepo().GetRevisionFromHash(
+            current_llvm_sha
+        )
+    )
+    logging.info("Current LLVM revision is %d", current_llvm_rev)
+    want_revisions = [current_llvm_rev]
+
+    llvm_next_rev = llvm_next.LLVM_NEXT_REV
+    if current_llvm_rev != llvm_next_rev:
+        logging.info("llvm-next rev is r%d", llvm_next_rev)
+        if profile_cache.has_profile_for_rev(llvm_next_rev):
+            want_revisions.append(llvm_next_rev)
+        else:
+            logging.info(
+                "No PGO profile exists for r%d; skip adding to profile list",
+                llvm_next_rev,
+            )
+
+    want_names = [
+        profile_cache.newest_profile_name_for(x) for x in want_revisions
+    ]
+    logging.info(
+        "Expected LLVM PGO profile version(s) in ebuild: %s", want_names
+    )
+
+    made_change = overwrite_llvm_pgo_listing(chromiumos_overlay, want_names)
+    if not made_change:
+        logging.info("No LLVM ebuild changes made")
+        return None
+
+    # Skip the manifest update in this case, since the profile cache we're
+    # using might have had a entry inserted by the profile generation stage of
+    # this script.
+    if dry_run:
+        logging.info("Skipping manifest update; --dry-run was passed")
+    else:
+        update_llvm_ebuild_manifest(chromeos_root, chromiumos_overlay)
+    return git_utils.commit_all_changes(
+        chromiumos_overlay,
+        textwrap.dedent(
+            """\
+            llvm: update PGO profile listing
+
+            This CL was generated by toolchain-utils'
+            pgo_tools/auto_update_llvm_pgo_profile.py script.
+
+            BUG=b:337284701
+            TEST=CQ
+            """
+        ),
+    )
+
+
+def main(argv: List[str]) -> None:
+    my_dir = Path(__file__).resolve().parent
+
+    pgo_utils.exit_if_in_chroot()
+
+    logging.basicConfig(
+        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
+        "%(message)s",
+        level=logging.INFO,
+    )
+    opts = parse_args(argv)
+
+    chromiumos_tree = opts.chromiumos_tree
+    chromiumos_overlay = chromiumos_tree / cros_paths.CHROMIUMOS_OVERLAY
+    dry_run = opts.dry_run
+
+    logging.info("Populating gs:// profile cache...")
+    profile_cache = GsProfileCache.fetch()
+    logging.info(
+        "Found %d LLVM PGO profiles in gs://.", profile_cache.num_profiles()
+    )
+
+    maybe_upload_new_llvm_next_profile(
+        chromiumos_tree=chromiumos_tree,
+        profile_cache=profile_cache,
+        dry_run=dry_run,
+        toolchain_utils=my_dir.parent,
+        clean_llvm=opts.clean_llvm,
+        force_generation=opts.force_llvm_next_pgo_generation,
+    )
+
+    # NOTE: `in_dir=chromiumos_tree` here is critical, since this function
+    # needs to enter the chroot to run `ebuild manifest`. Hence, the worktree
+    # must be trivially reachable from within the chroot.
+    with git_utils.create_worktree(
+        chromiumos_overlay, in_dir=chromiumos_tree
+    ) as worktree:
+        maybe_sha = create_llvm_pgo_ebuild_update(
+            chromiumos_tree,
+            worktree,
+            profile_cache,
+            dry_run,
+        )
+
+    if not maybe_sha:
+        logging.info("No changes made to LLVM ebuild; quit.")
+        return
+
+    if dry_run:
+        logging.info(
+            "LLVM ebuild changes committed as %s. --dry-run specified; quit.",
+            maybe_sha,
+        )
+        return
+
+    cls = git_utils.upload_to_gerrit(
+        chromiumos_overlay,
+        remote=git_utils.CROS_EXTERNAL_REMOTE,
+        branch=git_utils.CROS_MAIN_BRANCH,
+        ref=maybe_sha,
+    )
+    for cl in cls:
+        git_utils.set_autoreview_topic_and_labels(chromiumos_overlay, cl)
+    logging.info("%d CL(s) uploaded.", len(cls))
diff --git a/pgo_tools/auto_update_llvm_pgo_profile_test.py b/pgo_tools/auto_update_llvm_pgo_profile_test.py
new file mode 100644
index 00000000..cf853802
--- /dev/null
+++ b/pgo_tools/auto_update_llvm_pgo_profile_test.py
@@ -0,0 +1,101 @@
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Tests for auto_update_llvm_pgo_profile."""
+
+from pathlib import Path
+import subprocess
+import textwrap
+from unittest import mock
+
+from llvm_tools import test_helpers
+from pgo_tools import auto_update_llvm_pgo_profile
+
+
+EXAMPLE_LLVM_EBUILD_SNIPPET = """
+# foo
+# bar
+
+import baz
+
+# comments
+LLVM_PGO_PROFILE_REVS=(
+\t516547
+\t516548
+)
+# some more stuff
+"""
+
+
+class Test(test_helpers.TempDirTestCase):
+    """Tests for auto_update_llvm_pgo_profile."""
+
+    def make_tempdir_with_example_llvm_ebuild(self) -> Path:
+        cros_overlay = self.make_tempdir()
+        llvm_9999 = (
+            cros_overlay / auto_update_llvm_pgo_profile.LLVM_EBUILD_SUBPATH
+        )
+        llvm_9999.parent.mkdir(parents=True)
+        llvm_9999.write_text(EXAMPLE_LLVM_EBUILD_SNIPPET, encoding="utf-8")
+        return cros_overlay
+
+    def test_ebuild_updating_is_nop_when_revs_dont_change(self):
+        cros_overlay = self.make_tempdir_with_example_llvm_ebuild()
+        updated = auto_update_llvm_pgo_profile.overwrite_llvm_pgo_listing(
+            cros_overlay, ["516547", "516548"]
+        )
+        new_contents = (
+            cros_overlay / auto_update_llvm_pgo_profile.LLVM_EBUILD_SUBPATH
+        ).read_text(encoding="utf-8")
+        self.assertEqual(EXAMPLE_LLVM_EBUILD_SNIPPET, new_contents)
+        self.assertFalse(updated)
+
+    def test_ebuild_updating_works_when_rev_is_removed(self):
+        cros_overlay = self.make_tempdir_with_example_llvm_ebuild()
+        self.assertTrue(
+            auto_update_llvm_pgo_profile.overwrite_llvm_pgo_listing(
+                cros_overlay, ["516547"]
+            )
+        )
+        new_contents = (
+            cros_overlay / auto_update_llvm_pgo_profile.LLVM_EBUILD_SUBPATH
+        ).read_text(encoding="utf-8")
+        self.assertIn("\n\t516547\n", new_contents)
+        self.assertNotIn("\n\t516548\n", new_contents)
+
+    def test_ebuild_updating_works_when_rev_is_added(self):
+        cros_overlay = self.make_tempdir_with_example_llvm_ebuild()
+        self.assertTrue(
+            auto_update_llvm_pgo_profile.overwrite_llvm_pgo_listing(
+                cros_overlay, ["516547", "516548", "516549-v3"]
+            )
+        )
+        new_contents = (
+            cros_overlay / auto_update_llvm_pgo_profile.LLVM_EBUILD_SUBPATH
+        ).read_text(encoding="utf-8")
+        self.assertIn("\n\t516547\n", new_contents)
+        self.assertIn("\n\t516548\n", new_contents)
+        self.assertIn("\n\t516549-v3\n", new_contents)
+
+    @mock.patch.object(subprocess, "run")
+    def test_gs_parsing_works(self, mock_run):
+        run_return = mock.MagicMock()
+        run_return.stdout = textwrap.dedent(
+            """\
+            gs://chromeos-localmirror/distfiles/llvm-profdata-r1234-v1.xz
+            gs://chromeos-localmirror/distfiles/llvm-profdata-r1235.xz
+            gs://chromeos-localmirror/distfiles/llvm-profdata-r1235-v2.xz
+            gs://chromeos-localmirror/distfiles/llvm-profdata-r5678.xz
+            """
+        )
+        mock_run.return_value = run_return
+        cache = auto_update_llvm_pgo_profile.GsProfileCache.fetch()
+        self.assertEqual(cache.num_profiles(), 4)
+        self.assertTrue(cache.has_profile_for_rev(1234))
+        self.assertTrue(cache.has_profile_for_rev(1235))
+        self.assertTrue(cache.has_profile_for_rev(5678))
+        self.assertTrue(cache.has_profile(1234, "v1"))
+        self.assertTrue(cache.has_profile(1235, ""))
+        self.assertTrue(cache.has_profile(1235, "v2"))
+        self.assertTrue(cache.has_profile(5678, ""))
diff --git a/pgo_tools/benchmark_pgo_profiles.py b/pgo_tools/benchmark_pgo_profiles.py
old mode 100755
new mode 100644
index 121fb774..fdc79f5f
--- a/pgo_tools/benchmark_pgo_profiles.py
+++ b/pgo_tools/benchmark_pgo_profiles.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2023 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -22,18 +21,19 @@ from pathlib import Path
 import shlex
 import shutil
 import subprocess
-import sys
 from typing import IO, List, Optional, Union
 
-import pgo_tools
+from cros_utils import cros_paths
+from pgo_tools import pgo_utils
 
 
 # The full path to where `sys-devel/llvm` expects local profiles to be if
 # `USE=llvm_pgo_use_local` is specified.
-LOCAL_PROFILE_LOCATION = Path(
-    "/mnt/host/source/src/third_party/chromiumos-overlay",
-    "sys-devel/llvm/files/llvm-local.profdata",
-).resolve()
+LOCAL_PROFILE_LOCATION = (
+    cros_paths.CHROOT_SOURCE_ROOT
+    / cros_paths.CHROMIUMOS_OVERLAY
+    / "sys-devel/llvm/files/llvm-local.profdata"
+)
 
 CHROOT_HYPERFINE = Path.home() / ".cargo/bin/hyperfine"
 
@@ -88,7 +88,7 @@ def ensure_hyperfine_is_installed():
         return
 
     logging.info("Installing hyperfine for benchmarking...")
-    pgo_tools.run(
+    pgo_utils.run(
         [
             "cargo",
             "install",
@@ -106,7 +106,7 @@ def construct_hyperfine_cmd(
     llvm_binpkg: Path,
     use_thinlto: bool,
     export_json: Optional[Path] = None,
-) -> pgo_tools.Command:
+) -> pgo_utils.Command:
     if isinstance(profile, Path):
         if profile != LOCAL_PROFILE_LOCATION:
             shutil.copyfile(profile, LOCAL_PROFILE_LOCATION)
@@ -120,7 +120,7 @@ def construct_hyperfine_cmd(
 
     quickpkg_restore = shlex.join(
         str(x)
-        for x in pgo_tools.generate_quickpkg_restoration_command(llvm_binpkg)
+        for x in pgo_utils.generate_quickpkg_restoration_command(llvm_binpkg)
     )
 
     setup_cmd = (
@@ -140,7 +140,7 @@ def construct_hyperfine_cmd(
         f"sudo USE={shlex.quote(benchmark_use)} "
         f"ebuild {shlex.quote(str(llvm_ebuild))}"
     )
-    cmd: pgo_tools.Command = [
+    cmd: pgo_utils.Command = [
         CHROOT_HYPERFINE,
         "--max-runs=3",
         f"--setup={setup_cmd}",
@@ -208,14 +208,14 @@ def run_benchmark(
     ensure_hyperfine_is_installed()
 
     llvm_ebuild_path = Path(
-        pgo_tools.run(
+        pgo_utils.run(
             ["equery", "w", "sys-devel/llvm"], stdout=subprocess.PIPE
         ).stdout.strip()
     )
 
-    baseline_llvm_binpkg = pgo_tools.quickpkg_llvm()
+    baseline_llvm_binpkg = pgo_utils.quickpkg_llvm()
     accumulated_run_data = []
-    with pgo_tools.temporary_file(
+    with pgo_utils.temporary_file(
         prefix="benchmark_pgo_profile"
     ) as tmp_json_file:
         for profile in profiles:
@@ -233,14 +233,14 @@ def run_benchmark(
                 str(profile),
                 shlex.join(str(x) for x in cmd),
             )
-            pgo_tools.run(cmd)
+            pgo_utils.run(cmd)
 
             with tmp_json_file.open(encoding="utf-8") as f:
                 accumulated_run_data.append(RunData.from_json(str(profile), f))
 
     logging.info("Restoring original LLVM...")
-    pgo_tools.run(
-        pgo_tools.generate_quickpkg_restoration_command(baseline_llvm_binpkg)
+    pgo_utils.run(
+        pgo_utils.generate_quickpkg_restoration_command(baseline_llvm_binpkg)
     )
     return accumulated_run_data
 
@@ -275,13 +275,9 @@ def main(argv: List[str]):
     )
     opts = parser.parse_args(argv)
 
-    pgo_tools.exit_if_not_in_chroot()
+    pgo_utils.exit_if_not_in_chroot()
 
     profiles = opts.profile
     validate_profiles(parser, profiles)
 
     run_benchmark(opts.thinlto, profiles)
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/pgo_tools/benchmark_pgo_profiles_test.py b/pgo_tools/benchmark_pgo_profiles_test.py
old mode 100755
new mode 100644
index 7e71c78c..baa7d7d6
--- a/pgo_tools/benchmark_pgo_profiles_test.py
+++ b/pgo_tools/benchmark_pgo_profiles_test.py
@@ -8,7 +8,7 @@ import io
 import json
 import unittest
 
-import benchmark_pgo_profiles
+from pgo_tools import benchmark_pgo_profiles
 
 
 class Test(unittest.TestCase):
@@ -45,7 +45,3 @@ class Test(unittest.TestCase):
             self.assertIs(
                 profile, benchmark_pgo_profiles.parse_profile_path(str(profile))
             )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/pgo_tools/create_chroot_and_generate_pgo_profile.py b/pgo_tools/create_chroot_and_generate_pgo_profile.py
old mode 100755
new mode 100644
index c09cb4d8..370c12cd
--- a/pgo_tools/create_chroot_and_generate_pgo_profile.py
+++ b/pgo_tools/create_chroot_and_generate_pgo_profile.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2023 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -11,69 +10,122 @@ Do not run it inside of a chroot. It establishes a chroot of its own.
 import argparse
 import dataclasses
 import logging
-import os
 from pathlib import Path
 import re
 import shlex
 import shutil
-import sys
-from typing import List
+from typing import List, Tuple
 
-import pgo_tools
+from cros_utils import cros_paths
+from cros_utils import git_utils
+from llvm_tools import chroot
+from llvm_tools import get_llvm_hash
+from pgo_tools import pgo_utils
+
+
+SDK_VERSION_CONF_SUBDIR = (
+    cros_paths.CHROMIUMOS_OVERLAY
+    / "chromeos"
+    / "binhost"
+    / "host"
+    / "sdk_version.conf"
+)
 
 
 @dataclasses.dataclass(frozen=True)
 class ChrootInfo:
-    """Info that describes a unique chroot."""
+    """Describes a unique chroot, and the SDK version to pin it to."""
 
     chroot_name: str
     out_dir_name: str
+    sdk_version: str
 
 
-def find_repo_root(base_dir: Path) -> Path:
-    """Returns the root of the user's ChromeOS checkout."""
-    if (base_dir / ".repo").exists():
-        return base_dir
-
-    for parent in base_dir.parents:
-        if (parent / ".repo").exists():
-            return parent
-
-    raise ValueError(f"No repo found above {base_dir}")
+def detect_bootstrap_sdk_version(repo_root: Path) -> str:
+    sdk_version_conf = repo_root / SDK_VERSION_CONF_SUBDIR
+    bootstrap_version_re = re.compile(
+        r'^BOOTSTRAP_FROZEN_VERSION="([^"]+)"$',
+        re.MULTILINE,
+    )
+    results = bootstrap_version_re.findall(
+        sdk_version_conf.read_text(encoding="utf-8")
+    )
+    if len(results) != 1:
+        raise ValueError(
+            f"Expected exactly one match in {sdk_version_conf} for "
+            f"{bootstrap_version_re}; found {len(results)}"
+        )
+    return results[0]
 
 
-def create_fresh_bootstrap_chroot(repo_root: Path, chroot_info: ChrootInfo):
-    """Creates a `--bootstrap` chroot without any updates applied."""
-    pgo_tools.run(
+def create_fresh_chroot(
+    repo_root: Path,
+    chroot_info: ChrootInfo,
+):
+    """Creates a chroot. If it already exists, replaces it."""
+    pgo_utils.run(
         [
             "cros_sdk",
             "--replace",
             f"--chroot={chroot_info.chroot_name}",
             f"--out-dir={chroot_info.out_dir_name}",
-            "--bootstrap",
-            "--skip-chroot-upgrade",
+            f"--sdk-version={chroot_info.sdk_version}",
+            "--",
+            "true",
         ],
         cwd=repo_root,
     )
 
 
 def generate_pgo_profile(
+    *,
     repo_root: Path,
     chroot_info: ChrootInfo,
     chroot_output_file: Path,
-    use_var: str,
+    sha: str,
+    clean_llvm: bool,
 ):
     """Generates a PGO profile to `chroot_output_file`."""
-    pgo_tools.run(
-        [
-            "cros_sdk",
-            f"--chroot={chroot_info.chroot_name}",
-            f"--out-dir={chroot_info.out_dir_name}",
-            "--skip-chroot-upgrade",
-            f"USE={use_var}",
-            "--",
-            "/mnt/host/source/src/third_party/toolchain-utils/pgo_tools/"
-            "generate_pgo_profile.py",
+    cros_sdk: pgo_utils.Command = [
+        "cros_sdk",
+        f"--chroot={chroot_info.chroot_name}",
+        f"--out-dir={chroot_info.out_dir_name}",
+        f"--sdk-version={chroot_info.sdk_version}",
+        "--",
+    ]
+    llvm_project = repo_root / cros_paths.LLVM_PROJECT
+    if clean_llvm:
+        git_utils.discard_changes_and_checkout(git_dir=llvm_project, ref=sha)
+    else:
+        git_utils.checkout(git_dir=llvm_project, ref=sha)
+
+    logging.info("Working on all toolchain projects")
+    workon_llvm = (
+        cros_paths.CHROOT_SOURCE_ROOT
+        / cros_paths.TOOLCHAIN_UTILS
+        / "llvm_tools"
+        / "workon_llvm.sh"
+    )
+    pgo_utils.run(
+        cros_sdk
+        + [
+            workon_llvm,
+            # No need to set any boards up here.
+            "-",
+        ],
+        cwd=repo_root,
+    )
+    logging.info("Running PGO profile generation script inside of the chroot")
+    generate_pgo_profile_path = (
+        cros_paths.CHROOT_SOURCE_ROOT
+        / cros_paths.TOOLCHAIN_UTILS_PYBIN
+        / "pgo_tools"
+        / "generate_pgo_profile.py"
+    )
+    pgo_utils.run(
+        cros_sdk
+        + [
+            generate_pgo_profile_path,
             f"--output={chroot_output_file}",
         ],
         cwd=repo_root,
@@ -82,7 +134,7 @@ def generate_pgo_profile(
 
 def compress_pgo_profile(pgo_profile: Path) -> Path:
     """Compresses a PGO profile for upload to gs://."""
-    pgo_tools.run(
+    pgo_utils.run(
         ["xz", "-9", "-k", pgo_profile],
     )
     return Path(str(pgo_profile) + ".xz")
@@ -98,47 +150,16 @@ def translate_chroot_path_to_out_of_chroot(
     return repo_root / info.out_dir_name / str(path)[1:]
 
 
-def locate_current_llvm_ebuild(repo_root: Path) -> Path:
-    """Returns the path to our current LLVM ebuild."""
-    llvm_subdir = (
-        repo_root / "src/third_party/chromiumos-overlay/sys-devel/llvm"
-    )
-    candidates = [
-        x for x in llvm_subdir.glob("*pre*ebuild") if not x.is_symlink()
-    ]
-    assert (
-        len(candidates) == 1
-    ), f"Found {len(candidates)} viable ebuilds; expected 1: {candidates}"
-    return candidates[0]
-
-
-def parse_llvm_next_hash(llvm_ebuild_contents: str) -> List[str]:
-    """Parses the LLVM_NEXT hash from our LLVM ebuild."""
-    matches = re.findall(
-        r'^LLVM_NEXT_HASH="([a-f0-9A-F]{40})" # r\d+$',
-        llvm_ebuild_contents,
-        re.MULTILINE,
-    )
-    assert (
-        len(matches) == 1
-    ), f"Got {len(matches)} matches for llvm hash; expected 1"
-    return matches[0]
-
-
 def determine_upload_command(
-    repo_root: Path, profile_path: Path
-) -> pgo_tools.Command:
+    profile_path: Path, rev: int, suffix: str
+) -> pgo_utils.Command:
     """Returns a command that can be used to upload our PGO profile."""
-    llvm_ebuild = locate_current_llvm_ebuild(repo_root)
-    llvm_next_hash = parse_llvm_next_hash(
-        llvm_ebuild.read_text(encoding="utf-8")
-    )
-    upload_target = (
-        "gs://chromeos-localmirror/distfiles/llvm-profdata-"
-        f"{llvm_next_hash}.xz"
-    )
+    profile_name = f"llvm-profdata-r{rev}"
+    if suffix:
+        profile_name += f"-{suffix}"
+    upload_target = f"gs://chromeos-localmirror/distfiles/{profile_name}.xz"
     return [
-        "gsutil",
+        "gsutil.py",
         "cp",
         "-n",
         "-a",
@@ -148,29 +169,43 @@ def determine_upload_command(
     ]
 
 
-def main(argv: List[str]):
-    logging.basicConfig(
-        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
-        "%(message)s",
-        level=logging.INFO,
-    )
-
+def parse_args(argv: List[str]) -> argparse.Namespace:
     parser = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
+    parser.add_argument(
+        "--chromiumos-tree",
+        type=Path,
+        help="""
+        Path to the root of the ChromeOS tree to edit. Autodetected if not
+        specified.
+        """,
+    )
     parser.add_argument(
         "--chroot",
         default="llvm-next-pgo-chroot",
         help="""
-        Name of the chroot to create. Will be clobbered if it exists already.
+        Name of the chroot to create. Will be recreated if it exists already.
+        """,
+    )
+    parser.add_argument(
+        "--clean-llvm",
+        action="store_true",
+        help="Allow the overwriting of any local changes to LLVM.",
+    )
+    parser.add_argument(
+        "--profile-suffix",
+        default="",
+        help="""
+        Suffix to add to the profile. Only meaningful if --upload is passed.
         """,
     )
     parser.add_argument(
         "--out-dir",
         default="llvm-next-pgo-chroot_out",
         help="""
-        Name of the out/ directory to use. Will be clobbered if it exists
+        Name of the out/ directory to use. Will be recreated if it exists
         already.
         """,
     )
@@ -187,51 +222,91 @@ def main(argv: List[str]):
         creation.
         """,
     )
-    # This flag is required because the most common use-case (pardon the pun)
-    # for this script is "generate the PGO profile for the next LLVM roll." It:
-    # - seems very easy to forget to apply `USE=llvm-next`,
-    # - is awkward to _force_ llvm-next silently, since the "most common"
-    #   use-case is not the _only_ use-case, and
-    # - is awkward to have a duo of `--llvm-next` / `--no-llvm-next` flags,
-    #   since a single `--use=` provides way more flexibility.
     parser.add_argument(
-        "--use",
+        "--branch",
         required=True,
         help="""
-        The value to set for the USE variable when generating the profile. If
-        you're the mage, you want --use=llvm-next. If you don't want to use
-        anything, just pass `--use=`.
+        Branch of LLVM that a PGO profile should be generated for. It's
+        expected that the branch will have any patches applied already. Example
+        branch is 'chromeos/llvm-r547379-1'.
         """,
     )
-    opts = parser.parse_args(argv)
 
-    pgo_tools.exit_if_in_chroot()
+    return parser.parse_args(argv)
+
+
+def get_rev_info_from_branch(branch_name: str) -> Tuple[int, str]:
+    # Note that `GetUpToDateReadOnlyLLVMRepo` prints helpful messages when it
+    # goes to the network, so logging that this is happening here is redundant.
+    llvm_repo = get_llvm_hash.GetCachedUpToDateReadOnlyLLVMRepo()
+
+    sha = git_utils.resolve_ref(git_dir=llvm_repo.path, ref=branch_name)
+    llvm_main = f"{git_utils.CROS_EXTERNAL_REMOTE}/{git_utils.CROS_MAIN_BRANCH}"
+    merge_base = git_utils.merge_base(
+        git_dir=llvm_repo.path,
+        refs=[sha, llvm_main],
+    )
+    if not merge_base:
+        raise ValueError(f"LLVM's main branch shares no history with {sha}?")
+    rev = llvm_repo.GetRevisionFromHash(merge_base)
+    return rev, sha
+
+
+def main(argv: List[str]):
+    logging.basicConfig(
+        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
+        "%(message)s",
+        level=logging.INFO,
+    )
+
+    opts = parse_args(argv)
+    pgo_utils.exit_if_in_chroot()
+
+    rev, sha = get_rev_info_from_branch(opts.branch)
+    logging.info(
+        "Building PGO profile for %s (branched from upstream r%d)", sha, rev
+    )
+
+    if opts.chromiumos_tree:
+        repo_root = chroot.FindChromeOSRootAbove(opts.chromiumos_tree)
+    else:
+        repo_root = chroot.FindChromeOSRootAboveToolchainUtils()
 
-    repo_root = find_repo_root(Path(os.getcwd()))
     logging.info("Repo root is %s", repo_root)
 
     logging.info("Creating new SDK")
-    chroot_info = ChrootInfo(opts.chroot, opts.out_dir)
+    bootstrap_sdk_version = detect_bootstrap_sdk_version(repo_root)
+    logging.info("Detected bootstrap SDK version: %s", bootstrap_sdk_version)
+    bootstrap_chroot_info = ChrootInfo(
+        opts.chroot, opts.out_dir, bootstrap_sdk_version
+    )
     try:
-        create_fresh_bootstrap_chroot(repo_root, chroot_info)
+        create_fresh_chroot(repo_root, bootstrap_chroot_info)
         chroot_profile_path = Path("/tmp/llvm-next-pgo-profile.prof")
         generate_pgo_profile(
-            repo_root, chroot_info, chroot_profile_path, opts.use
+            repo_root=repo_root,
+            chroot_info=bootstrap_chroot_info,
+            chroot_output_file=chroot_profile_path,
+            sha=sha,
+            clean_llvm=opts.clean_llvm,
         )
         profile_path = translate_chroot_path_to_out_of_chroot(
-            repo_root, chroot_profile_path, chroot_info
+            repo_root, chroot_profile_path, bootstrap_chroot_info
         )
         if opts.output:
             shutil.copyfile(profile_path, opts.output)
 
         compressed_profile_path = compress_pgo_profile(profile_path)
         upload_command = determine_upload_command(
-            repo_root, compressed_profile_path
+            compressed_profile_path, rev, opts.profile_suffix
         )
+        friendly_upload_command = shlex.join(str(x) for x in upload_command)
         if opts.upload:
-            pgo_tools.run(upload_command)
+            logging.info(
+                "Running `%s` to upload the profile...", friendly_upload_command
+            )
+            pgo_utils.run(upload_command)
         else:
-            friendly_upload_command = shlex.join(str(x) for x in upload_command)
             logging.info(
                 "To upload the profile, run %r in %r",
                 friendly_upload_command,
@@ -241,18 +316,14 @@ def main(argv: List[str]):
         logging.warning(
             "NOTE: Chroot left at %s and out dir is left at %s. "
             "If you don't plan to rerun this script, delete them.",
-            chroot_info.chroot_name,
-            chroot_info.out_dir_name,
+            bootstrap_chroot_info.chroot_name,
+            bootstrap_chroot_info.out_dir_name,
         )
         raise
     else:
         logging.info(
             "Feel free to delete chroot %s and out dir %s when you're done "
             "with them.",
-            chroot_info.chroot_name,
-            chroot_info.out_dir_name,
+            bootstrap_chroot_info.chroot_name,
+            bootstrap_chroot_info.out_dir_name,
         )
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/pgo_tools/create_chroot_and_generate_pgo_profile_test.py b/pgo_tools/create_chroot_and_generate_pgo_profile_test.py
old mode 100755
new mode 100644
index ac5c9b82..41671ee8
--- a/pgo_tools/create_chroot_and_generate_pgo_profile_test.py
+++ b/pgo_tools/create_chroot_and_generate_pgo_profile_test.py
@@ -8,12 +8,32 @@
 from pathlib import Path
 import shutil
 import tempfile
-import textwrap
 import unittest
 
 # This script's name makes lines exceed 80 chars if it's not imported `as`
 # something shorter.
-import create_chroot_and_generate_pgo_profile as create_chroot_etc
+from pgo_tools import create_chroot_and_generate_pgo_profile as create_chroot_etc
+
+
+EXAMPLE_SDK_VERSION_CONF_FILE = r"""
+# Copyright 2022 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+# The last version of the sdk that we built & tested.
+SDK_LATEST_VERSION="2024.04.22.140014"
+
+# How to find the standalone toolchains from the above sdk.
+TC_PATH="2024/04/%(target)s-2024.04.22.140014.tar.xz"
+
+# Frozen version of SDK used for bootstrapping.
+# If unset, SDK_LATEST_VERSION will be used for bootstrapping.
+BOOTSTRAP_FROZEN_VERSION="2024.03.12.020106"
+
+# The Google Storage bucket containing the SDK tarball and toolchains.
+# If empty, Chromite will assume a default value, likely "chromiumos-sdk".
+SDK_BUCKET=""
+"""
 
 
 class Test(unittest.TestCase):
@@ -24,11 +44,24 @@ class Test(unittest.TestCase):
         self.addCleanup(lambda: shutil.rmtree(tempdir))
         return tempdir
 
+    def test_sdk_version_detection_works(self):
+        repo_root = self.make_tempdir()
+        sdk_version_conf = repo_root / create_chroot_etc.SDK_VERSION_CONF_SUBDIR
+        sdk_version_conf.parent.mkdir(parents=True)
+        sdk_version_conf.write_text(
+            EXAMPLE_SDK_VERSION_CONF_FILE, encoding="utf-8"
+        )
+        self.assertEqual(
+            create_chroot_etc.detect_bootstrap_sdk_version(repo_root),
+            "2024.03.12.020106",
+        )
+
     def test_path_translation_works(self):
         repo_root = Path("/some/repo")
         chroot_info = create_chroot_etc.ChrootInfo(
             chroot_name="my-chroot",
             out_dir_name="my-out",
+            sdk_version="123",
         )
         self.assertEqual(
             create_chroot_etc.translate_chroot_path_to_out_of_chroot(
@@ -36,46 +69,3 @@ class Test(unittest.TestCase):
             ),
             repo_root / "my-out" / "tmp/file/path",
         )
-
-    def test_llvm_ebuild_location(self):
-        tempdir = self.make_tempdir()
-
-        llvm_subdir = (
-            tempdir / "src/third_party/chromiumos-overlay/sys-devel/llvm"
-        )
-        want_ebuild = llvm_subdir / "llvm-18.0.0_pre12345.ebuild"
-        files = [
-            llvm_subdir / "llvm-15.ebuild",
-            llvm_subdir / "llvm-16.0.1-r3.ebuild",
-            want_ebuild,
-            llvm_subdir / "llvm-9999.ebuild",
-        ]
-
-        llvm_subdir.mkdir(parents=True)
-        for f in files:
-            f.touch()
-
-        self.assertEqual(
-            create_chroot_etc.locate_current_llvm_ebuild(tempdir),
-            want_ebuild,
-        )
-
-    def test_llvm_hash_parsing(self):
-        h = create_chroot_etc.parse_llvm_next_hash(
-            textwrap.dedent(
-                """\
-            # Copyright blah blah
-            EAPI=7
-            LLVM_HASH="98f5a340975bc00197c57e39eb4ca26e2da0e8a2" # r496208
-            LLVM_NEXT_HASH="14f0776550b5a49e1c42f49a00213f7f3fa047bf" # r498229
-            # Snip
-            CROS_WORKON_COMMIT=("${LLVM_NEXT_HASH}")
-            """
-            )
-        )
-
-        self.assertEqual(h, "14f0776550b5a49e1c42f49a00213f7f3fa047bf")
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/pgo_tools/ensure_pgo_is_a_win.py b/pgo_tools/ensure_pgo_is_a_win.py
old mode 100755
new mode 100644
index a630f94d..6d1b8ae0
--- a/pgo_tools/ensure_pgo_is_a_win.py
+++ b/pgo_tools/ensure_pgo_is_a_win.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2023 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -16,8 +15,8 @@ import logging
 import sys
 from typing import List
 
-import benchmark_pgo_profiles
-import pgo_tools
+from pgo_tools import benchmark_pgo_profiles
+from pgo_tools import pgo_utils
 
 
 NO_PROFILE = benchmark_pgo_profiles.SpecialProfile.NONE
@@ -61,7 +60,7 @@ def main(argv: List[str]):
     opts = parser.parse_args(argv)
     minimum_speedup = opts.minimum_speedup
 
-    pgo_tools.exit_if_not_in_chroot()
+    pgo_utils.exit_if_not_in_chroot()
 
     run_results = benchmark_pgo_profiles.run_benchmark(
         # It's likely safe to assume that a fast LLVM without ThinLTO is fast
@@ -85,7 +84,3 @@ def main(argv: List[str]):
             f"Minimum speedup of {minimum_speedup} is greater than "
             f"observed speedup of {pgo_speedup}. Exiting with error."
         )
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/pgo_tools/ensure_pgo_is_a_win_test.py b/pgo_tools/ensure_pgo_is_a_win_test.py
old mode 100755
new mode 100644
index 993094c0..1aab7580
--- a/pgo_tools/ensure_pgo_is_a_win_test.py
+++ b/pgo_tools/ensure_pgo_is_a_win_test.py
@@ -8,8 +8,8 @@
 from typing import Tuple
 import unittest
 
-import benchmark_pgo_profiles
-import ensure_pgo_is_a_win
+from pgo_tools import benchmark_pgo_profiles
+from pgo_tools import ensure_pgo_is_a_win
 
 
 def synthesize_run_data(
@@ -46,7 +46,3 @@ class Test(unittest.TestCase):
         self.assertEqual(
             ensure_pgo_is_a_win.calculate_pgo_speedup(no_profile, profile), 1.5
         )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/pgo_tools/generate_pgo_profile.py b/pgo_tools/generate_pgo_profile.py
old mode 100755
new mode 100644
index e966ad42..5a4b03fa
--- a/pgo_tools/generate_pgo_profile.py
+++ b/pgo_tools/generate_pgo_profile.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2023 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -16,6 +15,7 @@ Note that this script has a few (perhaps surprising) side-effects:
 """
 
 import argparse
+import contextlib
 import dataclasses
 import logging
 import os
@@ -26,9 +26,9 @@ import subprocess
 import sys
 import tempfile
 import textwrap
-from typing import Dict, FrozenSet, List, Optional
+from typing import Dict, Generator, List, Optional
 
-import pgo_tools
+from pgo_tools import pgo_utils
 
 
 # This script runs `quickpkg` on LLVM. This file saves the version of LLVM that
@@ -69,49 +69,80 @@ def ensure_llvm_binpkg_exists() -> bool:
         if pkg.exists():
             return False
 
-    pkg = pgo_tools.quickpkg_llvm()
+    pkg = pgo_utils.quickpkg_llvm()
     SAVED_LLVM_BINPKG_STAMP.write_text(str(pkg), encoding="utf-8")
     return True
 
 
-def restore_llvm_binpkg():
+class CrosWorkonState:
+    """Manages cros-workon state for a given host package."""
+
+    @staticmethod
+    def _is_cros_workoned(pkg: str) -> bool:
+        stdout = pgo_utils.run(
+            ["cros-workon", "--host", "list", pkg],
+            stdout=subprocess.PIPE,
+        ).stdout
+        # If `cros-workon --host list ${pkg}` printed anything other than
+        # spaces, it means that `pkg` is being worked on.
+        return bool(stdout.strip())
+
+    def __init__(self, pkg: str):
+        self.pkg = pkg
+        self.is_workoned = self._is_cros_workoned(pkg)
+
+    def set_workon(self, value: bool) -> None:
+        if self.is_workoned == value:
+            return
+
+        verb = "start" if value else "stop"
+        logging.info("%s'ing cros-workon for %s", verb, self.pkg)
+        pgo_utils.run(["cros-workon", "--host", verb, self.pkg])
+        self.is_workoned = value
+
+    @contextlib.contextmanager
+    def temporarily_set_workon(
+        self, value: bool
+    ) -> Generator[None, None, None]:
+        initial_workon_bit = self.is_workoned
+        self.set_workon(value)
+        try:
+            yield
+        finally:
+            self.set_workon(initial_workon_bit)
+
+
+def restore_llvm_binpkg() -> None:
     """Installs the binpkg created by ensure_llvm_binpkg_exists."""
     logging.info("Restoring non-PGO'ed LLVM installation")
     pkg = Path(SAVED_LLVM_BINPKG_STAMP.read_text(encoding="utf-8"))
     assert (
         pkg.exists()
     ), f"Non-PGO'ed binpkg at {pkg} does not exist. Can't restore"
-    pgo_tools.run(pgo_tools.generate_quickpkg_restoration_command(pkg))
 
-
-def find_missing_cross_libs() -> FrozenSet[str]:
-    """Returns cross-* libraries that need to be installed for workloads."""
-    equery_result = pgo_tools.run(
-        ["equery", "l", "--format=$cp", "cross-*/*"],
-        check=False,
-        stdout=subprocess.PIPE,
-    )
-
-    # If no matching package is found, equery will exit with code 3.
-    if equery_result.returncode == 3:
-        return ALL_NEEDED_CROSS_LIBS
-
-    equery_result.check_returncode()
-    has_packages = {x.strip() for x in equery_result.stdout.splitlines()}
-    return ALL_NEEDED_CROSS_LIBS - has_packages
-
-
-def ensure_cross_libs_are_installed():
-    """Ensures that we have cross-* libs for all `IMPORTANT_TRIPLES`."""
-    missing_packages = find_missing_cross_libs()
-    if not missing_packages:
-        logging.info("All cross-compiler libraries are already installed")
-        return
-
-    missing_packages = sorted(missing_packages)
-    logging.info("Installing cross-compiler libs: %s", missing_packages)
-    pgo_tools.run(
-        ["sudo", "emerge", "-j", "-G"] + missing_packages,
+    # If:
+    # - the binpkg that was built was of 9999, and we're not 9999, we need to
+    #   cros-workon, so the package is considered viable
+    # - the binpkg isn't 9999, but we've `cros-workon`'ed llvm, we need to
+    #   temporarily disable the cros-workon, since `package.mask` will block
+    #   the binpkg's installation.
+    llvm_workon_state = CrosWorkonState("sys-devel/llvm")
+    binpkg_is_cros_workon = "-9999" in pkg.name
+    with llvm_workon_state.temporarily_set_workon(binpkg_is_cros_workon):
+        pgo_utils.run(pgo_utils.generate_quickpkg_restoration_command(pkg))
+
+
+def ensure_toolchain_is_up_to_date():
+    """Ensures that all toolchain/cross-compiler packages are up-to-date."""
+    logging.info("Updating all toolchain packages to the new toolchain")
+    # Using the command from go/crostc-mage-misc#using-the-new-toolchain
+    pgo_utils.run(
+        [
+            "sudo",
+            "cros_setup_toolchains",
+            "--include-boards=amd64-generic",
+            "--nousepkg",
+        ],
     )
 
 
@@ -139,7 +170,7 @@ def emerge_pgo_generate_llvm():
     force_features = "ccache"
     features = (os.environ.get("FEATURES", "") + " " + force_features).strip()
     logging.info("Building LLVM with USE=%s", shlex.quote(use))
-    pgo_tools.run(
+    pgo_utils.run(
         [
             "sudo",
             f"FEATURES={features}",
@@ -167,7 +198,7 @@ def ensure_clang_invocations_generate_profiles(clang_bin: str, tmpdir: Path):
     """
     tmpdir = tmpdir / "ensure_profiles_generated"
     tmpdir.mkdir(parents=True)
-    pgo_tools.run(
+    pgo_utils.run(
         [clang_bin, "--help"],
         extra_env=build_profiling_env(tmpdir),
         stdout=subprocess.DEVNULL,
@@ -198,6 +229,36 @@ def write_unified_cmake_file(
     )
 
 
+def patch_absl(absl_dir: Path):
+    """Patches absl's sources."""
+    # b/366159701#comment7: the current absl has a bug in its cmake files. This
+    # is fixed by cl/675148126. Since the diff is a single line, patch it
+    # directly.
+    #
+    # If you've upgraded absl, this patching logic can probably be safely
+    # removed.
+    patch_text = textwrap.dedent(
+        """\
+        --- a/absl/base/CMakeLists.txt
+        +++ b/absl/base/CMakeLists.txt
+        @@ -534,6 +534,7 @@ absl_cc_test(
+             absl::no_destructor
+             absl::config
+             absl::raw_logging_internal
+        +    GTest::gmock
+             GTest::gtest_main
+         )
+        """
+    )
+    subprocess.run(
+        ["patch", "-p1"],
+        check=True,
+        cwd=absl_dir,
+        encoding="utf-8",
+        input=patch_text,
+    )
+
+
 def fetch_workloads_into(target_dir: Path):
     """Fetches PGO generation workloads into `target_dir`."""
     # The workload here is absl and gtest. The reasoning behind that selection
@@ -212,7 +273,7 @@ def fetch_workloads_into(target_dir: Path):
 
     def fetch_and_extract(gs_url: str, into_dir: Path):
         tgz_full = target_dir / os.path.basename(gs_url)
-        pgo_tools.run(
+        pgo_utils.run(
             [
                 "gsutil",
                 "cp",
@@ -222,7 +283,7 @@ def fetch_workloads_into(target_dir: Path):
         )
         into_dir.mkdir()
 
-        pgo_tools.run(
+        pgo_utils.run(
             ["tar", "xaf", tgz_full],
             cwd=into_dir,
         )
@@ -230,18 +291,20 @@ def fetch_workloads_into(target_dir: Path):
     absl_dir = target_dir / "absl"
     fetch_and_extract(
         gs_url="gs://chromeos-localmirror/distfiles/"
-        "abseil-cpp-a86bb8a97e38bc1361289a786410c0eb5824099c.tar.bz2",
+        "abseil-cpp-20240722.0.tar.gz",
         into_dir=absl_dir,
     )
 
+    unpacked_absl_dir = read_exactly_one_dirent(absl_dir)
+    patch_absl(unpacked_absl_dir)
+
     gtest_dir = target_dir / "gtest"
     fetch_and_extract(
         gs_url="gs://chromeos-mirror/gentoo/distfiles/"
-        "gtest-1b18723e874b256c1e39378c6774a90701d70f7a.tar.gz",
+        "gtest-1.14.0_p20220421.tar.gz",
         into_dir=gtest_dir,
     )
 
-    unpacked_absl_dir = read_exactly_one_dirent(absl_dir)
     unpacked_gtest_dir = read_exactly_one_dirent(gtest_dir)
     write_unified_cmake_file(
         into_dir=target_dir,
@@ -278,7 +341,7 @@ class WorkloadRunner:
         if sysroot:
             profiling_env["SYSROOT"] = sysroot
 
-        cmake_command: pgo_tools.Command = [
+        cmake_command: pgo_utils.Command = [
             "cmake",
             "-G",
             "Ninja",
@@ -298,13 +361,13 @@ class WorkloadRunner:
             )
 
         cmake_command.append(self.target_dir)
-        pgo_tools.run(
+        pgo_utils.run(
             cmake_command,
             extra_env=profiling_env,
             cwd=self.out_dir,
         )
 
-        pgo_tools.run(
+        pgo_utils.run(
             ["ninja", "-v", "all"],
             extra_env=profiling_env,
             cwd=self.out_dir,
@@ -370,7 +433,7 @@ def convert_profraw_to_pgo_profile(profraw_dir: Path) -> Path:
         "--instr",
         f"--output={output}",
     ]
-    pgo_tools.run(generate_command + profile_files)
+    pgo_utils.run(generate_command + profile_files)
     return output
 
 
@@ -403,7 +466,7 @@ def main(argv: List[str]):
     )
     opts = parser.parse_args(argv)
 
-    pgo_tools.exit_if_not_in_chroot()
+    pgo_utils.exit_if_not_in_chroot()
 
     output = opts.output
 
@@ -421,19 +484,21 @@ def main(argv: List[str]):
             )
         )
 
-    logging.info("Ensuring `cross-` libraries are installed")
-    ensure_cross_libs_are_installed()
+    logging.info("Installing the new toolchain...")
+    ensure_toolchain_is_up_to_date()
     tempdir = Path(tempfile.mkdtemp(prefix="generate_llvm_pgo_profile_"))
     try:
         workloads_path = tempdir / "workloads"
         logging.info("Fetching workloads")
         fetch_workloads_into(workloads_path)
 
-        # If our binpkg is not fresh, we may be operating with a weird LLVM
-        # (e.g., a PGO'ed one ;) ). Ensure we always start with that binpkg as
-        # our baseline.
-        if not llvm_binpkg_is_fresh:
-            restore_llvm_binpkg()
+        # Subtle: ensure we always start with the initial LLVM. The new one
+        # might emit a PGO profile version that the old one can't parse, which
+        # makes it impossible to build the new with the old (w/ PGO enabled).
+        #
+        # The cross-compiler libs being out of sync is fine; `sys-devel/llvm`
+        # ships with the cross-compiler libs it needs for host builds.
+        restore_llvm_binpkg()
 
         logging.info("Building PGO instrumented LLVM")
         emerge_pgo_generate_llvm()
@@ -471,7 +536,3 @@ def main(argv: List[str]):
     logging.info("Removing now-obsolete tempdir")
     shutil.rmtree(tempdir)
     logging.info("PGO profile is available at %s.", output)
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/pgo_tools/generate_pgo_profile_test.py b/pgo_tools/generate_pgo_profile_test.py
old mode 100755
new mode 100644
index 8265bf45..519dee98
--- a/pgo_tools/generate_pgo_profile_test.py
+++ b/pgo_tools/generate_pgo_profile_test.py
@@ -1,64 +1,19 @@
-#!/usr/bin/env python3
 # Copyright 2023 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
 """Tests for generate_pgo_profile."""
 
-from pathlib import Path
-import shutil
-import tempfile
-import unittest
 from unittest import mock
 
-import generate_pgo_profile
-import pgo_tools
+from llvm_tools import test_helpers
+from pgo_tools import generate_pgo_profile
+from pgo_tools import pgo_utils
 
 
-class Test(unittest.TestCase):
+class Test(test_helpers.TempDirTestCase):
     """Tests for generate_pgo_profile."""
 
-    @mock.patch.object(pgo_tools, "run")
-    def test_find_missing_cross_libs_works_for_empty_results(self, mock_run):
-        mock_run.return_value.returncode = 3
-        mock_run.return_value.stdout = ""
-        self.assertEqual(
-            generate_pgo_profile.find_missing_cross_libs(),
-            generate_pgo_profile.ALL_NEEDED_CROSS_LIBS,
-        )
-
-        mock_run.return_value.returncode = 0
-        self.assertEqual(
-            generate_pgo_profile.find_missing_cross_libs(),
-            generate_pgo_profile.ALL_NEEDED_CROSS_LIBS,
-        )
-
-    @mock.patch.object(pgo_tools, "run")
-    def test_find_missing_cross_libs_filters_results_properly(self, mock_run):
-        mock_run.return_value.returncode = 0
-        mock_run.return_value.stdout = "\n".join(
-            generate_pgo_profile.ALL_NEEDED_CROSS_LIBS
-        )
-        self.assertEqual(generate_pgo_profile.find_missing_cross_libs(), set())
-
-        some_cross_libs = sorted(generate_pgo_profile.ALL_NEEDED_CROSS_LIBS)
-        del some_cross_libs[len(some_cross_libs) // 3 :]
-        mock_run.return_value.stdout = "\n".join(
-            some_cross_libs + ["cross-foo/bar"]
-        )
-
-        expected_result = generate_pgo_profile.ALL_NEEDED_CROSS_LIBS - set(
-            some_cross_libs
-        )
-        self.assertEqual(
-            generate_pgo_profile.find_missing_cross_libs(), expected_result
-        )
-
-    def make_tempdir(self) -> Path:
-        tempdir = Path(tempfile.mkdtemp(prefix="generate_pgo_profile_test_"))
-        self.addCleanup(lambda: shutil.rmtree(tempdir))
-        return tempdir
-
     def test_read_exactly_one_dirent_works(self):
         tempdir = self.make_tempdir()
         ent = tempdir / "one-ent"
@@ -80,7 +35,7 @@ class Test(unittest.TestCase):
         with self.assertRaisesRegex(ValueError, "^Expected exactly one"):
             generate_pgo_profile.read_exactly_one_dirent(tempdir)
 
-    @mock.patch.object(pgo_tools, "run")
+    @mock.patch.object(pgo_utils, "run")
     def test_profraw_conversion_works(self, mock_run):
         tempdir = self.make_tempdir()
         profiles = [
@@ -106,7 +61,3 @@ class Test(unittest.TestCase):
             self.assertIn(p, run_cmd)
         self.assertNotIn(not_a_profile, run_cmd)
         self.assertIn(f"--output={result}", run_cmd)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/pgo_tools/pgo_tools.py b/pgo_tools/pgo_utils.py
similarity index 100%
rename from pgo_tools/pgo_tools.py
rename to pgo_tools/pgo_utils.py
diff --git a/pgo_tools/pgo_tools_test.py b/pgo_tools/pgo_utils_test.py
old mode 100755
new mode 100644
similarity index 81%
rename from pgo_tools/pgo_tools_test.py
rename to pgo_tools/pgo_utils_test.py
index ad0e853f..03efa0ae
--- a/pgo_tools/pgo_tools_test.py
+++ b/pgo_tools/pgo_utils_test.py
@@ -3,20 +3,20 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-"""Tests for pgo_tools."""
+"""Tests for pgo_utils."""
 
 from pathlib import Path
 import textwrap
 import unittest
 from unittest import mock
 
-import pgo_tools
+from pgo_tools import pgo_utils
 
 
 class Test(unittest.TestCase):
-    """Tests for pgo_tools."""
+    """Tests for pgo_utils."""
 
-    @mock.patch.object(pgo_tools, "run")
+    @mock.patch.object(pgo_utils, "run")
     def test_pgo_generate_checking_works(self, mock_run):
         equery_u_output = textwrap.dedent(
             """\
@@ -32,16 +32,16 @@ class Test(unittest.TestCase):
              """
         )
         mock_run.return_value.stdout = equery_u_output
-        self.assertTrue(pgo_tools.installed_llvm_has_pgo_generate_enabled())
+        self.assertTrue(pgo_utils.installed_llvm_has_pgo_generate_enabled())
 
         mock_run.assert_called_once()
 
         mock_run.return_value.stdout = equery_u_output.replace(
             "+ llvm_pgo_generate", "- llvm_pgo_generate"
         )
-        self.assertFalse(pgo_tools.installed_llvm_has_pgo_generate_enabled())
+        self.assertFalse(pgo_utils.installed_llvm_has_pgo_generate_enabled())
 
-    @mock.patch.object(pgo_tools, "run")
+    @mock.patch.object(pgo_utils, "run")
     def test_pgo_generate_checking_raises_on_zero_pgo_updates(self, mock_run):
         mock_run.return_value.stdout = textwrap.dedent(
             """\
@@ -52,9 +52,9 @@ class Test(unittest.TestCase):
              """
         )
         with self.assertRaisesRegex(ValueError, "^No llvm_pgo_generate"):
-            pgo_tools.installed_llvm_has_pgo_generate_enabled()
+            pgo_utils.installed_llvm_has_pgo_generate_enabled()
 
-    @mock.patch.object(pgo_tools, "run")
+    @mock.patch.object(pgo_utils, "run")
     def test_pgo_generate_checking_raises_on_many_pgo_updates(self, mock_run):
         mock_run.return_value.stdout = textwrap.dedent(
             """\
@@ -67,9 +67,9 @@ class Test(unittest.TestCase):
              """
         )
         with self.assertRaisesRegex(ValueError, "^Multiple llvm_pgo_generate"):
-            pgo_tools.installed_llvm_has_pgo_generate_enabled()
+            pgo_utils.installed_llvm_has_pgo_generate_enabled()
 
-    @mock.patch.object(pgo_tools, "run")
+    @mock.patch.object(pgo_utils, "run")
     def test_pgo_generate_ignores_nonexact_use_flags(self, mock_run):
         mock_run.return_value.stdout = textwrap.dedent(
             """\
@@ -82,27 +82,23 @@ class Test(unittest.TestCase):
              + - llvm_pgo_use                   : <unknown>
              """
         )
-        self.assertTrue(pgo_tools.installed_llvm_has_pgo_generate_enabled())
+        self.assertTrue(pgo_utils.installed_llvm_has_pgo_generate_enabled())
 
     def test_quickpkg_restoration_works(self):
         self.assertEqual(
-            pgo_tools.generate_quickpkg_restoration_command(
+            pgo_utils.generate_quickpkg_restoration_command(
                 Path("/path/to/sys-devel/llvm-1234-r1.tbz2")
             ),
             ["sudo", "emerge", "--usepkgonly", "=sys-devel/llvm-1234-r1"],
         )
 
     def test_temporary_file_creation_works(self):
-        with pgo_tools.temporary_file("foo_bar_") as tmp:
+        with pgo_utils.temporary_file("foo_bar_") as tmp:
             self.assertTrue(tmp.name.startswith("foo_bar_"), tmp.name)
             self.assertTrue(tmp.exists())
         self.assertFalse(tmp.exists())
 
     def test_temporary_file_deletion_is_fine_if_file_does_not_exist(self):
         # This test ensures this `with`'s `__exit__` block doesn't `raise`.
-        with pgo_tools.temporary_file("foo_bar_") as tmp:
+        with pgo_utils.temporary_file("foo_bar_") as tmp:
             tmp.unlink()
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/pgo_tools/update_llvm_manifest.py b/pgo_tools/update_llvm_manifest.py
deleted file mode 100755
index 7fff6f8d..00000000
--- a/pgo_tools/update_llvm_manifest.py
+++ /dev/null
@@ -1,105 +0,0 @@
-#!/usr/bin/env python3
-# Copyright 2024 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Updates the Manifest file for LLVM.
-
-Often used to pull a new PGO profile in.
-"""
-
-import argparse
-import contextlib
-import logging
-from pathlib import Path
-import re
-import subprocess
-import sys
-from typing import Generator, List
-
-import pgo_tools
-
-
-@contextlib.contextmanager
-def temporarily_add_llvm_next_pgo_to_src_uri(
-    llvm_9999_ebuild: Path,
-) -> Generator[None, None, None]:
-    old_contents = llvm_9999_ebuild.read_text(encoding="utf-8")
-
-    profdata_prefix = "gs://chromeos-localmirror/distfiles/llvm-profdata-"
-    profdata_re = re.compile(
-        # Leave room for a suffix on this, in case we're on the Nth version of
-        # llvm-profdata for some reason.
-        re.escape(profdata_prefix + "${LLVM_HASH}")
-        + r"\S*\.xz\s"
-    )
-    found_urls = list(profdata_re.finditer(old_contents))
-    if len(found_urls) != 1:
-        raise ValueError(
-            f"Want 1 instance of {profdata_re} in {llvm_9999_ebuild}; found "
-            f"{len(found_urls)}"
-        )
-
-    # Insert the new profdata URL right after the old one. The combination of
-    # USE variables gating this file doesn't have to make sense; the URL just
-    # has to be visible to Portage.
-
-    # Note that the regex ended with `\s`, so `.end()` will be after a space.
-    insert_url = profdata_prefix + "${LLVM_NEXT_HASH}.xz "
-    insert_point = found_urls[0].end()
-    new_contents = (
-        old_contents[:insert_point] + insert_url + old_contents[insert_point:]
-    )
-
-    llvm_9999_ebuild.write_text(new_contents, encoding="utf-8")
-    try:
-        yield
-    finally:
-        llvm_9999_ebuild.write_text(old_contents, encoding="utf-8")
-
-
-def update_manifest(llvm_9999_ebuild: Path):
-    subprocess.run(
-        ["ebuild", llvm_9999_ebuild, "manifest"],
-        check=True,
-        stdin=subprocess.DEVNULL,
-    )
-
-
-def main(argv: List[str]) -> None:
-    logging.basicConfig(
-        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
-        "%(message)s",
-        level=logging.INFO,
-    )
-
-    pgo_tools.exit_if_not_in_chroot()
-
-    my_dir = Path(__file__).resolve().parent
-    parser = argparse.ArgumentParser(
-        description=__doc__,
-        formatter_class=argparse.RawDescriptionHelpFormatter,
-    )
-    parser.add_argument(
-        "--llvm-next",
-        action="store_true",
-        help="Also update for the llvm-next PGO profile.",
-    )
-    parser.add_argument(
-        "--chromiumos-overlay",
-        default=my_dir.parent.parent / "chromiumos-overlay",
-        type=Path,
-        help="The chromiumos-overlay directory to work in. Default %(default)s",
-    )
-    opts = parser.parse_args(argv)
-
-    llvm_9999 = opts.chromiumos_overlay / "sys-devel/llvm/llvm-9999.ebuild"
-    if opts.llvm_next:
-        with temporarily_add_llvm_next_pgo_to_src_uri(llvm_9999):
-            update_manifest(llvm_9999)
-    else:
-        update_manifest(llvm_9999)
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/pgo_tools_rust/pgo_rust.py b/pgo_tools_rust/pgo_rust.py
old mode 100755
new mode 100644
index 7d9e4f7b..3357ca9e
--- a/pgo_tools_rust/pgo_rust.py
+++ b/pgo_tools_rust/pgo_rust.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2022 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -114,6 +113,8 @@ import subprocess
 import sys
 from typing import cast, List, Mapping, Optional
 
+from cros_utils import cros_paths
+
 
 TARGET_TRIPLES = [
     "x86_64-cros-linux-gnu",
@@ -466,9 +467,10 @@ def benchmark_pgo(args):
         crate_name=args.bench_crate_name, crate_version=args.bench_crate_version
     )
 
-    files_dir = Path(
-        "/mnt/host/source/src/third_party/chromiumos-overlay",
-        "dev-lang/rust/files",
+    files_dir = (
+        cros_paths.CHROOT_SOURCE_ROOT
+        / cros_paths.CHROMIUMOS_OVERLAY
+        / "dev-lang/rust/files"
     )
 
     logging.info("Copying profile data to be used in building Rust")
@@ -561,7 +563,7 @@ def main(argv: List[str]) -> int:
     )
 
     parser = argparse.ArgumentParser(
-        prog=argv[0],
+        prog=sys.argv[0],
         description=__doc__,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
@@ -663,7 +665,7 @@ def main(argv: List[str]) -> int:
         "rustc versions",
     )
 
-    args = parser.parse_args(argv[1:])
+    args = parser.parse_args(argv)
 
     (LOCAL_BASE / "crates").mkdir(parents=True, exist_ok=True)
     (LOCAL_BASE / "llvm-profraw").mkdir(parents=True, exist_ok=True)
@@ -673,7 +675,3 @@ def main(argv: List[str]) -> int:
     args.func(args)
 
     return 0
-
-
-if __name__ == "__main__":
-    sys.exit(main(sys.argv))
diff --git a/ping_automatic_cls.py b/ping_automatic_cls.py
new file mode 100644
index 00000000..504721bd
--- /dev/null
+++ b/ping_automatic_cls.py
@@ -0,0 +1,88 @@
+# Copyright 2025 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Pings an email address about any autogenerated CLs that need review."""
+
+import argparse
+import logging
+from pathlib import Path
+from typing import List
+
+from cros_utils import cros_paths
+from cros_utils import email_sender
+from cros_utils import git_utils
+
+
+def count_outstanding_cls(chromeos_root: Path) -> int:
+    query = " ".join(
+        (
+            "owner:mobiletc-prebuild@google.com",
+            "topic:crostc-auto-cl",
+            "status:open",
+            "-is:wip",
+            "-is:submittable",
+        )
+    )
+    cls = git_utils.query_gerrit(
+        chromeos_root,
+        query,
+    )
+    logging.info("CLs matching query %r: %s", query, cls)
+    return len(cls)
+
+
+def main(argv: List[str]) -> None:
+    # This is meant to be run from a ChromeOS tree for simplicity.
+    chromeos_root = cros_paths.script_chromiumos_checkout_or_exit()
+
+    logging.basicConfig(
+        format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
+        "%(message)s",
+        level=logging.INFO,
+    )
+
+    parser = argparse.ArgumentParser(
+        description=__doc__,
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+    )
+    parser.add_argument(
+        "--email",
+        help="Email to send ping to",
+    )
+    parser.add_argument(
+        "-n",
+        "--dry-run",
+        action="store_true",
+        help="Don't actually send an email",
+    )
+    opts = parser.parse_args(argv)
+
+    if not (opts.email or opts.dry_run):
+        parser.error("You must pass either --dry-run or --email")
+
+    logging.info("Searching for CLs that need a review...")
+    num_outstanding_cls = count_outstanding_cls(chromeos_root)
+    if not num_outstanding_cls:
+        logging.info("No outstanding CLs found; exit")
+        return
+
+    logging.info(
+        "Sending email notification of %d outstanding CLs", num_outstanding_cls
+    )
+
+    plural = "" if num_outstanding_cls == 1 else "s"
+    subject = f"{num_outstanding_cls} autogenerated CL{plural} need review"
+    body = "Please see http://go/crostc-unreviewed-auto-cls"
+    if opts.dry_run:
+        logging.info("--dry-run specified; skipping sending email")
+        logging.info("Subject: %s", subject)
+        logging.info("Body: %s", body)
+        return
+
+    email_sender.EmailSender().SendX20Email(
+        subject=subject,
+        identifier="cl-pings",
+        text_body=body,
+        direct_recipients=(opts.email,),
+    )
diff --git a/py/bin/afdo_redaction/redact_profile.py b/py/bin/afdo_redaction/redact_profile.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/afdo_redaction/redact_profile.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/afdo_redaction/remove_cold_functions.py b/py/bin/afdo_redaction/remove_cold_functions.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/afdo_redaction/remove_cold_functions.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/afdo_redaction/remove_indirect_calls.py b/py/bin/afdo_redaction/remove_indirect_calls.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/afdo_redaction/remove_indirect_calls.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/afdo_tools/bisection/afdo_prof_analysis.py b/py/bin/afdo_tools/bisection/afdo_prof_analysis.py
new file mode 120000
index 00000000..df619e68
--- /dev/null
+++ b/py/bin/afdo_tools/bisection/afdo_prof_analysis.py
@@ -0,0 +1 @@
+../../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/afdo_tools/generate_afdo_from_tryjob.py b/py/bin/afdo_tools/generate_afdo_from_tryjob.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/afdo_tools/generate_afdo_from_tryjob.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/afdo_tools/monitor_chrome_afdo.py b/py/bin/afdo_tools/monitor_chrome_afdo.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/afdo_tools/monitor_chrome_afdo.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/afdo_tools/run_afdo_tryjob.py b/py/bin/afdo_tools/run_afdo_tryjob.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/afdo_tools/run_afdo_tryjob.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/afdo_tools/update_kernel_afdo.py b/py/bin/afdo_tools/update_kernel_afdo.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/afdo_tools/update_kernel_afdo.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/auto_abandon_cls.py b/py/bin/auto_abandon_cls.py
new file mode 120000
index 00000000..63d45c29
--- /dev/null
+++ b/py/bin/auto_abandon_cls.py
@@ -0,0 +1 @@
+../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/bot_tools/fetch_all_subtest_logs.py b/py/bin/bot_tools/fetch_all_subtest_logs.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/bot_tools/fetch_all_subtest_logs.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/check_portable_toolchains.py b/py/bin/check_portable_toolchains.py
new file mode 120000
index 00000000..63d45c29
--- /dev/null
+++ b/py/bin/check_portable_toolchains.py
@@ -0,0 +1 @@
+../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/crate_ebuild_help.py b/py/bin/crate_ebuild_help.py
new file mode 120000
index 00000000..63d45c29
--- /dev/null
+++ b/py/bin/crate_ebuild_help.py
@@ -0,0 +1 @@
+../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/cros_utils/command_executer_timeout_test.py b/py/bin/cros_utils/command_executer_timeout_test.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/cros_utils/command_executer_timeout_test.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/cros_utils/perf_diff.py b/py/bin/cros_utils/perf_diff.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/cros_utils/perf_diff.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/cwp/cr_os/fetch_gn_descs.py b/py/bin/cwp/cr_os/fetch_gn_descs.py
new file mode 120000
index 00000000..df619e68
--- /dev/null
+++ b/py/bin/cwp/cr_os/fetch_gn_descs.py
@@ -0,0 +1 @@
+../../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/debug_info_test/debug_info_test.py b/py/bin/debug_info_test/debug_info_test.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/debug_info_test/debug_info_test.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/go/chromeos/setup_chromeos_testing.py b/py/bin/go/chromeos/setup_chromeos_testing.py
new file mode 120000
index 00000000..df619e68
--- /dev/null
+++ b/py/bin/go/chromeos/setup_chromeos_testing.py
@@ -0,0 +1 @@
+../../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_extra/create_ebuild_file.py b/py/bin/llvm_extra/create_ebuild_file.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_extra/create_ebuild_file.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/bb_add.py b/py/bin/llvm_tools/bb_add.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/bb_add.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/check_clang_diags.py b/py/bin/llvm_tools/check_clang_diags.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/check_clang_diags.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/clean_up_old_llvm_patches.py b/py/bin/llvm_tools/clean_up_old_llvm_patches.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/clean_up_old_llvm_patches.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/copy_helpers_to_chromiumos_overlay.py b/py/bin/llvm_tools/copy_helpers_to_chromiumos_overlay.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/copy_helpers_to_chromiumos_overlay.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/create_patch_file.py b/py/bin/llvm_tools/create_patch_file.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/create_patch_file.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/fetch_cq_size_diff.py b/py/bin/llvm_tools/fetch_cq_size_diff.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/fetch_cq_size_diff.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/fetch_cros_sdk_rolls.py b/py/bin/llvm_tools/fetch_cros_sdk_rolls.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/fetch_cros_sdk_rolls.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/generate_llvm_revert_report.py b/py/bin/llvm_tools/generate_llvm_revert_report.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/generate_llvm_revert_report.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/generate_warning_exemption_files.py b/py/bin/llvm_tools/generate_warning_exemption_files.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/generate_warning_exemption_files.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/get_llvm_hash.py b/py/bin/llvm_tools/get_llvm_hash.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/get_llvm_hash.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/get_patch.py b/py/bin/llvm_tools/get_patch.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/get_patch.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/git_llvm_rev.py b/py/bin/llvm_tools/git_llvm_rev.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/git_llvm_rev.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/lint_llvm_patches.py b/py/bin/llvm_tools/lint_llvm_patches.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/lint_llvm_patches.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/llvm_next_py_autoupdate.py b/py/bin/llvm_tools/llvm_next_py_autoupdate.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/llvm_next_py_autoupdate.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/llvm_simple_bisect.py b/py/bin/llvm_tools/llvm_simple_bisect.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/llvm_simple_bisect.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/nightly_revert_checker.py b/py/bin/llvm_tools/nightly_revert_checker.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/nightly_revert_checker.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/patch_manager.py b/py/bin/llvm_tools/patch_manager.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/patch_manager.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/ready_llvm_branch.py b/py/bin/llvm_tools/ready_llvm_branch.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/ready_llvm_branch.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/revert_checker.py b/py/bin/llvm_tools/revert_checker.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/revert_checker.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/stabilize_all_llvm_packages.py b/py/bin/llvm_tools/stabilize_all_llvm_packages.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/stabilize_all_llvm_packages.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/update_packages_and_run_tests.py b/py/bin/llvm_tools/update_packages_and_run_tests.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/update_packages_and_run_tests.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/upload_llvm_testing_helper_cl.py b/py/bin/llvm_tools/upload_llvm_testing_helper_cl.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/upload_llvm_testing_helper_cl.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/verify_patch_consistency.py b/py/bin/llvm_tools/verify_patch_consistency.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/verify_patch_consistency.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/llvm_tools/werror_logs.py b/py/bin/llvm_tools/werror_logs.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/llvm_tools/werror_logs.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/make_root_writable.py b/py/bin/make_root_writable.py
new file mode 120000
index 00000000..63d45c29
--- /dev/null
+++ b/py/bin/make_root_writable.py
@@ -0,0 +1 @@
+../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/pgo_tools/auto_update_llvm_pgo_profile.py b/py/bin/pgo_tools/auto_update_llvm_pgo_profile.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/pgo_tools/auto_update_llvm_pgo_profile.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/pgo_tools/benchmark_pgo_profiles.py b/py/bin/pgo_tools/benchmark_pgo_profiles.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/pgo_tools/benchmark_pgo_profiles.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/pgo_tools/create_chroot_and_generate_pgo_profile.py b/py/bin/pgo_tools/create_chroot_and_generate_pgo_profile.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/pgo_tools/create_chroot_and_generate_pgo_profile.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/pgo_tools/ensure_pgo_is_a_win.py b/py/bin/pgo_tools/ensure_pgo_is_a_win.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/pgo_tools/ensure_pgo_is_a_win.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/pgo_tools/generate_pgo_profile.py b/py/bin/pgo_tools/generate_pgo_profile.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/pgo_tools/generate_pgo_profile.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/pgo_tools_rust/pgo_rust.py b/py/bin/pgo_tools_rust/pgo_rust.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/pgo_tools_rust/pgo_rust.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/ping_automatic_cls.py b/py/bin/ping_automatic_cls.py
new file mode 120000
index 00000000..63d45c29
--- /dev/null
+++ b/py/bin/ping_automatic_cls.py
@@ -0,0 +1 @@
+../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/run_tests_for.py b/py/bin/run_tests_for.py
new file mode 120000
index 00000000..63d45c29
--- /dev/null
+++ b/py/bin/run_tests_for.py
@@ -0,0 +1 @@
+../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/rust_tools/auto_update_rust_bootstrap.py b/py/bin/rust_tools/auto_update_rust_bootstrap.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/rust_tools/auto_update_rust_bootstrap.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/rust_tools/copy_rust_bootstrap.py b/py/bin/rust_tools/copy_rust_bootstrap.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/rust_tools/copy_rust_bootstrap.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/rust_tools/rust_uprev.py b/py/bin/rust_tools/rust_uprev.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/rust_tools/rust_uprev.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/rust_tools/rust_watch.py b/py/bin/rust_tools/rust_watch.py
new file mode 120000
index 00000000..0f1ca492
--- /dev/null
+++ b/py/bin/rust_tools/rust_watch.py
@@ -0,0 +1 @@
+../../../python_wrapper.py
\ No newline at end of file
diff --git a/py/bin/seccomp_tools/mass_seccomp_editor/mass_seccomp_editor.py b/py/bin/seccomp_tools/mass_seccomp_editor/mass_seccomp_editor.py
new file mode 120000
index 00000000..df619e68
--- /dev/null
+++ b/py/bin/seccomp_tools/mass_seccomp_editor/mass_seccomp_editor.py
@@ -0,0 +1 @@
+../../../../python_wrapper.py
\ No newline at end of file
diff --git a/pyrightconfig.json b/pyrightconfig.json
new file mode 100644
index 00000000..a3ae0f1f
--- /dev/null
+++ b/pyrightconfig.json
@@ -0,0 +1,7 @@
+{
+  "pythonVersion": "3.11",
+  "exclude": [
+    "py/",
+    "llvm_tools/llvm-project-copy/"
+  ]
+}
diff --git a/python_wrapper.py b/python_wrapper.py
new file mode 100755
index 00000000..5e33eb9b
--- /dev/null
+++ b/python_wrapper.py
@@ -0,0 +1,92 @@
+#!/usr/bin/env python3
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+"""Wrapper for Python scripts in toolchain-utils.
+
+Python scripts here assume that they can import arbitrary modules. This is only
+consistently possible if the root of toolchain-utils is on PYTHONPATH. The
+simplest way to make that happen is to wrap the scripts.
+
+py/bin/foo.py will be invoked _similarly to_ `PYTHONPATH=. ./foo.py`. If the
+script has a `main` or `_main` function defined, it will be called with a
+single argument: `sys.argv[1:]`.
+"""
+
+import importlib.util
+import inspect
+import os
+from pathlib import Path
+import sys
+from typing import Any, Callable
+
+
+def find_file_to_execute(argv0: str) -> Path:
+    symlink_path = Path(os.getcwd(), argv0)
+    symlink_parent = symlink_path.parent.resolve()
+    me = (symlink_parent / symlink_path.name).resolve()
+    toolchain_utils = me.parent
+    relative_script_path = (
+        symlink_parent.relative_to(toolchain_utils) / symlink_path.name
+    )
+    prefix = "py/bin/"
+    relative_script_path_str = str(relative_script_path)
+    if not relative_script_path_str.startswith(prefix):
+        raise ValueError(
+            f"Expected argv0 to be in {prefix} - it's {relative_script_path}"
+        )
+    target_script = relative_script_path_str[len(prefix) :]
+    result = toolchain_utils / target_script
+    if not result.exists():
+        sys.exit(f"No script found at {target_script} - can't execute")
+    return result
+
+
+def main():
+    main_file = find_file_to_execute(sys.argv[0])
+    module_name = main_file.with_suffix("").name
+    spec = importlib.util.spec_from_file_location(
+        module_name,
+        main_file,
+    )
+    if not spec:
+        raise ValueError(f"Could not retrieve spec from module {module_name}")
+    main_module = importlib.util.module_from_spec(spec)
+    sys.modules[module_name] = main_module
+    if not spec.loader:
+        raise ValueError(f"Spec for {module_name} does not have a loader")
+    spec.loader.exec_module(main_module)
+
+    # We have various `main` conventions to support here, unfortunately:
+    # - Some return None; others return an exit code.
+    # - Some take argv; others take no args.
+    # - Some are called `main`, others are called `_main`.
+    # - Some capitalize `Main`, others don't.
+    # It'd be nice to make this more uniform, but it's easy enough to handle
+    # all of these until that happens.
+    main_fns = (
+        "main",
+        "Main",
+        "_main",
+        "_Main",
+    )
+    main_fn: Callable[..., Any]
+    for f in main_fns:
+        if my_fn := getattr(main_module, f, None):
+            main_fn = my_fn
+            break
+    else:
+        sys.exit(
+            f"No function called any of {main_fns} declared in {main_file}."
+        )
+    if inspect.signature(main_fn).parameters:
+        result = main_fn(sys.argv[1:])
+    else:
+        result = main_fn()
+    if result:
+        sys.exit(result)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/remote_test.py b/remote_test.py
deleted file mode 100755
index 01f3fe89..00000000
--- a/remote_test.py
+++ /dev/null
@@ -1,114 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Script to wrap test_that script.
-
-This script can login to the chromeos machine using the test private key.
-"""
-
-
-__author__ = "asharif@google.com (Ahmad Sharif)"
-
-import argparse
-import os
-import sys
-
-from cros_utils import command_executer
-from cros_utils import misc
-
-
-def Usage(parser, message):
-    print("ERROR: %s" % message)
-    parser.print_help()
-    sys.exit(0)
-
-
-def Main(argv):
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "-c",
-        "--chromeos_root",
-        dest="chromeos_root",
-        help="ChromeOS root checkout directory",
-    )
-    parser.add_argument(
-        "-r", "--remote", dest="remote", help="Remote chromeos device."
-    )
-    options = parser.parse_args(argv)
-    if options.chromeos_root is None:
-        Usage(parser, "chromeos_root must be given")
-
-    if options.remote is None:
-        Usage(parser, "remote must be given")
-
-    options.chromeos_root = os.path.expanduser(options.chromeos_root)
-
-    command = "ls -lt /"
-    ce = command_executer.GetCommandExecuter()
-    ce.CrosRunCommand(
-        command, chromeos_root=options.chromeos_root, machine=options.remote
-    )
-
-    version_dir_path, script_name = misc.GetRoot(sys.argv[0])
-    version_dir = misc.GetRoot(version_dir_path)[1]
-
-    # Tests to copy directories and files to the chromeos box.
-    ce.CopyFiles(
-        version_dir_path,
-        "/tmp/" + version_dir,
-        dest_machine=options.remote,
-        dest_cros=True,
-        chromeos_root=options.chromeos_root,
-    )
-    ce.CopyFiles(
-        version_dir_path,
-        "/tmp/" + version_dir + "1",
-        dest_machine=options.remote,
-        dest_cros=True,
-        chromeos_root=options.chromeos_root,
-    )
-    ce.CopyFiles(
-        sys.argv[0],
-        "/tmp/" + script_name,
-        recursive=False,
-        dest_machine=options.remote,
-        dest_cros=True,
-        chromeos_root=options.chromeos_root,
-    )
-    ce.CopyFiles(
-        sys.argv[0],
-        "/tmp/" + script_name + "1",
-        recursive=False,
-        dest_machine=options.remote,
-        dest_cros=True,
-        chromeos_root=options.chromeos_root,
-    )
-
-    # Test to copy directories and files from the chromeos box.
-    ce.CopyFiles(
-        "/tmp/" + script_name,
-        "/tmp/hello",
-        recursive=False,
-        src_machine=options.remote,
-        src_cros=True,
-        chromeos_root=options.chromeos_root,
-    )
-    ce.CopyFiles(
-        "/tmp/" + script_name,
-        "/tmp/" + script_name,
-        recursive=False,
-        src_machine=options.remote,
-        src_cros=True,
-        chromeos_root=options.chromeos_root,
-    )
-    board = ce.CrosLearnBoard(options.chromeos_root, options.remote)
-    print(board)
-    return 0
-
-
-if __name__ == "__main__":
-    Main(sys.argv[1:])
diff --git a/run_python_tests.sh b/run_python_tests.sh
new file mode 100755
index 00000000..fd4384f8
--- /dev/null
+++ b/run_python_tests.sh
@@ -0,0 +1,40 @@
+#!/bin/bash -eu
+# Copyright 2024 The ChromiumOS Authors
+# Use of this source code is governed by a BSD-style license that can be
+# found in the LICENSE file.
+
+my_dir="$(dirname "$(readlink -m "$0")")"
+cd "${my_dir}"
+
+if [[ ! -e /etc/cros_chroot_version ]]; then
+  echo "Re-exec'ing within the chroot..." >&2
+  exec cros_sdk \
+    --working-dir=. \
+    -- \
+    './run_python_tests.sh' \
+    "$@"
+fi
+
+# Note that PYTHONPATH is not necessary here for regular execution, but a
+# single script to run is specified, `pytest` will invoke it in a way that
+# breaks toolchain-utils module importing.
+#
+# `-p no:hypothesispytest` is added because some chroots come with a
+# 'hypothesis' pytest extension, which allows for property testing. We
+# don't use that, and it autocreates a `.hypothesis/` subdir, which is
+# slightly annoying.
+#
+# Exemptions:
+# - git_llvm_rev_test is ignored because it takes a while. It'd be nice to
+#   optionally enable it, but it's realistically very unlikely to break.
+# - debug_info_test/debug_info_test.py is ignored, since that's the name of
+#   a non-test script, and pytest is confused by this.
+# - py/ just has symlinks to stuff back in toolchain-utils; no point in
+#   checking that.
+PYTHONPATH="${PWD}:${PYTHONPATH:-}" pytest \
+  -p no:hypothesispytest \
+  --ignore=debug_info_test/debug_info_test.py \
+  --ignore=llvm_tools/llvm-project-copy \
+  --ignore=llvm_tools/git_llvm_rev_test.py \
+  --ignore=py/ \
+  "$@"
diff --git a/run_tests_for.py b/run_tests_for.py
old mode 100755
new mode 100644
index ef3b669e..616308da
--- a/run_tests_for.py
+++ b/run_tests_for.py
@@ -1,18 +1,9 @@
-#!/usr/bin/env python3
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
 """Runs tests for the given input files.
 
-Tries its best to autodetect all tests based on path name without being *too*
-aggressive.
-
-In short, there's a small set of directories in which, if you make any change,
-all of the tests in those directories get run. Additionally, if you change a
-python file named foo, it'll run foo_test.py or foo_unittest.py if either of
-those exist.
-
 All tests are run in parallel.
 """
 
@@ -36,12 +27,6 @@ from typing import Optional, Tuple
 
 TestSpec = collections.namedtuple("TestSpec", ["directory", "command"])
 
-# List of python scripts that are not test with relative path to
-# toolchain-utils.
-non_test_py_files = {
-    "debug_info_test/debug_info_test.py",
-}
-
 
 def _make_relative_to_toolchain_utils(toolchain_utils, path):
     """Cleans & makes a path relative to toolchain_utils.
@@ -57,29 +42,6 @@ def _make_relative_to_toolchain_utils(toolchain_utils, path):
     return result
 
 
-def _filter_python_tests(test_files, toolchain_utils):
-    """Returns all files that are real python tests."""
-    python_tests = []
-    for test_file in test_files:
-        rel_path = _make_relative_to_toolchain_utils(toolchain_utils, test_file)
-        if rel_path not in non_test_py_files:
-            python_tests.append(_python_test_to_spec(test_file))
-        else:
-            print("## %s ... NON_TEST_PY_FILE" % rel_path)
-    return python_tests
-
-
-def _gather_python_tests_in(rel_subdir, toolchain_utils):
-    """Returns all files that appear to be Python tests in a given directory."""
-    subdir = os.path.join(toolchain_utils, rel_subdir)
-    test_files = (
-        os.path.join(subdir, file_name)
-        for file_name in os.listdir(subdir)
-        if file_name.endswith("_test.py") or file_name.endswith("_unittest.py")
-    )
-    return _filter_python_tests(test_files, toolchain_utils)
-
-
 def _run_test(test_spec: TestSpec, timeout: int) -> Tuple[Optional[int], str]:
     """Runs a test.
 
@@ -127,49 +89,6 @@ def _run_test(test_spec: TestSpec, timeout: int) -> Tuple[Optional[int], str]:
             raise
 
 
-def _python_test_to_spec(test_file):
-    """Given a .py file, convert it to a TestSpec."""
-    # Run tests in the directory they exist in, since some of them are sensitive
-    # to that.
-    test_directory = os.path.dirname(os.path.abspath(test_file))
-    file_name = os.path.basename(test_file)
-
-    if os.access(test_file, os.X_OK):
-        command = ["./" + file_name]
-    else:
-        # Assume the user wanted py3.
-        command = ["python3", file_name]
-
-    return TestSpec(directory=test_directory, command=command)
-
-
-def _autodetect_python_tests_for(test_file, toolchain_utils):
-    """Given a test file, detect if there may be related tests."""
-    if not test_file.endswith(".py"):
-        return []
-
-    test_prefixes = ("test_", "unittest_")
-    test_suffixes = ("_test.py", "_unittest.py")
-
-    test_file_name = os.path.basename(test_file)
-    test_file_is_a_test = any(
-        test_file_name.startswith(x) for x in test_prefixes
-    ) or any(test_file_name.endswith(x) for x in test_suffixes)
-
-    if test_file_is_a_test:
-        test_files = [test_file]
-    else:
-        test_file_no_suffix = test_file[:-3]
-        candidates = [test_file_no_suffix + x for x in test_suffixes]
-
-        dir_name = os.path.dirname(test_file)
-        candidates += (
-            os.path.join(dir_name, x + test_file_name) for x in test_prefixes
-        )
-        test_files = (x for x in candidates if os.path.exists(x))
-    return _filter_python_tests(test_files, toolchain_utils)
-
-
 def _run_test_scripts(pool, all_tests, timeout, show_successful_output=False):
     """Runs a list of TestSpecs. Returns whether all of them succeeded."""
     results = [
@@ -235,41 +154,6 @@ def _compress_list(l):
     return result
 
 
-def _fix_python_path(toolchain_utils):
-    pypath = os.environ.get("PYTHONPATH", "")
-    if pypath:
-        pypath = ":" + pypath
-    os.environ["PYTHONPATH"] = toolchain_utils + pypath
-
-
-def _find_forced_subdir_python_tests(test_paths, toolchain_utils):
-    assert all(os.path.isabs(path) for path in test_paths)
-
-    # Directories under toolchain_utils for which any change will cause all
-    # tests in that directory to be rerun. Includes changes in subdirectories.
-    all_dirs = {
-        "crosperf",
-        "cros_utils",
-    }
-
-    relative_paths = [
-        _make_relative_to_toolchain_utils(toolchain_utils, path)
-        for path in test_paths
-    ]
-
-    gather_test_dirs = set()
-
-    for path in relative_paths:
-        top_level_dir = path.split("/")[0]
-        if top_level_dir in all_dirs:
-            gather_test_dirs.add(top_level_dir)
-
-    results = []
-    for d in sorted(gather_test_dirs):
-        results += _gather_python_tests_in(d, toolchain_utils)
-    return results
-
-
 def _find_go_tests(test_paths):
     """Returns TestSpecs for the go folders of the given files"""
     assert all(os.path.isabs(path) for path in test_paths)
@@ -277,7 +161,7 @@ def _find_go_tests(test_paths):
     dirs_with_gofiles = set(
         os.path.dirname(p) for p in test_paths if p.endswith(".go")
     )
-    command = ["go", "test", "-vet=all"]
+    command = ("go", "test", "-vet=all")
     # Note: We sort the directories to be deterministic.
     return [
         TestSpec(directory=d, command=command)
@@ -319,26 +203,23 @@ def main(argv):
         print("No files given. Exit.")
         return 0
 
-    _fix_python_path(toolchain_utils)
+    tests_to_run = []
+    if any(x.endswith(".py") for x in modified_files):
+        tests_to_run.append(
+            TestSpec(
+                directory=toolchain_utils,
+                command=("./run_python_tests.sh",),
+            )
+        )
 
-    tests_to_run = _find_forced_subdir_python_tests(
-        modified_files, toolchain_utils
-    )
-    for f in modified_files:
-        tests_to_run += _autodetect_python_tests_for(f, toolchain_utils)
     tests_to_run += _find_go_tests(modified_files)
 
     # TestSpecs have lists, so we can't use a set. We'd likely want to keep them
     # sorted for determinism anyway.
-    tests_to_run.sort()
-    tests_to_run = _compress_list(tests_to_run)
+    tests_to_run = sorted(set(tests_to_run))
 
     with multiprocessing.pool.ThreadPool() as pool:
         success = _run_test_scripts(
             pool, tests_to_run, args.timeout, show_all_output
         )
     return 0 if success else 1
-
-
-if __name__ == "__main__":
-    sys.exit(main(sys.argv[1:]))
diff --git a/rust_tools/auto_update_rust_bootstrap.py b/rust_tools/auto_update_rust_bootstrap.py
old mode 100755
new mode 100644
index c41fcd03..e30cf5ca
--- a/rust_tools/auto_update_rust_bootstrap.py
+++ b/rust_tools/auto_update_rust_bootstrap.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2023 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -27,18 +26,22 @@ import re
 import subprocess
 import sys
 import textwrap
-from typing import Dict, Iterable, List, Optional, Tuple, Union
+from typing import Callable, Dict, Iterable, List, Optional, Tuple, Union
 
-import copy_rust_bootstrap
+from cros_utils import cros_paths
+from cros_utils import git_utils
+from rust_tools import copy_rust_bootstrap
 
 
 # The bug to tag in all commit messages.
 TRACKING_BUG = "b:315473495"
 
-# Reviewers for all CLs uploaded.
-DEFAULT_CL_REVIEWERS = (
-    "gbiv@chromium.org",
-    "inglorion@chromium.org",
+# These match variable assignments in the rust-bootstrap ebuilds.
+RUST_BOOTSTRAP_USE_PREBUILTS_REGEX = re.compile(
+    r"^(THIS_VERSION_HAS_PREBUILT=)(.*)$", re.MULTILINE
+)
+RUST_BOOTSTRAP_PRIOR_VERSION_REGEX = re.compile(
+    r'^(PRIOR_RUST_BOOTSTRAP_VERSION=")([^"]*)(")', re.MULTILINE
 )
 
 
@@ -81,53 +84,6 @@ class EbuildVersion:
         return result
 
 
-def find_raw_bootstrap_sequence_lines(
-    ebuild_lines: List[str],
-) -> Tuple[int, int]:
-    """Returns the start/end lines of RUSTC_RAW_FULL_BOOTSTRAP_SEQUENCE."""
-    for i, line in enumerate(ebuild_lines):
-        if line.startswith("RUSTC_RAW_FULL_BOOTSTRAP_SEQUENCE=("):
-            start = i
-            break
-    else:
-        raise ValueError("No bootstrap sequence start found in text")
-
-    for i, line in enumerate(ebuild_lines[i + 1 :], i + 1):
-        if line.rstrip() == ")":
-            return start, i
-    raise ValueError("No bootstrap sequence end found in text")
-
-
-def read_bootstrap_sequence_from_ebuild(
-    rust_bootstrap_ebuild: Path,
-) -> List[EbuildVersion]:
-    """Returns a list of EbuildVersions from the given ebuild."""
-    ebuild_lines = rust_bootstrap_ebuild.read_text(
-        encoding="utf-8"
-    ).splitlines()
-    start, end = find_raw_bootstrap_sequence_lines(ebuild_lines)
-    results = []
-    for line in ebuild_lines[start + 1 : end]:
-        # Ignore comments.
-        line = line.split("#", 1)[0].strip()
-        if not line:
-            continue
-        assert len(line.split()) == 1, f"Unexpected line: {line!r}"
-        results.append(parse_raw_ebuild_version(line.strip()))
-    return results
-
-
-def version_listed_in_bootstrap_sequence(
-    ebuild: Path, rust_bootstrap_version: EbuildVersion
-) -> bool:
-    ebuild_lines = ebuild.read_text(encoding="utf-8").splitlines()
-    start, end = find_raw_bootstrap_sequence_lines(ebuild_lines)
-    str_version = str(rust_bootstrap_version.without_rev())
-    return any(
-        line.strip() == str_version for line in ebuild_lines[start + 1 : end]
-    )
-
-
 @functools.lru_cache(1)
 def fetch_most_recent_sdk_version() -> str:
     """Fetches the most recent official SDK version from gs://."""
@@ -236,16 +192,19 @@ def parse_ebuild_version(ebuild_name: str) -> EbuildVersion:
     )
 
 
-def collect_ebuilds_by_version(
+def collect_stable_ebuilds_by_version(
     ebuild_dir: Path,
 ) -> List[Tuple[EbuildVersion, Path]]:
     """Returns the latest ebuilds grouped by version.without_rev.
 
-    Result is always sorted by version, latest versions are last.
+    Result is always sorted by version, latest versions are last. 9999 ebuilds
+    are ignored.
     """
     ebuilds = ebuild_dir.glob("*.ebuild")
     versioned_ebuilds: Dict[EbuildVersion, Tuple[EbuildVersion, Path]] = {}
     for ebuild in ebuilds:
+        if ebuild.name.endswith("-9999.ebuild"):
+            continue
         version = parse_ebuild_version(ebuild.name)
         version_no_rev = version.without_rev()
         other = versioned_ebuilds.get(version_no_rev)
@@ -289,18 +248,6 @@ def maybe_copy_prebuilt_to_localmirror(
     return True
 
 
-def add_version_to_bootstrap_sequence(
-    ebuild: Path, version: EbuildVersion, dry_run: bool
-):
-    ebuild_lines = ebuild.read_text(encoding="utf-8").splitlines(keepends=True)
-    _, end = find_raw_bootstrap_sequence_lines(ebuild_lines)
-    # `end` is the final paren. Since we _need_ prebuilts for all preceding
-    # versions, always put this a line before the end.
-    ebuild_lines.insert(end, f"\t{version}\n")
-    if not dry_run:
-        ebuild.write_text("".join(ebuild_lines), encoding="utf-8")
-
-
 def is_ebuild_linked_to_in_dir(root_ebuild_path: Path) -> bool:
     """Returns whether symlinks point to `root_ebuild_path`.
 
@@ -360,71 +307,107 @@ def update_ebuild_manifest(rust_bootstrap_ebuild: Path):
     )
 
 
-def commit_all_changes(
-    git_dir: Path, rust_bootstrap_dir: Path, commit_message: str
-):
-    subprocess.run(
-        ["git", "add", rust_bootstrap_dir.relative_to(git_dir)],
-        cwd=git_dir,
-        check=True,
-        stdin=subprocess.DEVNULL,
-    )
-    subprocess.run(
-        ["git", "commit", "-m", commit_message],
-        cwd=git_dir,
-        check=True,
-        stdin=subprocess.DEVNULL,
+def upload_changes(git_dir: Path):
+    logging.info("Uploading changes...")
+    cl_ids = git_utils.upload_to_gerrit(
+        git_repo=git_dir,
+        remote=git_utils.CROS_EXTERNAL_REMOTE,
+        branch=git_utils.CROS_MAIN_BRANCH,
     )
+    for cl_id in cl_ids:
+        git_utils.set_autoreview_topic_and_labels(
+            cwd=git_dir,
+            cl_id=cl_id,
+        )
 
 
-def scrape_git_push_cl_id_strs(git_push_output: str) -> List[str]:
-    id_regex = re.compile(
-        r"^remote:\s+https://chromium-review\S+/\+/(\d+)\s", re.MULTILINE
+def is_rust_bootstrap_using_prebuilts(rust_bootstrap_contents: str) -> bool:
+    """Returns whether the given rust-bootstrap ebuild installs a prebuilt."""
+    matches = list(
+        RUST_BOOTSTRAP_USE_PREBUILTS_REGEX.finditer(rust_bootstrap_contents)
     )
-    results = id_regex.findall(git_push_output)
-    if not results:
+    if len(matches) != 1:
         raise ValueError(
-            f"Found 0 matches of {id_regex} in {git_push_output!r}; expected "
-            "at least 1."
+            "Expected precisely one match for "
+            "{RUST_BOOTSTRAP_USE_PREBUILTS_REGEX} in ebuild contents; got "
+            f"{len(matches)}."
         )
-    return results
+    var_value = matches[0].group(2)
+    return bool(var_value.split("#", 1)[0].strip())
 
 
-def upload_changes(git_dir: Path):
-    logging.info("Uploading changes")
-    result = subprocess.run(
-        ["git", "push", "cros", "HEAD:refs/for/main"],
-        check=True,
-        cwd=git_dir,
-        encoding="utf-8",
-        stdin=subprocess.DEVNULL,
-        stdout=subprocess.PIPE,
-        stderr=subprocess.STDOUT,
-    )
-    # Print this in case anyone's looking at the output.
-    print(result.stdout, end=None)
-    result.check_returncode()
+def substitute_exactly_once(
+    regex: re.Pattern, replace: Callable[[re.Match], str], string: str
+) -> str:
+    """re.subn, but `raise`s less or more than one replacement is made.
 
-    cl_ids = scrape_git_push_cl_id_strs(result.stdout)
-    logging.info(
-        "Uploaded %s successfully!", [f"crrev.com/c/{x}" for x in cl_ids]
-    )
-    for cl_id in cl_ids:
-        gerrit_commands = (
-            ["gerrit", "label-v", cl_id, "1"],
-            ["gerrit", "label-cq", cl_id, "1"],
-            ["gerrit", "label-as", cl_id, "1"],
-            ["gerrit", "reviewers", cl_id] + list(DEFAULT_CL_REVIEWERS),
-            ["gerrit", "ready", cl_id],
+    This is used instead of `re.subn(..., count=1)`, since it functions as a
+    nicer assertion that only exactly one match is expected.
+    """
+    new_contents, num_replacements = regex.subn(replace, string)
+    if num_replacements != 1:
+        raise ValueError(
+            f"Expected to replace exactly one instance of {regex}; replaced "
+            f"{num_replacements}"
         )
-        for command in gerrit_commands:
-            logging.info("Running gerrit command: %s", command)
-            subprocess.run(
-                command,
-                check=True,
-                stdin=subprocess.DEVNULL,
+    return new_contents
+
+
+def set_rust_bootstrap_prebuilt_use(
+    rust_bootstrap_contents: str, use_prebuilts: bool
+) -> str:
+    """Sets the use-prebuilts flag to `use_prebuilts`. in the given ebuild."""
+
+    def replace_instance(match: re.Match) -> str:
+        new_assignment = "1" if use_prebuilts else ""
+        result = match.group(1) + new_assignment
+
+        # If there's a comment at the end (w/ potential leading spaces),
+        # preserve it. This is done independently of
+        # RUST_BOOTSTRAP_USE_PREBUILTS_REGEX, since multiline regex matching is
+        # harder to reason about.
+        if current_assignment := match.group(2):
+            if m := re.search(r"(\s*#.*)$", current_assignment):
+                result += m.group(1)
+        return result
+
+    return substitute_exactly_once(
+        RUST_BOOTSTRAP_USE_PREBUILTS_REGEX,
+        replace_instance,
+        rust_bootstrap_contents,
+    )
+
+
+def build_commit_message_for_new_prebuilts(
+    versions_updated: List[Tuple[EbuildVersion, Optional[str]]],
+) -> str:
+    """Builds a commit message for adding new prebuilts."""
+    pretty_artifact_lines = []
+    for version, maybe_gs_path in versions_updated:
+        if maybe_gs_path:
+            pretty_artifact_lines.append(
+                f"- rust-bootstrap-{version.without_rev()} => {maybe_gs_path}"
+            )
+        else:
+            pretty_artifact_lines.append(
+                f"- rust-bootstrap-{version.without_rev()} was already on "
+                "localmirror"
             )
 
+    pretty_artifacts = "artifact" if len(versions_updated) == 1 else "artifacts"
+    commit_message_lines = [
+        "rust-bootstrap: use prebuilts",
+        "",
+        f"This CL used the following rust-bootstrap {pretty_artifacts}:",
+    ]
+    commit_message_lines += pretty_artifact_lines
+    commit_message_lines += (
+        "",
+        f"BUG={TRACKING_BUG}",
+        "TEST=CQ",
+    )
+    return "\n".join(commit_message_lines)
+
 
 def maybe_add_newest_prebuilts(
     copy_rust_bootstrap_script: Path,
@@ -443,10 +426,13 @@ def maybe_add_newest_prebuilts(
     """
     # A list of (version, maybe_prebuilt_location).
     versions_updated: List[Tuple[EbuildVersion, Optional[str]]] = []
-    for version, ebuild in collect_ebuilds_by_version(rust_bootstrap_dir):
+    for version, ebuild in collect_stable_ebuilds_by_version(
+        rust_bootstrap_dir
+    ):
         logging.info("Inspecting %s...", ebuild)
-        if version.without_rev() in read_bootstrap_sequence_from_ebuild(ebuild):
-            logging.info("Prebuilt already exists for %s.", ebuild)
+        current_ebuild_text = ebuild.read_text(encoding="utf-8")
+        if is_rust_bootstrap_using_prebuilts(current_ebuild_text):
+            logging.info("Prebuilt already in use for %s.", ebuild)
             continue
 
         logging.info("Prebuilt isn't in ebuild; checking remotely.")
@@ -459,7 +445,19 @@ def maybe_add_newest_prebuilts(
         uploaded = maybe_copy_prebuilt_to_localmirror(
             copy_rust_bootstrap_script, prebuilt, dry_run
         )
-        add_version_to_bootstrap_sequence(ebuild, version, dry_run)
+        new_ebuild_text = set_rust_bootstrap_prebuilt_use(
+            current_ebuild_text, use_prebuilts=True
+        )
+        if dry_run:
+            # N.B., the new ebuild's contents are still generated, so --dry-run
+            # runs can catch errors in that more easily.
+            logging.info(
+                "--dry-run was passed; skipping setting the prebuilt bit in %s",
+                ebuild,
+            )
+        else:
+            ebuild.write_text(new_ebuild_text, encoding="utf-8")
+
         uprevved_ebuild = uprev_ebuild(ebuild, version, dry_run)
         versions_updated.append((version, prebuilt if uploaded else None))
 
@@ -475,35 +473,10 @@ def maybe_add_newest_prebuilts(
     # updates for all ebuilds in the same package.
     update_ebuild_manifest(uprevved_ebuild)
 
-    pretty_artifact_lines = []
-    for version, maybe_gs_path in versions_updated:
-        if maybe_gs_path:
-            pretty_artifact_lines.append(
-                f"- rust-bootstrap-{version.without_rev()} => {maybe_gs_path}"
-            )
-        else:
-            pretty_artifact_lines.append(
-                f"- rust-bootstrap-{version.without_rev()} was already on "
-                "localmirror"
-            )
-
-    pretty_artifacts = "\n".join(pretty_artifact_lines)
-
     logging.info("Committing changes.")
-    commit_all_changes(
+    git_utils.commit_all_changes(
         chromiumos_overlay,
-        rust_bootstrap_dir,
-        commit_message=textwrap.dedent(
-            f"""\
-            rust-bootstrap: use prebuilts
-
-            This CL used the following rust-bootstrap artifacts:
-            {pretty_artifacts}
-
-            BUG={TRACKING_BUG}
-            TEST=CQ
-            """
-        ),
+        message=build_commit_message_for_new_prebuilts(versions_updated),
     )
     return True
 
@@ -513,8 +486,16 @@ def rust_dir_from_rust_bootstrap(rust_bootstrap_dir: Path) -> Path:
     return rust_bootstrap_dir.parent / "rust"
 
 
-class MissingRustBootstrapPrebuiltError(Exception):
-    """Raised when rust-bootstrap can't be landed due to a missing prebuilt."""
+def set_rust_bootstrap_prior_version(
+    rust_bootstrap_contents: str,
+    new_version: EbuildVersion,
+) -> str:
+    """Sets the prior rust bootstrap version to the given one."""
+    return substitute_exactly_once(
+        RUST_BOOTSTRAP_PRIOR_VERSION_REGEX,
+        lambda m: m.group(1) + str(new_version.without_rev()) + m.group(3),
+        rust_bootstrap_contents,
+    )
 
 
 def maybe_add_new_rust_bootstrap_version(
@@ -536,25 +517,20 @@ def maybe_add_new_rust_bootstrap_version(
     Returns:
         True if changes were made (or would've been made, in the case of
         dry_run being True). False otherwise.
-
-    Raises:
-        MissingRustBootstrapPrebuiltError if the creation of a new
-        rust-bootstrap ebuild wouldn't be buildable, since there's no
-        rust-bootstrap prebuilt of the prior version for it to sync.
     """
     # These are always returned in sorted error, so taking the last is the same
     # as `max()`.
     (
         newest_bootstrap_version,
         newest_bootstrap_ebuild,
-    ) = collect_ebuilds_by_version(rust_bootstrap_dir)[-1]
+    ) = collect_stable_ebuilds_by_version(rust_bootstrap_dir)[-1]
 
     logging.info(
         "Detected newest rust-bootstrap version: %s", newest_bootstrap_version
     )
 
     rust_dir = rust_dir_from_rust_bootstrap(rust_bootstrap_dir)
-    newest_rust_version, _ = collect_ebuilds_by_version(rust_dir)[-1]
+    newest_rust_version, _ = collect_stable_ebuilds_by_version(rust_dir)[-1]
     logging.info("Detected newest rust version: %s", newest_rust_version)
 
     # Generally speaking, we don't care about keeping up with new patch
@@ -569,43 +545,36 @@ def maybe_add_new_rust_bootstrap_version(
         logging.info("No missing rust-bootstrap versions detected.")
         return False
 
-    available_prebuilts = read_bootstrap_sequence_from_ebuild(
-        newest_bootstrap_ebuild
-    )
-    need_prebuilt = newest_rust_version.major_minor_only().prior_minor_version()
-
-    if all(x.major_minor_only() != need_prebuilt for x in available_prebuilts):
-        raise MissingRustBootstrapPrebuiltError(
-            f"want version {need_prebuilt}; "
-            f"available versions: {available_prebuilts}"
-        )
-
-    # Ensure the rust-bootstrap ebuild we're landing is a regular file. This
-    # makes cleanup of the old files trivial, since they're dead symlinks.
+    # Just copy the old ebuild, tweaking contents slightly as appropriate.
     prior_ebuild_resolved = newest_bootstrap_ebuild.resolve()
     new_ebuild = (
         rust_bootstrap_dir
         / f"rust-bootstrap-{newest_rust_version.without_rev()}.ebuild"
     )
-    if dry_run:
-        logging.info("Would move %s to %s.", prior_ebuild_resolved, new_ebuild)
-        return True
 
-    logging.info(
-        "Moving %s to %s, and creating symlink at the old location",
-        prior_ebuild_resolved,
-        new_ebuild,
+    prior_ebuild_contents = prior_ebuild_resolved.read_text(encoding="utf-8")
+    new_ebuild_contents = set_rust_bootstrap_prior_version(
+        set_rust_bootstrap_prebuilt_use(
+            prior_ebuild_contents,
+            use_prebuilts=False,
+        ),
+        new_version=newest_bootstrap_version,
     )
-    prior_ebuild_resolved.rename(new_ebuild)
-    prior_ebuild_resolved.symlink_to(new_ebuild.relative_to(rust_bootstrap_dir))
+    if dry_run:
+        logging.info(
+            "Would create new rust-bootstrap ebuild %s from %s.",
+            new_ebuild,
+            prior_ebuild_resolved,
+        )
+        return True
 
+    new_ebuild.write_text(new_ebuild_contents, encoding="utf-8")
     update_ebuild_manifest(new_ebuild)
     if commit:
         newest_no_rev = newest_rust_version.without_rev()
-        commit_all_changes(
+        git_utils.commit_all_changes(
             chromiumos_overlay,
-            rust_bootstrap_dir,
-            commit_message=textwrap.dedent(
+            message=textwrap.dedent(
                 f"""\
                 rust-bootstrap: add version {newest_no_rev}
 
@@ -671,12 +640,14 @@ def maybe_delete_old_rust_bootstrap_ebuilds(
         other ebuilds linking to it. It's still 'needed' in this case, but with
         some human intervention, it can be removed.
     """
-    rust_bootstrap_versions = collect_ebuilds_by_version(rust_bootstrap_dir)
+    rust_bootstrap_versions = collect_stable_ebuilds_by_version(
+        rust_bootstrap_dir
+    )
     logging.info(
         "Current rust-bootstrap versions: %s",
         [x for x, _ in rust_bootstrap_versions],
     )
-    rust_versions = collect_ebuilds_by_version(
+    rust_versions = collect_stable_ebuilds_by_version(
         rust_dir_from_rust_bootstrap(rust_bootstrap_dir)
     )
     # rust_versions is sorted, so taking the last is the same as max().
@@ -755,10 +726,9 @@ def maybe_delete_old_rust_bootstrap_ebuilds(
             "no longer needed.",
         ]
         message = textwrap.fill("\n".join(message_lines))
-        commit_all_changes(
+        git_utils.commit_all_changes(
             chromiumos_overlay,
-            rust_bootstrap_dir,
-            commit_message=textwrap.dedent(
+            message=textwrap.dedent(
                 f"""\
                 rust-bootstrap: remove unused ebuild{"s" if many else ""}
 
@@ -773,13 +743,15 @@ def maybe_delete_old_rust_bootstrap_ebuilds(
 
 
 def main(argv: List[str]):
+    cros_checkout = cros_paths.script_chromiumos_checkout_or_exit()
+    py_bin_dir = cros_checkout / cros_paths.TOOLCHAIN_UTILS_PYBIN
+
     logging.basicConfig(
         format=">> %(asctime)s: %(levelname)s: %(filename)s:%(lineno)d: "
         "%(message)s",
         level=logging.INFO,
     )
 
-    my_dir = Path(__file__).parent.resolve()
     parser = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawDescriptionHelpFormatter,
@@ -787,7 +759,7 @@ def main(argv: List[str]):
     parser.add_argument(
         "--chromiumos-overlay",
         type=Path,
-        default=my_dir.parent.parent / "chromiumos-overlay",
+        default=cros_checkout / cros_paths.CHROMIUMOS_OVERLAY,
     )
     parser.add_argument(
         "action",
@@ -812,7 +784,9 @@ def main(argv: List[str]):
         upload = True
 
     rust_bootstrap_dir = opts.chromiumos_overlay / "dev-lang/rust-bootstrap"
-    copy_rust_bootstrap_script = my_dir / "copy_rust_bootstrap.py"
+    copy_rust_bootstrap_script = (
+        py_bin_dir / "rust_tools" / "copy_rust_bootstrap.py"
+    )
 
     had_recoverable_error = False
     # Ensure prebuilts are up to date first, since it allows
@@ -824,15 +798,9 @@ def main(argv: List[str]):
         dry_run,
     )
 
-    try:
-        made_changes |= maybe_add_new_rust_bootstrap_version(
-            opts.chromiumos_overlay, rust_bootstrap_dir, dry_run
-        )
-    except MissingRustBootstrapPrebuiltError:
-        logging.exception(
-            "Ensuring newest rust-bootstrap ebuild exists failed."
-        )
-        had_recoverable_error = True
+    made_changes |= maybe_add_new_rust_bootstrap_version(
+        opts.chromiumos_overlay, rust_bootstrap_dir, dry_run
+    )
 
     try:
         made_changes |= maybe_delete_old_rust_bootstrap_ebuilds(
@@ -851,7 +819,3 @@ def main(argv: List[str]):
 
     if had_recoverable_error:
         sys.exit("Exiting uncleanly due to above error(s).")
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/rust_tools/auto_update_rust_bootstrap_test.py b/rust_tools/auto_update_rust_bootstrap_test.py
old mode 100755
new mode 100644
index 0578539d..69c9f2e6
--- a/rust_tools/auto_update_rust_bootstrap_test.py
+++ b/rust_tools/auto_update_rust_bootstrap_test.py
@@ -1,11 +1,9 @@
-#!/usr/bin/env python3
 # Copyright 2023 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
 """Tests for auto_update_rust_bootstrap."""
 
-import os
 from pathlib import Path
 import shutil
 import tempfile
@@ -13,47 +11,7 @@ import textwrap
 import unittest
 from unittest import mock
 
-import auto_update_rust_bootstrap
-
-
-_GIT_PUSH_OUTPUT = r"""
-remote: Waiting for private key checker: 2/2 objects left
-remote:
-remote: Processing changes: new: 1 (\)
-remote: Processing changes: new: 1 (|)
-remote: Processing changes: new: 1 (/)
-remote: Processing changes: refs: 1, new: 1 (/)
-remote: Processing changes: refs: 1, new: 1 (/)
-remote: Processing changes: refs: 1, new: 1 (/)
-remote: Processing changes: refs: 1, new: 1, done
-remote:
-remote: SUCCESS
-remote:
-remote:   https://chromium-review.googlesource.com/c/chromiumos/overlays/chromiumos-overlay/+/5018826 rust-bootstrap: use prebuilts [WIP] [NEW]
-remote:
-To https://chromium.googlesource.com/chromiumos/overlays/chromiumos-overlay
- * [new reference]             HEAD -> refs/for/main
-"""
-
-_GIT_PUSH_MULTI_CL_OUTPUT = r"""
-remote: Waiting for private key checker: 2/2 objects left
-remote:
-remote: Processing changes: new: 1 (\)
-remote: Processing changes: new: 1 (|)
-remote: Processing changes: new: 1 (/)
-remote: Processing changes: refs: 1, new: 1 (/)
-remote: Processing changes: refs: 1, new: 1 (/)
-remote: Processing changes: refs: 1, new: 1 (/)
-remote: Processing changes: refs: 1, new: 1, done
-remote:
-remote: SUCCESS
-remote:
-remote:   https://chromium-review.googlesource.com/c/chromiumos/overlays/chromiumos-overlay/+/5339923 rust-bootstrap: add version 1.75.0 [NEW]
-remote:   https://chromium-review.googlesource.com/c/chromiumos/overlays/chromiumos-overlay/+/5339924 rust-bootstrap: remove unused ebuilds [NEW]
-remote:
-To https://chromium.googlesource.com/chromiumos/overlays/chromiumos-overlay
- * [new reference]             HEAD -> refs/for/main
-"""
+from rust_tools import auto_update_rust_bootstrap
 
 
 class Test(unittest.TestCase):
@@ -66,21 +24,6 @@ class Test(unittest.TestCase):
         self.addCleanup(shutil.rmtree, tempdir)
         return tempdir
 
-    def test_git_cl_id_scraping(self):
-        self.assertEqual(
-            auto_update_rust_bootstrap.scrape_git_push_cl_id_strs(
-                _GIT_PUSH_OUTPUT
-            ),
-            ["5018826"],
-        )
-
-        self.assertEqual(
-            auto_update_rust_bootstrap.scrape_git_push_cl_id_strs(
-                _GIT_PUSH_MULTI_CL_OUTPUT
-            ),
-            ["5339923", "5339924"],
-        )
-
     def test_ebuild_linking_logic_handles_direct_relative_symlinks(self):
         tempdir = self.make_tempdir()
         target = tempdir / "target.ebuild"
@@ -128,30 +71,114 @@ class Test(unittest.TestCase):
             auto_update_rust_bootstrap.is_ebuild_linked_to_in_dir(target)
         )
 
-    def test_raw_bootstrap_seq_finding_functions(self):
+    def test_version_has_prebuilt_detection_works(self):
         ebuild_contents = textwrap.dedent(
             """\
             # Some copyright
             FOO=bar
-            # Comment about RUSTC_RAW_FULL_BOOTSTRAP_SEQUENCE=(
-            RUSTC_RAW_FULL_BOOTSTRAP_SEQUENCE=( # another comment
-                1.2.3 # (with a comment with parens)
-                4.5.6
+            # Comment about this cool var
+            THIS_VERSION_HAS_PREBUILT=     # a comment
+
+            # Another comment for posterity
+            """
+        )
+        self.assertFalse(
+            auto_update_rust_bootstrap.is_rust_bootstrap_using_prebuilts(
+                ebuild_contents
             )
+        )
+
+    def test_version_has_prebuilt_modification_works(self):
+        ebuild_contents = textwrap.dedent(
+            """\
+            # Some copyright
+            FOO=bar
+            # Comment about this cool var
+            THIS_VERSION_HAS_PREBUILT=     # a comment
+            # Another comment for posterity
             """
         )
+        with_set_has_ebuild = (
+            auto_update_rust_bootstrap.set_rust_bootstrap_prebuilt_use(
+                ebuild_contents,
+                use_prebuilts=True,
+            )
+        )
+        self.assertIn(
+            "THIS_VERSION_HAS_PREBUILT=1     # a comment\n", with_set_has_ebuild
+        )
 
-        ebuild_lines = ebuild_contents.splitlines()
-        (
-            start,
-            end,
-        ) = auto_update_rust_bootstrap.find_raw_bootstrap_sequence_lines(
-            ebuild_lines
+        with_unset_has_ebuild = (
+            auto_update_rust_bootstrap.set_rust_bootstrap_prebuilt_use(
+                ebuild_contents,
+                use_prebuilts=False,
+            )
         )
-        self.assertEqual(start, len(ebuild_lines) - 4)
-        self.assertEqual(end, len(ebuild_lines) - 1)
+        self.assertEqual(ebuild_contents, with_unset_has_ebuild)
+
+    def test_version_has_prebuilt_modification_works_without_comment(self):
+        ebuild_contents = textwrap.dedent(
+            """\
+            # Some copyright
+            FOO=bar
+            # Comment about this cool var
+            THIS_VERSION_HAS_PREBUILT=
 
-    def test_collect_ebuilds_by_version_ignores_older_versions(self):
+            # Another comment for posterity
+            """
+        )
+        with_set_has_ebuild = (
+            auto_update_rust_bootstrap.set_rust_bootstrap_prebuilt_use(
+                ebuild_contents,
+                use_prebuilts=True,
+            )
+        )
+        self.assertIn("THIS_VERSION_HAS_PREBUILT=1", with_set_has_ebuild)
+
+    def test_version_has_prebuilt_unsetting_works_with_comment(self):
+        ebuild_contents = textwrap.dedent(
+            """\
+            # Some copyright
+            FOO=bar
+            # Comment about this cool var
+            THIS_VERSION_HAS_PREBUILT=" 1" # baz
+
+            # Another comment for posterity
+            """
+        )
+        with_set_has_ebuild = (
+            auto_update_rust_bootstrap.set_rust_bootstrap_prebuilt_use(
+                ebuild_contents,
+                use_prebuilts=False,
+            )
+        )
+        self.assertIn("THIS_VERSION_HAS_PREBUILT= # baz", with_set_has_ebuild)
+
+    def test_set_rust_bootstrap_prior_version_works(self):
+        ebuild_contents = textwrap.dedent(
+            """\
+            # Some copyright
+            FOO=bar
+            # Comment about this cool var
+            PRIOR_RUST_BOOTSTRAP_VERSION="foo"
+
+            # Another comment for posterity
+            """
+        )
+        with_update = (
+            auto_update_rust_bootstrap.set_rust_bootstrap_prior_version(
+                ebuild_contents,
+                new_version=auto_update_rust_bootstrap.EbuildVersion(
+                    major=1,
+                    minor=2,
+                    patch=3,
+                    rev=4,
+                ),
+            )
+        )
+        self.assertIn('PRIOR_RUST_BOOTSTRAP_VERSION="1.2.3"', with_update)
+
+    def test_collect_ebuilds_by_version_ignores_old_versions_and_9999(self):
         tempdir = self.make_tempdir()
         ebuild_170 = tempdir / "rust-bootstrap-1.70.0.ebuild"
         ebuild_170.touch()
@@ -159,9 +186,12 @@ class Test(unittest.TestCase):
         ebuild_170_r1.touch()
         ebuild_171_r2 = tempdir / "rust-bootstrap-1.71.1-r2.ebuild"
         ebuild_171_r2.touch()
+        (tempdir / "rust-bootstrap-9999.ebuild").touch()
 
         self.assertEqual(
-            auto_update_rust_bootstrap.collect_ebuilds_by_version(tempdir),
+            auto_update_rust_bootstrap.collect_stable_ebuilds_by_version(
+                tempdir
+            ),
             [
                 (
                     auto_update_rust_bootstrap.EbuildVersion(
@@ -178,94 +208,6 @@ class Test(unittest.TestCase):
             ],
         )
 
-    def test_has_prebuilt_works(self):
-        tempdir = self.make_tempdir()
-        ebuild = tempdir / "rust-bootstrap-1.70.0.ebuild"
-        ebuild.write_text(
-            textwrap.dedent(
-                """\
-                # Some copyright
-                FOO=bar
-                # Comment about RUSTC_RAW_FULL_BOOTSTRAP_SEQUENCE=(
-                RUSTC_RAW_FULL_BOOTSTRAP_SEQUENCE=( # another comment
-                    1.67.0
-                    1.68.1
-                    1.69.0
-                )
-                """
-            ),
-            encoding="utf-8",
-        )
-
-        self.assertTrue(
-            auto_update_rust_bootstrap.version_listed_in_bootstrap_sequence(
-                ebuild,
-                auto_update_rust_bootstrap.EbuildVersion(
-                    major=1,
-                    minor=69,
-                    patch=0,
-                    rev=0,
-                ),
-            )
-        )
-
-        self.assertFalse(
-            auto_update_rust_bootstrap.version_listed_in_bootstrap_sequence(
-                ebuild,
-                auto_update_rust_bootstrap.EbuildVersion(
-                    major=1,
-                    minor=70,
-                    patch=0,
-                    rev=0,
-                ),
-            )
-        )
-
-    def test_ebuild_updating_works(self):
-        tempdir = self.make_tempdir()
-        ebuild = tempdir / "rust-bootstrap-1.70.0.ebuild"
-        ebuild.write_text(
-            textwrap.dedent(
-                """\
-                # Some copyright
-                FOO=bar
-                RUSTC_RAW_FULL_BOOTSTRAP_SEQUENCE=(
-                \t1.67.0
-                \t1.68.1
-                \t1.69.0
-                )
-                """
-            ),
-            encoding="utf-8",
-        )
-
-        auto_update_rust_bootstrap.add_version_to_bootstrap_sequence(
-            ebuild,
-            auto_update_rust_bootstrap.EbuildVersion(
-                major=1,
-                minor=70,
-                patch=1,
-                rev=2,
-            ),
-            dry_run=False,
-        )
-
-        self.assertEqual(
-            ebuild.read_text(encoding="utf-8"),
-            textwrap.dedent(
-                """\
-                # Some copyright
-                FOO=bar
-                RUSTC_RAW_FULL_BOOTSTRAP_SEQUENCE=(
-                \t1.67.0
-                \t1.68.1
-                \t1.69.0
-                \t1.70.1-r2
-                )
-                """
-            ),
-        )
-
     def test_ebuild_version_parsing_works(self):
         self.assertEqual(
             auto_update_rust_bootstrap.parse_ebuild_version(
@@ -332,12 +274,9 @@ class Test(unittest.TestCase):
             """\
             # Some copyright
             FOO=bar
-            RUSTC_RAW_FULL_BOOTSTRAP_SEQUENCE=(
-            \t1.67.0
-            \t1.68.1
-            \t1.69.0
-            \t1.70.0-r1
-            )
+
+            THIS_VERSION_HAS_PREBUILT=1
+            PRIOR_RUST_BOOTSTRAP_VERSION="1.69.0"
             """
         )
         rust_bootstrap_1_70.write_text(
@@ -352,49 +291,20 @@ class Test(unittest.TestCase):
         update_ebuild_manifest.assert_called_once()
         rust_bootstrap_1_71 = rust_bootstrap / "rust-bootstrap-1.71.0.ebuild"
 
-        self.assertTrue(rust_bootstrap_1_70.is_symlink())
-        self.assertEqual(
-            os.readlink(rust_bootstrap_1_70),
-            rust_bootstrap_1_71.name,
-        )
-        self.assertFalse(rust_bootstrap_1_71.is_symlink())
-        self.assertEqual(
-            rust_bootstrap_1_71.read_text(encoding="utf-8"),
+        self.assertTrue(
+            rust_bootstrap_1_70.read_text(encoding="utf-8"),
             rust_bootstrap_contents,
         )
-
-    def test_ensure_newest_version_breaks_if_prebuilt_is_not_available(self):
-        tempdir = self.make_tempdir()
-        rust = tempdir / "rust"
-        rust.mkdir()
-        (rust / "rust-1.71.0-r1.ebuild").touch()
-        rust_bootstrap = tempdir / "rust-bootstrap"
-        rust_bootstrap.mkdir()
-        rust_bootstrap_1_70 = rust_bootstrap / "rust-bootstrap-1.70.0-r2.ebuild"
-
-        rust_bootstrap_contents = textwrap.dedent(
-            """\
-            # Some copyright
-            FOO=bar
-            RUSTC_RAW_FULL_BOOTSTRAP_SEQUENCE=(
-            \t1.67.0
-            \t1.68.1
-            \t1.69.0
-            # Note: Missing 1.70.0 for rust-bootstrap-1.71.1
-            )
-            """
+        new_contents = rust_bootstrap_1_71.read_text(encoding="utf-8")
+        self.assertIn(
+            "THIS_VERSION_HAS_PREBUILT=\n",
+            new_contents,
         )
-        rust_bootstrap_1_70.write_text(
-            rust_bootstrap_contents, encoding="utf-8"
+        self.assertIn(
+            'PRIOR_RUST_BOOTSTRAP_VERSION="1.70.0"\n',
+            new_contents,
         )
 
-        with self.assertRaises(
-            auto_update_rust_bootstrap.MissingRustBootstrapPrebuiltError
-        ):
-            auto_update_rust_bootstrap.maybe_add_new_rust_bootstrap_version(
-                tempdir, rust_bootstrap, dry_run=True
-            )
-
     def test_version_deletion_does_nothing_if_all_versions_are_needed(self):
         tempdir = self.make_tempdir()
         rust = tempdir / "rust"
@@ -496,6 +406,50 @@ class Test(unittest.TestCase):
                 tempdir, rust_bootstrap, dry_run=True
             )
 
+    def test_prebuilt_commit_message_generation_with_one_update(self):
+        msg = auto_update_rust_bootstrap.build_commit_message_for_new_prebuilts(
+            [
+                (
+                    auto_update_rust_bootstrap.EbuildVersion(1, 70, 0, 0),
+                    "gs://some/path",
+                )
+            ]
+        )
+        self.assertEqual(
+            msg,
+            textwrap.dedent(
+                f"""\
+            rust-bootstrap: use prebuilts
+
+            This CL used the following rust-bootstrap artifact:
+            - rust-bootstrap-1.70.0 => gs://some/path
 
-if __name__ == "__main__":
-    unittest.main()
+            BUG={auto_update_rust_bootstrap.TRACKING_BUG}
+            TEST=CQ"""
+            ),
+        )
+
+    def test_prebuilt_commit_message_generation_with_multiple_updates(self):
+        msg = auto_update_rust_bootstrap.build_commit_message_for_new_prebuilts(
+            [
+                (
+                    auto_update_rust_bootstrap.EbuildVersion(1, 70, 0, 0),
+                    "gs://some/path",
+                ),
+                (auto_update_rust_bootstrap.EbuildVersion(1, 71, 1, 0), None),
+            ]
+        )
+        self.assertEqual(
+            msg,
+            textwrap.dedent(
+                f"""\
+            rust-bootstrap: use prebuilts
+
+            This CL used the following rust-bootstrap artifacts:
+            - rust-bootstrap-1.70.0 => gs://some/path
+            - rust-bootstrap-1.71.1 was already on localmirror
+
+            BUG={auto_update_rust_bootstrap.TRACKING_BUG}
+            TEST=CQ"""
+            ),
+        )
diff --git a/rust_tools/copy_rust_bootstrap.py b/rust_tools/copy_rust_bootstrap.py
old mode 100755
new mode 100644
index fd6770f7..a6ec32a4
--- a/rust_tools/copy_rust_bootstrap.py
+++ b/rust_tools/copy_rust_bootstrap.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2022 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -198,7 +197,3 @@ def main(argv: List[str]):
             )
         else:
             _upload(file_to_upload, target_path, opts.force)
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/rust_tools/rust_uprev.py b/rust_tools/rust_uprev.py
old mode 100755
new mode 100644
index 9845c7c7..0d172938
--- a/rust_tools/rust_uprev.py
+++ b/rust_tools/rust_uprev.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2020 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -13,20 +12,16 @@ failing step is fixed. Example usage to create a new version:
 
 1. (outside chroot) $ ./rust_tools/rust_uprev.py            \\
                      --state_file /tmp/rust-to-1.60.0.json  \\
-                     roll --uprev 1.60.0
+                     --uprev 1.60.0
 2. Step "compile rust" failed due to the patches can't apply to new version.
 3. Manually fix the patches.
-4. Execute the command in step 1 again, but add "--continue" before "roll".
+4. Execute the command in step 1 again, but add "--continue".
 5. Iterate 1-4 for each failed step until the tool passes.
 
-Besides "roll", the tool also support subcommands that perform
-various parts of an uprev.
-
 See `--help` for all available options.
 """
 
 import argparse
-import functools
 import json
 import logging
 import os
@@ -34,8 +29,8 @@ import pathlib
 from pathlib import Path
 import re
 import shlex
-import shutil
 import subprocess
+import textwrap
 import threading
 import time
 from typing import (
@@ -47,14 +42,15 @@ from typing import (
     Optional,
     Protocol,
     Sequence,
-    Tuple,
     TypeVar,
     Union,
 )
 import urllib.request
 
+from cros_utils import cros_paths
+from cros_utils import git_utils
 from llvm_tools import chroot
-from llvm_tools import git
+from pgo_tools_rust import pgo_rust
 
 
 T = TypeVar("T")
@@ -95,14 +91,14 @@ EQUERY = "equery"
 GPG = "gpg"
 GSUTIL = "gsutil.py"
 MIRROR_PATH = "gs://chromeos-localmirror/distfiles"
-EBUILD_PREFIX = SOURCE_ROOT / "src/third_party/chromiumos-overlay"
+EBUILD_PREFIX = SOURCE_ROOT / cros_paths.CHROMIUMOS_OVERLAY
 CROS_RUSTC_ECLASS = EBUILD_PREFIX / "eclass/cros-rustc.eclass"
 # Keyserver to use with GPG. Not all keyservers have Rust's signing key;
 # this must be set to a keyserver that does.
 GPG_KEYSERVER = "keyserver.ubuntu.com"
 PGO_RUST = Path(
     "/mnt/host/source"
-    "/src/third_party/toolchain-utils/pgo_tools_rust/pgo_rust.py"
+    "/src/third_party/toolchain-utils/py/bin/pgo_tools_rust/pgo_rust.py"
 )
 RUST_PATH = Path(EBUILD_PREFIX, "dev-lang", "rust")
 # This is the signing key used by upstream Rust as of 2023-08-09.
@@ -190,12 +186,6 @@ class RustVersion(NamedTuple):
         )
 
 
-class PreparedUprev(NamedTuple):
-    """Container for the information returned by prepare_uprev."""
-
-    template_version: RustVersion
-
-
 def compute_ebuild_path(category: str, name: str, version: RustVersion) -> Path:
     return EBUILD_PREFIX / category / name / f"{name}-{version}.ebuild"
 
@@ -248,14 +238,6 @@ def find_ebuild_path(
     return result[0]
 
 
-def get_rust_bootstrap_version():
-    """Get the version of the current rust-bootstrap package."""
-    bootstrap_ebuild = find_ebuild_path(rust_bootstrap_path(), "rust-bootstrap")
-    m = re.match(r"^rust-bootstrap-(\d+).(\d+).(\d+)", bootstrap_ebuild.name)
-    assert m, bootstrap_ebuild.name
-    return RustVersion(int(m.group(1)), int(m.group(2)), int(m.group(3)))
-
-
 def parse_commandline_args() -> argparse.Namespace:
     parser = argparse.ArgumentParser(
         description=__doc__,
@@ -281,86 +263,29 @@ def parse_commandline_args() -> argparse.Namespace:
         action="store_true",
         help="Continue the steps from the state file",
     )
-
-    create_parser_template = argparse.ArgumentParser(add_help=False)
-    create_parser_template.add_argument(
-        "--template",
-        type=RustVersion.parse,
-        default=None,
-        help="A template to use for creating a Rust uprev from, in the form "
-        "a.b.c The ebuild has to exist in the chroot. If not specified, the "
-        "tool will use the current Rust version in the chroot as template.",
-    )
-    create_parser_template.add_argument(
+    parser.add_argument(
         "--skip_compile",
         action="store_true",
         help="Skip compiling rust to test the tool. Only for testing",
     )
-
-    subparsers = parser.add_subparsers(dest="subparser_name")
-    subparser_names = []
-    subparser_names.append("create")
-    create_parser = subparsers.add_parser(
-        "create",
-        parents=[create_parser_template],
-        help="Create changes uprevs Rust to a new version",
-    )
-    create_parser.add_argument(
-        "--rust_version",
-        type=RustVersion.parse,
-        required=True,
-        help="Rust version to uprev to, in the form a.b.c",
-    )
-
-    subparser_names.append("remove")
-    remove_parser = subparsers.add_parser(
-        "remove",
-        help="Clean up old Rust version from chroot",
-    )
-    remove_parser.add_argument(
-        "--rust_version",
-        type=RustVersion.parse,
-        default=None,
-        help="Rust version to remove, in the form a.b.c If not "
-        "specified, the tool will remove the oldest version in the chroot",
-    )
-
-    subparser_names.append("roll")
-    roll_parser = subparsers.add_parser(
-        "roll",
-        parents=[create_parser_template],
-        help="A command can create and upload a Rust uprev CL, including "
-        "preparing the repo, creating new Rust uprev, deleting old uprev, "
-        "and upload a CL to crrev.",
-    )
-    roll_parser.add_argument(
+    parser.add_argument(
         "--uprev",
         type=RustVersion.parse,
         required=True,
         help="Rust version to uprev to, in the form a.b.c",
     )
-    roll_parser.add_argument(
-        "--remove",
-        type=RustVersion.parse,
-        default=None,
-        help="Rust version to remove, in the form a.b.c If not "
-        "specified, the tool will remove the oldest version in the chroot",
-    )
-    roll_parser.add_argument(
+    parser.add_argument(
         "--skip_cross_compiler",
         action="store_true",
         help="Skip updating cross-compiler in the chroot",
     )
-    roll_parser.add_argument(
+    parser.add_argument(
         "--no_upload",
         action="store_true",
         help="If specified, the tool will not upload the CL for review",
     )
 
     args = parser.parse_args()
-    if args.subparser_name not in subparser_names:
-        parser.error("one of %s must be specified" % subparser_names)
-
     if args.cont and args.restart:
         parser.error("Please select either --continue or --restart")
 
@@ -379,42 +304,6 @@ def parse_commandline_args() -> argparse.Namespace:
     return args
 
 
-def prepare_uprev(
-    rust_version: RustVersion, template: RustVersion
-) -> Optional[PreparedUprev]:
-    ebuild_path = find_ebuild_for_rust_version(template)
-
-    if rust_version <= template:
-        logging.info(
-            "Requested version %s is not newer than the template version %s.",
-            rust_version,
-            template,
-        )
-        return None
-
-    logging.info(
-        "Template Rust version is %s (ebuild: %s)",
-        template,
-        ebuild_path,
-    )
-
-    return PreparedUprev(template)
-
-
-def create_ebuild(
-    category: str,
-    name: str,
-    template_version: RustVersion,
-    new_version: RustVersion,
-) -> None:
-    template_ebuild = compute_ebuild_path(category, name, template_version)
-    new_ebuild = compute_ebuild_path(category, name, new_version)
-    shutil.copyfile(template_ebuild, new_ebuild)
-    subprocess.check_call(
-        ["git", "add", new_ebuild.name], cwd=new_ebuild.parent
-    )
-
-
 def set_include_profdata_src(ebuild_path: os.PathLike, include: bool) -> None:
     """Changes an ebuild file to include or omit profile data from SRC_URI.
 
@@ -442,21 +331,30 @@ def set_include_profdata_src(ebuild_path: os.PathLike, include: bool) -> None:
     Path(ebuild_path).write_text(contents, encoding="utf-8")
 
 
-def update_bootstrap_version(
-    path: PathOrStr, new_bootstrap_version: RustVersion
+def update_ebuild_variable_version(
+    path: PathOrStr, variable_name: str, new_version: RustVersion
 ) -> None:
     path = Path(path)
     contents = path.read_text(encoding="utf-8")
     contents, subs = re.subn(
-        r"^BOOTSTRAP_VERSION=.*$",
-        'BOOTSTRAP_VERSION="%s"' % (new_bootstrap_version,),
+        r"^" + re.escape(variable_name) + r"=.*$",
+        f'{variable_name}="{new_version}"',
         contents,
         flags=re.MULTILINE,
     )
     if not subs:
-        raise RuntimeError(f"BOOTSTRAP_VERSION not found in {path}")
+        raise RuntimeError(f"{variable_name} not found in {path}")
     path.write_text(contents, encoding="utf-8")
-    logging.info("Rust BOOTSTRAP_VERSION updated to %s", new_bootstrap_version)
+    logging.info("Rust %s updated to %s", variable_name, new_version)
+
+
+def cros_workon(start_or_stop: str, packages: List[str]) -> None:
+    """Runs `cros-workon` on the given host packages."""
+    subprocess.run(
+        ["cros", "workon", "--host", start_or_stop] + packages,
+        check=True,
+        stdin=subprocess.DEVNULL,
+    )
 
 
 def ebuild_actions(
@@ -535,16 +433,6 @@ it to the local mirror using gsutil cp.
         raise Exception("Could not verify that allUsers has READER permission")
 
 
-def fetch_bootstrap_distfiles(version: RustVersion) -> None:
-    """Fetches rust-bootstrap distfiles from the local mirror
-
-    Fetches the distfiles for a rust-bootstrap ebuild to ensure they
-    are available on the mirror and the local copies are the same as
-    the ones on the mirror.
-    """
-    fetch_distfile_from_mirror(compute_rustc_src_name(version))
-
-
 def fetch_rust_distfiles(version: RustVersion) -> None:
     """Fetches rust distfiles from the local mirror
 
@@ -694,19 +582,6 @@ def update_rust_packages(
         f.write(new_contents)
 
 
-def update_virtual_rust(
-    template_version: RustVersion, new_version: RustVersion
-) -> None:
-    template_ebuild = find_ebuild_path(
-        EBUILD_PREFIX.joinpath("virtual/rust"), "rust", template_version
-    )
-    virtual_rust_dir = template_ebuild.parent
-    new_name = f"rust-{new_version}.ebuild"
-    new_ebuild = virtual_rust_dir.joinpath(new_name)
-    shutil.copyfile(template_ebuild, new_ebuild)
-    subprocess.check_call(["git", "add", new_name], cwd=virtual_rust_dir)
-
-
 def unmerge_package_if_installed(pkgatom: str) -> None:
     """Unmerges a package if it is installed."""
     shpkg = shlex.quote(pkgatom)
@@ -750,45 +625,12 @@ def perform_step(
     return val
 
 
-def prepare_uprev_from_json(obj: Any) -> Optional[PreparedUprev]:
-    if not obj:
-        return None
-    version = obj[0]
-    return PreparedUprev(
-        RustVersion(*version),
-    )
-
-
-def prepare_uprev_to_json(
-    prepared_uprev: Optional[PreparedUprev],
-) -> Optional[Tuple[RustVersion]]:
-    if prepared_uprev is None:
-        return None
-    return (prepared_uprev.template_version,)
-
-
 def create_rust_uprev(
     rust_version: RustVersion,
     template_version: RustVersion,
     skip_compile: bool,
     run_step: RunStepFn,
 ) -> None:
-    prepared = run_step(
-        "prepare uprev",
-        lambda: prepare_uprev(rust_version, template_version),
-        result_from_json=prepare_uprev_from_json,
-        result_to_json=prepare_uprev_to_json,
-    )
-    if prepared is None:
-        return
-    template_version = prepared.template_version
-
-    run_step(
-        "mirror bootstrap sources",
-        lambda: mirror_rust_source(
-            template_version,
-        ),
-    )
     run_step(
         "mirror rust sources",
         lambda: mirror_rust_source(
@@ -800,32 +642,40 @@ def create_rust_uprev(
     # are not available on the mirror. To make them pass, fetch the
     # required files yourself, verify their checksums, then upload them
     # to the mirror.
+    run_step("fetch rust distfiles", lambda: fetch_rust_distfiles(rust_version))
     run_step(
-        "fetch bootstrap distfiles",
-        lambda: fetch_bootstrap_distfiles(template_version),
+        "update cros-rustc.eclass bootstrap version",
+        lambda: update_ebuild_variable_version(
+            CROS_RUSTC_ECLASS, "BOOTSTRAP_VERSION", template_version
+        ),
     )
-    run_step("fetch rust distfiles", lambda: fetch_rust_distfiles(rust_version))
+
     run_step(
-        "update bootstrap version",
-        lambda: update_bootstrap_version(CROS_RUSTC_ECLASS, template_version),
+        "update cros-rustc.eclass rust version",
+        lambda: update_ebuild_variable_version(
+            CROS_RUSTC_ECLASS, "RUSTC_STABLE_VERSION", rust_version
+        ),
     )
+
+    rust_workon_packages = ["dev-lang/rust-host"]
+    # Add all cross-compiler packages, except for host packages, which don't
+    # have a `rust` to install.
+    rust_workon_packages += (
+        f"cross-{x}/rust"
+        for x in pgo_rust.TARGET_TRIPLES
+        if not x.endswith("pc-linux-gnu")
+    )
+
+    run_step(
+        "cros-workon rust packages",
+        lambda: cros_workon("start", rust_workon_packages),
+    )
+
     run_step(
         "turn off profile data sources in cros-rustc.eclass",
         lambda: set_include_profdata_src(CROS_RUSTC_ECLASS, include=False),
     )
 
-    for category, name in RUST_PACKAGES:
-        run_step(
-            f"create new {category}/{name} ebuild",
-            functools.partial(
-                create_ebuild,
-                category,
-                name,
-                template_version,
-                rust_version,
-            ),
-        )
-
     run_step(
         "update dev-lang/rust-host manifest to add new version",
         lambda: ebuild_actions("dev-lang/rust-host", ["manifest"]),
@@ -854,7 +704,15 @@ def create_rust_uprev(
         lambda: ebuild_actions("dev-lang/rust-host", ["manifest"]),
     )
     if not skip_compile:
-        run_step("build packages", lambda: rebuild_packages(rust_version))
+        run_step(
+            "build packages", lambda: rebuild_packages(rust_workon_packages)
+        )
+
+    run_step(
+        "cros-workon stop rust packages",
+        lambda: cros_workon("stop", rust_workon_packages),
+    )
+
     run_step(
         "insert host version into rust packages",
         lambda: update_rust_packages(
@@ -865,32 +723,22 @@ def create_rust_uprev(
         "insert target version into rust packages",
         lambda: update_rust_packages("dev-lang/rust", rust_version, add=True),
     )
-    run_step(
-        "upgrade virtual/rust",
-        lambda: update_virtual_rust(template_version, rust_version),
-    )
 
 
-def find_rust_versions() -> List[Tuple[RustVersion, Path]]:
-    """Returns (RustVersion, ebuild_path) for base versions of dev-lang/rust.
-
-    This excludes symlinks to ebuilds, so if rust-1.34.0.ebuild and
-    rust-1.34.0-r1.ebuild both exist and -r1 is a symlink to the other,
-    only rust-1.34.0.ebuild will be in the return value.
-    """
-    return [
-        (RustVersion.parse_from_ebuild(ebuild), ebuild)
+def find_stable_rust_version() -> RustVersion:
+    """Returns the RustVersion of the stable dev-lang/rust ebuild."""
+    rust_versions = [
+        RustVersion.parse_from_ebuild(ebuild)
         for ebuild in RUST_PATH.iterdir()
-        if ebuild.suffix == ".ebuild" and not ebuild.is_symlink()
+        if ebuild.suffix == ".ebuild"
+        and not ebuild.name.endswith("-9999.ebuild")
+        and not ebuild.is_symlink()
     ]
-
-
-def find_oldest_rust_version() -> RustVersion:
-    """Returns the RustVersion of the oldest dev-lang/rust ebuild."""
-    rust_versions = find_rust_versions()
-    if len(rust_versions) <= 1:
-        raise RuntimeError("Expect to find more than one Rust versions")
-    return min(rust_versions)[0]
+    if len(rust_versions) != 1:
+        raise RuntimeError(
+            f"Expect to find exactly one Rust version; found {rust_versions}"
+        )
+    return rust_versions[0]
 
 
 def find_ebuild_for_rust_version(version: RustVersion) -> Path:
@@ -898,31 +746,23 @@ def find_ebuild_for_rust_version(version: RustVersion) -> Path:
     return find_ebuild_path(RUST_PATH, "rust", version)
 
 
-def rebuild_packages(version: RustVersion):
+def rebuild_packages(workon_packages: List[str]):
     """Rebuild packages modified by this script."""
-    # Remove all packages we modify to avoid depending on preinstalled
-    # versions. This ensures that the packages can really be built.
-    packages = [f"{category}/{name}" for category, name in RUST_PACKAGES]
-    for pkg in packages:
-        unmerge_package_if_installed(pkg)
-    # Mention only dev-lang/rust explicitly, so that others are pulled
-    # in as dependencies (letting us detect dependency errors).
-    # Packages we modify are listed in --usepkg-exclude to ensure they
-    # are built from source.
     try:
         run_in_chroot(
             [
                 "sudo",
                 "emerge",
+                # Use unlimited jobs, since we should only be emerging a small
+                # handful of packages.
+                "--jobs",
                 "--quiet-build",
-                "--usepkg-exclude",
-                " ".join(packages),
-                f"=dev-lang/rust-{version}",
-            ],
+            ]
+            + workon_packages,
         )
     except:
         logging.warning(
-            "Failed to build dev-lang/rust or one of its dependencies."
+            "Failed to build rust or one of its dependencies."
             " If necessary, you can restore rust and rust-host from"
             " binary packages:\n  sudo emerge --getbinpkgonly dev-lang/rust"
         )
@@ -958,65 +798,11 @@ def remove_files(filename: PathOrStr, path: PathOrStr) -> None:
     subprocess.check_call(["git", "rm", filename], cwd=path)
 
 
-def remove_rust_uprev(
-    rust_version: Optional[RustVersion],
-    run_step: RunStepFn,
-) -> None:
-    def find_desired_rust_version() -> RustVersion:
-        if rust_version:
-            return rust_version
-        return find_oldest_rust_version()
-
-    def find_desired_rust_version_from_json(obj: Any) -> RustVersion:
-        return RustVersion(*obj)
-
-    delete_version = run_step(
-        "find rust version to delete",
-        find_desired_rust_version,
-        result_from_json=find_desired_rust_version_from_json,
-    )
-
-    for category, name in RUST_PACKAGES:
-        run_step(
-            f"remove old {name} ebuild",
-            functools.partial(
-                remove_ebuild_version,
-                EBUILD_PREFIX / category / name,
-                name,
-                delete_version,
-            ),
-        )
-
-    run_step(
-        "update dev-lang/rust-host manifest to delete old version",
-        lambda: ebuild_actions("dev-lang/rust-host", ["manifest"]),
-    )
-    run_step(
-        "remove target version from rust packages",
-        lambda: update_rust_packages(
-            "dev-lang/rust", delete_version, add=False
-        ),
-    )
-    run_step(
-        "remove host version from rust packages",
-        lambda: update_rust_packages(
-            "dev-lang/rust-host", delete_version, add=False
-        ),
-    )
-    run_step("remove virtual/rust", lambda: remove_virtual_rust(delete_version))
-
-
-def remove_virtual_rust(delete_version: RustVersion) -> None:
-    remove_ebuild_version(
-        EBUILD_PREFIX.joinpath("virtual/rust"), "rust", delete_version
-    )
-
-
 def rust_bootstrap_path() -> Path:
     return EBUILD_PREFIX.joinpath("dev-lang/rust-bootstrap")
 
 
-def create_new_repo(rust_version: RustVersion) -> None:
+def create_rust_uprev_branch(rust_version: RustVersion) -> None:
     output = get_command_output(
         ["git", "status", "--porcelain"], cwd=EBUILD_PREFIX
     )
@@ -1025,7 +811,9 @@ def create_new_repo(rust_version: RustVersion) -> None:
             f"{EBUILD_PREFIX} has uncommitted changes, please either discard "
             "them or commit them."
         )
-    git.CreateBranch(EBUILD_PREFIX, f"rust-to-{rust_version}")
+    git_utils.create_branch(
+        EBUILD_PREFIX, branch_name=f"rust-to-{rust_version}"
+    )
 
 
 def build_cross_compiler(template_version: RustVersion) -> None:
@@ -1059,16 +847,25 @@ def build_cross_compiler(template_version: RustVersion) -> None:
 
 def create_new_commit(rust_version: RustVersion) -> None:
     subprocess.check_call(["git", "add", "-A"], cwd=EBUILD_PREFIX)
-    messages = [
-        f"[DO NOT SUBMIT] dev-lang/rust: upgrade to Rust {rust_version}",
-        "",
-        "This CL is created by rust_uprev tool automatically." "",
-        "BUG=None",
-        "TEST=Use CQ to test the new Rust version",
-    ]
-    branch = f"rust-to-{rust_version}"
-    git.CommitChanges(EBUILD_PREFIX, messages)
-    git.UploadChanges(EBUILD_PREFIX, branch)
+    sha = git_utils.commit_all_changes(
+        EBUILD_PREFIX,
+        message=textwrap.dedent(
+            f"""\
+            [DO NOT SUBMIT] dev-lang/rust: upgrade to Rust {rust_version}
+
+            This CL is created by rust_uprev tool automatically.
+
+            BUG=None
+            TEST=Use CQ to test the new Rust version
+            """
+        ),
+    )
+    git_utils.upload_to_gerrit(
+        EBUILD_PREFIX,
+        remote=git_utils.CROS_EXTERNAL_REMOTE,
+        branch=git_utils.CROS_MAIN_BRANCH,
+        ref=sha,
+    )
 
 
 def run_in_chroot(cmd: Command, *args, **kwargs) -> subprocess.CompletedProcess:
@@ -1180,42 +977,17 @@ def main() -> None:
             result_to_json,
         )
 
-    if args.subparser_name == "create":
-        sudo_keepalive()
-        create_rust_uprev(
-            args.rust_version, args.template, args.skip_compile, run_step
-        )
-    elif args.subparser_name == "remove":
-        remove_rust_uprev(args.rust_version, run_step)
-    else:
-        # If you have added more subparser_name, please also add the handlers
-        # above
-        assert args.subparser_name == "roll"
-
-        sudo_keepalive()
-        # Determine the template version, if not given.
-        template_version = args.template
-        if template_version is None:
-            rust_ebuild = find_ebuild_for_package("dev-lang/rust")
-            template_version = RustVersion.parse_from_ebuild(rust_ebuild)
-
-        run_step("create new repo", lambda: create_new_repo(args.uprev))
-        if not args.skip_cross_compiler:
-            run_step(
-                "build cross compiler",
-                lambda: build_cross_compiler(template_version),
-            )
-        create_rust_uprev(
-            args.uprev, template_version, args.skip_compile, run_step
+    sudo_keepalive()
+    template_version = find_stable_rust_version()
+    run_step(
+        "create rust upgrade branch",
+        lambda: create_rust_uprev_branch(args.uprev),
+    )
+    if not args.skip_cross_compiler:
+        run_step(
+            "build cross compiler",
+            lambda: build_cross_compiler(template_version),
         )
-        remove_rust_uprev(args.remove, run_step)
-        prepared = prepare_uprev_from_json(completed_steps["prepare uprev"])
-        assert prepared is not None, "no prepared uprev decoded from JSON"
-        if not args.no_upload:
-            run_step(
-                "create rust uprev CL", lambda: create_new_commit(args.uprev)
-            )
-
-
-if __name__ == "__main__":
-    main()
+    create_rust_uprev(args.uprev, template_version, args.skip_compile, run_step)
+    if not args.no_upload:
+        run_step("create rust uprev CL", lambda: create_new_commit(args.uprev))
diff --git a/rust_tools/rust_uprev_test.py b/rust_tools/rust_uprev_test.py
old mode 100755
new mode 100644
index a90f3d10..bd2924c2
--- a/rust_tools/rust_uprev_test.py
+++ b/rust_tools/rust_uprev_test.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2020 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -7,13 +6,12 @@
 
 import os
 from pathlib import Path
-import shutil
 import subprocess
 import tempfile
 import unittest
 from unittest import mock
 
-from llvm_tools import git
+from cros_utils import git_utils
 
 
 # rust_uprev sets SOURCE_ROOT to the output of `repo --show-toplevel`.
@@ -309,25 +307,21 @@ class FindEbuildPathTest(unittest.TestCase):
             self.assertEqual(result, ebuild)
 
 
-class FindRustVersionsTest(unittest.TestCase):
-    """Tests for rust_uprev.find_rust_versions."""
+class FindStableRustVersionTest(unittest.TestCase):
+    """Tests for rust_uprev.find_stable_rust_version."""
 
     def test_with_symlinks(self):
         with tempfile.TemporaryDirectory() as t:
             tmpdir = Path(t)
-            rust_1_49_1_ebuild = tmpdir / "rust-1.49.1.ebuild"
             rust_1_50_0_ebuild = tmpdir / "rust-1.50.0.ebuild"
             rust_1_50_0_r1_ebuild = tmpdir / "rust-1.50.0-r1.ebuild"
-            rust_1_49_1_ebuild.touch()
+            rust_9999_ebuild = tmpdir / "rust-9999.ebuild"
             rust_1_50_0_ebuild.touch()
             rust_1_50_0_r1_ebuild.symlink_to(rust_1_50_0_ebuild)
+            rust_9999_ebuild.touch()
             with mock.patch("rust_uprev.RUST_PATH", tmpdir):
-                actual = rust_uprev.find_rust_versions()
-                expected = [
-                    (rust_uprev.RustVersion(1, 49, 1), rust_1_49_1_ebuild),
-                    (rust_uprev.RustVersion(1, 50, 0), rust_1_50_0_ebuild),
-                ]
-                self.assertEqual(actual, expected)
+                actual = rust_uprev.find_stable_rust_version()
+                self.assertEqual(actual, rust_uprev.RustVersion(1, 50, 0))
 
 
 class MirrorHasFileTest(unittest.TestCase):
@@ -477,51 +471,6 @@ class RustVersionTest(unittest.TestCase):
         )
 
 
-class PrepareUprevTest(unittest.TestCase):
-    """Tests for prepare_uprev step in rust_uprev"""
-
-    def setUp(self):
-        self.version_old = rust_uprev.RustVersion(1, 2, 3)
-        self.version_new = rust_uprev.RustVersion(1, 3, 5)
-
-    @mock.patch.object(
-        rust_uprev,
-        "find_ebuild_for_rust_version",
-        return_value=Path("/path/to/ebuild"),
-    )
-    @mock.patch.object(rust_uprev, "get_command_output")
-    def test_success_with_template(self, mock_command, _ebuild_for_version):
-        expected = rust_uprev.PreparedUprev(self.version_old)
-        actual = rust_uprev.prepare_uprev(
-            rust_version=self.version_new, template=self.version_old
-        )
-        self.assertEqual(expected, actual)
-        mock_command.assert_not_called()
-
-    @mock.patch.object(
-        rust_uprev,
-        "find_ebuild_for_rust_version",
-        return_value="/path/to/ebuild",
-    )
-    @mock.patch.object(rust_uprev, "get_command_output")
-    def test_return_none_with_template_larger_than_input(
-        self, mock_command, *_args
-    ):
-        ret = rust_uprev.prepare_uprev(
-            rust_version=self.version_old, template=self.version_new
-        )
-        self.assertIsNone(ret)
-        mock_command.assert_not_called()
-
-    def test_prepare_uprev_from_json(self):
-        json_result = (list(self.version_new),)
-        expected = rust_uprev.PreparedUprev(
-            self.version_new,
-        )
-        actual = rust_uprev.prepare_uprev_from_json(json_result)
-        self.assertEqual(expected, actual)
-
-
 class ToggleProfileData(unittest.TestCase):
     """Tests functionality to include or exclude profile data from SRC_URI."""
 
@@ -577,14 +526,20 @@ some code here
             mock_write_text.assert_not_called()
 
 
-class UpdateBootstrapVersionTest(unittest.TestCase):
-    """Tests for update_bootstrap_version step in rust_uprev"""
+class UpdateEbuildVariableVersionTest(unittest.TestCase):
+    """Tests for update_ebuild_variable_version function in rust_uprev"""
 
     ebuild_file_before = """
+SOME_OTHER_VAR=foo
+# Comment
 BOOTSTRAP_VERSION="1.2.0"
+SOME_OTHER_VAR2=baz
     """
     ebuild_file_after = """
+SOME_OTHER_VAR=foo
+# Comment
 BOOTSTRAP_VERSION="1.3.6"
+SOME_OTHER_VAR2=baz
     """
 
     def setUp(self):
@@ -595,8 +550,10 @@ BOOTSTRAP_VERSION="1.3.6"
         # ebuild_file and new bootstrap version are deliberately different
         ebuild_file = "/path/to/rust/cros-rustc.eclass"
         with mock.patch("pathlib.Path.write_text") as mock_write_text:
-            rust_uprev.update_bootstrap_version(
-                ebuild_file, rust_uprev.RustVersion.parse("1.3.6")
+            rust_uprev.update_ebuild_variable_version(
+                ebuild_file,
+                "BOOTSTRAP_VERSION",
+                rust_uprev.RustVersion.parse("1.3.6"),
             )
             mock_write_text.assert_called_once_with(
                 self.ebuild_file_after, encoding="utf-8"
@@ -605,14 +562,16 @@ BOOTSTRAP_VERSION="1.3.6"
     def test_fail_when_ebuild_misses_a_variable(self):
         self.mock_read_text.return_value = ""
         ebuild_file = "/path/to/rust/rust-1.3.5.ebuild"
-        with self.assertRaises(RuntimeError) as context:
-            rust_uprev.update_bootstrap_version(
-                ebuild_file, rust_uprev.RustVersion.parse("1.2.0")
+        with self.assertRaisesRegex(
+            RuntimeError,
+            r"^BOOTSTRAP_VERSION not found in "
+            r"/path/to/rust/rust-1\.3\.5\.ebuild$",
+        ):
+            rust_uprev.update_ebuild_variable_version(
+                ebuild_file,
+                "BOOTSTRAP_VERSION",
+                rust_uprev.RustVersion.parse("1.2.0"),
             )
-        self.assertEqual(
-            "BOOTSTRAP_VERSION not found in /path/to/rust/rust-1.3.5.ebuild",
-            str(context.exception),
-        )
 
 
 class UpdateRustPackagesTests(unittest.TestCase):
@@ -676,146 +635,25 @@ class RustUprevOtherStagesTests(unittest.TestCase):
             rust_uprev.RUST_PATH, "rust-{self.new_version}.ebuild"
         )
 
-    @mock.patch.object(shutil, "copyfile")
-    @mock.patch.object(subprocess, "check_call")
-    def test_create_rust_ebuild(self, mock_call, mock_copy):
-        template_ebuild = (
-            rust_uprev.EBUILD_PREFIX
-            / f"dev-lang/rust/rust-{self.current_version}.ebuild"
-        )
-        new_ebuild = (
-            rust_uprev.EBUILD_PREFIX
-            / f"dev-lang/rust/rust-{self.new_version}.ebuild"
-        )
-        rust_uprev.create_ebuild(
-            "dev-lang", "rust", self.current_version, self.new_version
-        )
-        mock_copy.assert_called_once_with(
-            template_ebuild,
-            new_ebuild,
-        )
-        mock_call.assert_called_once_with(
-            ["git", "add", f"rust-{self.new_version}.ebuild"],
-            cwd=new_ebuild.parent,
-        )
-
-    @mock.patch.object(shutil, "copyfile")
-    @mock.patch.object(subprocess, "check_call")
-    def test_create_rust_host_ebuild(self, mock_call, mock_copy):
-        template_ebuild = (
-            rust_uprev.EBUILD_PREFIX
-            / f"dev-lang/rust-host/rust-host-{self.current_version}.ebuild"
-        )
-        new_ebuild = (
-            rust_uprev.EBUILD_PREFIX
-            / f"dev-lang/rust-host/rust-host-{self.new_version}.ebuild"
-        )
-        rust_uprev.create_ebuild(
-            "dev-lang", "rust-host", self.current_version, self.new_version
-        )
-        mock_copy.assert_called_once_with(
-            template_ebuild,
-            new_ebuild,
-        )
-        mock_call.assert_called_once_with(
-            ["git", "add", f"rust-host-{self.new_version}.ebuild"],
-            cwd=new_ebuild.parent,
-        )
-
-    @mock.patch.object(subprocess, "check_call")
-    def test_remove_virtual_rust(self, mock_call):
-        with tempfile.TemporaryDirectory() as tmpdir:
-            ebuild_path = Path(
-                tmpdir, f"virtual/rust/rust-{self.old_version}.ebuild"
-            )
-            os.makedirs(ebuild_path.parent)
-            ebuild_path.touch()
-            with mock.patch("rust_uprev.EBUILD_PREFIX", Path(tmpdir)):
-                rust_uprev.remove_virtual_rust(self.old_version)
-                mock_call.assert_called_once_with(
-                    ["git", "rm", str(ebuild_path.name)], cwd=ebuild_path.parent
-                )
-
-    @mock.patch.object(subprocess, "check_call")
-    def test_remove_virtual_rust_with_symlink(self, mock_call):
-        with tempfile.TemporaryDirectory() as tmpdir:
-            ebuild_path = Path(
-                tmpdir, f"virtual/rust/rust-{self.old_version}.ebuild"
-            )
-            symlink_path = Path(
-                tmpdir, f"virtual/rust/rust-{self.old_version}-r14.ebuild"
-            )
-            os.makedirs(ebuild_path.parent)
-            ebuild_path.touch()
-            symlink_path.symlink_to(ebuild_path.name)
-            with mock.patch("rust_uprev.EBUILD_PREFIX", Path(tmpdir)):
-                rust_uprev.remove_virtual_rust(self.old_version)
-                mock_call.assert_has_calls(
-                    [
-                        mock.call(
-                            ["git", "rm", ebuild_path.name],
-                            cwd=ebuild_path.parent,
-                        ),
-                        mock.call(
-                            ["git", "rm", symlink_path.name],
-                            cwd=ebuild_path.parent,
-                        ),
-                    ],
-                    any_order=True,
-                )
-
-    @mock.patch.object(rust_uprev, "find_ebuild_path")
-    @mock.patch.object(shutil, "copyfile")
-    @mock.patch.object(subprocess, "check_call")
-    def test_update_virtual_rust(self, mock_call, mock_copy, mock_find_ebuild):
-        ebuild_path = Path(
-            f"/some/dir/virtual/rust/rust-{self.current_version}.ebuild"
-        )
-        mock_find_ebuild.return_value = Path(ebuild_path)
-        rust_uprev.update_virtual_rust(self.current_version, self.new_version)
-        mock_call.assert_called_once_with(
-            ["git", "add", f"rust-{self.new_version}.ebuild"],
-            cwd=ebuild_path.parent,
-        )
-        mock_copy.assert_called_once_with(
-            ebuild_path.parent.joinpath(f"rust-{self.current_version}.ebuild"),
-            ebuild_path.parent.joinpath(f"rust-{self.new_version}.ebuild"),
-        )
-
-    @mock.patch("rust_uprev.find_rust_versions")
-    def test_find_oldest_rust_version_pass(self, rust_versions):
-        oldest_version_name = f"rust-{self.old_version}.ebuild"
-        rust_versions.return_value = [
-            (self.old_version, oldest_version_name),
-            (self.current_version, f"rust-{self.current_version}.ebuild"),
-            (self.new_version, f"rust-{self.new_version}.ebuild"),
-        ]
-        actual = rust_uprev.find_oldest_rust_version()
-        expected = self.old_version
-        self.assertEqual(expected, actual)
-
-    @mock.patch("rust_uprev.find_rust_versions")
-    def test_find_oldest_rust_version_fail_with_only_one_ebuild(
-        self, rust_versions
-    ):
-        rust_versions.return_value = [
-            (self.new_version, f"rust-{self.new_version}.ebuild"),
-        ]
-        with self.assertRaises(RuntimeError) as context:
-            rust_uprev.find_oldest_rust_version()
-        self.assertEqual(
-            "Expect to find more than one Rust versions", str(context.exception)
-        )
-
     @mock.patch.object(rust_uprev, "get_command_output")
-    @mock.patch.object(git, "CreateBranch")
-    def test_create_new_repo(self, mock_branch, mock_output):
+    @mock.patch.object(git_utils, "create_branch")
+    def test_create_rust_upgrade_branch(self, mock_create_branch, mock_output):
         mock_output.return_value = ""
-        rust_uprev.create_new_repo(self.new_version)
-        mock_branch.assert_called_once_with(
-            rust_uprev.EBUILD_PREFIX, f"rust-to-{self.new_version}"
+        rust_uprev.create_rust_uprev_branch(self.new_version)
+        mock_create_branch.assert_called_once_with(
+            rust_uprev.EBUILD_PREFIX, branch_name=f"rust-to-{self.new_version}"
         )
 
+    @mock.patch.object(rust_uprev, "get_command_output")
+    @mock.patch.object(git_utils, "create_branch")
+    def test_create_rust_upgrade_branch_raises_if_unclean(
+        self, mock_create_branch, mock_output
+    ):
+        mock_output.return_value = "some file has modifications"
+        with self.assertRaisesRegex(RuntimeError, ".*uncommitted changes.*"):
+            rust_uprev.create_rust_uprev_branch(self.new_version)
+        mock_create_branch.assert_not_called()
+
     @mock.patch.object(rust_uprev, "run_in_chroot")
     def test_build_cross_compiler(self, mock_run_in_chroot):
         cros_targets = [
@@ -835,7 +673,3 @@ class RustUprevOtherStagesTests(unittest.TestCase):
             ["sudo", "emerge", "-j", "-G"]
             + [f"cross-{x}/gcc" for x in cros_targets + ["arm-none-eabi"]]
         )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/rust_tools/rust_watch.py b/rust_tools/rust_watch.py
old mode 100755
new mode 100644
index ba760e78..8333e0b4
--- a/rust_tools/rust_watch.py
+++ b/rust_tools/rust_watch.py
@@ -1,4 +1,3 @@
-#!/usr/bin/env python3
 # Copyright 2020 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -426,7 +425,3 @@ def main(argv: List[str]) -> None:
             last_gentoo_sha=newest_sha,
         ),
     )
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/rust_tools/rust_watch_test.py b/rust_tools/rust_watch_test.py
old mode 100755
new mode 100644
index 79006f35..c383b7f8
--- a/rust_tools/rust_watch_test.py
+++ b/rust_tools/rust_watch_test.py
@@ -14,7 +14,7 @@ import unittest
 import unittest.mock
 
 from cros_utils import tiny_render
-import rust_watch
+from rust_tools import rust_watch
 
 
 class Test(unittest.TestCase):
@@ -180,7 +180,3 @@ class Test(unittest.TestCase):
                 newest_release=rust_watch.RustReleaseVersion(1, 0, 0),
             )
         )
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/seccomp_tools/mass_seccomp_editor/mass_seccomp_editor.py b/seccomp_tools/mass_seccomp_editor/mass_seccomp_editor.py
old mode 100755
new mode 100644
index 8b283d4d..ee4d4b89
--- a/seccomp_tools/mass_seccomp_editor/mass_seccomp_editor.py
+++ b/seccomp_tools/mass_seccomp_editor/mass_seccomp_editor.py
@@ -1,5 +1,3 @@
-#!/usr/bin/env python3
-
 # Copyright 2021 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -7,12 +5,14 @@
 """Script to make mass, CrOS-wide seccomp changes."""
 
 import argparse
+from dataclasses import dataclass
+from dataclasses import field
 import re
+import shutil
 import subprocess
 import sys
-import shutil
-from typing import Any, Iterable, Optional
-from dataclasses import dataclass, field
+from typing import Any, Dict, Iterable, List, Optional, Set, Tuple
+
 
 # Pre-compiled regexes.
 AMD64_RE = re.compile(r".*(amd|x86_)64.*\.policy")
@@ -25,13 +25,13 @@ ARM_RE = re.compile(r".*arm(v7)?.*\.policy")
 class Policies:
     """Dataclass to hold lists of policies which match certain types."""
 
-    arm: list[str] = field(default_factory=list)
-    x86_64: list[str] = field(default_factory=list)
-    x86: list[str] = field(default_factory=list)
-    arm64: list[str] = field(default_factory=list)
-    none: list[str] = field(default_factory=list)
+    arm: List[str] = field(default_factory=list)
+    x86_64: List[str] = field(default_factory=list)
+    x86: List[str] = field(default_factory=list)
+    arm64: List[str] = field(default_factory=list)
+    none: List[str] = field(default_factory=list)
 
-    def to_dict(self) -> dict[str, list[str]]:
+    def to_dict(self) -> Dict[str, List[str]]:
         """Convert this class to a dictionary."""
         return {**self.__dict__}
 
@@ -66,7 +66,7 @@ def main():
 
     syscall_lookup_table = _make_syscall_lookup_table(args)
 
-    for (type_, val) in separated.to_dict().items():
+    for type_, val in separated.to_dict().items():
         for fp in val:
             syscalls = syscall_lookup_table[type_]
             missing = check_missing_syscalls(syscalls, fp)
@@ -81,7 +81,7 @@ def main():
     if not args.edit:
         sys.exit(0 if success else 2)
 
-    for (type_, val) in separated.to_dict().items():
+    for type_, val in separated.to_dict().items():
         for fp in val:
             syscalls = syscall_lookup_table[type_]
             if args.force:
@@ -96,7 +96,7 @@ def main():
     sys.exit(0 if success else 2)
 
 
-def _make_syscall_lookup_table(args: Any) -> dict[str, list[str]]:
+def _make_syscall_lookup_table(args: Any) -> Dict[str, List[str]]:
     """Make lookup table, segmented by all/b32/b64/none policies.
 
     Args:
@@ -146,7 +146,7 @@ def _confirm_add(fp: str, syscalls: Iterable[str], noninteractive=None):
         print(f"Skipping {fp}")
 
 
-def check_missing_syscalls(syscalls: list[str], fp: str) -> Optional[set[str]]:
+def check_missing_syscalls(syscalls: List[str], fp: str) -> Optional[Set[str]]:
     """Return which specified syscalls are missing in the given file."""
     missing_syscalls = set(syscalls)
     with open(fp) as f:
@@ -161,7 +161,7 @@ def check_missing_syscalls(syscalls: list[str], fp: str) -> Optional[set[str]]:
     return missing_syscalls
 
 
-def _update_seccomp(fp: str, missing_syscalls: list[str]):
+def _update_seccomp(fp: str, missing_syscalls: List[str]):
     """Update the seccomp of the file based on the seccomp change type."""
     with open(fp, "a") as f:
         sorted_syscalls = sorted(missing_syscalls)
@@ -169,7 +169,7 @@ def _update_seccomp(fp: str, missing_syscalls: list[str]):
             f.write(to_write + ": 1\n")
 
 
-def _search_cmd(query: str, use_fd=True) -> list[str]:
+def _search_cmd(query: str, use_fd=True) -> List[str]:
     if use_fd and shutil.which("fdfind") is not None:
         return [
             "fdfind",
@@ -188,7 +188,7 @@ def _search_cmd(query: str, use_fd=True) -> list[str]:
     ]
 
 
-def find_potential_policy_files(packages: list[str]) -> tuple[list[str], bool]:
+def find_potential_policy_files(packages: List[str]) -> Tuple[List[str], bool]:
     """Find potentially related policy files to the given packages.
 
     Returns:
@@ -280,7 +280,3 @@ def parse_args() -> Any:
         " Does nothing without --edit.",
     )
     return parser.parse_args()
-
-
-if __name__ == "__main__":
-    main()
diff --git a/seccomp_tools/mass_seccomp_editor/test_mass_seccomp_editor.py b/seccomp_tools/mass_seccomp_editor/mass_seccomp_editor_test.py
old mode 100755
new mode 100644
similarity index 91%
rename from seccomp_tools/mass_seccomp_editor/test_mass_seccomp_editor.py
rename to seccomp_tools/mass_seccomp_editor/mass_seccomp_editor_test.py
index c1693da5..bfd6308c
--- a/seccomp_tools/mass_seccomp_editor/test_mass_seccomp_editor.py
+++ b/seccomp_tools/mass_seccomp_editor/mass_seccomp_editor_test.py
@@ -9,7 +9,7 @@
 import unittest
 from unittest import mock
 
-import mass_seccomp_editor
+from seccomp_tools.mass_seccomp_editor import mass_seccomp_editor
 
 BASE_SECCOMP_CONTENTS = """
 fstat: 1
@@ -32,7 +32,3 @@ class TestMassSeccompEditor(unittest.TestCase):
                 ["fstat", "dup", "fizzbuzz"], TEST_FP
             )
         self.assertEqual(out, set(["dup", "fizzbuzz"]))
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/tc_enter_chroot.py b/tc_enter_chroot.py
deleted file mode 100755
index 93b3be2d..00000000
--- a/tc_enter_chroot.py
+++ /dev/null
@@ -1,349 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2010 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Script to enter the ChromeOS chroot with mounted sources.
-
-This script enters the chroot with mounted sources.
-"""
-
-
-__author__ = "asharif@google.com (Ahmad Sharif)"
-
-import argparse
-import getpass
-import os
-import pwd
-import sys
-
-from cros_utils import command_executer
-from cros_utils import logger
-from cros_utils import misc
-
-
-class MountPoint(object):
-    """Mount point class"""
-
-    def __init__(self, external_dir, mount_dir, owner, options=None):
-        self.external_dir = os.path.realpath(external_dir)
-        self.mount_dir = os.path.realpath(mount_dir)
-        self.owner = owner
-        self.options = options
-
-    def CreateAndOwnDir(self, dir_name):
-        retv = 0
-        if not os.path.exists(dir_name):
-            command = "mkdir -p " + dir_name
-            command += " || sudo mkdir -p " + dir_name
-            retv = command_executer.GetCommandExecuter().RunCommand(command)
-        if retv != 0:
-            return retv
-        pw = pwd.getpwnam(self.owner)
-        if os.stat(dir_name).st_uid != pw.pw_uid:
-            command = "sudo chown -f " + self.owner + " " + dir_name
-            retv = command_executer.GetCommandExecuter().RunCommand(command)
-        return retv
-
-    def DoMount(self):
-        ce = command_executer.GetCommandExecuter()
-        mount_signature = "%s on %s" % (self.external_dir, self.mount_dir)
-        command = "mount"
-        retv, out, _ = ce.RunCommandWOutput(command)
-        if mount_signature not in out:
-            retv = self.CreateAndOwnDir(self.mount_dir)
-            logger.GetLogger().LogFatalIf(retv, "Cannot create mount_dir!")
-            retv = self.CreateAndOwnDir(self.external_dir)
-            logger.GetLogger().LogFatalIf(retv, "Cannot create external_dir!")
-            retv = self.MountDir()
-            logger.GetLogger().LogFatalIf(retv, "Cannot mount!")
-            return retv
-        else:
-            return 0
-
-    def UnMount(self):
-        ce = command_executer.GetCommandExecuter()
-        return ce.RunCommand("sudo umount %s" % self.mount_dir)
-
-    def MountDir(self):
-        command = (
-            "sudo mount --bind " + self.external_dir + " " + self.mount_dir
-        )
-        if self.options == "ro":
-            command += " && sudo mount --bind -oremount,ro " + self.mount_dir
-        retv = command_executer.GetCommandExecuter().RunCommand(command)
-        return retv
-
-    def __str__(self):
-        ret = ""
-        ret += self.external_dir + "\n"
-        ret += self.mount_dir + "\n"
-        if self.owner:
-            ret += self.owner + "\n"
-        if self.options:
-            ret += self.options + "\n"
-        return ret
-
-
-def Main(argv, return_output=False):
-    """The main function."""
-
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "-c",
-        "--chromeos_root",
-        dest="chromeos_root",
-        default="../..",
-        help="ChromeOS root checkout directory.",
-    )
-    parser.add_argument(
-        "-t",
-        "--toolchain_root",
-        dest="toolchain_root",
-        help="Toolchain root directory.",
-    )
-    parser.add_argument(
-        "-o", "--output", dest="output", help="Toolchain output directory"
-    )
-    parser.add_argument(
-        "--sudo",
-        dest="sudo",
-        action="store_true",
-        default=False,
-        help="Run the command with sudo.",
-    )
-    parser.add_argument(
-        "-r",
-        "--third_party",
-        dest="third_party",
-        help="The third_party directory to mount.",
-    )
-    parser.add_argument(
-        "-m",
-        "--other_mounts",
-        dest="other_mounts",
-        help="Other mount points in the form: " "dir:mounted_dir:options",
-    )
-    parser.add_argument(
-        "-s",
-        "--mount-scripts-only",
-        dest="mount_scripts_only",
-        action="store_true",
-        default=False,
-        help="Mount only the scripts dir, and not the sources.",
-    )
-    parser.add_argument(
-        "passthrough_argv",
-        nargs="*",
-        help="Command to be executed inside the chroot.",
-    )
-
-    options = parser.parse_args(argv)
-
-    chromeos_root = options.chromeos_root
-
-    chromeos_root = os.path.expanduser(chromeos_root)
-    if options.toolchain_root:
-        options.toolchain_root = os.path.expanduser(options.toolchain_root)
-
-    chromeos_root = os.path.abspath(chromeos_root)
-
-    tc_dirs = []
-    if options.toolchain_root is None or options.mount_scripts_only:
-        m = "toolchain_root not specified. Will not mount toolchain dirs."
-        logger.GetLogger().LogWarning(m)
-    else:
-        tc_dirs = [
-            options.toolchain_root + "/google_vendor_src_branch/gcc",
-            options.toolchain_root + "/google_vendor_src_branch/binutils",
-        ]
-
-    for tc_dir in tc_dirs:
-        if not os.path.exists(tc_dir):
-            logger.GetLogger().LogError(
-                "toolchain path " + tc_dir + " does not exist!"
-            )
-            parser.print_help()
-            sys.exit(1)
-
-    if not os.path.exists(chromeos_root):
-        logger.GetLogger().LogError(
-            "chromeos_root " + options.chromeos_root + " does not exist!"
-        )
-        parser.print_help()
-        sys.exit(1)
-
-    if not os.path.exists(chromeos_root + "/src/scripts/build_packages"):
-        logger.GetLogger().LogError(
-            options.chromeos_root + "/src/scripts/build_packages" " not found!"
-        )
-        parser.print_help()
-        sys.exit(1)
-
-    version_dir = os.path.realpath(
-        os.path.expanduser(os.path.dirname(__file__))
-    )
-
-    mounted_tc_root = "/usr/local/toolchain_root"
-    full_mounted_tc_root = chromeos_root + "/chroot/" + mounted_tc_root
-    full_mounted_tc_root = os.path.abspath(full_mounted_tc_root)
-
-    mount_points = []
-    for tc_dir in tc_dirs:
-        last_dir = misc.GetRoot(tc_dir)[1]
-        mount_point = MountPoint(
-            tc_dir,
-            full_mounted_tc_root + "/" + last_dir,
-            getpass.getuser(),
-            "ro",
-        )
-        mount_points.append(mount_point)
-
-    # Add the third_party mount point if it exists
-    if options.third_party:
-        third_party_dir = options.third_party
-        logger.GetLogger().LogFatalIf(
-            not os.path.isdir(third_party_dir),
-            "--third_party option is not a valid dir.",
-        )
-    else:
-        third_party_dir = os.path.abspath(
-            "%s/../../../third_party" % os.path.dirname(__file__)
-        )
-
-    if os.path.isdir(third_party_dir):
-        mount_point = MountPoint(
-            third_party_dir,
-            (
-                "%s/%s"
-                % (full_mounted_tc_root, os.path.basename(third_party_dir))
-            ),
-            getpass.getuser(),
-        )
-        mount_points.append(mount_point)
-
-    output = options.output
-    if output is None and options.toolchain_root:
-        # Mount the output directory at /usr/local/toolchain_root/output
-        output = options.toolchain_root + "/output"
-
-    if output:
-        mount_points.append(
-            MountPoint(
-                output, full_mounted_tc_root + "/output", getpass.getuser()
-            )
-        )
-
-    # Mount the other mount points
-    mount_points += CreateMountPointsFromString(
-        options.other_mounts, chromeos_root + "/chroot/"
-    )
-
-    last_dir = misc.GetRoot(version_dir)[1]
-
-    # Mount the version dir (v14) at /usr/local/toolchain_root/v14
-    mount_point = MountPoint(
-        version_dir, full_mounted_tc_root + "/" + last_dir, getpass.getuser()
-    )
-    mount_points.append(mount_point)
-
-    for mount_point in mount_points:
-        retv = mount_point.DoMount()
-        if retv != 0:
-            return retv
-
-    # Finally, create the symlink to build-gcc.
-    command = "sudo chown " + getpass.getuser() + " " + full_mounted_tc_root
-    retv = command_executer.GetCommandExecuter().RunCommand(command)
-
-    try:
-        CreateSymlink(
-            last_dir + "/build-gcc", full_mounted_tc_root + "/build-gcc"
-        )
-        CreateSymlink(
-            last_dir + "/build-binutils",
-            full_mounted_tc_root + "/build-binutils",
-        )
-    except Exception as e:
-        logger.GetLogger().LogError(str(e))
-
-    # Now call cros_sdk --enter with the rest of the arguments.
-    command = "cd %s/src/scripts && cros_sdk --enter" % chromeos_root
-
-    if len(options.passthrough_argv) > 1:
-        inner_command = " ".join(options.passthrough_argv[1:])
-        inner_command = inner_command.strip()
-        if inner_command.startswith("-- "):
-            inner_command = inner_command[3:]
-        command_file = "tc_enter_chroot.cmd"
-        command_file_path = chromeos_root + "/src/scripts/" + command_file
-        retv = command_executer.GetCommandExecuter().RunCommand(
-            "sudo rm -f " + command_file_path
-        )
-        if retv != 0:
-            return retv
-        with open(command_file_path, "w", encoding="utf-8") as f:
-            f.write(inner_command)
-        logger.GetLogger().LogCmd(inner_command)
-        retv = command_executer.GetCommandExecuter().RunCommand(
-            "chmod +x " + command_file_path
-        )
-        if retv != 0:
-            return retv
-
-        if options.sudo:
-            command += " sudo ./" + command_file
-        else:
-            command += " ./" + command_file
-        retv = command_executer.GetCommandExecuter().RunCommandGeneric(
-            command, return_output
-        )
-        return retv
-    else:
-        os.chdir("%s/src/scripts" % chromeos_root)
-        ce = command_executer.GetCommandExecuter()
-        _, out, _ = ce.RunCommandWOutput("which cros_sdk")
-        cros_sdk_binary = out.split()[0]
-        return os.execv(cros_sdk_binary, ["", "--enter"])
-
-
-def CreateMountPointsFromString(mount_strings, chroot_dir):
-    # String has options in the form dir:mount:options
-    mount_points = []
-    if not mount_strings:
-        return mount_points
-    mount_list = mount_strings.split()
-    for mount_string in mount_list:
-        mount_values = mount_string.split(":")
-        external_dir = mount_values[0]
-        mount_dir = mount_values[1]
-        if len(mount_values) > 2:
-            options = mount_values[2]
-        else:
-            options = None
-        mount_point = MountPoint(
-            external_dir,
-            chroot_dir + "/" + mount_dir,
-            getpass.getuser(),
-            options,
-        )
-        mount_points.append(mount_point)
-    return mount_points
-
-
-def CreateSymlink(target, link_name):
-    logger.GetLogger().LogFatalIf(
-        target.startswith("/"), "Can't create symlink to absolute path!"
-    )
-    real_from_file = misc.GetRoot(link_name)[0] + "/" + target
-    if os.path.realpath(real_from_file) != os.path.realpath(link_name):
-        if os.path.exists(link_name):
-            command = "rm -rf " + link_name
-            command_executer.GetCommandExecuter().RunCommand(command)
-        os.symlink(target, link_name)
-
-
-if __name__ == "__main__":
-    retval = Main(sys.argv)
-    sys.exit(retval)
diff --git a/toolchain_utils_githooks/check-presubmit.py b/toolchain_utils_githooks/check-presubmit.py
index 127726ad..724ee342 100755
--- a/toolchain_utils_githooks/check-presubmit.py
+++ b/toolchain_utils_githooks/check-presubmit.py
@@ -1,5 +1,4 @@
 #!/usr/bin/env python3
-#
 # Copyright 2019 The ChromiumOS Authors
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
@@ -9,6 +8,7 @@
 import argparse
 import dataclasses
 import datetime
+import functools
 import multiprocessing
 import multiprocessing.pool
 import os
@@ -22,6 +22,7 @@ import textwrap
 import threading
 import traceback
 from typing import (
+    Callable,
     Dict,
     Iterable,
     List,
@@ -33,17 +34,6 @@ from typing import (
 )
 
 
-# This was originally had many packages in it (notably scipy)
-# but due to changes in how scipy is built, we can no longer install
-# it in the chroot. See b/284489250
-#
-# For type checking Python code, we also need mypy. This isn't
-# listed here because (1) only very few files are actually type checked,
-# so we don't pull the dependency in unless needed, and (2) mypy
-# may be installed through other means than pip.
-PIP_DEPENDENCIES = ("numpy",)
-
-
 # Each checker represents an independent check that's done on our sources.
 #
 # They should:
@@ -68,6 +58,14 @@ CheckResult = NamedTuple(
 Command = Sequence[Union[str, os.PathLike]]
 CheckResults = Union[List[Tuple[str, CheckResult]], CheckResult]
 
+# Environment variable that's set to a nonempty value on bots. Used for
+# skipping some tasks on CI. Other presubmit checks detect whether a bot is
+# running the check in a similar way.
+SWARMING_TASK_ID_ENV = "SWARMING_TASK_ID"
+
+# Environment variables to forward to the `cros_sdk` invocation, if we're
+# re-execing in the chroot.
+CHROOT_FORWARDED_ENV = (SWARMING_TASK_ID_ENV,)
 
 # The files and directories on which we run the mypy typechecker. The paths are
 # relative to the root of the toolchain-utils repository.
@@ -84,6 +82,9 @@ MYPY_CHECKED_PATHS = (
     "toolchain_utils_githooks/check-presubmit.py",
 )
 
+# Path to the script that lints changes to ${toolchain_utils}/llvm_patches.
+LINT_LLVM_PATCHES_SCRIPT = "llvm_tools/lint_llvm_patches.py"
+
 
 def run_command_unchecked(
     command: Command,
@@ -140,7 +141,7 @@ class MyPyInvocation:
     pythonpath_additions: str
 
 
-def get_mypy() -> Optional[MyPyInvocation]:
+def maybe_get_or_install_mypy() -> Optional[MyPyInvocation]:
     """Finds the mypy executable and returns a command to invoke it.
 
     If mypy cannot be found and we're inside the chroot, this
@@ -156,6 +157,7 @@ def get_mypy() -> Optional[MyPyInvocation]:
     """
     if has_executable_on_path("mypy"):
         return MyPyInvocation(command=["mypy"], pythonpath_additions="")
+
     pip = get_pip()
     if not pip:
         assert not is_in_chroot()
@@ -185,7 +187,6 @@ def get_mypy() -> Optional[MyPyInvocation]:
         return from_pip
 
     if is_in_chroot():
-        assert pip is not None
         subprocess.check_call(pip + ["install", "--user", "mypy"])
         return get_from_pip()
     return None
@@ -392,7 +393,20 @@ def check_mypy(
     # Prefix output with the version information.
     prefix = f"Using {output.strip()}, "
 
-    cmd = mypy.command + ["--follow-imports=silent"] + list(files)
+    cmd = list(mypy.command)
+    cmd += (
+        # Suppress mypy errors in files that aren't specified on `argv`. Until
+        # toolchain-utils is overwhelmingly mypy-clean, this has to be the
+        # default.
+        "--follow-imports=silent",
+        # b/338058766: in toolchain-utils, mypy will infer that each file
+        # passed in is a package of its own. This leads to errors if one
+        # file transitively imports another. `--explicit-package-bases` causes
+        # mypy to treat $CWD (and other env var values) as the only package
+        # bases, and $CWD == toolchain_utils_root.
+        "--explicit-package-bases",
+    )
+    cmd += files
     exit_code, output = run_command_unchecked(
         cmd, cwd=toolchain_utils_root, env=fixed_env
     )
@@ -460,14 +474,20 @@ def check_py_format(
     files: Iterable[str],
 ) -> CheckResults:
     """Runs black on files to check for style bugs. Also checks for #!s."""
-    black = "black"
-    if not has_executable_on_path(black):
-        return CheckResult(
-            ok=False,
-            output="black isn't available on your $PATH. Please either "
-            "enter a chroot, or place depot_tools on your $PATH.",
-            autofix_commands=[],
-        )
+    # Prefer the `black` that ships with chromite first, as that's properly
+    # version-controlled.
+    black = os.path.normpath(
+        os.path.join(toolchain_utils_root, "../../../chromite/scripts/black")
+    )
+    if not os.path.exists(black):
+        black = "black"
+        if not has_executable_on_path(black):
+            return CheckResult(
+                ok=False,
+                output="black isn't available on your $PATH. Please either "
+                "enter a chroot, or place depot_tools on your $PATH.",
+                autofix_commands=[],
+            )
 
     python_files = [f for f in remove_deleted_files(files) if f.endswith(".py")]
     if not python_files:
@@ -526,11 +546,20 @@ def is_file_in_any_of(file: Path, files_and_dirs: List[Path]) -> bool:
 
 
 def check_py_types(
+    mypy: Optional[MyPyInvocation],
     toolchain_utils_root: str,
     thread_pool: multiprocessing.pool.ThreadPool,
     files: Iterable[str],
 ) -> CheckResults:
     """Runs static type checking for files in MYPY_CHECKED_FILES."""
+    if not mypy:
+        return CheckResult(
+            ok=False,
+            output="mypy not found. Please either enter a chroot "
+            "or install mypy",
+            autofix_commands=[],
+        )
+
     path_root = Path(toolchain_utils_root)
     check_locations = [path_root / x for x in MYPY_CHECKED_PATHS]
     to_check = [
@@ -546,15 +575,6 @@ def check_py_types(
             autofix_commands=[],
         )
 
-    mypy = get_mypy()
-    if not mypy:
-        return CheckResult(
-            ok=False,
-            output="mypy not found. Please either enter a chroot "
-            "or install mypy",
-            autofix_commands=[],
-        )
-
     tasks = [
         (
             "check_mypy",
@@ -579,12 +599,18 @@ def check_cros_lint(
 
     fixed_env = env_with_pythonpath(toolchain_utils_root)
 
+    # b/404578092: if `cros lint` is given absolute paths, it may skip
+    # linting for reasons that are unclear as of the time of writing.
+    # Just give it relative paths, since it's already invoked
+    # from toolchain_utils_root.
+    fixed_files = [os.path.relpath(x, toolchain_utils_root) for x in files]
+
     # We have to support users who don't have a chroot. So we either run `cros
     # lint` (if it's been made available to us), or we try a mix of
-    # pylint+golint.
+    # pylint+staticcheck.
     def try_run_cros_lint(cros_binary: str) -> Optional[CheckResult]:
         exit_code, output = run_command_unchecked(
-            [cros_binary, "lint", "--"] + list(files),
+            [cros_binary, "lint", "--"] + fixed_files,
             toolchain_utils_root,
             env=fixed_env,
         )
@@ -636,15 +662,15 @@ def check_cros_lint(
     go_files = [f for f in remove_deleted_files(files) if f.endswith(".go")]
     if go_files:
 
-        def run_golint() -> CheckResult:
-            if has_executable_on_path("golint"):
+        def run_staticcheck() -> CheckResult:
+            if has_executable_on_path("staticcheck"):
                 return check_result_from_command(
-                    ["golint", "-set_exit_status"] + go_files
+                    ["staticcheck", "-checks", "inherit,-SA1019"] + go_files
                 )
 
             complaint = (
-                "WARNING: go linting disabled. golint is not on your $PATH.\n"
-                "Please either enter a chroot, or install go locally. "
+                "WARNING: go linting disabled. staticcheck is not on your "
+                "$PATH.\nPlease either enter a chroot, or install go locally. "
                 "Continuing."
             )
             return CheckResult(
@@ -653,7 +679,7 @@ def check_cros_lint(
                 autofix_commands=[],
             )
 
-        tasks.append(("golint", thread_pool.apply_async(run_golint)))
+        tasks.append(("staticcheck", thread_pool.apply_async(run_staticcheck)))
 
     complaint = (
         "WARNING: No ChromeOS checkout detected, and no viable CrOS tree\n"
@@ -724,18 +750,30 @@ def check_go_format(toolchain_utils_root, _thread_pool, files):
     )
 
 
+def is_running_on_bot() -> bool:
+    """Returns True if this script is executing on a bot."""
+    return bool(os.environ.get(SWARMING_TASK_ID_ENV))
+
+
 def check_no_compiler_wrapper_changes(
     toolchain_utils_root: str,
     _thread_pool: multiprocessing.pool.ThreadPool,
-    files: List[str],
+    files: Iterable[str],
 ) -> CheckResult:
+    if is_running_on_bot():
+        return CheckResult(
+            ok=True,
+            output="Skipping compiler_wrapper change detection on bot",
+            autofix_commands=[],
+        )
+
     compiler_wrapper_prefix = (
         os.path.join(toolchain_utils_root, "compiler_wrapper") + "/"
     )
     if not any(x.startswith(compiler_wrapper_prefix) for x in files):
         return CheckResult(
             ok=True,
-            output="no compiler_wrapper changes detected",
+            output="No compiler_wrapper changes detected",
             autofix_commands=[],
         )
 
@@ -758,12 +796,16 @@ def check_no_compiler_wrapper_changes(
 def check_tests(
     toolchain_utils_root: str,
     _thread_pool: multiprocessing.pool.ThreadPool,
-    files: List[str],
+    files: Iterable[str],
 ) -> CheckResult:
     """Runs tests."""
+    run_tests_for = os.path.join(
+        toolchain_utils_root, "py", "bin", "run_tests_for.py"
+    )
+    cmd = [run_tests_for, "--"]
+    cmd += files
     exit_code, stdout_and_stderr = run_command_unchecked(
-        [os.path.join(toolchain_utils_root, "run_tests_for.py"), "--"] + files,
-        toolchain_utils_root,
+        cmd, toolchain_utils_root
     )
     return CheckResult(
         ok=exit_code == 0,
@@ -890,7 +932,7 @@ def is_in_chroot() -> bool:
 
 
 def maybe_reexec_inside_chroot(
-    autofix: bool, install_deps_only: bool, files: List[str]
+    autofix: bool, install_deps_only: bool, infer_files: bool, files: List[str]
 ) -> None:
     if is_in_chroot():
         return
@@ -932,6 +974,14 @@ def maybe_reexec_inside_chroot(
     args = [
         "cros_sdk",
         "--enter",
+    ]
+
+    for env_var in CHROOT_FORWARDED_ENV:
+        val = os.environ.get(env_var)
+        if val is not None:
+            args.append(f"{env_var}={val}")
+
+    args += [
         "--",
         rebase_path(__file__),
     ]
@@ -940,6 +990,8 @@ def maybe_reexec_inside_chroot(
         args.append("--no_autofix")
     if install_deps_only:
         args.append("--install_deps_only")
+    if infer_files:
+        args.append("--infer_files")
     args.extend(rebase_path(x) for x in files)
 
     if chdir_to is None:
@@ -960,16 +1012,92 @@ def can_import_py_module(module: str) -> bool:
     return exit_code == 0
 
 
-def ensure_pip_deps_installed() -> None:
-    if not PIP_DEPENDENCIES:
-        # No need to install pip if we don't have any deps.
-        return
+def infer_files_from_env_or_die(toolchain_utils_root: Path) -> List[str]:
+    env = os.environ
 
-    pip = get_pip()
-    assert pip, "pip not found and could not be installed"
+    # If we have PRESUBMIT_FILES, use those. It's a newline-delimeted list.
+    if presubmit_files := env.get("PRESUBMIT_FILES"):
+        return [x.strip() for x in presubmit_files.splitlines()]
+
+    # Otherwise, we're probably executing in the context of a
+    # fullcheckout-presubmit builder. These commit patches locally, then set up
+    # a branch with a properly-init'ed upstream for us. Scrape the diff between
+    # HEAD and that to determine what to lint.
+    upstream = subprocess.run(
+        ["git", "rev-parse", "@{u}"],
+        check=False,
+        cwd=toolchain_utils_root,
+        encoding="utf-8",
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+        stderr=subprocess.DEVNULL,
+    )
+    if upstream.returncode:
+        raise ValueError(
+            "No upstream could be parsed for inference - "
+            "make sure you're running on a branch."
+        )
+    upstream_main = upstream.stdout.strip()
+
+    # On builders, merge-base isn't necessary, but in case a dev is running
+    # this locally, this is helpful (e.g., if a dev has `git fetch`ed but not
+    # rebased, we don't want the newly-fetched diffs to show up in the git diff
+    # output).
+    merge_base = subprocess.run(
+        ["git", "merge-base", "HEAD", upstream_main],
+        check=True,
+        cwd=toolchain_utils_root,
+        encoding="utf-8",
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+    ).stdout.strip()
+
+    diff = subprocess.run(
+        ["git", "diff", merge_base],
+        check=True,
+        cwd=toolchain_utils_root,
+        encoding="utf-8",
+        stdin=subprocess.DEVNULL,
+        stdout=subprocess.PIPE,
+    ).stdout.strip()
+    if not diff:
+        raise ValueError(f"There's no diff between HEAD and {merge_base}.")
+
+    # N.B., if files are only deleted (`+++ /dev/null`), this will have no
+    # matches. That's fine.
+    return [
+        os.path.join(toolchain_utils_root, x)
+        for x in re.findall(r"^\+\+\+ b/([^\n]+)$", diff, re.MULTILINE)
+    ]
 
-    for package in PIP_DEPENDENCIES:
-        subprocess.check_call(pip + ["install", "--user", package])
+
+def files_that_modify_patches_checks(
+    toolchain_utils_root: str, files: List[str]
+) -> List[str]:
+    llvm_patches = os.path.join(toolchain_utils_root, "llvm_patches/")
+    return [
+        x
+        for x in files
+        if x.startswith(llvm_patches) or x.endswith(LINT_LLVM_PATCHES_SCRIPT)
+    ]
+
+
+def check_patches_subdir(
+    toolchain_utils_root: str,
+    _thread_pool: multiprocessing.pool.ThreadPool,
+    _files: Iterable[str],
+) -> CheckResult:
+    check_script = (
+        Path(toolchain_utils_root) / "py" / "bin" / LINT_LLVM_PATCHES_SCRIPT
+    )
+    return_code, stdstreams = run_command_unchecked(
+        [check_script], cwd=toolchain_utils_root
+    )
+    return CheckResult(
+        ok=return_code == 0,
+        output=stdstreams,
+        autofix_commands=[],
+    )
 
 
 def main(argv: List[str]) -> int:
@@ -994,21 +1122,41 @@ def main(argv: List[str]) -> int:
         being run, and quit. This skips all actual checking.
         """,
     )
+    parser.add_argument(
+        "--infer_files",
+        action="store_true",
+        help="""
+        If passed, the file list will be inferred from git state and the
+        environment. This is mutually exclusive with passing `files`.
+        """,
+    )
     parser.add_argument("files", nargs="*")
     opts = parser.parse_args(argv)
 
-    files = opts.files
     install_deps_only = opts.install_deps_only
-    if not files and not install_deps_only:
-        return 0
+    infer_files = opts.infer_files
+    files = opts.files
 
+    toolchain_utils_root = detect_toolchain_utils_root()
     if opts.enter_chroot:
-        maybe_reexec_inside_chroot(opts.autofix, install_deps_only, files)
+        maybe_reexec_inside_chroot(
+            opts.autofix, install_deps_only, infer_files, files
+        )
 
+    if not install_deps_only:
+        if bool(files) == infer_files:
+            parser.error(
+                "Either `--infer_files` or a list of files must be passed, "
+                "not both."
+            )
+        if infer_files:
+            files = infer_files_from_env_or_die(Path(toolchain_utils_root))
+            print(f"Inferred files to check: {files}")
+
+    mypy = maybe_get_or_install_mypy()
     # If you ask for --no_enter_chroot, you're on your own for installing these
     # things.
     if is_in_chroot():
-        ensure_pip_deps_installed()
         if install_deps_only:
             print(
                 "Dependency installation complete & --install_deps_only "
@@ -1022,18 +1170,42 @@ def main(argv: List[str]) -> int:
 
     files = [os.path.abspath(f) for f in files]
 
-    # Note that we extract .__name__s from these, so please name them in a
-    # user-friendly way.
-    checks = (
-        check_cros_lint,
-        check_py_format,
-        check_py_types,
-        check_go_format,
-        check_tests,
-        check_no_compiler_wrapper_changes,
-    )
+    CheckFn = Callable[
+        [str, multiprocessing.pool.ThreadPool, Iterable[str]], CheckResults
+    ]
 
-    toolchain_utils_root = detect_toolchain_utils_root()
+    style_exempt_files = {
+        # This file is mirrored from upstream llvm, so style checks need not
+        # apply.
+        os.path.join(toolchain_utils_root, "llvm_tools/revert_checker.py"),
+    }
+
+    style_checked_files = []
+    for f in files:
+        if f in style_exempt_files:
+            print(f"NOTE: Skipping some checks on {f}; it's style-exempt.")
+        else:
+            style_checked_files.append(f)
+
+    checks: List[Tuple[str, CheckFn, List[str]]] = [
+        ("check_cros_lint", check_cros_lint, style_checked_files),
+        ("check_py_format", check_py_format, style_checked_files),
+        (
+            "check_py_types",
+            functools.partial(check_py_types, mypy),
+            style_checked_files,
+        ),
+        ("check_go_format", check_go_format, style_checked_files),
+        ("check_tests", check_tests, files),
+        (
+            "check_no_compiler_wrapper_changes",
+            check_no_compiler_wrapper_changes,
+            files,
+        ),
+    ]
+
+    if x := files_that_modify_patches_checks(toolchain_utils_root, files):
+        checks.append(("check_patches_subdir", check_patches_subdir, x))
 
     # NOTE: As mentioned above, checks can block on threads they spawn in this
     # pool, so we need at least len(checks)+1 threads to avoid deadlock. Use *2
@@ -1044,16 +1216,24 @@ def main(argv: List[str]) -> int:
     # For our single print statement...
     spawn_print_lock = threading.RLock()
 
-    def run_check(check_fn):
-        name = check_fn.__name__
+    def run_check(
+        arg: Tuple[str, CheckFn, Iterable[str]],
+    ) -> Optional[Tuple[str, CheckResults]]:
+        name, check_fn, files = arg
         with spawn_print_lock:
+            if not files:
+                print("*** Skipping %s; no applicable files")
+                return None
             print("*** Spawning %s" % name)
         return name, check_fn(toolchain_utils_root, pool, files)
 
     with multiprocessing.pool.ThreadPool(num_threads) as pool:
         all_checks_ok = True
         all_autofix_commands = []
-        for check_name, result in pool.imap_unordered(run_check, checks):
+        for run_result in pool.imap_unordered(run_check, checks):
+            if not run_result:
+                continue
+            check_name, result = run_result
             ok, autofix_commands = process_check_result(
                 check_name, result, start_time
             )
diff --git a/update_telemetry_defaults.py b/update_telemetry_defaults.py
deleted file mode 100755
index 929ff07e..00000000
--- a/update_telemetry_defaults.py
+++ /dev/null
@@ -1,208 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-# Copyright 2020 The ChromiumOS Authors
-# Use of this source code is governed by a BSD-style license that can be
-# found in the LICENSE file.
-
-"""Script to maintain the Telemetry benchmark default results file.
-
-This script allows the user to see and update the set of default
-results to be used in generating reports from running the Telemetry
-benchmarks.
-"""
-
-
-__author__ = "cmtice@google.com (Caroline Tice)"
-
-import json
-import os
-import sys
-
-from cros_utils import misc
-
-
-Defaults = {}
-
-
-class TelemetryDefaults(object):
-    """Class for handling telemetry default return result fields."""
-
-    DEFAULTS_FILE_NAME = "crosperf/default-telemetry-results.json"
-
-    def __init__(self):
-        # Get the Crosperf directory; that is where the defaults
-        # file should be.
-        dirname, __ = misc.GetRoot(__file__)
-        fullname = os.path.join(dirname, self.DEFAULTS_FILE_NAME)
-        self._filename = fullname
-        self._defaults = {}
-
-    def ReadDefaultsFile(self):
-        if os.path.exists(self._filename):
-            with open(self._filename, "r", encoding="utf-8") as fp:
-                self._defaults = json.load(fp)
-
-    def WriteDefaultsFile(self):
-        with open(self._filename, "w", encoding="utf-8") as fp:
-            json.dump(self._defaults, fp, indent=2)
-
-    def ListCurrentDefaults(self, benchmark="all"):
-        # Show user current defaults. By default, show all.  The user
-        # can specify the name of a particular benchmark to see only that
-        # benchmark's default values.
-        if len(self._defaults) == 0:
-            print("The benchmark default results are currently empty.")
-        if benchmark == "all":
-            for b in self._defaults.keys():
-                results = self._defaults[b]
-                out_str = b + " : "
-                for r in results:
-                    out_str += r + " "
-                print(out_str)
-        elif benchmark in self._defaults:
-            results = self._defaults[benchmark]
-            out_str = benchmark + " : "
-            for r in results:
-                out_str += r + " "
-            print(out_str)
-        else:
-            print("Error:  Unrecognized benchmark '%s'" % benchmark)
-
-    def AddDefault(self, benchmark, result):
-        if benchmark in self._defaults:
-            resultList = self._defaults[benchmark]
-        else:
-            resultList = []
-        resultList.append(result)
-        self._defaults[benchmark] = resultList
-        print("Updated results set for '%s': " % benchmark)
-        print("%s : %s" % (benchmark, repr(self._defaults[benchmark])))
-
-    def RemoveDefault(self, benchmark, result):
-        if benchmark in self._defaults:
-            resultList = self._defaults[benchmark]
-            if result in resultList:
-                resultList.remove(result)
-                print("Updated results set for '%s': " % benchmark)
-                print("%s : %s" % (benchmark, repr(self._defaults[benchmark])))
-            else:
-                print(
-                    "'%s' is not in '%s's default results list."
-                    % (result, benchmark)
-                )
-        else:
-            print("Cannot find benchmark named '%s'" % benchmark)
-
-    def GetDefault(self):
-        return self._defaults
-
-    def RemoveBenchmark(self, benchmark):
-        if benchmark in self._defaults:
-            del self._defaults[benchmark]
-            print("Deleted benchmark '%s' from list of benchmarks." % benchmark)
-        else:
-            print("Cannot find benchmark named '%s'" % benchmark)
-
-    def RenameBenchmark(self, old_name, new_name):
-        if old_name in self._defaults:
-            resultsList = self._defaults[old_name]
-            del self._defaults[old_name]
-            self._defaults[new_name] = resultsList
-            print("Renamed '%s' to '%s'." % (old_name, new_name))
-        else:
-            print("Cannot find benchmark named '%s'" % old_name)
-
-    def UsageError(self, user_input):
-        # Print error message, then show options
-        print("Error:Invalid user input: '%s'" % user_input)
-        self.ShowOptions()
-
-    def ShowOptions(self):
-        print(
-            """
-Below are the valid user options and their arguments, and an explanation
-of what each option does.  You may either print out the full name of the
-option, or you may use the first letter of the option.  Case (upper or
-lower) does not matter, for the command (case of the result name DOES matter):
-
-    (L)ist                           - List all current defaults
-    (L)ist <benchmark>               - List current defaults for benchmark
-    (H)elp                           - Show this information.
-    (A)dd <benchmark> <result>       - Add a default result for a particular
-                                       benchmark (appends to benchmark's list
-                                       of results, if list already exists)
-    (D)elete <benchmark> <result>    - Delete a default result for a
-                                       particular benchmark
-    (R)emove <benchmark>             - Remove an entire benchmark (and its
-                                       results)
-    (M)ove <old-benchmark> <new-benchmark>    - Rename a benchmark
-    (Q)uit                           - Exit this program, saving changes.
-    (T)erminate                      - Exit this program; abandon changes.
-
-"""
-        )
-
-    def GetUserInput(self):
-        # Prompt user
-        print("Enter option> ")
-        # Process user input
-        inp = sys.stdin.readline()
-        inp = inp[:-1]
-        # inp = inp.lower()
-        words = inp.split(" ")
-        option = words[0]
-        option = option.lower()
-        if option in ("h", "help"):
-            self.ShowOptions()
-        elif option in ("l", "list"):
-            if len(words) == 1:
-                self.ListCurrentDefaults()
-            else:
-                self.ListCurrentDefaults(benchmark=words[1])
-        elif option in ("a", "add"):
-            if len(words) < 3:
-                self.UsageError(inp)
-            else:
-                benchmark = words[1]
-                resultList = words[2:]
-                for r in resultList:
-                    self.AddDefault(benchmark, r)
-        elif option in ("d", "delete"):
-            if len(words) != 3:
-                self.UsageError(inp)
-            else:
-                benchmark = words[1]
-                result = words[2]
-                self.RemoveDefault(benchmark, result)
-        elif option in ("r", "remove"):
-            if len(words) != 2:
-                self.UsageError(inp)
-            else:
-                benchmark = words[1]
-                self.RemoveBenchmark(benchmark)
-        elif option in ("m", "move"):
-            if len(words) != 3:
-                self.UsageError(inp)
-            else:
-                old_name = words[1]
-                new_name = words[2]
-                self.RenameBenchmark(old_name, new_name)
-        elif option in ("q", "quit"):
-            self.WriteDefaultsFile()
-
-        return option in ("q", "quit", "t", "terminate")
-
-
-def Main():
-    defaults = TelemetryDefaults()
-    defaults.ReadDefaultsFile()
-    defaults.ShowOptions()
-    done = defaults.GetUserInput()
-    while not done:
-        done = defaults.GetUserInput()
-    return 0
-
-
-if __name__ == "__main__":
-    retval = Main()
-    sys.exit(retval)
```

